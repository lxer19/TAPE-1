URL: http://www.eecs.umich.edu/PPP/PACT98.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: abandah@fet.ju.edu.jo  davidson@eecs.umich.edu  
Phone: Telephone: Intl. 732-562-3966.  
Title: creating new collective works for resale or redistribution  Origin 2000 Design Enhancements for Communication Intensive Applications  
Author: Gheith A. Abandah Amman Jordan Edward S. Davidson 
Address: Hoes Lane P.O. Box 1331 Piscataway, NJ 08855-1331, USA.  1301 Beal Avenue, Ann Arbor, MI 48109  
Affiliation: Service Center 445  Department of Electrical Engineering University of Jordan  Advanced Computer Architecture Laboratory University of Michigan  
Note: Copyright 1998 IEEE. Published in the Proceedings of PACT'98, 12-18 October 1998 in Paris, France. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for  to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions IEEE  
Abstract: The SGI Origin 2000 is designed to support a wide range of applications and has low local and remote memory latencies. However, it often has a high ratio of remote to local misses. In this paper, we evaluate the Origin 2000 performance with communication intensive applications. We use detailed execution-driven simulation of six shared-memory applications. This paper evaluates a base, Origin 2000-like system and three derived systems that incorporate techniques to reduce the communication cost by lowering the ratio of remote misses. We show that the performance of these applications is generally improved when the local bus is used in the snoopy mode, the number of processors per node is increased, the processors use the Illinois cache coherence protocol, or when adding a snoopy cache to retain remote data within each node. Illinois protocol and the interconnect cache reduce the average remote miss ratio by 16% and 21%, respectively. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Abandah. </author> <title> Reducing Communication Cost in Scalable Shared Memory Systems. </title> <type> PhD thesis, </type> <institution> University of Michi gan, </institution> <year> 1998. </year>
Reference-contexts: Additionally, since the node does not lose line ownership in RAW, there is no need to generate remote traffic if the reader later stores into the line. 4. Experimental Setup We use the Communication Contention Analysis Tool (CCAT) to evaluate the performance of these four systems using six applications <ref> [1] </ref>. CCAT is a system-level simulator that keeps track of the state of the secondary cache, interconnect cache, and directory, and fully implements the cache coherence protocol to satisfy cache misses. <p> The CCAT configuration file of system D2 is listed in <ref> [1] </ref>, which specifies all the used system parameters, occupancies, and latencies. The used latency and occupancy values are based on the typical processor and bus values from the R10000 user's manual [14], and an Origin 2000 microbenchmark evaluation [9]. Table 3. Values of the main latencies in nanoseconds. <p> The inherent characteristics of these applications are presented in [3]. Table 4 shows the two problem sizes used in this evaluation. Due to paper length restrictions, we present here the data for the larger problem size. However, an interested reader can refer to <ref> [1] </ref>, which contains the data for the two problem sizes. These applications are compiled on an SPP1600 and instrumented using SMAIT (a tool that instruments assembly code of shared-memory applications). The instrumented applications pipe detailed traces to CCAT in an execution-driven simulation setup [1]. <p> However, an interested reader can refer to <ref> [1] </ref>, which contains the data for the two problem sizes. These applications are compiled on an SPP1600 and instrumented using SMAIT (a tool that instruments assembly code of shared-memory applications). The instrumented applications pipe detailed traces to CCAT in an execution-driven simulation setup [1]. The evaluation presented in this paper is based on the performance of the main parallel phase. Table 4. Sizes of the two sets of problems analyzed. CG and SP are specified by problem size; number of itera tions. <p> Number of Memory Banks Successive cache lines are interleaved among the memory banks of the Origin 2000; each Origin 2000 node has 4 to 32 memory banks. Execution time is not very sensitive to the number of memory banks <ref> [1] </ref>; near maximum performance is achieved with only a few banks. In following evaluation, we use four banks per node. 5.3. Latency Overlapping In this section, we evaluate three processor models using the default parameters. <p> The IC size in system c does not capture the entire communication working set. SP has significant local producer-consumer communication. System C provides the best performance because it satisfies repetitive RAW and WAR locally (especially with problem size I <ref> [1] </ref>). In general, system IC performance is as good as system I, and better than I when the IC is large enough to capture the communication working set. System IC performs the best when there is significant local producer-consumer communication on remote lines. For all these applications, C2 out-performs D2.
Reference: [2] <author> G. Abandah and E. Davidson. </author> <title> A Comparative Study of Cache-Coherent Nonuniform Memory Access Systems. </title> <booktitle> In Proc. of the 12th Annual Symp. on High Performance Com puting Systems and Applications (HPCS'98), </booktitle> <month> May </month> <year> 1998. </year>
Reference-contexts: 1. Introduction The SGI Origin 2000 is designed to achieve low remote latency [11]. However, it often has a high ratio of remote to local misses, consequently its average latency and internode traffic are high <ref> [2] </ref>. The main causes of the Origin 2000's frequent remote misses are: 1. Each node contains only two processors, thus the shared data is often spread over a large number of nodes. 2. <p> local bus will be more heavily utilized and contention may increase the memory latency; allowing processors to snoop the requests of other processors decreases remote traffic, but the snooping overhead may affect the processor performance; and incorporating an interconnect cache, as in the Convex SPP1000, may increase the remote latency <ref> [6, 2] </ref>. In this paper, we evaluate four Origin 2000-like systems that address these design trade-offs using execution-driven simulation fl This research was funded in part by the National Science Foundation under Grant No. ACI9619020. of six applications.
Reference: [3] <author> G. Abandah and E. Davidson. </author> <title> Configuration Independent Analysis for Characterizing Shared-Memory Applications. </title> <booktitle> In Proc. 12th IPPS, </booktitle> <pages> pages 485491, </pages> <month> Mar. </month> <year> 1998. </year>
Reference-contexts: Applications We used six applications to evaluate the four systems: Radix, FFT, LU, and Cholesky from SPLASH-2 [19], and CG and SP from NPB [5]. The inherent characteristics of these applications are presented in <ref> [3] </ref>. Table 4 shows the two problem sizes used in this evaluation. Due to paper length restrictions, we present here the data for the larger problem size. However, an interested reader can refer to [1], which contains the data for the two problem sizes. <p> With more processors, the working sets generally get smaller. The working set of each of the six benchmarks with 32 processors is smaller than 4 MB <ref> [3] </ref>. Hence, the 4-MB cache is big enough to avoid most of the capacity misses. However, due to coherence misses, the relative contribution of miss time increases as the number of processors increases beyond 8 processors.
Reference: [4] <author> G. Astfalk and T. Brewer. </author> <title> An Overview of the HP/Convex Exemplar Hardware. </title> <type> Tech. paper, </type> <institution> Hewlett-Packard Co., </institution> <month> June </month> <year> 1997. </year> <note> http://www.hp.com/wsg/tech/technical.html. </note>
Reference-contexts: A Snoopy Interconnect Cache for Remote Data Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. Many approaches have used DRAMs to serve as large interconnect caches (Exemplar <ref> [4] </ref>, NUMA-Q [13], S3.mp [16], and NUMA-RC [20]), or SRAMs to serve as fast interconnect caches (DASH [12]), or both (R-NUMA [8] and VC-NUMA [15]).
Reference: [5] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <type> Tech. Rep. </type> <institution> RNR-94-07, NASA Ames Research Center, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Applications We used six applications to evaluate the four systems: Radix, FFT, LU, and Cholesky from SPLASH-2 [19], and CG and SP from NPB <ref> [5] </ref>. The inherent characteristics of these applications are presented in [3]. Table 4 shows the two problem sizes used in this evaluation. Due to paper length restrictions, we present here the data for the larger problem size.
Reference: [6] <author> T. Brewer. </author> <title> A Highly Scalable System Utilizing up to 128 PA-RISC Processors. </title> <booktitle> In Digest of papers, COMPCON'95, </booktitle> <pages> pages 133140, </pages> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: local bus will be more heavily utilized and contention may increase the memory latency; allowing processors to snoop the requests of other processors decreases remote traffic, but the snooping overhead may affect the processor performance; and incorporating an interconnect cache, as in the Convex SPP1000, may increase the remote latency <ref> [6, 2] </ref>. In this paper, we evaluate four Origin 2000-like systems that address these design trade-offs using execution-driven simulation fl This research was funded in part by the National Science Foundation under Grant No. ACI9619020. of six applications.
Reference: [7] <author> D. Culler, J. Singh, and A. Gupta. </author> <title> Parallel Computer Ar chitecture: A Hardware/Software Approach. </title> <publisher> Morgan Kauf mann, </publisher> <year> 1998. </year>
Reference-contexts: This review discusses these issues in the context of a coherence protocol similar to that of the Origin 2000. The level of detail of the protocol description in this paper is sufficient for our purposes; a more detailed description is presented in <ref> [7] </ref>. CC-NUMA systems are shared-memory parallel processors that interconnect nodes which contain processors and memory. In CC-NUMA systems, processors communicate through accessing shared data. The coherence protocol ensures that when a processor accesses a shared memory location, it will get the most up-to-date data value.
Reference: [8] <author> B. Falsafi and D. Wood. </author> <title> Reactive NUMA: A Design for Unifying S-COMA and CC-NUMA. </title> <booktitle> In Proc. 24th ISCA, </booktitle> <pages> pages 229240, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [4], NUMA-Q [13], S3.mp [16], and NUMA-RC [20]), or SRAMs to serve as fast interconnect caches (DASH [12]), or both (R-NUMA <ref> [8] </ref> and VC-NUMA [15]). The fourth system, IC2, is similar to S2 and uses one SRAM interconnect cache (IC) per node to reduce the cost of remote communication (Figure 4).
Reference: [9] <author> C. Hristea, D. Lenoski, and J. Keen. </author> <title> Measuring Memory Hierarchy Performance of Cache-Coherent Multiprocessors Using Micro Benchmarks. </title> <booktitle> In Supercomputing, </booktitle> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: The CCAT configuration file of system D2 is listed in [1], which specifies all the used system parameters, occupancies, and latencies. The used latency and occupancy values are based on the typical processor and bus values from the R10000 user's manual [14], and an Origin 2000 microbenchmark evaluation <ref> [9] </ref>. Table 3. Values of the main latencies in nanoseconds. Aspect Latency Processor request a 90 Processor snoop response 80 Processor data response 190 CCC 50 Memory 100 Router 40 Network link 10 a latency between detecting the miss and requesting the bus.
Reference: [10] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-29(9):241248, </volume> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: The processors run at 200 MHz and support the MESI cache-coherence protocol [18]. Each processor has a 4-MB, 2-way associative, combined secondary cache. The data cache line size is 128 bytes. The base system is sequentially consistent <ref> [10] </ref>, and maintains cache coherence by using a directory-based protocol. The directory is implemented in the memory and contains a sharing vector and a status field for each 128-byte line. The directory is accessed in parallel with each memory access. As shown in Figure 3, two nodes share one router.
Reference: [11] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: A ccNUMA Highly Scalable Server. </title> <booktitle> In Proc. 24th ISCA, </booktitle> <pages> pages 241251, </pages> <year> 1997. </year>
Reference-contexts: 1. Introduction The SGI Origin 2000 is designed to achieve low remote latency <ref> [11] </ref>. However, it often has a high ratio of remote to local misses, consequently its average latency and internode traffic are high [2]. The main causes of the Origin 2000's frequent remote misses are: 1.
Reference: [12] <author> D. Lenoski et al. </author> <title> The Stanford DASH Multiprocessor. </title> <address> Com puter, 25:6379, </address> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: The best approach to address their cost is to decrease the directory access and remote latencies. However, weaker consistency models address WAR by allowing the requester to perform its store without waiting for the acknowledgment signals <ref> [12] </ref>. The main remote home communication patterns are: RAW: Load miss for a dirty line. The local coherence controller generates a request signal to the home node. The directory in the home node is then checked, which points to the dirty node. <p> Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [4], NUMA-Q [13], S3.mp [16], and NUMA-RC [20]), or SRAMs to serve as fast interconnect caches (DASH <ref> [12] </ref>), or both (R-NUMA [8] and VC-NUMA [15]). The fourth system, IC2, is similar to S2 and uses one SRAM interconnect cache (IC) per node to reduce the cost of remote communication (Figure 4).
Reference: [13] <author> T. Lovett and R. Clapp. STiNG: </author> <title> A CC-NUMA Computer for the Commercial Marketplace. </title> <booktitle> In Proc. 23rd ISCA, </booktitle> <pages> pages 308317, </pages> <year> 1996. </year>
Reference-contexts: A Snoopy Interconnect Cache for Remote Data Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [4], NUMA-Q <ref> [13] </ref>, S3.mp [16], and NUMA-RC [20]), or SRAMs to serve as fast interconnect caches (DASH [12]), or both (R-NUMA [8] and VC-NUMA [15]). The fourth system, IC2, is similar to S2 and uses one SRAM interconnect cache (IC) per node to reduce the cost of remote communication (Figure 4).
Reference: [14] <institution> MIPS Technologies Inc. </institution> <note> MIPS R10000 Microprocessor User's Manual, </note> <month> Jan. </month> <year> 1997. </year> <note> Version 2.0. </note>
Reference-contexts: The figure shows eight routerseach utilizing five of its ports interconnecting 16 nodes in a cube configuration. The processor pair share one split-transaction bus to communicate with the rest of the system. This bus has two modes of operation, snoopy and point-to-point <ref> [14] </ref>. In the snoopy mode, each processor observes the bus requests of other processors and checks its cache to ensure cache coherence. <p> The CCAT configuration file of system D2 is listed in [1], which specifies all the used system parameters, occupancies, and latencies. The used latency and occupancy values are based on the typical processor and bus values from the R10000 user's manual <ref> [14] </ref>, and an Origin 2000 microbenchmark evaluation [9]. Table 3. Values of the main latencies in nanoseconds. Aspect Latency Processor request a 90 Processor snoop response 80 Processor data response 190 CCC 50 Memory 100 Router 40 Network link 10 a latency between detecting the miss and requesting the bus.
Reference: [15] <author> A. Moga and M. Dubois. </author> <title> The Effectiveness of SRAM Net work Caches in Clustered DSMs. </title> <booktitle> In Proc. </booktitle> <address> HPCA-4, </address> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [4], NUMA-Q [13], S3.mp [16], and NUMA-RC [20]), or SRAMs to serve as fast interconnect caches (DASH [12]), or both (R-NUMA [8] and VC-NUMA <ref> [15] </ref>). The fourth system, IC2, is similar to S2 and uses one SRAM interconnect cache (IC) per node to reduce the cost of remote communication (Figure 4).
Reference: [16] <author> A. Nowatzyk et al. </author> <title> The S3.mp Scalable Shared Memory Multiprocessor. </title> <booktitle> In Proc. ICPP, </booktitle> <pages> pages I.1I.10, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: A Snoopy Interconnect Cache for Remote Data Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [4], NUMA-Q [13], S3.mp <ref> [16] </ref>, and NUMA-RC [20]), or SRAMs to serve as fast interconnect caches (DASH [12]), or both (R-NUMA [8] and VC-NUMA [15]). The fourth system, IC2, is similar to S2 and uses one SRAM interconnect cache (IC) per node to reduce the cost of remote communication (Figure 4).
Reference: [17] <author> M. Papamarcos and J. Patel. </author> <title> A Low Overhead Coherence Solution for Multiprocessors with Private Cache Memories. </title> <booktitle> In Proc. 11th ISCA, </booktitle> <pages> pages 348354, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: Intuitively, we expect S2 to have better performance than D2 with applications that have enough local communication to overcome the added overhead of snooping. 3.3. Allowing Caches to Supply Clean Data The Illinois bus cache coherence protocol <ref> [17] </ref> is similar to the MESI protocol used in S2, with one main difference. Whereas, in the standard MESI protocol, a processor only supplies data when the requested line is dirty in its cache, the Illinois protocol allows a processor to supply the line whenever it has a copy.
Reference: [18] <author> P. Sweazy and A. Smith. </author> <title> A Class of Compatible Cache Con sistency Protocols and Their Support by the IEEE Futurebus. </title> <booktitle> In Proc. 13th ISCA, </booktitle> <pages> pages 414423, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: As shown in Figure 2, each node has a cache coherence controller. The CCC has multiple internal paths to interconnect the processor pair, four memory banks (with attached directory), the I/O devices, and the interconnection network. The processors run at 200 MHz and support the MESI cache-coherence protocol <ref> [18] </ref>. Each processor has a 4-MB, 2-way associative, combined secondary cache. The data cache line size is 128 bytes. The base system is sequentially consistent [10], and maintains cache coherence by using a directory-based protocol.
Reference: [19] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodology Considerations. </title> <booktitle> In Proc. 22nd ISCA, </booktitle> <pages> pages 2436, </pages> <year> 1995. </year>
Reference-contexts: When there is no contention, it takes 460 nsec to satisfy a processor miss from the local memory, and 690 nsec from a remote memory through one router. 4 4.2. Applications We used six applications to evaluate the four systems: Radix, FFT, LU, and Cholesky from SPLASH-2 <ref> [19] </ref>, and CG and SP from NPB [5]. The inherent characteristics of these applications are presented in [3]. Table 4 shows the two problem sizes used in this evaluation. Due to paper length restrictions, we present here the data for the larger problem size.
Reference: [20] <author> Z. Zhang and J. Torrellas. </author> <title> Reducing Remote Conflict Misses: NUMA with Remote Cache versus COMA. </title> <booktitle> In Proc. </booktitle> <volume> HPCA 3, </volume> <pages> pages 272281, </pages> <year> 1997. </year> <month> 10 </month>
Reference-contexts: A Snoopy Interconnect Cache for Remote Data Caching remote data in local specialized caches is a popular approach used mainly to reduce the cost of capacity misses. Many approaches have used DRAMs to serve as large interconnect caches (Exemplar [4], NUMA-Q [13], S3.mp [16], and NUMA-RC <ref> [20] </ref>), or SRAMs to serve as fast interconnect caches (DASH [12]), or both (R-NUMA [8] and VC-NUMA [15]). The fourth system, IC2, is similar to S2 and uses one SRAM interconnect cache (IC) per node to reduce the cost of remote communication (Figure 4).
References-found: 20

