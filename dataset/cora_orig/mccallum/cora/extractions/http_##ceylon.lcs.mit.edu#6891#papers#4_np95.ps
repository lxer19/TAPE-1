URL: http://ceylon.lcs.mit.edu/6891/papers/4_np95.ps
Refering-URL: http://ceylon.lcs.mit.edu/6891/reading_list.html
Root-URL: 
Email: can@cs.appstate.edu pollock@cis.udel.edu  
Phone: (704)262-2359 (302) 831-1953  
Title: An Experimental Study of Several Cooperative Register Allocation and Instruction Scheduling Strategies  
Author: Cindy Norris Lori L. Pollock 
Address: Boone, NC 28608 Newark, DE 19716  
Affiliation: Mathematical Sciences Computer and Information Sciences Appalachian State University University of Delaware  
Abstract: Compile-time reordering of low level instructions is successful in achieving large increases in performance of programs on fine-grain parallel machines. However, because of the interdependences between instruction scheduling and register allocation, a lack of cooperation between the scheduler and register allocator can result in generating code that contains excess register spills and/or a lower degree of parallelism than actually achievable. This paper describes a strategy for providing cooperation between register allocation and both global and local instruction scheduling. We experimentally compare this strategy with other cooperative and uncooperative scenarios. Our experiments indicate that the greatest speedups are obtained by performing either cooperative or uncooperative global instruction scheduling with cooperative register allocation and local instruction scheduling. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abraham and K. Padmanabhan. </author> <title> Instruction reorganization for variable-length pipelined microprocessor. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <address> New York, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: A scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler <ref> [1, 3, 16, 21, 2] </ref>; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler [18, 24, 4, 14, 32].
Reference: [2] <author> Steven. J. Beaty. </author> <title> Lookahead scheduling. </title> <booktitle> In Proceedings of the Twenty-fifth International Symposium on Microarchitecture, </booktitle> <pages> pages 256-259, </pages> <address> Portland, OR, </address> <year> 1992. </year>
Reference-contexts: A scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler <ref> [1, 3, 16, 21, 2] </ref>; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler [18, 24, 4, 14, 32].
Reference: [3] <author> David Bernstein. </author> <title> An improved approximation algorithm for scheduling pipelined machines. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: A scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler <ref> [1, 3, 16, 21, 2] </ref>; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler [18, 24, 4, 14, 32].
Reference: [4] <author> David Bernstein and Michael Rodeh. </author> <title> Global instruction scheduling for superscalar machines. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, CANADA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler [1, 3, 16, 21, 2]; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler <ref> [18, 24, 4, 14, 32] </ref>. The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses.
Reference: [5] <author> David A. Berson, Rajiv Gupta, and Mary Lou Sof-fa. </author> <title> URSA: A unified resource allocator for registers and functional units in VLIW architectures. In IFIP Working Conference on Architectures and Compilation Techniques for Fine and Medium Grain Parallelism, </title> <address> Orlando, Florida, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation [17], cooperation between local instruction scheduling and global register allocation [7, 26, 29], and cooperation between global instruction scheduling and register allocation <ref> [15, 25, 5, 6, 28] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation.
Reference: [6] <author> David A. Berson, Rajiv Gupta, and Mary Lou Sof-fa. </author> <title> Resource spackling: A framework for integrating register allocation in local and global schedulers. </title> <booktitle> In PACT `94: International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation [17], cooperation between local instruction scheduling and global register allocation [7, 26, 29], and cooperation between global instruction scheduling and register allocation <ref> [15, 25, 5, 6, 28] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation.
Reference: [7] <author> David G. Bradlee, Susan J. Eggers, and Robert R. Henry. </author> <title> Integrating register allocation and instruction scheduling for RISCs. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 122-131, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation [17], cooperation between local instruction scheduling and global register allocation <ref> [7, 26, 29] </ref>, and cooperation between global instruction scheduling and register allocation [15, 25, 5, 6, 28]. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> The procedure, ips sched, based on integrated postpass scheduling (IPS) [17] and improved IPS <ref> [7] </ref>, performs local scheduling on each region while keeping track of the number of live variables. The ips sched procedure works by oscillating in its heuristic for scheduling based on whether the current number of live variables has reached the register limit.
Reference: [8] <author> Preston Briggs, Keith D. Cooper, and Linda Torczon. </author> <title> Rematerialization. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure, <p> SSG was designed by making key phases of a global register allocator <ref> [8] </ref> sensitive to the subsequent local instruction scheduling phase. 3.1 The base allocator Like most global register allocators, our base allocator, OA, which is an implementation of the optimistic allocator developed by Briggs, Cooper, and Torczon [8], is based on a graph coloring allocation method, namely, the method developed by Chaitin <p> SSG was designed by making key phases of a global register allocator <ref> [8] </ref> sensitive to the subsequent local instruction scheduling phase. 3.1 The base allocator Like most global register allocators, our base allocator, OA, which is an implementation of the optimistic allocator developed by Briggs, Cooper, and Torczon [8], is based on a graph coloring allocation method, namely, the method developed by Chaitin [10]. Figure 5 depicts the high level view of the base allocator, OA. The input to the first phase, the RENUMBER phase, consists of intermediate code generated assuming an unlimited number of virtual registers. <p> In order to do this comparison, the optimistic allocator (OA) of Briggs, Cooper, and Torczon <ref> [8] </ref> was implemented to perform the conventional register allocation. The registers are allocated using a round robin approach, because experimental results indicate that round robin provides a better allocation in the context of code scheduling and global register allocation than a first fit approach.
Reference: [9] <author> David Callahan and Brian Koblenz. </author> <title> Register allocation via hierarchical graph coloring. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 192-203, </pages> <address> Toronto, CANADA, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure,
Reference: [10] <author> Gregory Chaitin, Marc Auslander, Ashok K. Chan-dra, John Cocke, Martin E. Hopkins, and Peter W. Markstein. </author> <title> Register allocation via coloring. </title> <journal> Computer Languages, </journal> <volume> 6 </volume> <pages> 47-57, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure, <p> sensitive to the subsequent local instruction scheduling phase. 3.1 The base allocator Like most global register allocators, our base allocator, OA, which is an implementation of the optimistic allocator developed by Briggs, Cooper, and Torczon [8], is based on a graph coloring allocation method, namely, the method developed by Chaitin <ref> [10] </ref>. Figure 5 depicts the high level view of the base allocator, OA. The input to the first phase, the RENUMBER phase, consists of intermediate code generated assuming an unlimited number of virtual registers. First, the Static Single Assignment (SSA) graph [12] is computed.
Reference: [11] <author> Frederick Chow and John Hennessy. </author> <title> The priority--based coloring approach to register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure,
Reference: [12] <author> Ron Cytron, Jeanne Ferrante, Barry Rosen, and Mark Wegman. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Figure 5 depicts the high level view of the base allocator, OA. The input to the first phase, the RENUMBER phase, consists of intermediate code generated assuming an unlimited number of virtual registers. First, the Static Single Assignment (SSA) graph <ref> [12] </ref> is computed. This representation transforms the code so that each variable use corresponds to a single definition. The RENUMBER stage also determines which values in the program can be rematerialized.
Reference: [13] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <year> 1987. </year>
Reference-contexts: The PDG representation of a program is described, followed by region scheduling, and finally RASER. Due to space limitations, these techniques are described briefly here. We refer the reader to [28] for more details. 2.1 Program dependence graphs The program dependence graph (PDG) <ref> [13] </ref> for a program is a directed graph that represents the relevant control and data dependences between statements in the program. The nodes of the graph are statements and predicate expressions that occur in the program. An edge represents either a control dependence or a data dependence among program components.
Reference: [14] <author> J. A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30(7) </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler [1, 3, 16, 21, 2]; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler <ref> [18, 24, 4, 14, 32] </ref>. The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses.
Reference: [15] <author> S. M. Freudenberger and J. C. Ruttenberg. </author> <title> Phase ordering of register allocation and instruction scheduling. In Code Generation Concepts, Tools, Techniques: </title> <booktitle> Proceedings of the International Workshop on Code Generation, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation [17], cooperation between local instruction scheduling and global register allocation [7, 26, 29], and cooperation between global instruction scheduling and register allocation <ref> [15, 25, 5, 6, 28] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation.
Reference: [16] <author> P. B. Gibbons and S. S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: 1 Introduction The major focus of optimizing compilers for architectures supporting instruction level parallelism has been the rearrangement of the low level code with the ultimate goal of increasing the amount of parallelism that a program can exploit by hiding instruction latencies and thus reducing possible run-time delays <ref> [21, 16, 34] </ref>. <p> A scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler <ref> [1, 3, 16, 21, 2] </ref>; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler [18, 24, 4, 14, 32]. <p> strategy. 4 Experimental study 4.1 Experimental setup In this section, we report on our results of comparing the performance of the cooperative schemes to the performance of the postpass scheduling approach where a conventional register allocator is followed by a local code scheduler (BBSCH) based on the Gibbons and Muchnick <ref> [16] </ref> basic block scheduling algorithm. In order to do this comparison, the optimistic allocator (OA) of Briggs, Cooper, and Torczon [8] was implemented to perform the conventional register allocation.
Reference: [17] <author> James R. Goodman and Wei-Chung Hsu. </author> <title> Code scheduling and register allocation in large basic blocks. </title> <booktitle> In Supercomputing '88 Proceedings, </booktitle> <pages> pages 442-452, </pages> <address> Orlando, Florida, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation <ref> [17] </ref>, cooperation between local instruction scheduling and global register allocation [7, 26, 29], and cooperation between global instruction scheduling and register allocation [15, 25, 5, 6, 28]. <p> The procedure, ips sched, based on integrated postpass scheduling (IPS) <ref> [17] </ref> and improved IPS [7], performs local scheduling on each region while keeping track of the number of live variables. The ips sched procedure works by oscillating in its heuristic for scheduling based on whether the current number of live variables has reached the register limit.
Reference: [18] <author> Rajiv Gupta and Mary Lou Soffa. </author> <title> Region scheduling: An approach for detecting and redistributing parallelism. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(4) </volume> <pages> 421-431, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler [1, 3, 16, 21, 2]; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler <ref> [18, 24, 4, 14, 32] </ref>. The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. <p> Section 5 summarizes the work and mentions future directions. 2 Cooperative global scheduling We provide cooperation between global scheduling and register allocation via our technique known as Register Allocation Sensitive Region scheduling (RASER) [28], which is a modification of a global instruction scheduling technique known as region scheduling <ref> [18] </ref> to take into consideration the requirements of a subsequent register allocation phase. Region scheduling performs scheduling over a program dependence graph (PDG) attempting to create regions in the PDG that contain equal amounts of fine-grain parallelism. <p> After region nodes are inserted, each predicate node has at most one true outgoing edge and one false outgoing edge. Figure 1 shows a program segment and its PDG representation. 2.2 Region scheduling Region scheduling <ref> [18] </ref> is a global instruction scheduling technique which operates on the PDG. <p> The estimated parallelism in a region R i is defined as the ratio O i =D i , where O i is the number of operations in the region and D i is the length of the longest data dependence path in the data dependence subgraph of the region <ref> [18] </ref>. The need parallelism function builds a list of regions with estimates of parallelism that are less than that available in the underlying architecture. <p> A region contains an excess amount of parallelism if the estimate in the region is greater than the amount exploitable by the underlying architecture. Like <ref> [18] </ref>, the region scheduling transformations are applied in order of increasing difficulty. List i is the current region being examined for code movement because it was found to have insufficient parallelism. <p> SSG may also reorder the code to reflect dag edges added dur 1 Our prototype implementation of region scheduling is not as aggressive as <ref> [18] </ref>. We would expect greater speedups than those indicated in this paper by a more aggressive implementation. ing register allocation. SSG also accepts as input an opcode file indicating the degree of pipelining of the target architecture.
Reference: [19] <author> Rajiv Gupta, Mary Lou Soffa, and Tim Steele. </author> <title> Register allocation via clique separators. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure,
Reference: [20] <author> Laurie J. Hendren, Guang R. Gao, Erik R. Altman, and Chandrika Mukerji. </author> <title> A register allocation framework based on hierarchical cyclic interval graphs. </title> <booktitle> In International Workshop on Compiler Construction, </booktitle> <address> Paderdorn, GERMANY, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure,
Reference: [21] <author> J. L. Hennessy and Thomas Gross. </author> <title> Postpass code optimization of pipeline constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: 1 Introduction The major focus of optimizing compilers for architectures supporting instruction level parallelism has been the rearrangement of the low level code with the ultimate goal of increasing the amount of parallelism that a program can exploit by hiding instruction latencies and thus reducing possible run-time delays <ref> [21, 16, 34] </ref>. <p> A scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler <ref> [1, 3, 16, 21, 2] </ref>; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler [18, 24, 4, 14, 32].
Reference: [22] <author> Wei Chung Hsu, Charles N. Fischer, and James R. Goodman. </author> <title> On the minimization of loads/stores in local register allocation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(10) </volume> <pages> 1252-1260, </pages> <year> 1989. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local <ref> [22] </ref>, global [11, 10, 9, 30, 8, 20, 27, 23, 19], or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212.
Reference: [23] <author> P. Kolte and Mary Jean Harrold. </author> <title> Load/store range analysis for global register allocation. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure,
Reference: [24] <author> Monica Lam. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <address> Atlanta, Geor-gia, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler [1, 3, 16, 21, 2]; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler <ref> [18, 24, 4, 14, 32] </ref>. The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses.
Reference: [25] <author> S-M Moon and K. Ebcioglu. </author> <title> An efficient resource-constrained global scheduling technique for superscalar and VLIW processors. </title> <booktitle> In Proceedings of the Twenty-fifth International Symposium on Microarchi-tecture, </booktitle> <address> Portland, OR, </address> <year> 1992. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation [17], cooperation between local instruction scheduling and global register allocation [7, 26, 29], and cooperation between global instruction scheduling and register allocation <ref> [15, 25, 5, 6, 28] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation.
Reference: [26] <author> Cindy Norris and Lori L. Pollock. </author> <title> A scheduler-sensitive global register allocator. </title> <booktitle> In Supercomputing '93 Proceedings, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation [17], cooperation between local instruction scheduling and global register allocation <ref> [7, 26, 29] </ref>, and cooperation between global instruction scheduling and register allocation [15, 25, 5, 6, 28]. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> The global instruction scheduler has been made sensitive to the subsequent register allocation phase [28]. The register allocator has been modified to take into consideration the actions of the subsequent local instruction scheduling phase <ref> [26] </ref>. <p> Loop peeling, without also eliminating dependences, will never increase the number of interferences found by the subsequent register allocation phase. 3 Cooperative register allocation To provide cooperation between local instruction scheduling and register allocation, we used a Scheduler Sensitive Global Register Allocator (SSG) <ref> [26] </ref>. <p> Figure 7 (b) contains the interference graph for the modified dag in SSG can add dag edges to eliminate interferences before the BUILD phase, while the interference graph is being built, or only during the SIMPLIFY phase <ref> [26] </ref>. The tradeoffs between these strategies are unnecessary restriction on the scheduler by the unnecessary addition of dag edges or a bad choice of dag edges to add, and the compile time expense in determining the best edges to add. <p> Experimentally, we determined that the best strategy is to add dag edges to eliminate interferences before building the interference graph, and during the SIMPLIFY stage as needed to color the interference graph <ref> [26] </ref>. Like the base allocator's SIMPLIFY phase, the interference graph is first simplified by removing each node which has fewer than k neighbors where k is the number of physical registers.
Reference: [27] <author> Cindy Norris and Lori L. Pollock. </author> <title> Register allocation over the program dependence graph. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure, <p> This combination was particularly beneficial on a highly-pipelined machine. Our future work includes investigating the incorporation of scheduler-sensitivity into a hierarchical global register allocator <ref> [27] </ref> with the goal of reducing the compile-time expense of SSG.
Reference: [28] <author> Cindy Norris and Lori L. Pollock. </author> <title> Register allocation sensitive region scheduling. </title> <booktitle> In PACT `95: International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <address> Limassol, Cyprus, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation [17], cooperation between local instruction scheduling and global register allocation [7, 26, 29], and cooperation between global instruction scheduling and register allocation <ref> [15, 25, 5, 6, 28] </ref>. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation. <p> Our approach has been to maintain three separate phases of global instruction scheduling, register allocation, and local instruction scheduling. The global instruction scheduler has been made sensitive to the subsequent register allocation phase <ref> [28] </ref>. The register allocator has been modified to take into consideration the actions of the subsequent local instruction scheduling phase [26]. <p> Section 5 summarizes the work and mentions future directions. 2 Cooperative global scheduling We provide cooperation between global scheduling and register allocation via our technique known as Register Allocation Sensitive Region scheduling (RASER) <ref> [28] </ref>, which is a modification of a global instruction scheduling technique known as region scheduling [18] to take into consideration the requirements of a subsequent register allocation phase. <p> The PDG representation of a program is described, followed by region scheduling, and finally RASER. Due to space limitations, these techniques are described briefly here. We refer the reader to <ref> [28] </ref> for more details. 2.1 Program dependence graphs The program dependence graph (PDG) [13] for a program is a directed graph that represents the relevant control and data dependences between statements in the program. The nodes of the graph are statements and predicate expressions that occur in the program. <p> The RASSG scenario performs slightly better than REGSSG for 8 registers and worse than REGSSG for 16, 24, and 32 registers. This agrees with the results noted in <ref> [28] </ref>. As the number of registers increases, reordering to increase fine-grain parallelism becomes more important than reordering to reduce spilling.
Reference: [29] <author> S. S. Pinter. </author> <title> Register allocation with instruction scheduling: a new approach. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Techniques have been developed to provide communication of requirements and cooperation between local instruction scheduling and local register allocation [17], cooperation between local instruction scheduling and global register allocation <ref> [7, 26, 29] </ref>, and cooperation between global instruction scheduling and register allocation [15, 25, 5, 6, 28]. Experimental results from these groups indicate that the cooperative schemes indeed generate more efficient code than a conventional code generator that treats register allocation and instruction scheduling in isolation.
Reference: [30] <author> Todd A. Proebsting and Charles N. Fischer. </author> <title> Probabilistic register allocation. </title> <booktitle> In Proceedings of the SIG-PLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 300-310, </pages> <address> San Francis-co, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global <ref> [11, 10, 9, 30, 8, 20, 27, 23, 19] </ref>, or interprocedural [33, 31] depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure,
Reference: [31] <author> Peter Steenkiste and John Hennessy. </author> <title> A simple inter-procedural register allocation algorithm and its effectiveness for LISP. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <month> January </month> <year> 1989. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global [11, 10, 9, 30, 8, 20, 27, 23, 19], or interprocedural <ref> [33, 31] </ref> depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure, or across procedure boundaries, respectively.
Reference: [32] <author> Philip H. Sweany and Steven J. Beaty. </author> <title> Dominator-path scheduling a global scheduling method. </title> <booktitle> In Proceedings of the Twenty-fifth International Symposium on Microarchitecture, </booktitle> <pages> pages 260-263, </pages> <address> Portland, OR, </address> <year> 1992. </year>
Reference-contexts: scheduler that rearranges code within a basic block in isolation of the rest of the program is called a local scheduler [1, 3, 16, 21, 2]; a scheduler that moves instructions across basic blocks by considering the effects of code movement on a global level is called a global scheduler <ref> [18, 24, 4, 14, 32] </ref>. The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses.
Reference: [33] <author> David W. Wall. </author> <title> Register allocation at link time. </title> <journal> SIG-PLAN Notices, </journal> <volume> 21(7) </volume> <pages> 264-275, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: The goal of an ambitious register allocator is to allocate the machine's physical registers to program values to minimize the number of run-time memory accesses. Register allocation techniques are either local [22], global [11, 10, 9, 30, 8, 20, 27, 23, 19], or interprocedural <ref> [33, 31] </ref> depending on whether the allocator attempts an assignment of registers to values within basic blocks in isolation of other basic blocks, across basic blocks of fl This work was partially supported by NSF under grant CCR-9300212. a procedure, or across procedure boundaries, respectively.
Reference: [34] <author> S. Weiss and J. E. Smith. </author> <title> A study of scalar compilation techniques for pipelined supercomputers. </title> <booktitle> In Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: 1 Introduction The major focus of optimizing compilers for architectures supporting instruction level parallelism has been the rearrangement of the low level code with the ultimate goal of increasing the amount of parallelism that a program can exploit by hiding instruction latencies and thus reducing possible run-time delays <ref> [21, 16, 34] </ref>.
References-found: 34

