URL: http://www.isi.edu/isd/VET/AAAI-Fall97-itswrkshp.ps.gz
Refering-URL: http://www.isi.edu/isd/VET/vet-body.html
Root-URL: http://www.isi.edu
Email: rickel@isi.edu  
Title: Series: Intelligent Tutoring Systems Authoring Tools Agents that Learn to Instruct 1  
Author: Richard Angros, Jr. W. Lewis Johnson and Jeff Rickel angros, johnson, 
Address: 4676 Admiralty Way, Marina del Rey, CA 90292-6695  
Affiliation: Hughes Aircraft Company Information Sciences Institute Computer Sciences Department University of Southern California  Information Sciences Institute Computer Sciences Department University of Southern California  
Note: To appear in AAAI 1997 Fall Symposium  
Abstract: 1 Copyright 1997, American Association for Artificial Intelligence. All rights reserved . The official version of this paper has been published by the American Association for Artificial Intelligence (http://www.aaai.org). Abstract This paper describes a software agent that learns procedural knowledge from a human instructor well enough to teach human students. In order to teach, the agent needs more than the ability to perform a procedure. It must also be able to monitor human students performing the procedure and be able to articulate the reasons why actions are necessary. Our research concentrates on helping an instructor instruct the agent in a natural manner, on reducing the burden on the instructor, and on focusing learning on the procedure being taught. Initially the agent has little domain knowledge. The instructor demonstrates a procedure by directly manipulating a simulated environment. However, one demonstration is not sufficient for understanding the causal relationships between a demonstrations actions. Unfortunately, the more demonstrations a procedure requires, the greater the instructors burden. However, fewer demonstrations can be required if the agent autonomously experiments. Our experiments attempt to understand the causal dependencies between a demonstrations actions by perturbing the order of the demonstrations actions. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Benson, S. </author> <year> 1995. </year> <title> Inductive learning of reactive action models. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> 47-54. </pages> <address> San Francisco, Calif.; </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For our purposes, a domain task (or procedure ) is a sequence of steps, where each step represents an action to perform. In fact, almost all systems that learn procedural knowledge only attempt to achieve this goal <ref> (Pearson 1995, Huffman and Laird 1995, Shen 1994, Benson 1995) </ref>. In contrast, our goal is more challenging because teaching requires using knowledge in multiple ways. An agent that teaches also needs to be able to explain a task to students and to monitor human students performing a task. <p> While many systems contain enough knowledge to produce some form of explanation, their explanations would seem inadequate to humans. Many explanations would be either too complicated (Shen 1994), contain incorrect knowledge (Pearson 1995) or the have unacceptable content <ref> (Benson 1995) </ref>. One class of system that has problems explaining causal relationships are systems that only learn how to react in specific situations (Huffman and Laird 1995, Pearson 95). <p> Some systems learn a representation for preconditions that does not support articulating explanations that a human would find reasonable. TRAIL <ref> (Benson 1995) </ref> and LIVE (Shen 1994) learn preconditions that are in disjunctive normal form. The preconditions are formed so that they cover all examples of when the operator should be applied. There is no effort to learn preconditions of which a human would approve.
Reference: <editor> Cypher, A. et al., eds. </editor> <year> 1993. </year> <title> Watch What I Do: Programming by Demonstration . Cambridge, </title> <address> Mass.: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: The instructor demonstrates a procedure by directly manipulating the simulated environment. In our current system, direct manipulation is done by using a mouse in a 3-D virtual world. Acquiring procedures in this manner is called programming by demonstration <ref> (Cypher et al. 1993) </ref>. Because demonstrations are interactive, it would be too burdensome on the instructor to require more than a few demonstrations. The limited number of demonstrations requires that the agent get maximum use out of each. <p> Related Work Many Programming By Demonstration (PBD) systems learn how to perform simple procedures. Most of the basic techniques needed by Diligent have been discussed in the literature. The best reference is Cyphers book <ref> (Cypher et al. 1993) </ref>. Previous PBD systems learn to correctly perform a procedure by executing it steps in some order rather than to understand the causal relationships between the steps.
Reference: <author> Chi, M. T. H.; Bassok, M.; Lewis, M. W.; Reimann, P.; and Glaser, R. </author> <year> 1989. </year> <title> Self-explanations: How students study and use examples to solve problems. </title> <booktitle> Cognitive Science , 13 </booktitle> <pages> 145-182. </pages>
Reference-contexts: In fact, studies of human learning seem to support this approach. Human students who are good problem solvers tend to study a few examples in detail rather than take a shallow look at many examples <ref> (Chi et al. 1989) </ref>. Experiments can be focused on a demonstration by perturbing a trace of the demonstration, executing it and observing what happens. Some earlier systems perturb a demonstration by changing the state of the environment (Porter and Kubler 1986). <p> Diligent performs experiments by observing how the actions in a demonstration influence each other. Even though Diligent makes no attempt to model human cognition, this approach was motivated by studies of human learning <ref> (Chi et al. 1989) </ref>. Another system that learns by examining a demonstration in detail is PET (Porter and Kubler 1986). PET requires the ability to make small changes to the state of the environment. In contrast, Diligent does not assume the ability to make arbitrary changes to the environments state.
Reference: <author> Gil, Y. </author> <year> 1992. </year> <title> Acquiring Domain Knowledge for Planning by Experimentation. </title> <type> Ph.D. </type> <institution> diss., School of Computer Science, Carnegie Mellon Univ. </institution>
Reference-contexts: In contrast, a practice problem has an initial state and a final, goal state but does not specify the sequence of actions necessary to change the environment to the goal state. Practice problems are useful for learning general knowledge in systems that do not use demonstrations <ref> ( Gil 1992) </ref>. Practice problems are also useful in systems have a lot of domain knowledge. However, practice problems are hard to solve with little domain knowledge and may not be focused on understanding the dependencies between the actions in a demonstration. <p> Instructo-Soar (Huffman and Laird 1995) and IMPROV (Pearson 1996) learn rules to reactively perform procedures. Reactive systems learn to recognize a portion of the current state rather than the preconditions that cause dependencies between states. Some systems require many practice problems or demonstrations. EXPO <ref> (Gil 1992) </ref> refines an incomplete domain theory using practice problems, but EXPO is unable to correct errors in its knowledge and does not use demonstrations. OBSERVER (Wang 96) learns operator models using many demonstrations and practice problems. Diligent and OBSERVER use very similar precondition learning algorithms.
Reference: <author> Huffman, S. B.; and Laird, J. E. </author> <year> 1995. </year> <title> Flexibly instructable agents. </title> <journal> Journal of Artificial Intelligence Research , 3 </journal> <pages> 271-324. </pages>
Reference-contexts: For our purposes, a domain task (or procedure ) is a sequence of steps, where each step represents an action to perform. In fact, almost all systems that learn procedural knowledge only attempt to achieve this goal <ref> (Pearson 1995, Huffman and Laird 1995, Shen 1994, Benson 1995) </ref>. In contrast, our goal is more challenging because teaching requires using knowledge in multiple ways. An agent that teaches also needs to be able to explain a task to students and to monitor human students performing a task. <p> Many explanations would be either too complicated (Shen 1994), contain incorrect knowledge (Pearson 1995) or the have unacceptable content (Benson 1995). One class of system that has problems explaining causal relationships are systems that only learn how to react in specific situations <ref> (Huffman and Laird 1995, Pearson 95) </ref>. In short, the system should not only be able to explain what it is doing but also be able to describe the causal relationships between its actions. Another part of teaching is monitoring students as they perform a task. <p> TRAIL (Benson 1995) and LIVE (Shen 1994) learn preconditions that are in disjunctive normal form. The preconditions are formed so that they cover all examples of when the operator should be applied. There is no effort to learn preconditions of which a human would approve. Instructo-Soar <ref> (Huffman and Laird 1995) </ref> and IMPROV (Pearson 1996) learn rules to reactively perform procedures. Reactive systems learn to recognize a portion of the current state rather than the preconditions that cause dependencies between states. Some systems require many practice problems or demonstrations.
Reference: <author> Johnson, W. L. </author> <year> 1995. </year> <title> Pedagogical agents in virtual learning environments. </title> <booktitle> In Proceedings of the International Conference on Computers in Education, </booktitle> <pages> 41-48; AACE. </pages>
Reference: <author> Johnson, W. L.; Rickel, J.; Stiles, R.; and Munro, A. </author> <year> 1997. </year> <title> Integrating pedagogical agents into virtual environments. Presence , forthcoming. </title>
Reference: <author> Krishnamurthy, B. ed. </author> <year> 1995. </year> <title> Practical Reusable UNIX Software . New York, </title> <publisher> NY.; John Wiley & Sons. </publisher> <address> http://portal.research.bell-labs.com/orgs/ssr/book/reuse Laird, </address> <note> J. </note> <author> E.; Newell, A.; and Rosenbloom, P. S. </author> <year> 1987. </year> <title> Soar: </title> <booktitle> An architecture for general intelligence . Artificial Intelligence , 33(1) </booktitle> <pages> 1-64. </pages>
Reference-contexts: After a procedure has demonstrated, the agent needs a representation of the procedure that can be understood by 4 The graph was produced with TCL/TK (Ousterhout 1994) and the tkdot portion of the Graph Visualization tools from AT&T Laboratories and Bell Laboratories (Lucent Technology) <ref> (Krishnamurthy 1995) </ref>. the instructor. Figure 4 shows a procedure represented as a graph. Each of the procedures steps is represented by node. By selecting a node, the instructor can access menus that allow examination and modification of the agents understanding of a step and its relation to other steps.
Reference: <author> McAllester, D.; and Rosenblitt, D. </author> <year> 1991. </year> <title> Systematic Nonlinear Planning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> 634-639. </pages> <address> Menlo Park, Calif.; </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A partially ordered plan contains a set of steps and sets of causal links and ordering constraints. Each step corresponds to an action performed in the environment. Causal links record how the effects of one step are preconditions for subsequent steps <ref> (McAllester and Rosenblitt 1991) </ref>. Ordering constraints indicate the required order of execution between two steps. Ordering constraints prevent latter steps from clobbering the preconditions of earlier steps. The preconditions and effects of steps are derived using operator models.
Reference: <author> Mitchell, T. M. </author> <year> 1978. </year> <title> Version Spaces: An Approach to Concept Learning. </title> <type> Ph.D. </type> <institution> diss., Dept. of Computer Science, Stanford Univ. </institution>
Reference-contexts: Because the preconditions are heuristic, the agent is uncertain about them. The uncertainty in the heuristic preconditions is represented by using a version space to bound the set of candidate preconditions. A version space identifies the most specific and most general candidate preconditions <ref> (Mitchell 1978) </ref>. By bounding its uncertainty, the agent can focus learning towards reducing the uncertainty. Additionally, the explicit representation of the agents uncertainty supports communication with the instructor. After the agent has finished experimenting, it can ask the instructor questions to verify its task knowledge. <p> Each step corresponds to some action in the environment. If the agent has not seen the action before, it creates a new operator. Otherwise, an existing operator is refined. Each operator maps a set of preconditions to a set of effects. Preconditions are stored in a version space representation <ref> (Mitchell 1978) </ref>. The version space bounds the real precondition between a most specific and a most general candidate precondition. Because version space learning may converge slowly, the agent uses a heuristic precondition in between the most general and specific candidate preconditions. The heuristics favor state changes during the procedure.
Reference: <author> Munro, A. ; Johnson, M. C.; Surmon, D. S.; and Wogulis, J. L. </author> <year> 1993. </year> <title> Attribute-centered simulation authoring for instruction. </title> <booktitle> In Proceedings of the AI-ED 93 World Conference of Artificial Intelligence in Education, </booktitle> <pages> 82-89. </pages> <address> Edinburgh, Scotland. </address>
Reference-contexts: The preconditions and effects of steps are derived using operator models. Operator models map actions performed in the environment to their preconditions and effects. The agent also outputs a set of operator models, which can be reused in other procedures. 3 Our current simulation environment is authored with Rides <ref> (Munro et al. 1993) </ref> figure 4: Plan represented as a graph 4 Learning Procedures into a partially ordered plan. Initially, the instructor provides the agent with a demonstration. The agent creates a trace of the demonstration, which is the demonstrations sequence of steps.
Reference: <author> Ousterhout, J. K. </author> <year> 1994. </year> <title> Tcl and the Tk Toolkit . Reading, </title> <address> Mass.; </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: As shown in figure 2, the agent also creates an initial model for the close valve operator. After a procedure has demonstrated, the agent needs a representation of the procedure that can be understood by 4 The graph was produced with TCL/TK <ref> (Ousterhout 1994) </ref> and the tkdot portion of the Graph Visualization tools from AT&T Laboratories and Bell Laboratories (Lucent Technology) (Krishnamurthy 1995). the instructor. Figure 4 shows a procedure represented as a graph. Each of the procedures steps is represented by node.
Reference: <author> Pearson, D. J. </author> <year> 1995. </year> <title> Active learning in correcting domain theories: Help or hindrance. </title> <booktitle> In AAAI Symposium on Active Learning. </booktitle>
Reference-contexts: For our purposes, a domain task (or procedure ) is a sequence of steps, where each step represents an action to perform. In fact, almost all systems that learn procedural knowledge only attempt to achieve this goal <ref> (Pearson 1995, Huffman and Laird 1995, Shen 1994, Benson 1995) </ref>. In contrast, our goal is more challenging because teaching requires using knowledge in multiple ways. An agent that teaches also needs to be able to explain a task to students and to monitor human students performing a task. <p> Many systems have this capability. However, another type of explanation is explaining the causal relationships between steps. While many systems contain enough knowledge to produce some form of explanation, their explanations would seem inadequate to humans. Many explanations would be either too complicated (Shen 1994), contain incorrect knowledge <ref> (Pearson 1995) </ref> or the have unacceptable content (Benson 1995). One class of system that has problems explaining causal relationships are systems that only learn how to react in specific situations (Huffman and Laird 1995, Pearson 95). <p> Many explanations would be either too complicated (Shen 1994), contain incorrect knowledge (Pearson 1995) or the have unacceptable content (Benson 1995). One class of system that has problems explaining causal relationships are systems that only learn how to react in specific situations <ref> (Huffman and Laird 1995, Pearson 95) </ref>. In short, the system should not only be able to explain what it is doing but also be able to describe the causal relationships between its actions. Another part of teaching is monitoring students as they perform a task.
Reference: <author> Porter, B. W.; and Kubler, D.F. </author> <year> 1986. </year> <title> Experimental goal regression: A method for learning problemsolving heuristics. </title> <booktitle> Machine Learning , 1 </booktitle> <pages> 249-286. </pages>
Reference-contexts: Experiments can be focused on a demonstration by perturbing a trace of the demonstration, executing it and observing what happens. Some earlier systems perturb a demonstration by changing the state of the environment <ref> (Porter and Kubler 1986) </ref>. However, the simulation that controls the environment is an external program that does not allow the agent to make arbitrary state changes. The agent overcomes this by perturbing the demonstrations sequence of actions rather the environments state. <p> Diligent performs experiments by observing how the actions in a demonstration influence each other. Even though Diligent makes no attempt to model human cognition, this approach was motivated by studies of human learning (Chi et al. 1989). Another system that learns by examining a demonstration in detail is PET <ref> (Porter and Kubler 1986) </ref>. PET requires the ability to make small changes to the state of the environment. In contrast, Diligent does not assume the ability to make arbitrary changes to the environments state.
Reference: <author> Redmond, M. A. </author> <year> 1992. </year> <title> Learning by observing and understanding expert problem solving. </title> <type> Ph.D. </type> <institution> diss., Georgia Institute of Technology. </institution>
Reference-contexts: Any agent that teaches needs to understand causal relationships in order to monitor students performing procedures and to explain the causal dependencies between steps. Many previous systems that learn from tutorial instruction require detailed domain theories. Two such systems are ODYSSEUS (Wilkins 1990) and CELIA <ref> (Redmond 1992) </ref>. This reduces the portability between domains and requires a human who has the expertise to construct the domain theory. Diligent does not need a detailed domain model because it exploits a simulation and a helpful, human instructor.
Reference: <author> Rickel, J.; and Johnson ,W. L. </author> <year> 1997a. </year> <title> Integrating pedagogical capabilities in a virtual environment agent. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents.; </booktitle> <publisher> ACM Press. </publisher>
Reference: <author> Rickel, J.; and Johnson ,W. L. </author> <year> 1997b. </year> <title> Intelligent tutoring in virtual reality: a preliminary report. </title> <booktitle> In Proceedings of the Eighth World Conference on AI in Education.; </booktitle> <publisher> IOS Press. </publisher>
Reference: <author> Shen ,W. </author> <title> 1994 . Autonomous Learning From The Environment . New York, NY: </title> <editor> W. H. </editor> <publisher> Freeman. </publisher>
Reference-contexts: Many systems have this capability. However, another type of explanation is explaining the causal relationships between steps. While many systems contain enough knowledge to produce some form of explanation, their explanations would seem inadequate to humans. Many explanations would be either too complicated <ref> (Shen 1994) </ref>, contain incorrect knowledge (Pearson 1995) or the have unacceptable content (Benson 1995). One class of system that has problems explaining causal relationships are systems that only learn how to react in specific situations (Huffman and Laird 1995, Pearson 95). <p> Some systems learn a representation for preconditions that does not support articulating explanations that a human would find reasonable. TRAIL (Benson 1995) and LIVE <ref> (Shen 1994) </ref> learn preconditions that are in disjunctive normal form. The preconditions are formed so that they cover all examples of when the operator should be applied. There is no effort to learn preconditions of which a human would approve.
Reference: <author> Tecuci, G.; and Hieb, M. R. </author> <year> 1996. </year> <title> Teaching intelligent agents: </title> <booktitle> The disciple approach . International Journal of Human-Computer Interaction . 8(3) </booktitle> <pages> 259-285. </pages>
Reference-contexts: One alternative is to provide a software program (or agent ) to serve as an instructor. Before instructing a student, the agent needs to acquire the knowledge of domain tasks necessary for teaching. One method used by the AI community is tutorial instruction aided by machine learning techniques <ref> (Tecuci and Hieb 1996, Huffman and Laird 1995, Redmond 1992) </ref>. The most basic type of knowledge is being able to demonstrate (or perform) a task. For our purposes, a domain task (or procedure ) is a sequence of steps, where each step represents an action to perform. <p> Tecuci and Hieb <ref> (Tecuci and Hieb 1996) </ref> describe such a system, but it learns a very different type of knowledge than our agent and requires much more interaction with the instructor. How the Agent Learns The agent gets its initial knowledge about a procedure from an instructors demonstration. <p> This reduces the portability between domains and requires a human who has the expertise to construct the domain theory. Diligent does not need a detailed domain model because it exploits a simulation and a helpful, human instructor. The DISCIPLE <ref> (Tecuci and Hieb 1996) </ref> systems require simpler domain models, but DISCIPLE systems ask instructors more questions than Diligent because DISCIPLE systems do not use a simulation to perform autonomous experiments. Some systems learn a representation for preconditions that does not support articulating explanations that a human would find reasonable.
Reference: <author> Wang , X. </author> <year> 1996. </year> <title> Learning Planning Operators by Observation and Practice. </title> <type> Ph.D. </type> <institution> diss., School of Computer Science, Carnegie Mellon Univ. </institution>
Reference-contexts: However, practice problems are hard to solve with little domain knowledge and may not be focused on understanding the dependencies between the actions in a demonstration. This is seen in OBSERVER, which requires many demonstrations and practice problems <ref> (Wang 96) </ref>. Requiring many demonstrations and practice problems also places a much greater burden on the instructor. The burden on the instructor is greater because demonstrations are tedious and time consuming and because selecting practice problems requires care if the agent is to solve them. <p> Some systems require many practice problems or demonstrations. EXPO (Gil 1992) refines an incomplete domain theory using practice problems, but EXPO is unable to correct errors in its knowledge and does not use demonstrations. OBSERVER <ref> (Wang 96) </ref> learns operator models using many demonstrations and practice problems. Diligent and OBSERVER use very similar precondition learning algorithms. OBSERVER, however, does not perform experiments based on demonstrations. Instead, OBSERVER tries to learn general knowledge by solving practice problems.
Reference: <author> Wilkins, D. C. </author> <year> 1990. </year> <title> Knowledge base refinement as improving an incorrect and incomplete domain theory. </title> <booktitle> In Machine Learning An Artificial Intelligence Approach, volume III , 493-513. </booktitle> <address> San Mateo, Calif.: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Any agent that teaches needs to understand causal relationships in order to monitor students performing procedures and to explain the causal dependencies between steps. Many previous systems that learn from tutorial instruction require detailed domain theories. Two such systems are ODYSSEUS <ref> (Wilkins 1990) </ref> and CELIA (Redmond 1992). This reduces the portability between domains and requires a human who has the expertise to construct the domain theory. Diligent does not need a detailed domain model because it exploits a simulation and a helpful, human instructor.
References-found: 21

