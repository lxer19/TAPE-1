URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/benel96.ps
Refering-URL: http://ilk.kub.nl/~ilk/papers/abstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: walter.daelemans@kub.nl  
Title: ABSTRACTION CONSIDERED HARMFUL: LAZY LEARNING OF LANGUAGE PROCESSING  
Author: Walter Daelemans 
Note: and Center for Dutch Language and Speech,  1 Empirical Learning of Natural Language  
Address: Netherlands,  Belgium  
Affiliation: Computational Linguistics Tilburg University, The  University of Antwerp,  
Abstract-found: 0
Intro-found: 0
Reference: <institution> Recent papers can be retrieved as postscript from http://itkwww.kub.nl:2080/tki/Docs/Projects/Walter/pubs-long.html or search Alta Vista with walter daelemans recent. </institution>
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. </author> <year> (1991). </year> <title> `Instance-based learning algorithms'. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 37-66. </pages> <editor> van den Bosch, A. and W. Daelemans. </editor> <year> (1992). </year> <title> Linguistic Pattern Matching Capabilities of Connectionist Networks. </title> <editor> In: Jan van Eijck and Wilfried Meyer Viol (Eds.) </editor> <booktitle> Computational Linguistics in the Netherlands. Papers from the Second CLIN-meeting 1991. Utrecht: OTS, </booktitle> <pages> 40-53. </pages>
Reference-contexts: values is measured using equation (2), an overlap metric, for sym bolic features (we will have no numeric features in the tagging application). ffi (x i ; y i ) = 0 if x i = y i ; else 1 (2) We will refer to this approach as IB1 <ref> (Aha et al., 1991) </ref>.
Reference: <author> Van den Bosch, A. and Daelemans, W. </author> <year> (1993). </year> <title> `Data-oriented methods for grapheme-to-phoneme conversion.' </title> <booktitle> Proceedings of the Sixth conference of the European chapter of the ACL, ACL, </booktitle> <pages> 45-53. </pages>
Reference: <author> Van den Bosch, A., W, Daelemans, T. Weijters. </author> <year> (1996). </year> <title> `Morphological Analysis as Classification: an Inductive-Learning Approach.' </title> <booktitle> Proceedings of NEMLAP 1996, </booktitle> <institution> Ankara, Turkey. </institution>
Reference: <author> Cardie, C. </author> <year> (1994). </year> <title> `Domain-Specific Knowledge Acquisition for Conceptual Sentence Analysis'. </title> <type> Ph.D. Thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Chandler, S. </author> <year> (1992). </year> <title> `Are rules and modules really necessary for explaining language?' Journal of Psycholinguistic research, </title> <booktitle> 22(6): </booktitle> <pages> 593-606. </pages>
Reference: <author> Daelemans, W. </author> <year> (1995). </year> <title> `Memory-based lexical acquisition and processing.' </title> <editor> In Steffens, P., editor, </editor> <booktitle> Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence 898. </booktitle> <address> Berlin: </address> <publisher> Springer, </publisher> <pages> 85-98. </pages>
Reference-contexts: It is our claim that all useful linguistic tasks can be redefined this way and can thus be taken on in a ML context. All linguistic problems can be described as mappings of two kinds: disambiguation (or identification) and segmentation (identification of boundaries) <ref> (see Daelemans, 1995) </ref>. * Disambiguation. Given a set of possible categories and a relevant context in terms of attribute values, determine the correct category for this context.
Reference: <author> Daelemans, W., Van den Bosch, A. </author> <year> (1992a). </year> <title> `Generalisation performance of backpropagation learning on a syllabification task.' </title> <editor> In M. Drossaers & A. Nijholt (Eds.), TWLT3: </editor> <booktitle> Connectionism and Natural Language Processing. </booktitle> <institution> Enschede: Twente University, </institution> <month> 27-38. </month>
Reference: <author> Daelemans, W. and A. van den Bosch. </author> <title> (1992b)`A Neural Network for Hyphenation.' </title> <editor> In: I. Aleksander and J. Taylor (eds.) </editor> <booktitle> Artificial Neural Networks II: Proceedings of the International Conference on Artificial Neural Networks., </booktitle> <publisher> Elsevier Science Publishers, </publisher> <pages> 1647-1650. </pages>
Reference: <author> Daelemans, W. and A. van den Bosch. </author> <year> (1993). </year> <title> `TABTALK: Reusability in Data-oriented grapheme-to-phoneme conversion.' </title> <booktitle> Proceedings of Eurospeech, </booktitle> <address> Berlin, </address> <pages> 1459-1466. </pages>
Reference: <author> Daelemans, W. and A. van den Bosch. </author> <year> (1994). </year> <title> `A language-independent, data-oriented architecture for grapheme-to-phoneme conversion.' </title> <booktitle> In: Proceedings of the ESCA-IEEE conference on Speech Synthesis, </booktitle> <address> New York, </address> <pages> 199-203. </pages>
Reference: <author> Daelemans, W., S. Gillis and G. Durieux. </author> <year> (1994). </year> <title> `The Acquisition of Stress, a data-oriented approach.' </title> <booktitle> Computational Linguistics 20 (3), </booktitle> <pages> 421-451. </pages>
Reference: <author> Daelemans, W., P. Berck, and S. Gillis. </author> <year> (1995). </year> <title> `Linguistics as Data Mining: Dutch Diminutives', </title> <editor> in Andernach, T., M. Moll, and A. Nijholt (eds). </editor> <booktitle> CLIN V, Papers from the Fifth CLIN Meeting, </booktitle> <pages> 59-72. </pages>
Reference-contexts: It is our claim that all useful linguistic tasks can be redefined this way and can thus be taken on in a ML context. All linguistic problems can be described as mappings of two kinds: disambiguation (or identification) and segmentation (identification of boundaries) <ref> (see Daelemans, 1995) </ref>. * Disambiguation. Given a set of possible categories and a relevant context in terms of attribute values, determine the correct category for this context.
Reference: <author> Daelemans, W., J. Zavrel, P. Berck, S. Gillis. </author> <year> (1996). </year> <title> `MBT: A Memory-Based Part of Speech Tagger-Generator'. </title> <editor> In: E. Ejerhed and I. Dagan (eds.) </editor> <booktitle> Proceedings of the Fourth Workshop on Very Large Corpora, Copenhagen, Denmark, </booktitle> <pages> 14-27. </pages>
Reference-contexts: For numeric features kd-trees have been proposed (Friedman et al., 1977) as a solution on single-processor machines. The advantages of this approach do not generalize easily to symbolic features, however. We developed IGTrees <ref> (Daelemans et al., 1996) </ref> to compress memory for symbolic features. IGTree is a heuristic approximation of the IB-IG algorithm. The asymptotic complexity of IGTree (i.e, in the worst case) is extremely favorable.
Reference: <author> Daelemans, W., P. Berck, S. Gillis. </author> <title> (1996).`Unsupervised Discovery of Phonological Categories through Supervised Learning of Morphological Rules.' </title> <booktitle> Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), Copenhagen, Denmark, </booktitle> <pages> 95-100. </pages>
Reference-contexts: For numeric features kd-trees have been proposed (Friedman et al., 1977) as a solution on single-processor machines. The advantages of this approach do not generalize easily to symbolic features, however. We developed IGTrees <ref> (Daelemans et al., 1996) </ref> to compress memory for symbolic features. IGTree is a heuristic approximation of the IB-IG algorithm. The asymptotic complexity of IGTree (i.e, in the worst case) is extremely favorable.
Reference: <author> Daelemans, W., Van den Bosch, A., Weijters, T. </author> <year> (1996). </year> <title> `IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms.' </title> <editor> In Aha, D. (ed.). </editor> <title> AI Review Special Issue on Lazy Learning, </title> <publisher> forthcoming. </publisher>
Reference-contexts: For numeric features kd-trees have been proposed (Friedman et al., 1977) as a solution on single-processor machines. The advantages of this approach do not generalize easily to symbolic features, however. We developed IGTrees <ref> (Daelemans et al., 1996) </ref> to compress memory for symbolic features. IGTree is a heuristic approximation of the IB-IG algorithm. The asymptotic complexity of IGTree (i.e, in the worst case) is extremely favorable.
Reference: <author> Derwing, B. L. and Skousen, R. </author> <year> (1989). </year> <title> `Real Time Morphology: Symbolic Rules or Analogical Networks'. </title> <booktitle> Berkeley Linguistic Society 15: </booktitle> <pages> 48-62. </pages>
Reference: <author> Federici S. and V. Pirelli. </author> <year> (1996). </year> <title> `Analogy, Computation and Linguistic Theory.' </title> <editor> In Jones, D. (ed.) </editor> <booktitle> New Methods in Language Processing. </booktitle> <address> London: </address> <publisher> UCL Press, forthcoming. </publisher>
Reference: <author> Friedman, J., Bentley, J., and Ari Finkel, R. </author> <year> (1977). </year> <title> `An algorithm for finding best matches in logarithmic expected time.' </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 3(3), </volume> <pages> 209-227. </pages>
Reference-contexts: The same asymptotic complexity is of course found for memory storage in this approach. Hardware solutions to the complexity problem have been proposed: massively parallel computing (Stanfill & Waltz, 1986) or even wafer-scale integration (Kitano, 1993). For numeric features kd-trees have been proposed <ref> (Friedman et al., 1977) </ref> as a solution on single-processor machines. The advantages of this approach do not generalize easily to symbolic features, however. We developed IGTrees (Daelemans et al., 1996) to compress memory for symbolic features. IGTree is a heuristic approximation of the IB-IG algorithm.
Reference: <author> Hunt, E., J. Marin, P. Stone. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <editor> Jones, D. </editor> <booktitle> Analogical Natural Language Processing. (1996). </booktitle> <address> London: </address> <publisher> UCL Press. </publisher>
Reference: <author> Kitano, H. </author> <year> (1993). </year> <title> `Challenges of massive parallelism.' </title> <booktitle> In proceedings of IJCAI, </booktitle> <pages> 813-834. </pages>
Reference-contexts: The same asymptotic complexity is of course found for memory storage in this approach. Hardware solutions to the complexity problem have been proposed: massively parallel computing (Stanfill & Waltz, 1986) or even wafer-scale integration <ref> (Kitano, 1993) </ref>. For numeric features kd-trees have been proposed (Friedman et al., 1977) as a solution on single-processor machines. The advantages of this approach do not generalize easily to symbolic features, however. We developed IGTrees (Daelemans et al., 1996) to compress memory for symbolic features.
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-Based Reasoning. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher> <address> 9 Quinlan, J. </address> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Salzberg, S. </author> <title> (1990) `A nearest hyperrectangle learning method'. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 251-276. </pages>
Reference: <author> Scha, R. </author> <booktitle> (1992) `Virtuele Grammatica's en Creatieve Algoritmen.' </booktitle> <volume> Gramma/TTT 1 (1), </volume> <pages> 57-77. </pages>
Reference: <author> Skousen, R. </author> <year> (1989). </year> <title> Analogical Modeling of Language. </title> <publisher> Dordrecht: Kluwer. </publisher>
Reference: <author> Sejnowski, T. J., Rosenberg, C. S. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference: <author> Stanfill, C. and Waltz, D. </author> <year> (1986). </year> <title> `Toward memory-based reasoning.' </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1212-1228. </pages>
Reference-contexts: The same asymptotic complexity is of course found for memory storage in this approach. Hardware solutions to the complexity problem have been proposed: massively parallel computing <ref> (Stanfill & Waltz, 1986) </ref> or even wafer-scale integration (Kitano, 1993). For numeric features kd-trees have been proposed (Friedman et al., 1977) as a solution on single-processor machines. The advantages of this approach do not generalize easily to symbolic features, however.
Reference: <author> Wettschereck, D., Aha, D.W. & Mohri, T. </author> <year> (1996). </year> <title> `A review and comparative evaluation of feature weighting methods for lazy learning algorithms' Technical Report AIC-95-012. </title> <address> Washington, DC: </address> <institution> Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. 10 </note>
Reference-contexts: We will call this algorithm IB-IG. Many other methods to weigh the relative importance of features have been designed, both in statistical pattern recognition and in machine learning <ref> (see Wettschereck et al. 1996 for an overview) </ref>. The main idea of information gain weighting is to interpret the training set as an information source capable of generating a number of messages (the different category labels) with a certain probability.
References-found: 29

