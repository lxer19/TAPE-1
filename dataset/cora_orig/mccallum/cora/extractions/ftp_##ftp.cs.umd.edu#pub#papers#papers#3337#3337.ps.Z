URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3337/3337.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Stable Encoding of Large Finite-State Automata in Recurrent Neural Networks with Sigmoid Discriminants  
Author: Christian W. Omlin a;b C. Lee Giles a;c 
Address: 4 Independence Way, Princeton, NJ 08540  Troy, NY 12180  College Park, MD 20742  
Affiliation: a NEC Research Institute,  b CS Department, Rensselaer Polytechnic Institute,  c UMIACS, U. of Maryland,  University of Maryland  
Pubnum: Technical Report CS-TR-3337 UMIACS-TR-94-101  
Abstract: We propose an algorithm for encoding deterministic finite-state automata (DFAs) in second-order recurrent neural networks with sigmoidal discriminant function and we prove that the languages accepted by the constructed network and the DFA are identical. The desired finite-state network dynamics is achieved by programming a small subset of all weights. A worst case analysis reveals a relationship between the weight strength and the maximum allowed network size which guarantees finite-state behavior of the constructed network. We illustrate the method by encoding random DFAs with 10, 100, and 1,000 states. While the theory predicts that the weight strength scales with the DFA size, we find the weight strength to be almost constant for all the experiments. These results can be explained by noting that the generated DFAs represent average cases. We empirically demonstrate the existence of extreme DFAs for which the weight strength scales with DFA size.
Abstract-found: 1
Intro-found: 1
Reference: [Alon et al., 1991] <author> Alon, N., Dewdney, A., and Ott, T. </author> <year> (1991). </year> <title> Efficient simulation of finite automata by neural nets. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 38(2) </volume> <pages> 495-514. </pages>
Reference-contexts: The internal representation of learned DFA states can deteriorate due to the dynamical nature of recurrent networks making predictions about the generalization performance of trained recurrent networks difficult [Zeng et al., 1993]. Methods for constructing DFAs in recurrent networks with hard-limiting neurons discriminant functions have been proposed <ref> [Alon et al., 1991, Horne and Hush, 1994, Minsky, 1967] </ref>; 1 methods for constructing networks with sigmoidal and radial-basis discriminant functions are discussed in [Frasconi et al., 1993, Gori et al., 1994, Giles and Omlin, 1993].
Reference: [Elman, 1990] <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211. </pages>
Reference: [Frasconi et al., 1991] <author> Frasconi, P., Gori, M., Maggini, M., and Soda, G. </author> <year> (1991). </year> <title> A unified approach for integrating explicit knowledge and learning by example in recurrent networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> page 811. </pages> <publisher> IEEE 91CH3049-4. </publisher>
Reference: [Frasconi et al., 1993] <author> Frasconi, P., Gori, M., and Soda, G. </author> <year> (1993). </year> <title> Injecting nondeterministic finite state automata into recurrent networks. </title> <type> Technical report, </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy, Florence, Italy. </institution>
Reference: [Giles et al., 1992] <author> Giles, C., Miller, C., Chen, D., Chen, H., Sun, G., and Lee, Y. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent neural networks. Neural Computation, </title> <publisher> 4(3):380. </publisher>
Reference: [Giles and Omlin, 1993] <author> Giles, C. and Omlin, C. </author> <year> (1993). </year> <title> Extraction, insertion and refinement of symbolic rules in dynamically driven recurrent neural networks. </title> <journal> Connection Science, </journal> <volume> 5(3 </volume> & 4):307-337. 
Reference: [Gori et al., 1994] <author> Gori, M., Maggini, M., and Soda, G. </author> <year> (1994). </year> <title> Insertion of finite state automata in recurrent radial basis function networks. </title> <type> Technical report, </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy. </institution>
Reference: [Horne and Hush, 1994] <author> Horne, B. and Hush, D. </author> <year> (1994). </year> <title> Bounds on the complexity of recurrent neural network implementations of finite state machines. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 359-366. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The internal representation of learned DFA states can deteriorate due to the dynamical nature of recurrent networks making predictions about the generalization performance of trained recurrent networks difficult [Zeng et al., 1993]. Methods for constructing DFAs in recurrent networks with hard-limiting neurons discriminant functions have been proposed <ref> [Alon et al., 1991, Horne and Hush, 1994, Minsky, 1967] </ref>; 1 methods for constructing networks with sigmoidal and radial-basis discriminant functions are discussed in [Frasconi et al., 1993, Gori et al., 1994, Giles and Omlin, 1993].
Reference: [Minsky, 1967] <author> Minsky, M. </author> <year> (1967). </year> <title> Computation: Finite and Infinite Machines, </title> <booktitle> chapter 3, </booktitle> <pages> pages 32-66. </pages> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ. </address> <month> 18 </month>
Reference-contexts: The internal representation of learned DFA states can deteriorate due to the dynamical nature of recurrent networks making predictions about the generalization performance of trained recurrent networks difficult [Zeng et al., 1993]. Methods for constructing DFAs in recurrent networks with hard-limiting neurons discriminant functions have been proposed <ref> [Alon et al., 1991, Horne and Hush, 1994, Minsky, 1967] </ref>; 1 methods for constructing networks with sigmoidal and radial-basis discriminant functions are discussed in [Frasconi et al., 1993, Gori et al., 1994, Giles and Omlin, 1993].
Reference: [Omlin, 1995] <author> Omlin, C. </author> <year> (1995). </year> <title> Symbolic Knowledge in Recurrent Neural Networks: Issues of Training and Representation. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180. </address>
Reference-contexts: We only give the proofs of the theorems which establish our results; for proofs of auxiliary lemmas see <ref> [Omlin, 1995] </ref>. 3.1 Fixed Point Analysis Recall that the recurrent network changes its state according to equation (1). <p> We state here without proof the following facts about fixed points of the function h (:); the proofs can be found in <ref> [Omlin, 1995] </ref>. Lemma 3.1.1 For 0 &lt; H &lt; 4, h (x; H) has the following fixed point: 0 = 0:5 Furthermore, h (x; H) converge to 0 for any choice of a start value x 0 . <p> n h t1 (15) Similarly, we can quantify high signals: Lemma 3.2.2 The high signals are bounded from below by the fixed point + of the function h t 8 : h (h t1 (x; H); H) t &gt; 1 The derivation of these iterated functions can be found in <ref> [Omlin, 1995] </ref>; the functions (16) and (17) converge toward their fixed points and + according to lemma 3.1.3. In practice, only few neurons ever exceed or fall below the fixed points and + , respectively.
Reference: [Omlin and Giles, 1994] <author> Omlin, C. and Giles, C. </author> <year> (1994). </year> <title> Stable encoding of large finite-state automata in recurrent neural networks with sigmoid discriminants. Neural Computation. </title> <note> Submitted. </note>
Reference-contexts: Since constructed recurrrent networks are able to regenerate their internal signals and since typical DFAs do not have the worst case 8 properties assumed in this analysis, the conditions guaranteeing stable low and high signals are generally much too strong for some given DFA. Scaling issues are discussed elsewhere <ref> [Omlin and Giles, 1994] </ref>.
Reference: [Pollack, 1991] <author> Pollack, J. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252. </pages>
Reference: [Servan-Schreiber et al., 1991] <author> Servan-Schreiber, D., Cleeremans, A., and McClelland, J. </author> <year> (1991). </year> <title> Graded state machine: The representation of temporal contingencies in simple recurrent networks. Machine Learning, </title> <publisher> 7:161. </publisher>
Reference: [Tino, 1994] <author> Tino, P. </author> <year> (1994). </year> <type> Personal communication. </type>
Reference-contexts: This value corresponds to a DFA whose states have `average' indegree n = 1:5. [The magic value 6 also seems to occur for networks which are trained. Consider a neuron S i ; then, the weight which causes transitions between dynamical attractors often has a value 6 <ref> [Tino, 1994] </ref>.] However, there exist DFAs which exhibit the scaling behavior that is predicted by the theory. We will briefly discuss such DFAs.
Reference: [Watrous and Kuhn, 1992] <author> Watrous, R. and Kuhn, G. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. Neural Computation, </title> <publisher> 4(3):406. </publisher>
Reference: [Zeng et al., 1993] <author> Zeng, Z., Goodman, R., and Smyth, P. </author> <year> (1993). </year> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 976-990. 19 </pages>
Reference-contexts: The internal representation of learned DFA states can deteriorate due to the dynamical nature of recurrent networks making predictions about the generalization performance of trained recurrent networks difficult <ref> [Zeng et al., 1993] </ref>.
References-found: 16

