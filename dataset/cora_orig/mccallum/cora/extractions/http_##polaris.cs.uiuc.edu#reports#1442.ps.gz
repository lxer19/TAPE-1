URL: http://polaris.cs.uiuc.edu/reports/1442.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: Effective Automatic Parallelization with Polaris  
Author: William Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, William Pottenger, Lawrence Rauchwerger, Peng Tu and Stephen Weatherford 
Affiliation: Center for Supercomputing Research and Development Coordinated Science Laboratory University of Illinois  
Abstract: The Polaris project has delivered a new parallelizing compiler that overcomes severe limitations of current compilers. While available parallelizing compilers may succeed on small kernels, they often fail to extract any meaningful parallelism from large applications. In contrast, Polaris has proven to speed up real programs significantly beyond the degree achieved by the parallelization tools available on the SGI Challenge machine. The techniques implemented are interprocedural symbolic program analysis, scalar and array privatization, symbolic dependence analysis, and advanced induction and reduction recognition and elimination. We will present preliminary results of this compiler as of Fall 94 and we will describe new techniques for runtime detection of parallelism, which we will implement in the future. 
Abstract-found: 1
Intro-found: 1
Reference: [Ban88] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: The range test can be considered an extension of a symbolic version of Triangular Banerjee's Inequalities test <ref> [WB87, Ban88, HP91] </ref>. In the range test, we mark a loop as parallel if we can prove that the range of elements accessed by an iteration of that loop do not overlap with the range of elements accessed by other iterations.
Reference: [BCK + 89] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evalution of Supercomputers. </title> <booktitle> Int'l. Journal of Supercomputer Applications, Fall 1989, </booktitle> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: New measurements on a representative set of real programs were made possible, thanks to the Perfect Benchmarks R fl effort, which were developed in a related project <ref> [BCK + 89] </ref>. Based on these observations, we have hand parallelized the program suite as a major new approach to identifying effective program transformations [EHLP91, EHP94].
Reference: [BE92] <author> William Blume and Rudolf Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks Programs. </title> <journal> IEEE Transactions of Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Parallelizing compilers have been playing an important role in this quest. The present project has its early roots in a compiler evaluation effort of the late 80s, where we have found that despite the success on kernel benchmarks, available compilers were not very effective on large programs <ref> [EHLP91, BE92] </ref>. New measurements on a representative set of real programs were made possible, thanks to the Perfect Benchmarks R fl effort, which were developed in a related project [BCK + 89].
Reference: [BE94a] <author> William Blume and Rudolf Eigenmann. </author> <title> An Overview of Symbolic Analysis Techniques Needed for the Effective Parallelization of the Perfect Benchmarks. </title> <booktitle> Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <address> pages II233 II238, </address> <month> August, </month> <year> 1994. </year>
Reference-contexts: In our experience with the Perfect Benchmarks, such nonlinear expressions do occur in practice. In fact, four of the twelve codes (i.e., DYFESM, QCD, OCEAN, and TRFD) that we hand-parallelized would exhibit a speedup of at most two if we could not parallelize loops with nonlinear array subscripts <ref> [BE94a] </ref>. For some of these loops, nonlinear expressions occurred in the original program text. For other loops, nonlinear expressions were introduced by the compiler. The two most common compiler passes that can introduce nonlinearities into array subscript expressions are induction variable substitution and array linearization. <p> However, like most other known tests, it makes the assumption that array indices contain only linear subscript expressions, which we have found to be a serious limitation <ref> [BE94a] </ref>. The range test subsumes Banerjee's Inequalities and handles many of the symbolic expressions we have seen in the Perfect Benchmarks. Because of this, one can expect this new data dependence test to be very effective in practice. Our implementation of the range test in Polaris supports these claims.
Reference: [BE94b] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Non-linear Expressions. </title> <booktitle> Proceedings of Supercomputing '94, November 1994, </booktitle> <address> Washington D.C., </address> <pages> pages 528-537, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: This loop nest, taken from TRFD, accounts for about 70% of the code's sequential execution time. 3.3.1 Range Test To handle such nonlinear expressions, we have developed a symbolic dependence test called the range test <ref> [BE94b] </ref>. The range test can be considered an extension of a symbolic version of Triangular Banerjee's Inequalities test [WB87, Ban88, HP91]. <p> See <ref> [BE94b] </ref> for other tests that use these minimum and maximum values of f and g. Returning to the example for Figure 1, we will now apply the dependence test described above to prove that A (f) does not carry any dependences for the outermost loop.
Reference: [EHLP91] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year> <note> 4 CSRD reports are available via anonymous FTP from ftp.csrd.uiuc.edu:CSRD Info, or the World Wide Web site http://www.csrd.uiuc.edu </note>
Reference-contexts: Parallelizing compilers have been playing an important role in this quest. The present project has its early roots in a compiler evaluation effort of the late 80s, where we have found that despite the success on kernel benchmarks, available compilers were not very effective on large programs <ref> [EHLP91, BE92] </ref>. New measurements on a representative set of real programs were made possible, thanks to the Perfect Benchmarks R fl effort, which were developed in a related project [BCK + 89]. <p> Based on these observations, we have hand parallelized the program suite as a major new approach to identifying effective program transformations <ref> [EHLP91, EHP94] </ref>. As a result, we have found that not only can real applications be parallelized effectively, but the transformations can also be automated in a parallelizing compiler.
Reference: [EHP94] <author> Rudolf Eigenmann, Jay Hoeflinger, and David Padua. </author> <title> On the Automatic Parallelization of the Perfect Benchmarks . Technical Report 1392, </title> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Based on these observations, we have hand parallelized the program suite as a major new approach to identifying effective program transformations <ref> [EHLP91, EHP94] </ref>. As a result, we have found that not only can real applications be parallelized effectively, but the transformations can also be automated in a parallelizing compiler.
Reference: [FHP + 94] <author> Keith A. Faigin, Jay P. Hoeflinger, David A. Padua, Paul M. Petersen, and Stephen A. Weatherford. </author> <title> The Polaris Internal Representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5) </volume> <pages> 553-286, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Section 3.5 describes a promising new techniques that is currently being implemented. Finally, in Section 4 we present results of the Polaris compiler. 2 Internal Organization of the Compiler The aim in the design of Polaris' internal organization <ref> [FHP + 94] </ref> was to create an internal representation (IR) that enforced correctness, was robust and, through high-level functionality, easy to use. Our view of the IR is that it is more than just the structure of the data within the compiler.
Reference: [HP91] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Dependence Analysis for High-Performance Parallelizing Compilers. </title> <booktitle> Parallel and Distributed Computing: Advances in Languages and Compilers for Parallel Processing, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pages 310-330, </pages> <year> 1991. </year>
Reference-contexts: The range test can be considered an extension of a symbolic version of Triangular Banerjee's Inequalities test <ref> [WB87, Ban88, HP91] </ref>. In the range test, we mark a loop as parallel if we can prove that the range of elements accessed by an iteration of that loop do not overlap with the range of elements accessed by other iterations.
Reference: [PE95] <author> Bill Pottenger and Rudolf Eigenmann. </author> <title> Parallelization in the Presence of Generalized Induction and Reduction Variables. </title> <type> Technical Report 1396, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: In data dependence terms, this forms a cycle in the dependence graph, which serializes the loop. The following section provides an overview of Polaris' techniques to handle these idioms. More details can be found in <ref> [PE95] </ref>. 3.2.0.1 Reduction variables most often accumulate values computed in each loop iteration, typically of the form sum = sum + &lt;expression&gt;. Because the "+" operation is commutative and distributive, partial sums can be accumulated on parallel processors and summed at the end of the loop.
Reference: [PKK91] <author> K. Psarris, D. Klappholz, and X. Kong. </author> <title> On the accuracy of the Banerjee test. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 152-157, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Simplified version of loop nest FTRVMT/109 from OCEAN Banerjee's Inequalities, have been shown to be one of the most effective data dependence tests <ref> [PKK91] </ref> for real programs [PP93]. However, like most other known tests, it makes the assumption that array indices contain only linear subscript expressions, which we have found to be a serious limitation [BE94a].
Reference: [PP93] <author> Paul M. Petersen and David A. Padua. </author> <title> Static and Dynamic Evaluation of Data Dependence Analysis. </title> <booktitle> Presented at ICS'93, </booktitle> <address> Tokyo, Japan, </address> <pages> pages 107-116, </pages> <month> July 19-23, </month> <year> 1993. </year>
Reference-contexts: There has been much research in the area of data dependence analysis. Because of this, modern day data dependence tests have become very accurate and efficient <ref> [PP93] </ref>. <p> Simplified version of loop nest FTRVMT/109 from OCEAN Banerjee's Inequalities, have been shown to be one of the most effective data dependence tests [PKK91] for real programs <ref> [PP93] </ref>. However, like most other known tests, it makes the assumption that array indices contain only linear subscript expressions, which we have found to be a serious limitation [BE94a]. The range test subsumes Banerjee's Inequalities and handles many of the symbolic expressions we have seen in the Perfect Benchmarks.
Reference: [RP94] <author> Lawrence Rauchwerger and David Padua. </author> <title> The PRIVATIZING DOALL Test: A Run-Time Technique for DOALL Loop Identification and Array Privatization . Proceedings of the 8th ACM International Conference on Supercomputing, </title> <address> Manchester, England, </address> <pages> pages 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: In order to implement such a strategy, we have developed a run-time technique, called the Privatizing Doall test (PD test), for detecting the presence of cross-iteration dependences in a loop <ref> [RP94] </ref>.
Reference: [SMC91] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: For these cases we are developing run-time methods for the recognition and implementation of parallelism. These techniques are not currently implemented in Polaris but will include inspector/executor <ref> [SMC91] </ref> style implementations as well as implementations based on speculative execution. 3.1 Inline Expansion The Polaris inliner is designed to provide three types of services: complete inline expansion of subprograms for analysis, selective inline expansion of subprograms for code generation, and selective modification of subprograms.
Reference: [TP93] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <editor> In Utpal BanerjeeDavid Gelern-terAlex NicolauDavid Padua, editor, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science., </booktitle> <volume> volume 768, </volume> <pages> pages 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: In our experience, the most important of these transformations is array privatization <ref> [TP93] </ref>. Array privatization identifies scalars and arrays that are used as temporary work spaces by a loop iteration, and allocates a local copy of those scalars and arrays for that iteration. Such variables can then be safely ignored by the data dependence test.
Reference: [TP94] <author> Peng Tu and David Padua. </author> <title> Demand-Driven Symbolic Analysis. </title> <type> Technical Report 1336, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> Febraury </month> <year> 1994. </year>
Reference-contexts: Thus, to prove that A is privatizable, we only need to prove that MP M fl P. To prove this, we need to find out how the symbolic variables are related from their global def-use relations. In Polaris, we use a demand-driven algorithm <ref> [TP94] </ref>, based on a Static Single Assignment (SSA) S 1 : M = : : : S 2 : MP = M * P do I = 1, N A (J) = : : : : : : do L = 1, P end do end do Fig. 3.

References-found: 16

