URL: http://www.media.mit.edu/~nitin/papers/Env_Snds/EnvSnds_TR97.ps
Refering-URL: http://www.media.mit.edu/~nitin/projects/NomadicRadio/
Root-URL: http://www.media.mit.edu
Title: Situational Awareness from Environmental Sounds  
Date: 13, 1997  
Note: June  
Abstract: Nitin Sawhney (nitin@media.mit.edu) Speech Interface Group, MIT Media Lab Final Project Report for Modeling Adaptive Behavior (MAS 738) Pattie Maes Abstract Environmental sounds provide many contextual cues that enable us to recognize important aspects of our surroundings. The goal of this project is to consider techniques to allow machines to extract and classify features from predefined classes of sounds in the environment. We will present the different phases of the project: capture of environmental audio, preprocessing of audio data, feature extraction using power spectral density and filter-banks, training and testing via a simple nearest-neighbor algorithm. We will discuss some preliminary results and future work on incorporating such techniques on a wearable computer to provide a form of "situational awareness" to the system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bregman, Albert S. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference: [2] <author> Ellis, Daniel P. </author> <title> Prediction-driven Computational Scene Analysis, </title> <type> Ph.D. </type> <institution> Thesis in Media Arts and Sciences, MIT. </institution> <month> June </month> <year> 1996. </year>
Reference: [3] <author> Feiten, B. and S. Gunzel, </author> <title> Automatic Indexing of a Sound Database using Self-Organizing Neural Nets. </title> <journal> Computer Music Journal, </journal> <volume> 18:3, </volume> <pages> pp. 53-65, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: The MAP Gaussian classifier does a better job of discriminating music from speech and vise versa whereas the k-d spatial classifier has nearly the same performance on all classes. In the past, both Cluster [4][7] and Neural Net-based <ref> [3] </ref> approaches have been utilized for some level of sound classification. A cluster analysis on a set of variables can be done by selecting a set of representative objects in the data set. <p> If the network is trained to detect certain features as important, a larger number of neurons are utilized to represent that feature, and their syntactic weights are stored in corresponding locations in a layer called a feature map. In a related work <ref> [3] </ref>, Kohonen feature maps were used to segment similarity between sounds relative to their distances in the map. The advantage of a Neural Net approach would be that nonlinear mappings in the audio data would be more easily represented.
Reference: [4] <author> Nicolas Saint-Arnaud, </author> <title> Classification of Sound Textures. M.S. Thesis in Media Arts and Sciences, </title> <publisher> MIT. </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: The corresponding clusters can be found by assigning the remaining variables to 2 the nearest representative object (the medoid of the cluster). Yet such techniques (PAM - partitioning around mediods) generally create spherical or elliptical clusters, and is not suited to discover drawn out clusters. Recent approaches <ref> [4] </ref> use a high dimensionally probability density function (PDF) by describing a set of clusters that approximate a PDF. Such a model encodes the most likely transition of ordered features. The centroid of each cluster, its variances and relative weight form an estimate of the statistics of the training vectors. <p> The upper limit of 8000 Hz is chosen as half the sampling rate of audio samples used (16 KHz) as dictated by the Nyquest frequency. The lower limit of 80 Hz is chosen because little energy is observed below it in most sound textures <ref> [4] </ref>. The forward and feedback parameters produced from the filterbank are used to compute an array of filter outputs for a specified waveform (using the FilterBank function in Matlab). Each channel of the filterbank only allows components within a narrow frequency band to go through.
Reference: [5] <author> Schrider, Eric and Malcolm Slaney, </author> <title> Construction and Evaluation of a Robust Multi-feature Speech/Music Discriminator, </title> <booktitle> Proc. ICASSP-97, </booktitle> <pages> Apr 21-24, </pages> <address> Munich, Germany, </address> <year> 1997. </year>
Reference-contexts: We will now describe prior work in the area, the methodology used and step through the techniques for each phase of the project. Related Work Several approaches have been used in the past to distinguish sounds, using multiple features and different classification techniques. Scheirer <ref> [5] </ref> used a multidimensional classification framework by examining 13 features to measure distinct properties of speech and music signals. Some of the successful features include 4Hz modulation energy, low-energy frame percentage, variance of spectral flux, and a pulse metric. <p> for the Nearest Neighbor Framework A spatial partitioning scheme such as k-d spatial classification can be used to conduct a class vote among the k nearest neighbors to a point, and only consider those training points in the particular region of space grouped together by a k-d tree partitioning algorithm <ref> [5] </ref>. This should make more efficient the process of determining the closest point in the training set and may be faster than true nearest neighbor schemes. Hierarchical Classification Several ambiguities exist in the sounds implicitly grouped under specific classes.
Reference: [6] <author> Slaney, Malcolm, </author> <title> Auditory Toolbox: A Matlab Toolbox for Auditory Modeling Work, </title> <institution> Apple Advanced Technology Report #45, Apple Computer Inc., Advanced Technology Group. </institution>
Reference-contexts: Roy Patterson has proposed a model of psychoacoustic filtering based on critical bands. This auditory front-end combines a Gammatone filter bank implemented within the Auditory Toolbox <ref> [6] </ref> in Matlab. The first step is to compute the filter coefficients for a bank of Gammatone filters, defined by Patterson and Holdsworth for simulating the cochlea (using the MakeERB function in Matlab). Each filter in the filter bank is a fifth order IIR filter with both poles and zeros.
Reference: [7] <author> Wold, E., T. Blum, D. Keislar, and J. Wheaton. </author> <title> Content-based Classification Search and Retrieval of Audio. </title> <journal> IEEE Multimedia Magazine, </journal> <month> Fall </month> <year> 1996. </year>
Reference: [8] <author> Hermansky, Hynek, N. Morgan, A. Bayya, P. Kohn, </author> <title> RASTA-PLP Speech Analysis, </title> <type> Technical Report (TR-91-069), </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA., </address> <month> Dec. </month> <year> 1991. </year> <month> 7 </month>
Reference-contexts: Due to project time constraints, we did not attempt to do much preprocessing of the data. Three main types of features were considered for analysis: RASTA The RelAtive SpecTrAl (RASTA) methodology makes use of Perceptual Linear Predictive (PLP) speech analysis and makes it more robust for linear spectral distortions <ref> [8] </ref> i.e. steady state spectral factors in speech that are less influenced by the frequency response in the communication channel. The short-term absolute spectrum is replaced by a spectral estimate in which each frequency channel is bandpass filtered with sharp spectral zero at the zero frequency.
References-found: 8

