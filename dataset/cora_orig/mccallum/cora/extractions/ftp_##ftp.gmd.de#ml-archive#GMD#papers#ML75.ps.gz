URL: ftp://ftp.gmd.de/ml-archive/GMD/papers/ML75.ps.gz
Refering-URL: http://www.cs.bham.ac.uk/~anp/bibtex/kdd.bib.html
Root-URL: 
Email: E-Mail stefan.wrobel@gmd.de  
Title: Extensibility in data mining systems  On Knowledge Discovery and Data Mining,  
Author: Stefan Wrobel and Dietrich Wettschereck and Edgar Sommer and Werner Emde Simoudis, E. and Han, J., 
Keyword: data mining, system architecture, extensibility, KEPLER  
Note: To appear in Proc. 2nd Int. Conf.  eds., AAAI Press,  
Address: Schlo Birlinghoven, 53754 Sankt Augustin, Germany  Menlo Park, CA, USA.  
Affiliation: GMD, FIT.KI (Institute of Applied Information Technology, Artificial Intelligence Research Division)  
Abstract: The successful application of data mining techniques ideally requires both system support for the entire knowledge discovery process and the right analysis algorithms for the particular task at hand. While there are a number of successful data mining systems that support the entire mining process, they usually are limited to a fixed selection of analysis algorithms. In this paper, we argue in favor of extensibility as a key feature of data mining systems, and discuss the requirements that this entails for system architecture. We identify in which points existing data mining systems fail to meet these requirements, and then describe a new integration architecture for data mining systems that addresses these problems based on the concept of "plug-ins". KEPLER, our data mining system built according to this architecture, is presented and discussed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Apte, C., and Hong, S. </author> <year> 1996. </year> <title> Predicting equity returns from securities data. </title> <booktitle> chapter 22, </booktitle> <volume> 542 - 560. </volume> <editor> In (Fayyad et al. </editor> <year> 1996). </year>
Reference-contexts: Our choice was also motivated by the fact that many other published data-mining applications fall 2 KEPLER will be demonstrated at the conference. into this class (e.g. <ref> (Apte & Hong 1996) </ref> 10 4 , (Dzeroski & Grbovic 1995) 10 3 , (Feelders, le Loux, & van't Zand 1995) 10 5 objects, (Li & Biswas 1995) 10 5 , (Sanjeev & Zytkow 1995) 10 3 , (Simoudis, Livezey, & Kerber 1995) 10 6 ), and by the fact that
Reference: <author> Cheeseman, P., and Stutz, J. </author> <year> 1996. </year> <title> Bayesian classification (AutoClass): Theory and results. </title> <booktitle> chapter 6, </booktitle> <volume> 153 - 180. </volume> <editor> In (Fayyad et al. </editor> <year> 1996). </year>
Reference: <author> Dzeroski, S., and Grbovic, J. </author> <year> 1995. </year> <title> Knowledge discovery in a water quality database. </title> <type> 81 - 86. </type> <note> In (Fayyad & Uthurusamy 1995). </note>
Reference-contexts: Our choice was also motivated by the fact that many other published data-mining applications fall 2 KEPLER will be demonstrated at the conference. into this class (e.g. (Apte & Hong 1996) 10 4 , <ref> (Dzeroski & Grbovic 1995) </ref> 10 3 , (Feelders, le Loux, & van't Zand 1995) 10 5 objects, (Li & Biswas 1995) 10 5 , (Sanjeev & Zytkow 1995) 10 3 , (Simoudis, Livezey, & Kerber 1995) 10 6 ), and by the fact that this application size, there exists a number
Reference: <author> Dzeroski, S.; Muggleton, S.; and Russell, S. </author> <year> 1992. </year> <title> PAC-learnability of determinate logic programs. </title> <booktitle> In Proc. 5th ACM Workshop on Comput. Learning Theory, </booktitle> <pages> 128-135. </pages>
Reference-contexts: It is therefore no problem to handle applications that require e.g. an unusual aggregation of tuples, a particular re-representation of time series, or some other preprocessing operation for a particular analysis method. For example, a specific transformation (from DINUS <ref> (Dzeroski, Muggleton, & Russell 1992) </ref>) is available for transforming first-order representations (across several relations) into a manageable propositional representation in one relation. These plug-ins complement the standard ASCII input formats and predefined transformation operators like sampling and discretization.
Reference: <author> Dzeroski, S.; Schulze-Kremer, S.; Heidtke, K.; Siems, K.; and Wettschereck, D. </author> <year> 1996. </year> <title> Knowledge discovery for diterpene structure elucidation from 13C NMR spectra. </title> <booktitle> Proc. ECAI-96 workshop on Intelligent Data Analysis for Medicine and Pharmacology. </booktitle>
Reference-contexts: We have constructed and fully implemented a data mining system termed KEPLER 2 to prove the feasibility of the proposed architecture. Given the data mining applications we are working on (market study data, 10 4 objects, ecological system analysis, 10 5 objects, protein structure prediction <ref> (Dzeroski et al. 1996) </ref>, 10 4 objects), we decided to target KEPLER to medium-range data mining problems (10 4 to 10 6 objects). <p> These plug-ins complement the standard ASCII input formats and predefined transformation operators like sampling and discretization. Evaluation KEPLER has been evaluated on three data mining applications: analysis of retail data, ecological system analysis, and protein structure prediction <ref> (Dzeroski et al. 1996) </ref>. In the retail data application (roughly 15.000 tuples), the primary goal was discovery of interesting customer groups (carried out with EXPLORA). In addition, certain customer groups were characterized using C4.5. <p> In the protein structure application (ca. 10.000 tuples), the goal was to predict protein structure from spectography data, mostly performed using ILP techniques <ref> (Dzeroski et al. 1996) </ref>.
Reference: <author> Emde, W.; Kietz, J.-U.; Sommer, E.; and Wrobel, S. </author> <year> 1993. </year> <title> Cooperation between internal and external learning modules in mobal: different facets of multistrategy learning. </title> <booktitle> MLnet workshop on multistrategy learning, </booktitle> <address> Blanes, Spain. </address>
Reference-contexts: Architecture Within multi-strategy tool architectures, a popular distinction is to separate approaches that integrate at the micro level and those that integrate at the macro level <ref> (Emde et al. 1993) </ref>. In macro level integration, each method to be integrated remains a separate module with its own internal representations and storage structures, but is coupled to other modules by receiving inputs and passing results back across a suitable channel.
Reference: <author> Ezawa, K., and Norton, S. W. </author> <year> 1995. </year> <title> Knowledge discovery in telecommunications services data using bayesian network models. </title> <type> 100 - 105. </type> <note> In (Fayyad & Uthurusamy 1995). </note>
Reference: <editor> Fayyad, U., and Uthurusamy, R., eds. </editor> <booktitle> 1995. Proc. First Int. Conf. on Knowledge Discovery and Data Mining. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <editor> Fayyad, U.; Piatetsky-Shapiro, G.; Smyth, P.; and Uthu-rusamy, R., eds. </editor> <booktitle> 1996. Advances in Knowledge Discovery and Data Mining. </booktitle> <address> Cambridge, USA: </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Data mining researchers have responded to this challenge by creating data mining systems that combine support for all steps of the data mining process <ref> (Fayyad, Piatetsky-Shapiro, & Smyth 1996, p. 10) </ref> with a fixed selection of analysis algorithms in one integrated environment. <p> ISL's Clemen-tine (Integral Solutions Ltd. 1996) and Lockheed's Recon (Simoudis, Livezey, & Kerber 1996) are two commercially 1 In fact, some authors reserve the term KDD to denote the entire process, whereas data mining is used to refer to a single analysis step <ref> (Fayyad, Piatetsky-Shapiro, & Smyth 1996) </ref>. available examples of such systems, the former offering decision trees and neural networks, the latter also including clustering and instance-based algorithms.
Reference: <author> Fayyad, U.; Piatetsky-Shapiro, G.; and Smyth, P. </author> <year> 1996. </year> <title> From data mining to knowledge discovery: An overview. </title> <booktitle> chapter 1, </booktitle> <volume> 1 - 34. </volume> <editor> In (Fayyad et al. </editor> <year> 1996). </year>
Reference-contexts: Data mining researchers have responded to this challenge by creating data mining systems that combine support for all steps of the data mining process <ref> (Fayyad, Piatetsky-Shapiro, & Smyth 1996, p. 10) </ref> with a fixed selection of analysis algorithms in one integrated environment. <p> ISL's Clemen-tine (Integral Solutions Ltd. 1996) and Lockheed's Recon (Simoudis, Livezey, & Kerber 1996) are two commercially 1 In fact, some authors reserve the term KDD to denote the entire process, whereas data mining is used to refer to a single analysis step <ref> (Fayyad, Piatetsky-Shapiro, & Smyth 1996) </ref>. available examples of such systems, the former offering decision trees and neural networks, the latter also including clustering and instance-based algorithms.
Reference: <author> Feelders, A.; le Loux, A.; and van't Zand, J. </author> <year> 1995. </year> <title> Data mining for loan evaluation at ABN AMRO: a case study. </title> <note> 106 - 111,. In (Fayyad & Uthurusamy 1995). </note>
Reference: <author> Frawley, W.; Piatetsky-Shapiro, G.; and Matheus, C. </author> <year> 1991. </year> <title> Knowledge discovery in databases: An overview. </title> <editor> In Piatetsky-Shapiro, G., and Frawley, W., eds., </editor> <booktitle> Knowledge Discovery in Databases. </booktitle> <address> Cambridge, USA: </address> <publisher> AAAI/MIT Press. </publisher> <address> chapter 1, 1 - 27. </address>
Reference: <author> Friedman, J. </author> <year> 1991. </year> <title> Multivariate adaptive regression splines (with discussion). </title> <journal> Annals of Statistics 19(1) </journal> <pages> 1-141. </pages>
Reference-contexts: (Zell 1994), as well as our own instance-based method KNN (Wettschereck 1994) and Salzberg's NGE (Salzberg 1991; Wettschereck & Dietterich 1995)). * for clustering tasks, the AUTOCLASS algorithm (Cheese man & Stutz 1996) as available from the authors * for regression tasks, the MARS (multiple adaptive re-gresssion spline) algorithm of <ref> (Friedman 1991) </ref>, in our own implementation * the pattern discovery algorithm EXPLORA (Klosgen 1996) developed at GMD (ported from Macintosh to Sun) All of these are operational in KEPLER at present.
Reference: <author> Han, J., and Fu, Y. </author> <year> 1996. </year> <title> Exploration of the power of attribute-oriented induction in data mining. </title> <booktitle> chapter 16, </booktitle> <volume> 399 - 421. </volume> <editor> In (Fayyad et al. </editor> <year> 1996). </year>
Reference: <author> Han, J.; Cai, Y.; Cercone, N.; and Huang, Y. </author> <year> 1992. </year> <title> DBLEARN: A knowledge discovery system for databases. </title> <booktitle> In Proc. 1st Int. Conf. Inf. & Knowl. Managmt., </booktitle> <volume> 473 - 481. </volume> <publisher> Integral Solutions Ltd. </publisher> <year> 1996. </year> <title> Clementine data mining system: Decisions from data. </title> <note> WWW http://www.isl.co.uk. </note>
Reference: <author> Klosgen, W. </author> <year> 1996. </year> <note> Explora: A multipattern and multi-strategy discovery assistant. chapter 10, 249 - 271. </note> <editor> In (Fayyad et al. </editor> <year> 1996). </year>
Reference-contexts: and Salzberg's NGE (Salzberg 1991; Wettschereck & Dietterich 1995)). * for clustering tasks, the AUTOCLASS algorithm (Cheese man & Stutz 1996) as available from the authors * for regression tasks, the MARS (multiple adaptive re-gresssion spline) algorithm of (Friedman 1991), in our own implementation * the pattern discovery algorithm EXPLORA <ref> (Klosgen 1996) </ref> developed at GMD (ported from Macintosh to Sun) All of these are operational in KEPLER at present.
Reference: <author> Kohavi, R.; John, G.; Long, R.; Manley, D.; and Pfleger, K. </author> <year> 1994. </year> <title> MLC++: A machine learning library in C++. </title> <booktitle> In Proc. Tools with Artificial Intelligence, </booktitle> <volume> 740 - 743. </volume> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Among other multi-strategy systems, there is DBMINER (previously DBLEARN) (Han et al. 1992; Han & Fu 1996) which discovers multiple kinds of knowledge, but is based on a single attribute-oriented discovery methods and is not extensible. MLC++ <ref> (Kohavi et al. 1994) </ref> is a collection of C++ programs designed to be configured by a user into a working Machine learning algorithm. Since the source code is available, MLC++ is an extensible system.
Reference: <author> Li, C., and Biswas, G. </author> <year> 1995. </year> <note> Knowledge-based scientific discovery in geological databases. 204 - 210. In (Fayyad & Uthurusamy 1995). </note>
Reference-contexts: choice was also motivated by the fact that many other published data-mining applications fall 2 KEPLER will be demonstrated at the conference. into this class (e.g. (Apte & Hong 1996) 10 4 , (Dzeroski & Grbovic 1995) 10 3 , (Feelders, le Loux, & van't Zand 1995) 10 5 objects, <ref> (Li & Biswas 1995) </ref> 10 5 , (Sanjeev & Zytkow 1995) 10 3 , (Simoudis, Livezey, & Kerber 1995) 10 6 ), and by the fact that this application size, there exists a number of available ML and KDD algorithms (from our own group and others) that could be used to <p> 2 KEPLER will be demonstrated at the conference. into this class (e.g. (Apte & Hong 1996) 10 4 , (Dzeroski & Grbovic 1995) 10 3 , (Feelders, le Loux, & van't Zand 1995) 10 5 objects, (Li & Biswas 1995) 10 5 , (Sanjeev & Zytkow 1995) 10 3 , <ref> (Simoudis, Livezey, & Kerber 1995) </ref> 10 6 ), and by the fact that this application size, there exists a number of available ML and KDD algorithms (from our own group and others) that could be used to test the feasibility of the plug-in concept.
Reference: <author> Michalski, R. et. al.. </author> <year> 1992. </year> <title> Mining for knowledge in databases: The INLEN architecture, initial implementation and first results. </title> <journal> J. Intell. Inf. Sys. </journal> <volume> 1(1):85 - 113. </volume>
Reference: <author> Morik, K.; Wrobel, S.; Kietz, J.-U.; and Emde, W. </author> <year> 1993. </year> <title> Knowledge Acquisition and Machine Learning: Theory Methods and Applications. </title> <publisher> London: Academic Press. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5 programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: At first, we chose to include as plug-ins for internal use at our site: * for classification tasks, the decision tree algorithm C4.5 in the original C version available from Ross Quinlan <ref> (Quinlan 1993) </ref>, a backpropagation neural network realized using SNSS (Zell 1994), as well as our own instance-based method KNN (Wettschereck 1994) and Salzberg's NGE (Salzberg 1991; Wettschereck & Dietterich 1995)). * for clustering tasks, the AUTOCLASS algorithm (Cheese man & Stutz 1996) as available from the authors * for regression tasks,
Reference: <author> Ribeiro, J. S.; Kaufman, K. A.; and Kerschberg, L. </author> <year> 1995. </year> <title> Knowledge discovery from multiple databases. </title> <type> 240 - 245. </type>
Reference: <institution> In (Fayyad & Uthurusamy 1995). </institution>
Reference: <author> Salzberg, S. </author> <year> 1991. </year> <title> A nearest hyperrectangle learning method. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 277-309. </pages>
Reference: <author> Sanjeev, A., and Zytkow, J. </author> <year> 1995. </year> <note> Discovering enrollment knowledge in university databases. 246 - 251. In (Fayyad & Uthurusamy 1995). </note>
Reference-contexts: that many other published data-mining applications fall 2 KEPLER will be demonstrated at the conference. into this class (e.g. (Apte & Hong 1996) 10 4 , (Dzeroski & Grbovic 1995) 10 3 , (Feelders, le Loux, & van't Zand 1995) 10 5 objects, (Li & Biswas 1995) 10 5 , <ref> (Sanjeev & Zytkow 1995) </ref> 10 3 , (Simoudis, Livezey, & Kerber 1995) 10 6 ), and by the fact that this application size, there exists a number of available ML and KDD algorithms (from our own group and others) that could be used to test the feasibility of the plug-in concept.
Reference: <author> Simoudis, E.; Livezey, B.; and Kerber, R. </author> <year> 1995. </year> <title> Using Recon for data cleaning. </title> <type> 282 - 287. </type> <note> In (Fayyad & Uthurusamy 1995). </note>
Reference-contexts: 2 KEPLER will be demonstrated at the conference. into this class (e.g. (Apte & Hong 1996) 10 4 , (Dzeroski & Grbovic 1995) 10 3 , (Feelders, le Loux, & van't Zand 1995) 10 5 objects, (Li & Biswas 1995) 10 5 , (Sanjeev & Zytkow 1995) 10 3 , <ref> (Simoudis, Livezey, & Kerber 1995) </ref> 10 6 ), and by the fact that this application size, there exists a number of available ML and KDD algorithms (from our own group and others) that could be used to test the feasibility of the plug-in concept.
Reference: <author> Simoudis, E.; Livezey, B.; and Kerber, R. </author> <year> 1996. </year> <title> Integrating inductive and deductive reasoning for data mining. </title> <booktitle> chapter 14, </booktitle> <volume> 353 - 374. </volume> <editor> In (Fayyad et al. </editor> <year> 1996). </year> <institution> Thinking Machines Corp. </institution> <year> 1996. </year> <title> Darwin: Intelligent data mining. </title> <note> WWW http://www.think.com. </note>
Reference-contexts: ISL's Clemen-tine (Integral Solutions Ltd. 1996) and Lockheed's Recon <ref> (Simoudis, Livezey, & Kerber 1996) </ref> are two commercially 1 In fact, some authors reserve the term KDD to denote the entire process, whereas data mining is used to refer to a single analysis step (Fayyad, Piatetsky-Shapiro, & Smyth 1996). available examples of such systems, the former offering decision trees and neural <p> While existing multi-strategy systems like RECON <ref> (Simoudis, Livezey, & Kerber 1996) </ref> or CLEMEN-TINE (Integral Solutions Ltd. 1996) alleviate this problem somewhat by offering multiple analysis algorithms (see related work section below), they still offer a fixed choice of algorithms, leaving the unsolved problem of what to do if the necessary method happens not to be included. <p> In micro-level integration, all modules directly rely on a common repository of data without transformation, and cooperate during processing, not only when they have finished. Multi-strategy data mining systems have so far mostly been realized by macro-level integration, e.g. in Recon <ref> (Simoudis, Livezey, & Kerber 1996) </ref> where several modules are linked to a data server. Micro-level integration is only beginning to be attempted, e.g. in the KESO project, where all search modules use a common hypothesis space manager and can share description generation operators (Wrobel et al. 1996). <p> For commercial tools, there is ISL's CLEMENTINE system (Integral Solutions Ltd. 1996), integrating decision trees and neural networks. This system offers an excellent user interface, but appears limited to classification and clustering tasks. Lockheed's Recon system <ref> (Simoudis, Livezey, & Kerber 1996) </ref> addresses a wider range of problems, including also instance based methods. Both systems, however, seem to lack the extensibility that characterizes the architecture presented here, and seem to require kernel reprogramming to add new algorithms.
Reference: <author> Wettschereck, D., and Dietterich, T. </author> <year> 1995. </year> <title> An experimental comparison of the nearest-neighbor and nearest-hyperrectangle algorithms. </title> <booktitle> Machine Learning 19 </booktitle> <pages> 5-28. </pages>
Reference: <author> Wettschereck, D. </author> <year> 1994. </year> <title> A Study of Distance-Based Machine Learning Algorithms. </title> <type> Ph.D. Dissertation, </type> <institution> Oregon State University. </institution>
Reference-contexts: first, we chose to include as plug-ins for internal use at our site: * for classification tasks, the decision tree algorithm C4.5 in the original C version available from Ross Quinlan (Quinlan 1993), a backpropagation neural network realized using SNSS (Zell 1994), as well as our own instance-based method KNN <ref> (Wettschereck 1994) </ref> and Salzberg's NGE (Salzberg 1991; Wettschereck & Dietterich 1995)). * for clustering tasks, the AUTOCLASS algorithm (Cheese man & Stutz 1996) as available from the authors * for regression tasks, the MARS (multiple adaptive re-gresssion spline) algorithm of (Friedman 1991), in our own implementation * the pattern discovery algorithm
Reference: <author> Wrobel, S.; Wettschereck, D.; Verkamo, A. I.; Siebes, A.; Mannila, H.; Kwakkel, F.; and Klosgen, W. </author> <year> 1996. </year> <title> User interactivity in very large scale data mining. Contact keso-develop@cwi.nl. </title>
Reference-contexts: Micro-level integration is only beginning to be attempted, e.g. in the KESO project, where all search modules use a common hypothesis space manager and can share description generation operators <ref> (Wrobel et al. 1996) </ref>. Clearly, it is difficult to make a micro-level integration architecture extensible in the sense defined above, since to integrate a new method, the internals of existing methods and the system kernel must be known. The architecture we have chosen is therefore based on macro-level integration. <p> Similarly, GLS (Zhong & Ohsuga 1995), is a multi-strategy system with four fixed analysis methods without extension facilities. Finally, a useful comparison is with the architectural concepts of the KESO data mining project in which we are also involved <ref> (Wrobel et al. 1996) </ref>. In KESO, the very explicit goal at the outset was to create a data mining system capable of handling the very large scale problems (&gt;>10 6 objects).
Reference: <author> Zell, A. e. </author> <year> 1994. </year> <note> SNNS user manual, version 3.2. Fakultatsbericht 6/94, </note> <institution> IPVR, Universitat Stuttgart, Ger-many. </institution>
Reference-contexts: At first, we chose to include as plug-ins for internal use at our site: * for classification tasks, the decision tree algorithm C4.5 in the original C version available from Ross Quinlan (Quinlan 1993), a backpropagation neural network realized using SNSS <ref> (Zell 1994) </ref>, as well as our own instance-based method KNN (Wettschereck 1994) and Salzberg's NGE (Salzberg 1991; Wettschereck & Dietterich 1995)). * for clustering tasks, the AUTOCLASS algorithm (Cheese man & Stutz 1996) as available from the authors * for regression tasks, the MARS (multiple adaptive re-gresssion spline) algorithm of (Friedman
Reference: <author> Zhong, N., and Ohsuga, S. </author> <year> 1995. </year> <title> Toward a multi-strategy and cooperative discovery system. </title> <type> 337 - 342. </type> <note> In (Fayyad & Uthurusamy 1995). </note>
Reference-contexts: The INLEN system (Michalski 1992; Ribeiro, Kaufman, & Kerschberg 1995) is related to the work presented here since it also conceptualizes data man agement and analysis as operators, however without a focus on extensibility and closely tied to AQ and related methods. Similarly, GLS <ref> (Zhong & Ohsuga 1995) </ref>, is a multi-strategy system with four fixed analysis methods without extension facilities. Finally, a useful comparison is with the architectural concepts of the KESO data mining project in which we are also involved (Wrobel et al. 1996).
References-found: 32

