URL: http://www.cse.ogi.edu/Sparse/paper/wakatani.parle.94.ps
Refering-URL: http://www.cse.ogi.edu/Sparse/sparse.papers.html
Root-URL: http://www.cse.ogi.edu
Email: wakatani@isl.mei.co.jp  mwolfe@cse.ogi.edu  
Phone: 2  
Title: A New Approach to Array Redistribution: Strip Mining Redistribution  
Author: Akiyoshi Wakatani and Michael Wolfe 
Affiliation: 1 Matsushita Electric Industrial  Oregon Graduate Institute of Science Technology  
Abstract: Languages such as High Performance Fortran are used to implement parallel algorithms by distributing large data structures across a multicomputer system. To reduce the communication time for the redistribution of arrays, we proposes a new scheme, strip mining redistribution. By using this scheme, the communication overhead is almost completely overlapped with the preceding or following computation. We have implemented a library for two-dimensional arrays using strip mining redistribution, and have achieved a speedup of 1.7 for a 2048 x 2048 ADI program. 
Abstract-found: 1
Intro-found: 1
Reference: [HKT92] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for distributed-memory machines. </title> <journal> Communications ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: On the other hand, since the second step is to solve a tridiagonal system in the second dimension, u should be distributed by (BLOCK, *). Therefore, every step requires the redistribution of the distributed array variable. Even when redistribution is not indicated explicitly, communication optimization <ref> [HKT92] </ref> often effectively requires redistribution. Consider the following simple HPF program fragment. real*8,dimension (n,n) ::A, B c$hpf distribute (BLOCK,BLOCK) :: A c$hpf distribute (*, BLOCK):: B : : The assignment statement (B = A) doesn't explicitly require redistribution.
Reference: [HPFF92] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification Version 1.0. </title> <type> Technical report, </type> <institution> Rice University, Houston Texas, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Distributed memory multicomputers are candidates for the next generation of supercomputers and HPF <ref> [HPFF92] </ref> is a programming language designed for such a machine. HPF allows users to make a program executable on any distributed memory multicomputer. However, users have to be careful not to reduce the performance due to communication overhead. Data structure redistribution is one of the most expensive communication routines.
Reference: [K + 90] <author> K. Kaneko et al. </author> <title> Processing element design for a parallel computer. </title> <journal> IEEE MICRO, </journal> <volume> 10(2) </volume> <pages> 26-38, </pages> <year> 1990. </year>
Reference: [NWD93] <author> Michaell Noakes, Deborah Wallach, and William Dally. </author> <title> The J-Machine multicomputer: An architectural evaluation. </title> <booktitle> In Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 224-235, </pages> <year> 1993. </year>
Reference-contexts: However, as shown in the analysis and the experiment, our scheme can be applied to only relatively large arrays to acquire improvement of performance. We have several possibilities to make our scheme more effective. Since current message passing multicomputers, such as the J-machine <ref> [NWD93] </ref>, have extremely small overhead to invoke messages, the startup time of condition (7) is expected to decrease. So, the proposed scheme can be applied to the redistribution of smaller arrays. The second possibility is to use a hardware facility for collective communications.
Reference: [Pre88] <author> William Press. </author> <booktitle> Numerical Recipes in C, </booktitle> <pages> pages 665-666. </pages> <address> Cam-bridge, </address> <year> 1988. </year>
Reference: [Wol89] <author> Michael Wolfe. </author> <booktitle> Optimizaing Supercompilers for Supercomputers, </booktitle> <pages> pages 97-123. </pages> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: However, other cases arise in actual applications, such as when an outer loop cannot be par-allelized, or an outer loop is not identical to the distributed dimension, or neither. To cope with such cases, we utilize loop interchanging <ref> [Wol89 ] </ref> and delayed redistribution. Consider the two dimensional programs shown in Table. 2. It is assumed that array A is distributed in (BLOCK,*) manner before the redistribute directive.
Reference: [WW94a] <author> Akiyoshi Wakatani and Michael Wolfe. </author> <title> The effectiveness of message strip mining. </title> <booktitle> In Seventh Int'l Conf. on Parallel and Distributed Computing Systems (submitted), </booktitle> <year> 1994. </year>
Reference-contexts: Namely, by aggregating several messages that are to be sent to the same processor into one message, the startup cost can be dramatically reduced. However, the more messages are combined, the less the effectiveness of strip redistribution. The detailed discussion will be appeared at another paper <ref> [WW94a] </ref>. 5.2 Other Loops So far, we dealt with only case where an outer loop is both parallelized and identical to the distributed dimension of arrays.
Reference: [WW94b] <author> Akiyoshi Wakatani and Michael Wolfe. </author> <title> Optimization of the redistribution of arrays for distributed memory multicomput-ers. </title> <note> In Parallel Computing (submitted), </note> <year> 1994. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: However, in order to reduce the communication overhead, message vectorization may be applied to the statement. The communication pat tern is essentially the same as redistributing the array A. In <ref> [WW94b] </ref>, we proposed a spiral mapping of formal processors onto actual processors that can reduce communication conflicts of redistribution of distributed arrays and compared the spiral mapping with the normal mapping. This method can translate the global communication that redistribution needs into neighborhood communication.
References-found: 8

