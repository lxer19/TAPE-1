URL: http://www.cs.washington.edu/homes/waynew/papers/papsub.ps.gz
Refering-URL: http://www.cs.washington.edu/homes/waynew/papers.html
Root-URL: 
Email: e-mail fwaynew, baerg@cs.washington.edu  
Title: DRAM On-chip Caching (draft do not distribute)  
Author: Wayne Wong and Jean-Loup Baer 
Date: February 21, 1997  
Address: Box 352350  Seattle, Wa 98195-2350  
Affiliation: Department of Computer Science and Engineering,  University of Washington  
Abstract: This paper presents methods to reduce memory latency in the main memory subsystem below the board-level cache. We consider conventional page-mode DRAMs and cached DRAMs. Evaluation is performed via trace-driven simulation of a suite of nine benchmarks. In the case of page-mode DRAMs we show that it can be detrimental to use page-mode naively. We propose two enhancements that reduce overall memory latency in this case: one is the remapping of address bits and the other is selective usage of page-mode under the control of the memory controller. In the case of cached DRAM we quantify the improvements that can be attained by introducing some SRAM cache on the DRAM chip. We evaluate various design alternatives for the line size and associativity of the SRAM cache. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Texas Instruments MOS Memory Data Book. </institution> <year> 1996. </year>
Reference-contexts: Main memory is usually composed of dynamic random-access memory (DRAM) devices [6]. Current DRAMs have an optimized feature called page-mode. In page-mode, the device has an on-chip buffer, that provides access which is a factor of two or more faster than a normal DRAM access <ref> [1] </ref>. However, as we explain later, page-mode operation lacks flexibility and can result in performance degradation rather than improvement. This is because page-mode is meant to be a high-bandwidth mechanism and its buffer is not designed to be used as a conventional cache.
Reference: [2] <author> Dave Bursky. </author> <title> Fast DRAMs can be swapped for SRAM caches. </title> <booktitle> Electronic Design, </booktitle> <pages> pages 55-56,60-67, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: This is because page-mode is meant to be a high-bandwidth mechanism and its buffer is not designed to be used as a conventional cache. Among the emerging DRAMs (synchronous DRAMs, EDODRAMs, etc.), two that are particularly attractive for caching are cached DRAMs ( e.g., CDRAM and EDRAM <ref> [2, 12] </ref>). Both devices replace the page-mode buffer with static RAM (SRAM). In addition to providing higher bandwidth, the SRAM can be used as a conventional cache to reduce memory latency [5]. The thrust of this paper is two-fold. <p> This addressing pattern is typical of vector applications [11, 7]. However, page-mode operation can be detrimental for applications accessing memory "randomly". We will return to this problem and present some solutions in Section 4. 2.2 Cached DRAMs In newer DRAM technologies such as CDRAM and EDRAM <ref> [2, 12, 5] </ref>, the page buffer is replaced with a small SRAM cache. The design of these DRAMs encourage on-chip DRAM caching and eliminate the drawbacks of page-mode DRAMs.
Reference: [3] <author> Michael Deering, Stephen Schlapp, and Michael Lavelle. FBRAM: </author> <title> A new form of memory optimized for 3d graphics. </title> <booktitle> In Proc. SIGGRAPH Conference, </booktitle> <pages> pages 167-174, </pages> <year> 1994. </year>
Reference-contexts: Section 5 investigates the impact of the SRAM cache in DRAM devices and evaluates several design alternatives. Finally, in Section 6 we conclude and place this study in the perspective of integrated processor/memory systems <ref> [3, 14] </ref>. 2 Page-mode DRAM and Cached DRAM Operation In this section, we review very briefly the operation of page-mode DRAMs and cached DRAMs. 2.1 Page-mode DRAM An access in DRAM devices usually consists of a row access followed by a column access (see Figure 1). <p> The first one, corresponding to the large board-level cache, indicates that the correct application of page-mode DRAMs and a rather straightforward implementation of a cached DRAM can provide important benefits to all applications. The second relates to the recent proposals on integrating processor and memory on the same chip <ref> [3, 14] </ref>. If indeed this integration becomes technologically feasible and cost-effective, then the processor, small board-level cache, and the SRAM cache and the DRAMs can all be integrated. What our study shows is that the SRAM can be effective even with a very simple organization. draft do not distribute 19
Reference: [4] <author> Jeffrey Gee, Dionisios Pnevmatikatos, Mark Hill, and Alan Smith. </author> <title> Cache performance of the SPEC benchmark suite. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 17-28, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In our study we concentrate only on data references. For our benchmarks, the instruction reference working set fits well in a small instruction cache and would generate few memory references beyond the compulsory misses [10]. Our workload consists of benchmarks from SPEC92 <ref> [4] </ref> that exhibited significant data reference misses per instruction (MPI). The benchmarks are described in Table 1.
Reference: [5] <author> Charles Hart. </author> <title> CDRAM in a unified memory architecture. </title> <booktitle> In Proc. Spring CompCon, </booktitle> <pages> pages 261-266, </pages> <year> 1994. </year>
Reference-contexts: Both devices replace the page-mode buffer with static RAM (SRAM). In addition to providing higher bandwidth, the SRAM can be used as a conventional cache to reduce memory latency <ref> [5] </ref>. The thrust of this paper is two-fold. First, we investigate methods of reducing memory latency using page-mode DRAMS in a selective manner. Second, we explore the various organization parameters of SRAM cache design for a cached DRAM. The remainder of the paper is organized as follows. <p> This addressing pattern is typical of vector applications [11, 7]. However, page-mode operation can be detrimental for applications accessing memory "randomly". We will return to this problem and present some solutions in Section 4. 2.2 Cached DRAMs In newer DRAM technologies such as CDRAM and EDRAM <ref> [2, 12, 5] </ref>, the page buffer is replaced with a small SRAM cache. The design of these DRAMs encourage on-chip DRAM caching and eliminate the drawbacks of page-mode DRAMs.
Reference: [6] <author> John Hennessy and David Patterson. </author> <title> Computer architecture: A quantitative approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1990. </year>
Reference-contexts: Generally, main memory has been viewed as having uniform access latency. This is however an over simplification which does not take advantage of developments in memory technology. Main memory is usually composed of dynamic random-access memory (DRAM) devices <ref> [6] </ref>. Current DRAMs have an optimized feature called page-mode. In page-mode, the device has an on-chip buffer, that provides access which is a factor of two or more faster than a normal DRAM access [1]. <p> Before data can be read from the DRAM array, it has to be precharged. The precharge latency dictates the minimum time between array accesses and can be hidden if there is sufficient time between accesses. Typical DRAM precharge time is 30 to 50 nanoseconds. In page-mode <ref> [6] </ref>, row accesses can be eliminated on successive requests if the desired bits are in the page buffer. The access time is then reduced to the column access time. Thus, the page buffer can be used as a cache with a single long line. <p> Therefore, we model the cache hierarchy as a single-level cache 1 . This cache uses a write-back policy which minimizes memory traffic <ref> [6] </ref> and which is the one most often used in caches closest to main memory. The processor is modelled with an ideal pipeline, i.e., each instruction executes in a single cycle, except when there is a cache miss.
Reference: [7] <author> W.-C Hsu and James Smith. </author> <title> Performance of cached DRAM organizations in vector supercomputers. </title> <booktitle> In Proc. 20th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 327-336, </pages> <year> 1996. </year>
Reference-contexts: What this simplified analysis shows though is that page-mode operation will be effective for access to large data sets, overflowing or bypassing caches, with low stride between consecutive accesses to individual DRAM banks. This addressing pattern is typical of vector applications <ref> [11, 7] </ref>. However, page-mode operation can be detrimental for applications accessing memory "randomly".
Reference: [8] <author> Norm Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proc. of 17th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <year> 1990. </year>
Reference-contexts: The SRAM cache will play the role of a (small) second-level cache. * Conflict misses in the board-level cache. The SRAM cache could play the role of a victim cache <ref> [8] </ref>. * Sequential accesses to large data structures. The SRAM cache could play the role of a stream buffer [8]. <p> The SRAM cache will play the role of a (small) second-level cache. * Conflict misses in the board-level cache. The SRAM cache could play the role of a victim cache <ref> [8] </ref>. * Sequential accesses to large data structures. The SRAM cache could play the role of a stream buffer [8]. In our simulations, we varied the line size from 8 KB (the original one) to 128 bytes and we varied the associativity from direct-mapped (the original one) to 4-way (of course, only when the capacity allowed us to do it).
Reference: [9] <author> Johny Lee and Alan Smith. </author> <title> Branch prediction strategies and branch target buffer design. </title> <journal> Computer, </journal> <volume> 17(1) </volume> <pages> 6-22, </pages> <year> 1984. </year>
Reference-contexts: The two other schemes, affinity1 and affinity2, attempt to predict whether to leave the DRAM bank in page-mode or to precharge. Similar to simple branch prediction logic <ref> [9] </ref> affinity1 and affinity2 use a 1-bit and 2-bit, respectively, prediction scheme as to whether to use page-mode or precharge. Incrementing or draft do not distribute 13 decrementing the prediction counter is done on read accesses only. in the remapping evaluation.
Reference: [10] <author> Ann Marie Maynard, Colette Donnelly, and Bret Olszewski. </author> <title> Contrasting characteristics and cache performance of technical and multi-user commercial workloads. </title> <booktitle> In Proc. 8th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 145-156, </pages> <year> 1994. </year>
Reference-contexts: The generated references are those of an uninstrumented binary. In our study we concentrate only on data references. For our benchmarks, the instruction reference working set fits well in a small instruction cache and would generate few memory references beyond the compulsory misses <ref> [10] </ref>. Our workload consists of benchmarks from SPEC92 [4] that exhibited significant data reference misses per instruction (MPI). The benchmarks are described in Table 1.
Reference: [11] <author> Sally McKee and William Wulf. </author> <title> Access ordering and memory-conscious cache utilization. </title> <booktitle> In Proc. of 1st Int. Symp. on High-Performance Computer Architec ture, </booktitle> <year> 1995. </year>
Reference-contexts: What this simplified analysis shows though is that page-mode operation will be effective for access to large data sets, overflowing or bypassing caches, with low stride between consecutive accesses to individual DRAM banks. This addressing pattern is typical of vector applications <ref> [11, 7] </ref>. However, page-mode operation can be detrimental for applications accessing memory "randomly".
Reference: [12] <author> Ray Ng. </author> <title> Fast computer memories. </title> <journal> IEEE Spectrum, </journal> <pages> pages 36-39, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This is because page-mode is meant to be a high-bandwidth mechanism and its buffer is not designed to be used as a conventional cache. Among the emerging DRAMs (synchronous DRAMs, EDODRAMs, etc.), two that are particularly attractive for caching are cached DRAMs ( e.g., CDRAM and EDRAM <ref> [2, 12] </ref>). Both devices replace the page-mode buffer with static RAM (SRAM). In addition to providing higher bandwidth, the SRAM can be used as a conventional cache to reduce memory latency [5]. The thrust of this paper is two-fold. <p> This addressing pattern is typical of vector applications [11, 7]. However, page-mode operation can be detrimental for applications accessing memory "randomly". We will return to this problem and present some solutions in Section 4. 2.2 Cached DRAMs In newer DRAM technologies such as CDRAM and EDRAM <ref> [2, 12, 5] </ref>, the page buffer is replaced with a small SRAM cache. The design of these DRAMs encourage on-chip DRAM caching and eliminate the drawbacks of page-mode DRAMs.
Reference: [13] <author> Steven Przybylski. </author> <title> New DRAMs improve bandwidth (part 1). </title> <type> Microprocessor Report, </type> <pages> pages 18-21, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Second, instead of having a single long cache line, the cache can be organized with parameters more like a traditional cache. For example, the cache in the CDRAM has 256 entries <ref> [13] </ref>. Moreover, with separate address lines (for the DRAM array and the SRAM cache) and external tag logic, the CDRAM's cache can be made set-associative. In Section 5, we explore the design space of teh SRAM cache.
Reference: [14] <author> Ashley Saulsbury, Fong Pong, and Andreas Nowatzy. </author> <title> Missing the memory wall: The case for processor/memory integration. </title> <booktitle> In Proc. of 23rd Int. Symp. on Computer Architecture, </booktitle> <pages> pages 90-101, </pages> <year> 1996. </year>
Reference-contexts: Section 5 investigates the impact of the SRAM cache in DRAM devices and evaluates several design alternatives. Finally, in Section 6 we conclude and place this study in the perspective of integrated processor/memory systems <ref> [3, 14] </ref>. 2 Page-mode DRAM and Cached DRAM Operation In this section, we review very briefly the operation of page-mode DRAMs and cached DRAMs. 2.1 Page-mode DRAM An access in DRAM devices usually consists of a row access followed by a column access (see Figure 1). <p> In this case, the memory latencies depend on the memory subsystem being modelled as explained below. We have performed our experiments with two "extreme" cache sizes: 8 KB and 256 KB. The small 8KB capacity corresponds to low-end machines, e.g. the MicroSparc as indicated in <ref> [14] </ref>. <p> The first one, corresponding to the large board-level cache, indicates that the correct application of page-mode DRAMs and a rather straightforward implementation of a cached DRAM can provide important benefits to all applications. The second relates to the recent proposals on integrating processor and memory on the same chip <ref> [3, 14] </ref>. If indeed this integration becomes technologically feasible and cost-effective, then the processor, small board-level cache, and the SRAM cache and the DRAMs can all be integrated. What our study shows is that the SRAM can be effective even with a very simple organization. draft do not distribute 19
Reference: [15] <author> Alan Smith. </author> <title> Line (block) size choice for CPU cache memories. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-36(9):1063-1075, </volume> <month> September </month> <year> 1987. </year> <title> draft do not distribute 20 </title>
Reference-contexts: Many studies have been performed to characterize the best choice of line size and associativity for a given cache capacity (see, e.g., <ref> [15] </ref>). The parameters for the design of the SRAM cache on the DRAM chip could be completely different since this cache does not receive requests comparable to those of a (processor) cache.
Reference: [16] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM a system for building customized program analysis tools. </title> <booktitle> In Proc. of the SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <year> 1994. </year>
Reference-contexts: The height of the bar is the MPI on an 8 KB cache. The lower component of each bar is the MPI on a 256 KB cache. traces were generated from an Alpha 21064 using the ATOM <ref> [16] </ref> software instrumentation tool. ATOM inserts instrumentation code in the object code. The generated references are those of an uninstrumented binary. In our study we concentrate only on data references.
Reference: [17] <author> John Zurawski, John Murray, and Paul Lemmon. </author> <title> The design and verification of the AlphaS-tation 600 5-series workstation. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1) </volume> <pages> 89-99, </pages> <year> 1995. </year>
Reference-contexts: One way to circumvent this problem is to use other bits of the address for the DRAM row and column addresses. In the Alphastation 600 <ref> [17] </ref>, the low order column bits are exchanged with the row bits and the high-order bits are used for bank selection. Now both the (board-level) cache line and the one it has just replaced can be cached by the same DRAM row.
References-found: 17

