URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/webb96a.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/webb96a.html
Root-URL: 
Email: webb@deakin.edu.au  
Title: Further Experimental Evidence against the Utility of Occam's Razor  
Author: Geoffrey I. Webb 
Address: Geelong, Vic, 3217, Australia.  
Affiliation: School of Computing and Mathematics Deakin University  
Note: Journal of Artificial Intelligence Research 4 (1996) 397-417 Submitted 12/95; published 6/96  Occam's razor as it is commonly applied in modern machine learning.  
Abstract: This paper presents new experimental evidence against the utility of Occam's razor. A systematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision tree's complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: Each partition, represented by a leaf, contains the objects that are similar in relevant respects and thus are expected to belong to the same class. This raises the issue of how similarity should be measured. Instance-based learning methods <ref> (Aha, Kibler, & Albert, 1991) </ref> tend to map the instance space onto an n-dimensional geometric space and then employ geometric distance measures within that space to measure similarity. Such an approach is problematic on a number of grounds. First, it assumes that the underlying metrics of different attributes are commensurable.
Reference: <author> Ali, K., Brunk, C., & Pazzani, M. </author> <year> (1994). </year> <title> On learning multiple descriptions of a concept. </title> <booktitle> In Proceedings of Tools with Artificial Intelligence New Orleans, </booktitle> <address> LA. </address>
Reference: <author> Berkman, N. C., & Sandholm, T. W. </author> <year> (1995). </year> <title> What should be minimized in a decision tree: A re-examination. </title> <type> Technical report 95-20, </type> <institution> University of Massachusetts at Amherst, Computer Science Department, </institution> <address> Amherst, Mass. </address>
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. </author> <year> (1987). </year> <title> Occam's Razor. </title> <journal> Information Processing Letters, </journal> <volume> 24, </volume> <pages> 377-380. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International, </booktitle> <address> Belmont, Ca. </address>
Reference: <author> Brodley, C. E. </author> <year> (1995). </year> <title> Automatic selection of split criterion during tree growing based on node selection. </title> <booktitle> In Proceedings of the Twelth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 73-80 Taho City, Ca. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bunge, M. </author> <year> (1963). </year> <title> The Myth of Simplicity. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: This research has considered only one version of Occam's razor that favors minimization of syntactic complexity in the expectation that this will tend to increase predictive accuracy. Other interpretations of Occam's razor are also possible, such as that one should minimize semantic complexity. While others <ref> (Bunge, 1963) </ref> have provided philosophical objections to such formulations of Occam's razor, this paper has not sought to investigate them. The version of Occam's razor examined in this research has been used widely in machine learning with apparent success.
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1990). </year> <title> What should be minimized in a decision tree? In AAAI-90: </title> <booktitle> Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 749-754 Boston, Ma. </address>
Reference: <author> Good, I. J. </author> <year> (1977). </year> <title> Explicativity: A mathematical theory of explanation with statistical applications. </title> <journal> Proceedings of the Royal Society of London Series A, </journal> <volume> 354, </volume> <pages> 303-330. </pages>
Reference-contexts: The modern interpretation of Occam's razor has been characterized as "of two hypotheses H and H 0 , both of which explain E, the simpler is to be preferred" <ref> (Good, 1977) </ref>. However, this does not specify what aspect of a theory should be measured for simplicity. Syntactic, semantic, epistemological and pragmatic simplicity are all alternative criteria that can and have been employed Bunge (1963).
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 (1), </volume> <pages> 63-90. </pages>
Reference: <author> Holte, R. C., Acker, L. E., & Porter, B. W. </author> <year> (1989). </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 813-818 Detroit. </address> <publisher> Morgan Kaufmann. 415 Webb Lubinsky, </publisher> <editor> D. J. </editor> <year> (1995). </year> <title> Increasing the performance and consistency of classification trees by using the accuracy criterion at the leaves. </title> <booktitle> In Proceedings of the Twelth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 371-377 Taho City, Ca. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Michalski, R. S. </author> <year> (1984). </year> <title> A theory and methodology of inductive learning. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pp. 83-129. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Murphy, P. M. </author> <year> (1995). </year> <title> An empirical analysis of the benefit of decision tree size biases as a function of concept distribution. </title> <type> Tech. rep. 95-29, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: A bias toward simplicity performed well when the target concept was best described by a simple classifier and a bias toward complexity performed well when the target concept was best described by a complex classifier <ref> (Murphy, 1995) </ref>. In addition, the simplest classifiers obtained better than average (over all consistent classifiers) predictive accuracy when the data was augmented with irrelevant attributes or attributes strongly correlated to the target concept, but not required for classification.
Reference: <author> Murphy, P. M., & Aha, D. W. </author> <year> (1993). </year> <title> UCI repository of machine learning databases. [Machine-readable data repository]. </title> <institution> University of California, Department of Information and Computer Science, </institution> <address> Irvine, CA. </address>
Reference-contexts: Webb (1994) presented results that suggest that for a wide range of learning tasks from the UCI repository of learning tasks <ref> (Murphy & Aha, 1993) </ref>, the relative generality of the classifiers is a better predictor of classification performance than is the relative surface syntactic complexity. <p> Holte (1993) compared learning very simple classification rules with the use of a sophisticated learner of complex decision trees. He found that, for a number of tasks from the UCI repository of machine learning datasets <ref> (Murphy & Aha, 1993) </ref>, the simple rules achieved accuracies of within a few percentage points of the complex trees. This could be considered as supportive of the Occam thesis. However, in no case did the simple rules outperform the more complex decision trees. <p> This procedure takes the form of a post-processor for decision trees produced by C4.5 (Quin-lan, 1993). The application of this procedure to a range of learning tasks from the UCI repository of learning tasks <ref> (Murphy & Aha, 1993) </ref> is demonstrated to result, on average, 401 Webb in increased predictive accuracy when the inferred decision trees are applied to previously unseen data. 3.1 Theoretical Basis for the Decision Tree Post-processor The similarity assumption is a common assumption in machine learning|that objects that are similar have high <p> C4.5X produces post-processed versions of both of these trees. 3.3 Evaluation To evaluate the post-processor it was applied to all datasets containing continuous attributes from the UCI machine learning repository <ref> (Murphy & Aha, 1993) </ref> that were then held (due to previous machine learning experimentation) in the local repository at Deakin University. These datasets are believed to be broadly representative of those in the repository as a whole.
Reference: <author> Murphy, P. M., & Pazzani, M. J. </author> <year> (1994). </year> <title> Exploring the decision forest: An empirical inves tigation of Occam's razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 257-275. </pages>
Reference: <author> Niblett, T., & Bratko, I. </author> <year> (1986). </year> <title> Learning decision rules in noisy domains. </title> <editor> In Bramer, M. A. (Ed.), </editor> <booktitle> Research and Development in Expert Systems III, </booktitle> <pages> pp. 25-34. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: The level of support for a given threshold is evaluated using a Laplacian accuracy estimate <ref> (Niblett & Bratko, 1986) </ref>. Because each leaf relates to a binary classification (an object belongs to the class in question or does not), the binary form of Laplace is used.
Reference: <author> Nock, R., & Gascuel, O. </author> <year> (1995). </year> <title> On learning decision committees. </title> <booktitle> In Proceedings of the Twelth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 413-420 Taho City, Ca. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Oliver, J. J., & Hand, D. J. </author> <year> (1995). </year> <title> On pruning and averaging decision trees. </title> <booktitle> In Proceedings of the Twelth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 430-437 Taho City, Ca. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pearl, J. </author> <year> (1978). </year> <title> On the connection between the complexity and credibility of inferred models. </title> <journal> International Journal of General Systems, </journal> <volume> 4, </volume> <pages> 255-264. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1991). </year> <title> Improved estimates for the accuracy of small disjuncts. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 93-98. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs For Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos. </address>
Reference-contexts: As a further example, consider a learning task for which the class attribute is a simple count of the number of missing attribute values for an object. Assume that such a learning task was submitted to a system, such as C4.5 <ref> (Quinlan, 1993) </ref>, that develops classifiers that have no mechanism for testing during classification whether an attribute value is missing. Again, the majority of machine learning researchers would be unconcerned that their systems failed to perform well in such circumstances. Machine learning is simply unsuited to such tasks.
Reference: <author> Rao, R. B., Gordon, D., & Spears, W. </author> <year> (1995). </year> <title> For every generalization action is there really an equal and opposite reaction? Analysis of the conservation law for generalization performance. </title> <booktitle> In Proceedings of the Twelth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 471-479 Taho City, </address> <publisher> Ca. </publisher> <editor> Morgan Kaufmann. </editor> <title> 416 Further Experimental Evidence against the Utility of Occam's Razor Rendell, </title> <editor> L., & Seshu, R. </editor> <year> (1990). </year> <title> Learning hard concepts through constructive induction: Framework and rationale. </title> <journal> Computational Intelligence, </journal> <volume> 6, </volume> <pages> 247-270. </pages>
Reference: <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11, </volume> <pages> 416-431. </pages>
Reference-contexts: Two key approaches have been developed, Minimum Message Length (MML) (Wallace & Boulton, 1968) and Minimum Description Length (MDL) <ref> (Rissanen, 1983) </ref>. Both approaches admit to probabilistic interpretations. Given prior probabilities for both theories and data, minimization of the MML encoding closely approximates maximization of posterior probability (Wal-lace & Freeman, 1987). An MDL code length defines an upper bound on "unconditional likelihood" (Rissanen, 1987).
Reference: <author> Rissanen, J. </author> <year> (1987). </year> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society Series B, </journal> <volume> 49 (3), </volume> <pages> 223-239. </pages>
Reference-contexts: Both approaches admit to probabilistic interpretations. Given prior probabilities for both theories and data, minimization of the MML encoding closely approximates maximization of posterior probability (Wal-lace & Freeman, 1987). An MDL code length defines an upper bound on "unconditional likelihood" <ref> (Rissanen, 1987) </ref>. The two approaches differ in that MDL employs a universal prior, which Rissanen (1983) explicitly justifies in terms of Occam's razor, while MML allows the specification of distinct appropriate priors for each induction task.
Reference: <author> Schaffer, C. </author> <year> (1992). </year> <title> Sparse data and the effect of overfitting avoidance in decision tree induction. </title> <booktitle> In AAAI-92: Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 147-152 San Jose, CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 153-178. </pages>
Reference: <author> Schaffer, C. </author> <year> (1994). </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the 1994 International Conference on Machine Learning San Mateo, </booktitle> <address> Ca. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Where relevant, an outline will be provided of reasons why each contribution may have failed to persuade the other side of the debate. 2.1 The Law of Conservation of Generalization Performance The conservation law of generalization performance <ref> (Schaffer, 1994) </ref> proves that no learning bias can outperform any other bias over the space of all possible learning tasks 1 . It follows that if Occam's razor is a valuable learning bias, it can only be so for some subset of all possible learning tasks.
Reference: <author> Ting, K. M. </author> <year> (1994). </year> <title> The problem of small disjuncts: Its remedy in decision trees. </title> <booktitle> In Proceedings of the Tenth Canadian Conference on Artificial Intelligence, </booktitle> <pages> pp. 63-70. </pages> <publisher> Morgan Kaufmann,. </publisher>
Reference: <author> Wallace, C. S., & Boulton, D. M. </author> <year> (1968). </year> <title> An information measure for classification. </title> <journal> Com puter Journal, </journal> <volume> 11, </volume> <pages> 185-194. </pages>
Reference-contexts: Two key approaches have been developed, Minimum Message Length (MML) <ref> (Wallace & Boulton, 1968) </ref> and Minimum Description Length (MDL) (Rissanen, 1983). Both approaches admit to probabilistic interpretations. Given prior probabilities for both theories and data, minimization of the MML encoding closely approximates maximization of posterior probability (Wal-lace & Freeman, 1987).
Reference: <author> Wallace, C. S., & Freeman, P. R. </author> <year> (1987). </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society Series B, </journal> <volume> 49 (3), </volume> <pages> 240-265. </pages>
Reference: <author> Webb, G. I. </author> <year> (1994). </year> <title> Generality is more significant than complexity: Toward alternatives to Occam's razor. </title> <editor> In Zhang, C., Debenham, J., & Lukose, D. (Eds.), </editor> <booktitle> AI'94 Proceed- ings of the Seventh Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 60-67 Armidale. </address> <publisher> World Scientific. </publisher> <pages> 417 </pages>
References-found: 34

