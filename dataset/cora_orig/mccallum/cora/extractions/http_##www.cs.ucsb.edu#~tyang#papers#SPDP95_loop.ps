URL: http://www.cs.ucsb.edu/~tyang/papers/SPDP95_loop.ps
Refering-URL: http://www.cs.ucsb.edu/~tyang/papers/
Root-URL: http://www.cs.ucsb.edu
Email: ftyang, ibarrag@cs.ucsb.edu  
Title: On Symbolic Scheduling and Parallel Complexity of Loops  
Author: Tao Yang and Oscar H. Ibarra 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California,  
Abstract: In this paper, we first consider the symbolic scheduling and performance prediction of a partitioned single loop on message-passing architectures with nonzero communication and a sufficient number of processors. The loop body contains a set of coarse-grain tasks whose computational weights change during the course of the iterations. Using the macro-dataflow task model and software pipelining techniques, we develop an algorithm for computing a heuristic schedule, and provide analytic and experimental results on the correctness and asymptotic performance of this schedule. Using this result, we can show the impact of partitioning on the performance of parallelization. While this result is effective for a class of loops, there are many other methods proposed for loop parallelization. An interesting fundamental question is whether every instance of a nested loop can be efficiently executed. We present some positive and negative results on this issue. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aiken and A. Nicolau, </author> <title> Optimal Loop Parallelization, </title> <booktitle> SIG-PLAN 88 Conf. on Programming Language Design and Implementation. </booktitle> <address> pp.308-317. </address>
Reference-contexts: 1 Introduction The problem of loop parallelization has been addressed extensively in the literature <ref> [1, 14, 24] </ref>. There are many loop transformation and partitioning techniques proposed to uncover loop parallelism. Since many techniques are applicable to each given loop, a scheduling algorithm that predicts achievable performance is helpful in selecting transformation and also for producing efficient code. <p> There are many loop transformation and partitioning techniques proposed to uncover loop parallelism. Since many techniques are applicable to each given loop, a scheduling algorithm that predicts achievable performance is helpful in selecting transformation and also for producing efficient code. Scheduling techniques for fine-grain machines have been studied in <ref> [1, 12, 16, 25] </ref>. Computational weight information has been used for load balancing, but communication cost is usually not included because this is not needed for the architectures at which they are targeted. For message-passing architectures, communication could be asynchronous and message transmission delay affects the efficiency of computation. <p> We will provide an analysis of its asymptotic performance. Then we add more processors if necessary to improve the scheduling performance. Periodic scheduling used in software pipelining <ref> [1] </ref> executes tasks using a constant interval, i.e. if task T k i is executed at time t then task T k+1 i is executed at time t + c where c is a constant. This does not apply to our case because task weights change during iterations. <p> An explanation for this is that processors that execute the tasks T x with a x = a max are overloaded. Next we examine how to use more processors to improve the performance. 3.3 Adding more processors We use loop unrolling technique <ref> [1, 15] </ref> to increase the number of tasks within the loop body and use more processors to execute these tasks.
Reference: [2] <author> S. A. Cook. </author> <title> A taxonomy of problems with fast parallel algorithms. </title> <journal> Information and Control, </journal> <volume> 64 </volume> <pages> 2-22, </pages> <year> 1985. </year>
Reference-contexts: There is an alternative definition of N C using Boolean circuits. If N C i is the class of problems solvable by uniform Boolean circuits of polynomial size (= number of gates) and depth O (log i n), then N C = S i1 N C i <ref> [2, 23] </ref>. We state the results using the Boolean circuit model of parallel computation. The situation is analogous to that for sequential computation. The class of problems solvable by by Turing machines in polynomial time is called P .
Reference: [3] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. </author> <title> von Eicken LogP: Towards a Realistic Model of Parallel Computation, </title> <booktitle> Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993 </year>
Reference-contexts: In task computation model, exploring data locality means to localize data communication between items from a local memory or communication buffer is negligible compared to the cost of transferring a message from one processor to another. Inter-processor communication costs are modeled in <ref> [3] </ref>. We assume that physical processor distances does not significantly affect communication delay in a wormhole routing net work. [5, 18] discussed methods for estimating com munication and computation cost. We will discuss the assumptions on weight functions later.
Reference: [4] <author> Y. C. Chung and S. Ranka, </author> <title> Application and performance analysis of a compile-time optimization approach for list scheduling algorithms on distributed-memory multiprocessors, </title> <booktitle> Supercomputing'92, </booktitle> <pages> 512-521. </pages>
Reference-contexts: For message-passing architectures, communication could be asynchronous and message transmission delay affects the efficiency of computation. Coarse grain partitioning has been used for loop paralleliza-tion to avoid high communication startup cost. The previous work on task scheduling and performance prediction in <ref> [4, 7, 13] </ref> has considered communication overhead; however, these results are for directed acyclic task graphs. Techniques for generating coarse grain dependence graphs for loop programs are discussed in [5].
Reference: [5] <author> M. Cosnard and M. Loi, </author> <title> Automatic Task Graph Generation Techniques, </title> <booktitle> Proc. of the Hawaii International Conference on System Sciences, IEEE, </booktitle> <volume> Vol II. </volume> <year> 1995, </year> <pages> pp. 113-122. </pages>
Reference-contexts: The previous work on task scheduling and performance prediction in [4, 7, 13] has considered communication overhead; however, these results are for directed acyclic task graphs. Techniques for generating coarse grain dependence graphs for loop programs are discussed in <ref> [5] </ref>. The difficulty in providing a good loop schedule with accurate performance prediction is that loop bounds may be unknown, and coarse grain tasks may contain large chunks of computation and may involve loop indices and task weights may change during iterations. <p> Inter-processor communication costs are modeled in [3]. We assume that physical processor distances does not significantly affect communication delay in a wormhole routing net work. <ref> [5, 18] </ref> discussed methods for estimating com munication and computation cost. We will discuss the assumptions on weight functions later. Let ST (T k i ) be the starting time of task T k i in a schedule.
Reference: [6] <author> A. Darte , Y. Robert, </author> <title> Constructive methods for scheduling uniform loop nests. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> Aug. </month> <year> 1994, </year> <note> vol.5, (no.8):814-22. </note>
Reference: [7] <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acyclic Task Graphs, </title> <journal> IEEE Trans. on Parallel and Distributed Systems., </journal> <volume> Vol. 4, no. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp 686-701. </pages>
Reference-contexts: For message-passing architectures, communication could be asynchronous and message transmission delay affects the efficiency of computation. Coarse grain partitioning has been used for loop paralleliza-tion to avoid high communication startup cost. The previous work on task scheduling and performance prediction in <ref> [4, 7, 13] </ref> has considered communication overhead; however, these results are for directed acyclic task graphs. Techniques for generating coarse grain dependence graphs for loop programs are discussed in [5]. <p> A scheduling algorithm needs to consider symbolic information, weight variations and communication delay. In this paper, we demonstrate a solution for symbolic scheduling of a class of partitioned single loop programs by utilizing the ideas of the macro dataflow task model <ref> [7, 18] </ref> and pipelining techniques. We assume that weights of tasks in the loop body change monotonically during the course of iteration and there is a sufficient number of processors. <p> A definition and formal analysis on granularity of a DAG is given in <ref> [7] </ref> and we use a slightly different definition to quantify the granularity of a task dependence graph G as: g = min f min T y 2succ (T x ) a x g: Theorem 3.3 If a max and b max are bounded con stants, let Q (G) = max all
Reference: [8] <author> O. H. Ibarra and N. Q. Tran. </author> <title> On the parallel complexity of solving recurrence equations. </title> <booktitle> Proceedings of the 5th International Symposium on Algorithms and Computation, </booktitle> <volume> 834: </volume> <pages> 469-477, </pages> <year> 1994. </year>
Reference: [9] <author> J. Jaja. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1992. </year>
Reference: [10] <author> N. Jones and W. </author> <title> Lasser. Complete problems for deterministic polynomial time. </title> <journal> Theoretical Computer Science, </journal> <volume> 3(1) </volume> <pages> 105-117, </pages> <year> 1976. </year>
Reference: [11] <author> V. Kumar, A. Grama, A. Gupta, G. Karypis, </author> <title> Introduction to Parallel Computing, </title> <publisher> Benjamin/Cummings Pub, </publisher> <year> 1994. </year>
Reference-contexts: For the partitioning we use, the scheduling results in a good speedup with small memory consumption. The memory-efficient algorithms for matrix multiplication have been studied in the previous work (see <ref> [11] </ref>) and task communication is needed to transfer matrix elements in these algorithms. code method to embed a linear processor array in a hypercube. The task dependence graph for (a). The weight functions are monotonically increasing. We execute this graph on P processors.
Reference: [12] <author> M. Lam, </author> <title> Software pipelining: an effective scheduling technique for VLIW machines, </title> <booktitle> Proc. of ACM Conf. on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> 318-328. </pages>
Reference-contexts: There are many loop transformation and partitioning techniques proposed to uncover loop parallelism. Since many techniques are applicable to each given loop, a scheduling algorithm that predicts achievable performance is helpful in selecting transformation and also for producing efficient code. Scheduling techniques for fine-grain machines have been studied in <ref> [1, 12, 16, 25] </ref>. Computational weight information has been used for load balancing, but communication cost is usually not included because this is not needed for the architectures at which they are targeted. For message-passing architectures, communication could be asynchronous and message transmission delay affects the efficiency of computation.
Reference: [13] <author> S. S. Pande, D. P. Agrawal and J. Mauney, </author> <title> A scalable scheduling scheme for functional parallelism on distributed memory multiprocessor systems, </title> <type> Tech. Report, </type> <institution> North Carolina State Univ. </institution> <year> 1993. </year>
Reference-contexts: For message-passing architectures, communication could be asynchronous and message transmission delay affects the efficiency of computation. Coarse grain partitioning has been used for loop paralleliza-tion to avoid high communication startup cost. The previous work on task scheduling and performance prediction in <ref> [4, 7, 13] </ref> has considered communication overhead; however, these results are for directed acyclic task graphs. Techniques for generating coarse grain dependence graphs for loop programs are discussed in [5].
Reference: [14] <author> C. D. Polychronopoulos, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction The problem of loop parallelization has been addressed extensively in the literature <ref> [1, 14, 24] </ref>. There are many loop transformation and partitioning techniques proposed to uncover loop parallelism. Since many techniques are applicable to each given loop, a scheduling algorithm that predicts achievable performance is helpful in selecting transformation and also for producing efficient code. <p> another (i.e. a max is as small as possible, close to a avg ), and Q (G) is small, the scheduling performance will be reasonable. 5 The parallel complexity of loops As mentioned in Section 1, there are various efforts in developing techniques to parallelize loops for different cases, e.g. <ref> [14, 24] </ref>. These techniques including our result presented above preserve data dependence. There are more aggressive methods which break data dependence but still preserve the input-output semantics of a loop [17]. Run-time compilation techniques [17, 20] provide more avenues to parallelize loops which contain unknown information at compile-time.
Reference: [15] <author> K. K. Parhi and D. G. Messerschmitt, </author> <title> Static rate-optimal scheduling of iterative dataflow programs via optimum unfolding, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40:2, </volume> <year> 1991, </year> <pages> pp. 178-195. </pages>
Reference-contexts: An explanation for this is that processors that execute the tasks T x with a x = a max are overloaded. Next we examine how to use more processors to improve the performance. 3.3 Adding more processors We use loop unrolling technique <ref> [1, 15] </ref> to increase the number of tasks within the loop body and use more processors to execute these tasks.
Reference: [16] <author> J. Ramanujam, </author> <title> Optimal software pipelining of nested loops, </title> <booktitle> Proc. of 1994 IPPS, </booktitle> <pages> 335-342. </pages>
Reference-contexts: There are many loop transformation and partitioning techniques proposed to uncover loop parallelism. Since many techniques are applicable to each given loop, a scheduling algorithm that predicts achievable performance is helpful in selecting transformation and also for producing efficient code. Scheduling techniques for fine-grain machines have been studied in <ref> [1, 12, 16, 25] </ref>. Computational weight information has been used for load balancing, but communication cost is usually not included because this is not needed for the architectures at which they are targeted. For message-passing architectures, communication could be asynchronous and message transmission delay affects the efficiency of computation.
Reference: [17] <author> L. Rauchwerger and D. Padua, </author> <title> The privatizing DOALL test: A run-time technique for DOALL loop identification and array privatization, </title> <booktitle> Proc. of 1994 ACM Int. Conf. on Supercomputing, </booktitle> <pages> 33-43. </pages>
Reference-contexts: These techniques including our result presented above preserve data dependence. There are more aggressive methods which break data dependence but still preserve the input-output semantics of a loop <ref> [17] </ref>. Run-time compilation techniques [17, 20] provide more avenues to parallelize loops which contain unknown information at compile-time. We expect that compiling of loops will be still a major research focus since loops contain most parallelism in application programs and more sophisticated techniques will be developed. <p> These techniques including our result presented above preserve data dependence. There are more aggressive methods which break data dependence but still preserve the input-output semantics of a loop [17]. Run-time compilation techniques <ref> [17, 20] </ref> provide more avenues to parallelize loops which contain unknown information at compile-time. We expect that compiling of loops will be still a major research focus since loops contain most parallelism in application programs and more sophisticated techniques will be developed.
Reference: [18] <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: A scheduling algorithm needs to consider symbolic information, weight variations and communication delay. In this paper, we demonstrate a solution for symbolic scheduling of a class of partitioned single loop programs by utilizing the ideas of the macro dataflow task model <ref> [7, 18] </ref> and pipelining techniques. We assume that weights of tasks in the loop body change monotonically during the course of iteration and there is a sufficient number of processors. <p> Also the solution to this problem produces a number of task clusters which can be merged to a limited number of clusters for multi-threaded computation. The idea of such multi-stage scheduling approaches has been used in <ref> [18, 26] </ref>. Finding an optimal scheduling solution is NP-complete [18]; thus we will be comparing the performance of this heuristic schedule with the optimal solution obtained by searching the entire iteration space. <p> Also the solution to this problem produces a number of task clusters which can be merged to a limited number of clusters for multi-threaded computation. The idea of such multi-stage scheduling approaches has been used in [18, 26]. Finding an optimal scheduling solution is NP-complete <ref> [18] </ref>; thus we will be comparing the performance of this heuristic schedule with the optimal solution obtained by searching the entire iteration space. Using the result of performance analysis, we show how program partitioning and task granularity affect the performance of parallelization. <p> We use the macro-dataflow task model for computation execution <ref> [18] </ref>. A task receives all input before starting execution, executes to completion without interruption, and immediately sends the output to all successor tasks. Let the instance of task T i at iteration k be T k i (0 k N 1). <p> Inter-processor communication costs are modeled in [3]. We assume that physical processor distances does not significantly affect communication delay in a wormhole routing net work. <ref> [5, 18] </ref> discussed methods for estimating com munication and computation cost. We will discuss the assumptions on weight functions later. Let ST (T k i ) be the starting time of task T k i in a schedule.
Reference: [19] <author> V. Sarkar, </author> <title> Determining average program execution times and their variance, </title> <booktitle> Proc. of 1989 SIGPLAN, ACM, </booktitle> <pages> pp. 298-312. </pages>
Reference: [20] <author> S. D. Sharma, R. Ponnusamy, B. Moon, Y. Hwang, R. Das and J. Saltz, </author> <title> Run-time and Compile-time Support for Adaptive Irregular Problems, </title> <booktitle> SC '94, </booktitle> <pages> pp. 97-106. </pages> <publisher> IEEE. </publisher> <month> Nov, </month> <year> 1994. </year>
Reference-contexts: These techniques including our result presented above preserve data dependence. There are more aggressive methods which break data dependence but still preserve the input-output semantics of a loop [17]. Run-time compilation techniques <ref> [17, 20] </ref> provide more avenues to parallelize loops which contain unknown information at compile-time. We expect that compiling of loops will be still a major research focus since loops contain most parallelism in application programs and more sophisticated techniques will be developed.
Reference: [21] <author> J. Ullman and A. Van Gelder. </author> <title> Parallel complexity of logical query programs. </title> <journal> Algorthmica, </journal> <volume> 3 </volume> <pages> 5-42, </pages> <year> 1988. </year>
Reference: [22] <editor> J. Van Leeuwen and M. Nivat. </editor> <title> Efficient recognition of rational relations. </title> <journal> Information Processing Letter, </journal> <volume> 14 </volume> <pages> 34-38, </pages> <year> 1982. </year>
Reference-contexts: R (0; 0) := S; for j = 1 to n endfor endfor Example: Consider the string shu*ing problem <ref> [22] </ref>, which is defined as follows. Given an alphabet and three strings x; y; z 2 fl , where jxj = jyj and jzj = jxj + jyj, determine whether z is a shuffle of x and y.
Reference: [23] <author> Walter L. Ruzzo. </author> <title> On uniform circuit complexity. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 22 </volume> <pages> 365-383, </pages> <year> 1981. </year>
Reference-contexts: There is an alternative definition of N C using Boolean circuits. If N C i is the class of problems solvable by uniform Boolean circuits of polynomial size (= number of gates) and depth O (log i n), then N C = S i1 N C i <ref> [2, 23] </ref>. We state the results using the Boolean circuit model of parallel computation. The situation is analogous to that for sequential computation. The class of problems solvable by by Turing machines in polynomial time is called P .
Reference: [24] <author> M. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction The problem of loop parallelization has been addressed extensively in the literature <ref> [1, 14, 24] </ref>. There are many loop transformation and partitioning techniques proposed to uncover loop parallelism. Since many techniques are applicable to each given loop, a scheduling algorithm that predicts achievable performance is helpful in selecting transformation and also for producing efficient code. <p> We assume that the cost of fetching data 1 Here we consider true data dependence only. Output and anti-dependence can be removed by renaming techniques. Control dependence is not considered in this paper. 2 Some dependence graphs may contain imprecise dependence direction information <ref> [24] </ref> instead of precise distance values. Our result can be extended for this case and but will not be discussed here due to space constraint. 3 This is related to the concept of "exploiting data locality" used in the literature. <p> For this case, it is not easy to summarize inter-iteration dependence accurately. In fact, current dependence analysis algorithms <ref> [24] </ref> may overestimate dependence since accurate estimation may be intractable; however, overestimation does not affect the correctness of parallel execution. angular system. t x (k) = k + 1, t y (k) = 1:5k + 1 and t z (k) = 2k + 1. <p> another (i.e. a max is as small as possible, close to a avg ), and Q (G) is small, the scheduling performance will be reasonable. 5 The parallel complexity of loops As mentioned in Section 1, there are various efforts in developing techniques to parallelize loops for different cases, e.g. <ref> [14, 24] </ref>. These techniques including our result presented above preserve data dependence. There are more aggressive methods which break data dependence but still preserve the input-output semantics of a loop [17]. Run-time compilation techniques [17, 20] provide more avenues to parallelize loops which contain unknown information at compile-time.
Reference: [25] <author> A. Zaky and P. Sadayappan, </author> <title> Optimal static scheduling of sequential loops on multiprocessors. </title> <booktitle> Proc. of ICPP 1989, </booktitle> <volume> Vol 3, </volume> <pages> 130-137 </pages>
Reference-contexts: There are many loop transformation and partitioning techniques proposed to uncover loop parallelism. Since many techniques are applicable to each given loop, a scheduling algorithm that predicts achievable performance is helpful in selecting transformation and also for producing efficient code. Scheduling techniques for fine-grain machines have been studied in <ref> [1, 12, 16, 25] </ref>. Computational weight information has been used for load balancing, but communication cost is usually not included because this is not needed for the architectures at which they are targeted. For message-passing architectures, communication could be asynchronous and message transmission delay affects the efficiency of computation.
Reference: [26] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors, </title> <booktitle> Proc. of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <address> Wash-ington D.C., </address> <year> 1992, </year> <pages> pp. 428-437. </pages>
Reference-contexts: Also the solution to this problem produces a number of task clusters which can be merged to a limited number of clusters for multi-threaded computation. The idea of such multi-stage scheduling approaches has been used in <ref> [18, 26] </ref>. Finding an optimal scheduling solution is NP-complete [18]; thus we will be comparing the performance of this heuristic schedule with the optimal solution obtained by searching the entire iteration space.
Reference: [27] <author> T. Yang, C. Fu, A. Gerasoulis and V. Sarkar, </author> <title> Mapping iterative task graphs on distributed-memory machines. </title> <booktitle> To appear in Proc. of 24th Inter. Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Our future work is to investigate the techniques to execute tasks with more general types of weight functions on a limited number of processors. Some result to derive a schedule when the number of processors is limited, weights are constant but communication is nonzero, is in <ref> [27] </ref>. We have examined the general question of whether efficient loop parallelization is always possible by using any compilation technique. It indicates that it is unlikely that we can find a general technique that will work for all loops.
References-found: 27

