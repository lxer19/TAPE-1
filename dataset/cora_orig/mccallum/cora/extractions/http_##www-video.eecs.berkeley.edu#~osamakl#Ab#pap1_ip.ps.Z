URL: http://www-video.eecs.berkeley.edu/~osamakl/Ab/pap1_ip.ps.Z
Refering-URL: http://www-video.eecs.berkeley.edu/~osamakl/Ab/
Root-URL: 
Title: Lossy Compression of Noisy Images  
Author: Osama K. Al-Shaykh and Russell M. Mersereau Osama K. Al-Shaykh 
Address: Atlanta, GA 30332-0250  
Note: Corresponding author:  Submitted for consideration to the IEEE Transactions on Image Processing. IEEE IP EDICS number: IP 1.1 (as listed in the  This work was supported in part by the National Science Foundation under contract MIP-9205853.  
Affiliation: School of Electrical and Computer Engineering Georgia Institute of Technology  Address: School of Electrical and Computer Engineering Digital Signal Processing Laboratory Georgia Institute of Technology  
Email: email: osamakl@eedsp.gatech.edu  
Phone: Telephone: (404) 853-0160 Fax: (404) 894-8363  
Date: January 30, 1997  November 1995 T-IP).  
Abstract: Noise degrades the performance of any image compression algorithm. This paper studies the effect of noise on lossy image compression. The effect of Gaussian, Poisson, and film-grain noise on compression is studied. To reduce the effect of the noise on compression, the distortion is measured with respect to the original image not to the input of the coder. Results of noisy source coding are then used to design the optimal coder. In the minimum-mean-square-error (MMSE) sense, this is equivalent to an MMSE estimator followed by an MMSE coder. The coders for the Poisson noise and the film-grain noise cases are derived and their performance is studied. The effect of this preprocessing step is studied using standard coders, e.g., JPEG, also. As will be demonstrated, higher quality is achieved at lower bit rates. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. K. Jain, </author> <title> Fundamentals of Digital Image Processing. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction Image compression is concerned with minimizing the number of bits required to represent an image at a given level of fidelity <ref> [1] </ref>. Any image compression method attempts to answer two fundamental questions: what information should be stored or transmitted, and how should the chosen information be compressed [2]. The answers to both questions are related to the properties of the image and the requirements of the application.
Reference: [2] <author> T. Berger, </author> <title> Rate Distortion Theory. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall, </publisher> <year> 1971. </year>
Reference-contexts: 1 Introduction Image compression is concerned with minimizing the number of bits required to represent an image at a given level of fidelity [1]. Any image compression method attempts to answer two fundamental questions: what information should be stored or transmitted, and how should the chosen information be compressed <ref> [2] </ref>. The answers to both questions are related to the properties of the image and the requirements of the application. Images are, in many cases, degraded even before they are encoded. <p> The problem of noisy source coding has been extensively studied [2],[10]-[16]. Dobrushin and Tsybakov have shown that noisy source coding is equivalent to the classical source coding problem, where, the distortion measure, or fidelity criterion, is a weighted version of the original distortion measure <ref> [2, 10, 12] </ref>. Wolf and Ziv have proven that if the distortion measure is the mean square error (MSE), the noisy coding problem will reduce to two independent problems [11]. First, the signal should be optimally estimated in the MSE since. <p> Dobrushin and Tsybakov have shown that this problem is equivalent to the classical source coding problem. However the distortion measure is modified to depend only on the input of the encoder and the output of the decoder [10]. Berger <ref> [2] </ref> noticed that the modified distortion measure, ^ d (x; y) is the conditional average of the original distortion measure.
Reference: [3] <author> A. Macovski, </author> <title> Medical Imaging. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: Images are, in many cases, degraded even before they are encoded. For example, emission and transmission tomography images are usually corrupted by data-dependent noise which can be modeled as quantum noise or Poisson noise <ref> [3] </ref>. Generally, images formed at low light levels are also corrupted by this kind of noise [4]. Quantum noise is due to the quantization of energy into photons each having energy h-, where h is Planck's constant and is the frequency of the radiation [5].
Reference: [4] <author> M. Rabbani, </author> <title> "Bayesian filtering of Poisson noise using local statistics," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 36, </volume> <pages> pp. </pages> <address> 933 -937, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: For example, emission and transmission tomography images are usually corrupted by data-dependent noise which can be modeled as quantum noise or Poisson noise [3]. Generally, images formed at low light levels are also corrupted by this kind of noise <ref> [4] </ref>. Quantum noise is due to the quantization of energy into photons each having energy h-, where h is Planck's constant and is the frequency of the radiation [5].
Reference: [5] <author> H. H. Barrett and W. Swindell, </author> <title> Radiological Imaging, </title> <booktitle> vol. II. </booktitle> <address> New York, N.Y.: </address> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: Generally, images formed at low light levels are also corrupted by this kind of noise [4]. Quantum noise is due to the quantization of energy into photons each having energy h-, where h is Planck's constant and is the frequency of the radiation <ref> [5] </ref>. At low light levels, the number of photons recorded is small and the statistical variability of the number of counts about the mean limits the quality of the images. For emission and transmission tomography images this difficulty arises because the energy, h-, of each photon is large. <p> For emission and transmission tomography images this difficulty arises because the energy, h-, of each photon is large. This coupled with the necessity for low radiation dosage in human subjects results in fewer emitted photons and lower quality images <ref> [5] </ref>. Quantum noise is Poisson distributed.
Reference: [6] <author> R. P. Kleihorst, </author> <title> Noise Filtering of Image Sequences. </title> <type> Ph.D. Thesis: </type> <institution> Delft University of Technology, Delft, </institution> <address> The Netherlands, </address> <year> 1994. </year>
Reference-contexts: variable whose value is the photon count at a pixel which has a mean gray level of f , the conditional probability of x given f will be given by p (xjf ) = x! where is the proportionality factor relating the gray level value to the number of counts <ref> [6] </ref>. This probabilistic relation demonstrates the data-dependent nature of Poisson noise. Another example of signal dependent noise occurs when scanning images recorded on photographic films for storage and transmission. The resulting image is proportional to the film density [7].
Reference: [7] <author> A. M. Tekalp and G. Pavlovic, </author> <title> "Image restoration with multiplicative noise: incorporating the sensor nonlinearity," </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> vol. 39, </volume> <pages> pp. 2132-2136, </pages> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: This probabilistic relation demonstrates the data-dependent nature of Poisson noise. Another example of signal dependent noise occurs when scanning images recorded on photographic films for storage and transmission. The resulting image is proportional to the film density <ref> [7] </ref>.
Reference: [8] <author> D. T. Kuan, A. A. Sawchuk, T. C. Strand, and P. Chavel, </author> <title> "Adaptive noise smoothing filter for images with signal-dependent noise," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 7, </volume> <pages> pp. 165-177, </pages> <month> Mar. </month> <year> 1985. </year>
Reference-contexts: The resulting image is proportional to the film density [7]. If the film is processed in the linear region of the D-logE curve and ignoring the blurring effect of the model, the observed image, x (i; j), can be modeled by <ref> [8] </ref> x (i; j) = f (i; j) + ff [f (i; j)] fl n (i; j) (2) where f (i; j) is the original image density, ff is a proportionality constant, fl is a constant with a value between 1=3 and 1=2, and n (i; j) is white Gaussian noise
Reference: [9] <author> S. C. B. Lo, B. Krasner, and S. K. Mun, </author> <title> "Noise impact on error-free image compression," </title> <journal> IEEE Trans. Medical Imaging, </journal> <volume> vol. 9, </volume> <pages> pp. 202 - 206, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The study is conducted using both the JPEG lossy image compression standard and Lloyd-Max quantization. As will be demonstrated, noise degrades the performance of any image compression algorithm. This is because noise reduces interpixel correlation <ref> [9] </ref>. Moreover, in some cases, as will be shown shortly, the quality of the decompressed image might be closer to the original image than that of the input of the coder is. <p> This can also be seen in the reported results of Aberle et al.[21] where compressed images gave better diagnostic accuracy than noisy originals in terms of receiver-operating-characteristic (ROC) analysis for eleven out of fifteen reported experiments. Lo et al. <ref> [9] </ref> discuss the effect of noise on lossless medical image compression. They conclude that noise decreases the compression ratio. This is because the noise reduces interpixel correlation. Melnychuck et al. [22] discuss the effect of film-grain noise on DPCM coders.
Reference: [10] <author> R. L. Dobrushin and B. S. Tsybakov, </author> <title> "Information transmission with additional noise," </title> <journal> IRE Trans. Information Theory, </journal> <volume> vol. IT-8, </volume> <pages> pp. 293-304, </pages> <year> 1962. </year>
Reference-contexts: The problem of noisy source coding has been extensively studied [2],[10]-[16]. Dobrushin and Tsybakov have shown that noisy source coding is equivalent to the classical source coding problem, where, the distortion measure, or fidelity criterion, is a weighted version of the original distortion measure <ref> [2, 10, 12] </ref>. Wolf and Ziv have proven that if the distortion measure is the mean square error (MSE), the noisy coding problem will reduce to two independent problems [11]. First, the signal should be optimally estimated in the MSE since. <p> The problem of lossy compression of noisy images is a special case of the problem of noisy source coding as described in Figure 6 (b) <ref> [10, 11] </ref>, in that there is no receiver degradation block. This paper will assume no receiver degradation. Thus, it will assume that the transformation R (y; w) is the identity operator, i.e., y = w. <p> Dobrushin and Tsybakov have shown that this problem is equivalent to the classical source coding problem. However the distortion measure is modified to depend only on the input of the encoder and the output of the decoder <ref> [10] </ref>. Berger [2] noticed that the modified distortion measure, ^ d (x; y) is the conditional average of the original distortion measure.
Reference: [11] <author> J. K. Wolf and J. Ziv, </author> <title> "Transmission of noisy information to a noisy receiver with minimum distortion," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> vol. IT-16, </volume> <pages> pp. 406 - 411, </pages> <month> July </month> <year> 1970. </year>
Reference-contexts: Wolf and Ziv have proven that if the distortion measure is the mean square error (MSE), the noisy coding problem will reduce to two independent problems <ref> [11] </ref>. First, the signal should be optimally estimated in the MSE since. Then, the restored image is coded using classical source coding theorem results. Ephraim and Gray [13] extended these results for waveform and autoregressive model vector quantization. <p> The problem of lossy compression of noisy images is a special case of the problem of noisy source coding as described in Figure 6 (b) <ref> [10, 11] </ref>, in that there is no receiver degradation block. This paper will assume no receiver degradation. Thus, it will assume that the transformation R (y; w) is the identity operator, i.e., y = w. <p> p (x) f X p (f jx)d (f ; y) (3) where p (x) = f When the distortion is measured in terms of the mean squared error (MSE), d (f ; y) = E , the problem can be decomposed into an optimum estimator followed by an optimum coder <ref> [11] </ref>. Thus, the minimum distortion d fl is given by d fl = min E kf yk 2 6 = E kEff jxg xk 2 o g;h n kEff jxg yk 2 o This means that the distortion can never be less than E kEff jxg xk 2 .
Reference: [12] <author> H. S. Witsenhausen, </author> <title> "Indirect rate distortion problems," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> vol. IT-26, </volume> <pages> pp. 518 - 521, </pages> <month> Sept. </month> <year> 1980. </year> <month> 15 </month>
Reference-contexts: The problem of noisy source coding has been extensively studied [2],[10]-[16]. Dobrushin and Tsybakov have shown that noisy source coding is equivalent to the classical source coding problem, where, the distortion measure, or fidelity criterion, is a weighted version of the original distortion measure <ref> [2, 10, 12] </ref>. Wolf and Ziv have proven that if the distortion measure is the mean square error (MSE), the noisy coding problem will reduce to two independent problems [11]. First, the signal should be optimally estimated in the MSE since.
Reference: [13] <author> Y. Ephraim and R. M. Gray, </author> <title> "A unified approach for encoding clean and noisy sources by means of waveform and autoregressive model vector quantization," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> vol. 34, </volume> <pages> pp. 826 - 834, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: First, the signal should be optimally estimated in the MSE since. Then, the restored image is coded using classical source coding theorem results. Ephraim and Gray <ref> [13] </ref> extended these results for waveform and autoregressive model vector quantization. Fischer, Gibson and Koo [14] showed that the optimal MSE coder for noisy sources under an alphabet-constrained formulation still decomposes into the two problems as proven for a block coder by Wolf and Ziv.
Reference: [14] <author> T. R. Fischer, J. D. Gibson, and B. Koo, </author> <title> "Estimation and noisy source coding," </title> <journal> IEEE Trans. Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 38, </volume> <pages> pp. 23 - 34, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: First, the signal should be optimally estimated in the MSE since. Then, the restored image is coded using classical source coding theorem results. Ephraim and Gray [13] extended these results for waveform and autoregressive model vector quantization. Fischer, Gibson and Koo <ref> [14] </ref> showed that the optimal MSE coder for noisy sources under an alphabet-constrained formulation still decomposes into the two problems as proven for a block coder by Wolf and Ziv.
Reference: [15] <author> E. Ayanoglu, </author> <title> "On optimum quantization of noisy sources," </title> <journal> IEEE Trans. Information Theory, </journal> <volume> vol. IT-36, </volume> <pages> pp. 1450 - 1452, </pages> <month> Nov. </month> <year> 1990. </year>
Reference: [16] <author> J. D. Gibson and B. K. S. D. Gray, </author> <title> "Filtering of colored noise for speech enhancement and coding," </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> vol. 39, </volume> <pages> pp. </pages> <address> 1732 -1742, </address> <month> Aug. </month> <year> 1991. </year>
Reference: [17] <author> S. Geman and D. Geman, </author> <title> "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 6, </volume> <pages> pp. 721 - 741, </pages> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: Since the exact MMSE solution is computationally nonfeasible, as will be demonstrated, an approximation based on mean field theory (MFT) and a saddle point approximation will be developed for these cases. This approach has been extensively studied and proven useful for data-independent additive Gaussian noise <ref> [17, 18, 19] </ref>. 3 This paper is organized as follows: Section 2 discusses the effect of noise on image compression. Section 3 formulates the problem in terms of noisy source coding and reviews some of the noisy source coding results that are used in this paper. <p> Their generality for modeling and capacity to incorporate different constraints and characteristics of images, e.g., smooth regions, edges, and local dependence between neighboring pixels, have made them popular. They have been used to develop algorithms to deconvolve blur and filter white noise <ref> [17] </ref>. This section will briefly discuss MRFs. <p> j)jf (k; l); (k; l) 2 S M =(i; j)) = p (f (i; j)jf (k; l); (k; l) 2 N (i;j) ); (7) where S M =(i; j) is the set S M excluding the point (i; j) and N (i;j) is the set of neighbors of (i; j) <ref> [17, 18] </ref>. <p> The joint probability distribution of f is given by the Gibbs distribution <ref> [17] </ref> p (f ) = Z 1 exp [fiU (f )] (8) where Z is a normalization constant, also called the partition function Z = f and U (f ) is the energy function defined as U (f ) = c where V c (:) are the clique potentials. <p> The choice of the clique potentials determines the joint probability distribution, p (f ). The compound-Gaussian-Markov clique potential used in [24] increasingly penalizes the separation between neighboring pixels. However, the clique potential used by Geman and Geman <ref> [17] </ref> penalizes large separations equally, which results in sharper edges in the model at the expense of less smoothness.
Reference: [18] <author> J. Zhang, </author> <title> "The mean field theory in EM procedures for blind Markov random field image restoration," </title> <journal> IEEE Trans. Image Processing, </journal> <volume> vol. 2, </volume> <pages> pp. 27 - 40, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Since the exact MMSE solution is computationally nonfeasible, as will be demonstrated, an approximation based on mean field theory (MFT) and a saddle point approximation will be developed for these cases. This approach has been extensively studied and proven useful for data-independent additive Gaussian noise <ref> [17, 18, 19] </ref>. 3 This paper is organized as follows: Section 2 discusses the effect of noise on image compression. Section 3 formulates the problem in terms of noisy source coding and reviews some of the noisy source coding results that are used in this paper. <p> j)jf (k; l); (k; l) 2 S M =(i; j)) = p (f (i; j)jf (k; l); (k; l) 2 N (i;j) ); (7) where S M =(i; j) is the set S M excluding the point (i; j) and N (i;j) is the set of neighbors of (i; j) <ref> [17, 18] </ref>.
Reference: [19] <author> D. Geiger and F. Girosi, </author> <title> "Parallel and deterministic algorithms from MRF's: surface reconstruction," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 13, </volume> <pages> pp. 401 - 412, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Since the exact MMSE solution is computationally nonfeasible, as will be demonstrated, an approximation based on mean field theory (MFT) and a saddle point approximation will be developed for these cases. This approach has been extensively studied and proven useful for data-independent additive Gaussian noise <ref> [17, 18, 19] </ref>. 3 This paper is organized as follows: Section 2 discusses the effect of noise on image compression. Section 3 formulates the problem in terms of noisy source coding and reviews some of the noisy source coding results that are used in this paper. <p> Although the complexity of the problem is reduced, it still involves evaluating a summation over all possible realizations of f (i; j). Geiger and Girosi <ref> [19] </ref> proposed using the saddle point approximation. This method is based on neglecting the statistical fluctuations of the field and considering only the contribution of the maximum term of the partition function.
Reference: [20] <author> P. C. Cosman, R. M. Gray, and R. A. Olshen, </author> <title> "Evaluation quality of compressed medical images: SNR, subjective rating, and diagnostic accuracy," </title> <journal> Proc. IEEE, </journal> <volume> vol. 82, </volume> <pages> pp. 919 - 932, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Section 5 studies the two proposed approaches on synthetic and real images. Finally, Section 6 concludes the paper. 2 Effect of Noise on Compression Cosman et al. <ref> [20] </ref> notice, in their evaluation of the quality of compressed medical images, that slightly vector-quantized images are often superior to the originals because noise is suppressed by a clustering algorithm. The superiority is evident in both visual clarity and diagnostic accuracy.
Reference: [21] <author> D. R. Aberle, F. Gleeson, J. W. Sayre, K. Brown, P. Batra, D. A. Young, B. K. Stewart, B. K. T. Ho, and H. K. Huang, </author> <title> "The effect of irreversible image compression on diagnostic accuracy in thoracic imaging," </title> <journal> Optical Engineering, </journal> <volume> vol. 28, </volume> <pages> pp. 398 - 403, </pages> <month> May </month> <year> 1993. </year>
Reference: [22] <author> P. W. Melnychuck, M. J. Barry, and M. S. Mathieu, </author> <title> "The effect of noise and MTF on the compressibility of high resolution color images," </title> <booktitle> in Image Processing Algorithms and Techniques, </booktitle> <volume> vol. 1244, </volume> <pages> pp. 255 - 262, SPIE, </pages> <year> 1990. </year>
Reference-contexts: Lo et al. [9] discuss the effect of noise on lossless medical image compression. They conclude that noise decreases the compression ratio. This is because the noise reduces interpixel correlation. Melnychuck et al. <ref> [22] </ref> discuss the effect of film-grain noise on DPCM coders. They conclude that the bit rate of the noisy images is almost twice that of noise free images. As a remedy, they suggest ideal filtering of the noise, they did not base this suggestion on noisy source coding theory.
Reference: [23] <author> O. K. Al-Shaykh and R. M. Mersereau, </author> <title> "Restoration of lossy compressed noisy images," </title> <note> Submitted to IEEE Trans. Image Processing, Dec. 1995. 16 </note>
Reference-contexts: Based on this, at low bit rates, we propose coding the image using a conventional coder. Afterwards, the image is restored based on the available information about the image, the degradation process, and the artifacts introduced by the coder. This approach is discussed in <ref> [23] </ref>. 3 Noisy Source Coding The conventional problem of source coding or optimal quantization of a random signal assumes that the signal will not be corrupted before encoding or after decoding as implied by the communication channel given in Figure 6 (a).
Reference: [24] <author> T. Ozcelik, J. C. Brailean, and A. K. Katsaggelos, </author> <title> "Image and video compression algorithms based on recovery techniques using mean field annealing," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 83, </volume> <pages> pp. 304 - 316, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The choice of the clique potentials determines the joint probability distribution, p (f ). The compound-Gaussian-Markov clique potential used in <ref> [24] </ref> increasingly penalizes the separation between neighboring pixels. However, the clique potential used by Geman and Geman [17] penalizes large separations equally, which results in sharper edges in the model at the expense of less smoothness.
Reference: [25] <author> J. Zhang, </author> <title> "The mean field theory in EM procedures for Markov random fields," </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> vol. 40, </volume> <pages> pp. 2570 - 2583, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Eff (i; j)jxg = f As a result, computing Eff jxg is exponentially complex and computationally nonfeasible <ref> [25] </ref>. Zhang [25] used mean field theory to approximate the summation. Mean field theory approximates this summation by assuming that the influence of f (k; l), where (k; l) 6= (i; j), can be approximated by Eff (k; l)jxg. <p> Eff (i; j)jxg = f As a result, computing Eff jxg is exponentially complex and computationally nonfeasible <ref> [25] </ref>. Zhang [25] used mean field theory to approximate the summation. Mean field theory approximates this summation by assuming that the influence of f (k; l), where (k; l) 6= (i; j), can be approximated by Eff (k; l)jxg.
Reference: [26] <author> C. L. Chan, A. K. Katsaggelos, and A. V. Sahakian, </author> <title> "Image sequence filtering in quantum-limited noise with application to low-dose fluoroscopy," </title> <journal> IEEE Trans. Medical Imaging, </journal> <volume> vol. 12, </volume> <pages> pp. 610 - 621, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Thus, the problem reduces to solving fi fi @ U (i;j) (f (i; j)jx) fi fi = 0: (14) 4.3 Poisson noise case For images limited by counting statistics, the number of photons counted for each pixel is statistically independent of the counts for other pixels <ref> [26] </ref>.
Reference: [27] <author> L. A. Shepp and B. F. Logan, </author> <title> "The Fourier reconstruction of a head section," </title> <journal> IEEE Trans. Nuclear Science, </journal> <volume> vol. 21, no. 3, </volume> <pages> pp. 21-43, </pages> <year> 1974. </year>
Reference-contexts: These are standard test images in the discipline. The ellipses are as described in <ref> [27] </ref>. The intensities of the small ellipses of the phantom shown in Figure 8 (a) are in 25% and 75% increments instead of the 1% and 2% as described in [27]. This phantom will be referred to as "the modified 11 phantom" in the rest of the paper. <p> These are standard test images in the discipline. The ellipses are as described in <ref> [27] </ref>. The intensities of the small ellipses of the phantom shown in Figure 8 (a) are in 25% and 75% increments instead of the 1% and 2% as described in [27]. This phantom will be referred to as "the modified 11 phantom" in the rest of the paper. The phantom shown in Figure 9 (a) is the same as that described in [27] and will be referred to as "the original phantom" in the rest of the paper. <p> Figure 8 (a) are in 25% and 75% increments instead of the 1% and 2% as described in <ref> [27] </ref>. This phantom will be referred to as "the modified 11 phantom" in the rest of the paper. The phantom shown in Figure 9 (a) is the same as that described in [27] and will be referred to as "the original phantom" in the rest of the paper. The noisy source coding approach was also studied on the Lenna image, shown in Figure 1.
Reference: [28] <author> S. P. Lloyd, </author> <title> "Least squares optimization in PCM," </title> <journal> IEEE Trans. </journal> <note> Information Theory (reproduction of a paper presented at the Institute of Mathematical Statistics meeting in Atlantic City, </note> <author> N. J., </author> <month> September 10-13, </month> <year> 1957), </year> <title> vol. </title> <journal> IT-28, pp. </journal> <volume> 129 - 137, </volume> <month> Mar. </month> <year> 1982. </year>
Reference-contexts: The three original, three noisy, and three filtered images are then scalar-quantized using a Lloyd-Max quantizer which is an MMSE scalar quantizer <ref> [28, 29] </ref>. The rate distortion curves of the free of noise, noisy, and restored modified phantom are shown in Figure 8 (d). The corresponding curves for the original phantom and the Lenna image are shown in Figures 9 (d) and 10 (d), respectively.

References-found: 28

