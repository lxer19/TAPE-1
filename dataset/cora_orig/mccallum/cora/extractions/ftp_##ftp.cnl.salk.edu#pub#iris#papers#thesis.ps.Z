URL: ftp://ftp.cnl.salk.edu/pub/iris/papers/thesis.ps.Z
Refering-URL: http://www.cnl.salk.edu/cgi-bin/pub-search/
Root-URL: 
Title: VARIOUS ASPECTS OF ARTIFICIAL AND BIOLOGICAL NEURAL NETWORKS  
Author: Iris Ginzburg Beverly and Raymond Sackler 
Degree: Thesis submitted in partial fulfillment of the requirement for the Degree "Doctor of Philosophy" 1994 by  SUBMITTED TO THE SENATE OF TEL-AVIV UNIVERSITY  
Date: OCTOBER 1994  
Address: Tel-Aviv University, Tel-Aviv 69978, Israel  
Affiliation: School of Physics and Astronomy  Faculty of Exact Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Baum E. B. & Haussler D., </author> <year> 1989, </year> <title> "What size net gives valid generalization?" Neural Computation 1, </title> <type> 151-160. </type>
Reference: [2] <author> Geman, S., Bienenstock, E., & Doursat, R., </author> <year> 1992. </year> <title> "Neural networks and the bias/variance dilemma". </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-58. </pages>
Reference: [3] <author> Ginzburg, I. & Horn, D. </author> <year> 1992. </year> <title> "Learning the rule of a time series". </title> <journal> Int. Journal of Neural Systems 3, </journal> <pages> 167-177. </pages>
Reference-contexts: Chapter 2 Learning the Rule of a Time Series based on the paper "Learning the Rule of a Time Series" by I. Ginzburg & D. Horn, 1992 <ref> [3] </ref> 2.1 Introduction Using feed-forwards networks for predictions of time-series generally leads to good performance, as was first demonstrated by Weigend et. al. [12], as far as short-term predictions are concerned.
Reference: [4] <author> Ginzburg, I. & Horn, D. </author> <year> 1994. </year> <title> "Combined Neural Networks for Time-Series Analysis", </title> <note> to appear in Advances in Neural Information Processing systems edited by J. </note> <editor> D. Cowan, G. Tesauro, and J. Alspector, </editor> <publisher> (Morgan Kaufmann, </publisher> <year> 1994), </year> <note> Vol. 6. </note>
Reference-contexts: Chapter 3 Combined Neural Networks for Time Series Analysis Based on the paper "Combined Neural Networks for Time Series Analysis" by I. Ginzburg & D. Horn, 1994 <ref> [4] </ref> In this Chapter we propose a method for improving the performance of any network designed to predict the next value of a time series. We advocate analyzing the deviations of the network's predictions from the data in the training set.
Reference: [5] <author> Hu M. J. C. </author> <year> 1964. </year> <title> "Application of the Adaline System to Weather Forcasting" E. E. </title> <type> Degree Thesis. Technical Report 6775-1, </type> <institution> Stanford Electronic Laboratories, Stanford, </institution> <address> CA June. </address>
Reference: [6] <author> Lapedes, A. S., & Farber, R., </author> <year> 1987. </year> <title> "Nonlinear Signal processing using neural networks: Prediction and system modeling". </title> <institution> Los Alamos National Laboratory Technical Report. </institution> <note> 41 Bibiliography: Time-Series 42 </note>
Reference-contexts: The problem we choose to investigate is the chaotic quadratic map y n = y n1 (1 y n1 ) = 4 (1:1) whose successful implementation on neural networks has already been announced before <ref> [6] </ref> [7]. We analyze first the results of (4,6,1) networks composed of an m = 4 set of inputs, a hidden layer with 6 sigmoidal neurons, and a linear output neuron.
Reference: [7] <author> Moody,J. & Darken, C.J., </author> <year> 1989. </year> <title> "Fast learning in networks of locally-tuned processing units", </title> <journal> Neural Computation. </journal> <volume> 1, </volume> <pages> 281-294. </pages>
Reference-contexts: The problem we choose to investigate is the chaotic quadratic map y n = y n1 (1 y n1 ) = 4 (1:1) whose successful implementation on neural networks has already been announced before [6] <ref> [7] </ref>. We analyze first the results of (4,6,1) networks composed of an m = 4 set of inputs, a hidden layer with 6 sigmoidal neurons, and a linear output neuron.
Reference: [8] <author> Nowlan, S. J. & Hinton, G. E. </author> <year> 1992. </year> <title> "Simplifying neural networks by soft weight-sharing". </title> <journal> Neural Computation. </journal> <volume> 4, </volume> <pages> 473-493. </pages>
Reference-contexts: Using m = 12 and a weight-elimination method which led to d = 3, they obtained results which compare favorably with the leading statistical model by Tong and Lim [11]. Both models do well in predicting the next element of the sunspots series. Recently, Nowlan and Hinton <ref> [8] </ref> have shown that a significantly better network can be obtained if the training procedure includes a complexity penalty term in which the Combined Neural Networks for Time-Series Analysis 30 distribution of weights is modelled as a mixture of multiple gaussians whose parameters vary in an adaptive manner as the system
Reference: [9] <author> Rumelhart, D. E., & McClelland, J. L. </author> <title> and the PDP group, </title> <booktitle> 1986. Parallele Distributed Processing, Volume 1: Foundations. </booktitle> <publisher> MIT Press. </publisher>
Reference: [10] <author> Schwartz D. B., Samalam V. K., Solla S. A. and Denker J. S., </author> <year> 1990. </year> <title> "Exhaustive Learning", </title> <booktitle> Neural Computation 2, </booktitle> <pages> 374-385, </pages> <note> and references quoted therein. </note>
Reference: [11] <author> Tong, H., & Lim, K. S., </author> <year> 1980. </year> <title> "Threshold autoregression, limit cycles and cyclical data". </title> <journal> J. R. Stat. Soc. </journal> <volume> B 42, </volume> <pages> 245. </pages>
Reference-contexts: Using m = 12 and a weight-elimination method which led to d = 3, they obtained results which compare favorably with the leading statistical model by Tong and Lim <ref> [11] </ref>. Both models do well in predicting the next element of the sunspots series. <p> An alternative parameter is often used [12], in which the error is normalized by the standard deviation of the data. This leads to an average relative variance (arv) which is related to the average error through arv S = S S Following the statistical model <ref> [11] </ref> and Weigend et. al. [12] we choose m = 12 neurons in the first layer and kT k = 220 data points for the training set. The following kP k = 35 years are used for testing the predictions of our network.
Reference: [12] <author> Weigend, A. S., Huberman, B. A., & Rumelhart, D. E., </author> <year> 1990. </year> <title> "Predicting the Future: </title>
Reference-contexts: Ginzburg & D. Horn, 1992 [3] 2.1 Introduction Using feed-forwards networks for predictions of time-series generally leads to good performance, as was first demonstrated by Weigend et. al. <ref> [12] </ref>, as far as short-term predictions are concerned. However when reusing the output in consecutive time steps to generate long term predictions, the results deviate considerably from the function they should represent. The inability of a network to generate long term predictions can stem from three sources: 1. <p> Nonetheless, each variable can in principle be described by a lag-space representation of the type 1.1 . This is valid even if the y = y (t) solution is unpredictable as in chaotic phenomena. Weigend Huberman and Rumelhart <ref> [12] </ref> have studied the experimental series of yearly averages of sunspots activity using this approach. They have realized the lag-space representation on an (m; d; 1) network, where the notation implies a hidden layer of d sigmoidal neurons and one linear output. <p> Let us define the average error by * S = u t kSk n2S where the set S is either T or P . An alternative parameter is often used <ref> [12] </ref>, in which the error is normalized by the standard deviation of the data. This leads to an average relative variance (arv) which is related to the average error through arv S = S S Following the statistical model [11] and Weigend et. al. [12] we choose m = 12 neurons <p> An alternative parameter is often used <ref> [12] </ref>, in which the error is normalized by the standard deviation of the data. This leads to an average relative variance (arv) which is related to the average error through arv S = S S Following the statistical model [11] and Weigend et. al. [12] we choose m = 12 neurons in the first layer and kT k = 220 data points for the training set. The following kP k = 35 years are used for testing the predictions of our network. <p> The errors of the primary networks, in particular those of the prediction set * P , are quite higher than those quoted by Weigend et. al. <ref> [12] </ref> who started out from a (12,8,1) network and brought it down through a weight elimination technique to a (12,5,1) structure. They have obtained the values * T = 0:059 * P = 0:06. <p> We see that the errors of the primary networks are reduced by about 20%. The quality of these long term predictions is within the range of results presented by Weigend et. al. <ref> [12] </ref> Using the regression on (predicted) functional values, as in Eq. 3.14 , the results are improved by up to 15% as shown in Table 4. # * 2 * 0 5 * 11 * 0 1 0.118 0.098 0.162 0.109 0.150 0.116 3 0.117 0.099 0.164 0.112 0.136 0.099 5
References-found: 12

