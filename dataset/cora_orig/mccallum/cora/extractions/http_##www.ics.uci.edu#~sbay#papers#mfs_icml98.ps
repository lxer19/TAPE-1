URL: http://www.ics.uci.edu/~sbay/papers/mfs_icml98.ps
Refering-URL: http://www.ics.uci.edu/~sbay/
Root-URL: 
Email: sbay@ics.uci.edu  
Title: Combining Nearest Neighbor Classifiers Through Multiple Feature Subsets  
Author: Stephen D. Bay 
Address: Irvine, CA 92697, USA  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: Combining multiple classifiers is an effective technique for improving accuracy. There are many general combining algorithms, such as Bagging or Error Correcting Output Coding, that significantly improve classifiers like decision trees, rule learners, or neural networks. Unfortunately, many combining methods do not improve the nearest neighbor classifier. In this paper, we present MFS, a combining algorithm designed to improve the accuracy of the nearest neighbor (NN) classifier. MFS combines multiple NN classifiers each using only a random subset of features. The experimental results are encouraging: On 25 datasets from the UCI Repository, MFS significantly improved upon the NN, k nearest neighbor (kNN), and NN classifiers with forward and backward selection of features. MFS was also robust to corruption by irrelevant features compared to the kNN classifier. Finally, we show that MFS is able to reduce both bias and variance components of error.
Abstract-found: 1
Intro-found: 1
Reference: <author> D. W. Aha and R. L. Bankert. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <booktitle> In Proceedings of the AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <pages> pages 106-112. </pages>
Reference-contexts: We compared these to four other algorithms: nearest neighbor (NN), k nearest neighbor (kNN), nearest neighbor with forward (FSS) and backward (BSS) se quential selection of features <ref> (Aha and Bankert, 1994) </ref>. The use of FSS and BSS should provide an interesting contrast with MFS. FSS and BSS try to find a single good subset of features, while MFS uses multiple random subsets without regard to their performance.
Reference: <author> K. M. Ali and M. J. Pazzani. </author> <year> (1996). </year> <title> Error reduc-tion through learning multiple descriptions. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 173-202. </pages>
Reference: <author> E. Alpaydin. </author> <year> (1997). </year> <title> Voting over multiple condensed nearest neighbors. </title> <journal> Artificial Intelligence Review, 11(1-5):115-132. </journal>
Reference: <author> S. D. Bay. </author> <year> (1997). </year> <title> Nearest neighbour classification from multiple data representations. </title> <type> Master's thesis, </type> <institution> University of Waterloo, Department of Systems Design Engineering. </institution>
Reference-contexts: For MFS, we use n NN classifiers, so its complexity is O (nef ). For training, we use cross-validation and MFS requires O (ne 2 f v) time, where v is the number of folds <ref> (Bay, 1997) </ref>. This analysis shows how the computational requirements of MFS change as a function of the number of examples and features. However, it does not give any indication of actual running times on real datasets. <p> We chose boolean irrelevant features because they are more difficult for nearest neighbor methods to handle than continuous irrelevant features. This is because while they both have the same range and mean, boolean variables have greater variance. Table 3 shows the results for several domains. The remaining results <ref> (Bay, 1997) </ref> are not shown here for space reasons, but they follow a similar pattern. As expected, irrelevant features always hurt both kNN and MFS to some degree. However, the results are surprising because they reveal that on some domains kNN is critically sensitive while MFS is stable.
Reference: <author> L. Breiman. </author> <year> (1996). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 123-140. </pages>
Reference-contexts: One method of generating a diverse ensemble of clas-sifiers is to perturb some aspect of the training inputs for which the classifier is unstable. For example, Bagging <ref> (Breiman, 1996) </ref> perturbs the training patterns available to each classifier in the ensemble. Since decision trees are unstable to the patterns, Bagging generates a diverse and effective ensemble. Nearest neighbor classifiers are stable to the patterns, so Bagging generates poor NN ensembles.
Reference: <author> L. Breiman. </author> <year> (1996b). </year> <title> Bias, variance, and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> Statistics Department, University of California, Berkeley. </institution>
Reference: <author> K. J. Cherkauer. </author> <year> (1996). </year> <title> Human expert-level performance on a scientific image analysis task by a system using combined artificial neural networks. </title> <editor> In P. Chan, editor, </editor> <booktitle> Working Notes of the AAAI Workshop on Integrating Multiple Learned Models, </booktitle> <pages> pages 15-21. </pages> <note> Available from http://www.cs.fit.edu/~imlm. </note>
Reference: <author> T. M. Cover and P. E. Hart. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13(1) </volume> <pages> 21-27. </pages>
Reference-contexts: By using the nearest neighbor classifier in the MFS scheme we lose its asymptotic optimality properties. Specifically, as the number of training examples approaches infinity the NN classifier is bounded by twice the Bayes error rate <ref> (Cover, 1967) </ref>. The kNN classifier is Bayes optimal in the limit with proper choice of k (Fix and Hodges, 1951).
Reference: <author> B. V. Dasarathy. </author> <year> (1991). </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: Since its inception by Fix and Hodge (1951), researchers have investigated many methods for improving the NN classifier, but most work has concentrated on changing the distance metric or manipulating the patterns in the training set <ref> (Dasarathy, 1991) </ref>. Recently, researchers have begun experimenting with general algorithms for improving classification accuracy by combining multiple versions of a single classifier, also known as a multiple model or ensemble approach.
Reference: <author> R. O. Duda and P. E. Hart. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference: <author> R. A. Fisher. </author> <year> (1936). </year> <title> The use of multiple measurements in taxonomic problems. </title> <booktitle> Annual Eugenics, </booktitle> <volume> 7 </volume> <pages> 179-188. </pages>
Reference: <author> E. Fix and J. L. Hodges. </author> <year> (1951). </year> <title> Discriminatory analysis: Nonparametric discrimination: Consistency properties. </title> <type> Technical Report Project 21-49-004, Report Number 4, </type> <institution> USAF School of Aviation Medicine, Randolf Field, Texas. </institution>
Reference-contexts: Specifically, as the number of training examples approaches infinity the NN classifier is bounded by twice the Bayes error rate (Cover, 1967). The kNN classifier is Bayes optimal in the limit with proper choice of k <ref> (Fix and Hodges, 1951) </ref>. We can make no such claims about MFS. 2.2 PARAMETER SELECTION The MFS algorithm has two parameter values that need to be set: the size of the feature subsets, and the number of classifiers to combine.
Reference: <author> L. K. Hansen and P. Salamon. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 993-1001. </pages>
Reference: <author> P. E. Hart. </author> <year> (1968). </year> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14 </volume> <pages> 515-516. </pages>
Reference-contexts: We are only aware of Skalak's (1996) work on combining NN classifiers with small prototype sets, Alpaydin's (1997) work with condensed nearest neighbor (CNN) classifiers <ref> (Hart, 1968) </ref>, and Ricci and Aha's (1998) work on combining NN, feature selection, and ECOC. Skalak and Alpaydin approach the problem of combining NN classifiers similarly. They drastically reduce the size of each classifier's prototype set to destabilize the NN classifier.
Reference: <author> G. James and T. Hastie. </author> <year> (1997). </year> <title> Generalizations of the bias/variance decomposition for prediction error. </title> <address> http://stat.stanford.edu/~gareth. </address>
Reference: <author> R. Kohavi and G. H. John. </author> <year> (1996). </year> <title> Wrappers for feature subset selection. </title> <journal> Artificial Intelligence, 97(1-2):273-324. </journal>
Reference-contexts: For the kNN classifier, the value of k was set using cross-validation performance estimates on the training set. For feature selection, we used cross-validation accuracy on the training set for our objective function (also known as a wrapper approach <ref> (Kohavi and John, 1996) </ref>). We evaluated the algorithms on twenty-five datasets from the UCI Repository of Machine Learning Databases (Merz and Murphy, 1998).
Reference: <author> R. Kohavi and D. H. Wolpert. </author> <year> (1996). </year> <title> Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle>
Reference-contexts: For the kNN classifier, the value of k was set using cross-validation performance estimates on the training set. For feature selection, we used cross-validation accuracy on the training set for our objective function (also known as a wrapper approach <ref> (Kohavi and John, 1996) </ref>). We evaluated the algorithms on twenty-five datasets from the UCI Repository of Machine Learning Databases (Merz and Murphy, 1998).
Reference: <author> E. B. Kong and T. G. Dietterich. </author> <year> (1996). </year> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 725-730. </pages>
Reference: <author> P. Langley and W. Iba. </author> <year> (1993). </year> <title> Average-case analysis of a nearest neighbor algorithm. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 889-894. </pages>
Reference: <author> C. J. Merz and P. M. Murphy. </author> <year> (1998). </year> <title> UCI repository of machine learning databases. </title> <institution> University of Califor-nia, Irvine, Dept. of Information and Computer Science. </institution> <note> http://www.ics.uci.edu/~mlearn/. </note>
Reference-contexts: For feature selection, we used cross-validation accuracy on the training set for our objective function (also known as a wrapper approach (Kohavi and John, 1996)). We evaluated the algorithms on twenty-five datasets from the UCI Repository of Machine Learning Databases <ref> (Merz and Murphy, 1998) </ref>. We first normalized the datasets so that continuous features ranged from [0; 1], and then we ran thirty trials where the training set contained 2/3 of the patterns (randomly selected) and the test set contained the remaining 1/3. There were a few exceptions to this procedure.
Reference: <author> J. R. Quinlan. </author> <year> (1996). </year> <title> Bagging, Boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 725-730. </pages>
Reference-contexts: There were a few exceptions to this procedure. For Waveform, we used 300 training cases and 4700 test cases to maintain consistency with reported results <ref> (Quinlan, 1996) </ref>. For Satimage, we used the original division into a training and test set, so the results represent one run of each algorithm.
Reference: <author> F. Ricci and D. W. Aha. </author> <year> (1998). </year> <title> Error-correcting output codes for local learners. </title> <booktitle> In Proceedings of the 10th European Conference on Machine Learning. </booktitle>
Reference: <author> R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. </author> <year> (1997). </year> <title> Boosting the margin: A new explanation for the effectiveness of voting methods. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference. </booktitle>
Reference: <author> D. B. Skalak. </author> <year> (1996). </year> <title> Prototype Selection for Composite Nearest Neighbor Classifiers. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Mas-sachusetts. </institution>
Reference: <author> R. Tibshirani. </author> <year> (1996). </year> <title> Bias, variance and prediction error for classification rules. </title> <type> Technical report, </type> <institution> Department of Statistics, University of Toronto. </institution>
Reference: <author> K. Tumer and J. Ghosh. </author> <year> (1996). </year> <title> Error correlation and error reduction in ensemble classifiers. </title> <journal> Connection Science, </journal> <volume> 8 </volume> <pages> 385-404. </pages> <booktitle> Special issue on combining artificial neural networks: ensemble approaches. </booktitle>
References-found: 26

