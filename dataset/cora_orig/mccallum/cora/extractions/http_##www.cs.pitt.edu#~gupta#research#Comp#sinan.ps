URL: http://www.cs.pitt.edu/~gupta/research/Comp/sinan.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/arch.html
Root-URL: 
Email: fsoner,guptag@cs.pitt.edu  
Title: SINAN: An Argument Forwarding Multithreaded Architecture  
Author: Soner Onder and Rajiv Gupta 
Address: Pittsburgh Pittsburgh PA 15260  
Affiliation: Department of Computer Science University of  
Abstract: The direct execution of data flow graphs by data flow machines exposes the maximum amount of parallelism in a computation. However, it also results in undesirable properties such as: high overhead due to data replication required to implement high fanouts in the data flow graph; poor instruction and data locality since the scheduling of ready instructions is not sensitive to data locality; and less efficient execution of sequentially dependent code sections and vector operations due the inability of the data flow machines to support deep pipelines. SINAN offers a novel approach called argument forwarding that can eliminate much of the data fanout overhead and provides greatly improved locality. We show that vector operations and sequential segments can be efficiently handled with the dataflow paradigm by a novel approach that dynamically forms the activity templates. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. A. Adams, </author> <title> "A model for parallel computations," In L.C. </title> <editor> Hobbs et al., editor, </editor> <booktitle> Parallel Processor Systems, Technologies, and Applications, </booktitle> <pages> pages 311-333. </pages> <publisher> Spartan, </publisher> <year> 1970. </year>
Reference-contexts: 1 Introduction Since the introduction of the data flow concept in the late 1960s by Adams <ref> [1] </ref> the concept has evolved into many forms. The resulting architectures can be classified into two broad categories, namely, pure dataflow architectures and dataflow based mul-tithreaded architectures.
Reference: [2] <author> M. L. Anido, D. J. Allerton, and E. J. Zaluska, </author> <title> "A three-port/three-access register file for concurrent processing and I/O communication in a RISC-like graphics engine," </title> <booktitle> Proc. 16th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 354-361, </pages> <year> 1989. </year>
Reference-contexts: loop n times, and assume that the values of the vector a has been computed and forwarded to the data segment containing the vector loop, we obtain the set of activity templates shown in Fig. 3 (b). (a) for (i=1; i &lt; n; i++) a [i]=a [i]+c add ! a <ref> [2] </ref> c add ! a [n] c. In the unrolled example, one of the source operands remains the same, and the same instruction is repeatedly used. Although the instructions have different destination addresses, each instruction's result is separated from the preceding one by a constant stride. <p> An implementation of a register file that allows storing of instructions and data directly to the registers is given by Anido et al. in <ref> [2] </ref>. In fact, these decisions are consistent with the practice of many vector supercomputers which also do not employ caches but rely on fast register files and interleaved accesses to multiple banks of memory.
Reference: [3] <author> Arvind and K. P. Gostelow, </author> <title> "Some relationships between asynchronous interpreters of a dataflow language," </title> <booktitle> Proc. IFIP WG2.2 Conf. on Formal Description of Programming Concepts, </booktitle> <year> 1978. </year>
Reference-contexts: The resulting architectures can be classified into two broad categories, namely, pure dataflow architectures and dataflow based mul-tithreaded architectures. Most of the latter architectures are dataflow Von Neumann hybrids while the former are descendants of either static dataflow [7] or dynamic dataflow <ref> [3, 4] </ref> architectures. Both static and dynamic dataflow machines have similar implementation and efficiency problems which are discussed below. First, a dataflow node may generate a value which is needed in multiple destinations [8].
Reference: [4] <author> Arvind, V. Kathail, and K. Pingali, </author> <title> "A dataflow architecture with tagged tokens," </title> <type> Technical Report 174, </type> <institution> MIT, Lab for Computer Science, </institution> <year> 1980. </year>
Reference-contexts: The resulting architectures can be classified into two broad categories, namely, pure dataflow architectures and dataflow based mul-tithreaded architectures. Most of the latter architectures are dataflow Von Neumann hybrids while the former are descendants of either static dataflow [7] or dynamic dataflow <ref> [3, 4] </ref> architectures. Both static and dynamic dataflow machines have similar implementation and efficiency problems which are discussed below. First, a dataflow node may generate a value which is needed in multiple destinations [8].
Reference: [5] <author> D. E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek, </author> <title> "Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine," </title> <booktitle> Proc. 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <year> 1991. </year>
Reference-contexts: This kind of approach has a number of advantages over existing ones. Partitioning the program into threads by observing the control dependences enable us to exploit coarse grain parallelism <ref> [5] </ref>. <p> However, we believe SINAN is one of the first architecture to show the close relationship between the way flow of data is handled, locality and vector operations. We share the ideas with the TAM approach <ref> [5] </ref> and the Monsoon [17] in that, effective solutions can only be found if a good model of execution is superimposed on the hardware and software design.
Reference: [6] <author> J. B. Dennis and G. R. Gao, </author> <title> "An efficient pipelined dataflow processor architecture," </title> <booktitle> Proc. IEEE/ACMConf. on Supercomputing, </booktitle> <year> 1988. </year>
Reference-contexts: Multithreaded machines aim at solving the above problems by handling dataflow at multiple levels [14, 9], or switching between the Von Neumann and dataflow style of execution [10]. A dataflow hybrid machine designed by Dennis and Gao <ref> [6, 11] </ref> proposes argument fetching as a solution to the fanout and excessive token circulation problems. In this solution, the availability of data values is signalled by using a separate signal graph, while instructions fetch operands from the memory and store results into the memory like a Von Neumann machine.
Reference: [7] <author> J. B. Dennis and D. P. Misunas, </author> <title> "A preliminary architecture for a basic data flow computer," </title> <booktitle> Proc. 2nd Annual Symp. on Computer Architecture, </booktitle> <year> 1975. </year>
Reference-contexts: The resulting architectures can be classified into two broad categories, namely, pure dataflow architectures and dataflow based mul-tithreaded architectures. Most of the latter architectures are dataflow Von Neumann hybrids while the former are descendants of either static dataflow <ref> [7] </ref> or dynamic dataflow [3, 4] architectures. Both static and dynamic dataflow machines have similar implementation and efficiency problems which are discussed below. First, a dataflow node may generate a value which is needed in multiple destinations [8]. <p> The number fl Supported in part by the National Science Foundation Presidential Young Investigator Award CCR-9157371, Hewlett-Packard Labs, and Intel Corporation to the Univ. of Pittsburgh. of destinations is defined to be the fanout of an instruction. Some implementations allow an arbitrary number of destinations <ref> [7] </ref> which result in long running instructions that require cycles proportional to the fanout. On the other hand, some implementations allow only a limited fanout, and introduce explicit forwarding instructions to handle any additional destinations [18]. Second, direct execution of dataflow graphs may lead to poor locality.
Reference: [8] <author> J. B. Dennis, </author> <title> "The evolution of "static" data-flow architecture," </title> <editor> In Jean-Luc Gaudiot and Lubomir Bic, editors, </editor> <booktitle> Advanced Topics in Data-Flow Computing, </booktitle> <pages> pages 35-91. </pages> <address> New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Both static and dynamic dataflow machines have similar implementation and efficiency problems which are discussed below. First, a dataflow node may generate a value which is needed in multiple destinations <ref> [8] </ref>. The number fl Supported in part by the National Science Foundation Presidential Young Investigator Award CCR-9157371, Hewlett-Packard Labs, and Intel Corporation to the Univ. of Pittsburgh. of destinations is defined to be the fanout of an instruction. <p> As shown by Dennis <ref> [8] </ref>, static and dynamic dataflow models are not that different. The difference between the models lies mainly in the way frame allocation is handled. Although Sinan is based on the static dataflow concept in the way the activation templates are formed, activation templates can also be formed dynamically.
Reference: [9] <author> P. Evripidou and J-L. Gaudiot, </author> <title> "The USC decoupled multilevel data-flow execution model," </title> <editor> In Jean-Luc Gaudiot and Lubomir Bic, editors, </editor> <booktitle> Advanced Topics in Data-Flow Computing, </booktitle> <pages> pages 347-379. </pages> <address> New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: On the other hand, incorporation of vector operations into dataflow graphs is non-trivial. Multithreaded machines aim at solving the above problems by handling dataflow at multiple levels <ref> [14, 9] </ref>, or switching between the Von Neumann and dataflow style of execution [10]. A dataflow hybrid machine designed by Dennis and Gao [6, 11] proposes argument fetching as a solution to the fanout and excessive token circulation problems.
Reference: [10] <author> G. R. Gao, </author> <title> "A flexible architecture model for hybrid data-flow and control-flow evaluation," </title> <editor> In Jean-Luc Gaudiot and Lubomir Bic, editors, </editor> <booktitle> Advanced Topics in Data-Flow Computing, </booktitle> <pages> pages 327-346. </pages> <address> New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: On the other hand, incorporation of vector operations into dataflow graphs is non-trivial. Multithreaded machines aim at solving the above problems by handling dataflow at multiple levels [14, 9], or switching between the Von Neumann and dataflow style of execution <ref> [10] </ref>. A dataflow hybrid machine designed by Dennis and Gao [6, 11] proposes argument fetching as a solution to the fanout and excessive token circulation problems.
Reference: [11] <author> G. R. Gao, R. Tio, and H. H. J. Hum, </author> <title> "Design of an efficient dataflow architecture without data flow," </title> <type> Technical Report TR-SOCS-88.14, </type> <institution> School of Computer Science, McGill University, </institution> <year> 1988. </year>
Reference-contexts: Multithreaded machines aim at solving the above problems by handling dataflow at multiple levels [14, 9], or switching between the Von Neumann and dataflow style of execution [10]. A dataflow hybrid machine designed by Dennis and Gao <ref> [6, 11] </ref> proposes argument fetching as a solution to the fanout and excessive token circulation problems. In this solution, the availability of data values is signalled by using a separate signal graph, while instructions fetch operands from the memory and store results into the memory like a Von Neumann machine.
Reference: [12] <author> V.G. Grafe, G.S. Davidson, J.E. Hoch, and V.P. Holmes, </author> <title> "The *psilon dataflow processor," </title> <booktitle> Proc. 16th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 36-45, </pages> <year> 1989. </year>
Reference-contexts: In addition, frame allocation is done dynamically as it is the case with dynamic dataflow machines. In this respect, it is also a hybrid of static and dynamic dataflow machines. The design of SINAN has been inspired by many other processors <ref> [13, 12, 17, 16, 15] </ref>, and is primarily based upon concepts of data-flow computing. However, we believe SINAN is one of the first architecture to show the close relationship between the way flow of data is handled, locality and vector operations.
Reference: [13] <editor> R.A. Iannucci, </editor> <title> "Toward a dataflow/von Neumann hybrid architecture," </title> <booktitle> Proc. 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 131-140, </pages> <year> 1988. </year>
Reference-contexts: As shown in Fig. 5, the SINAN architecture is modeled as an array of identical processors connected through an interconnection network similar to Ian-nucci's machine <ref> [13] </ref>. However, each node of SINAN contains multiple processing elements called register units which contain one such fast store and communicate over a local bus with the thread manager and local memory. SINAN uses a uniform, descriptor based addressing scheme. <p> In addition, frame allocation is done dynamically as it is the case with dynamic dataflow machines. In this respect, it is also a hybrid of static and dynamic dataflow machines. The design of SINAN has been inspired by many other processors <ref> [13, 12, 17, 16, 15] </ref>, and is primarily based upon concepts of data-flow computing. However, we believe SINAN is one of the first architecture to show the close relationship between the way flow of data is handled, locality and vector operations.
Reference: [14] <author> W. Najjar and J-L. Gaudiot, </author> <title> "Macro data-flow architecture," In J.A. Sharp, editor, Data Flow Computing: </title> <journal> Theory and Practice, </journal> <pages> pages 272-291. </pages> <publisher> Ablex Publishing Corporation, </publisher> <year> 1992. </year>
Reference-contexts: On the other hand, incorporation of vector operations into dataflow graphs is non-trivial. Multithreaded machines aim at solving the above problems by handling dataflow at multiple levels <ref> [14, 9] </ref>, or switching between the Von Neumann and dataflow style of execution [10]. A dataflow hybrid machine designed by Dennis and Gao [6, 11] proposes argument fetching as a solution to the fanout and excessive token circulation problems.
Reference: [15] <author> R. Nikhil and Arvind, </author> <title> "Can dataflow subsume von neumann computing?," </title> <booktitle> Proc. 16th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 262-272, </pages> <year> 1989. </year>
Reference-contexts: In addition, frame allocation is done dynamically as it is the case with dynamic dataflow machines. In this respect, it is also a hybrid of static and dynamic dataflow machines. The design of SINAN has been inspired by many other processors <ref> [13, 12, 17, 16, 15] </ref>, and is primarily based upon concepts of data-flow computing. However, we believe SINAN is one of the first architecture to show the close relationship between the way flow of data is handled, locality and vector operations.
Reference: [16] <author> R. S. Nikhil, G.M. Papadopoulos, and Arvind, </author> <title> "*T:a multithreaded massively parallel architecture," </title> <booktitle> Proc. 19th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <year> 1992. </year>
Reference-contexts: The units contained in a node operate asynchronously and communicate with each other by message passing. These messages perform activities such as acquiring a register unit, loading a thread, performing loads and stores, initiating thread execution and freeing of register units. As it is pointed out in <ref> [16] </ref>, a direct consequence of message passing style communication is that transactions must be split phase transactions, i.e., a transaction is split into request and response messages. SINAN's split phase transactions involve simultaneous transmission of an opcode and two descriptor words. <p> In addition, frame allocation is done dynamically as it is the case with dynamic dataflow machines. In this respect, it is also a hybrid of static and dynamic dataflow machines. The design of SINAN has been inspired by many other processors <ref> [13, 12, 17, 16, 15] </ref>, and is primarily based upon concepts of data-flow computing. However, we believe SINAN is one of the first architecture to show the close relationship between the way flow of data is handled, locality and vector operations.
Reference: [17] <author> G.M. Papadopoulos and D.E. Culler, "Monsoon: </author> <title> An explicit token-store architecture," </title> <booktitle> Proc. 17th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 82-91, </pages> <year> 1990. </year>
Reference-contexts: In addition, frame allocation is done dynamically as it is the case with dynamic dataflow machines. In this respect, it is also a hybrid of static and dynamic dataflow machines. The design of SINAN has been inspired by many other processors <ref> [13, 12, 17, 16, 15] </ref>, and is primarily based upon concepts of data-flow computing. However, we believe SINAN is one of the first architecture to show the close relationship between the way flow of data is handled, locality and vector operations. <p> However, we believe SINAN is one of the first architecture to show the close relationship between the way flow of data is handled, locality and vector operations. We share the ideas with the TAM approach [5] and the Monsoon <ref> [17] </ref> in that, effective solutions can only be found if a good model of execution is superimposed on the hardware and software design.
Reference: [18] <author> G. M. Papadopoulos, </author> <title> "Implementation of a general purpose dataflow multiprocessor," </title> <type> Technical Report TR-432, </type> <institution> Laboratory for Computer Science, Mas-sachusetts Institute of Technology, </institution> <year> 1988. </year>
Reference-contexts: Some implementations allow an arbitrary number of destinations [7] which result in long running instructions that require cycles proportional to the fanout. On the other hand, some implementations allow only a limited fanout, and introduce explicit forwarding instructions to handle any additional destinations <ref> [18] </ref>. Second, direct execution of dataflow graphs may lead to poor locality. Poor locality makes the use of caches less effective, and may contribute to the inefficiency of the execution. Finally, well structured program constructs such as vector operations and sequential segments may be executed more efficiently by conventional processors.
Reference: [19] <author> M. Sato, Y. Kodama, S. Sakai, Y. Yamaguchi, and Y. Koumura, </author> <title> "Thread-based programming for the EM-4 hybrid dataflow machine," </title> <booktitle> Proc. 19th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 146-155, </pages> <year> 1992. </year>
Reference-contexts: In section 4, we conclude with discussion of some other related work. 2 The Machine Model SINAN envisions the user program as a collection of threads and array objects. Each thread corresponds to a loop free connected region of the flow graph <ref> [19] </ref>. The creation of a thread instance results in allocation of a data segment for the instance. The instance of a thread may be initiated as soon as it is created or at a later point in the program.
Reference: [20] <author> K. E. Schauser, D. E. Culler, and T. von Eicken, </author> <title> "Compiler-controlled multithreading for lenient parallel architectures," </title> <type> Technical Report TR-91-640, </type> <institution> EECS Dept., Univ. of California, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: The destination address portions of any result tokens produced must be incremented if a vector result is being formed. Special registers are provided to hold the constant stride and the vector size. 2.4 Program partitioning Compiler techniques already developed for lenient languages <ref> [20] </ref> can be successfully employed on SINAN architecture. In order to provide an understanding of how SINAN achieves the balance between the right amount of hardware and compiler support for mul-tithreading, we give a brief sketch of the process of partitioning an imperative program into its threads.
References-found: 20

