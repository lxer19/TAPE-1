URL: ftp://ftp.eecs.umich.edu/people/durfee/uai96.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/durfee/vita.html
Root-URL: http://www.cs.umich.edu
Title: Plan Development using Local Probabilistic Models  
Author: Ella M. Atkins Edmund H. Durfee Kang G. Shin 
Keyword: CIRCA performance.  
Address: 1101 Beal Ave. Ann Arbor, Michigan 48109  
Affiliation: The University of Michigan AI Lab  
Abstract: Approximate models of world state transitions are necessary when building plans for complex systems operating in dynamic environments. External event probabilities can depend on state feature values as well as time spent in that particular state. We assign temporally-dependent probability functions to state transitions. These functions are used to locally compute state probabilities, which are then used to select highly probable goal paths and eliminate improbable states. This probabilistic model has been implemented in the Cooperative Intelligent Real-time Control Architecture (CIRCA), which combines an AI planner with a separate real-time system such that plans are developed, scheduled, and executed with real-time guarantees. We present flight simulation tests that demonstrate how our probabilistic model may improve 
Abstract-found: 1
Intro-found: 1
Reference: <author> E. M. Atkins, E. H. Durfee, and K. G. Shin, </author> <title> "Expecting the Unexpected: Detecting and Reacting to Unplanned-for World States," to appear in Proceedings of A A A I Workshop on Theories of Action and Planning: Bridging the Gap, </title> <month> August </month> <year> 1996. </year>
Reference-contexts: These states are "safe" because all TTFs are preempted by actions, but the system has no chance of achieving its goals from those states. Replanning for goal achievement when a deadend state is encountered is discussed in <ref> (Atkins, Durfee, and Shin 1996) </ref>. CIRCA's control plans are represented as cyclic schedules of testaction pairs (TAPs). Tests involve reading sensors; actions involve sending actuator commands or transferring data between CIRCA modules. <p> If one of the low-probability "removed" states is actually reached, CIRCA detects it and replans as discussed in <ref> (Atkins, Durfee, and Shin 1996) </ref>. <p> This can be done if we can explicitly describe the cumulative probability of each state transition as a function of time. 2 Separation of planning and plan execution was used to allow unrestricted planning time while meeting execution deadlines. Our recent work with handling unexpected states <ref> (Atkins, Durfee, and Shin 1996) </ref> will require us to restrict planning time, using methods similar to those described in MDP work, but CIRCAs planner will still require few constraints so long as states remain within the handled region. We use a simple model for action transition probabilities. <p> We give the planner a better chance to reach its goals by reducing the number of required actions via low-probability state removal. In the unlikely case that a removed state is reached, we <ref> (Atkins, Durfee, and Shin 1996) </ref> have developed an algorithm to detect and replan so that such an occurrence does not spell certain doom. tt F ttf tt .... <p> Action transitions are specified to control navigation settings (OBS - omnibearing selector), altitude, and heading. A low-level control system interfaces the high-level CIRCA actions with simulator actuators, as discussed in <ref> (Atkins, Durfee, and Shin 1996) </ref>.
Reference: <author> C. Boutilier and R. Dearden, </author> <title> Using Abstractions for Decision-Theoretic Planning with Time Constraints, </title> <booktitle> Proceedings of AAAI, </booktitle> <pages> pp. 1016-1022, </pages> <year> 1994. </year>
Reference-contexts: For example, (Dean, Kaelbling, Kirman, and Nicholson 1993) build an envelope of states based on initial state and selected actions, but this envelope may model an inadequate state set with a short plan completion deadline. Alternatively, <ref> (Boutilier and Dearden 1994) </ref> discuss feature abstraction to reduce statespace size, but significant speedup occurs only if abstraction is extensive. MDP utility allows a planner to trade off execution costs with the benefit of continued planning.
Reference: <author> T. Dean, L. P. Kaelbling, J. Kirman, and A. Nicholson, </author> <title> Planning with Deadlines in Stochastic Domains, </title> <booktitle> Proceedings of AAAI, </booktitle> <pages> pp. 574-579, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: As described above, events can occur over time without explicit provocation by the agent, and are generally less predictable than state changes due to actions. The Markov Decision Process (MDP) based models (Littman, Dean, and Kaelbling 1995) of <ref> (Dean, Kaelbling, Kirman, and Nicholson 1993) </ref>, (Horvitz and Barry 1995), and (Tash and Russell 1994) may probabilistically model events as no-op actions, but do not represent the possibility that individual event probabilities change as a function of time spent in a specific state. <p> For example, <ref> (Dean, Kaelbling, Kirman, and Nicholson 1993) </ref> build an envelope of states based on initial state and selected actions, but this envelope may model an inadequate state set with a short plan completion deadline.
Reference: <author> M. L. Ginsberg, </author> <title> "Universal Planning: An (Almost) Universally Bad Idea," </title> <journal> AI Magazine, </journal> <volume> vol. 10, no. 4, </volume> <month> Winter </month> <year> 1989. </year>
Reference-contexts: CIRCA's planner is based on the philosophy that building a plan to handle all world states - a "universal plan" (Schoppers 1987) - is unrealistic due to the possibility of exponential planner execution time <ref> (Ginsberg 1989) </ref>, so it uses heuristics to limit state expansion and minimizes its set of selected actions by requiring only one goal path and guaranteeing failure avoidance along all other paths.
Reference: <author> P. Haddawy, </author> <title> Representing Plans Under Uncertainty: A Logic of Time, Chance, and Action, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1994. </year>
Reference-contexts: We are working to automate probability dependency calculation, building on methods such as <ref> (Haddawy 1994) </ref>. One of CIRCA's main premises is that of guaranteed failure avoidance. <p> While these results are promising, we continue to improve our probabilistic model. We may better model action transition delays using time functions analogous to the temporal transition probability functions, similar to action probability models proposed in <ref> (Haddawy 1994) </ref>. We are working to incorporate such functions into our state probability calculation algorithm. Next, we need to better quantify state probability accuracy, which is difficult to estimate during planning.
Reference: <author> E. Horvitz and M. Barry, </author> <title> Display of Information for Time-Critical Decision Making, </title> <booktitle> in Proceedings of UAI-95, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: As described above, events can occur over time without explicit provocation by the agent, and are generally less predictable than state changes due to actions. The Markov Decision Process (MDP) based models (Littman, Dean, and Kaelbling 1995) of (Dean, Kaelbling, Kirman, and Nicholson 1993), <ref> (Horvitz and Barry 1995) </ref>, and (Tash and Russell 1994) may probabilistically model events as no-op actions, but do not represent the possibility that individual event probabilities change as a function of time spent in a specific state. <p> MDP utility allows a planner to trade off execution costs with the benefit of continued planning. Such utilities may be constructed to promote planner termination before reaching a deadline (as in <ref> (Horvitz and Barry 1995) </ref>), but plan completeness may suffer if the cost of computation time is given sufficient importance to meet hard deadlines. 3 USER SPECIFICATION OF TRANSITION PROBABILITY The CIRCA planner builds the reachable state set based on action and temporal transitions specified in the domain knowledge base.
Reference: <author> F. F. Ingrand and M. P. Georgeff, </author> <title> "Managing Deliberation and Reasoning in Real-Time AI Systems," </title> <booktitle> in Proc. Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> pp. 284-291, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: This scheduled plan is then executed on a separate "real-time" processor. subsystem (AIS) contains both the planner and the scheduler. The "shell" around all AIS operations consists of meta-rules controlling a set of knowledge areas, similar to the PRS architecture <ref> (Ingrand and Georgeff 1990) </ref>. Working memory contains tasks that are ready to be executed. These tasks include planning, downloading plans from the AIS to the real-time subsystem (RTS), and reading/processing feedback data from the RTS.
Reference: <author> N. K. Kushmerick, S. Hanks, D. Weld, </author> <title> An Algorithm for Probabilistic Least-Commitment Planning, </title> <booktitle> Proc. of AAAI, </booktitle> <pages> pp. 1073-1078, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: We conclude by describing future enhancements to improve our probabilistic model (Section 7). 2 BACKGROUND Many probabilistic planning algorithms have been developed, but most do not consider event probabilities as functions of time. In fact, those of <ref> (Kushmerick, Hanks, and Weld 1994) </ref> concentrate only on probabilistic properties of actions that may be controlled by the agent, not external events. As described above, events can occur over time without explicit provocation by the agent, and are generally less predictable than state changes due to actions.
Reference: <author> M. L. Littman, T. L. Dean, and L. P. Kaelbling, </author> <title> On the Complexity of Solving Markov Decision Problems, </title> <booktitle> Proceedings of UAI-95, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: As described above, events can occur over time without explicit provocation by the agent, and are generally less predictable than state changes due to actions. The Markov Decision Process (MDP) based models <ref> (Littman, Dean, and Kaelbling 1995) </ref> of (Dean, Kaelbling, Kirman, and Nicholson 1993), (Horvitz and Barry 1995), and (Tash and Russell 1994) may probabilistically model events as no-op actions, but do not represent the possibility that individual event probabilities change as a function of time spent in a specific state. <p> To-date, our tests have used relatively simple models with few features (see Section 6); otherwise, we would probably still be building our knowledge base. Unfortunately, such complexity is present in many probabilstic models, including ours and MDPs <ref> (Littman, Dean, and Kaelbling 1995) </ref>, in which at least a constant probability must be specified for transitioning from each state to any other state. We are working to automate probability dependency calculation, building on methods such as (Haddawy 1994). One of CIRCA's main premises is that of guaranteed failure avoidance.
Reference: <author> C. L. Liu and J. W. Layland, </author> <title> "Scheduling Algorithms for Multiprogramming in a Hard Real-Time Environment," </title> <journal> Journal of the ACM, </journal> <volume> vol. 20, no. 1, </volume> <pages> pp. 46-61, </pages> <month> January </month> <year> 1973. </year>
Reference-contexts: Tests involve reading sensors; actions involve sending actuator commands or transferring data between CIRCA modules. When the AIS planner creates a TAP, it stores an associated worst-case execution time and execution deadline to enable safety guarantees. These TAP attributes are then used by a deadline-driven scheduler <ref> (Liu and Layland 1973) </ref> to create a periodic TAP schedule. If the scheduler is unable to create a schedule that supports all deadlines, the AIS backtracks to the planner, which then selects different 1 Previously (Musliner, Durfee, and Shin 1995), CIRCA contained three transition types: action, temporal, and event.
Reference: <author> D. J. Musliner, E. H. Durfee, and K. G. Shin, </author> <title> "World Modeling for the Dynamic Construction of Real-Time Control Plans", </title> <journal> Artificial Intelligence, </journal> <volume> vol. 74, </volume> <pages> pp. 83-127, </pages> <year> 1995. </year>
Reference-contexts: We have implemented these algorithms in the Cooperative Intelligent Real-time Control Architecture (CIRCA). CIRCA combines an AI planner, scheduler, and real-time plan execution module to provide guaranteed performance for controlling complex real-world systems <ref> (Musliner, Durfee, and Shin 1995) </ref>. <p> This paper explicitly considers temporally-dependent event probabilities, using them to compute state probabilities which help direct planning in CIRCA. CIRCA <ref> (Musliner, Durfee, and Shin 1995) </ref> was designed to provide guarantees about system performance with limited sensing, actuating, and processing power. Based on userspecified domain knowledge, CIRCA uses traditional AI techniques to create plans that will keep a system safe (i.e., avoid failure) while working to achieve each plan's goal. <p> CIRCA continues state expansion for all other initial states and their reachable descendants until at least one goal state is found and all reachable TTFs are guaranteed to be avoided. Note that the planner is minimally satisfied with only one goal path due to tradeoffs between completeness and schedulability <ref> (Musliner, Durfee, and Shin 1995) </ref>. Thus, as shown in the figure, some reachable states (labeled "deadend") do not lead to the goal. These states are "safe" because all TTFs are preempted by actions, but the system has no chance of achieving its goals from those states. <p> These TAP attributes are then used by a deadline-driven scheduler (Liu and Layland 1973) to create a periodic TAP schedule. If the scheduler is unable to create a schedule that supports all deadlines, the AIS backtracks to the planner, which then selects different 1 Previously <ref> (Musliner, Durfee, and Shin 1995) </ref>, CIRCA contained three transition types: action, temporal, and event. Events can occur instantaneously while temporals have a nonzero delay. We now model events and temporals as temporal transitions, with differences specified using probability functions as described in Section 3. actions. <p> We use a simple model for action transition probabilities. As with the previous version of CIRCA <ref> (Musliner, Durfee, and Shin 1995) </ref>, we assume action transitions will affect state features following a constant delay after being executed on the RTS. <p> Goal-reaching and deadend states Removed States Deadend States States that reach goal b) New CIRCA. probability c) Original CIRCA. In previous versions of CIRCA <ref> (Musliner, Durfee, and Shin 1995) </ref>, the planner expanded states in depth-first order. The planner selected actions primarily to avoid TTFs and secondarily to achieve goals. <p> Without state removal, CIRCA's planner may completely fail since it requires plans with guaranteed failure avoidance. This problem was recognized and discussed in <ref> (Musliner, Durfee, and Shin 1995) </ref>, but no formal algorithms for considering characteristics such as probability were presented. We give the planner a better chance to reach its goals by reducing the number of required actions via low-probability state removal.
Reference: <author> R. Rainey, </author> <title> ACM: The Aerial Combat Simulation for X11. </title> <month> February </month> <year> 1994. </year>
Reference-contexts: G ac D D = Deadend States Goal State low-probability tt high-probability tt tt tt I high-probability tt low-probability tt - Downstream states REMOVEDtt a) Before State Removal. b) After State Removal. 6 Tests from Flight Simulation The ACM aircraft flight simulator <ref> (Rainey 1994) </ref> was chosen to test the new CIRCA probabilistic model. We selected the "flight around a pattern" task for our automated aircraft, as illustrated in Figure 9. A series of subgoals to arrive at different "fixes" allow the planner to consider a small flight segment in each planning cycle.
Reference: <author> M. J. Schoppers, </author> <title> "Universal Plans for Reactive Robots in Unpredictable Environments," </title> <booktitle> in Proc. Int'l Joint Conf. on Artificial Intelligence, </booktitle> <pages> pp. 1039-1046, </pages> <year> 1987. </year>
Reference-contexts: CIRCA combines an AI planner, scheduler, and real-time plan execution module to provide guaranteed performance for controlling complex real-world systems (Musliner, Durfee, and Shin 1995). CIRCA's planner is based on the philosophy that building a plan to handle all world states - a "universal plan" <ref> (Schoppers 1987) </ref> - is unrealistic due to the possibility of exponential planner execution time (Ginsberg 1989), so it uses heuristics to limit state expansion and minimizes its set of selected actions by requiring only one goal path and guaranteeing failure avoidance along all other paths.
Reference: <author> J. Tash and S. Russell, </author> <title> Control Strategies for a Stochastic Planner, </title> <booktitle> Proc. of AAAI, </booktitle> <volume> vol. 2, </volume> <pages> pp. 1079-1085, </pages> <year> 1994. </year>
Reference-contexts: As described above, events can occur over time without explicit provocation by the agent, and are generally less predictable than state changes due to actions. The Markov Decision Process (MDP) based models (Littman, Dean, and Kaelbling 1995) of (Dean, Kaelbling, Kirman, and Nicholson 1993), (Horvitz and Barry 1995), and <ref> (Tash and Russell 1994) </ref> may probabilistically model events as no-op actions, but do not represent the possibility that individual event probabilities change as a function of time spent in a specific state.
References-found: 14

