URL: http://www.isr.umd.edu/Labs/CACSE/FSQP/jota.ps
Refering-URL: http://www.isr.umd.edu/Labs/CACSE/FSQP/fsqp.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Nonmonotone Line Search for Minimax Problems 1  
Author: Jian L. Zhou and Andre L. Tits 
Keyword: Key words. Minimax problems, SQP direction, Maratos effect, Superlinear convergence.  
Address: College Park, MD 20742  
Affiliation: Electrical Engineering Department and Systems Research Center University of Maryland,  
Date: 3. March 1993  
Note: journal of optimization theory and applications: Vol. 76, No.  Copyright 1993 Plenum Publishing Corporation. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or distribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from Plenum Publishing Corporation.  
Abstract: It was recently shown that, in the solution of smooth constrained optimization problems by sequential quadratic programming (SQP), the Maratos effect can be prevented by means of a certain nonmonotone (more precisely, three-step or four-step monotone) line search. Using a well known transformation, this scheme can be readily extended to the case of minimax problems. It turns out however that, due to the structure of these problems, one can use a simpler scheme. Such a scheme is proposed and analyzed in this paper. Numerical experiments indicate a significant advantage of the proposed line search over the (monotone) Armijo search. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.A. Polyak, </author> <title> "Smooth Optimization Methods for Minimax Problems," </title> <journal> SIAM J. Control Optim. </journal> <volume> 26 (1988), </volume> <pages> 1274-1286. 15 </pages>
Reference-contexts: k ) + hrf i (x k ); td k + t 2 ~ d k i + o (td k + t 2 ~ d k ) : Thus, in view of (10) and the boundedness of d k and ~ d k , we have, for t 2 <ref> [0; 1] </ref> and i = 1; ; p, f i (x k + td k + t 2 ~ d k ) = f i (x k ) + thrf i (x k ); d k i + o (t) f i (x k ) + t max ff i (x
Reference: [2] <author> E. Polak, D.Q. Mayne & J.E. Higgins, </author> <title> "A Superlinearly Convergent Algorithm for Min--max Problems," </title> <booktitle> Proceedings of the 28th IEEE Conference on Decision and Control (De-cember 1989). </booktitle>
Reference: [3] <author> A.R. Conn & Y. Li, </author> <title> "An Efficient Algorithm for Nonlinear Minimax Problems," </title> <institution> University of Waterloo, Research Report CS-88-41, Waterloo, </institution> <address> Ontario, N2L 3G1 Canada, </address> <month> November, </month> <year> 1989 </year> . 
Reference-contexts: Unfortunately, it is known that in general the full step does not yield a decrease of f and thus the line search may prevent superlinear convergence to take place (Maratos-like effect). As pointed out by Womersley and Fletcher [9] and by Conn and Li <ref> [3] </ref>, the watchdog technique [10] and the bending technique [11,12], proposed for circumventing the Maratos effect in the context of smooth constrained optimization, can be easily extended to the minimax framework. Both approaches however have drawbacks. <p> Problems BARD down to WONG are unconstrained and MAD1 down to MAD8 are linearly constrained minimax problems. In Table 1, the performance of Algorithm NLS is compared with that of the same algorithm with an Armijo type line search (ALS) 5 and with that of algorithms proposed in <ref> [3] </ref> (CL) and [23] (MS). To make such comparison meaningful, we attempted to best approximate the stopping rule used in each of the references.
Reference: [4] <author> S.P. Han, </author> <title> "Superlinear Convergence of a Minimax Method," </title> <institution> Department of Computer Science, Cornell University, TR78-336, </institution> <year> 1978. </year>
Reference: [5] <author> S.P. Han, </author> <title> "Variable Metric Methods for Minimizing a Class of Nondifferentiable Functions," Math. </title> <booktitle> Programming 20 (1981), </booktitle> <pages> 1-13. </pages>
Reference-contexts: The following lemma shows that d k is a direction of descent for f (x) at x k (see, e.g., <ref> [5] </ref>). Lemma 3.1.
Reference: [6] <author> A.R. Conn, </author> <title> "An Efficient Second Order Method to Solve the (Constrained) Minimax Problem," </title> <institution> Department of Combinatorics and Optimization, Research Report CORR 79-5, University of Waterloo, </institution> <address> Ontario, </address> <year> 1979. </year>
Reference: [7] <author> W. </author> <title> Murray & M.L. Overton, "A Projected Lagrangian Algorithm for Nonlinear Minimax Optimization," </title> <note> SIAM J. </note> <institution> Sci. Stat. Comput. </institution> <month> 1 (September, </month> <year> 1980). </year>
Reference: [8] <author> M.L. Overton, </author> <title> "Algorithms for Nonlinear ` 1 and ` 1 Fitting," in Nonlinear Optimization 1981, </title> <editor> M.J.D. Powell, ed., </editor> <publisher> Acadamic Press, </publisher> <address> London, </address> <year> 1982, </year> <pages> 213-221. </pages>
Reference: [9] <author> R.S. Womersley & R. Fletcher, </author> <title> "An Algorithm for Composite Nonsmooth Optimization Problems," </title> <journal> J. Optim. Theory Appl. </journal> <volume> 48 (1986), </volume> <pages> 493-523. </pages>
Reference-contexts: Unfortunately, it is known that in general the full step does not yield a decrease of f and thus the line search may prevent superlinear convergence to take place (Maratos-like effect). As pointed out by Womersley and Fletcher <ref> [9] </ref> and by Conn and Li [3], the watchdog technique [10] and the bending technique [11,12], proposed for circumventing the Maratos effect in the context of smooth constrained optimization, can be easily extended to the minimax framework. Both approaches however have drawbacks.
Reference: [10] <author> R.M. Chamberlain, M.J.D. Powell, C. Lemarechal & H.C. Pedersen, </author> <title> "The Watchdog Technique for Forcing Convergence in Algorithms for Constrained Optimization," </title> <journal> Math. Programming Stud. </journal> <volume> 16 (1982), </volume> <pages> 1-17. </pages>
Reference-contexts: Unfortunately, it is known that in general the full step does not yield a decrease of f and thus the line search may prevent superlinear convergence to take place (Maratos-like effect). As pointed out by Womersley and Fletcher [9] and by Conn and Li [3], the watchdog technique <ref> [10] </ref> and the bending technique [11,12], proposed for circumventing the Maratos effect in the context of smooth constrained optimization, can be easily extended to the minimax framework. Both approaches however have drawbacks. <p> The proof of this results involves the following lemma which is a simple extension of a result shown in <ref> [10] </ref> in the case of smooth constrained optimization problems. Lemma 3.7. There exists c 1 &gt; 0 such that, for all x close to x fl , f (x) f (x fl ) c 1 kx x fl k 2 : Theorem 3.8.
Reference: [11] <author> R. Fletcher, </author> <title> "Second Order Corrections for Non-differentiable Optimization," in Numerical Analysis, Dundee, </title> <booktitle> 1981, Lecture Notes in Mathematics 912, </booktitle> <address> G.A. Watson, </address> <publisher> ed., Springer-Verlag, </publisher> <year> 1981, </year> <pages> 85-114. </pages>
Reference: [12] <author> D. Q. Mayne & E. Polak, </author> <title> "A Superlinearly Convergent Algorithm for Constrained Optimization Problems," </title> <journal> Math. Programming Stud. </journal> <volume> 16 (1982), </volume> <pages> 45-61. </pages>
Reference: [13] <author> L. Grippo, F. Lampariello & S. Lucidi, </author> <title> "A Nonmonotone Line Search Technique for Newton's Method," </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 23 (1986), </volume> <pages> 707-716. </pages>
Reference-contexts: Both approaches however have drawbacks. The watchdog technique may result in repeated backtracking in early iterations and the bending technique requires an additional evaluation of f at each iteration. A few years ago, in the context of Newton's method for smooth unconstrained optimization, Grippo, Lampariello and Lucidi <ref> [13] </ref> proposed a "nonmonotone" line search according to which the objective function is not forced to decrease at every iteration but merely every M iterations, where M is a freely selected positive integer. <p> It is known to induce global convergence 3 A nonmonotone decrease criterion using f 0 (x k ; d k ) instead of hd k ; H k d k i is also possible; see Remark 3.2 below. 3 when f is smooth <ref> [13] </ref>; we show below (Theorem 3.3) that it still does here. In view of (3), the nonmonotone line search criterion would accept the full step of one provided the "undamped" iteration (2) has been used for the last two iterations. <p> In the sequel, we assume the latter. The following property, which holds even though monotone line search is not enforced, is a key to global convergence. Although the underlying ideas of the proof are analogous to those used by Grippo et al . in the smooth unconstrained case <ref> [13] </ref>, the details of the extension to the present situation are nontrivial. Lemma 3.2. The sequence fx k g is bounded and the sequences ft k d k g and fx k+1 x k g both converge to zero. Proof.
Reference: [14] <author> E.R. Panier & A.L. </author> <title> Tits, "Avoiding the Maratos Effect by Means of a Nonmonotone Line Search. I. General Constrained Problems," </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 28 (1991), </volume> <pages> 1183-1195. </pages>
Reference-contexts: Recently, it was shown that making use of a suitable extension of this scheme to smooth constrained optimization, in the framework of SQP with penalty function-based line search, has the additional advantage of automatically allowing a full step to be taken locally and thus avoiding the Maratos effect <ref> [14] </ref>. Many of the schemes that have been proposed for the solution of minimax problems can be viewed as follows. <p> The question thus arises here of whether similar refinements on the nonmonotone line search scheme of <ref> [14] </ref> are viable. Specifically, (i) does a nonmonotone line search in the "max" function f still enforce global convergence? (ii) does such a line search prevent the Maratos effect? It turns out that the answer to both questions are positive. <p> In (7) and (8), hd k ; H k d k i could be replaced by f 0 (x k ; d k ). A quantity closely related to the latter was used in <ref> [14] </ref> while the former was used in [15]. In [14] the "max" in the line search criterion is over four steps (` = f0; 1; 2; 3g). In [15] it is observed that, with the modification just pointed out, the "max" can now be taken over three steps. <p> In (7) and (8), hd k ; H k d k i could be replaced by f 0 (x k ; d k ). A quantity closely related to the latter was used in <ref> [14] </ref> while the former was used in [15]. In [14] the "max" in the line search criterion is over four steps (` = f0; 1; 2; 3g). In [15] it is observed that, with the modification just pointed out, the "max" can now be taken over three steps. <p> Extension to the linearly constrained case presents no difficulty, but an assumption of linear independence of gradients of active constraints has to be imposed on the analysis of global convergence to ensure that multipliers associated with constraints are bounded. For nonlinearly constrained minimax problems, either the algorithm given in <ref> [14] </ref> could be invoked with suitable modifications concerning our max function f (x) if feasibility of successive iterates is not required, or the algorithm in [15] could be invoked, as has already been suggested there, if feasibility is required at each iteration starting, from an initial feasible point (nonlinear equality constraints <p> 4 FSQP is available from the authors. 5 FSQP gives the user the option to choose either NLS or ALS with bending (i.e., replace ` = 0; 1; 2 by ` = 0). such algorithms can be easily carried out by combining the results in this paper and results in <ref> [14] </ref> or results in [15]. In fact, Algorithm NLS has been combined with that in [15] and has been successfully implemented in FSQP [20] to solve nonlinearly constrained minimax problems. Table 2 contains some numerical results.
Reference: [15] <author> J.F. Bonnans, E.R. Panier, A.L. Tits & J. Zhou, </author> <title> "Avoiding the Maratos Effect by Means of a Nonmonotone Line Search. II. Inequality Constrained Problems Feasible Iterates," </title> <institution> Systems Research Center, University of Maryland, </institution> <type> Technical Report SRC-TR-89-42r1, </type> <institution> College Park, MD 20742, </institution> <year> 1989. </year> <month> 16 </month>
Reference-contexts: In (7) and (8), hd k ; H k d k i could be replaced by f 0 (x k ; d k ). A quantity closely related to the latter was used in [14] while the former was used in <ref> [15] </ref>. In [14] the "max" in the line search criterion is over four steps (` = f0; 1; 2; 3g). In [15] it is observed that, with the modification just pointed out, the "max" can now be taken over three steps. <p> A quantity closely related to the latter was used in [14] while the former was used in <ref> [15] </ref>. In [14] the "max" in the line search criterion is over four steps (` = f0; 1; 2; 3g). In [15] it is observed that, with the modification just pointed out, the "max" can now be taken over three steps. In the minimax context, again, substitution of f 0 (x k ; d k ) in the line search would require the "max" to be taken over four steps. 4. <p> For nonlinearly constrained minimax problems, either the algorithm given in [14] could be invoked with suitable modifications concerning our max function f (x) if feasibility of successive iterates is not required, or the algorithm in <ref> [15] </ref> could be invoked, as has already been suggested there, if feasibility is required at each iteration starting, from an initial feasible point (nonlinear equality constraints are not allowed). <p> from the authors. 5 FSQP gives the user the option to choose either NLS or ALS with bending (i.e., replace ` = 0; 1; 2 by ` = 0). such algorithms can be easily carried out by combining the results in this paper and results in [14] or results in <ref> [15] </ref>. In fact, Algorithm NLS has been combined with that in [15] and has been successfully implemented in FSQP [20] to solve nonlinearly constrained minimax problems. Table 2 contains some numerical results. <p> choose either NLS or ALS with bending (i.e., replace ` = 0; 1; 2 by ` = 0). such algorithms can be easily carried out by combining the results in this paper and results in [14] or results in <ref> [15] </ref>. In fact, Algorithm NLS has been combined with that in [15] and has been successfully implemented in FSQP [20] to solve nonlinearly constrained minimax problems. Table 2 contains some numerical results.
Reference: [16] <author> E.R. Panier & A.L. </author> <title> Tits, "A Superlinearly Convergent Method of Feasible Directions for Optimization Problems Arising in the Design of Engineering Systems," </title> <booktitle> in Proc. of the Seventh International Conference on Analysis and Optimization of Systems | Antibes, </booktitle> <month> June 25-27, </month> <year> 1986, </year> <editor> A. Bensoussan & J.L. Lions, eds., </editor> <booktitle> Lecture Notes in Control and Information Sciences #83, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin-Heidelberg-New York-Tokyo, </address> <month> June </month> <year> 1986, </year> <pages> 65-73. </pages>
Reference-contexts: Global and local convergence are analyzed in Section 3. Numerical results are presented in Section 4. Section 5 is devoted to final remarks. 2. The Algorithm. Our algorithm can be viewed as dealing with (P ) in the same spirit as in [4,5] and <ref> [16] </ref>.
Reference: [17] <author> M.J.D. Powell, </author> <title> "A Fast Algorithm for Nonlinearly Constrained Optimization Calculations," in Numerical Analysis, Dundee, </title> <booktitle> 1977, Lecture Notes in Mathematics 630, </booktitle> <address> G.A. Watson, </address> <publisher> ed., Springer-Verlag, </publisher> <year> 1978, </year> <pages> 144-157. </pages>
Reference-contexts: Assumption A6 has been observed to often hold, e.g., under some conditions, when H k is updated using Powell's modification of the BFGS formula (see <ref> [17] </ref>). In the presence of the strong properties stated in Proposition 3.4, it ensures that the iteration is close enough to the Newton iteration that a full step is eventually accepted by the line search. Proposition 3.5. <p> An efficient implementation of the algorithm de-scribed in this paper has been incorporated into a more general code (FSQP 4 Version 2.4 [20]). In this implementation, ff = 0:1, fi = 0:5, and H k is updated by means of the BFGS formula with Powell's modification <ref> [17] </ref>, with H 0 = I the identity matrix. Results obtained on selected minimax problems are summarized in Table 1. All computations were performed on a SUN 4/SPARC station 1.
Reference: [18] <author> M.J.D. Powell, </author> <title> "The Convergence of Variable Metric Methods for Nonlinearly Constrained Optimization Calculations," in Nonlinear Programming 3, O.L. </title> <editor> Mangasarian, R.R. Meyer & S.M. Robinson, eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978, </year> <pages> 27-63. </pages>
Reference: [19] <author> J. Stoer, </author> <title> "The Convergence of Sequential Quadratic Programming Methods for Solving Nonlinear Programs," in Recent Advances in Communication and Control Theory , R.E. Kalman, G.I. Marchuk, A.E. </title> <editor> Ruberti & A.J. Viterbi, eds., </editor> <booktitle> Optimization Software, </booktitle> <publisher> Inc., </publisher> <address> New York, N.Y., </address> <year> 1987, </year> <pages> 412-421. </pages>
Reference: [20] <author> J. Zhou & A.L. </author> <title> Tits, "User's Guide for FSQP Version 2.4 A Fortran Code for Solving Optimization Problems, Possibly Minimax, with General Inequality Constraints and Linear Equality Constraints, Generating Feasible Iterates," </title> <institution> Systems Research Center, University of Maryland, SRC TR-90-60r1b, College Park, MD 20742, </institution> <year> 1991. </year>
Reference-contexts: An efficient implementation of the algorithm de-scribed in this paper has been incorporated into a more general code (FSQP 4 Version 2.4 <ref> [20] </ref>). In this implementation, ff = 0:1, fi = 0:5, and H k is updated by means of the BFGS formula with Powell's modification [17], with H 0 = I the identity matrix. Results obtained on selected minimax problems are summarized in Table 1. <p> In fact, Algorithm NLS has been combined with that in [15] and has been successfully implemented in FSQP <ref> [20] </ref> to solve nonlinearly constrained minimax problems. Table 2 contains some numerical results.
Reference: [21] <author> G.A. Watson, </author> <title> "The Minimax Solution of an Overdetermined System of Non-linear Equations," </title> <journal> J. Inst. Math. Appl. </journal> <volume> 23 (1979), </volume> <pages> 167-180. </pages>
Reference-contexts: All computations were performed on a SUN 4/SPARC station 1. Gradients were computed by finite differences (for the ith component, the perturbation parameter was 2 fi 10 8 maxf1; jx i k jg). Problems BARD, DAVD2, F&R, HETTICH, and WATS are from <ref> [21] </ref>; CB2, CB3, R-S, WONG and COLV are from [22, Examples 5.1-5]; MAD1 to MAD8 are from [23, Examples 1-8].
Reference: [22] <author> C. Charalambous & A.R. Conn, </author> <title> "An Efficient Method to Solve the Minimax Problem Directly," </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 15 (1978), </volume> <pages> 162-187. </pages>
Reference-contexts: Gradients were computed by finite differences (for the ith component, the perturbation parameter was 2 fi 10 8 maxf1; jx i k jg). Problems BARD, DAVD2, F&R, HETTICH, and WATS are from [21]; CB2, CB3, R-S, WONG and COLV are from <ref> [22, Examples 5.1-5] </ref>; MAD1 to MAD8 are from [23, Examples 1-8]. Some of these test problems allow one to freely select the number of variables; problems WATS-6 and WATS-20 correspond to 6 and 20 variables, respectively, and MAD8-10, MAD8-30 and MAD8-50 to 10, 30 and 50 variables respectively.
Reference: [23] <author> K. Madsen & H. Schjr-Jacobsen, </author> <title> "Linearly Constrained Minimax Optimization," Math. </title> <booktitle> Programming 14 (1978), </booktitle> <pages> 208-223. </pages>
Reference-contexts: Gradients were computed by finite differences (for the ith component, the perturbation parameter was 2 fi 10 8 maxf1; jx i k jg). Problems BARD, DAVD2, F&R, HETTICH, and WATS are from [21]; CB2, CB3, R-S, WONG and COLV are from [22, Examples 5.1-5]; MAD1 to MAD8 are from <ref> [23, Examples 1-8] </ref>. Some of these test problems allow one to freely select the number of variables; problems WATS-6 and WATS-20 correspond to 6 and 20 variables, respectively, and MAD8-10, MAD8-30 and MAD8-50 to 10, 30 and 50 variables respectively. <p> In Table 1, the performance of Algorithm NLS is compared with that of the same algorithm with an Armijo type line search (ALS) 5 and with that of algorithms proposed in [3] (CL) and <ref> [23] </ref> (MS). To make such comparison meaningful, we attempted to best approximate the stopping rule used in each of the references. <p> WATS-20 is peculiar since from iteration 20 on, the 14 significant digits printed out by FSQP do not change. On the MAD problems for which the Haar condition holds, the performance of NLS appears to be comparable to that of the algorithm of <ref> [23] </ref>. 5. Concluding remarks. We have described and analyzed an SQP based algorithm for unconstrained nonlinear minimax problems with nonmonotone line search. It is proved that the Maratos-like effect can be avoided while auxiliary function evaluations are performed only during early iterations.
Reference: [24] <author> W. Hock & K. Schittkowski, </author> <title> Test Examples for Nonlinear Programming Codes, </title> <booktitle> Lecture Notes in Economics and Mathematical Systems (187), </booktitle> <publisher> Springer Verlag, </publisher> <year> 1981. </year> <month> 17 </month>
Reference-contexts: In fact, Algorithm NLS has been combined with that in [15] and has been successfully implemented in FSQP [20] to solve nonlinearly constrained minimax problems. Table 2 contains some numerical results. These problems are obtained from problems 43, 84, 113 and 117 in <ref> [24] </ref> by removing certain constraints and including instead additional objectives of the form f i (x) = f (x) + ff i g j (x) where the ff i 's are positive scalars and g j (x) 0.
References-found: 24

