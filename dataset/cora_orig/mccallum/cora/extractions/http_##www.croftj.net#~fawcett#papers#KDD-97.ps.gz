URL: http://www.croftj.net/~fawcett/papers/KDD-97.ps.gz
Refering-URL: http://www.croftj.net/~fawcett/ROCCH/
Root-URL: 
Email: foster@nynexst.com  fawcett@nynexst.com  
Title: Analysis and Visualization of Classifier Performance: Comparison under Imprecise Class and Cost Distributions  
Author: Foster Provost Tom Fawcett 
Address: 400 Westchester Avenue White Plains, New York 10604  400 Westchester Avenue White Plains, New York 10604  
Affiliation: NYNEX Science and Technology  NYNEX Science and Technology  
Abstract: Applications of inductive learning algorithms to real-world data mining problems have shown repeatedly that using accuracy to compare classifiers is not adequate because the underlying assumptions rarely hold. We present a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs. The ROC convex hull method combines techniques from ROC analysis, decision analysis and computational geometry, and adapts them to the particulars of analyzing learned classifiers. The method is efficient and incremental, minimizes the management of classifier performance data, and allows for clear visual comparisons and sensitivity analyses. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barber, C.; Dobkin, D.; and Huhdanpaa, H. </author> <year> 1993. </year> <title> The quickhull algorithm for convex hull. </title> <type> Technical Report GCG53, </type> <institution> University of Min-nesota. </institution> <note> Available from ftp://geom.umn.edu/pub/ software/qhull.tar.Z. </note>
Reference-contexts: Thus, a classifier is potentially optimal if and only if it lies on the northwest boundary (i.e., above the line y = x) of the convex hull <ref> (Barber, Dobkin, & Huhdanpaa 1993) </ref> of the set of points in ROC space. <p> Find the convex hull of the set of points representing the predictive behavior of all classifiers of interest. For n classifiers this can be done in O (n log (n)) time by the QuickHull algorithm <ref> (Barber, Dobkin, & Huhdanpaa 1993) </ref>. 3. For each set of class and cost distributions of inter est, find the slope (or range of slopes) of the corre sponding iso-performance lines. 4.
Reference: <author> Beck, J. R., and Schultz, E. K. </author> <year> 1986. </year> <title> The use of ROC curves in test performance evaluation. </title> <institution> Arch Pathol Lab Med 110 </institution> <month> 13-20. </month>
Reference-contexts: ROC analysis has been extended for use in visualizing and analyzing the behavior of diagnostic systems (Swets 1988), and is used for visualization in medicine <ref> (Beck & Schultz 1986) </ref>. We will use the term ROC space to denote the classifier performance space used for visualization in ROC analysis. On an ROC graph, T P is plotted on the Y axis and F P is plotted on the X axis.
Reference: <author> Bloedorn, E.; Mani, I.; and MacMillan, T. </author> <year> 1996. </year> <title> Machine learning of user profiles: Representational issues. </title> <booktitle> In Proceedings of Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 433-438. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Also, methods such as these should consider statistical tests for comparing performance curves, so that the user has confidence that differences in performance are significant. The tradeoff between TP and FP rates is similar to the tradeoff between precision and recall, commonly used in Information Retrieval <ref> (Bloedorn, Mani, & MacMillan 1996) </ref>. However, precision and recall do not take into account the relative size of the population of "uninteresting" entities, which is necessary to deal with changing class distributions. 2 Existing cost-sensitive learning methods are brittle with respect to imprecise or changing distributions.
Reference: <author> Breiman, L.; Friedman, J.; Olshen, R.; and Stone, C. </author> <year> 1984. </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: In such cases, accuracy maximization should be replaced with cost minimization. The problems of unequal error costs and uneven class distributions are related. It has been suggested that high-cost instances can be compensated for by increasing their prevalence in an instance set <ref> (Breiman et al. 1984) </ref>. Unfortunately, little work has been published on either problem. There exist several dozen articles (Turney 1996) in which techniques are suggested, but little is done to evaluate and compare them (the article of Pazzani et al. (1994) being the exception).
Reference: <author> Catlett, J. </author> <year> 1995. </year> <title> Tailoring rulesets to misclassifica-tioin costs. </title> <booktitle> In Proceedings of the 1995 Conference on AI and Statistics, </booktitle> <pages> 88-94. </pages>
Reference: <author> Clearwater, S., and Stern, E. </author> <year> 1991. </year> <title> A rule-learning program in high energy physics event classification. </title> <journal> Comp Physics Comm 67 </journal> <pages> 159-182. </pages>
Reference-contexts: A simple rule, always classify as the maximum likelihood class, gives a 99.9% accuracy. Presumably this is not satisfactory if a non-trivial solution is sought. Skews of 10 2 are common in fraud detection and skews greater than 10 6 have been reported in other classifier learning applications <ref> (Clearwater & Stern 1991) </ref>. Evaluation by classification accuracy also tacitly assumes equal error costs|that a false positive error is equivalent to a false negative error. In the real world this is rarely the case, because classifications lead to actions which have consequences, sometimes grave.
Reference: <author> Duda, R. O., and Hart, P. E. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference: <author> Egan, J. P. </author> <year> 1975. </year> <title> Signal Detection Theory and ROC Analysis. </title> <booktitle> Series in Cognitition and Perception. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: However, as discussed above, such analyses assume that the distributions are precise and static. 1 Error costs include benefits not realized. Receiver Operating Characteristic (ROC) graphs have long been used in signal detection theory to depict tradeoffs between hit rate and false alarm rate <ref> (Egan 1975) </ref>. ROC analysis has been extended for use in visualizing and analyzing the behavior of diagnostic systems (Swets 1988), and is used for visualization in medicine (Beck & Schultz 1986). We will use the term ROC space to denote the classifier performance space used for visualization in ROC analysis. <p> The first assumption is essential to the use of a two dimensional graph; the second assumption is essential to the creation of iso-performance lines. Furthermore, the method is based upon the maximization of expected value as the decision goal. Other decision goals are possible <ref> (Egan 1975) </ref>. For example, the Neyman-Pearson observer strategy tries to maximize the hit rate for a fixed false-alarm rate.
Reference: <author> Ezawa, K.; Singh, M.; and Norton, S. </author> <year> 1996. </year> <title> Learning goal oriented bayesian networks for telecommunications risk management. </title> <booktitle> In Proceedings of IMLC-96, </booktitle> <pages> 139-147. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fawcett, T., and Provost, F. </author> <year> 1996. </year> <title> Combining data mining and machine learning for effective user profiling. </title> <booktitle> In Proceedings of KDD-96, </booktitle> <pages> 8-13. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: An ROC curve illustrates the error tradeoffs available with a given classifier. Figure 1 shows a plot of the performance of four classifiers, A through D, typical of what we see in the creation of alarms for fraud detection <ref> (Fawcett & Provost 1996) </ref>. For orientation, several points on an ROC graph should be noted.
Reference: <author> Pazzani, M.; Merz, C.; Murphy, P.; Ali, K.; Hume, T.; and Brunk, C. </author> <year> 1994. </year> <title> Reducing misclassification costs. </title> <booktitle> In Proc. 11th International Conference on Machine Learning, </booktitle> <pages> 217-225. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If a classifier produces posterior probabilities, decision analysis gives us a way to produce cost-sensitive classifications from the classifier (Weinstein & Fineberg 1980). Classifier error frequencies can be used to approximate probabilities <ref> (Pazzani et al. 1994) </ref>.
Reference: <author> Provost, F., and Buchanan, B. </author> <year> 1992. </year> <title> Inductive policy. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> 255-261. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Provost, F., and Buchanan, B. </author> <year> 1995. </year> <title> Inductive policy: The pragmatics of bias selection. </title> <booktitle> Machine Learning 20 </booktitle> <pages> 35-61. </pages>
Reference: <author> Swets, J. </author> <year> 1988. </year> <title> Measuring the accuracy of diagnostic systems. </title> <booktitle> Science 240 </booktitle> <pages> 1285-1293. </pages>
Reference-contexts: Receiver Operating Characteristic (ROC) graphs have long been used in signal detection theory to depict tradeoffs between hit rate and false alarm rate (Egan 1975). ROC analysis has been extended for use in visualizing and analyzing the behavior of diagnostic systems <ref> (Swets 1988) </ref>, and is used for visualization in medicine (Beck & Schultz 1986). We will use the term ROC space to denote the classifier performance space used for visualization in ROC analysis.
Reference: <author> Turney, P. </author> <year> 1995. </year> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <type> JAIR 2 </type> <pages> 369-409. </pages>
Reference: <author> Turney, P. </author> <year> 1996. </year> <title> Cost sensitive learning bibliography. </title> <address> http://ai.iit.nrc.ca/bibliographies/ cost-sensitive.html. </address>
Reference-contexts: The problems of unequal error costs and uneven class distributions are related. It has been suggested that high-cost instances can be compensated for by increasing their prevalence in an instance set (Breiman et al. 1984). Unfortunately, little work has been published on either problem. There exist several dozen articles <ref> (Turney 1996) </ref> in which techniques are suggested, but little is done to evaluate and compare them (the article of Pazzani et al. (1994) being the exception). The literature provides even less guidance in situations where distributions are imprecise or can change.
Reference: <author> Weinstein, M. C., and Fineberg, H. V. </author> <year> 1980. </year> <title> Clinical Decision Analysis. </title> <address> Philadelphi, PA: </address> <publisher> W. B. Saunders Company. </publisher>
Reference-contexts: If a classifier produces posterior probabilities, decision analysis gives us a way to produce cost-sensitive classifications from the classifier <ref> (Weinstein & Fineberg 1980) </ref>. Classifier error frequencies can be used to approximate probabilities (Pazzani et al. 1994).
References-found: 17

