URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3436/3436.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Compiler-Assisted Scheduling for Real-Time Applications: A Static Alternative to Low-Level Tuning  
Degree: Seongsoo Hong, Doctor of Philosophy, 1994 Dissertation directed by: Assistant Professor Richard Gerber  
Affiliation: Department of Computer Science  
Note: Abstract Title of Dissertation:  
Abstract: Developing a real-time system requires finding a balance between the timing constraints and the functional requirements. Achieving this balance often requires last-minute, low-level intervention in the code modules via intensive hardware-based instrumentation and manual program optimizations. In this dissertation we present an automated, static alternative to this kind of human-intensive work. Our approach is motivated by recent advances in compiler technologies, which we extend to two specific issues on real-time programming, that is, feasibility and schedulability. A task is infeasible if its execution time stretches over its deadline. To eliminate such faults, we have developed a synthesis method that (1) inspects all infeasible paths, and then (2) moves instructions out of those paths to shorten the execution time. On the other hand, schedulability of a task set denotes an ability to guarantee the deadlines of all tasks in the application. This property is affected by interactions between the tasks, as well as their individual execution times and deadlines. To address the schedulability problem, we have developed a task transformation method based on program slicing. The method decomposes a task into two subthreads: the IO-handler component that must meet the original deadline, and the state-update component that can be postponed past the deadline. This delayed-deadline approach contributes to the schedulability of the overall application. We also present a new fixed-priority preemptive scheduling strategy, which yields both a feasible priority ordering and a feasible task-slicing metric. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Agrawal, R. DeMillo, and E. Spafford. </author> <title> Debugging with dynamic slicing and backtracking. </title> <journal> Software Practice and Experience, </journal> <volume> 23(6) </volume> <pages> 590-616, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: These buttons are used to save the current slice onto the secondary window, load the saved slice onto the primary window, and swap the two slices. 7.2.3 Implementation The prototype implementation of TimeWare/SLICE is based on a dynamic program slicing tool SPYDER developed at Purdue University <ref> [1] </ref>. SPYDER is originally a program debugging tool relying on dynamic slicing, and it consists of two components: a modified version of GCC (GNU C compiler) and GDB (GNU symbolic debugger).
Reference: [2] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: A similar but more advanced approach was reported in [32]. While the latter approach is able to predict pipeline stalls as well, both approaches essentially rely on attribute grammars <ref> [2] </ref> to propagate cache hit information backward in a flow graph. However, no static timing tool is precise enough to be used with complete confidence for developing production-quality software. Moreover, even sophisticated timing analysis methods such as [32, 44] are not appropriate for fine-grained instruction timing. <p> and throttle for heading. fl/ wflap = compFlapw (roll, vel, dtheta); throttle = compThrot (roll, vel, dtheta); output (THROT, throttle); output (FLAP Cntrl, wflap); g 20 Chapter 4 Basic Notations The output of the TCEL compiler's machine-independent pass is the code in an intermediate representation, encapsulated in a flow graph <ref> [2] </ref>. We extend the format of a flow graph to hold the original timing information. For example, Figure 3.3 (B) shows the extended flow graph for our flight controller program in Figure 3.3 (A), where for the sake of brevity we have left the code in its original C form. <p> Dominator and Postdominator. A node d is called a dominator of node n, if every path from entry (B) to n goes through d. Similarly, a node p is a postdominator of node n, if every path from n to exit (B) goes through d <ref> [2] </ref>. Data dependence. Let Def (n) and Use (n) be sets of variables defined and used by node n in B, respectively. <p> = fhL1; datai; hL9; cmdig. 6.2.2 Assigning Times to Subtasks Program slicing may well increase worst-case execution times of tasks for a number of reasons: (1) control structures are replicated and will be executed twice; (2) splitting a basic block may increase the number of register load and store operations <ref> [2] </ref>; and (3) worst-case execution time paths of the two resultant subtasks may be incorrectly derived.
Reference: [3] <author> A. Aiken and A. Nicolau. </author> <title> A development environment for horizontal microcode. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 584-594, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 5.3 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [3, 10, 12, 14, 38, 48] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling [12] or Percolation Scheduling <ref> [3, 10] </ref>. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility.
Reference: [4] <author> F. Allen, B. Rosen, and K. Zadeck. </author> <title> the forthcoming Optimization in Compilers. </title> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1992. </year>
Reference-contexts: The next two assumptions simplify problems induced by spurious data dependences such as anti-dependences and output dependences <ref> [4] </ref>. However, we can partially alleviate the restrictions, relying on dependence breaking transformations, such as scalar expansion [4]. Static single assignment is one such transformation. <p> The next two assumptions simplify problems induced by spurious data dependences such as anti-dependences and output dependences <ref> [4] </ref>. However, we can partially alleviate the restrictions, relying on dependence breaking transformations, such as scalar expansion [4]. Static single assignment is one such transformation.
Reference: [5] <author> N. Audsley. </author> <title> Optimal priority assignment and feasibility of static priority tasks with arbitrary start times. </title> <type> Technical Report YCS 164, </type> <institution> Department of Computer Science, University of York, </institution> <address> England, </address> <month> December </month> <year> 1991. </year>
Reference-contexts: Most recently, a group of researchers at the University of York developed a set of analytical techniques which can provide schedulability tests for broad classes of tasks, including those whose deadlines are greater than their periods. <ref> [5, 51, 50] </ref>. In this dissertation work we choose the York model, mainly because of its generality.
Reference: [6] <author> A. Burns. </author> <title> Fixed priority scheduling with deadlines prior to completion. </title> <type> Technical Report YCS 212 (1993), </type> <institution> Department of Computer Science, University of York, </institution> <address> England, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Its principal weakness is that the online component lacks the simplicity found in pure, static priority scheduling due to its semi-dynamic dual priority assignment. 64 Thus the following question arises: when can a set of transformed TCEL tasks be scheduled under a fully preemptive, static priority scheme? Burns <ref> [6] </ref> provides an answer to this question after identifying a simple, but essential fact about the TCEL task model. That is, whenever we let a task's deadline be greater than its period, this represents a relaxation of the classical rate-monotonic restrictions put forth in [34]. <p> In <ref> [6] </ref> Burns presents a search algorithm to generate the feasible static-priority order or to detect when no such order exists. Thus the approach includes the following components. Online Scheduler: This is a simple, preemptive dispatching mechanism, in which priority "ties" are broken in favor of the task dispatched first.
Reference: [7] <author> J.-D. Choi, R. Cytron, and J. Ferrante. </author> <title> On the efficient engineering of ambitious program analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(2) </volume> <pages> 105-114, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: The static single assignment (SSA) form of a program can be considered not only as a sparse representation of flow data dependences, but also as a notation where the spurious data dependences such as output and anti-dependences are eliminated <ref> [7, 8] </ref>. A program is defined to be in SSA form if each use of a variable is reached by exactly one assignment to it [8].
Reference: [8] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and F. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13 </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The static single assignment (SSA) form of a program can be considered not only as a sparse representation of flow data dependences, but also as a notation where the spurious data dependences such as output and anti-dependences are eliminated <ref> [7, 8] </ref>. A program is defined to be in SSA form if each use of a variable is reached by exactly one assignment to it [8]. <p> A program is defined to be in SSA form if each use of a variable is reached by exactly one assignment to it <ref> [8] </ref>. <p> Thus we take the following alternative approach, in which feasible tasks are synthesized in a two-step process section decomposition (Section 5.2) and code scheduling (Section 5.3). Section Decomposition. First the code is translated into its gated single assignment (GSA) form <ref> [8, 20] </ref>. This representation serves two purposes: (1) it yields a compact means of representing the data-dependence relation discussed in Chapter 4; and (2) GSA's convention of uniquely naming each variable assignment is precisely what we require in the code transformations phase. <p> For the sake of brevity, we assume the following: * Function calls are inlined. * Loops are unrolled. * The intermediate code of programs is translated into static single assignment form <ref> [8, 21, 16] </ref>. 59 The first assumption allows us to avoid interprocedural slicing [22]. The next two assumptions simplify problems induced by spurious data dependences such as anti-dependences and output dependences [4]. However, we can partially alleviate the restrictions, relying on dependence breaking transformations, such as scalar expansion [4].
Reference: [9] <author> B. Dasarathy. </author> <title> Timing constraints of real-time systems: Constructs for expressing them, method for validating them. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(1) </volume> <pages> 80-86, </pages> <month> Jan-uary </month> <year> 1985. </year>
Reference-contexts: Functional specifications define valid translations from inputs into outputs. As such they are realized by a set of programs, which consume CPU time. Temporal requirements, on the other hand, place upper and lower bounds between occurrences of events <ref> [9, 24] </ref>. An example is the robot arm must receive a next-position update every 10ms. Such a constraint arises from the system's requirements, or from a detailed analysis of the application environment. <p> The reason is straightforward: Language constructs such as "within 10ms do B" establish constraints on blocks of code. However, "true" real-time properties establish constraints between the occurrences of events <ref> [9, 24] </ref>. While language-based constraints are very sensitive to a program's execution time, specification-based constraints must be maintained regardless of the platform's CPU characteristics, memory cycle times, bus arbitration delays, etc. <p> Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." In this section we survey four such methodologies: Real-Time Logic [24], RTRL <ref> [9] </ref>, Timed IO Automata [37] and ACSR [29]. RTL. Real-Time Temporal Logic possesses an underlying event-action model. It captures the temporal ordering between an application's actions and its events. An action in RTL is the execution of an operation which consumes a certain amount of system resources.
Reference: [10] <author> K. Ebcioglu and A. Nicolau. </author> <title> A global resource-constrained parallelization technique. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 154-163. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1989. </year> <month> 85 </month>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 5.3 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [3, 10, 12, 14, 38, 48] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling [12] or Percolation Scheduling <ref> [3, 10] </ref>. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility.
Reference: [11] <author> J. Ferrante and K. Ottenstein. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9 </volume> <pages> 319-345, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: 2 is not nested within any loop or conditional; or 2. n 1 represents a control predicate and n 2 is immediately nested within the loop or the condi tional whose predicate is represented by n 1 . 23 Our definition of control dependence is simpler than that found in <ref> [11] </ref>, since it covers a restricted language possessing only structured program constructs. Reaching Definitions. <p> Thus the computation of slices is based on data dependence as well as control dependence. In this regard, using a program dependence graph <ref> [11, 22, 41] </ref> is ideal, since it represents both types of dependences in a single graph. (See Section 4.2 for definition.) The program dependence graph PDG of our controller task t 2 is shown in Figure 6.9. 1 We intentionally include L9, as will be discussed in Algorithm 6.1. 60 The
Reference: [12] <author> J. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30 </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: In this case, it is impossible for any scheduler to generate a feasible schedule for the task set. We develop a compiler transformation method to automatically correct such feasibility faults based on a variant of Trace Scheduling <ref> [12] </ref>. We name this method feasible code synthesis. In this method, the compiler picks the worst-case execution time paths of an infeasible task and moves unobservable code to eliminate the overload. The remainder of this chapter is organized as follows. <p> Then, the surrounding node is handled. If this level is found inconsistent, the inner nodes are "opened up" once again, and more aggressive optimization is carried out. The actual transformations are similar to those used in Trace Scheduling <ref> [12] </ref>. As the name implies, the Trace Scheduling algorithm works on specific traces: it selects a path (or trace) from a given code block, and then selects instructions on that path to move. <p> 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 5.3 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [3, 10, 12, 14, 38, 48] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling <ref> [12] </ref> or Percolation Scheduling [3, 10]. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility. <p> In turn it processes section S3, which may now contain newly moved code. To perform greedy code motion, we have adapted a technique from the approach to Trace Scheduling in <ref> [12] </ref>, and we use it as a component of the code scheduling algorithm. In our approach, nodes lying on paths that exceed their section's duration constraint are considered for code motion. We distinguish such paths as critical traces.
Reference: [13] <author> M. Garey and D. Johnson. </author> <title> Computer and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: Clause (3) means that the new code is feasible. This problem is NP-hard, due to the existence of immovable operations and data dependences. Theorem 5.1 Feasible code synthesis is NP-hard. Proof: The proof follows by a straightforward transformation from "Partition [SP12]" <ref> [13] </ref> to feasible code synthesis. Consider an instance (A; s) of Partition, where A = fa 1 ; a 2 ; : : : ; a n g is a set of elements, and where s : A 7! NN is the cost of each element.
Reference: [14] <author> F. Gasperoni. </author> <title> Compilation techniques for VLIW architectures. </title> <type> Technical Report RC 14915(#66741), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 5.3 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [3, 10, 12, 14, 38, 48] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput.
Reference: [15] <author> R. Gerber and S. Hong. </author> <title> Semantics-based compiler transformations for enhanced schedulabil-ity. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 232-242. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: If the task set is found unschedulable, the slicer algorithm is invoked to decompose t n into its two constituent threads, which then replace t n in . If the updated set is still deemed unschedulable, the procedure goes to work on t n1 , and so on. In <ref> [15] </ref> we present a detailed alternative to this approach, in which tasks are processed from t 1 to t n ; i.e., the first task found unschedulable is selected for slicing. One can imagine other alternatives as well. <p> In this section we present an analytic approach that systematically addresses (1) and (2). A static priority approach: In <ref> [15] </ref> we present a RMS based method which enjoys a simple priority assignment rule and analysis test for its dual-priority scheme. This is certainly one of its strengths.
Reference: [16] <author> R. Gerber and S. Hong. </author> <title> Compiling real-time programs with timing constraint refinement and structural code motion. </title> <journal> IEEE Transactions on Software Engineering, </journal> <note> 1995. To Appear. </note>
Reference-contexts: For the sake of brevity, we assume the following: * Function calls are inlined. * Loops are unrolled. * The intermediate code of programs is translated into static single assignment form <ref> [8, 21, 16] </ref>. 59 The first assumption allows us to avoid interprocedural slicing [22]. The next two assumptions simplify problems induced by spurious data dependences such as anti-dependences and output dependences [4]. However, we can partially alleviate the restrictions, relying on dependence breaking transformations, such as scalar expansion [4].
Reference: [17] <author> R. Gerber, S. Hong, and M. Saksena. </author> <title> Guaranteeing end-to-end timing constraints by calibrating intermediate processes. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 192-203. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1994. </year> <note> Also to appear in IEEE Transactions on Software Engineering. </note>
Reference-contexts: Although our compiler-based approach provides a partial solution by means of a new programming language, we plan to push it into a higher level in the design hierarchy. We recently laid the foundation to achieve this goal, via an automated design methodology that works hand-in-hand with real-time design tools <ref> [17] </ref>. We call this end-to-end design. The methodology links two more phases in the design hierarchy, i.e., a high-level component derives intermediate constraints using the low-level tuning tools. The result is a way to aid pro 83 grammers in decomposing tasks and deriving the intermediate rate constraints.
Reference: [18] <author> P. Gopinath and R. Gupta. </author> <title> Applying compiler techniques to scheduling in real-time systems. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 247-256. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1990. </year>
Reference-contexts: The result of the analysis affects that of scheduling, and vice versa. Compiler-Assisted Adaptive Scheduling. Gopinath and Gupta introduced a technique called compiler-assisted adaptive scheduling in <ref> [18] </ref>. In their work, the compiler indexes a piece of code into four classes on the basis of predictability and monotonicity. Then it rearranges the code to support adaptive run-time scheduling.
Reference: [19] <author> M. Harmon, T. Baker, and D. Whalley. </author> <title> A retargetable technique for predicting execution time. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 68-77. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1992. </year>
Reference-contexts: The technique reported in [42] is based on a simple source-level timing schema, and it is fairly straightforward to implement in a tool. In <ref> [19] </ref> another approach for more accurate timing was proposed. Since the resulting tool is able to analyze micro-instruction streams using machine-description rules, it is retargetable to various architectures. <p> (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [19, 32, 42, 47, 55] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [20] <author> P. Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Such a coupling will be avoidable, if we parameterize the -function with the associated predicate. Gated single assignment (GSA) form solves this problem <ref> [20] </ref>. GSA form is an extension of SSA form with new functions that encode control over value assignment. <p> defined in a loop body, a definition v 0 = (v init ; v iter ) is inserted at the loop header, where v init is the initial value of v reaching the header from outside and v iter is the iterative value reaching along the back-edge of the loop <ref> [20] </ref>. <p> Thus we take the following alternative approach, in which feasible tasks are synthesized in a two-step process section decomposition (Section 5.2) and code scheduling (Section 5.3). Section Decomposition. First the code is translated into its gated single assignment (GSA) form <ref> [8, 20] </ref>. This representation serves two purposes: (1) it yields a compact means of representing the data-dependence relation discussed in Chapter 4; and (2) GSA's convention of uniquely naming each variable assignment is precisely what we require in the code transformations phase.
Reference: [21] <author> S. Hong and R. Gerber. </author> <title> Compiling real-time programs into schedulable code. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation. </booktitle> <publisher> ACM Press, </publisher> <month> June </month> <year> 1993. </year> <journal> SIGPLAN Notices, </journal> <volume> 28(6) </volume> <pages> 166-176. </pages>
Reference-contexts: For the sake of brevity, we assume the following: * Function calls are inlined. * Loops are unrolled. * The intermediate code of programs is translated into static single assignment form <ref> [8, 21, 16] </ref>. 59 The first assumption allows us to avoid interprocedural slicing [22]. The next two assumptions simplify problems induced by spurious data dependences such as anti-dependences and output dependences [4]. However, we can partially alleviate the restrictions, relying on dependence breaking transformations, such as scalar expansion [4].
Reference: [22] <author> S. Horwitz, T. Reps, and D. Binkley. </author> <title> Interprocedural slicing using dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12 </volume> <pages> 26-60, </pages> <month> January </month> <year> 1990. </year> <month> 86 </month>
Reference-contexts: For the sake of brevity, we assume the following: * Function calls are inlined. * Loops are unrolled. * The intermediate code of programs is translated into static single assignment form [8, 21, 16]. 59 The first assumption allows us to avoid interprocedural slicing <ref> [22] </ref>. The next two assumptions simplify problems induced by spurious data dependences such as anti-dependences and output dependences [4]. However, we can partially alleviate the restrictions, relying on dependence breaking transformations, such as scalar expansion [4]. Static single assignment is one such transformation. <p> Thus the computation of slices is based on data dependence as well as control dependence. In this regard, using a program dependence graph <ref> [11, 22, 41] </ref> is ideal, since it represents both types of dependences in a single graph. (See Section 4.2 for definition.) The program dependence graph PDG of our controller task t 2 is shown in Figure 6.9. 1 We intentionally include L9, as will be discussed in Algorithm 6.1. 60 The
Reference: [23] <author> Y. Ishikawa, H. Tokuda, and C. Mercer. </author> <title> Object-oriented real-time language design: Constructs for timing constraints. </title> <booktitle> In Proceedings of OOPSLA-90, </booktitle> <pages> pages 289-298, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: This allows us to define a class of semantics-preserving transformations for them. Unfortunately, most existing real-time programming languages do not fulfill the requirement. We conceive this as a problem of "code-based specifications." Consider experimental real-time programming languages which have been proposed in the literature <ref> [23, 27, 30, 33, 40] </ref>. They provide high-level real-time constructs such as "within 10ms do B," where the block of code "B" must be executed within a 10 millisecond time frame.
Reference: [24] <author> F. Jahanian and A. Mok. </author> <title> Safety analysis of timing properties in real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(9) </volume> <pages> 890-904, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Functional specifications define valid translations from inputs into outputs. As such they are realized by a set of programs, which consume CPU time. Temporal requirements, on the other hand, place upper and lower bounds between occurrences of events <ref> [9, 24] </ref>. An example is the robot arm must receive a next-position update every 10ms. Such a constraint arises from the system's requirements, or from a detailed analysis of the application environment. <p> The reason is straightforward: Language constructs such as "within 10ms do B" establish constraints on blocks of code. However, "true" real-time properties establish constraints between the occurrences of events <ref> [9, 24] </ref>. While language-based constraints are very sensitive to a program's execution time, specification-based constraints must be maintained regardless of the platform's CPU characteristics, memory cycle times, bus arbitration delays, etc. <p> Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." In this section we survey four such methodologies: Real-Time Logic <ref> [24] </ref>, RTRL [9], Timed IO Automata [37] and ACSR [29]. RTL. Real-Time Temporal Logic possesses an underlying event-action model. It captures the temporal ordering between an application's actions and its events. An action in RTL is the execution of an operation which consumes a certain amount of system resources.
Reference: [25] <author> K. Kenny and K. Lin. </author> <title> Building flexible real-time systems using the Flex language. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 70-78, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Similarly, let S4.start and S4.finish represent the start and finish times of section S4. Using these variables we can represent the section decomposition of a TCEL construct in a manner similar to that found in the Flex language <ref> [25] </ref>. Recall the flight controller program from Figure 3.3. Figure 5.3 illustrates its constituent sections. The constraint-expression for S6 corresponds to the program's outer, periodic loop.
Reference: [26] <author> M. Kim. Hyperstories: </author> <title> Combining time, space and asynchrony in multimedia documents. </title> <type> Technical Report RC 19277 (#83726), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Indeed, perhaps a graphical language may prove better suited to this purpose. Though their application domain is different from ours, several recent multimedia tools have been developed, which allow users to specify temporal relationships between several continuous media <ref> [26] </ref>. We plan to borrow some of the ideas used in these languages to graphically describe timing relationships in a real-time program.
Reference: [27] <author> E. Kligerman and A. Stoyenko. </author> <title> Real-Time Euclid: A language for reliable real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12 </volume> <pages> 941-949, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: This allows us to define a class of semantics-preserving transformations for them. Unfortunately, most existing real-time programming languages do not fulfill the requirement. We conceive this as a problem of "code-based specifications." Consider experimental real-time programming languages which have been proposed in the literature <ref> [23, 27, 30, 33, 40] </ref>. They provide high-level real-time constructs such as "within 10ms do B," where the block of code "B" must be executed within a 10 millisecond time frame. <p> In this section we survey tools, and then show where we can place our tools in the realm of real-time programming. Schedulability Analyzer for Real-Time Euclid. Among the early approaches was the schedu-lability analyzer [49], specifically developed for the timing and scheduling analyses of programs written in Real-Time Euclid <ref> [27] </ref>. The schedulability analyzer consists of a front-end and a back-end. The front-end is incorporated into the code generator of the Real-Time Euclid compiler, and it produces a segment tree that represents compilation units such as modules, monitors or routines.
Reference: [28] <author> J. Krause. </author> <title> GN&C domain modeling: Functionality requirements for fixed rate algorithms. </title> <type> Technical Report (DRAFT) version 0.2, </type> <institution> Honeywell Systems and Research Center, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: failure Period Constraints control software possesses many representative properties that can be found in other applications (e.g., multimedia, vision, etc.), our approach can be easily adopted to other types of real-time systems. 6.1.1 Characterization of Discrete Control Software Many discrete control algorithms possess computations that fit a fixed-rate algorithm paradigm <ref> [28] </ref>, i.e., control-loops which execute repetitively with fixed periods. During each period, the physical world measurement data is sampled, and then actuator commands are computed. Meanwhile, a set of states is updated based on the current state and the sampled data.
Reference: [29] <author> I. Lee, P. Bremond-Gregoire, and R. Gerber. </author> <title> A process algebraic approach to the specification and analysis of resource-bound real-time systems. </title> <journal> IEEE Proceedings, </journal> <volume> 82(1), </volume> <month> January </month> <year> 1994. </year>
Reference-contexts: Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." In this section we survey four such methodologies: Real-Time Logic [24], RTRL [9], Timed IO Automata [37] and ACSR <ref> [29] </ref>. RTL. Real-Time Temporal Logic possesses an underlying event-action model. It captures the temporal ordering between an application's actions and its events. An action in RTL is the execution of an operation which consumes a certain amount of system resources.
Reference: [30] <author> I. Lee and V. Gehlot. </author> <title> Language constructs for real-time programming. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 57-66. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1985. </year>
Reference-contexts: This allows us to define a class of semantics-preserving transformations for them. Unfortunately, most existing real-time programming languages do not fulfill the requirement. We conceive this as a problem of "code-based specifications." Consider experimental real-time programming languages which have been proposed in the literature <ref> [23, 27, 30, 33, 40] </ref>. They provide high-level real-time constructs such as "within 10ms do B," where the block of code "B" must be executed within a 10 millisecond time frame. <p> Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [33, 30, 54] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives. <p> Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in [33, 30, 54]), or they are postulated in a separate interface, and then passed to the scheduler as directives. A common language-based approach (first 11 presented in <ref> [30] </ref>) is to provide constructs such as "within t do f: : : g," "at t do f: : : g" and "after t do f: : : g." An alternative, taken in [33], is to set up linear constraint expressions on the the start times and deadlines of code blocks. <p> To this end, we embedded timing constructs in C, and then extended it as a real-time programming language. Our constructs are syntactic descendents of the temporal scope in <ref> [30] </ref>. * To keep the definition of an observable event as general as possible. The notion of an observable event is a relative term depending on the application system.
Reference: [31] <author> J. Leung and M. Merill. </author> <title> A note on the preemptive scheduling of periodic, real-time tasks. </title> <journal> Information Processing Letters, </journal> <volume> 11(3) </volume> <pages> 115-118, </pages> <month> November </month> <year> 1980. </year>
Reference-contexts: However, their algorithm is applicable only to the periodic task model where tasks have fixed periods, deadlines are equal to periods, and tasks are totally independent of each other. Recent research has made significant enhancements to this model which relaxes the original restrictions. In <ref> [31] </ref> Leung and Merrill showed that deadline monotonic priority assignment is also optimal where deadlines are shorter than periods. In [45] Sha et al. presented two protocols which enable tasks to interact via shared resources, while still guaranteeing the tasks' deadlines.
Reference: [32] <author> S. Lim, Y. Bae, C. Jang, B. Rhee, S. Min, C. Park, H. Shin, K. Park, and C. Kim. </author> <title> An accurate worst case timing analysis for risc processors. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 97-108. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1994. </year>
Reference-contexts: In [44] Arnold et al. developed a 13 timing prediction method called static cache simulation to statically analyze memory and cache reference patterns. A similar but more advanced approach was reported in <ref> [32] </ref>. While the latter approach is able to predict pipeline stalls as well, both approaches essentially rely on attribute grammars [2] to propagate cache hit information backward in a flow graph. However, no static timing tool is precise enough to be used with complete confidence for developing production-quality software. <p> However, no static timing tool is precise enough to be used with complete confidence for developing production-quality software. Moreover, even sophisticated timing analysis methods such as <ref> [32, 44] </ref> are not appropriate for fine-grained instruction timing. In Chapter 7 we explain how we can effectively use these tools in spite of the limitations, by also taking advantage of software profiling, as well as static timing prediction. <p> (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [19, 32, 42, 47, 55] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [33] <author> K. Lin and S. Natarajan. </author> <title> Expressing and maintaining timing constraints in FLEX. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1988. </year>
Reference-contexts: This allows us to define a class of semantics-preserving transformations for them. Unfortunately, most existing real-time programming languages do not fulfill the requirement. We conceive this as a problem of "code-based specifications." Consider experimental real-time programming languages which have been proposed in the literature <ref> [23, 27, 30, 33, 40] </ref>. They provide high-level real-time constructs such as "within 10ms do B," where the block of code "B" must be executed within a 10 millisecond time frame. <p> Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [33, 30, 54] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives. <p> A common language-based approach (first 11 presented in [30]) is to provide constructs such as "within t do f: : : g," "at t do f: : : g" and "after t do f: : : g." An alternative, taken in <ref> [33] </ref>, is to set up linear constraint expressions on the the start times and deadlines of code blocks. We have borrowed from both approaches: in the TCEL source we use the higher-level constructs, while in our intermediate code we make use of the constraint representation.
Reference: [34] <author> C. Liu and J. Layland. </author> <title> Scheduling algorithm for multiprogramming in a hard real-time environment. </title> <journal> Journal of the ACM, </journal> <volume> 20(1) </volume> <pages> 46-61, </pages> <month> January </month> <year> 1973. </year>
Reference-contexts: Fixed-priority, preemptive scheduling algorithms are well-suited for control domain applications, not only because they possess periodic behavior, but also because efficient schedulability tests can be applied. Rate-monotonic scheduling, originally developed by Lui and Layland, is the first well-known algorithm of this kind. In their seminal paper <ref> [34] </ref> they proposed a priority assignment algorithm, in which a task with the shorter period is assigned the higher priority (hence the name rate-monotonic scheduling (RMS)). <p> That is, whenever we let a task's deadline be greater than its period, this represents a relaxation of the classical rate-monotonic restrictions put forth in <ref> [34] </ref>. Thus the rate-monotonic priority assignment may not be the optimal one.
Reference: [35] <author> C. Locke, D. Vogel, and T Mesler. </author> <title> Building a predictable avionics platform in ada: A case study. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 181-189. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1991. </year> <month> 87 </month>
Reference-contexts: Otherwise, the algorithm slices t and sees if t 0 is feasible. If t 0 is deemed feasible, it returns L@[t 0 ]. 6.4.3 A Larger Example We constructed a task set consisting of eighteen periodic tasks, based on the avionics application described in <ref> [35, 51] </ref>. We added tighter deadlines to the original task set, and modified the execution times of some of the tasks. As a result, the task set had utilization of 0.836, and it was unschedulable.
Reference: [36] <author> T. Marlowe and S. Masticola. </author> <title> Safe optimization for hard real-time programming. </title> <booktitle> In Second International Conference on Systems Integration, </booktitle> <pages> pages 438-446, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Safe Real-Time Code Optimizations. Conventional compiler optimizations are designed to reduce the expected or average execution time of programs without taking in account the exact timing behavior of programs. On the other hand, optimizations for hard real-time must meet stringent timing constraints. In <ref> [36] </ref> Marlowe and Masticola examine a large class of conventional code transformations, and then classify them for application in real-time programming by the notion of safe real-time code transformations.
Reference: [37] <author> M. Merritt, F. Modungo, and M. Tuttle. </author> <title> Time-Constrained Automata. </title> <booktitle> In CONCUR '91, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." In this section we survey four such methodologies: Real-Time Logic [24], RTRL [9], Timed IO Automata <ref> [37] </ref> and ACSR [29]. RTL. Real-Time Temporal Logic possesses an underlying event-action model. It captures the temporal ordering between an application's actions and its events. An action in RTL is the execution of an operation which consumes a certain amount of system resources.
Reference: [38] <author> A. Nicolau. </author> <title> Parallelism, Memory Anti-aliasing and Correctness Issues for a Trace Scheduling Compiler. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> June </month> <year> 1984. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 5.3 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [3, 10, 12, 14, 38, 48] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput.
Reference: [39] <author> V. Nirkhe. </author> <title> Application of Partial Evaluation to Hard Real-Time Programming. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Maryland at College Park, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Partial evaluation may conceptually be understood as compile-time eval-uation of constant expressions. Partial evaluation of a source program not only reduces constant expressions in the program into simple values, but also simplifies some control structures such as loops and conditionals. In <ref> [39] </ref> Nirke and Pugh applied the technique of partial evaluation to real-time programming to produce residual code that is both more optimized and more deterministic. Safe Real-Time Code Optimizations.
Reference: [40] <author> V. Nirkhe and W. Pugh. </author> <title> Partial evaluator for the Maruti hard real-time system. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 64-73. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1991. </year>
Reference-contexts: This allows us to define a class of semantics-preserving transformations for them. Unfortunately, most existing real-time programming languages do not fulfill the requirement. We conceive this as a problem of "code-based specifications." Consider experimental real-time programming languages which have been proposed in the literature <ref> [23, 27, 30, 33, 40] </ref>. They provide high-level real-time constructs such as "within 10ms do B," where the block of code "B" must be executed within a 10 millisecond time frame.
Reference: [41] <author> K. Ottenstein and L. Ottenstein. </author> <title> The program dependence graph in a software development environment. </title> <booktitle> In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pages 177-184, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: We can realize this benefit by transforming a task, so that its time-sensitive component always executes within its frame, while postponing the rest of the task. To systematically carry out the transformation, we harness a novel application of program slicing <ref> [41, 52, 53] </ref>. An unschedulable task is decomposed into two subthreads: one that is "time-critical" and the other "unobservable." The unobservable subthread is then appended to the end of time-critical thread, with TCEL semantics being maintained. Figure 1.4 pictorially illustrates the net effect of this transformation. <p> Many factors make this the case, among which are intertwined threads of control, nested control structures, complex data dependences between statements, procedure calls in the task code, etc. To cope with these problems in a systematic manner, we harness a novel application of program slicing <ref> [41, 52, 53] </ref>. For the sake of brevity, we assume the following: * Function calls are inlined. * Loops are unrolled. * The intermediate code of programs is translated into static single assignment form [8, 21, 16]. 59 The first assumption allows us to avoid interprocedural slicing [22]. <p> Thus the computation of slices is based on data dependence as well as control dependence. In this regard, using a program dependence graph <ref> [11, 22, 41] </ref> is ideal, since it represents both types of dependences in a single graph. (See Section 4.2 for definition.) The program dependence graph PDG of our controller task t 2 is shown in Figure 6.9. 1 We intentionally include L9, as will be discussed in Algorithm 6.1. 60 The
Reference: [42] <author> C. Park and A. Shaw. </author> <title> Experimenting with a program timing tool based on source-level timing schema. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 72-81. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1990. </year>
Reference-contexts: Indeed, the results of our compiler-based methods rely on timing tools, since both the feasibility and schedulability of tasks are a function of predicted worst-case executions of the transformed tasks. The technique reported in <ref> [42] </ref> is based on a simple source-level timing schema, and it is fairly straightforward to implement in a tool. In [19] another approach for more accurate timing was proposed. Since the resulting tool is able to analyze micro-instruction streams using machine-description rules, it is retargetable to various architectures. <p> (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [19, 32, 42, 47, 55] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [43] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation. </booktitle> <publisher> ACM Press, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: However, dependence analyzers are improving at a rapid rate, and our algorithm will improve along with them. For example, if we incorporate the recent advances in loop dependence analyses such as those in the Omega Test <ref> [43] </ref>, we may not have to unroll loops to slice a real-time task. We can obtain better slices for loops using techniques like loop distribution. 7.1.2 Limits of Timing Analysis Another limiting factor is the difficulty of achieving accurate, static timing analysis in the face of more complicated architectures.
Reference: [44] <author> D. Whalley R. Arnold, F. Mueller. </author> <title> Bounding worst-case instruction cache performance. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 172-181. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1994. </year>
Reference-contexts: In [55] Zhang et al. presented a timing analyzer based on a mathematical model of the pipelined Intel 80C188 processor. This analysis method is able to take into account the overlap between instruction execution and fetching, which is an improvement over schemes where instruction executions are treated individually. In <ref> [44] </ref> Arnold et al. developed a 13 timing prediction method called static cache simulation to statically analyze memory and cache reference patterns. A similar but more advanced approach was reported in [32]. <p> However, no static timing tool is precise enough to be used with complete confidence for developing production-quality software. Moreover, even sophisticated timing analysis methods such as <ref> [32, 44] </ref> are not appropriate for fine-grained instruction timing. In Chapter 7 we explain how we can effectively use these tools in spite of the limitations, by also taking advantage of software profiling, as well as static timing prediction.
Reference: [45] <author> L. Sha, R. Rajkumar, and J. Lehoczky. </author> <title> Priority inheritance protocols: An approach to real-time synchronization. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 39 </volume> <pages> 1175-1185, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Recent research has made significant enhancements to this model which relaxes the original restrictions. In [31] Leung and Merrill showed that deadline monotonic priority assignment is also optimal where deadlines are shorter than periods. In <ref> [45] </ref> Sha et al. presented two protocols which enable tasks to interact via shared resources, while still guaranteeing the tasks' deadlines.
Reference: [46] <author> U. Shankar. </author> <title> A simple assertional proof system for real-time systems. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 167-176. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1992. </year>
Reference-contexts: A timed I/O automaton has a set of timing assumptions, each of which specifies an event set and an interval during which the event set can be continuously enabled since its last occurrence. Thus the timing assumptions place constraints on the event-firing times <ref> [46] </ref>. A "behavioral abstraction" of a timed I/O automaton allows reasoning about only the event sequences, since local state information can be ignored in the abstraction. ACSR. The computation model of ACSR (Algebra of Communicating Shared Resources) addresses two key issues in modeling a real-time system: concurrency and communication.
Reference: [47] <author> A. Shaw. </author> <title> Reasoning about time in higher level language software. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 875-889, </pages> <month> July </month> <year> 1989. </year> <month> 88 </month>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [19, 32, 42, 47, 55] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [48] <author> M. Smith, M. Horowitz, and M. Lam. </author> <title> Efficient superscalar performance through boosting. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 248-259. </pages> <publisher> ACM Press, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next section we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 5.3 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [3, 10, 12, 14, 38, 48] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput.
Reference: [49] <author> A. Stoyenko. </author> <title> A schedulability analyzer for Real-Time Euclid. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 218-227. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1987. </year>
Reference-contexts: However, they share a common goal, namely, enhancing predictability and schedulability of real-time applications. In this section we survey tools, and then show where we can place our tools in the realm of real-time programming. Schedulability Analyzer for Real-Time Euclid. Among the early approaches was the schedu-lability analyzer <ref> [49] </ref>, specifically developed for the timing and scheduling analyses of programs written in Real-Time Euclid [27]. The schedulability analyzer consists of a front-end and a back-end.
Reference: [50] <author> K. Tindell. </author> <title> Using offset information to analyse static priority pre-emptively scheduled task sets. </title> <type> Technical Report YCS 182 (1992), </type> <institution> Department of Computer Science, University of York, </institution> <address> England, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Most recently, a group of researchers at the University of York developed a set of analytical techniques which can provide schedulability tests for broad classes of tasks, including those whose deadlines are greater than their periods. <ref> [5, 51, 50] </ref>. In this dissertation work we choose the York model, mainly because of its generality.
Reference: [51] <author> K. Tindell, A. Burns, and A. Wellings. </author> <title> An extendible approach for analysing fixed priority hard real-time tasks. </title> <journal> The Journal of Real-Time Systems, </journal> <volume> 6(2) </volume> <pages> 133-152, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Most recently, a group of researchers at the University of York developed a set of analytical techniques which can provide schedulability tests for broad classes of tasks, including those whose deadlines are greater than their periods. <ref> [5, 51, 50] </ref>. In this dissertation work we choose the York model, mainly because of its generality. <p> This is because uncompleted iterations of t i can now interfere with the current one. In this case the following general equation is used instead as discussed in <ref> [51] </ref>. R i = max fr i;q q T i g (Eq6.2) where r i;q = (q + 1)c i + t j 2hp (i) r i;q ec j Consider the case of three periodic tasks, where the source of task t 2 is given in Figure 6.5. <p> Although q is denoted as an unbounded number in Eq 6.3, it can be trivially shown that there exists bounded response time R IO i as long as utilization of tasks in hp (i) and t i is less than 100% <ref> [51] </ref>. <p> Otherwise, the algorithm slices t and sees if t 0 is feasible. If t 0 is deemed feasible, it returns L@[t 0 ]. 6.4.3 A Larger Example We constructed a task set consisting of eighteen periodic tasks, based on the avionics application described in <ref> [35, 51] </ref>. We added tighter deadlines to the original task set, and modified the execution times of some of the tasks. As a result, the task set had utilization of 0.836, and it was unschedulable.
Reference: [52] <author> G. Venkatesh. </author> <title> The semantic approach to program slicing. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: We can realize this benefit by transforming a task, so that its time-sensitive component always executes within its frame, while postponing the rest of the task. To systematically carry out the transformation, we harness a novel application of program slicing <ref> [41, 52, 53] </ref>. An unschedulable task is decomposed into two subthreads: one that is "time-critical" and the other "unobservable." The unobservable subthread is then appended to the end of time-critical thread, with TCEL semantics being maintained. Figure 1.4 pictorially illustrates the net effect of this transformation. <p> DDC (n; B) = fm 2 V j m * + ng [ fng In fact, a dependence closure and a data dependence closure are equivalent to a static backward program slice and a static data slice, respectively <ref> [52] </ref>. Program Dependence Graph. A program dependence graph is an intermediate program rep resentation that stores both the data and control dependences for each operation in a program. <p> Many factors make this the case, among which are intertwined threads of control, nested control structures, complex data dependences between statements, procedure calls in the task code, etc. To cope with these problems in a systematic manner, we harness a novel application of program slicing <ref> [41, 52, 53] </ref>. For the sake of brevity, we assume the following: * Function calls are inlined. * Loops are unrolled. * The intermediate code of programs is translated into static single assignment form [8, 21, 16]. 59 The first assumption allows us to avoid interprocedural slicing [22].
Reference: [53] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 10 </volume> <pages> 352-357, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: We can realize this benefit by transforming a task, so that its time-sensitive component always executes within its frame, while postponing the rest of the task. To systematically carry out the transformation, we harness a novel application of program slicing <ref> [41, 52, 53] </ref>. An unschedulable task is decomposed into two subthreads: one that is "time-critical" and the other "unobservable." The unobservable subthread is then appended to the end of time-critical thread, with TCEL semantics being maintained. Figure 1.4 pictorially illustrates the net effect of this transformation. <p> Many factors make this the case, among which are intertwined threads of control, nested control structures, complex data dependences between statements, procedure calls in the task code, etc. To cope with these problems in a systematic manner, we harness a novel application of program slicing <ref> [41, 52, 53] </ref>. For the sake of brevity, we assume the following: * Function calls are inlined. * Loops are unrolled. * The intermediate code of programs is translated into static single assignment form [8, 21, 16]. 59 The first assumption allows us to avoid interprocedural slicing [22].
Reference: [54] <author> V. Wolfe, S. Davidson, and I. Lee. RTC: </author> <title> Language support for real-time concurrency. </title> <booktitle> In Proceedings of IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 43-52. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1991. </year>
Reference-contexts: Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [33, 30, 54] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives.
Reference: [55] <author> N. Zhang, A. Burns, and M. Nicholson. </author> <title> Pipelined processors and worst case execution times. </title> <journal> The Journal of Real-Time Systems, </journal> <volume> 5(4), </volume> <month> October </month> <year> 1993. </year> <month> 89 </month>
Reference-contexts: On the other hand, both approaches do not address the problem of predicting architecture-specific timing behaviors due to various latencies in the memory hierarchy and pipelines. Recently, several approaches have been developed to account for this timing variance. In <ref> [55] </ref> Zhang et al. presented a timing analyzer based on a mathematical model of the pipelined Intel 80C188 processor. This analysis method is able to take into account the overlap between instruction execution and fetching, which is an improvement over schemes where instruction executions are treated individually. <p> (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [19, 32, 42, 47, 55] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
References-found: 55

