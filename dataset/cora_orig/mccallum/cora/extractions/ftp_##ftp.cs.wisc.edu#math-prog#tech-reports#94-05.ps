URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/94-05.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: STABILITY PROPERTIES OF THE GRADIENT PROJECTION METHOD WITH APPLICATIONS TO THE BACKPROPAGATION ALGORITHM  
Author: M. V. Solodov and S. K. Zavriev 
Keyword: KEY WORDS. gradient projection, error-stability, parallelization, backpropagation convergence.  
Date: June 6, 1994  
Abstract: Convergence properties of the generalized gradient projection algorithm in the presence of data perturbations are investigated. It is shown that every trajectory of the method is attracted, in a certain sense, to an "-stationary set of the problem, where " depends on the magnitude of the perturbations. Estimates for the attraction sets of the iterates are given in the general (nonsmooth and nonconvex) case. In the convex case, our results imply convergence to an *-optimal set. The results are further strengthened for weakly sharp and strongly convex problems. Convergence of the parallel algorithm in the case of the additive objective function is established. One of the principal applications of our results is the stability analysis of the classical backpropagation algorithm for training artificial neural networks. fl The first author is supported by Air Force Office of Scientific Research Grant F49620-94-1-0036 and National Science Foundation Grant CCR-9101801. y Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madi-son, WI 53706, U.S.A. Email : solodov@cs.wisc.edu. z Center for the Mathematical Sciences, University of Wisconsin, Madison, WI 53715. Ful-bright Scholar, on leave from Operations Research Department, Faculty of Computational Mathematics and Cybernetics, Moscow State University, Moscow, Russia, 119899. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.V. Burke and M.C. Ferris. </author> <title> Weak sharp minima in mathematical programming. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31(5) </volume> <pages> 1340-1359, </pages> <year> 1993. </year> <month> 18 </month>
Reference-contexts: These include convex and strongly convex problems, and problems with weak sharp minima <ref> [16, 1] </ref>. We start with the following lemma. Lemma 4.1 Let "(x) maxf"; r (x)g 8x 2 X; where " 0; 1 &gt; 0. Then X stat ("()) X stat ("): 12 In particular, if " = 0, then X stat ("()) = X stat : Proof. <p> This establishes the first two assertions of the lemma. For the last assertion, just note that (Lemma 1.4.3, [16]) for any x 2 X 2l (f (x) min f (y)) k@f (x)k 2 : Definition 4.1 <ref> [1] </ref> We say that problem (1.1) is weakly sharp with parameter &gt; 0 if f (x) min f (y) d (x; X opt ) 8x 2 X: We have the following important corollary. Corollary 4.1 Let f () be convex on X.
Reference: [2] <author> F.H. Clarke. </author> <title> Optimization and Nonsmooth Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction We consider the following general optimization problem min f (x);(1.1) where X is a convex compact set in &lt; n , and the objective function f : X ! &lt; is at least Lipschitz continuous on X and regular (in the sense of Clarke, <ref> [2] </ref>). <p> sets of problem (1.1) respectively, that is X opt = fx 2 X j f (x) = min f (y)g; X stat = fx 2 X j 0 2 @f (x) + N X (x)g; where @f (x) is the set of all generalized gradients (in the sense of Clarke, <ref> [2] </ref>) of f () at x, and N X (x) &lt; n is the normal cone to the set X at the point x 2 X : N X (x) = fy 2 &lt; n j 8z 2 X hy; z xi 0g: The following notions will play an important role
Reference: [3] <author> P. A. Dorofeev. </author> <title> On some properties of quasi-gradient method. USSR Computational Mathematics and Mathematical Physics, </title> <publisher> Pergamon Press, </publisher> <pages> 25 181-189, </pages> <year> 1985. </year>
Reference-contexts: Remark 3.1. Theorems 3.1,3.2 generalize the results on convergence properties of the generalized gradient projection method obtained in <ref> [13, 3, 25] </ref>. 4 Important Special Cases In this section we consider the standard optimization problem (1.1), and establish stronger convergence properties of GGPM in a number of important special cases. These include convex and strongly convex problems, and problems with weak sharp minima [16, 1].
Reference: [4] <author> D. Girard and H. Paugam-Moisy. </author> <title> Strategies for weight updating for parallel backpropagation. </title> <type> Technical Report 93-39, </type> <institution> Laboratoire de l'Informatique du Parallelisme, Ecole Normale Superieure de Lyon, </institution> <address> 46, Allee d'Italie, 69364 Lyon Cedex 07, France, </address> <year> 1993. </year>
Reference-contexts: We first describe our notation for stating and establishing convergence of the parallel perturbed generalized gradient projection method (GGPM) for solving (1.5) and its modifications. The type of parallelization considered here is primarily motivated by neural network training (see <ref> [11, 14, 4] </ref>). Another related work is [21]. We first consider the most general case. <p> Thus BP is a special case of Algorithm 3.1. Many other computationally important BP modifications, such as parallel BP <ref> [11, 14, 4] </ref>, BP with momentum term [7], and BP with varying smoothing parameter [20] all fall within the framework of Section 3. We now discuss stability issues in neural network training.
Reference: [5] <author> L. Grippo. </author> <title> A class of unconstrained minimization methods for neural network training. Optimization Methods and Software, </title> <note> 1994. to appear. </note>
Reference-contexts: It is therefore important to provide rigorous mathematical foundation to neural networks theory and algorithms. Stochastic analysis of BP is given in [22]. The first deterministic convergence results (without data perturbations) were recently obtained in [11, 8]. An interesting new training method is proposed in <ref> [5] </ref>. In this Section we give a precise characterization to empirically observed stability of neural networks [19, 6]. We also discuss BP modifications with varying smoothing parameter.
Reference: [6] <author> B. Hassibi and D.G. Stork. </author> <title> Optimal brain sergeon. </title> <editor> In G. Tesauro J.D. Cowan and J. Al-spector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Francisco, CA, 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Stochastic analysis of BP is given in [22]. The first deterministic convergence results (without data perturbations) were recently obtained in [11, 8]. An interesting new training method is proposed in [5]. In this Section we give a precise characterization to empirically observed stability of neural networks <ref> [19, 6] </ref>. We also discuss BP modifications with varying smoothing parameter. <p> This in turn yields the guaranteed "()-stationarity of all the accumulation points of the BP iterates. As another source of perturbations in the neural network training, we note the technique presented in <ref> [6] </ref>. To simplify the network topology and improve the network generalization properties, it is proposed in [6] to eliminate at the late stages of training the arcs with sufficiently small weights. The latter is equivalent to setting the corresponding weights to zero, and can also be treated as induced perturbations. <p> This in turn yields the guaranteed "()-stationarity of all the accumulation points of the BP iterates. As another source of perturbations in the neural network training, we note the technique presented in <ref> [6] </ref>. To simplify the network topology and improve the network generalization properties, it is proposed in [6] to eliminate at the late stages of training the arcs with sufficiently small weights. The latter is equivalent to setting the corresponding weights to zero, and can also be treated as induced perturbations. Another possible application of our analysis is devising new algorithms with varying smoothing parameter ff.
Reference: [7] <author> T. Khanna. </author> <title> Foundations of neural networks. </title> <publisher> Addison-Wesley, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: Now applying Theorem 2.1 and Corollary 2.1, we immediately obtain the desired results. Adding the "heavy ball" term [16] in Algorithm 3.1, we arrive at the following modification of the parallel GGPM. In neural network literature, methods of this type are usually referred to as backpropagation with momentum term <ref> [7, 10] </ref>. Algorithm 3.2 (Parallel GGPM with Momentum term). Start with any x 0 2 X. <p> Then every sequence fx i g generated by Algorithm 4.1 converges to X opt (" 2 =2l). 5 Backpropagation With Noise In this Section we apply the results of Section 3 to reveal some important properties of the backpropagation (BP) algorithm for training artificial neural networks <ref> [18, 7] </ref>. Due to numerous successful applications, a lot of empirical knowledge has been accumulated in the neural networks field. It is therefore important to provide rigorous mathematical foundation to neural networks theory and algorithms. Stochastic analysis of BP is given in [22]. <p> Thus BP is a special case of Algorithm 3.1. Many other computationally important BP modifications, such as parallel BP [11, 14, 4], BP with momentum term <ref> [7] </ref>, and BP with varying smoothing parameter [20] all fall within the framework of Section 3. We now discuss stability issues in neural network training.
Reference: [8] <author> Z.-Q. Luo and P. Tseng. </author> <title> Analysis of an approximate gradient projection method with applications to the backpropagation algorithm. Optimization Methods and Software, </title> <note> 1994. to appear. </note>
Reference-contexts: It is therefore important to provide rigorous mathematical foundation to neural networks theory and algorithms. Stochastic analysis of BP is given in [22]. The first deterministic convergence results (without data perturbations) were recently obtained in <ref> [11, 8] </ref>. An interesting new training method is proposed in [5]. In this Section we give a precise characterization to empirically observed stability of neural networks [19, 6]. We also discuss BP modifications with varying smoothing parameter.
Reference: [9] <author> O.L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: We assume that A is bounded. Problems of the form (1.5) arise, for example, in least-norm minimization, neural networks applications, and approximation theory. Among some important practical applications that involve parameters in the objective function, we note the adaptive smoothing techniques [12], and the neural network training <ref> [18, 10, 9] </ref>. We assume that each function f j (; ff) is Lipschitz continuous with modulus L &gt; 0 and regular on an open neighborhood of X for every ff 2 A. We also assume that the map @f j (; ) is upper semicontinuous. <p> An interesting new training method is proposed in [5]. In this Section we give a precise characterization to empirically observed stability of neural networks [19, 6]. We also discuss BP modifications with varying smoothing parameter. We regard training artificial neural network as minimization of the following error function (see <ref> [9] </ref>) : x2X&lt; n f (x; ff) = j=1 f j (w; ; v; t; ff) := s i=1 ! ! 2 where h = fixed integer number of hidden units K = fixed integer number of given training samples ~ j in &lt; m t j = 0 or 1
Reference: [10] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Backpropagation convergence via deterministic nonmonotone perturbed minimization. </title> <editor> In G. Tesauro J.D. Cowan and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: We assume that A is bounded. Problems of the form (1.5) arise, for example, in least-norm minimization, neural networks applications, and approximation theory. Among some important practical applications that involve parameters in the objective function, we note the adaptive smoothing techniques [12], and the neural network training <ref> [18, 10, 9] </ref>. We assume that each function f j (; ff) is Lipschitz continuous with modulus L &gt; 0 and regular on an open neighborhood of X for every ff 2 A. We also assume that the map @f j (; ) is upper semicontinuous. <p> Now applying Theorem 2.1 and Corollary 2.1, we immediately obtain the desired results. Adding the "heavy ball" term [16] in Algorithm 3.1, we arrive at the following modification of the parallel GGPM. In neural network literature, methods of this type are usually referred to as backpropagation with momentum term <ref> [7, 10] </ref>. Algorithm 3.2 (Parallel GGPM with Momentum term). Start with any x 0 2 X.
Reference: [11] <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4 </volume> <pages> 103-116, </pages> <year> 1994. </year>
Reference-contexts: We first describe our notation for stating and establishing convergence of the parallel perturbed generalized gradient projection method (GGPM) for solving (1.5) and its modifications. The type of parallelization considered here is primarily motivated by neural network training (see <ref> [11, 14, 4] </ref>). Another related work is [21]. We first consider the most general case. <p> It is therefore important to provide rigorous mathematical foundation to neural networks theory and algorithms. Stochastic analysis of BP is given in [22]. The first deterministic convergence results (without data perturbations) were recently obtained in <ref> [11, 8] </ref>. An interesting new training method is proposed in [5]. In this Section we give a precise characterization to empirically observed stability of neural networks [19, 6]. We also discuss BP modifications with varying smoothing parameter. <p> Thus BP is a special case of Algorithm 3.1. Many other computationally important BP modifications, such as parallel BP <ref> [11, 14, 4] </ref>, BP with momentum term [7], and BP with varying smoothing parameter [20] all fall within the framework of Section 3. We now discuss stability issues in neural network training.
Reference: [12] <author> D.Q. Mayne and E. Polak. </author> <title> Nondifferentiable optimization via adaptive smoothing. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 43(4) </volume> <pages> 601-614, </pages> <year> 1984. </year> <month> 19 </month>
Reference-contexts: We assume that A is bounded. Problems of the form (1.5) arise, for example, in least-norm minimization, neural networks applications, and approximation theory. Among some important practical applications that involve parameters in the objective function, we note the adaptive smoothing techniques <ref> [12] </ref>, and the neural network training [18, 10, 9]. We assume that each function f j (; ff) is Lipschitz continuous with modulus L &gt; 0 and regular on an open neighborhood of X for every ff 2 A.
Reference: [13] <author> V. S. Mikhalevitch, A. M. Gupal, and V. I. Norkin. </author> <title> Methods of nonconvex optimization. </title> <publisher> Nauka, </publisher> <address> Moscow, </address> <year> 1987. </year> <note> (in Russian). </note>
Reference-contexts: Remark 3.1. Theorems 3.1,3.2 generalize the results on convergence properties of the generalized gradient projection method obtained in <ref> [13, 3, 25] </ref>. 4 Important Special Cases In this section we consider the standard optimization problem (1.1), and establish stronger convergence properties of GGPM in a number of important special cases. These include convex and strongly convex problems, and problems with weak sharp minima [16, 1].
Reference: [14] <author> H. Paugam-Moisy. </author> <title> On parallel algorithm for backpropagation by partitioning the training set. </title> <booktitle> In Neural Networks and Their Applications. Proceedings of Fifth International Conference, </booktitle> <address> Nimes, France, </address> <month> November 2-6, </month> <year> 1992. </year>
Reference-contexts: We first describe our notation for stating and establishing convergence of the parallel perturbed generalized gradient projection method (GGPM) for solving (1.5) and its modifications. The type of parallelization considered here is primarily motivated by neural network training (see <ref> [11, 14, 4] </ref>). Another related work is [21]. We first consider the most general case. <p> Thus BP is a special case of Algorithm 3.1. Many other computationally important BP modifications, such as parallel BP <ref> [11, 14, 4] </ref>, BP with momentum term [7], and BP with varying smoothing parameter [20] all fall within the framework of Section 3. We now discuss stability issues in neural network training.
Reference: [15] <author> E. Polak. </author> <title> Computational methods in optimization: A unified approach. </title> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1971. </year>
Reference-contexts: This technique can be viewed as generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes <ref> [17, 23, 15, 16] </ref>. Roughly, this approach reduces the analysis of the stability properties of a process to the analysis of the local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function).
Reference: [16] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: This technique can be viewed as generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes <ref> [17, 23, 15, 16] </ref>. Roughly, this approach reduces the analysis of the stability properties of a process to the analysis of the local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function). <p> According to the classical approach, V () is assumed to be a descent function of the process <ref> [16] </ref>. The key difference of the presented technique is that we relax this monotonicity assumption. We thus refer to V () as the pseudo-Lyapunov function. This generalization makes our approach applicable to a wider class of algorithms. We now state the Generalized Lyapunov Direct Method. <p> Hence x 62 A 0 , and it follows that A 0 X stat ("()). Now applying Theorem 2.1 and Corollary 2.1, we immediately obtain the desired results. Adding the "heavy ball" term <ref> [16] </ref> in Algorithm 3.1, we arrive at the following modification of the parallel GGPM. In neural network literature, methods of this type are usually referred to as backpropagation with momentum term [7, 10]. Algorithm 3.2 (Parallel GGPM with Momentum term). Start with any x 0 2 X. <p> These include convex and strongly convex problems, and problems with weak sharp minima <ref> [16, 1] </ref>. We start with the following lemma. Lemma 4.1 Let "(x) maxf"; r (x)g 8x 2 X; where " 0; 1 &gt; 0. Then X stat ("()) X stat ("): 12 In particular, if " = 0, then X stat ("()) = X stat : Proof. <p> This establishes the first two assertions of the lemma. For the last assertion, just note that (Lemma 1.4.3, <ref> [16] </ref>) for any x 2 X 2l (f (x) min f (y)) k@f (x)k 2 : Definition 4.1 [1] We say that problem (1.1) is weakly sharp with parameter &gt; 0 if f (x) min f (y) d (x; X opt ) 8x 2 X: We have the following important corollary.
Reference: [17] <author> N. Rouche, P. Habets, and M Laloy. </author> <title> Stability Theory by Liapunov's Direct Method. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: This technique can be viewed as generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes <ref> [17, 23, 15, 16] </ref>. Roughly, this approach reduces the analysis of the stability properties of a process to the analysis of the local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function).
Reference: [18] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362, </pages> <address> Cambridge, Massachusetts, 1986. </address> <publisher> MIT Press. </publisher>
Reference-contexts: We assume that A is bounded. Problems of the form (1.5) arise, for example, in least-norm minimization, neural networks applications, and approximation theory. Among some important practical applications that involve parameters in the objective function, we note the adaptive smoothing techniques [12], and the neural network training <ref> [18, 10, 9] </ref>. We assume that each function f j (; ff) is Lipschitz continuous with modulus L &gt; 0 and regular on an open neighborhood of X for every ff 2 A. We also assume that the map @f j (; ) is upper semicontinuous. <p> Then every sequence fx i g generated by Algorithm 4.1 converges to X opt (" 2 =2l). 5 Backpropagation With Noise In this Section we apply the results of Section 3 to reveal some important properties of the backpropagation (BP) algorithm for training artificial neural networks <ref> [18, 7] </ref>. Due to numerous successful applications, a lot of empirical knowledge has been accumulated in the neural networks field. It is therefore important to provide rigorous mathematical foundation to neural networks theory and algorithms. Stochastic analysis of BP is given in [22].
Reference: [19] <author> T.J. Sejnowski and C.R. Rosenberg. </author> <title> Paralel networks that learn to pronounce english text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: Stochastic analysis of BP is given in [22]. The first deterministic convergence results (without data perturbations) were recently obtained in [11, 8]. An interesting new training method is proposed in [5]. In this Section we give a precise characterization to empirically observed stability of neural networks <ref> [19, 6] </ref>. We also discuss BP modifications with varying smoothing parameter.
Reference: [20] <author> A. Sperduti and A. Starita. </author> <title> Speed up learning and network optimization with extended backpropagation. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 365-383, </pages> <year> 1993. </year>
Reference-contexts: Thus BP is a special case of Algorithm 3.1. Many other computationally important BP modifications, such as parallel BP [11, 14, 4], BP with momentum term [7], and BP with varying smoothing parameter <ref> [20] </ref> all fall within the framework of Section 3. We now discuss stability issues in neural network training. <p> Another possible application of our analysis is devising new algorithms with varying smoothing parameter ff. There exists empirical evidence that changing ff during training can significantly speed up the learning process <ref> [20] </ref>. Unfortunately, most applications that take advantage of this idea usually employ some kind of heuristic to control the parameter. It will be interesting to develop a more rigorous algorithmic approach.
Reference: [21] <author> J.N. Tsitsiklis, D.P. Bertsekas, and M. Athans. </author> <title> Distributed asynchronous deterministic and stochastic gradient optimization algorithms. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-31(9):803-812, </volume> <year> 1986. </year>
Reference-contexts: We first describe our notation for stating and establishing convergence of the parallel perturbed generalized gradient projection method (GGPM) for solving (1.5) and its modifications. The type of parallelization considered here is primarily motivated by neural network training (see [11, 14, 4]). Another related work is <ref> [21] </ref>. We first consider the most general case.
Reference: [22] <author> H. White. </author> <title> Some asymptotic results for learning in single hidden-layer feedforward network models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84(408) </volume> <pages> 1003-1013, </pages> <year> 1989. </year>
Reference-contexts: Due to numerous successful applications, a lot of empirical knowledge has been accumulated in the neural networks field. It is therefore important to provide rigorous mathematical foundation to neural networks theory and algorithms. Stochastic analysis of BP is given in <ref> [22] </ref>. The first deterministic convergence results (without data perturbations) were recently obtained in [11, 8]. An interesting new training method is proposed in [5]. In this Section we give a precise characterization to empirically observed stability of neural networks [19, 6]. We also discuss BP modifications with varying smoothing parameter.
Reference: [23] <author> W.I. Zangwill. </author> <title> Nonlinear Programming: A Unified Approach. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> En-glewood Cliffs, New Jersey, </address> <year> 1969. </year> <month> 20 </month>
Reference-contexts: This technique can be viewed as generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes <ref> [17, 23, 15, 16] </ref>. Roughly, this approach reduces the analysis of the stability properties of a process to the analysis of the local improvement of this process with respect to some scalar criterion V () (usually called the Lyapunov function).
Reference: [24] <author> S. K. Zavriev. </author> <title> Stochastic subgradient methods for Minmax problems. </title> <address> Izdatelstvo MGU, Moscow, </address> <year> 1984. </year> <note> (in Russian). </note>
Reference-contexts: We say that a set C &lt; n is V ()-connected, if the set V (C) = fv 2 &lt; j 9x 2 X; v = V (x)g &lt; is connected. Let fA (fl) g; fl 2 be the (unique) decomposition of A 0 into V ()-connected components <ref> [24] </ref>, that is A 0 = [ fl2 A (fl) ; A (fl 0 ) 6= A (fl 00 ) ; fl 0 6= fl 00 ; fl 0 ; fl 00 2 : The following theorem will play an important role in the subsequent analysis.
Reference: [25] <author> S.K. Zavriev and A.G. Perevozchikov. </author> <title> Attraction of trajectories of finite-difference inclusions and stability of numerical methods of stochastic nonsmooth optimization. </title> <journal> Soviet Phys. Doklady, </journal> <volume> 313 </volume> <pages> 1373-1376, </pages> <year> 1990. </year>
Reference-contexts: Remark 3.1. Theorems 3.1,3.2 generalize the results on convergence properties of the generalized gradient projection method obtained in <ref> [13, 3, 25] </ref>. 4 Important Special Cases In this section we consider the standard optimization problem (1.1), and establish stronger convergence properties of GGPM in a number of important special cases. These include convex and strongly convex problems, and problems with weak sharp minima [16, 1].
Reference: [26] <author> S.K. Zavriev and A.G. Perevozchikov. </author> <title> Direct Lyapunov's method in attraction analysis of finite-difference inclusions. USSR Computational Mathematics and Mathematical Physics, </title> <publisher> Pergamon Press, </publisher> <pages> 30(1) 22-32, </pages> <year> 1990. </year> <month> 21 </month>
Reference-contexts: We give a precise estimate for "() in terms of asymptotic behavior of the perturbations. Our analysis is based on the novel technique developed in <ref> [26] </ref>. <p> P X () will stand for the orthogonal projection map onto a closed convex set X. 3 2 Generalized Lyapunov Direct Method In this Section we outline the novel convergence analysis technique that was first proposed in <ref> [26] </ref>. This technique can be viewed as generalization of the Lyapunov Direct Method for convergence analysis of nonlinear iterative processes. The Lyapunov Direct Method has proved to be a powerful tool for stability analysis of both continuous and discrete time processes [17, 23, 15, 16]. <p> Theorem 2.1 <ref> [26] </ref> For every sequence fx i g generated by the process (2.1)-(2.2), and satis fying (2.3), there exists a fl 2 such that the following properties hold : lt i!1 V (x i ) = V lt i!1 fx i g " A (fl) ; and every subsequence fx i m <p> Corollary 2.1 <ref> [26] </ref> Let the set V (A 0 ) be nowhere dense in &lt;.
References-found: 26

