URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-367.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: Active Gesture Recognition using Partially Observable Markov Decision Processes  
Author: Trevor Darrell and Alex Pentland 
Affiliation: Perceptual Computing Group, MIT Media Lab  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 367 Appeared 13th IEEE Intl. Conference on Pattern Recognition (ICPR '96), Vienna, Austria. Abstract We present a foveated gesture recognition system that guides an active camera to foveate salient features based on a reinforcement learning paradigm. Using vision routines previously implemented for an interactive environment, we determine the spatial location of salient body parts of a user and guide an active camera to obtain images of gestures or expressions. A hidden-state reinforcement learning paradigm based on the Partially Observable Markov Decision Process (POMDP) is used to implement this visual attention. The attention module selects targets to foveate based on the goal of successful recognition, and uses a new multiple-model Q-learning formulation. Given a set of target and distractor gestures, our system can learn where to foveate to maximally discriminate a particular gesture.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Cassandra, L. P. Kaelbling, and M. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proc. AAAI-94, </booktitle> <year> 1994. </year>
Reference-contexts: In practice, T and O are not easily obtainable, and we use methods which do not require them a priori. Rather than attempt to solve the POMDP explicitly, we look to techniques for hidden state reinforcement learning to find a solution <ref> [12, 10, 8, 1] </ref>. Our state is defined by the users pose, facial expression, and hand configurations, expressed in nine variables Three of these are boolean, person-present, left-arm-extended, and right-arm-extended, and are provided directly by the person tracker.
Reference: [2] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> C.S. T.R. </type> <pages> 91-57, </pages> <address> U. Mass, </address> <year> 1991. </year>
Reference-contexts: The Q function maps state and action pairs to a utility value, which is the expected discounted future reward given that state and action. Optimal Q values can be computed on-line using a Temporal Differences method [14], which is an incremental form of a full Dynamic Programming approach <ref> [2] </ref>. With of deterministic state transitions, the optimal Q function must satisfy Q (s; a) = r + fl max Q (s 0 ; a) ; where s 0 is the next state after executing action a in state s.
Reference: [3] <author> T., Darrell, and A., Pentland, </author> <title> Space-Time Gestures. </title> <booktitle> Proceedings IEEE CVPR-93, </booktitle> <address> New York, </address> <publisher> IEEE Comp. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: We have been developing methods which combine low-level vision and learning to implement interfaces which can directly respond to a human user via visual perception. Previously, we presented a method for view-based recognition of spatio-temporal hand gestures <ref> [3] </ref> and a similar mechanism for the analysis/real-time tracking of facial expressions [5]. <p> does not depend on r [t]; this allows considerable computational savings in the multiple model system since v (a) need not depend on j. 4 Experimental Results We experimented with our recognition system using a real-time implementation connected to the perceptual outputs of our person tracking and gesture analysis systems <ref> [17, 3] </ref>, as described in Section 2.1. To train our system, we follow a three stage procedure.
Reference: [4] <author> T. Darrell, P. Maes, B. Blumberg, and A. P. Pentland, </author> <title> A Novel Environment for Situated Vision and Behavior, </title> <booktitle> Proc. IEEE Workshop for Visual Behaviors, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1994 </year>
Reference-contexts: The active camera was guided by vision routines which could track people and identify head/hand locations as they walk about a room based on coarse-scale (wide field-of-view) images from a second, static camera <ref> [4, 17] </ref>. In this paper we address the problem of where to foveate in an active recognition framework, e.g., how to plan observations given a particular recognition task. We explore a reinforcement learning approach, in which foveation actions are based on prior supervised training experience.
Reference: [5] <author> T. Darrell, I. Essa, and A. P. Pentland, </author> <title> Correlation and Interpolation Networks for Real-time Expression Analysis/Synthesis, </title> <booktitle> In Advances NIPS 7, </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: We have been developing methods which combine low-level vision and learning to implement interfaces which can directly respond to a human user via visual perception. Previously, we presented a method for view-based recognition of spatio-temporal hand gestures [3] and a similar mechanism for the analysis/real-time tracking of facial expressions <ref> [5] </ref>. These methods offered real-time performance and a relatively high level of accuracy, but required foveated images of the object performing the gesture and were thus of limited usefulness in unconstrained domains, such as intelligent rooms or interactive virtual environments.
Reference: [6] <author> T. Darrell and A. Pentland, A., </author> <title> Attention-driven Expression and Gesture Analysis in an Interactive Environment, </title> <booktitle> in Proc. </booktitle> <address> IWAFGR '95, Zurich, Switzerland, </address> <year> 1995. </year>
Reference-contexts: These methods offered real-time performance and a relatively high level of accuracy, but required foveated images of the object performing the gesture and were thus of limited usefulness in unconstrained domains, such as intelligent rooms or interactive virtual environments. In <ref> [6] </ref> and [7], we expanded our gesture recognition method to include an active component, utilizing an active image sensor that can foveate a person's hand or face at high resolution.
Reference: [7] <author> Darrell, T., Moghaddam, B., and Pentland, A., </author> <title> Active Face Tracking and Pose Estimation in an Interactive Room, </title> <note> to appear Proc. CVPR-96. </note> <year> 1996. </year>
Reference-contexts: These methods offered real-time performance and a relatively high level of accuracy, but required foveated images of the object performing the gesture and were thus of limited usefulness in unconstrained domains, such as intelligent rooms or interactive virtual environments. In [6] and <ref> [7] </ref>, we expanded our gesture recognition method to include an active component, utilizing an active image sensor that can foveate a person's hand or face at high resolution.
Reference: [8] <author> T. Jaakkola, S. Singh, and M. Jordan. </author> <title> Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems. </title> <booktitle> In Advances NIPS 7, </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: In practice, T and O are not easily obtainable, and we use methods which do not require them a priori. Rather than attempt to solve the POMDP explicitly, we look to techniques for hidden state reinforcement learning to find a solution <ref> [12, 10, 8, 1] </ref>. Our state is defined by the users pose, facial expression, and hand configurations, expressed in nine variables Three of these are boolean, person-present, left-arm-extended, and right-arm-extended, and are provided directly by the person tracker.
Reference: [9] <author> T. Poggio and F. Girosi, </author> <title> A Theory of Networks for Approximation and Learning. </title> <institution> MIT AI Lab TR-1140, </institution> <year> 1989. </year>
Reference: [10] <author> L. Lin and T. Michell. </author> <title> Reinforcement learning with hidden states. </title> <booktitle> In Proc. AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: In practice, T and O are not easily obtainable, and we use methods which do not require them a priori. Rather than attempt to solve the POMDP explicitly, we look to techniques for hidden state reinforcement learning to find a solution <ref> [12, 10, 8, 1] </ref>. Our state is defined by the users pose, facial expression, and hand configurations, expressed in nine variables Three of these are boolean, person-present, left-arm-extended, and right-arm-extended, and are provided directly by the person tracker.
Reference: [11] <author> W. Lovejoy. </author> <title> A survey of algorithmic methods of partially observed markov decision processes. </title> <journal> Annals of Operation Research, </journal> <volume> 28:4766, </volume> <year> 1991. </year>
Reference-contexts: The AGR task can be considered as a Partially Observable Markov Decision Process (POMDP), which is essentially a Markov Decision Process (MDP) without direct access to state <ref> [13, 11] </ref>. A POMDP consists of a set of states in the world S, a set of observations O, a set of actions A, and a reward function R.
Reference: [12] <author> R. A. McCallum. </author> <title> Instance-based State Identification for Reinforcement Learning. </title> <booktitle> In Advances NIPS 7, </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: In practice, T and O are not easily obtainable, and we use methods which do not require them a priori. Rather than attempt to solve the POMDP explicitly, we look to techniques for hidden state reinforcement learning to find a solution <ref> [12, 10, 8, 1] </ref>. Our state is defined by the users pose, facial expression, and hand configurations, expressed in nine variables Three of these are boolean, person-present, left-arm-extended, and right-arm-extended, and are provided directly by the person tracker. <p> Given the reward function defined in the AGR task, this will correspond to a successful recognition. 2.2 Hidden-State Learning To find policies for AGR tasks we have implemented an instance-based method for hidden state reinforcement learning, based on earlier work by McCallum <ref> [12] </ref>. This method performs Q-learning [14, 15] (see Appendix A), but replaces the absolute state with a distributed memory-based state representation. <p> Further, U j [T + 1] = max Q j [T + 1] ; (6) (7) Note that our sequence match criteria, unlike that in <ref> [12] </ref>, does not depend on r [t]; this allows considerable computational savings in the multiple model system since v (a) need not depend on j. 4 Experimental Results We experimented with our recognition system using a real-time implementation connected to the perceptual outputs of our person tracking and gesture analysis systems
Reference: [13] <author> E. J. Sondik. </author> <title> The optimal control of partially observable markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Reserach, </journal> <volume> 26(2):282304, </volume> <year> 1978. </year>
Reference-contexts: The AGR task can be considered as a Partially Observable Markov Decision Process (POMDP), which is essentially a Markov Decision Process (MDP) without direct access to state <ref> [13, 11] </ref>. A POMDP consists of a set of states in the world S, a set of observations O, a set of actions A, and a reward function R.
Reference: [14] <author> R. S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <booktitle> Machine Learning, </booktitle> <address> 3:944, </address> <year> 1988. </year>
Reference-contexts: Given the reward function defined in the AGR task, this will correspond to a successful recognition. 2.2 Hidden-State Learning To find policies for AGR tasks we have implemented an instance-based method for hidden state reinforcement learning, based on earlier work by McCallum [12]. This method performs Q-learning <ref> [14, 15] </ref> (see Appendix A), but replaces the absolute state with a distributed memory-based state representation. <p> The Q function maps state and action pairs to a utility value, which is the expected discounted future reward given that state and action. Optimal Q values can be computed on-line using a Temporal Differences method <ref> [14] </ref>, which is an incremental form of a full Dynamic Programming approach [2].
Reference: [15] <author> C. Watkins and P. Dayan. Q-learning. </author> <title> Machine Learning, </title> <address> 8:279292, </address> <year> 1992. </year>
Reference-contexts: Given the reward function defined in the AGR task, this will correspond to a successful recognition. 2.2 Hidden-State Learning To find policies for AGR tasks we have implemented an instance-based method for hidden state reinforcement learning, based on earlier work by McCallum [12]. This method performs Q-learning <ref> [14, 15] </ref> (see Appendix A), but replaces the absolute state with a distributed memory-based state representation. <p> the two sides of this equation is minimized: Q (s; a) (1 )Q (s; a) + (r + fl max Q (s 0 ; a)) : This has been shown to converge to optimal policies when the environment is Markovian and the Q-function is represented literally as a lookup table <ref> [15] </ref>. In the case of instance-based POMDP reinforcement learning, we do not use a table representation, i.e. Q (s; a). Time plays the role of state, in essence, and so we store a Q function over time, i.e. Q [T ].
Reference: [16] <author> Whitehead, S., </author> <title> Active perception and reinforcement learning. </title> <booktitle> In Proc. 7th Intl. Conf. ML, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The parameter ff expresses the trade-off between false alarms and misses; for the results presented here we have taken a conservative approach and set ff = 10. (This is similar to the idea of disproportionally penalizing the Q-value of perceptually aliased states, in Whitehead's Lion algorithm <ref> [16] </ref>.) Zero reward is given whenever a foveation action is performed.
Reference: [17] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, Pfinder: </author> <title> Real-Time Tracking of the Human Body, MIT Media Lab Perceptual Computing Group Technical Report No. </title> <type> 353, </type> <year> 1994 </year>
Reference-contexts: The active camera was guided by vision routines which could track people and identify head/hand locations as they walk about a room based on coarse-scale (wide field-of-view) images from a second, static camera <ref> [4, 17] </ref>. In this paper we address the problem of where to foveate in an active recognition framework, e.g., how to plan observations given a particular recognition task. We explore a reinforcement learning approach, in which foveation actions are based on prior supervised training experience. <p> does not depend on r [t]; this allows considerable computational savings in the multiple model system since v (a) need not depend on j. 4 Experimental Results We experimented with our recognition system using a real-time implementation connected to the perceptual outputs of our person tracking and gesture analysis systems <ref> [17, 3] </ref>, as described in Section 2.1. To train our system, we follow a three stage procedure.
References-found: 17

