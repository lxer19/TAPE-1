URL: http://www.cs.cmu.edu/~tcm/tcm_papers/tlds_tech97.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/steffan/www/index.html
Root-URL: 
Email: fsteffan,tcmg@eecg.toronto.edu  
Title: The Potential for Thread-Level Data Speculation in Tightly-Coupled Multiprocessors  
Author: J. Gregory Steffan and Todd C. Mowry 
Date: February 1997  
Address: Toronto, Canada M5S 3G4  
Affiliation: Department of Electrical and Computer Engineering University of Toronto  
Pubnum: Technical Report CSRI-TR-350  
Abstract: To fully exploit the potential of single-chip multiprocessors, we must find a way to parallelize non-numeric applications. However, compilers have had little success in parallelizing non-numeric codes due to their complex data access patterns. This paper explores the potential for using thread-level data speculation (TLDS) to overcome this limitation by allowing the compiler to view parallelization solely as a cost/benefit tradeoff, rather than something which may violate program correctness. Experimental results demonstrate that TLDS can offer significant program speedups. We also demonstrate that through modest hardware extensions, a standard single-chip multiprocessor could support TLDS by augmenting the cache coherence scheme to detect dependence violations, and by using the primary data caches to buffer speculative state. We quantify the impact of this implementation on performance, and we also evaluate the compiler support necessary to exploit TLDS.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Table 1 summarizes the ten non-numeric applications studied, which are taken from the SPEC92 [4], SPEC95 [3], and NAS Parallel <ref> [1] </ref> benchmark suites. These applications were compiled with -O2 optimization using the standard MIPS compilers under IRIX 5.3, and the source code and resulting object files were not modified in any way. Table 2 lists the speculative regions analyzed in this study.
Reference: [2] <author> E. Bugnion, J. M. Anderson, T. C. Mowry, M. Rosenblum, and M. S. Lam. </author> <title> Compiler-directed page coloring for multiprocessors. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 244-255, </pages> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: Instead, the preferred solution would be for the compiler to parallelize programs automatically. Unfortunately, compilers have only been successful so far at parallelizing numeric applications <ref> [2, 10, 22] </ref>. For single-chip multiprocessing to have an impact on the majority of users, we must also find a way to automatically parallelize non-numeric applications.
Reference: [3] <author> Standard Performance Evaluation Corporation. </author> <title> The spec95int benchmark suite. </title> <type> Technical report. </type> <note> http://www.spechbench.org. </note>
Reference-contexts: Table 1 summarizes the ten non-numeric applications studied, which are taken from the SPEC92 [4], SPEC95 <ref> [3] </ref>, and NAS Parallel [1] benchmark suites. These applications were compiled with -O2 optimization using the standard MIPS compilers under IRIX 5.3, and the source code and resulting object files were not modified in any way. Table 2 lists the speculative regions analyzed in this study.
Reference: [4] <author> K. M. Dixit. </author> <title> New cpu benchmark suites from spec. </title> <booktitle> In COMPCON, </booktitle> <month> Spring </month> <year> 1992. </year>
Reference-contexts: Table 1 summarizes the ten non-numeric applications studied, which are taken from the SPEC92 <ref> [4] </ref>, SPEC95 [3], and NAS Parallel [1] benchmark suites. These applications were compiled with -O2 optimization using the standard MIPS compilers under IRIX 5.3, and the source code and resulting object files were not modified in any way. Table 2 lists the speculative regions analyzed in this study.
Reference: [5] <author> M. Farrens, G. Tyson, and A. R. Pleszkun. </author> <title> A study of single-chip processor/cache organizations for large numbers of transistors. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 338-347, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: While there have been several proposals [6, 7, 14, 15, 23, 28, 27], perhaps one of the more compelling options is to integrate multiple processors onto a single chip <ref> [5, 20, 17, 14, 7] </ref>. From a VLSI perspective, single-chip multiprocessors are attractive because their distributed nature allows the bulk of the interconnections to be localized, thus avoiding the delays associated with long wires [20].
Reference: [6] <author> M. Fillo, S. W. Keckler, W. J. Dally, N. P. Carter, A. Chang, Y. Gurevich, and W. S. Lee. </author> <title> The m-machine multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microrchitecture, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction As the number of transistors that can be integrated onto a single VLSI chip continues its dramatic rate of increase, processor architects are faced with the pleasant challenge of finding the best way to translate these additional resources into improved performance. While there have been several proposals <ref> [6, 7, 14, 15, 23, 28, 27] </ref>, perhaps one of the more compelling options is to integrate multiple processors onto a single chip [5, 20, 17, 14, 7]. <p> Data transfer can occur either through memory or through registers [7, 27], and synchronization can be performed either explicitly using wait/signal or implicitly using full/empty bits <ref> [6, 14, 24] </ref>. If multiple values are to be forwarded, the synchronization can occur at either a coarse granularity (once per epoch) or a fine granularity (once per value), as illustrated in Figure 5.
Reference: [7] <author> M. Franklin. </author> <title> The Multiscalar Architecture. </title> <type> PhD thesis, </type> <institution> University of Wisconsin - Madison, </institution> <year> 1993. </year>
Reference-contexts: 1 Introduction As the number of transistors that can be integrated onto a single VLSI chip continues its dramatic rate of increase, processor architects are faced with the pleasant challenge of finding the best way to translate these additional resources into improved performance. While there have been several proposals <ref> [6, 7, 14, 15, 23, 28, 27] </ref>, perhaps one of the more compelling options is to integrate multiple processors onto a single chip [5, 20, 17, 14, 7]. <p> While there have been several proposals [6, 7, 14, 15, 23, 28, 27], perhaps one of the more compelling options is to integrate multiple processors onto a single chip <ref> [5, 20, 17, 14, 7] </ref>. From a VLSI perspective, single-chip multiprocessors are attractive because their distributed nature allows the bulk of the interconnections to be localized, thus avoiding the delays associated with long wires [20]. <p> In this section, we briefly discuss the two most relevant works to this study, the privatizing doall test [22] and the Wisconsin multiscalar architecture <ref> [7, 27] </ref>. 1.2.1 The Privatizing Doall Test Padua et al. [22] have devised a method of parallelizing loops for numeric codes in the presence of ambiguous data dependences. <p> Because of this, the PD test is only effective in parallelizing a narrow class of loops. 1.2.2 The Multiscalar Architecture The most relevant work to this study is the Wisconsin multiscalar architecture <ref> [7, 27, 8] </ref>. This architecture performs aggressive control and data speculation through the use of devoted hardware structures and complex forwarding mechanisms. This section describes the multiscalar architecture and how it executes an application, beginning with the compilation process. <p> Data transfer can occur either through memory or through registers <ref> [7, 27] </ref>, and synchronization can be performed either explicitly using wait/signal or implicitly using full/empty bits [6, 14, 24]. If multiple values are to be forwarded, the synchronization can occur at either a coarse granularity (once per epoch) or a fine granularity (once per value), as illustrated in Figure 5. <p> Therefore aggressive compiler scheduling can potentially eliminate the need for expensive forwarding hardware, thus allowing us to forward data through the normal cache hierarchy. 7 An example of fast forwarding support is the register forwarding mechanism proposed by the Multiscalar architecture <ref> [7, 27] </ref>. 14 in 10 cycles, fast = special hardware support to forward in 2 cycles, B = base TLDS hardware with no compiler support, Br = register dependences eliminated or forwarded, R = case R dependences eliminated, Sf = case S dependences eliminated and fine-grain synchronization, Ss = case S
Reference: [8] <author> M. Franklin and G. S. Sohi. Arb: </author> <title> A hardware mechanism for dynamic reordering of memory references. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 45(5), </volume> <month> May </month> <year> 1996. </year>
Reference-contexts: Because of this, the PD test is only effective in parallelizing a narrow class of loops. 1.2.2 The Multiscalar Architecture The most relevant work to this study is the Wisconsin multiscalar architecture <ref> [7, 27, 8] </ref>. This architecture performs aggressive control and data speculation through the use of devoted hardware structures and complex forwarding mechanisms. This section describes the multiscalar architecture and how it executes an application, beginning with the compilation process. <p> In the receiving processor, each register has a busy bit which is set when the register value arrives, thus synchronizing the communication of register values between tasks. In order to support data speculation, the multiscalar architecture includes the address resolution buffer (ARB) <ref> [8] </ref> which performs dynamic memory disambiguation. The ARB sits between the processors and the first-level cache, and all memory accesses are filtered through it. When a store to memory occurs, the store address and the value are kept in one of the ARB's associative entries for the corresponding processor.
Reference: [9] <author> D. M. Gallagher, W. Y. Chen, S. A. Mahlke, J. C. Gyllenhaal, and W. W. Hwu. </author> <title> Dynamic memory disambiguation using the memory conflict buffer. </title> <booktitle> In Proceedings of the 6th International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-195, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: At run-time, we can verify the safety of this speculative operation either through a simple software check (i.e. compare p with q), or with the help of hardware support such as the "memory conflict buffer" <ref> [9] </ref>. Previous studies have demonstrated that instruction-level data speculation can significantly increase the amount of instruction-level parallelism (ILP) within a program [9, 12]. <p> Previous studies have demonstrated that instruction-level data speculation can significantly increase the amount of instruction-level parallelism (ILP) within a program <ref> [9, 12] </ref>. Thread-Level Data Speculation (TLDS) is analogous to instruction-level data speculation, except that the load and store are executed by separate threads of control which run in parallel, as illustrated in Figure 1 (c). <p> data speculation (as we discuss in detail later in Section 4.2), our results in this paper demonstrate that TLDS can significantly increase the amount of thread-level parallelism (TLP) within non-numeric programs, and can be implemented in a cost-effective manner. 1.2 Related Work While instruction-level data speculation has received much attention <ref> [9, 12, 26] </ref>, there has been relatively little work so far on thread-level data speculation.
Reference: [10] <author> G. Goff, K. Kennedy, and C. W. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 15-29, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Instead, the preferred solution would be for the compiler to parallelize programs automatically. Unfortunately, compilers have only been successful so far at parallelizing numeric applications <ref> [2, 10, 22] </ref>. For single-chip multiprocessing to have an impact on the majority of users, we must also find a way to automatically parallelize non-numeric applications. <p> To address this problem in numeric codes, a considerable amount of research has focused on ana 1 (a) Original Execution (b) Instruction-Level Data Speculation (c) Thread-Level Data Speculation lyzing array accesses within DO loops <ref> [10, 16, 21] </ref>. Although progress has been made in this area, the problem is considerably more difficult for non-numeric codes due to their complex access patterns, including pointers to heap-allocated objects and complex control flow.
Reference: [11] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <month> May </month> <year> 1988. </year> <month> 27 </month>
Reference-contexts: We may then use Amdahl's law <ref> [11] </ref> to compute program speedup: Seedup program with T LDS = T original program T parallel + T sequential : (13)
Reference: [12] <author> A. S. Huang, G. Slavenburg, and J. P. Shen. </author> <title> Speculative disambiguation: A compilation technique for dynamic memory disambiguation. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 200-210, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Previous studies have demonstrated that instruction-level data speculation can significantly increase the amount of instruction-level parallelism (ILP) within a program <ref> [9, 12] </ref>. Thread-Level Data Speculation (TLDS) is analogous to instruction-level data speculation, except that the load and store are executed by separate threads of control which run in parallel, as illustrated in Figure 1 (c). <p> data speculation (as we discuss in detail later in Section 4.2), our results in this paper demonstrate that TLDS can significantly increase the amount of thread-level parallelism (TLP) within non-numeric programs, and can be implemented in a cost-effective manner. 1.2 Related Work While instruction-level data speculation has received much attention <ref> [9, 12, 26] </ref>, there has been relatively little work so far on thread-level data speculation.
Reference: [13] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Therefore, two-way associativity is perhaps the most attractive option. 4.3.3 Adding A Victim Cache Rather than giving up as soon as a speculatively accessed line is forced out of the cache, another possibility is to capture these spilled lines within a small victim cache <ref> [13] </ref>. Figure 15 shows the maximum number of victim entries necessary to capture all speculatively loaded or modified lines that would be ejected from a 16KB cache of various associativities.
Reference: [14] <author> S. W. Keckler and W. J. Dally. </author> <title> Processor coupling: Integrating compile time and runtime scheduling for parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 202-213, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction As the number of transistors that can be integrated onto a single VLSI chip continues its dramatic rate of increase, processor architects are faced with the pleasant challenge of finding the best way to translate these additional resources into improved performance. While there have been several proposals <ref> [6, 7, 14, 15, 23, 28, 27] </ref>, perhaps one of the more compelling options is to integrate multiple processors onto a single chip [5, 20, 17, 14, 7]. <p> While there have been several proposals [6, 7, 14, 15, 23, 28, 27], perhaps one of the more compelling options is to integrate multiple processors onto a single chip <ref> [5, 20, 17, 14, 7] </ref>. From a VLSI perspective, single-chip multiprocessors are attractive because their distributed nature allows the bulk of the interconnections to be localized, thus avoiding the delays associated with long wires [20]. <p> Data transfer can occur either through memory or through registers [7, 27], and synchronization can be performed either explicitly using wait/signal or implicitly using full/empty bits <ref> [6, 14, 24] </ref>. If multiple values are to be forwarded, the synchronization can occur at either a coarse granularity (once per epoch) or a fine granularity (once per value), as illustrated in Figure 5.
Reference: [15] <author> J. Laudon, A. Gupta, and M. Horowitz. </author> <title> Interleaving: A multithreading technique targeting multiprocessors and workstations. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 308-318, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: 1 Introduction As the number of transistors that can be integrated onto a single VLSI chip continues its dramatic rate of increase, processor architects are faced with the pleasant challenge of finding the best way to translate these additional resources into improved performance. While there have been several proposals <ref> [6, 7, 14, 15, 23, 28, 27] </ref>, perhaps one of the more compelling options is to integrate multiple processors onto a single chip [5, 20, 17, 14, 7].
Reference: [16] <author> D. E. Maydan. </author> <title> Accurate Analysis of Array References. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> Septem-ber </month> <year> 1992. </year>
Reference-contexts: To address this problem in numeric codes, a considerable amount of research has focused on ana 1 (a) Original Execution (b) Instruction-Level Data Speculation (c) Thread-Level Data Speculation lyzing array accesses within DO loops <ref> [10, 16, 21] </ref>. Although progress has been made in this area, the problem is considerably more difficult for non-numeric codes due to their complex access patterns, including pointers to heap-allocated objects and complex control flow.
Reference: [17] <author> B. A. Nayfeh, L. Hammond, and K. Olukotun. </author> <title> Evaluation of design alternatives for a multiprocessor microprocessor. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 67-77, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: While there have been several proposals [6, 7, 14, 15, 23, 28, 27], perhaps one of the more compelling options is to integrate multiple processors onto a single chip <ref> [5, 20, 17, 14, 7] </ref>. From a VLSI perspective, single-chip multiprocessors are attractive because their distributed nature allows the bulk of the interconnections to be localized, thus avoiding the delays associated with long wires [20].
Reference: [18] <author> A. Nicolau. </author> <title> Run-time disambiguation: coping with statically unpredictable dependencies. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38 </volume> <pages> 663-678, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: large number of addresses that must be compared against each other at the end of an epoch to determine safety, and given the fact that the exact interleaving of accesses between threads is not known a priori since they run asynchronously, a purely software-based approach of explicitly comparing memory addresses <ref> [18] </ref> would appear to be impractical. Instead, we propose extending cache coherence schemes to allow hardware to detect potential dependence violations with little overhead, and letting software control the recovery process. We will discuss this mechanism in greater detail later in Section 4.2.
Reference: [19] <author> R. S. Nikhil and Arvind. </author> <title> Can dataflow subsume von Neumann computing. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 262-272, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The disadvantage of the dynamic approach is the runtime overhead of frequent thread creation, which may be reduced to some extent through a lightweight fork instruction <ref> [19] </ref>.
Reference: [20] <author> K. Olukotun, B. A. Nayfeh, L. Hammond, K. Wilson, and K. Chang. </author> <title> The case for a single-chip multiprocessor. </title> <booktitle> In Proceedings of the 7th Annual International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: While there have been several proposals [6, 7, 14, 15, 23, 28, 27], perhaps one of the more compelling options is to integrate multiple processors onto a single chip <ref> [5, 20, 17, 14, 7] </ref>. From a VLSI perspective, single-chip multiprocessors are attractive because their distributed nature allows the bulk of the interconnections to be localized, thus avoiding the delays associated with long wires [20]. <p> From a VLSI perspective, single-chip multiprocessors are attractive because their distributed nature allows the bulk of the interconnections to be localized, thus avoiding the delays associated with long wires <ref> [20] </ref>. While single-chip multiprocessing will clearly increase computational throughput, it will only reduce the execution time of applications that can exploit parallelism.
Reference: [21] <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> Communications of the ACM, </journal> <month> August </month> <year> 1992. </year>
Reference-contexts: To address this problem in numeric codes, a considerable amount of research has focused on ana 1 (a) Original Execution (b) Instruction-Level Data Speculation (c) Thread-Level Data Speculation lyzing array accesses within DO loops <ref> [10, 16, 21] </ref>. Although progress has been made in this area, the problem is considerably more difficult for non-numeric codes due to their complex access patterns, including pointers to heap-allocated objects and complex control flow.
Reference: [22] <author> L. Rauchwerger and D. Padua. </author> <title> The lrpd test: Speculative run-time parallelization of loops with privatization and reduction parallelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 218-232, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Instead, the preferred solution would be for the compiler to parallelize programs automatically. Unfortunately, compilers have only been successful so far at parallelizing numeric applications <ref> [2, 10, 22] </ref>. For single-chip multiprocessing to have an impact on the majority of users, we must also find a way to automatically parallelize non-numeric applications. <p> In this section, we briefly discuss the two most relevant works to this study, the privatizing doall test <ref> [22] </ref> and the Wisconsin multiscalar architecture [7, 27]. 1.2.1 The Privatizing Doall Test Padua et al. [22] have devised a method of parallelizing loops for numeric codes in the presence of ambiguous data dependences. <p> In this section, we briefly discuss the two most relevant works to this study, the privatizing doall test <ref> [22] </ref> and the Wisconsin multiscalar architecture [7, 27]. 1.2.1 The Privatizing Doall Test Padua et al. [22] have devised a method of parallelizing loops for numeric codes in the presence of ambiguous data dependences. Their approach, called the privatizing doall (PD) test is entirely software-based, allowing the compiler to parallelize loops without fully disambiguating all memory references.
Reference: [23] <author> A. Saulsbury, F. Pong, and A. Nowatzyk. </author> <title> Missing the memory wall: The case for processor/memory integration. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Introduction As the number of transistors that can be integrated onto a single VLSI chip continues its dramatic rate of increase, processor architects are faced with the pleasant challenge of finding the best way to translate these additional resources into improved performance. While there have been several proposals <ref> [6, 7, 14, 15, 23, 28, 27] </ref>, perhaps one of the more compelling options is to integrate multiple processors onto a single chip [5, 20, 17, 14, 7].
Reference: [24] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> SPIE, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: Data transfer can occur either through memory or through registers [7, 27], and synchronization can be performed either explicitly using wait/signal or implicitly using full/empty bits <ref> [6, 14, 24] </ref>. If multiple values are to be forwarded, the synchronization can occur at either a coarse granularity (once per epoch) or a fine granularity (once per value), as illustrated in Figure 5.
Reference: [25] <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: These regions were then inspected by hand to determine whether they were good candidates for exploiting TLDS. If so, these regions were explicitly identified to the simulator through their instruction addresses. The simulator reads sequential execution traces generated by the MIPS pixie utility <ref> [25] </ref> to measure the exact data dependences between epochs in each speculative region. For the sake of simplicity and simulation speed, instructions are assumed to execute on an ideal single-issue processor-in other words, we assume that one instruction is issued every cycle.
Reference: [26] <author> M. D. Smith. </author> <title> Support for Speculative Execution in High-Performance Processors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: data speculation (as we discuss in detail later in Section 4.2), our results in this paper demonstrate that TLDS can significantly increase the amount of thread-level parallelism (TLP) within non-numeric programs, and can be implemented in a cost-effective manner. 1.2 Related Work While instruction-level data speculation has received much attention <ref> [9, 12, 26] </ref>, there has been relatively little work so far on thread-level data speculation.
Reference: [27] <author> G. S. Sohi, S. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: 1 Introduction As the number of transistors that can be integrated onto a single VLSI chip continues its dramatic rate of increase, processor architects are faced with the pleasant challenge of finding the best way to translate these additional resources into improved performance. While there have been several proposals <ref> [6, 7, 14, 15, 23, 28, 27] </ref>, perhaps one of the more compelling options is to integrate multiple processors onto a single chip [5, 20, 17, 14, 7]. <p> In this section, we briefly discuss the two most relevant works to this study, the privatizing doall test [22] and the Wisconsin multiscalar architecture <ref> [7, 27] </ref>. 1.2.1 The Privatizing Doall Test Padua et al. [22] have devised a method of parallelizing loops for numeric codes in the presence of ambiguous data dependences. <p> Because of this, the PD test is only effective in parallelizing a narrow class of loops. 1.2.2 The Multiscalar Architecture The most relevant work to this study is the Wisconsin multiscalar architecture <ref> [7, 27, 8] </ref>. This architecture performs aggressive control and data speculation through the use of devoted hardware structures and complex forwarding mechanisms. This section describes the multiscalar architecture and how it executes an application, beginning with the compilation process. <p> Data transfer can occur either through memory or through registers <ref> [7, 27] </ref>, and synchronization can be performed either explicitly using wait/signal or implicitly using full/empty bits [6, 14, 24]. If multiple values are to be forwarded, the synchronization can occur at either a coarse granularity (once per epoch) or a fine granularity (once per value), as illustrated in Figure 5. <p> Therefore aggressive compiler scheduling can potentially eliminate the need for expensive forwarding hardware, thus allowing us to forward data through the normal cache hierarchy. 7 An example of fast forwarding support is the register forwarding mechanism proposed by the Multiscalar architecture <ref> [7, 27] </ref>. 14 in 10 cycles, fast = special hardware support to forward in 2 cycles, B = base TLDS hardware with no compiler support, Br = register dependences eliminated or forwarded, R = case R dependences eliminated, Sf = case S dependences eliminated and fine-grain synchronization, Ss = case S
Reference: [28] <author> D. M. Tullsen, S. J. Eggers, and H. M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June </month> <year> 1995. </year> <month> 28 </month>
Reference-contexts: 1 Introduction As the number of transistors that can be integrated onto a single VLSI chip continues its dramatic rate of increase, processor architects are faced with the pleasant challenge of finding the best way to translate these additional resources into improved performance. While there have been several proposals <ref> [6, 7, 14, 15, 23, 28, 27] </ref>, perhaps one of the more compelling options is to integrate multiple processors onto a single chip [5, 20, 17, 14, 7].
Reference: [29] <author> Michael Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year> <month> 29 </month>
Reference-contexts: We will now briefly describe how these optimizations would work. Dependences due to induction variables <ref> [29] </ref> may be eliminated, given that there is a mapping between each epoch number (described in Section 4.1) and the value of the induction variable for that epoch. This is the case if epoch numbers are consecutive integers, and they are somehow visible to software. <p> Memory allocation library routines may also cause RAW dependences between epochs. These routines would be trivial to parallelize, since each processor could maintain a free list of its own portion of available shared memory. Reductions <ref> [29] </ref> are another source of frequent RAW data dependences between epochs, and must be optimized to fully exploit TLDS. Since a reduction applies an associative operation to a variable, the order in which the operation is applied by different epochs does not matter.
References-found: 29

