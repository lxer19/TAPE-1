URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/98-08.ps
Refering-URL: http://www.cs.wisc.edu/~olvi/olvi.html
Root-URL: http://www.cs.wisc.edu
Email: email: paulb@cs.wisc.edu, olvi@cs.wisc.edu  
Title: k-Plane Clustering  
Author: P. S. Bradley and O. L. Mangasarian 
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: A finite new algorithm is proposed for clustering m given points in n-dimensional real space into k clusters by generating k planes that constitute a local solution to the nonconvex problem of minimizing the sum of squares of the 2-norm distances between each point and a nearest plane. The key to the algorithm lies in a formulation that generates a plane in n-dimensional space that minimizes the sum of the squares of the 2-norm distances to each of m 1 given points in the space. The plane is generated by an eigenvector corresponding to a smallest eigenvalue of an nfin simple matrix derived from the m 1 points. The algorithm was tested on the publicly available Wisconsin Breast Prognosis Cancer database to generate well separated patient survival curves. In contrast, the k-mean algorithm did not generate such well-separated survival curves.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. R. Anderberg. </author> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The justification for this approach is that data sometimes naturally falls into clusters grouped around flat surfaces such as planes. This approach yields interesting theoretical results that lead to an efficiently implementable algorithm which gives better computational results than the standard k-mean algorithm <ref> [1] </ref> on a publicly available dataset. We outline the contents of the paper now. In Section 2 we formulate the k-plane clustering problem and state the k-plane clustering algorithm. <p> The algorithm is similar to the k-mean <ref> [1] </ref> and k-median [4] algorithms in that it alternates between assigning points to a nearest cluster plane (Cluster Assignment) and, for a given cluster, computing a cluster plane that minimizes the sum of the squares of distances to all points in the cluster (Cluster Update). <p> In the second set of tests the ability to recover class labels by clustering unlabeled data was tested. In the first set of tests the kPC algorithm was tested on the Wisconsin Prognostic Breast Cancer (WPBC) Database [13] along with the k-mean algorithm <ref> [1] </ref> using only two features: tumor size and lymph node status. These two features were normalized to have zero mean and standard deviation 1. This dataset consists of 198 points in R 2 .
Reference: [2] <author> H. C. Andrews. </author> <title> Introduction to Mathematical Techniques in Pattern Recognition. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: 1 Introduction There are many approaches to clustering such as statistical <ref> [2, 9, 6] </ref>, machine learning [7, 8] and mathematical programming [15, 16, 4]. In this work we take a mathematical programming approach with a novel idea.
Reference: [3] <author> M. W. Berry, S. T. Dumais, and G. W. O'Brein. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year> <note> http://www.cs.utk.edu/~berry. </note>
Reference-contexts: A different clustering approach, latent semantic indexing, is given in <ref> [3] </ref> that also uses singular value decomposition. 5 We end this section by establishing the finiteness of the kPC Algorithm. Theorem 3.7 (Finite Termination of the kPC Algorithm 2.1) The kPC Algorithm 2.1 terminates in a finite number of steps at a cluster assignment that is locally optimal.
Reference: [4] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Clustering via concave minimization. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -9-, </booktitle> <pages> pages 368-374, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-03.ps.Z. </publisher>
Reference-contexts: 1 Introduction There are many approaches to clustering such as statistical [2, 9, 6], machine learning [7, 8] and mathematical programming <ref> [15, 16, 4] </ref>. In this work we take a mathematical programming approach with a novel idea. <p> The algorithm is similar to the k-mean [1] and k-median <ref> [4] </ref> algorithms in that it alternates between assigning points to a nearest cluster plane (Cluster Assignment) and, for a given cluster, computing a cluster plane that minimizes the sum of the squares of distances to all points in the cluster (Cluster Update).
Reference: [5] <author> T. Cavalier and B. Melloy. </author> <title> An iterative linear programming solution to the Euclidean regression model. </title> <journal> Computers and Operations Research, </journal> <volume> 28 </volume> <pages> 781-793, </pages> <year> 1995. </year>
Reference-contexts: It is the latter computation, which is a one step replacement of an algorithm for the Euclidean Regression Problem <ref> [17, 5] </ref> which does not use squared distances, that makes the following kPC algorithm possible.
Reference: [6] <author> G. Celeux and G. Govaert. </author> <title> Gaussian parsimonious clustering models. </title> <journal> Pattern Recognition, </journal> <volume> 28 </volume> <pages> 781-793, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction There are many approaches to clustering such as statistical <ref> [2, 9, 6] </ref>, machine learning [7, 8] and mathematical programming [15, 16, 4]. In this work we take a mathematical programming approach with a novel idea.
Reference: [7] <author> D. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year> <month> 9 </month>
Reference-contexts: 1 Introduction There are many approaches to clustering such as statistical [2, 9, 6], machine learning <ref> [7, 8] </ref> and mathematical programming [15, 16, 4]. In this work we take a mathematical programming approach with a novel idea.
Reference: [8] <author> M. H. Hassoun. </author> <title> Fundamentals of Artificial Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction There are many approaches to clustering such as statistical [2, 9, 6], machine learning <ref> [7, 8] </ref> and mathematical programming [15, 16, 4]. In this work we take a mathematical programming approach with a novel idea.
Reference: [9] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction There are many approaches to clustering such as statistical <ref> [2, 9, 6] </ref>, machine learning [7, 8] and mathematical programming [15, 16, 4]. In this work we take a mathematical programming approach with a novel idea.
Reference: [10] <author> E. L. Kaplan and P. Meier. </author> <title> Nonparametric estimation from incomplete observations. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 53 </volume> <pages> 457-481, </pages> <year> 1958. </year>
Reference-contexts: This dataset consists of 198 points in R 2 . The number of clusters was set to 3 (k = 3) in an attempt to find 3 groups of patients with distinct survival characteristics (see Kaplan-Meier survival curves <ref> [10, 11] </ref> were constructed for each cluster, representing expected percent of surviving patients as a function of time, for patients in that cluster. Figure 1 depicts the three planes (lines in R 2 ) obtained by the kPC Algorithm 2.1.
Reference: [11] <author> David G. Kleinbaum. </author> <title> Survival Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: This dataset consists of 198 points in R 2 . The number of clusters was set to 3 (k = 3) in an attempt to find 3 groups of patients with distinct survival characteristics (see Kaplan-Meier survival curves <ref> [10, 11] </ref> were constructed for each cluster, representing expected percent of surviving patients as a function of time, for patients in that cluster. Figure 1 depicts the three planes (lines in R 2 ) obtained by the kPC Algorithm 2.1.
Reference: [12] <author> O. L. Mangasarian. </author> <title> Arbitrary-norm separating plane. </title> <type> Technical Report 97-07, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <month> May </month> <year> 1997. </year> <note> Operations Research Letters, submitted. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-07.ps.Z. </note>
Reference-contexts: We give in the next section the theoretical justification for the kPC algorithm and establish its finite termination. 2 3 Theoretical Justification of kPC Algorithm We first note that the cluster assignment rule defined in Step (a) of the kPC Algorithm 2.1 follows from the well known fact <ref> [12] </ref> that the 2-norm distance between between a point A i 2 R n and the plane P ` := fx j x 2 R n ; x 0 w ` = fl ` g is jA i w ` fl ` j=kw ` k = jA i w ` fl
Reference: [13] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <type> Technical report, </type> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <year> 1992. </year> <note> www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: In the second set of tests the ability to recover class labels by clustering unlabeled data was tested. In the first set of tests the kPC algorithm was tested on the Wisconsin Prognostic Breast Cancer (WPBC) Database <ref> [13] </ref> along with the k-mean algorithm [1] using only two features: tumor size and lymph node status. These two features were normalized to have zero mean and standard deviation 1. This dataset consists of 198 points in R 2 . <p> Similarly, testing correctness at trial j is the percentage of T j correctly classified by the majority label of the cluster that the point was assigned to. Table 1 summarizes average training and testing results on 2 publicly available datasets <ref> [13] </ref>. The Johns Hopkins Ionosphere dataset consists of 351 data points with 34 real-valued features characterizing radar returns from the ionosphere. One class corresponds to radar returns showing evidence of structure. The other class corresponds to those returns showing no structure.
Reference: [14] <author> B. Noble and J. W. Daniel. </author> <title> Applied Linear Algebra. </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, New Jersey, third edition, </address> <year> 1988. </year>
Reference-contexts: is the smallest eigenvalue of B and w is its corresponding eigenvector. 2 Remark 3.6 Relation to Singular Value Decomposition It can be shown, after some straightforward algebra, that the w obtained in the above Theorem 3.5 can also be obtained by taking a singular value decomposition U SV 0 <ref> [14, 19] </ref> of the m fi n matrix: H := (I m where U and V are orthogonal matrices of dimensions m fi m and n fi n respectively, and S is an m fi n diagonal matrix with nonnegative diagonal elements in decreasing order. <p> It can then be shown that the desired w given by Theorem 3.5 corresponds to the last column of the matrix V corresponding to a smallest singular value of H, and fl is again given by (12) above. This result can be derived by noting that <ref> [14, Theorem 8.19] </ref> the squares of the singular values of H (possibly with some zeros added) are also the eigenvalues of both HH 0 and H 0 H with associated eigenvectors being columns of U and V respectively.
Reference: [15] <author> M. R. Rao. </author> <title> Cluster analysis and mathematical programming. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 66 </volume> <pages> 622-626, </pages> <year> 1971. </year>
Reference-contexts: 1 Introduction There are many approaches to clustering such as statistical [2, 9, 6], machine learning [7, 8] and mathematical programming <ref> [15, 16, 4] </ref>. In this work we take a mathematical programming approach with a novel idea.
Reference: [16] <author> S. Z. Selim and M. A. Ismail. </author> <title> K-Means-Type algorithms: a generalized convergence theorem and characterization of local optimality. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6:81-87, </volume> <year> 1984. </year>
Reference-contexts: 1 Introduction There are many approaches to clustering such as statistical [2, 9, 6], machine learning [7, 8] and mathematical programming <ref> [15, 16, 4] </ref>. In this work we take a mathematical programming approach with a novel idea.
Reference: [17] <author> H. Spath. </author> <title> Mathematical Algorithms for Linear Regression. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1992. </year>
Reference-contexts: It is the latter computation, which is a one step replacement of an algorithm for the Euclidean Regression Problem <ref> [17, 5] </ref> which does not use squared distances, that makes the following kPC algorithm possible.
Reference: [18] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: The datasets used here had two classes, hence k = 2 in these tests. A ten-fold 6 indicated by fl. Data assigned to Plane 2 is indicated by +. Data assigned to Plane 3 is indicated by 3. (a) kPC (b) k-Mean 7 cross-validation <ref> [18] </ref> scheme was employed. In this procedure the dataset is ran-domly divided into 10 disjoint sets of approximately equal size, T 1 ; T 2 ; : : : ; T 10 . Then 10 trials are conducted.
Reference: [19] <author> G. Strang. </author> <title> Introduction to Linear Algebra. </title> <publisher> Wellesley-Cambridge Press, </publisher> <address> Wellesley, MA, </address> <year> 1993. </year> <month> 10 </month>
Reference-contexts: is the smallest eigenvalue of B and w is its corresponding eigenvector. 2 Remark 3.6 Relation to Singular Value Decomposition It can be shown, after some straightforward algebra, that the w obtained in the above Theorem 3.5 can also be obtained by taking a singular value decomposition U SV 0 <ref> [14, 19] </ref> of the m fi n matrix: H := (I m where U and V are orthogonal matrices of dimensions m fi m and n fi n respectively, and S is an m fi n diagonal matrix with nonnegative diagonal elements in decreasing order.
References-found: 19

