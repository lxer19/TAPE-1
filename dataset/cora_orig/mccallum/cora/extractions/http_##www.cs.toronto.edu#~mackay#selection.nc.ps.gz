URL: http://www.cs.toronto.edu/~mackay/selection.nc.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Email: mackay@hope.caltech.edu  
Title: Information-based objective functions for active data selection  
Author: David J.C. MacKay 
Date: 589-603  
Note: Appeared in Neural Computation 4 4 pp.  
Address: Pasadena CA 91125  
Affiliation: Computation and Neural Systems California Institute of Technology 139-74  
Abstract: Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed which measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness. 
Abstract-found: 1
Intro-found: 1
Reference: <author> E.B. </author> <title> Baum (1991). `Neural net algorithms that learn in polynomial time from examples and queries', </title> <journal> IEEE Trans. on neural networks 2 1, </journal> <pages> 5-19. </pages>
Reference: <author> J. </author> <title> Berger (1985). Statistical decision theory and Bayesian analysis, </title> <publisher> Springer. </publisher>
Reference-contexts: Bayesian inference is consistent with this principle; there is no need to undo biases introduced by the data collecting strategy, because it is not possible for such biases to be introduced | as long as we perform inference using all the data gathered <ref> (Berger, 1985, Loredo, 1989) </ref>. When the models are concerned with estimating the distribution of output variables t given input variables x, we are allowed to look at the x value of a datum, and decide whether or not to include the datum in the data set.
Reference: <author> M.A. </author> <month> El-Gamal </month> <year> (1991). </year> <title> `The role of priors in active Bayesian learning in the sequential statistical decision framework', in Maximum Entropy and Bayesian Methods, </title> <editor> W.T. Grandy, Jr. and L.H. Schick, eds., </editor> <publisher> Kluwer, </publisher> <pages> 33-38. </pages>
Reference: <author> V.V. </author> <month> Fedorov </month> <year> (1972). </year> <title> `Theory of optimal experiments', </title> <publisher> Academic press. </publisher>
Reference-contexts: The more complex task of selecting multiple new data points will not be addressed here, but the methods used can be generalised to solve this task, as is discussed in <ref> (Fedorov, 1972, Luttrell, 1985) </ref>. The similar problem of choosing the x N+1 at which a vector of outputs t N+1 is measured will not be addressed either. The first and third definitions of information gain have both been studied in the abstract by Lindley (1956). <p> Let us now see what property of a datum causes it to be maximally informative. The new entropy S N+1 is equal to 1 2 log m 2 det A N+1 , neglecting additive constants. This determinant can be analytically evaluated <ref> (Fedorov, 1972) </ref>, using the identities [A + figg T ] 1 fiA 1 gg T A 1 and det [A + figg T ] = (det A)(1 + fig T A 1 g); (8) from which we obtain: Total information gain = 1 log m 2 det A = 2 6 <p> This rule is the same as that resulting from the `D-optimal' and `minimax' design criteria <ref> (Fedorov, 1972) </ref>. For many interpolation models, the error bars are largest beyond the most extreme points where data have been gathered. <p> This is the same as the `Q-optimal' design <ref> (Fedorov, 1972) </ref>. <p> The solutions apply to linear and non-linear interpolation models, but depend on the validity of a local gaussian approximation. Each solution has an analog in the non-Bayesian literature <ref> (Fedorov, 1972) </ref>, and generalisations to multiple measurements and multiple output variables can be found there, and also in (Luttrell, 1985). In each case a function of x has been derived that predicts the information gain for a measurement at that x.
Reference: <author> J-N. Hwang, J.J. Choi, S. Oh, </author> <title> and R.J. Marks II (1991). `Query-based learning applied to partially trained multilayer perceptrons', </title> <journal> IEEE Trans. on neural networks 2 1, </journal> <pages> 131-136. </pages>
Reference: <author> E.T. </author> <title> Jaynes (1986). `Bayesian methods: general background', in Maximum Entropy and Bayesian Methods in applied statistics, </title> <editor> ed. J.H. Justice, </editor> <publisher> C.U.P. </publisher>
Reference: <author> D.V. </author> <title> Lindley (1956). `On a measure of the information provided by an experiment', </title> <journal> Ann. Math. Statist. </journal> <volume> 27, </volume> <pages> 986-1005. </pages>
Reference-contexts: This proof also implicitly demonstrates that E (S) is independent of the measure m (w). Other properties of E (S) are proved in <ref> (Lindley, 1956) </ref>. The rest of this paper will use S as the information measure, with m (w) set to a constant. 3 Maximising total information gain Let us now solve the first task: how to choose x N+1 so that the expected information gain about w is maximised. <p> Then the parameter vector w and the values of the interpolant fy (u) g are in one to one (locally) linear correspondence with each other. This means that the change in entropy of P (fy (u) g) is identical to the change in entropy of P (w) <ref> (Lindley, 1956) </ref>. This 8 can be confirmed by substitution of Y 1 = G 1 AG 1 T into (12), which yields (9). <p> And this function could form the basis of a stopping rule, i.e. a rule for deciding whether to gather more data, given a desired exchange rate of information gain per measurement <ref> (Lindley, 1956) </ref>. A possible weakness of these information-based approaches is that they estimate the utility of a measurement assuming that the model is correct. This might lead to undesirable results. The search for ideal measures of data utility is still open. 13
Reference: <author> T.J. </author> <month> Loredo </month> <year> (1989). </year> <title> `From Laplace to supernova SN 1987A: Bayesian inference in astrophysics', in Maximum Entropy and Bayesian Methods, </title> <editor> ed. P. Fougere, </editor> <publisher> Kluwer. </publisher>
Reference: <author> S.P. </author> <month> Luttrell </month> <year> (1985). </year> <title> `The use of transinformation in the design of data sampling schemes for inverse problems', Inverse Problems 1, 199-218 D.J.C. MacKay (1991a) `Bayesian interpolation', Neural Computation, this volume. </title>
Reference-contexts: The solutions apply to linear and non-linear interpolation models, but depend on the validity of a local gaussian approximation. Each solution has an analog in the non-Bayesian literature (Fedorov, 1972), and generalisations to multiple measurements and multiple output variables can be found there, and also in <ref> (Luttrell, 1985) </ref>. In each case a function of x has been derived that predicts the information gain for a measurement at that x. This function can be used to search for an optimal value of x (which in large-dimensional input spaces may not be a trivial task).
Reference: <author> D.J.C. </author> <title> MacKay (1991b) `A practical Bayesian framework for backprop networks', Neural Computation, this volume. </title>
Reference-contexts: We will use this quadratic approximation from here on. If M has other minima, those can be treated as distinct models as in <ref> (MacKay, 1991b) </ref>. First we will need to know what the entropy of a gaussian distribution is. <p> The data were generated from a smooth function by adding noise with standard deviation oe = 0:05. The neural network was adapted to the data using weight decay terms ff c which were controlled using the methods of <ref> (MacKay, 1991b) </ref> and noise level fi fixed to 1=oe 2 . The data and the resulting interpolant, with error bars, are shown in figure 1a. The expected total information gain, i.e. the change in entropy of the parameters, is shown as a function of x in figure 1b.
Reference: <author> D.J.C. </author> <title> MacKay (1991d) `The evidence framework applied to classification networks', </title> <note> in preparation. </note>
Reference-contexts: In contrast, this paper (which discusses noisy interpolation problems) derives criteria from defined objective functions; each objective function leads to a different data selection criterion. A future paper will discuss the application of the same ideas to classification problems <ref> (MacKay, 1991d) </ref>.
Reference: <author> M. Plutowski and H. </author> <title> White (1991). `Active selection of training examples for network learning in noiseless environments', </title> <institution> Dept. Computer Science, </institution> <type> UCSD, TR 90-011. </type>
Reference: <author> J. </author> <title> Skilling (1992). `Bayesian solution of ordinary differential equations', in Maximum Entropy and Bayesian Methods, Seattle 1991, </title> <editor> G.J. Erickson and C.R. Smith, eds., </editor> <publisher> Kluwer. </publisher>
Reference-contexts: This work was directly stimulated by a presentation given by John Skilling at Maxent 91 <ref> (Skilling, 1992) </ref>.
Reference: <author> I thank Allen Knutsen, Tom Loredo, </author> <title> Marcus Mitchell and the referees for helpful feedback. This work was supported by a Caltech Fellowship and a Studentship from SERC, </title> <address> UK. </address> <month> 14 </month>
References-found: 14

