URL: ftp://ftp.cs.umass.edu/pub/anw/pub/dprecup/Precup-Utgoff-ICML98.ps
Refering-URL: http://www.cs.wisc.edu/icml98/schedule.html
Root-URL: 
Email: dprecup@cs.umass.edu  utgoff@cs.umass.edu  
Title: Classification Using -Machines and Constructive Function Approximation  
Author: Doina Precup Paul E. Utgoff 
Address: Amherst, MA 01003-4610  Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  Department of Computer Science University of Massachusetts  
Abstract: The classification algorithm CLEF combines a version of a linear machine known as a - machine with a non-linear function approxima-tor that constructs its own features. The algorithm finds non-linear decision boundaries by constructing features that are needed to learn the necessary discriminant functions. The CLEF algorithm is proven to separate all consistently labelled training instances, even when they are not linearly separable in the input variables. The algorithm is illustrated on a variety of tasks, showing an improvement over C4.5, a state-of-art de cision tree learning algorithm.
Abstract-found: 1
Intro-found: 1
Reference: <author> Duda, R. O. & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <publisher> Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: In the case of Boolean input variables, one alternative would be to choose f j from a set of basis functions, such as Rademacher-Walsh or Bahadur-Lazarsfeld polynomials <ref> (Duda and Hart, 1973) </ref>.
Reference: <author> Elomaaa, T. & Rousu, J. </author> <year> (1996). </year> <title> Finding optimal multi-splits for numerical attributes in decision tree learning. </title> <type> Technical Report NC-TR-96-041, </type> <institution> NeuroCOLT. </institution>
Reference: <author> Fahlman, S. E. & Lebiere, C. </author> <year> (1990). </year> <title> The cascade correlation architecture. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <pages> Volume 2 (pp. 524532). </pages>
Reference: <author> Fayyad, U. & Irani, K. </author> <year> (1993). </year> <title> Multi-interval discretiza-tion of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the Thirteen International Joint Conference on Artificial Intelligence (pp. </booktitle> <volume> 1022 1027). </volume> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> Small nets and short paths: Optimising neural computation. </title> <type> PhD thesis, </type> <institution> Center for Cognitive Science, University of Edinburgh. </institution>
Reference-contexts: If no linear separation can be found given the current feature set, by gradually reducing the size of the corrections, the weights will still settle into a particular range <ref> (Frean, 1990) </ref>. In this case, a new feature will be added, and training will resume with a new machine. In the worst case, the process will continue until all the 2 n features that are possible have been generated.
Reference: <author> Fulton, T., Kasif, S. & Salzberg, S. </author> <year> (1995). </year> <title> Efficient algorithms for finding multi-way splits for decision trees. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <volume> 244251). </volume> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Hanson, S. J. </author> <year> (1990). </year> <title> Meiosis networks. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <pages> Volume 2 (pp. 533541). </pages>
Reference-contexts: Such a unit is split into two units, and the weights are set so that the units are moved apart from each other along an advantageous axis. A meiosis network <ref> (Hanson, 1990) </ref> is a feed-forward network in which the variance of each weight is maintained. For a hidden unit (feature) that has one or more weights of high variance, the unit is split into two.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994). </year> <title> UCI repository of machine learning databases. </title> <institution> University of California, Irvine, CA: Department of Information and Computer Science. </institution>
Reference-contexts: Feature interpretation can be generated automatically, by printing the negation of each test for which there is a `0' in the feature's pattern. Table 1 illustrates the features that have been constructed for one of the units (discriminant functions) in the hepatitis task from the UCI data repository <ref> (Murphy and Aha, 1994) </ref>. This is a two-class problem, thus the corresponding linear machine will have two discriminant functions, one for each class. However, due to the training procedure, these discriminant functions are always trained with equal amounts of error having opposite signs. <p> other classification algorithms? Will it find a separating -machine in a reasonable amount of time? Will it construct a large number of features, perhaps producing an incomprehensible classifier? In order to answer these questions empirically, CLEF and C4.5 were run on several classification tasks, mostly from the UCI data repository <ref> (Murphy and Aha, 1994) </ref>. This allows for a comparison in terms of classification accuracy, and provides some insight on the efficiency of CLEF and the form of the function it provides.
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: To evaluate an instance using a discriminant function, one computes the linear combination of the feature values and feature weights. To update the approximation, the training procedure revisits the training instances and adjusts the weights of the discriminant functions using the fractional error correction rule <ref> (Nilsson, 1965) </ref>. Only features that matched the instance have their weights adjusted, because features that did not match have value 0. For each feature, the algorithm keeps track of the errors associated with each input bit, in order to determine which feature is having the greatest difficulty in fitting.
Reference: <author> Parekh, R., Yang, J. & Honavar, V. </author> <year> (1997). </year> <title> Constructive neural network learning algorithms for multi-category real-valued pattern classification. </title> <type> Technical report, </type> <institution> Iowa State University, Computer Science Department. </institution>
Reference-contexts: A large class of algorithms construct networks of thresholded logic units, by adding boundaries that correct for misclassified examples <ref> (Parekh et. al, 1997) </ref>. These algorithms also separate consistently labelled examples. The experimental results that have been published regarding these algorithms are limited, so they do not provide a good basis for comparison with CLEF.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for machine learning. </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: 2.3 78.4 2.8 va 28.1 12.7 26.7 10.0 32.9 7.2 votes 95.7 3.7 96.6 3.3 94.3 3.1 waveform 69.7 10.4 70.0 10.7 73.9 9.1 wine 93.3 6.0 93.3 6.0 94.2 8.3 zoo 92.7 6.8 91.8 6.4 96.4 4.5 Table 3: Duncan Multiple Range Test C4.5 C4.5p CLEF 69.6 71.4 77.3 <ref> (Quinlan, 1993) </ref>, both with and without pruning. The reason for including the results without pruning as well is that CLEF does not currently use any mechanism for avoiding overfitting.
Reference: <author> Utgoff, P. E. & Precup, D. </author> <year> (1998). </year> <title> Constructive function approximation. </title> <editor> In H. Motoda & H. Liu (Eds.), </editor> <title> Feature extraction, construction, and selection: A data-mining perspective. </title> <publisher> Kluwer. </publisher>
Reference-contexts: The search for a good set of discriminant functions is therefore quite difficult. An automatic method for constructing a -machine adequate for the task at hand is needed. To this end, we use the ELF function approximation algorithm, <ref> (Utgoff & Precup, 1998) </ref> which constructs new features as needed, by identifying subsets of instances that share intrinsic properties. One could substitute ELF with any other algorithm that can automatically construct linearly independent features. ELF assumes that the instances are represented using Boolean input variables.
Reference: <author> Vapnik, V. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: The input weights that define the feature, and the output weight for the linear combination are altered so that the two units are moved away from their means in opposite directions. Support Vector Machines <ref> (Vapnik, 1995) </ref> can also be viewed as constructing features automatically, but the form of the features that are constructed needs to be defined a priori.
Reference: <author> Wynne-Jones, M. </author> <year> (1992). </year> <title> Node splitting: A constructive algorithm for feed-forward neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems (pp. </booktitle> <pages> 10721079). </pages>
References-found: 14

