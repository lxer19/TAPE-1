URL: http://ai.fri.uni-lj.si/matjazk/bibliography/aimed96.ps
Refering-URL: http://ai.fri.uni-lj.si/matjazk/bibliography/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: fmatjaz.kukar, igor.kononenkog@fer.uni-lj.si  
Title: Machine learning in prognosis of the femoral neck fracture recovery examples, estimating attributes, explanation ability,
Author: Matjaz Kukar Igor Kononenko Tomaz Silvester 
Keyword: learning  
Note: from  
Address: SI-61001 Ljubljana, Slovenia tel./fax: +386-61-1768386  
Affiliation: University of Ljubljana Faculty of computer and information science, Trzaska 25 Medical faculty, Zaloska 2  
Abstract: We compare the performance of several machine learning algorithms in the problem of prognos-tics of the femoral neck fracture recovery: the K-nearest neighbours algorithm, the semi-naive Bayesian classifier, backpropagation with weight elimination learning of the multilayered neural networks, the LFC (lookahead feature construction) algorithm, and the Assistant-I and Assistant-R algorithms for top down induction of decision trees using information gain and RELIEFF as search heuristics, respectively. We compare the prognostic accuracy and the explanation ability of different classifiers. Among the different algorithms the semi-naive Bayesian classifier and Assistant-R seem to be the most appropriate. We analyze the combination of decisions of several classifiers for solving prediction problems and show that the combined classifier improves both performance and the explanation ability. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Cestnik. </author> <title> Estimating probabilities: A crucial task in machine learning. </title> <booktitle> In Proc. European Conference on Artificial Intelligence 1990, </booktitle> <pages> pages 147-149, </pages> <address> Stockholm, Sweden, </address> <year> 1990. </year> <month> 26 </month>
Reference-contexts: For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval <ref> [0; 1] </ref>. Normalization with n guarantees that all weights are in the interval [1; 1]. The function `diff' is used also for calculating the distance between instances to find the nearest neighbors. <p> For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval [0; 1]. Normalization with n guarantees that all weights are in the interval <ref> [1; 1] </ref>. The function `diff' is used also for calculating the distance between instances to find the nearest neighbors. The total distance is simply the sum of differences over all attributes (in fact the original RELIEF uses the squared difference, which for discrete attributes is equivalent to `diff'. <p> estimating probabilities more reliably and extended it to deal with incomplete and multi-class data sets. 4 Another difference between Assistant and Assistant-R is that, wherever appropriate, instead of the relative frequency, Assistant-R uses the m-estimate of probabilities, which was shown to often significantly increase the performance of machine learning algorithms <ref> [1, 2, 5, 34] </ref>. <p> Both estimates are very useful especially when estimating probabilities of small datasets. In our experiments, the parameter m was set to 2 (this setting is usually used as default and, empirically, gives satisfactory results <ref> [1, 2] </ref> although with tuning better results may be expected). The m-estimate is used in the naive Bayesian formula (1), for postpruning instead of Laplace's law of succession as proposed by Cestnik and Bratko [2], and for RELIEFF's estimates of probabilities.
Reference: [2] <author> B. Cestnik and I. Bratko. </author> <title> On estimating probabilities in tree pruning. </title> <editor> In Y. Kodratoff, editor, </editor> <booktitle> Proc. European Working Session on Learning, </booktitle> <pages> pages 138-150, </pages> <address> Porto, Portugal, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: estimating probabilities more reliably and extended it to deal with incomplete and multi-class data sets. 4 Another difference between Assistant and Assistant-R is that, wherever appropriate, instead of the relative frequency, Assistant-R uses the m-estimate of probabilities, which was shown to often significantly increase the performance of machine learning algorithms <ref> [1, 2, 5, 34] </ref>. <p> Both estimates are very useful especially when estimating probabilities of small datasets. In our experiments, the parameter m was set to 2 (this setting is usually used as default and, empirically, gives satisfactory results <ref> [1, 2] </ref> although with tuning better results may be expected). The m-estimate is used in the naive Bayesian formula (1), for postpruning instead of Laplace's law of succession as proposed by Cestnik and Bratko [2], and for RELIEFF's estimates of probabilities. <p> The m-estimate is used in the naive Bayesian formula (1), for postpruning instead of Laplace's law of succession as proposed by Cestnik and Bratko <ref> [2] </ref>, and for RELIEFF's estimates of probabilities. Assistant-I is a variant of Assistant-R that, instead of RELIEFF uses information gain for the selection criterion, as does Assistant. However, the other differences to Assistant remain (m-estimate of probabilities). This algorithm enables us to evaluate the contribution of RELIEFF.
Reference: [3] <author> B. Cestnik, I. Kononenko, and I. Bratko. </author> <title> ASSISTANT 86: A knowledge elicitation tool for sophisticated users. </title> <editor> In I. Bratko and N. Lavrac, editors, </editor> <booktitle> Progress in Machine Learning. </booktitle> <publisher> Sigma Press, </publisher> <address> Wilmslow, England, </address> <year> 1987. </year>
Reference-contexts: The trivial `default classifier' would simply classify every instance into `majority class' (the class which most frequently occurs in the training set). 2.2 Assistant-R and Assistant-I Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees <ref> [3] </ref>. The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. [9] and reimplemented by several authors (see [26] for an overview).
Reference: [4] <author> M. W. Craven and J. W. Shavlik. </author> <title> Learning symbolic rules using artificial neural networks. </title> <booktitle> In Proc. 10 th Intern. Conf. on Machine Learning, </booktitle> <pages> pages 73-80, </pages> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is due to the large number of real-valued weights which all influence the result. In some cases it is possible to extract symbolic rules from the trained neural network. However, the rules tend to be large and relatively complex. Craven and Shavlik <ref> [4] </ref> compare rules extracted from a neural network with rules produced by Quinlan's C4.5 system [25]. The rules for a `NetTalk' data set extracted from a neural network have, on average, over 30 antecedents per rule compared to 2 antecedens for C4.5.
Reference: [5] <author> J. Cussens. </author> <title> Bayes and pseudo-Bayes estimates of conditional probabilities and their reliability. </title> <booktitle> In Proc. European Conf. on Machine Learning, </booktitle> <pages> pages 136-152, </pages> <address> Vienna, Austria, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: estimating probabilities more reliably and extended it to deal with incomplete and multi-class data sets. 4 Another difference between Assistant and Assistant-R is that, wherever appropriate, instead of the relative frequency, Assistant-R uses the m-estimate of probabilities, which was shown to often significantly increase the performance of machine learning algorithms <ref> [1, 2, 5, 34] </ref>.
Reference: [6] <author> T. G. Dietterich and J. W. Shavlik. </author> <booktitle> Readings in machine learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In recent years many of machine learning algorithms have been developed that can be used as efficient tools for the analysis of databases and for extracting the classification knowledge that can be used to solve new problems in the given problem domain <ref> [6, 21, 22] </ref>. There have been many applications of machine learning to medical diagnostic problems [7, 8, 15, 17, 24, 27, 31].
Reference: [7] <author> S. Hojker, I. Kononenko, A. Jauk, V. Fidler, and M. Porenta. </author> <booktitle> Expert system's development in the management of thyroid diseases. In Proc. European Congress for Nuclear Medicine, </booktitle> <address> Milano, Italy, </address> <year> 1988. </year>
Reference-contexts: There have been many applications of machine learning to medical diagnostic problems <ref> [7, 8, 15, 17, 24, 27, 31] </ref>. As medical prognosis is a difficult task for physicians due to large time delays (several years) in recognizing the correct prognosis [36], machine learning may be more useful for solving prognostic rather than diagnostic problems.
Reference: [8] <author> K. A. Horn, P. Compton, L. Lazarus, and J. R. Quinlan. </author> <title> An expert system for the interpretation of thyroid assays in a clinical laboratory. </title> <journal> The Australian Computer Journal, </journal> <volume> Vol. 17 No. 1 </volume> <pages> 7-11, </pages> <year> 1985. </year>
Reference-contexts: There have been many applications of machine learning to medical diagnostic problems <ref> [7, 8, 15, 17, 24, 27, 31] </ref>. As medical prognosis is a difficult task for physicians due to large time delays (several years) in recognizing the correct prognosis [36], machine learning may be more useful for solving prognostic rather than diagnostic problems.
Reference: [9] <author> E. Hunt, J. Martin, and P. Stone. </author> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. <ref> [9] </ref> and reimplemented by several authors (see [26] for an overview). The main features of the original Assistant are: Binarization of attributes: The algorithm generates binary decision trees by binarizing each attribute at each decision step.
Reference: [10] <author> K. Kira and L. Rendell. </author> <title> The feature selection problem: traditional methods and new algorithm. </title> <booktitle> In Proc. AAAI'92, </booktitle> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: The main difference between Assistant and its reimplementation Assistant-R is that RELIEFF is used for attribute selection. RELIEFF [16] is an extension of RELIEF <ref> [10, 11] </ref>. The key idea of RELIEF is to estimate attributes according to how well their values distinguish among the instances that are near to each other.
Reference: [11] <author> K. Kira and L. Rendell. </author> <title> A practical approach to feature selection. </title> <editor> In D. Sleeman and P. Edwards, editors, </editor> <booktitle> Proc. Intern. Conf. on Machine Learning, </booktitle> <pages> pages 249-256, </pages> <address> Aberdeen, UK, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The main difference between Assistant and its reimplementation Assistant-R is that RELIEFF is used for attribute selection. RELIEFF [16] is an extension of RELIEF <ref> [10, 11] </ref>. The key idea of RELIEF is to estimate attributes according to how well their values distinguish among the instances that are near to each other.
Reference: [12] <author> I. Kononenko and I. Bratko. </author> <title> Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> Vol. 6 </volume> <pages> 67-80, </pages> <year> 1991. </year>
Reference-contexts: Each system used the same subsets of instances for learning and for testing in order to provide the same experimental conditions. Besides the classification accuracy, we also measured the average information score <ref> [12] </ref>. This measure eliminates the influence of prior probabilities and deals appropriately with the probabilistic answers of a classifier.
Reference: [13] <author> I. Kononenko. </author> <title> Semi-naive Bayesian classifier. </title> <editor> In Y. Kodratoff, editor, </editor> <booktitle> Proc. European Working Session on Learning-91, </booktitle> <pages> pages 206-219, </pages> <address> Porto, Potrugal, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A new instance is classified into the class with maximum calculated probability. The m-estimate of probabilities was used (see Section 2.2) and the parameter m was set to 2 in all experiments. The semi-naive Bayesian classifier <ref> [13] </ref> addresses the independence assumption, which is often not justified. Usually, the attributes are defined by a human (for example in medical data), and 5 are therefore relatively independent, as humans tend to think linearly. However, it is not always so. <p> To avoid this problem, the algorithm should detect the dependencies among attributes and join dependent attributes together. In addition, instead of joining whole attributes, only single values of different attributes can be joined, providing a more flexible solution. The `semi-naive Bayesian classifier', described in more details in <ref> [13] </ref>, tries to solve this trade-off between non-naivety and reliability of approximations of probabilities.
Reference: [14] <author> I. Kononenko. </author> <title> Combining decisions of multiple rules. </title> <editor> In B. du Boulayand and V. Sgorev, editors, </editor> <booktitle> Artificial intelligence V: methodology, systems, applications. </booktitle> <publisher> Elsevier Science Publications, </publisher> <year> 1992. </year>
Reference-contexts: A combined system should also offer better explanation abilities and higher accuracy than any single classifier, no matter how fine tuned. There are many different methods for combining decisions. However, in previous experiments it was shown <ref> [14, 19] </ref> that in most cases the naive Bayesian combination outperforms other methods. Also, the explanation of the combined classifier is fairly straightforward. In essence, it is the same as that of the naive Bayesian classifier; except that instead of attributes we have the classifiers. <p> The classification of each algorithm is in the form of conditional probabilities P (C j jAlg i ) for every class C j . When the naive Bayesian combination <ref> [14, 34] </ref> is used, the probability of an instance belonging to the class C j is calculated with: P (C j jAlg 1 ; : : : ; Alg k ) = P (C j ) k Y P (C j jAlg i ) P (C j ) 22 6.2 Experimental
Reference: [15] <author> I. Kononenko. </author> <title> Inductive and Bayesian learning in medical diagnosis. </title> <journal> Applied Artificial Intelligence, </journal> <volume> Vol. 7 </volume> <pages> 317-337, </pages> <year> 1993. </year> <month> 27 </month>
Reference-contexts: There have been many applications of machine learning to medical diagnostic problems <ref> [7, 8, 15, 17, 24, 27, 31] </ref>. As medical prognosis is a difficult task for physicians due to large time delays (several years) in recognizing the correct prognosis [36], machine learning may be more useful for solving prognostic rather than diagnostic problems. <p> This approach is very similar to the approach used by domain experts who make decisions on the basis of previously known similar cases (see Figure 2). * Naive and semi-naive Bayes: Their decisions can be naturally interpreted as the the sum of information gains <ref> [15] </ref>.
Reference: [16] <author> I. Kononenko. </author> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <editor> In L. De Raedt and F. Bergadano, editors, </editor> <booktitle> Proc. European Conf. on Machine Learning, </booktitle> <pages> pages 171-182, </pages> <address> Catania, Italy, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: For classification, the `null' leaves are already labeled with the calculated class probability distribution and are used for classification in the same manner as ordinary leaves. The main difference between Assistant and its reimplementation Assistant-R is that RELIEFF is used for attribute selection. RELIEFF <ref> [16] </ref> is an extension of RELIEF [10, 11]. The key idea of RELIEF is to estimate attributes according to how well their values distinguish among the instances that are near to each other. <p> In all our experiments, there was no significant difference between results using `diff' or squared difference). The original RELIEF can deal with discrete and continuous attributes. However, it cannot deal with incomplete data and is limited to two-class problems only. Kononenko <ref> [16] </ref> showed that RELIEF's estimates are strongly related to impurity functions.
Reference: [17] <author> I. Kononenko, I. Bratko, and E. Roskar. </author> <title> Experiments in automatic learning of medical diagnostic rules. </title> <booktitle> In International School for the Synthesis of Expert's Knowledge Workshop, Bled, Slovenia, </booktitle> <year> 1984. </year>
Reference-contexts: There have been many applications of machine learning to medical diagnostic problems <ref> [7, 8, 15, 17, 24, 27, 31] </ref>. As medical prognosis is a difficult task for physicians due to large time delays (several years) in recognizing the correct prognosis [36], machine learning may be more useful for solving prognostic rather than diagnostic problems.
Reference: [18] <author> M. Kukar. </author> <title> An application of machine learning in the femoral neck fracture diagnosis. B.Sc. </title> <type> Thesis. </type> <institution> University of Ljubljana, Faculty of electrical eng. & computer science, Ljubljana, Slovenia, </institution> <year> 1993. </year> <note> In Slovene. </note>
Reference-contexts: Acknowledgements We thank prof. dr. Andrej Baraga and as. dr. France Zupancic from the University Trauma-tology Clinic in Ljubljana for enabling access to the data and commenting preliminary results in <ref> [18] </ref>. Assistant-I and Assistant-R were implemented by Edvard Simec, and LFC was reimple-mented by Marko Robnik. This work was supported by the Slovenian Ministry of Science and Technology.
Reference: [19] <author> M. Kukar. </author> <title> Multistrategy attribute learning. </title> <booktitle> In Proc. 3 rd electrotechnical and computer science conference ERK'94, Portoroz, Slovenia, </booktitle> <year> 1994. </year> <note> In Slovene. </note>
Reference-contexts: A combined system should also offer better explanation abilities and higher accuracy than any single classifier, no matter how fine tuned. There are many different methods for combining decisions. However, in previous experiments it was shown <ref> [14, 19] </ref> that in most cases the naive Bayesian combination outperforms other methods. Also, the explanation of the combined classifier is fairly straightforward. In essence, it is the same as that of the naive Bayesian classifier; except that instead of attributes we have the classifiers.
Reference: [20] <author> M. Li and P. Vitanyi. </author> <title> An introduction to Kolmogorov complexity and its applications. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: It is also formally founded by the `minimal description length priciple' <ref> [20] </ref>. The simplest among theories with the identical performance is also the most probable and the most transparent due to its simplicity. However, the optimal prediction does not use just the single best theory but rather all possible theories. <p> However, the optimal prediction does not use just the single best theory but rather all possible theories. The final decision should be the weighted combinations of decisions of different theories where the weights correspond to probabilities of the theories given the evidence <ref> [20] </ref>. This `principle of multiple explanations' is useful also in the cases where the explanation of a decision may be even more important than the accuracy of the decision. Our experiments confirm the multiple explanation principle.
Reference: [21] <author> R. S. Michalski, J. G. Carbonell, and T. M. Mitchell. </author> <title> Machine Learning: An Artificial Intelligence Approach. </title> <publisher> Tioga Publ. </publisher> <address> Comp., </address> <year> 1983. </year>
Reference-contexts: In recent years many of machine learning algorithms have been developed that can be used as efficient tools for the analysis of databases and for extracting the classification knowledge that can be used to solve new problems in the given problem domain <ref> [6, 21, 22] </ref>. There have been many applications of machine learning to medical diagnostic problems [7, 8, 15, 17, 24, 27, 31].
Reference: [22] <author> R. S. Michalski, J. G. Carbonell, and T. M. Mitchell. </author> <title> Machine Learning: An Artificial Intelligence Approach, volume II. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: In recent years many of machine learning algorithms have been developed that can be used as efficient tools for the analysis of databases and for extracting the classification knowledge that can be used to solve new problems in the given problem domain <ref> [6, 21, 22] </ref>. There have been many applications of machine learning to medical diagnostic problems [7, 8, 15, 17, 24, 27, 31].
Reference: [23] <author> T. Niblett and I. Bratko. </author> <title> Learning decision rules in noisy domains. </title> <booktitle> In Proc. Expert Systems 86, </booktitle> <address> Brighton, UK, </address> <year> 1986. </year>
Reference-contexts: For prepruning, three user-defined thresholds are provided: minimal number of training instances, minimal attribute information gain and maximal probability of the majority class in the current node. For postpruning, the method developed by Niblett and Bratko <ref> [23] </ref> is used. It uses Laplace's law of succession for estimating the expected classification error of the current node committed by pruning or not pruning its subtree.
Reference: [24] <author> V. Pirnat, I. Kononenko, T. Janc, and I. Bratko. </author> <title> Medical estimation of automatically induced decision rules. </title> <booktitle> In Proc. of 2nd Europ. Conf. on Artificial Intelligence in Medicine, </booktitle> <pages> pages 24-36. </pages> <address> City University, </address> <year> 1989. </year>
Reference-contexts: There have been many applications of machine learning to medical diagnostic problems <ref> [7, 8, 15, 17, 24, 27, 31] </ref>. As medical prognosis is a difficult task for physicians due to large time delays (several years) in recognizing the correct prognosis [36], machine learning may be more useful for solving prognostic rather than diagnostic problems. <p> Correspondingly, the paths from the root to the leaves are shorter, contaning only few attributes, although they are the most informative. In many cases the domain experts feel that such a tree describes the patients too poorly to make reliable decisions <ref> [24] </ref>. * Lookahead feature construction (LFC) also generates decision trees. However, in each node a complex logical expression is used instead of a simple attribute value (Figure 5). The generated trees can therefore be smaller and often easier to understand. The expressions often represent valid concepts from the domain theory.
Reference: [25] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: In some cases it is possible to extract symbolic rules from the trained neural network. However, the rules tend to be large and relatively complex. Craven and Shavlik [4] compare rules extracted from a neural network with rules produced by Quinlan's C4.5 system <ref> [25] </ref>. The rules for a `NetTalk' data set extracted from a neural network have, on average, over 30 antecedents per rule compared to 2 antecedens for C4.5.
Reference: [26] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> Vol. 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. [9] and reimplemented by several authors (see <ref> [26] </ref> for an overview). The main features of the original Assistant are: Binarization of attributes: The algorithm generates binary decision trees by binarizing each attribute at each decision step. At each step the binarized version of each attribute is selected so that it maximizes the information gain of the attribute.
Reference: [27] <author> J.R. Quinlan, P. Compton, K.A. Horn, and L. Lazarus. </author> <title> Inductive knowledge acquisition: A case study. </title> <editor> In J.R. Quinlan, editor, </editor> <booktitle> Applications of expert systems. </booktitle> <publisher> Turing Institute Press & Addison-Wesley, </publisher> <year> 1987. </year> <booktitle> (Also in Proc. 2 nd Australian Conf. on Applications of Expert Systems, </booktitle> <address> Sydney, </address> <month> May </month> <year> 1986). </year>
Reference-contexts: There have been many applications of machine learning to medical diagnostic problems <ref> [7, 8, 15, 17, 24, 27, 31] </ref>. As medical prognosis is a difficult task for physicians due to large time delays (several years) in recognizing the correct prognosis [36], machine learning may be more useful for solving prognostic rather than diagnostic problems.
Reference: [28] <author> H. Ragavan and L. Rendell. </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Proc. 10 th Intern. Conf. on Machine Learning, </booktitle> <pages> pages 252-259, </pages> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: E 0 X w 2 w 2 1 + ij 0 However, this change introduces a new parameter which needs to be large enough to eliminate redundant weights and small enough not to disturb the error surface too much. 2.6 LFC Ragavan and Rendell <ref> [28] </ref>, and Ragavan et al. [29] use limited lookahead in their LFC (Lookahead Feature Construction) algorithm for top down induction of decision trees to detect significant dependencies between attributes for constructive induction. They show interesting results on some data sets.
Reference: [29] <author> H. Ragavan, L. Rendell, M. Shaw, and A. Tessmer. </author> <title> Learning complex real-world concepts through feature construction. </title> <type> Technical report UIUC-BI-AI-93-03, </type> <institution> The Beckman Institute, University of Illinois, </institution> <year> 1993. </year> <month> 28 </month>
Reference-contexts: E 0 X w 2 w 2 1 + ij 0 However, this change introduces a new parameter which needs to be large enough to eliminate redundant weights and small enough not to disturb the error surface too much. 2.6 LFC Ragavan and Rendell [28], and Ragavan et al. <ref> [29] </ref> use limited lookahead in their LFC (Lookahead Feature Construction) algorithm for top down induction of decision trees to detect significant dependencies between attributes for constructive induction. They show interesting results on some data sets. The reimplementation that we used in our experiments was developed by Robnik [30].
Reference: [30] <author> M. Robnik. </author> <title> Constructive induction with decision trees. B.Sc. </title> <type> Thesis. </type> <institution> University of Ljubl--jana, Faculty of electrical eng. & computer science, Ljubljana, Slovenia, </institution> <year> 1993. </year> <note> In Slovene. </note>
Reference-contexts: They show interesting results on some data sets. The reimplementation that we used in our experiments was developed by Robnik <ref> [30] </ref>. LFC generates binary decision trees. At each node, the algorithm constructs new binary attributes from the original attributes, using logical operators (conjunction, disjunction, and negation).
Reference: [31] <author> E. Roskar, P. Abrams, I. Bratko, and I. Kononenko. </author> <title> MCUDS an expert system for the diagnostics of lower urinary tract disorders. </title> <journal> Journal of Biomedical Measurements, Informatics and Control, </journal> <volume> Vol. 1, No. 4 </volume> <pages> 201-204, </pages> <year> 1986. </year>
Reference-contexts: There have been many applications of machine learning to medical diagnostic problems <ref> [7, 8, 15, 17, 24, 27, 31] </ref>. As medical prognosis is a difficult task for physicians due to large time delays (several years) in recognizing the correct prognosis [36], machine learning may be more useful for solving prognostic rather than diagnostic problems.
Reference: [32] <editor> D.E. Rumelhart and J. L. McClelland. </editor> <booktitle> Parallel Distributed Processing, volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: i A l ) (5) To join the two values two conditions should be satisfied: the values of (4) and (5) should be sufficiently different while the approximation of P (C j jA i A l ) remains sufficiently reliable. 2.5 Backpropagation with weight elimination A multilayered feedforward neural network <ref> [32] </ref> is a hierarchical network consisting of fully interconetced `layers' of processing `units' (often called `neurons'). The output of each unit is connected to every unit in the next layer. A network consists of at least two layers the input and the output. <p> = ffi pj output pi ; with ffi pj being ffi pj = &gt; &lt; (expected pj output pj ) output pj (1 output pj ); for output units output pj (1 output pj ) P k ffi pk w kj ; for hidden units (9) It can be shown <ref> [32] </ref> that modifying weights using the formula (9) implements a gradient descent on the error surface obtained from (8). Various improvements of the basic learning process are possible, such as including a `momentum term' [32], which prevents the weight oscillations. <p> ) P k ffi pk w kj ; for hidden units (9) It can be shown <ref> [32] </ref> that modifying weights using the formula (9) implements a gradient descent on the error surface obtained from (8). Various improvements of the basic learning process are possible, such as including a `momentum term' [32], which prevents the weight oscillations. The idea is to keep some inertia (proportional to the small real-valued parameter ff) from the previous change of the weight: w ij = ffi j output pi + ffw (n) Perhaps the most annoying problem of backpropagation is `overfitting' the training data.
Reference: [33] <author> T. Silvester. </author> <title> The rate of femoral head necrosis depending on the time interval from the injury to the internal fixation. </title> <journal> Medicinski razgledi 1992, </journal> <volume> Vol. 31 </volume> <pages> 293-313, </pages> <year> 1992. </year> <note> In Slovene. </note>
Reference-contexts: To make results comparable to Assistant-R we equipped LFC with pruning and the probability estimation facilities as described in Section 2.2. 3 Description of the data The data, originating from the University Trauma Clinic in Ljubljana, Slovenia, was provided by T. Silvester <ref> [33] </ref>. He examined medical records, follow-up results, and radiographs of 197 7 patients (134 women and 63 men) with fractures of the femoral neck (see Figure 1), treated at the University Trauma Clinic in Ljubljana in 1987. <p> Unfortunately, it is highly unlikely that a physician, when dealing with otherwise healthy, younger patients with good regenerative abilities, would use an artificial implant (artroplastic), although this method proved to be almost 100% successful when applied to older patients. This result is also consistent with Silvester's <ref> [33] </ref> suggestion that the artroplastic therapy should be applied more frequently to older patients and patients with biomechanically unsuitable fractures (such as Pauwels III or Garden IV). 5 Explanation abilities of different classifiers 5.1 General explanation abilities of different classifiers In many fields of interest, and especially in medicine, it is
Reference: [34] <author> P. Smyth, R. M. Goodman, and C. Higgins. </author> <title> A hybrid rule-based bayesian classifier. </title> <booktitle> In Proc.European Conf. on Artificial Intelligence, </booktitle> <pages> pages 610-615, </pages> <address> Stockholm, Sweden, </address> <year> 1990. </year>
Reference-contexts: estimating probabilities more reliably and extended it to deal with incomplete and multi-class data sets. 4 Another difference between Assistant and Assistant-R is that, wherever appropriate, instead of the relative frequency, Assistant-R uses the m-estimate of probabilities, which was shown to often significantly increase the performance of machine learning algorithms <ref> [1, 2, 5, 34] </ref>. <p> The classification of each algorithm is in the form of conditional probabilities P (C j jAlg i ) for every class C j . When the naive Bayesian combination <ref> [14, 34] </ref> is used, the probability of an instance belonging to the class C j is calculated with: P (C j jAlg 1 ; : : : ; Alg k ) = P (C j ) k Y P (C j jAlg i ) P (C j ) 22 6.2 Experimental
Reference: [35] <author> S. Weigand, A. Huberman, and D. E. Rumelhart. </author> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> Vol. 1(3), </volume> <year> 1990. </year>
Reference-contexts: The trained network becomes too specialised for describing training instances and is unable to successfully classify unseen instances. This phenomenon is usually a consequence of using an oversized network with too many hidden units. `Weight elimination' <ref> [35] </ref> at least partially overcomes this problem. With a slight change in the error function (8), the network is forced to keep the weights as small as possible and possibly eliminate some of them.
Reference: [36] <author> M. Zwitter, I. Bratko, and I. Kononenko. </author> <title> Rational and irrational reservations against the use of computer in medical diagnosis and prognosis. </title> <booktitle> In Proc. 3 rd Mediterranean conf. on medical and biological engineering, Portoroz, Slovenia, </booktitle> <year> 1983. </year> <month> 29 </month>
Reference-contexts: There have been many applications of machine learning to medical diagnostic problems [7, 8, 15, 17, 24, 27, 31]. As medical prognosis is a difficult task for physicians due to large time delays (several years) in recognizing the correct prognosis <ref> [36] </ref>, machine learning may be more useful for solving prognostic rather than diagnostic problems. Besides prediction accuracy, the explanation ability of the classifier is also very important. To support the prognostic process in everyday practice, physicians need a classifier that is able to explain its decisions.
References-found: 36

