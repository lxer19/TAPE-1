URL: ftp://ftp.cis.upenn.edu/pub/msingh/lect_stat95.ps.Z
Refering-URL: http://www.cis.upenn.edu/~msingh/frames/papers_list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: provan@camis.stanford.edu  msingh@gradient.cis.upenn.edu  
Title: 28 Learning Bayesian Networks Using Feature Selection  
Author: Gregory M. Provan and Moninder Singh yy 
Note: 28.1 Introduction  
Address: 4984 El Camino Real, Los Altos, CA, 94022  Philadelphia, PA 19104-6389  
Affiliation: Institute for Decision Systems Research  Dept. of Computer and Information Science yy University of Pennsylvania  
Abstract: This paper introduces a novel enhancement for learning Bayesian networks with a bias for small, high-predictive-accuracy networks. The new approach selects a subset of features that maximizes predictive accuracy prior to the network learning phase. We examine explicitly the effects of two aspects of the algorithm, feature selection and node ordering. Our approach generates networks that are computationally simpler to evaluate and display predictive accuracy comparable Bayesian networks are being increasingly recognized as an important representation for probabilistic reasoning. For many domains, the need to specify the probability distributions for a Bayesian network is considerable, and learning these probabilities from data using an algorithm like K2 [Cooper92] could alleviate such specification difficulties. We describe an extension to the Bayesian network learning approaches introduced in K2. Our goal is to construct networks that are simpler to evaluate but still have high predictive accuracy relative to networks that model all features. Rather than use all database features (or attributes) for constructing the network, we select a subset of features that maximize the predictive accuracy of the network. Then the learning process uses only the selected features as nodes in learning the Bayesian network. We examine explicitly the effects of two aspects of the algorithm: (a) feature selection, and (b) node ordering. Our experimental results verify that this approach generates networks that are compu-tationally simpler to evaluate and display predictive accuracy comparable to the predictive accuracy of Bayesian networks that model all features. Our results, similar to those observed by other studies of feature selection in learning [Caruana94, John94, Langley94a, Langley94b], demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 18% of the features for the to that of Bayesian networks which model all attributes.
Abstract-found: 1
Intro-found: 1
Reference: [Aha94] <author> Aha, D.W. and R.L. </author> <month> Bankert </month> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types. </title> <booktitle> In AAAI Workshop on Case-based Reasoning, </booktitle> <pages> 106-112, </pages> <address> Seattle, WA. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: It is expected that in such domains feature selection may not make a significant impact. One exception is the study of cloud classification by Aha and Bankert <ref> [Aha94] </ref>, in which a set of 204 attributes were significantly pruned, leading the greatly improved performance. 7 Better understanding of data sets and of domains may lead to a deeper understanding of the role of feature selection, and improved performance from feature selection algorithms. 6 Langley [Langley94a] presents a thorough review
Reference: [Amuallim91] <author> Amuallim, H. and T.G. </author> <title> Dietterich (1991). Learning with Many Irrelevant Features. </title> <booktitle> In Proc. Conf. of the AAAI, </booktitle> <pages> 547-552. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: Three filter-model approaches that have been taken are: the Relief algorithm [Kira92a, Kira92b] (this has been extended in [Kononenko94]), the FOCUS algorithm <ref> [Amuallim91] </ref>, and an extended nearest-neighbor algorithm [Cardie93]. Wrapper-based approaches have been studied in [John94, Caruana94, Langley94b], among others. 6 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [Andersen89] <author> Andersen, S.K., K.G. Olesen, F.V. Jensen and F. </author> <title> Jensen (1989). "HUGIN| a Shell for Building Bayesian Belief Universes for Expert Systems". Procs. </title> <booktitle> IJCAI, </booktitle> <pages> 1080-1085. </pages>
Reference-contexts: The third part of the database, the test data, is then used for determining the predictive accuracy of the network derived from the network construction phase. We performed inference on the networks using the Lauritzen-Spiegelhalter inference algorithm as implemented in the HUGIN <ref> [Andersen89] </ref> system. The K2-AS approach trades off the time required to construct a network from the full feature set (as done in K2) with precomputing a feature subset and subsequently constructing a network with this feature subset.
Reference: [Buntine92] <author> Buntine, W.L. and T. </author> <title> Niblett (1992). A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 7. </volume>
Reference: [Cardie93] <author> Cardie, C. </author> <year> (1993). </year> <title> Using Decision Trees to Improve Case-based Learning. </title> <booktitle> In Proc. Machine Learning, </booktitle> <pages> 25-32. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Three filter-model approaches that have been taken are: the Relief algorithm [Kira92a, Kira92b] (this has been extended in [Kononenko94]), the FOCUS algorithm [Amuallim91], and an extended nearest-neighbor algorithm <ref> [Cardie93] </ref>. Wrapper-based approaches have been studied in [John94, Caruana94, Langley94b], among others. 6 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [Caruana94] <author> Caruana, R. and D. </author> <title> Freitag (1994). Greedy attribute selection. </title> <editor> In W. Cohen and H. Hirsch, editors, </editor> <booktitle> Proc. Machine Learning, </booktitle> <pages> 28-36. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our experimental results verify that this approach generates networks that are compu-tationally simpler to evaluate and display predictive accuracy comparable to the predictive accuracy of Bayesian networks that model all features. Our results, similar to those observed by other studies of feature selection in learning <ref> [Caruana94, John94, Langley94a, Langley94b] </ref>, demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 18% of the features for the 1 Learning from Data: AI and Statistics V. Edited by D. Fisher and H.-J. <p> Three filter-model approaches that have been taken are: the Relief algorithm [Kira92a, Kira92b] (this has been extended in [Kononenko94]), the FOCUS algorithm [Amuallim91], and an extended nearest-neighbor algorithm [Cardie93]. Wrapper-based approaches have been studied in <ref> [John94, Caruana94, Langley94b] </ref>, among others. 6 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [Cheeseman94] <author> Cheeseman, P. and W. Oldford, editors. </author> <year> (1994). </year> <title> Selecting Models from Data: AI and Statistics IV. </title> <publisher> Springer-Verlag. </publisher>
Reference: [Cooper92] <author> Cooper, G.F. and E. </author> <title> Herskovits (1992). A Bayesian Method for the Induction of of Probabilistic Networks from Data. </title> <booktitle> In Machine Learning 9, </booktitle> <pages> 54-62. </pages> <note> 300 Gregory M. Provan and Moninder Singh </note>
Reference-contexts: 28.1 Introduction Bayesian networks are being increasingly recognized as an important representation for probabilistic reasoning. For many domains, the need to specify the probability distributions for a Bayesian network is considerable, and learning these probabilities from data using an algorithm like K2 <ref> [Cooper92] </ref> could alleviate such specification difficulties. We describe an extension to the Bayesian network learning approaches introduced in K2. Our goal is to construct networks that are simpler to evaluate but still have high predictive accuracy relative to networks that model all features. <p> Since CB uses the K2 algorithm to generate the Bayesian network from a particular ordering, CB is correct in the same sense that K2 is [Singh95]. Singh and Valtorta show the importance 3 We use the nomenclature used in the papers on K2 by Herskovits and Cooper <ref> [Herskovits90, Cooper92] </ref>.
Reference: [Dawid92] <author> Dawid, A.P. </author> <year> (1992). </year> <title> Prequential Analysis, Stochastic Complexity and Bayesian Inference. </title> <editor> In J.M. Bernardo, J. Berger, A. Dawid, and A. Smith, editors, </editor> <booktitle> Bayesian Statistics 4, </booktitle> <pages> 109-125. </pages> <publisher> Oxford Science Publications. </publisher>
Reference-contexts: This statistical approach to subset selection shares many principles with other statistical notions of information minimality, like MDL. For example, Dawid discusses the close relation between subset selection and the MDL principle in <ref> [Dawid92] </ref>. The computer vision community has extensively studied feature selection [Dejviver82], and has formed a sub-community within computational vision called pattern recognition.
Reference: [Dejviver82] <author> Dejviver, P. and J. </author> <title> Kittler (1982). Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice-Hall. </publisher>
Reference-contexts: This statistical approach to subset selection shares many principles with other statistical notions of information minimality, like MDL. For example, Dawid discusses the close relation between subset selection and the MDL principle in [Dawid92]. The computer vision community has extensively studied feature selection <ref> [Dejviver82] </ref>, and has formed a sub-community within computational vision called pattern recognition. The mainstream vision community typically does not use statistical feature selection to identify relevant features, but makes assumptions about what the features should be present to represent, for example, a class of objects.
Reference: [Herskovits90] <author> Herskovits, E. </author> <title> and G.F. Cooper (1990). KUTATO: An Entropy-Driven System for Construction of Probabilistic Expert Systems from Databases. </title> <booktitle> In Proc. Conf. Uncertainty in Artificial Intelligence, </booktitle> <pages> 54-62. </pages>
Reference-contexts: Since CB uses the K2 algorithm to generate the Bayesian network from a particular ordering, CB is correct in the same sense that K2 is [Singh95]. Singh and Valtorta show the importance 3 We use the nomenclature used in the papers on K2 by Herskovits and Cooper <ref> [Herskovits90, Cooper92] </ref>.
Reference: [John94] <author> John, G., R. Kohavi, and K. </author> <month> Pfleger </month> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> Proc. Machine Learning, </booktitle> <pages> 121-129. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our experimental results verify that this approach generates networks that are compu-tationally simpler to evaluate and display predictive accuracy comparable to the predictive accuracy of Bayesian networks that model all features. Our results, similar to those observed by other studies of feature selection in learning <ref> [Caruana94, John94, Langley94a, Langley94b] </ref>, demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 18% of the features for the 1 Learning from Data: AI and Statistics V. Edited by D. Fisher and H.-J. <p> We call this approach K2-AS, since it uses the basic K2 algorithm allied with Attribute Selection in the attribute selection phase. The algorithm we use is what has been described as a wrapper model <ref> [John94] </ref>, in that "the feature subset selection algorithms conducts a search for a good subset using the induction algorithm itself as part of the evaluation function" [John94, page 124]. Our learning approach consists of two main steps, attribute selection and network construction. <p> The algorithm we use is what has been described as a wrapper model [John94], in that "the feature subset selection algorithms conducts a search for a good subset using the induction algorithm itself as part of the evaluation function" <ref> [John94, page 124] </ref>. Our learning approach consists of two main steps, attribute selection and network construction. In the attribute selection phase, we choose the set of nodes from which the final network is constructed. <p> Feature selection has received considerable attention in the last few years within the computational learning community, using both filter-based and wrapper-based approaches <ref> [John94] </ref>. A filter model filters out less relevant features using an algorithm different from the induction algorithm used for the learning, and a wrapper model uses induction algorithm itself for feature selection. <p> Three filter-model approaches that have been taken are: the Relief algorithm [Kira92a, Kira92b] (this has been extended in [Kononenko94]), the FOCUS algorithm [Amuallim91], and an extended nearest-neighbor algorithm [Cardie93]. Wrapper-based approaches have been studied in <ref> [John94, Caruana94, Langley94b] </ref>, among others. 6 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [Kira92a] <author> Kira, K. and L. </author> <title> Rendell (1992). A practical approach to feature selection. </title> <booktitle> In Proc. Machine Learning, </booktitle> <pages> 249-256, </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A filter model filters out less relevant features using an algorithm different from the induction algorithm used for the learning, and a wrapper model uses induction algorithm itself for feature selection. Three filter-model approaches that have been taken are: the Relief algorithm <ref> [Kira92a, Kira92b] </ref> (this has been extended in [Kononenko94]), the FOCUS algorithm [Amuallim91], and an extended nearest-neighbor algorithm [Cardie93].
Reference: [Kira92b] <author> Kira, K. and L. </author> <title> Rendell (1992). The Feature Selection Problem: Traditional Methods and a New Algorithm. </title> <booktitle> In Proc. AAAI, </booktitle> <pages> 129-134. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: A filter model filters out less relevant features using an algorithm different from the induction algorithm used for the learning, and a wrapper model uses induction algorithm itself for feature selection. Three filter-model approaches that have been taken are: the Relief algorithm <ref> [Kira92a, Kira92b] </ref> (this has been extended in [Kononenko94]), the FOCUS algorithm [Amuallim91], and an extended nearest-neighbor algorithm [Cardie93].
Reference: [Kononenko94] <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extension of relief. </title> <booktitle> In Proc. European Conf. on Machine Learning, </booktitle> <pages> 171-182. </pages>
Reference-contexts: A filter model filters out less relevant features using an algorithm different from the induction algorithm used for the learning, and a wrapper model uses induction algorithm itself for feature selection. Three filter-model approaches that have been taken are: the Relief algorithm [Kira92a, Kira92b] (this has been extended in <ref> [Kononenko94] </ref>), the FOCUS algorithm [Amuallim91], and an extended nearest-neighbor algorithm [Cardie93]. Wrapper-based approaches have been studied in [John94, Caruana94, Langley94b], among others. 6 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [Langley94a] <author> Langley, P. </author> <year> (1994). </year> <title> Selection of relevant features in machine learning. </title> <editor> In R. Greiner, editor, </editor> <booktitle> Proc. AAAI Fall Symposium on Relevance. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: Our experimental results verify that this approach generates networks that are compu-tationally simpler to evaluate and display predictive accuracy comparable to the predictive accuracy of Bayesian networks that model all features. Our results, similar to those observed by other studies of feature selection in learning <ref> [Caruana94, John94, Langley94a, Langley94b] </ref>, demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 18% of the features for the 1 Learning from Data: AI and Statistics V. Edited by D. Fisher and H.-J. <p> by Aha and Bankert [Aha94], in which a set of 204 attributes were significantly pruned, leading the greatly improved performance. 7 Better understanding of data sets and of domains may lead to a deeper understanding of the role of feature selection, and improved performance from feature selection algorithms. 6 Langley <ref> [Langley94a] </ref> presents a thorough review of feature selection approaches studied within the Machine Learning literature. 7 In this domain it is likely that the features were not pre-selected for relevance, as there was no a priori knowledge of relevance and irrelevance.
Reference: [Langley94b] <author> Langley, P. and S. </author> <title> Sage (1994). Induction of selective bayesian classifiers. </title> <booktitle> In Proc. Conf. on Uncertainty in AI, </booktitle> <pages> 399-406. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our experimental results verify that this approach generates networks that are compu-tationally simpler to evaluate and display predictive accuracy comparable to the predictive accuracy of Bayesian networks that model all features. Our results, similar to those observed by other studies of feature selection in learning <ref> [Caruana94, John94, Langley94a, Langley94b] </ref>, demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 18% of the features for the 1 Learning from Data: AI and Statistics V. Edited by D. Fisher and H.-J. <p> Three filter-model approaches that have been taken are: the Relief algorithm [Kira92a, Kira92b] (this has been extended in [Kononenko94]), the FOCUS algorithm [Amuallim91], and an extended nearest-neighbor algorithm [Cardie93]. Wrapper-based approaches have been studied in <ref> [John94, Caruana94, Langley94b] </ref>, among others. 6 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [Madigan93] <author> Madigan, D., A. Raftery, J. York, J. Bradshaw, and R. </author> <month> Almond </month> <year> (1993). </year> <title> Strategies for Graphical Model Selection. </title> <booktitle> In Proc. International Workshop on AI and Statistics, </booktitle> <pages> 331-336. </pages>
Reference-contexts: Provan and Moninder Singh FIGURE 1. Induction rates for Gene-splice and Chess databases model with good predictive accuracy. It is possible to compute multiple models and average over them <ref> [Madigan93] </ref> to obtain the best predictive accuracy, and we hope to take this approach in future work. In addition, we restrict our attention to Bayesian networks.
Reference: [Marill63] <author> Marill, T. and D. </author> <title> Green (1963). On the effectiveness of receptors in recognition systems. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 9 </volume> <pages> 11-17. </pages>
Reference-contexts: In statistics, research on feature selection has focused primarily on selecting a subset of features within linear regression. Techniques developed include sequential backward selection <ref> [Marill63] </ref>, branch&bound [Narendra77, Xu89], and search algorithms [Siedlecki88]. A 1993 meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"[Cheeseman94], and contains a large number of papers on feature selection.
Reference: [Murphy92] <author> Murphy, P.M. and D.W. </author> <title> Aha (1992). UCI Repository of Machine Learning Databases. </title> <institution> Dept. of Information and Computer Science, Univ. of California, Irvine. </institution>
Reference-contexts: Learning Bayesian Networks Using Feature Selection 295 28.5 Results We have performed a set of experiments to compare the networks generated by our approach with those created by CB. We tested this method on four databases acquired from the University of California, Irvine Repository of Machine Learning databases <ref> [Murphy92] </ref>, namely Michalski's Soybean database, Slate's Letter Recognition database, the Gene-Splicing database due to Towell, Noordewier, and Shavlik, 4 , Schlimmer's voting database and Shapiro's Chess Endgame database.
Reference: [Narendra77] <author> Narendra, M. and K. </author> <title> Fukunaga (1977). A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Trans. on Computers, C-26(9):917-922. </journal>
Reference-contexts: In statistics, research on feature selection has focused primarily on selecting a subset of features within linear regression. Techniques developed include sequential backward selection [Marill63], branch&bound <ref> [Narendra77, Xu89] </ref>, and search algorithms [Siedlecki88]. A 1993 meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"[Cheeseman94], and contains a large number of papers on feature selection.
Reference: [Siedlecki88] <author> Siedlecki, W. and J. </author> <title> Sklansky (1988). On automatic feature selection. </title> <journal> Itnl. J. of Pattern Recognition and Artificial Intelligence, </journal> <volume> 2(2) </volume> <pages> 197-220. </pages>
Reference-contexts: In statistics, research on feature selection has focused primarily on selecting a subset of features within linear regression. Techniques developed include sequential backward selection [Marill63], branch&bound [Narendra77, Xu89], and search algorithms <ref> [Siedlecki88] </ref>. A 1993 meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"[Cheeseman94], and contains a large number of papers on feature selection. This statistical approach to subset selection shares many principles with other statistical notions of information minimality, like MDL.
Reference: [Singh95] <author> Singh, M. and M. </author> <month> Valtorta </month> <year> (1995). </year> <title> Construction of Bayesian Network Structures from Data: a Brief Survey and an Efficient Algorithm. </title> <journal> Int. Journal of Approximate Reasoning, </journal> <volume> 12, </volume> <pages> 111-131. </pages>
Reference-contexts: The second phase computes the network (from the set of features ) which maximizes the predictive accuracy over the test data. The learning algorithm that we use, called CB, is a modified version of K2 <ref> [Singh95] </ref>. Whereas K2 assumes a node ordering, CB uses conditional independence (CI) tests to generate a "good" node ordering, and then uses the K2 algorithm to generate the Bayesian network from the database D using this node ordering. <p> Since CB uses the K2 algorithm to generate the Bayesian network from a particular ordering, CB is correct in the same sense that K2 is <ref> [Singh95] </ref>. Singh and Valtorta show the importance 3 We use the nomenclature used in the papers on K2 by Herskovits and Cooper [Herskovits90, Cooper92]. Learning Bayesian Networks Using Feature Selection 293 of deriving a good node ordering [Singh95], given the n! possible node orderings on n features. 28.3 Feature Selection Algorithm <p> a particular ordering, CB is correct in the same sense that K2 is <ref> [Singh95] </ref>. Singh and Valtorta show the importance 3 We use the nomenclature used in the papers on K2 by Herskovits and Cooper [Herskovits90, Cooper92]. Learning Bayesian Networks Using Feature Selection 293 of deriving a good node ordering [Singh95], given the n! possible node orderings on n features. 28.3 Feature Selection Algorithm We implemented the Feature Selection Algorithm using the CB algorithm in both the attribute selection as well as the network construction phase.
Reference: [Xu89] <author> Xu, L., P. Yan, and T. Chang. </author> <year> (1989). </year> <title> Best-first strategy for feature selection. </title> <booktitle> In Proc. Ninth International Conf. on Pattern Recognition, </booktitle> <pages> 706-708. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In statistics, research on feature selection has focused primarily on selecting a subset of features within linear regression. Techniques developed include sequential backward selection [Marill63], branch&bound <ref> [Narendra77, Xu89] </ref>, and search algorithms [Siedlecki88]. A 1993 meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"[Cheeseman94], and contains a large number of papers on feature selection.
References-found: 24

