URL: ftp://ftp.sics.se/pub/SICS-reports/Reports/SICS-R--95-05--SE.ps.Z
Refering-URL: http://www.sics.se/libindex.html
Root-URL: 
Title: Syntax Analyzer Generator for Agents  
Author: Anders Andersson 
Keyword: Concurrent constraints, Bottom-up parsing, LALR(1), AKL, Agents  
Address: Box 1263, S-164 28 KISTA, Sweden  
Affiliation: Programming Systems Group Swedish Institute of Computer Science  
Note: SAGA  
Email: email: andand@sics.se  
Date: 13 October 1995  
Abstract: ISSN 0283-3836 Report R95:05 ISRN SICS-R|95/05|SE Abstract LALR(1) parser generators in conjunction with imperative programming languages is the standard solution for parsing applications. Contrary to popular belief, this situation is not entirely satisfactory. With imperative tools like Yacc, it is difficult to add actions to a grammar without introducing conflicts. Also, LALR(1) is not powerful enough to meet the needs of languages like C++. We argue that LALR(1) parsing in conjuction with concurrent constraint programming is an interesting option that solves the former problem. We also show how deep guards and don't-know non-determinism can be exploited to solve the latter problem. These ideas have been incorporated in the Saga parser generator described in this report. Saga is an integrated generator for lexical analyzers and parsers. It is based on AKL, a multiparadigm programming language based on a concurrent constraint framework. It is intended both as a research tool to demonstrate the power offered by concurrent constraints, and as a practical tool for the Agents programming system. As a practical tool, Saga features many improvements over tools like Yacc. For example, Saga offers powerful syntax, elegant reporting and resolution of conflicts and powerful error handling in the generated syntax analyzers. 
Abstract-found: 1
Intro-found: 1
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi and Jeffrey D. Ullman. </author> <booktitle> Compilers | Principles, Techniques, and Tools, </booktitle> <publisher> Addison-Wesley 1986. </publisher>
Reference-contexts: Chapter 5: Implementation 95 5 Implementation In this chapter we describe some key points of the implementation of Saga. Much of the implementation is based on well-known techniques described in <ref> [ASU86] </ref>. The reader is expected to be familiar with the material on LALR parsing covered in Chapter 4 of that book. The reader is also expected to know basic automata theory, especially covering regular expressions and deterministic finite automata. This material may be found in Chapter 3 of [ASU86], but any <p> described in <ref> [ASU86] </ref>. The reader is expected to be familiar with the material on LALR parsing covered in Chapter 4 of that book. The reader is also expected to know basic automata theory, especially covering regular expressions and deterministic finite automata. This material may be found in Chapter 3 of [ASU86], but any good text on automata theory might suffice. Many parts of the implementation are very straight-forward. For example, the parsing of the Saga input file is made by a Saga generated parser and is straightforward, albeit rather extensive. The various checking and preprocessing steps are also straight-forward. <p> If the state number indicating the state transferred from matches the current state, then we have found a transition. If it does not match, then we have found an empty cell. We follow the traditional approach <ref> [ASU86] </ref> and use two arrays: the check array for the state we transfer from, and the next array for the state we transfer to. <p> In Saga we do not use the state numbers (row numbers) at all, but rather we use the actual base addresses. This means that we can omit the base array. This way we get either two or four array references. In the scheme described in <ref> [ASU86] </ref>, the default entries are put in a separate array. Our suggested optimization is therefore unusable in that setting. In Saga the scheme without the base array works well, but both the default value and the code for the accepted token must be stored as columns. <p> Some extensions must then be made, to handle the matching of different kinds of tokens and so on. 5.2.1 Regular Expressions to DFA The algorithm that Saga uses to generate a DFA from a regular expression is not based on the algorithm in <ref> [ASU86] </ref>, but rather on a simpler method based on transformation of regular expressions. The idea is outlined in [Hop93]. This algorithm is easy to understand, implement and to prove correct. Also, we feel that it is more fundamental than the standard. <p> Then the algorithm is called recursively on all the disjuncts and turns them into normal form. The last transformation step is to merge the normal form versions of the disjuncts. Finally, the generated DFA is minimized with the standard partitioning algorithm described in <ref> [ASU86] </ref> (Section 3.9, Algorithm 3.6). Chapter 5: Implementation 105 5.2.2 Turning a DFA into a Lexical Analyzer It is not enough to be able to build a DFA. We also need to keep track of the actions associated with different tokens. <p> If neither set is a subset, then a conflict is reported to the user of the generator. 5.3 Building of the Parser The generation of the parse tables follow the algorithms from Chapter 4 of <ref> [ASU86] </ref> quite well, but the computations have been restructured to take advantage of common computations. Due to this restructuring, some of the suggested optimizations could be omitted without performance loss. We also use a propagation algorithm that is different from the one used in [ASU86]. 106 SAGA | Syntax Analyzer Generator <p> the algorithms from Chapter 4 of <ref> [ASU86] </ref> quite well, but the computations have been restructured to take advantage of common computations. Due to this restructuring, some of the suggested optimizations could be omitted without performance loss. We also use a propagation algorithm that is different from the one used in [ASU86]. 106 SAGA | Syntax Analyzer Generator for Agents 5.3.1 Overview In [ASU86] it is suggested that parsing actions and goto transitions should be determined from the kernel items without the use of an explicit closure operation. <p> Due to this restructuring, some of the suggested optimizations could be omitted without performance loss. We also use a propagation algorithm that is different from the one used in <ref> [ASU86] </ref>. 106 SAGA | Syntax Analyzer Generator for Agents 5.3.1 Overview In [ASU86] it is suggested that parsing actions and goto transitions should be determined from the kernel items without the use of an explicit closure operation. This eliminates the need to ever generate any non-kernel items, and is therefore considered an optimization. <p> Each item in the closure corresponds to either a reduction or a kernel item in one of the states in the range of the goto function. This means that this simple approach is provably at least as efficient as the one in <ref> [ASU86] </ref> in the Ordo sense. Saga then uses a non-iterative algorithm to propagate the look-ahead sets, instead of the iterative one in [ASU86]. The first thing that is done in the generation is to compute two tables that represents the functions Nullable and First, both with the non-terminals as domain. <p> This means that this simple approach is provably at least as efficient as the one in <ref> [ASU86] </ref> in the Ordo sense. Saga then uses a non-iterative algorithm to propagate the look-ahead sets, instead of the iterative one in [ASU86]. The first thing that is done in the generation is to compute two tables that represents the functions Nullable and First, both with the non-terminals as domain. Nullable decides if a non-terminal can derive an empty string of terminals. <p> nearly linear time, so this method is guaranteed to be efficient. 5.3.4 Building LALR (1) Sets of Items The sets of items are represented with just the kernel items, and the LR (1) closure is taken on each item set with a dummy look-ahead symbol as in Algorithm 4.12 of <ref> [ASU86] </ref>. Then the entire image of the goto function from the state is computed at once.
Reference: [Dai85] <author> Julia A. Dain. </author> <title> Error Recovery for Yacc Parsers, </title> <booktitle> Proceedings European Unix Systems User Group Autumn 1985 Conference, </booktitle> <address> Copenhagen, </address> <month> Sept </month> <year> 1985. </year>
Reference-contexts: Thus we should settle for small repairs. This view seems to be common, see [GHJ79], <ref> [Dai85] </ref>. <p> If a few, say five, input tokens could be successfully shifted, then we may judge the repair to be a good one <ref> [Dai85] </ref>. 4.5.2 Pros and Cons of Automatic Error Handling Error handling is usually a rather ungrateful task that is usually dealt with late in the development cycle. It would be a great advantage if the parser generator allowed good error handling to be implemented with a minimal amount of work. <p> Even in generators that do not have the special problems caused by constrained attributes, fully automatic error handling is hardly considered. Even authors that claim that their schemes are automatic usually manage to sneak in some user supplied information somewhere. For instance, Julia Dains well-written paper <ref> [Dai85] </ref> discusses automatic recovery, but her scheme does require information on how to synthesize semantic attributes for different rules. 4.5.2.1 Automatization of Error Reporting Even error reporting becomes hard to do well without any assistance from the grammar writer.
Reference: [ElS90] <author> Margaret E. Ellis and Bjarne Strostrup. </author> <title> The Annotated C++ Reference Manual, </title> <publisher> Addison-Wesley 1990. </publisher>
Reference-contexts: Natural language parsing is the most obvious example of a difficult parsing problem. While natural languages are slightly outside the range of languages we require Saga to handle, there are modern programming languages that are very complex to parse. The most prominent example is C++ <ref> [ElS90] </ref>. The parsing of C++ requires unbounded look-ahead, and actually not even that is quite sufficient. For example, there is no bound on how far ahead we have to look to distinguish an expression from a declaration. Thus, unbounded look-ahead is required.
Reference: [FiL88] <author> Charles N. Fischer and Richard J. LeBlanc. </author> <title> Crafting a Compiler, </title> <month> Benjamin-Cummings </month> <year> 1988. </year>
Reference: [GHJ79] <author> S.L. Graham, C.B. Haley and W.N. Joy. </author> <title> Practical LR Error Recovery, </title> <journal> ACM SIGPLAN Notices 14:8 1979. </journal>
Reference-contexts: Thus we should settle for small repairs. This view seems to be common, see <ref> [GHJ79] </ref>, [Dai85]. <p> This type of problem gets much worse if we perform some syntactic checks in the semantic phase. Such checks are not uncommon, either to simplify the grammar or to improve error reporting, or both <ref> [GHJ79] </ref>. Generating spurious semantic errors may not seem all that bad, but it may be argued that they are bad as a result of local repairs. Local repairs can be reported in an exact, informative manner and the user should usually be able to trust them to be reliable. <p> Interestingly enough, the problems with ambiguities seem to be linked to repairs rather than to recovery, so combining error productions with a local repair strategy can be very successful <ref> [GHJ79] </ref>.
Reference: [Hop93] <author> Mark Hopkins. </author> <title> Regular Expression ! Minimal DFA Algorithm II, </title> <note> Post 101 to comp.compilers newsgroup on Usenet, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The idea is outlined in <ref> [Hop93] </ref>. This algorithm is easy to understand, implement and to prove correct. Also, we feel that it is more fundamental than the standard. Essentially it transforms a regular expression to a regular grammar, that is then made into a DFA.
Reference: [Jan94] <author> Sverker Janson. </author> <title> AKL | A Multiparadigm Programming Language, </title> <type> Ph.D. </type> <institution> diss., Computing Science Department, Uppsala University, and Swedish Institute of Computer Science 1994. </institution>
Reference-contexts: If most of the grammar is deterministic, the simple technique of resolving all conflicts with non-determinism will make it hard to detect mistakes in the design of the grammar. 4.4.2 Inspiration from AKL Since Saga's host language AKL <ref> [Jan94] </ref> has powerful constructs to handle non-determinism, including the ability to encapsulate it, it was interesting to try to provide similar constructs in Saga. <p> A user defined corrector can make different changes to the input, and may also delete items from the parse stack. Different changes may be tested with forward moves. In order to interface the agent with the parser state, an AKL port object <ref> [Jan94] </ref> is used. This roughly corresponds to the object concept in object-oriented languages.
Reference: [Joh75] <author> S.C. Johnson. </author> <title> Yacc | Yet Another Compiler Compiler, </title> <type> Computing Science Technical Report 32, </type> <institution> AT&T Bell Labratories, </institution> <address> Murray Hill, N.J. </address> <year> 1975. </year>
Reference-contexts: One extreme is represented by the standard Unix tools Lex [Les75] and Yacc <ref> [Joh75] </ref>. Lex and Yacc are two completely separate tools that generate lexical analyzers and parsers, respectively. The other extreme is a scannerless parser where the lexical analyzer is totally integrated into the parser (see [SaC89] for an example of this approach).
Reference: [KeR89] <author> Brian W. Kerninghan and Dennis M. Ritchie. </author> <title> The C Programming Language, </title> <publisher> Prentice-Hall 1989. </publisher>
Reference: [Knu65] <author> Donald E. Knuth. </author> <title> On the Translation of Languages from Left to Right, </title> <booktitle> Information and Control 8, </booktitle> <year> 1965. </year>
Reference: [Les75] <author> M.E. Lesk. </author> <title> Lex | a Lexical Analyzer Generator, </title> <type> Computing Science Technical Report 39, </type> <institution> AT&T Bell Labratories, </institution> <address> Murray Hill, N.J. </address> <year> 1975. </year>
Reference-contexts: One extreme is represented by the standard Unix tools Lex <ref> [Les75] </ref> and Yacc [Joh75]. Lex and Yacc are two completely separate tools that generate lexical analyzers and parsers, respectively. The other extreme is a scannerless parser where the lexical analyzer is totally integrated into the parser (see [SaC89] for an example of this approach).
Reference: [MTH83] <author> Yuji Matsumoto, Hozumi Tanaka, Hideki Hirakawa, Hideo Miyoshi and Hideki Yasukawa. </author> <title> BUP: A Bottom-Up Parser Embedded in Prolog, </title> <journal> New Generation Computing 1, </journal> <volume> No. 2, </volume> <year> 1983. </year>
Reference: [Par93] <author> Terence J. Parr. </author> <title> Obtaining Practical Variants of LL(k) and LR(k) for k&gt;1 by Splitting the Atomic k-Tuple, </title> <type> Ph.D. </type> <institution> diss., Purdue University 1993. </institution>
Reference: [PDC92] <author> T.J. Parr, H.G. Dietz and W.E. Cohen. </author> <title> PCCTS Reference Manual Vesion 1.00, </title> <journal> ACM SIGPLAN Notices, </journal> <month> February </month> <year> 1992. </year>
Reference-contexts: We may list the kind of construction we expected to see, and the kind of construct we failed to recognize, but it is hard to know what names to use in these references. The names used in the grammar might be used; PCCTS <ref> [PDC92] </ref> does this kind of error reporting. However, PCCTS is LL (k) based, so this information tends to be less complex than for an LR based parser. The messages can be very confusing, especially if we use names of non-terminals of a complex grammar.
Reference: [PeW80] <author> F. Pereira and D. Warren. </author> <title> Definite Clause Grammar for Language Analysis | A Survey of the Formalism and a Comparison with Augmented Transition Networks, </title> <booktitle> Artificial Intelligence 13, </booktitle> <year> 1980. </year> <title> 116 SAGA | Syntax Analyzer Generator for Agents </title>
Reference: [PPW78] <author> L. Pereira, F. Pereira and D. Warren. </author> <title> User's Guide to DEC System-10 Prolog, </title> <institution> Department of Artificial Intelligence, University of Edinburgh, </institution> <year> 1978. </year>
Reference: [RiD78] <author> G.D. Ripley and F.C. Druseikis. </author> <title> A Statistical Analysis of Syntax Errors, </title> <booktitle> Computer Language 3, </booktitle> <year> 1978. </year>
Reference-contexts: Ripley and Drusekis <ref> [RiD78] </ref> found that on student Pascal programs, 90% of all errors were single token errors and that 80% of all erroneous statements only had a single error.
Reference: [SaC89] <author> D.J. Salomon and G.V. Cormack. </author> <booktitle> Scannerless NSLR(1) Parsing of Programming Languages, ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <month> June 21-23, </month> <year> 1989. </year>
Reference-contexts: One extreme is represented by the standard Unix tools Lex [Les75] and Yacc [Joh75]. Lex and Yacc are two completely separate tools that generate lexical analyzers and parsers, respectively. The other extreme is a scannerless parser where the lexical analyzer is totally integrated into the parser (see <ref> [SaC89] </ref> for an example of this approach). Most programming languages are defined in a way that favours a rather clear-cut separation of the lexical analyzer from the parser. There are a number of reasons that support this tradition.
Reference: [Sar89] <author> Vijay A. Saraswat. </author> <title> Concurrent Constraint Programming Languages, </title> <type> Ph.D. </type> <institution> diss., Carnegie-Mellon University 1989. </institution>
Reference: [Sed88] <author> Robert Sedgewick. </author> <title> Algorithms, Second Edition, </title> <publisher> Addison-Wesley 1988. </publisher>
Reference-contexts: The propagation can be done in a non-iterative fashion, with the aid of some graph algorithms from one of the standard Agents libraries. First the graph is reduced to its strongly-connected components (SCCs) with Tarjans algorithm (see <ref> [Sed88] </ref>). Each SCC takes on the value of the union of the values of the nodes in it. <p> This is because each member of a set in one of the original nodes can be propagated to every other node in the SCC 108 SAGA | Syntax Analyzer Generator for Agents (by definition of SCC). Then the reduced graph is sorted topologically (see <ref> [Sed88] </ref>). Since a reduced graph always is a DAG, the topological sort always succeeds. The values can then be propagated through the DAG in a single pass.
Reference: [Sta94] <author> Allan M. Stavely. </author> <title> Inherited Attributes in yacc, </title> <type> CSR 197, </type> <institution> New Mexico Tech, </institution> <address> Socorro, N.M. </address> <year> 1994. </year>

References-found: 21

