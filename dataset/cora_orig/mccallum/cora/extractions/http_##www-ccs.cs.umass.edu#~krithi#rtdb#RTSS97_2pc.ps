URL: http://www-ccs.cs.umass.edu/~krithi/rtdb/RTSS97_2pc.ps
Refering-URL: http://www-ccs.cs.umass.edu/rtdb/publications.html
Root-URL: 
Title: More Optimism about Real-Time Distributed Commit Processing  
Author: Ramesh Gupta Jayant Haritsa Krithi Ramamritham 
Abstract: In [6], we proposed a new commit protocol, OPT, specially designed for use in distributed firm-deadline real-time database systems. OPT allows transactions to optimistically borrow uncommitted prepared data in a controlled manner. This controlled borrowing reduces the data inaccessibility and the priority inversion that is inherent in real-time commit processing. Experimental evaluations showed the new OPT protocol to be highly successful, as compared to the classical distributed commit protocols, in minimizing the number of missed transaction deadlines. In this paper, we extend and improve upon this prior work in the following ways: First, we consider parallel distributed transactions whereas the previous study was restricted to sequential transactions. Second, we evaluate the extent to which OPT's real-time performance is adversely affected by those cases where its optimism turns out to be misplaced. This is achieved by comparing OPT's performance with that of Shadow-OPT, a protocol that augments OPT with the shadow transaction approach of [3] and ensures that the right decision about access to uncommitted data is always eventually made. In all of our experiments, which considered a wide range of workloads and system configurations, the difference between OPT and Shadow-OPT never exceeded ten percent. Moreover, the difference was reduced to less than two percent when OPT was enhanced with a simple healthy lenders heuristic. Finally, we compare the performance of OPT to that of an alternative priority inheritance-based approach to addressing priority inversion during commit processing. Our results show that the benefits that priority inheritance provides are much smaller than those obtained with the OPT approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Abbott and H. Garcia-Molina, </author> <title> Scheduling Real-Time Transactions: A Performance Evaluation, </title> <booktitle> Proc. of 14th VLDB Conf., </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: For concurrency control, the 2PL High Priority scheme <ref> [1] </ref> is employed. 5.1.
Reference: [2] <author> P. Bernstein, V. Hadzilacos and N. Goodman, </author> <title> Concurrency Control and Recovery in Database Systems, </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: Finally, as explained in detail in [6], although OPT permits use of uncommitted data, because only transactions in the prepared state are allowed to lend, the borrowing does not result in the well-known problem of cascading aborts <ref> [2] </ref>. 3.2. Shadow-OPT As mentioned in the Introduction, we wished to evaluate the efficiency of OPT with respect to the extent to which its real-time performance was adversely affected by those cases where its optimism turned out to be misplaced.
Reference: [3] <author> A. Bestavros and S. Braoudakis, </author> <title> Timeliness via Speculation for Real-time Databases, </title> <booktitle> Proc. of 15th Real-Time Systems Symp., </booktitle> <month> De-cember </month> <year> 1994. </year>
Reference-contexts: We quantitatively evaluate the efficiency of OPT here by comparing its performance to that of Shadow-OPT, a protocol that combines OPT with the shadow transaction approach suggested in <ref> [3] </ref>. As explained later, if we ignore the overheads of the shadow mechanism (which may be significant in practice), Shadow-OPT represents the best on-line performance that could be achieved using the optimistic approach. <p> This was achieved by comparing its performance with that of the Shadow-OPT protocol, described below. The Shadow-OPT protocol combines the OPT protocol with the shadow transaction approach suggested in <ref> [3] </ref>. In this combined technique, a cohort forks off a replica of the transaction, called a shadow, whenever it borrows a data page. The original incarnation of the transaction continues the execution while the shadow transaction is blocked at the point of borrowing.
Reference: [4] <author> J. Gray, </author> <booktitle> Notes on Database Operating Systems, Operating Systems: An Advanced Course, Lecture Notes in Computer Science, </booktitle> <volume> 60, </volume> <year> 1978. </year>
Reference-contexts: A variety of transaction commit protocols have been devised for this model, most of which are based on the classical two phase commit (2PC) protocol <ref> [4] </ref>. In this section, we briefly describe the 2PC protocol and a few popular variations of this protocol - complete descriptions are available in [9, 10, 12].
Reference: [5] <author> R. Gupta, J. Haritsa and K. Ramamritham, </author> <title> More Optimism about Real-Time Distributed Commit Processing, </title> <institution> TR-97-04, DSL/SERC, Indian Institute of Science. </institution>
Reference-contexts: The simulator implements a more realistic version of the model used in our previous study [6]. Due to space limitations, we only highlight the main features here the complete details are in <ref> [5] </ref>. A summary of the parameters used in the model are given in Table 1. The database is a collection of DBSize pages that are uniformly distributed across all the N umSites sites. <p> If the transaction's deadline expires either before this point, or before the master has written the global decision log record, the transaction is killed (the precise semantics of firm deadlines in a distributed environment are defined in <ref> [5] </ref>). As mentioned earlier, transactions in an RTDBS are typically assigned priorities so as to minimize the number of missed deadlines. In our model, all cohorts inherit their parent transaction's priority. <p> Due to space limitations, we discuss only a representative set of results here the complete details are available in <ref> [5] </ref>. <p> The above experiment was repeated for a pure data contention (pure DC) environment, wherein there is no queuing for the physical resources, in order to isolate the influence of data contention on the real-time performance. The performance (not shown here but available in <ref> [5] </ref>) of OPT in this experiment was, unlike the RC+DC environment, comparable to that for the corresponding sequential transaction experiment. The reason for this is that in the pure DC scenario the OPT approach, due to its increased concurrency, reduces the data contention significantly.
Reference: [6] <author> R. Gupta, J. Haritsa, K. Ramamritham and S. Seshadri, </author> <title> Commit Processing in Distributed Real-Time Database Systems, </title> <booktitle> Proc. of 17th Real-Time Systems Symp., </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: Due to these costs, commit processing can result in a significant increase in transaction execution times [10, 14]. Consequently, the choice of commit protocol is an especially important design decision for distributed real-time database systems (RTDBS). In a recent paper <ref> [6] </ref>, using a detailed simulation model of a distributed RTDBS, we profiled the performance of the above-mentioned commit protocols for real-time applications with firm deadlines [7], wherein transactions that miss their deadlines are considered to be worthless and are immediately killed, that is, aborted and discarded from the system without being <p> The ability to borrow helps to reduce the blocking arising out of prepared data and also to reduce the impact of the priority inversion to which the commit phase in a distributed RTDBS is inherently susceptible <ref> [6] </ref>. OPT also incorporates novel features such as Active Abort and Silent Kill that are specifically designed to improve its performance in a real-time environment. A special feature of OPT is that it does not, unlike previous efforts in the area [13, 15], require transaction atomicity requirements to be weakened. <p> A special feature of OPT is that it does not, unlike previous efforts in the area [13, 15], require transaction atomicity requirements to be weakened. Our experimental results in <ref> [6] </ref> showed that, with respect to the metric of the steady-state percentage of missed dead-lines, OPT provided by far the best performance, primarily due to its optimistic borrowing and active abort policies. <p> A fundamental problem with all of the above protocols is that cohorts may become blocked waiting for a decision in the event of a failure at the master site and remain blocked until the failed site recovers <ref> [6] </ref>. To address the blocking problem, a three phase commit (3PC) protocol was proposed in [12]. This protocol achieves a non-blocking capability by inserting an extra phase, called the precom-mit phase, in between the two phases of the 2PC protocol. <p> That is, in all the commit protocols, including 3PC, transactions can be affected by prepared data blocking. Moreover, such data blocking occurs during normal processing whereas decision blocking occurs only during failure situations. 3.1. The OPT Protocol The OPT protocol <ref> [6] </ref> was designed to address the abovementioned issue of prepared data blocking. The main feature of OPT (the complete description is available in [6]) is that transactions requesting data items held by other transactions in the prepared state are allowed to access this data. <p> Moreover, such data blocking occurs during normal processing whereas decision blocking occurs only during failure situations. 3.1. The OPT Protocol The OPT protocol <ref> [6] </ref> was designed to address the abovementioned issue of prepared data blocking. The main feature of OPT (the complete description is available in [6]) is that transactions requesting data items held by other transactions in the prepared state are allowed to access this data. That is, prepared cohorts lend uncommitted data to concurrently executing transactions in the optimistic belief that this data will eventually be committed. <p> Therefore, cohorts in OPT inform the master as soon as they decide to abort locally. Finally, as explained in detail in <ref> [6] </ref>, although OPT permits use of uncommitted data, because only transactions in the prepared state are allowed to lend, the borrowing does not result in the well-known problem of cascading aborts [2]. 3.2. <p> Simulation Model To evaluate the performance of the various commit protocols described in the previous sections, we used a detailed simulator of a distributed RTDBS. The simulator implements a more realistic version of the model used in our previous study <ref> [6] </ref>. Due to space limitations, we only highlight the main features here the complete details are in [5]. A summary of the parameters used in the model are given in Table 1. The database is a collection of DBSize pages that are uniformly distributed across all the N umSites sites. <p> For concurrency control, the 2PL High Priority scheme [1] is employed. 5.1. Comparative Protocols To help isolate and understand the performance effects of distribution and atomicity, we have also simulated, just as in <ref> [6] </ref>, the performance behavior for two additional scenarios, CENT and DPCC, described below: In CENT (Centralized), a centralized database system that is equivalent (in terms of database size and physical resources) to the distributed database system is modeled. <p> The reason for 4 The sequential results shown here were obtained using the refined system model described in Section 4 and are therefore quantitatively different from those shown in <ref> [6] </ref>. this is the following: The data contention level is smaller with parallel execution than with sequential execution since locks are held for shorter times on average. <p> Conclusions In this paper, we have significantly extended and improved on our earlier simulation-based study <ref> [6] </ref> of the performance implications of supporting transaction atom-icity in a distributed firm-deadline RTDBS environment. These improvements included enhancements in the model, the workloads, the protocols and the paradigms considered for real-time commit processing. <p> These improvements included enhancements in the model, the workloads, the protocols and the paradigms considered for real-time commit processing. The main conclusions of our current study are the following: First, the results obtained for parallel distributed transaction workloads were generally similar to those observed for the sequential-transaction environment in <ref> [6] </ref>.
Reference: [7] <author> J. Haritsa, M. Carey and M. Livny, </author> <title> Data Access Scheduling in Firm Real-Time Database Systems, </title> <journal> Real-Time Systems Journal, </journal> <volume> 4 (3), </volume> <year> 1992. </year>
Reference-contexts: Consequently, the choice of commit protocol is an especially important design decision for distributed real-time database systems (RTDBS). In a recent paper [6], using a detailed simulation model of a distributed RTDBS, we profiled the performance of the above-mentioned commit protocols for real-time applications with firm deadlines <ref> [7] </ref>, wherein transactions that miss their deadlines are considered to be worthless and are immediately killed, that is, aborted and discarded from the system without being executed to completion. We also developed and evaluated a new commit protocol called OPT that incorporates modifications to the 2PC protocol.
Reference: [8] <author> J. Huang, J.A. Stankovic, K. Ramamritham, D. Towsley and B. Purimetla, </author> <title> Priority Inheritance In Soft Real-Time Databases, </title> <journal> Real-Time Systems Journal, </journal> <volume> 4 (3), </volume> <year> 1992. </year>
Reference-contexts: A positive feature of the PI approach is that it does not run the risk of transaction aborts, unlike the optimistic approach. Further, a study of PI in the context of (centralized) transaction concurrency control was made in <ref> [8] </ref> and the results suggest that priority inheritance is useful only if it occurs towards the end of the low priority transaction's lifetime. This seems to fit well with handling priority inversion during commit processing since this stage occurs at the end of transaction execution.
Reference: [9] <author> C. Mohan, B. Lindsay and R. Obermarck, </author> <title> Transaction Management in the R fl Distributed Database Management System, </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 11(4), </volume> <year> 1986. </year>
Reference-contexts: A variety of transaction commit protocols have been devised for this model, most of which are based on the classical two phase commit (2PC) protocol [4]. In this section, we briefly describe the 2PC protocol and a few popular variations of this protocol - complete descriptions are available in <ref> [9, 10, 12] </ref>. In the two phase commit protocol, the master initiates the first phase of the commit protocol by sending PREPARE (to commit) messages in parallel to all the cohorts. <p> Finally, the master, after receiving acknowledgments from all the prepared cohorts, writes an end log record and then forgets the transaction. Two variants of the 2PC protocol called presumed abort (PA) and presumed commit (PC) were presented in <ref> [9] </ref>. These protocols try to reduce the message and logging over-heads by requiring all participating cohorts to follow certain rules at failure recovery time.
Reference: [10] <author> G. Samaras, K. Britton, A. Citron and C. Mohan, </author> <title> Two-Phase Commit Optimizations in a Commercial Distributed Environment, </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 3(4), </volume> <booktitle> 1995 (also in Proc. of 9th IEEE Intl. Conf. on Data Engineering, </booktitle> <month> April </month> <year> 1993). </year>
Reference-contexts: In addition, several log records are generated, some of which have to be forced, that is, flushed to disk immediately. Due to these costs, commit processing can result in a significant increase in transaction execution times <ref> [10, 14] </ref>. Consequently, the choice of commit protocol is an especially important design decision for distributed real-time database systems (RTDBS). <p> A variety of transaction commit protocols have been devised for this model, most of which are based on the classical two phase commit (2PC) protocol [4]. In this section, we briefly describe the 2PC protocol and a few popular variations of this protocol - complete descriptions are available in <ref> [9, 10, 12] </ref>. In the two phase commit protocol, the master initiates the first phase of the commit protocol by sending PREPARE (to commit) messages in parallel to all the cohorts. <p> The protocols have been implemented in a number of database products and PA is, in fact, now part of the ISO-OSI and X/OPEN distributed transaction processing standards <ref> [10] </ref>. A fundamental problem with all of the above protocols is that cohorts may become blocked waiting for a decision in the event of a failure at the master site and remain blocked until the failed site recovers [6].
Reference: [11] <author> L. Sha, R. Rajkumar and J. Lehoczky, </author> <title> Priority inheritance protocols: an approach to real-time synchronization, </title> <type> Tech. Report CMU-CS-87-181, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Finally, OPT addresses the priority inversion problem in the commit phase by allowing high priority transactions to access the uncommitted prepared data of low priority transactions. A plausible alternative approach is the well-known priority inheritance (PI) mechanism <ref> [11] </ref>. In this scheme, a low priority transaction that blocks a high priority transaction inherits the priority of the high priority transaction. <p> Real-Time Commit Processing The commit protocols described above were designed for conventional database systems and do not take transaction priorities into account. In a real-time environment, this is clearly undesirable since it may result in priority inversion <ref> [11] </ref>, wherein high priority transactions are made to wait by low priority transactions. Priority inversion is usually prevented by resolving all conflicts in favor of transactions with higher priority. Removing priority inversion in the commit protocol, however, is not fully feasible. <p> The PIC Protocol As discussed above, OPT addresses the priority inversion problem in the commit phase by allowing transactions to access uncommitted prepared data. A plausible alternative approach is the well-known priority inheritance (PI) mechanism <ref> [11] </ref>. In this scheme, a low priority transaction that blocks a high priority transaction inherits the priority of the high priority transaction.
Reference: [12] <author> D. Skeen, </author> <title> NonblockingCommit Protocols, </title> <booktitle> Proc. of ACM SIGMOD Conf., </booktitle> <month> June </month> <year> 1981. </year>
Reference-contexts: A variety of transaction commit protocols have been devised for this model, most of which are based on the classical two phase commit (2PC) protocol [4]. In this section, we briefly describe the 2PC protocol and a few popular variations of this protocol - complete descriptions are available in <ref> [9, 10, 12] </ref>. In the two phase commit protocol, the master initiates the first phase of the commit protocol by sending PREPARE (to commit) messages in parallel to all the cohorts. <p> To address the blocking problem, a three phase commit (3PC) protocol was proposed in <ref> [12] </ref>. This protocol achieves a non-blocking capability by inserting an extra phase, called the precom-mit phase, in between the two phases of the 2PC protocol. In the precommit phase, a preliminary decision is reached regarding the fate of the transaction.
Reference: [13] <author> N. Soparkar et al, </author> <title> Adaptive Commitment for Real-Time Distributed Transactions, TR-92-15, </title> <type> CS, </type> <institution> Univ. of Texas (Austin), </institution> <year> 1992. </year>
Reference-contexts: OPT also incorporates novel features such as Active Abort and Silent Kill that are specifically designed to improve its performance in a real-time environment. A special feature of OPT is that it does not, unlike previous efforts in the area <ref> [13, 15] </ref>, require transaction atomicity requirements to be weakened. Our experimental results in [6] showed that, with respect to the metric of the steady-state percentage of missed dead-lines, OPT provided by far the best performance, primarily due to its optimistic borrowing and active abort policies.
Reference: [14] <author> P. Spiro, A. Joshi and T. Rengarajan, </author> <title> Designing an Optimized Transaction Commit Protocol, </title> <journal> Digital Technical Journal, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: In addition, several log records are generated, some of which have to be forced, that is, flushed to disk immediately. Due to these costs, commit processing can result in a significant increase in transaction execution times <ref> [10, 14] </ref>. Consequently, the choice of commit protocol is an especially important design decision for distributed real-time database systems (RTDBS).
Reference: [15] <author> Y. Yoon, </author> <title> Transaction Scheduling and Commit Processing for Real-Time Distributed Database Systems, </title> <type> Ph.D. Thesis, </type> <institution> Korea Advanced Institute of Science and Technology, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: OPT also incorporates novel features such as Active Abort and Silent Kill that are specifically designed to improve its performance in a real-time environment. A special feature of OPT is that it does not, unlike previous efforts in the area <ref> [13, 15] </ref>, require transaction atomicity requirements to be weakened. Our experimental results in [6] showed that, with respect to the metric of the steady-state percentage of missed dead-lines, OPT provided by far the best performance, primarily due to its optimistic borrowing and active abort policies.
References-found: 15

