URL: ftp://ftp.csd.abdn.ac.uk/pub/apreece/ECAI96/Abecker.ps
Refering-URL: http://www.csd.abdn.ac.uk/~apreece/ECAI96/workshop.html
Root-URL: 
Title: From Theory Refinement to KB Maintenance: a Position Statement  
Author: Andreas Abecker and Klaus Schmid 
Abstract: Since we consider theory refinement (TR) as a possible key concept for a methodologically clear view of knowledge-base maintenance, we try to give a structured overview about the actual state-of-the-art in TR. This overview is arranged along the description of TR as a search problem. We explain the basic approach, show the variety of existing systems and try to give some hints about the direction future research should go. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Abecker, H. Boley, K. Hinkelmann, H. Wache, and F. Schmalhofer, </author> <title> `An Environment for Exploring and Validating Declarative Knowledge', </title> <note> DFKI Technical Memo TM-95-03, </note> <institution> DFKI GmbH, </institution> <month> (November </month> <year> 1995). </year> <title> Also in: </title> <booktitle> Proc. Workshop on Logic Programming Environments at ILPS'95, </booktitle> <address> Portland, Oregon, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Based upon experiences from previous projects and application studies concerning logic-based knowledge representation [6, 2, 13] and knowledge evolution [15, 11], the aim of the VEGA project (Knowledge Validation and Exploration by Global Analysis) is to contribute to a methodology of maintaining declarative knowledge <ref> [1, 12, 14] </ref>. This goal demands techniques and methods for all maintenance activities (e.g. debugging, adaptation, change) within the life-cycle of knowledge bases (KBs)|relying on elementary building blocks (exploration and validation algorithms) and a methodology defining how to use and combine them for specific maintenance tasks. <p> A very first step concerning the technical provisions for such a redirection of TR towards real-world KBS maintenance are made in our VEGA Knowledge Evolution System KES <ref> [1] </ref>. Another promising approach are systems like WHY [25] or NeoDisciple [31]|referred to by Craw & Sleeman [7] as Multi-Strategy Revision Systems|that apply several learning techniques to several different knowledge sources.
Reference: [2] <author> A. Abecker and H. Wache, </author> <title> `A layer architecture for the integration of rules, inheritance, and constraints', in Integration of Declarative Paradigms, </title> <booktitle> Proceedings of the ICLP-94 Post-Conference Workshop, </booktitle> <editor> eds., H. At-Kaci, M. Hanus, and J.J. Moreno Navarro, </editor> <booktitle> MPI-I-94-224, </booktitle> <pages> pp. 12-23, </pages> <month> (June </month> <year> 1994). </year>
Reference-contexts: 1 Introduction Based upon experiences from previous projects and application studies concerning logic-based knowledge representation <ref> [6, 2, 13] </ref> and knowledge evolution [15, 11], the aim of the VEGA project (Knowledge Validation and Exploration by Global Analysis) is to contribute to a methodology of maintaining declarative knowledge [1, 12, 14].
Reference: [3] <author> H. Ade, L. de Raedt, and M. Bruynooghe, </author> <title> `Theory revision', in ILP '93, </title> <booktitle> Proceedings of the 3rd International Workshop on Inductive Logic Programming, Bled, Slovenia, </booktitle> <month> April 1-3, </month> <year> 1993, </year> <editor> ed., S. </editor> <title> Muggleton, </title> <type> Stefan Institute Technical Report, Ljubljana, Slovenia, </type> <pages> pp. 179-182, </pages> <year> (1993). </year>
Reference-contexts: However, after TR a de-operationalization step is necessary for reconstructing the initial concept hierarchy. As the rules change during theory refinement this is not a trivial task. Ruth has been strongly influenced by concepts from deductive databases <ref> [4, 3] </ref>. This lead to the substitution of the example set by an integrity theory (i.e., a set of range-restricted functor-free clauses) and of the theory by a deductive database.
Reference: [4] <author> H. Ade, B. Malfait, and L. de Raedt, `RUTH: </author> <title> an ILP theory revision system', </title> <booktitle> in ISMIS '94, Methodologies for Intelligent Systems, 8th International Symposium, Charlotte, North-Carolina, USA, </booktitle> <editor> eds., Zbigniew W. Ras and Maria Ze-mankova, </editor> <booktitle> volume 869 of Lecture Notes on Artificial Intelligence, </booktitle> <publisher> LNAI, </publisher> <pages> pp. 336-345. </pages> <publisher> Springer Verlag, </publisher> <month> (October </month> <year> 1994). </year>
Reference-contexts: However, after TR a de-operationalization step is necessary for reconstructing the initial concept hierarchy. As the rules change during theory refinement this is not a trivial task. Ruth has been strongly influenced by concepts from deductive databases <ref> [4, 3] </ref>. This lead to the substitution of the example set by an integrity theory (i.e., a set of range-restricted functor-free clauses) and of the theory by a deductive database. <p> Often, the operators are selected such that they are non-interfering, i.e., a generalization step does not introduce new commission errors and a specialization step does not introduce new omission errors. Further, some systems employ macro-operators, e.g. Ruth <ref> [4] </ref> possesses a specialization operator which deletes a rule and adds the instantiated antecedents to the example set. 9 4.2.2 Control The basic control-loop consists of six steps: * Selection of examples for current loop * Setting up inference structure * Identification of possible revision points * Selection/ranking of revision points
Reference: [5] <author> M. Bain and Stephen H. Muggleton, </author> <title> `Non-monotonic learning', </title> <journal> Machine Intelligence, </journal> <volume> 12, </volume> <pages> 105-119, </pages> <year> (1991). </year>
Reference-contexts: Some authors refine this goal to the additional requirement that the revised theory should be as close as possible to the original one. However, problems arise with this requirement as different notions of minimality are sometimes used (cf. <ref> [5, 34] </ref>). Sometimes, a non-minimal revision may also be desirable, e.g., due to an extremely over-specialized theory [17] or because basic concepts should be corrected [23]. 2 In this view of TR as a search problem, an operator application carries out a modification of the theory. <p> To our knowledge no measure for comparing a revised theory with the initial one is given in literature. However, several minimal specialization operators are given <ref> [5, 34] </ref>. The aim of these operators is to exclude as few derivations as possible during specialization. 6 * The resulting theory should be minimal. This requirement is generally interpreted syntactically (e.g., [24, 23, 18]).
Reference: [6] <author> H. Boley, Ph. Hanschke, K. Hinkelmann, and M. Meyer, `Co-Lab: </author> <title> A hybrid knowledge representation and compilation laboratory', </title> <journal> Annals of Operations Research, </journal> <volume> (55), </volume> <year> (1995). </year>
Reference-contexts: 1 Introduction Based upon experiences from previous projects and application studies concerning logic-based knowledge representation <ref> [6, 2, 13] </ref> and knowledge evolution [15, 11], the aim of the VEGA project (Knowledge Validation and Exploration by Global Analysis) is to contribute to a methodology of maintaining declarative knowledge [1, 12, 14].
Reference: [7] <author> S. Craw and D. Sleeman, </author> <title> `Knowledge-based refinement of knowledge based systems', </title> <type> Technical Report TR9505, </type> <institution> University of Aberdeen, Department of Computing Science, </institution> <year> (1995). </year>
Reference-contexts: described by Meseguer [16]: "Theory refinement considers the improvement of an approximate domain theory from a set of cases with known solution." Adopting this definition of TR it can be seen as the basis for KB debugging or updates as well as for KB evolution in a slowly changing domain <ref> [7] </ref>. Since|according to Craw & Sleeman [8]|such KB maintenance activities can be understood as "refinement in response to validation" and thus performed by collaboration of exploration and validation algorithms, analysis of existing TR systems is a good starting point for our research concerning a KB maintenance methodology. <p> This leads to suggestions for further work in TR opening a wider range of application for TR technology. As Craw and Sleeman <ref> [7] </ref> mentioned, the classical concept of TR stemming from data-intensive concept learning (cf. [19]) is not adequate for the refinement of knowledge-based systems. <p> A very first step concerning the technical provisions for such a redirection of TR towards real-world KBS maintenance are made in our VEGA Knowledge Evolution System KES [1]. Another promising approach are systems like WHY [25] or NeoDisciple [31]|referred to by Craw & Sleeman <ref> [7] </ref> as Multi-Strategy Revision Systems|that apply several learning techniques to several different knowledge sources.
Reference: [8] <author> S. Craw and D. Sleeman, </author> <title> `Refinement in response to validation', </title> <journal> Expert Systems with Applications, </journal> <volume> 8(3), </volume> <year> (1995). </year>
Reference: [9] <author> L. de Raedt and M. Bruynooghe, </author> <title> `Belief updating from integrity constraints and queries', </title> <journal> Artificial Intelligence, </journal> <volume> (53), </volume> <year> (1992). </year>
Reference-contexts: However, we consider de Raedt et al.'s lifting of both incremental concept learning and intensional knowledge base updates to a more general notion of Inductive Logic Programming <ref> [22, 9] </ref> as an important step into this desired direction. ILP is a still emerging research area providing a number of elaborated well-founded knowledge-intensive generalization, specialization, and restructuring techniques that must be put into the application context of knowledge refinement.
Reference: [10] <author> L. de Raedt and M. Bruynooghe, </author> <title> `A unifying framework for concept-learning algorithms', </title> <journal> The knowledge engineering review, </journal> <volume> 7, </volume> <pages> 251-269, </pages> <year> (1992). </year>
Reference-contexts: Such exploration as well as the easy combination of different existing ideas could be supported by an experimental workbench implemented as an open toolbox together with a generic control mechanism (cf. the GENCOL / GENCON algorithm for concept learning and ILP, resp., introduced in <ref> [10, 22, 28] </ref>). On the other hand, regarding our initial goal of approaching a methodology for KB maintenance, we can check techniques and methods already existing in TR against requirements coming from KBS development and use.
Reference: [11] <author> K. Hinkelmann and O. Kuhn, </author> <title> `Revising and Updating a Corporate Memory', </title> <booktitle> in EUROVAV-95, Proc. of the European Symposium on Validation and Verification of Knowledge-based Systems, </booktitle> <month> (June </month> <year> 1995). </year>
Reference-contexts: 1 Introduction Based upon experiences from previous projects and application studies concerning logic-based knowledge representation [6, 2, 13] and knowledge evolution <ref> [15, 11] </ref>, the aim of the VEGA project (Knowledge Validation and Exploration by Global Analysis) is to contribute to a methodology of maintaining declarative knowledge [1, 12, 14].
Reference: [12] <author> K. Hinkelmann, M. Meyer, and F. Schmalhofer, </author> <title> `Knowledge-Base Evolution for Product and Production Planning', </title> <journal> AI Communications, </journal> <volume> 7(2), </volume> <pages> 98-113, </pages> <month> (June </month> <year> 1994). </year>
Reference-contexts: 1 Introduction Based upon experiences from previous projects and application studies concerning logic-based knowledge representation [6, 2, 13] and knowledge evolution [15, 11], the aim of the VEGA project (Knowledge Validation and Exploration by Global Analysis) is to contribute to a methodology of maintaining declarative knowledge <ref> [1, 12, 14] </ref>. This goal demands techniques and methods for all maintenance activities (e.g. debugging, adaptation, change) within the life-cycle of knowledge bases (KBs)|relying on elementary building blocks (exploration and validation algorithms) and a methodology defining how to use and combine them for specific maintenance tasks.
Reference: [13] <author> G. Kamp and H. Wache, `CTL: </author> <title> A description logic with expressive concrete domains', </title> <booktitle> in Proc. KI-96, 20. Deutsche Jahrestagung fur Kunstliche Intelligenz, </booktitle> <publisher> ed., </publisher> <address> St. </address> <publisher> Holldobler. Springer Verlag, </publisher> <year> (1996). </year> <note> To appear. </note>
Reference-contexts: 1 Introduction Based upon experiences from previous projects and application studies concerning logic-based knowledge representation <ref> [6, 2, 13] </ref> and knowledge evolution [15, 11], the aim of the VEGA project (Knowledge Validation and Exploration by Global Analysis) is to contribute to a methodology of maintaining declarative knowledge [1, 12, 14].
Reference: [14] <author> O. Kuhn, </author> <title> `Knowledge sharing and knowledge evolution', </title> <booktitle> in IJCAI`93 Workshop on Knowledge Sharing and Information Interchange, </booktitle> <year> (1993). </year>
Reference-contexts: 1 Introduction Based upon experiences from previous projects and application studies concerning logic-based knowledge representation [6, 2, 13] and knowledge evolution [15, 11], the aim of the VEGA project (Knowledge Validation and Exploration by Global Analysis) is to contribute to a methodology of maintaining declarative knowledge <ref> [1, 12, 14] </ref>. This goal demands techniques and methods for all maintenance activities (e.g. debugging, adaptation, change) within the life-cycle of knowledge bases (KBs)|relying on elementary building blocks (exploration and validation algorithms) and a methodology defining how to use and combine them for specific maintenance tasks.
Reference: [15] <author> G. Lohse and O. Kuhn, </author> <title> `Product and Knowledge Conserving with the Design Support System KONUS', </title> <booktitle> in Proceedings of ICED-95, 10th Int. Conf. on Engineering Design, </booktitle> <pages> pp. 1443-1448, </pages> <month> (August </month> <year> 1995). </year>
Reference-contexts: 1 Introduction Based upon experiences from previous projects and application studies concerning logic-based knowledge representation [6, 2, 13] and knowledge evolution <ref> [15, 11] </ref>, the aim of the VEGA project (Knowledge Validation and Exploration by Global Analysis) is to contribute to a methodology of maintaining declarative knowledge [1, 12, 14].
Reference: [16] <author> P. Meseguer, </author> <title> `Expert system validation through knowledge base refinement', </title> <booktitle> in IJCAI-93, </booktitle> <year> (1993). </year>
Reference-contexts: For this purpose, we examined the already well-established field of theory refinement (TR) as described by Meseguer <ref> [16] </ref>: "Theory refinement considers the improvement of an approximate domain theory from a set of cases with known solution." Adopting this definition of TR it can be seen as the basis for KB debugging or updates as well as for KB evolution in a slowly changing domain [7].
Reference: [17] <author> R.J. Mooney, </author> <title> `Batch versus incremental theory refinement', </title> <booktitle> in Proc. AAAI Spring Symposium on Knowledge Assimilation, p. </booktitle> <pages> 16 pages, </pages> <year> (1992). </year>
Reference-contexts: However, problems arise with this requirement as different notions of minimality are sometimes used (cf. [5, 34]). Sometimes, a non-minimal revision may also be desirable, e.g., due to an extremely over-specialized theory <ref> [17] </ref> or because basic concepts should be corrected [23]. 2 In this view of TR as a search problem, an operator application carries out a modification of the theory. <p> While some systems try to correct the classification of one example at a time, others try to correct a whole batch of examples at a time. The number of examples taken into account in one theory refinement step can have considerable impact on the resulting theory (cf. <ref> [17] </ref>). When correcting one example at a time, many systems rely on an externally provided order of the examples, while others make a selection of their own. An example of the latter is the strategy used in Audrey II [33].
Reference: [18] <author> R.J. Mooney and D. Ourston, </author> <title> `Constructive induction in theory refinement', </title> <booktitle> in Proc. of the 8th Int. Machine Learning Workshop, </booktitle> <pages> pp. 178-182, </pages> <month> (June </month> <year> 1991). </year>
Reference-contexts: However, several minimal specialization operators are given [5, 34]. The aim of these operators is to exclude as few derivations as possible during specialization. 6 * The resulting theory should be minimal. This requirement is generally interpreted syntactically (e.g., <ref> [24, 23, 18] </ref>). A semantic interpretation of the requirement would also be possible, e.g., to allow no more derivations than necessary to explain the given examples. However, this is generally not desirable, as it will probably lead to an over-specialization of the theory.
Reference: [19] <author> R.J. Mooney and J.M. Zelle, </author> <title> `Integrating ILP and EBL', </title> <journal> SIGART Bulletin, </journal> <volume> 5(1), </volume> <pages> 12-21, </pages> <year> (1994). </year>
Reference-contexts: Operationalization is done as in explanation based learning: complete proof traces are summarized by single rules, however, this is made for all possible proofs and not for a single example (cp. <ref> [29, 19] </ref>). 4 This has the advantage that derivations during TR consist of a single step, thus simplifying the task. However, after TR a de-operationalization step is necessary for reconstructing the initial concept hierarchy. As the rules change during theory refinement this is not a trivial task. <p> These techniques are similar to operationalization/de-operationalization approaches as used in RX. However, there the goal is to improve the performance of the system. The techniques used in both cases are strongly related to EBL <ref> [19] </ref>, folding/unfolding [27], and inverse resolution [21]. 11 In Ruth besides the examples (here integrity theory) a set of hypotheses Hypo exists. However, both sets can be combined into one. <p> This leads to suggestions for further work in TR opening a wider range of application for TR technology. As Craw and Sleeman [7] mentioned, the classical concept of TR stemming from data-intensive concept learning (cf. <ref> [19] </ref>) is not adequate for the refinement of knowledge-based systems. Here, we rather have a data-sparse process exploiting various sources of background-knowledge in order to manage continuous change in evolving domains or application-driven debugging with very little example information.
Reference: [20] <author> K. Morik, S. Wrobel, J. Kietz, and W. Emde, </author> <title> Knowledge Acquisition and Machine Learning: Theory, Methods and Applications, </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: In order to illustrate this breadth, we will exemplarily describe some interesting approaches briefly highlighting their specific features. Mobal is aimed at supporting knowledge acquisition and knowledge base maintenance <ref> [20, 36] </ref>. In Mobal rules may be accompanied by an exception set, i.e., a rule may only be applied to facts not contained in this set. Further, a confidence can be attributed to rules and facts. <p> First contributions along this line of research came from the MOBAL group <ref> [20] </ref>; Wrobel demonstrates the cooperation of different knowledge sources during knowledge acquisition and revision [35, 36, 34] and grounds his approach in belief change [26], a research area which could also yield the very basic theoretical framework for a comprehensive view on KB maintenance.
Reference: [21] <author> S. Muggleton, </author> <title> `Inverse entailment and progol', </title> <journal> New Generation Computing, </journal> <volume> 13(3, 4), </volume> <pages> 245-286, </pages> <year> (1995). </year>
Reference-contexts: These techniques are similar to operationalization/de-operationalization approaches as used in RX. However, there the goal is to improve the performance of the system. The techniques used in both cases are strongly related to EBL [19], folding/unfolding [27], and inverse resolution <ref> [21] </ref>. 11 In Ruth besides the examples (here integrity theory) a set of hypotheses Hypo exists. However, both sets can be combined into one. As we mentioned above, when using a search method like hill-climbing it is possible that the TR system gets stuck in a local minimum.
Reference: [22] <author> S. Muggleton and L. de Raedt, </author> <title> `Inductive logic programming: Theory and methods', </title> <type> Report CW 178, </type> <institution> Department of Computing Science, Katholieke Universiteit Leuven, </institution> <month> (May </month> <year> 1993). </year> <note> Also in Journal of Logic Programming, 1994, Vol. 19,20:629-679. </note>
Reference-contexts: The state space consists of all tuples of example sets and theories. Usually, examples are given as ground facts. However, this restriction is not necessary, e.g., in Ruth integrity constraints assume the role of examples. Adopting the viewpoint of Inductive Logic Programming (ILP) <ref> [22] </ref> which we consider in a sense the more general concept than TR, theories are logic programs. Usually, the programs are further restricted by the requirement that they may only use predicates which already exist in the original theory. <p> Such exploration as well as the easy combination of different existing ideas could be supported by an experimental workbench implemented as an open toolbox together with a generic control mechanism (cf. the GENCOL / GENCON algorithm for concept learning and ILP, resp., introduced in <ref> [10, 22, 28] </ref>). On the other hand, regarding our initial goal of approaching a methodology for KB maintenance, we can check techniques and methods already existing in TR against requirements coming from KBS development and use. <p> However, we consider de Raedt et al.'s lifting of both incremental concept learning and intensional knowledge base updates to a more general notion of Inductive Logic Programming <ref> [22, 9] </ref> as an important step into this desired direction. ILP is a still emerging research area providing a number of elaborated well-founded knowledge-intensive generalization, specialization, and restructuring techniques that must be put into the application context of knowledge refinement.
Reference: [23] <author> D. Ourston and R.J. Mooney, </author> <title> `Theory refinement combining analytical and empirical methods', </title> <journal> Artificial Intelligence, </journal> <volume> 66, </volume> <pages> 273-309, </pages> <year> (1994). </year>
Reference-contexts: However, problems arise with this requirement as different notions of minimality are sometimes used (cf. [5, 34]). Sometimes, a non-minimal revision may also be desirable, e.g., due to an extremely over-specialized theory [17] or because basic concepts should be corrected <ref> [23] </ref>. 2 In this view of TR as a search problem, an operator application carries out a modification of the theory. <p> Concept introduction is initiated if the exception set of a rule gets too large. 2 Correcting more fundamental concepts, i.e., concepts which are next to the leafs of the classification tree, is clearly not semantically minimal. However, it is not only useful (cf. <ref> [23] </ref>), but also corresponds to human behaviour (cf. [32]). 3 Thus, the optimization criterion is slightly different from the one stated above. RX | The most distinguishing feature of RX is that TR is performed on an operationalized version of the theory. <p> However, several minimal specialization operators are given [5, 34]. The aim of these operators is to exclude as few derivations as possible during specialization. 6 * The resulting theory should be minimal. This requirement is generally interpreted syntactically (e.g., <ref> [24, 23, 18] </ref>). A semantic interpretation of the requirement would also be possible, e.g., to allow no more derivations than necessary to explain the given examples. However, this is generally not desirable, as it will probably lead to an over-specialization of the theory. <p> In this section we will discuss these topics in the context of TR. 5 Note, that here the transformations performed during TR are different from the elementary edit-operations. 6 Sometimes, non-minimal semantic changes are desired, e.g., if in the Either-system <ref> [23] </ref> several rules can be modified for achieving correct classification, the most basic rule is chosen. This leads to a change in the definition of a very basic predicate, thus modifying a large number of derivations.
Reference: [24] <author> B.L. Richards and R.J. Mooney, </author> <title> `Automated refinement of first-order horn-clause domain theories', </title> <journal> Machine Learning, </journal> <volume> 19(2), </volume> <month> (April </month> <year> 1995). </year>
Reference-contexts: Basically, any search method can be used for implementing TR. However, typically a hill-climbing approach is used with the number of inconsistent examples as a quality measure on the theory. A system that illustrates the view of TR as search very well is Forte <ref> [24] </ref>. Its main control loop is depicted in Figure 2. <p> However, several minimal specialization operators are given [5, 34]. The aim of these operators is to exclude as few derivations as possible during specialization. 6 * The resulting theory should be minimal. This requirement is generally interpreted syntactically (e.g., <ref> [24, 23, 18] </ref>). A semantic interpretation of the requirement would also be possible, e.g., to allow no more derivations than necessary to explain the given examples. However, this is generally not desirable, as it will probably lead to an over-specialization of the theory. <p> In classification, no facts are contained in the theory and only basic features are given in the examples. Consequently, a whole proof must be abduced leading from the concepts in the example to the classification, including all intermediate concepts. Identification of Possible Revision Points Similar to <ref> [24] </ref>, we call the points in the theory (or in the example set), an error may be attributed to, revision points. In the case of a commission error, all facts and rules (and all literals in all rules) used in a proof trace are revision points.
Reference: [25] <author> L. Saitta, M. Botta, and F. Neri, </author> <title> `Multistrategy learning and theory revision', </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 153-172, </pages> <year> (1993). </year>
Reference-contexts: A very first step concerning the technical provisions for such a redirection of TR towards real-world KBS maintenance are made in our VEGA Knowledge Evolution System KES [1]. Another promising approach are systems like WHY <ref> [25] </ref> or NeoDisciple [31]|referred to by Craw & Sleeman [7] as Multi-Strategy Revision Systems|that apply several learning techniques to several different knowledge sources.
Reference: [26] <author> L. Sombe, </author> <title> `A glance at revision and updating in knowledge bases', </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 9, </volume> <pages> 1-27, </pages> <year> (1994). </year>
Reference-contexts: First contributions along this line of research came from the MOBAL group [20]; Wrobel demonstrates the cooperation of different knowledge sources during knowledge acquisition and revision [35, 36, 34] and grounds his approach in belief change <ref> [26] </ref>, a research area which could also yield the very basic theoretical framework for a comprehensive view on KB maintenance.
Reference: [27] <author> E. Sommer, </author> <title> `Rulebase stratification: An approach to theory restructuring', </title> <booktitle> in Proc. Fourth Int. Workshop on Inductive Logic Programming (ILP 94), </booktitle> <month> (September </month> <year> 1994). </year>
Reference-contexts: Internal requirements like performance or usage of a representation language in TR different from the externally used language. 3. Providing the system with a means to escape from local minima. For making a theory more readable one can introduce intermediate concepts, which improve its hierarchical organization <ref> [27] </ref>. These techniques are similar to operationalization/de-operationalization approaches as used in RX. However, there the goal is to improve the performance of the system. The techniques used in both cases are strongly related to EBL [19], folding/unfolding [27], and inverse resolution [21]. 11 In Ruth besides the examples (here integrity theory) <p> theory more readable one can introduce intermediate concepts, which improve its hierarchical organization <ref> [27] </ref>. These techniques are similar to operationalization/de-operationalization approaches as used in RX. However, there the goal is to improve the performance of the system. The techniques used in both cases are strongly related to EBL [19], folding/unfolding [27], and inverse resolution [21]. 11 In Ruth besides the examples (here integrity theory) a set of hypotheses Hypo exists. However, both sets can be combined into one. <p> 2 -operator are used in Miles, cf. [28]). * The instances of the antecedents of a rule that are not contained in the exception set are aggregated in a concept (Mobal, cf. [36]). * In stratification the new concepts are generated from iden tical parts in different rules (Fender, cf. <ref> [27] </ref>). Restructuring techniques can be distinguished according to their effect on the model of the theory. While the first two above-mentioned approaches change the semantics of the theory, the third does not. Whether restructuring achieves the goal of leaving the local minimum cannot be decided a priori.
Reference: [28] <author> I. Stahl and B. Tausend, </author> <title> `Miles | Eine Programmierumge-bung fur die Induktive Logische Programmierung', </title> <booktitle> in Beitrage zum 7 Fachgruppentreffen Maschinelles Lernen, </booktitle> <address> Kaiserslautern, </address> <month> August </month> <year> 1994, </year> <pages> pp. 60-64, </pages> <institution> Fachbereich Infor-matik, Universitat Kaiserslautern, </institution> <year> (1995). </year>
Reference-contexts: By changing the representation language, the range of possible results of the specialization and generalization operators is increased. Some techniques for introducing new concepts are: * Inverse resolution techniques like inter- and intra-construction (e.g., intra-construction and the G 2 -operator are used in Miles, cf. <ref> [28] </ref>). * The instances of the antecedents of a rule that are not contained in the exception set are aggregated in a concept (Mobal, cf. [36]). * In stratification the new concepts are generated from iden tical parts in different rules (Fender, cf. [27]). <p> Such exploration as well as the easy combination of different existing ideas could be supported by an experimental workbench implemented as an open toolbox together with a generic control mechanism (cf. the GENCOL / GENCON algorithm for concept learning and ILP, resp., introduced in <ref> [10, 22, 28] </ref>). On the other hand, regarding our initial goal of approaching a methodology for KB maintenance, we can check techniques and methods already existing in TR against requirements coming from KBS development and use.
Reference: [29] <author> S. Tangkitvanich and M. Shimura, </author> <title> `Refining a relational theory with multiple faults in the concept and subconcepts', </title> <booktitle> in Proc. of the 9th Int. Workshop on Machine Learning, </booktitle> <pages> pp. 436-444, </pages> <year> (1992). </year>
Reference-contexts: Operationalization is done as in explanation based learning: complete proof traces are summarized by single rules, however, this is made for all possible proofs and not for a single example (cp. <ref> [29, 19] </ref>). 4 This has the advantage that derivations during TR consist of a single step, thus simplifying the task. However, after TR a de-operationalization step is necessary for reconstructing the initial concept hierarchy. As the rules change during theory refinement this is not a trivial task.
Reference: [30] <author> S. Tangkitvanich and M. Shimura, </author> <title> `Learning from an approximate theory and noisy examples', </title> <booktitle> in Proc. AAAI-93, </booktitle> <pages> pp. 466-471, </pages> <year> (1993). </year>
Reference-contexts: This can be regarded as consistency with a modified example set. However, some systems explicitly allow for misclassifica-tions. In the Latex-system <ref> [30] </ref> the MDL-principle is used for making a trade-off between a modification of the theory and acceptance of a misclassified example.
Reference: [31] <author> G. </author> <title> Tecuci, `Cooperation in knowledge base refinement', </title> <booktitle> in Proc. of the 9th Int. Workshop on Machine Learning, </booktitle> <pages> pp. 445-450, </pages> <year> (1992). </year>
Reference: [32] <author> Edward J. Wisniewski and Douglas L. Medin, `Harpoons and long sticks: </author> <title> The interaction of theory and similarity in rule induction', in Concept Formation: Knowledge and Experience in Unsupervised Learning, </title> <editor> eds., D.H. Fisher, M.J. Paz-zani, and P. Langley, </editor> <volume> chapter 9, </volume> <pages> 237-278, </pages> <publisher> Morgan Kaufmann, </publisher> <year> (1991). </year>
Reference-contexts: However, it is not only useful (cf. [23]), but also corresponds to human behaviour (cf. <ref> [32] </ref>). 3 Thus, the optimization criterion is slightly different from the one stated above. RX | The most distinguishing feature of RX is that TR is performed on an operationalized version of the theory.
Reference: [33] <author> J. Wogulis and M.J. Pazzani, </author> <title> `A methodology for evaluating theory revision systems: Results with audrey ii', </title> <booktitle> in Proc. </booktitle> <volume> IJ-CAI '93, </volume> <editor> ed., R. Bajcsy, </editor> <volume> volume 2, </volume> <pages> pp. 1128-1134. </pages> <publisher> Morgan Kaufmann, </publisher> <year> (1993). </year>
Reference-contexts: Most authors interpret it syntactically. Several approaches for measuring syntactic change can be given. The most simple measure is to count the number of operator applications (e.g., transaction length in Ruth). Wogulis and Pazzani proposed a somewhat more sophisticated measure in <ref> [33] </ref>, namely the number of primitive edit-operations (i.e., delete/add/replace literal) needed to transform the original theory into the revised one. 5 The requirement of minimal semantic change is much more seldom. To our knowledge no measure for comparing a revised theory with the initial one is given in literature. <p> When correcting one example at a time, many systems rely on an externally provided order of the examples, while others make a selection of their own. An example of the latter is the strategy used in Audrey II <ref> [33] </ref>.
Reference: [34] <author> S. Wrobel, </author> <title> `On the proper definition of minimality in specialization and theory revision', </title> <booktitle> in Proc. of the Sixth European Conf. on Machine Learning, </booktitle> <editor> ed., Pavel B. Brazdil, </editor> <booktitle> volume 667 of LNAI, </booktitle> <pages> pp. 65-82. </pages> <publisher> Springer Verlag, </publisher> <month> (April </month> <year> 1993). </year>
Reference-contexts: Some authors refine this goal to the additional requirement that the revised theory should be as close as possible to the original one. However, problems arise with this requirement as different notions of minimality are sometimes used (cf. <ref> [5, 34] </ref>). Sometimes, a non-minimal revision may also be desirable, e.g., due to an extremely over-specialized theory [17] or because basic concepts should be corrected [23]. 2 In this view of TR as a search problem, an operator application carries out a modification of the theory. <p> To our knowledge no measure for comparing a revised theory with the initial one is given in literature. However, several minimal specialization operators are given <ref> [5, 34] </ref>. The aim of these operators is to exclude as few derivations as possible during specialization. 6 * The resulting theory should be minimal. This requirement is generally interpreted syntactically (e.g., [24, 23, 18]). <p> First contributions along this line of research came from the MOBAL group [20]; Wrobel demonstrates the cooperation of different knowledge sources during knowledge acquisition and revision <ref> [35, 36, 34] </ref> and grounds his approach in belief change [26], a research area which could also yield the very basic theoretical framework for a comprehensive view on KB maintenance.
Reference: [35] <author> S. Wrobel, </author> <title> Concept Formation and Knowledge Revision, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: First contributions along this line of research came from the MOBAL group [20]; Wrobel demonstrates the cooperation of different knowledge sources during knowledge acquisition and revision <ref> [35, 36, 34] </ref> and grounds his approach in belief change [26], a research area which could also yield the very basic theoretical framework for a comprehensive view on KB maintenance.
Reference: [36] <author> S. Wrobel, </author> <title> `Concept formation during interactive theory revision', </title> <journal> Machine Learning, </journal> <volume> 14(2), </volume> <pages> 169-191, </pages> <month> (February </month> <year> 1994). </year> <title> (Special Issue on Evaluating and Changing Representation). </title>
Reference-contexts: In order to illustrate this breadth, we will exemplarily describe some interesting approaches briefly highlighting their specific features. Mobal is aimed at supporting knowledge acquisition and knowledge base maintenance <ref> [20, 36] </ref>. In Mobal rules may be accompanied by an exception set, i.e., a rule may only be applied to facts not contained in this set. Further, a confidence can be attributed to rules and facts. <p> for introducing new concepts are: * Inverse resolution techniques like inter- and intra-construction (e.g., intra-construction and the G 2 -operator are used in Miles, cf. [28]). * The instances of the antecedents of a rule that are not contained in the exception set are aggregated in a concept (Mobal, cf. <ref> [36] </ref>). * In stratification the new concepts are generated from iden tical parts in different rules (Fender, cf. [27]). Restructuring techniques can be distinguished according to their effect on the model of the theory. While the first two above-mentioned approaches change the semantics of the theory, the third does not. <p> Whether restructuring achieves the goal of leaving the local minimum cannot be decided a priori. In general, this can only be decided after performing and evaluating further specialization and generalization steps. However, Wrobel identifies some criteria for concepts, which point to their usefulness in TR (cf. <ref> [36] </ref>). Up to now, only few first order TR systems employ restructuring methods. 5 Conclusions and Future Work As already pointed out in section 1, theory refinement systems can perform various tasks frequently needed when maintaining a knowledge base. <p> First contributions along this line of research came from the MOBAL group [20]; Wrobel demonstrates the cooperation of different knowledge sources during knowledge acquisition and revision <ref> [35, 36, 34] </ref> and grounds his approach in belief change [26], a research area which could also yield the very basic theoretical framework for a comprehensive view on KB maintenance.
References-found: 36

