URL: http://www.cs.berkeley.edu/~remzi/Conferences/SOSP97/dcpi.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~remzi/Conferences/SOSP97/
Root-URL: 
Title: Continuous Profiling: Where Have All the Cycles Gone?  
Author: Jennifer Anderson Lance M. Berc Jeffrey Dean Sanjay Ghemawat Monika R. Henzinger Shun-Tak Leung Richard L. Sites Mark T. Vandevoorde Carl A. Waldspurger William E. Weihl 
Date: October 1997.  
Note: DRAFT: Revised version to appear in the 16th SOSP,  
Abstract: This paper describes the Digital Continuous Profiling Infrastructure, a sample-based profiling system designed to run continuously on production systems. The system supports multiprocessors, works on unmodified executables, and collects profiles for entire systems, including user programs, shared libraries, and the operating system kernel. Samples are collected at a high rate (over 5200 samples/sec per 333-MHz processor), yet with low overhead (13% slowdown for most workloads). Analysis tools supplied with the profiling system use the sample data to produce an accurate accounting, down to the level of pipeline stalls incurred by individual instructions, of where time is being spent. When instructions incur stalls, the tools identify possible reasons, such as cache misses, branch mis-predictions, and functional unit contention. The fine-grained instruction-level analysis guides users and automated optimizers to the causes of performance problems and provides important insights for fixing them. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson and E. D. Lazowska. Quartz: </author> <title> A tool for tuning parallel program performance. </title> <booktitle> Proceedings of the ACM SIGMET-RICS 1990 Conference on Measurement & Modeling of Computer Systems, </booktitle> <address> 18(1):115125, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The stalls column indicates whether the system can subdivide the time spent at an instruction into components like cache miss latency, branch misprediction delays, etc. The systems fall into two groups. The first includes pixie [13], gprof [9], jprof [15], quartz <ref> [1] </ref>, SimOS [16], System Overhead Scope Grain Stalls pixie High App inst count none gprof High App proc count none jprof High App proc count none quartz High App proc count none SimOS High Sys inst time precise Vtune (dynamic) High App inst time precise prof Low App inst time none
Reference: [2] <author> T. Ball and J. Larus. </author> <title> Optimally profiling and tracing programs. </title> <journal> ACM TOPLAS, </journal> <volume> 16(4):13191360, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: These systems use binary modification, compiler support, or direct simulation of programs to gather measurements. They all have high overhead and usually require significant user intervention. The slowdown is too large for continuous measurements during production use, despite techniques that reduce instrumentation overhead substantially <ref> [2] </ref>. Most systems in the second group, including prof [14], iprobe [11], Morph [5], the Vtune sampler [18], ss-run [19], and our system, use statistical sampling to collect fine-grain information on program or system behavior.
Reference: [3] <author> D. Blickstein et al. </author> <title> The GEM optimizing compiler system. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4), </volume> <year> 1992. </year>
Reference-contexts: The profiling system has been running on Digital Alpha processors under Digital Unix since September 1996, and ports are in progress to Alpha/NT and OpenVMS. Work is also underway to feed the output of our tools into Digital's optimizing backend <ref> [3] </ref> and into the OM post-linker optimization framework [6]. As discussed in Section 3, the combination of fine-grained instruction-level analysis and detailed profiling of long-running workloads has produced insights into performance that are difficult to achieve with other tools.
Reference: [4] <author> D. Carta. </author> <title> Two fast implementations of the `minimal standard' random number generator. </title> <journal> CACM, </journal> <volume> 33(1):8788, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: To minimize any systematic correlation between the timing of the interrupts and the code being run, we randomize the length of the sampling period by writing a pseudo-random value <ref> [4] </ref> into the performance counter at the end of each interrupt. The default sampling period is distributed uniformly between 60K and 64K when monitoring CYCLES. 4.1.2 Attributing Events to PCs To accurately interpret samples, it is important to understand the PC delivered to the interrupt handler.
Reference: [5] <author> J. B. Chen et al. </author> <title> Morph: A framework for platform-specific optimization, </title> <month> March </month> <year> 1996. </year> <note> http://www.eecs.harvard.edu/morph/. </note>
Reference-contexts: They all have high overhead and usually require significant user intervention. The slowdown is too large for continuous measurements during production use, despite techniques that reduce instrumentation overhead substantially [2]. Most systems in the second group, including prof [14], iprobe [11], Morph <ref> [5] </ref>, the Vtune sampler [18], ss-run [19], and our system, use statistical sampling to collect fine-grain information on program or system behavior. SGI SpeedShop's perfex gathers total numbers of events such as cycles, data cache misses, etc. over an entire program run [19].
Reference: [6] <author> R. Cohn and P. G. Lowney. </author> <title> Hot cold optimization of large Windows/NT applications. </title> <booktitle> In 29th Annual International Symposium on Microarchitecture (Micro-29), </booktitle> <address> Paris, France, </address> <month> December </month> <year> 1996. </year> <month> 16 </month>
Reference-contexts: The profiling system has been running on Digital Alpha processors under Digital Unix since September 1996, and ports are in progress to Alpha/NT and OpenVMS. Work is also underway to feed the output of our tools into Digital's optimizing backend [3] and into the OM post-linker optimization framework <ref> [6] </ref>. As discussed in Section 3, the combination of fine-grained instruction-level analysis and detailed profiling of long-running workloads has produced insights into performance that are difficult to achieve with other tools. These insights have been used to improve the performance of several major commercial applications.
Reference: [7] <author> Digital Equipment Corporation. </author> <title> Alpha 21164 Microprocessor Hardware Reference Manual. </title> <address> Maynard, MA, </address> <year> 1995. </year> <title> Order Number EC-QAEQB-TE. </title>
Reference-contexts: The rest of this section describes these pieces in more detail, beginning with the hardware performance counters. 4.1 Alpha Performance Counters Alpha processors <ref> [8, 7] </ref> provide a small set of hardware performance counters that can each be configured to count a specified event. The precise number of counters, set of supported events, and other interface details vary across Alpha processor implementations. <p> When a performance counter overflows, it generates a high-priority interrupt that delivers the PC of the next instruction to be executed <ref> [17, 7] </ref> and the identity of the overflowing counter. When the device driver handles this interrupt, it records the process identifier (PID) of the interrupted process, the PC delivered by the interrupt, and the event type that caused the interrupt. Our system's default configuration monitors CYCLES and IMISS events.
Reference: [8] <author> Digital Equipment Corporation. </author> <title> DECchip 21064 and DECchip 21064A Alpha AXP Microprocessors Hardware Reference Manual. </title> <address> Maynard, MA, </address> <year> 1995. </year> <title> Order Number EC-Q9ZUA-TE. </title>
Reference-contexts: The rest of this section describes these pieces in more detail, beginning with the hardware performance counters. 4.1 Alpha Performance Counters Alpha processors <ref> [8, 7] </ref> provide a small set of hardware performance counters that can each be configured to count a specified event. The precise number of counters, set of supported events, and other interface details vary across Alpha processor implementations.
Reference: [9] <author> S. Graham, P. Kessler, and M. McKusick. </author> <title> gprof: A call graph execution profiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 17(6):120126, </volume> <month> June </month> <year> 1982. </year>
Reference-contexts: The stalls column indicates whether the system can subdivide the time spent at an instruction into components like cache miss latency, branch misprediction delays, etc. The systems fall into two groups. The first includes pixie [13], gprof <ref> [9] </ref>, jprof [15], quartz [1], SimOS [16], System Overhead Scope Grain Stalls pixie High App inst count none gprof High App proc count none jprof High App proc count none quartz High App proc count none SimOS High Sys inst time precise Vtune (dynamic) High App inst time precise prof Low
Reference: [10] <author> M. Hall et al. </author> <title> Maximizing multiprocessor performance with the SUIF compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12):8489, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: The system was driven so as to maintain 8 outstanding queries. DSS 2786 35 300 MHz 8-CPU ALPHASERVER 8400 A decision-support system (DSS) query based upon the TPCD specification. parallel SPECfp 2777 168 300 MHz 4-CPU ALPHASERVER 4100 The SPECfp95 programs, parallelized by the Stanford SUIF com piler <ref> [10] </ref>. timesharing 7 days 300 MHz 4-CPU ALPHASERVER 4100 A timeshared server used for office and technical applications, running the default configuration of our system. We used this work load to gather statistics for a long-running profile session.
Reference: [11] <author> Iprobe. </author> <title> Digital internal tool. </title>
Reference-contexts: They all have high overhead and usually require significant user intervention. The slowdown is too large for continuous measurements during production use, despite techniques that reduce instrumentation overhead substantially [2]. Most systems in the second group, including prof [14], iprobe <ref> [11] </ref>, Morph [5], the Vtune sampler [18], ss-run [19], and our system, use statistical sampling to collect fine-grain information on program or system behavior. SGI SpeedShop's perfex gathers total numbers of events such as cycles, data cache misses, etc. over an entire program run [19].
Reference: [12] <author> R. Johnson, D. Pearson, and K. Pingali. </author> <title> The program structure tree: Computing control regions in linear time. </title> <booktitle> In ACM PLDI, </booktitle> <pages> pages 171185, </pages> <year> 1994. </year>
Reference-contexts: Otherwise, we use an extended version of the cycle equivalence algorithm in <ref> [12] </ref> to identify sets of blocks and edges that are guaranteed to be executed the same number of times. Each such set constitutes one equivalence class.
Reference: [13] <institution> MIPS Computer Systems. UMIPS-V Reference Manual (pixie and pixstats). </institution> <address> Sunnyvale, CA, </address> <year> 1990. </year> <title> [14] prof. Digital Unix man page. </title>
Reference-contexts: The stalls column indicates whether the system can subdivide the time spent at an instruction into components like cache miss latency, branch misprediction delays, etc. The systems fall into two groups. The first includes pixie <ref> [13] </ref>, gprof [9], jprof [15], quartz [1], SimOS [16], System Overhead Scope Grain Stalls pixie High App inst count none gprof High App proc count none jprof High App proc count none quartz High App proc count none SimOS High Sys inst time precise Vtune (dynamic) High App inst time precise
Reference: [15] <author> J. F. Reiser and J. P. Skudlarek. </author> <title> Program profiling problems, and a solution via machine language rewriting. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(1):3745, </volume> <month> January </month> <year> 1994. </year>
Reference-contexts: The stalls column indicates whether the system can subdivide the time spent at an instruction into components like cache miss latency, branch misprediction delays, etc. The systems fall into two groups. The first includes pixie [13], gprof [9], jprof <ref> [15] </ref>, quartz [1], SimOS [16], System Overhead Scope Grain Stalls pixie High App inst count none gprof High App proc count none jprof High App proc count none quartz High App proc count none SimOS High Sys inst time precise Vtune (dynamic) High App inst time precise prof Low App inst
Reference: [16] <author> M. Rosenblum, S. Herrod, E. Witchel, and A. Gupta. </author> <title> Complete computer simulation: The SimOS approach. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <month> Fall </month> <year> 1995. </year>
Reference-contexts: The stalls column indicates whether the system can subdivide the time spent at an instruction into components like cache miss latency, branch misprediction delays, etc. The systems fall into two groups. The first includes pixie [13], gprof [9], jprof [15], quartz [1], SimOS <ref> [16] </ref>, System Overhead Scope Grain Stalls pixie High App inst count none gprof High App proc count none jprof High App proc count none quartz High App proc count none SimOS High Sys inst time precise Vtune (dynamic) High App inst time precise prof Low App inst time none iprobe High
Reference: [17] <author> R. Sites and R. Witek. </author> <title> Alpha AXP Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <address> Newton, MA, </address> <year> 1995. </year>
Reference-contexts: When a performance counter overflows, it generates a high-priority interrupt that delivers the PC of the next instruction to be executed <ref> [17, 7] </ref> and the identity of the overflowing counter. When the device driver handles this interrupt, it records the process identifier (PID) of the interrupted process, the PC delivered by the interrupt, and the event type that caused the interrupt. Our system's default configuration monitors CYCLES and IMISS events. <p> In general, samples for events other than CYCLES and IMISS are helpful in tracking down performance problems, but less useful for detailed analysis. 4.1.3 Blind Spots: Deferred Interrupts Performance-counter interrupts execute at the highest kernel priority level (spldevrt), but are deferred while running non-interruptible PALcode <ref> [17] </ref> or system code at the highest priority level. 1 Events in PALcode and high-priority interrupt code are still counted, but samples for those events will be associated with the instruction that runs after the PALcode finishes or the interrupt level drops below spldevrt. 1 This makes profiling the performance-counter interrupt <p> Therefore these accesses do not generate any more cache misses. 4.2.3 Reducing Synchronization Synchronization is eliminated between interrupt handlers on different processors in a multiprocessor, and minimized between the handlers and other driver routines. Synchronization operations (in particular, memory barriers <ref> [17] </ref>) are expensive, costing on the order of 100 cycles, so even a small number of them in the interrupt handler would result in unacceptable overhead.
Reference: [18] <author> Vtune: </author> <title> Intel's visual tuning environment. </title> <address> http://developer.intel.com/design/perftool/vtune. </address>
Reference-contexts: inst time none iprobe High Sys inst time imprecise Morph Low Sys inst time none Vtune (sampler) Low Sys inst time imprecise perfex Low Sys total time none ssrun Low App inst time imprecise Our system Low Sys inst time precise Table 1: Profiling systems and the Vtune dynamic analyzer <ref> [18] </ref>. These systems use binary modification, compiler support, or direct simulation of programs to gather measurements. They all have high overhead and usually require significant user intervention. The slowdown is too large for continuous measurements during production use, despite techniques that reduce instrumentation overhead substantially [2]. <p> They all have high overhead and usually require significant user intervention. The slowdown is too large for continuous measurements during production use, despite techniques that reduce instrumentation overhead substantially [2]. Most systems in the second group, including prof [14], iprobe [11], Morph [5], the Vtune sampler <ref> [18] </ref>, ss-run [19], and our system, use statistical sampling to collect fine-grain information on program or system behavior. SGI SpeedShop's perfex gathers total numbers of events such as cycles, data cache misses, etc. over an entire program run [19].
Reference: [19] <author> M. Zagha et al. </author> <title> Performance analysis using the MIPS R10000 performance counters. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: They all have high overhead and usually require significant user intervention. The slowdown is too large for continuous measurements during production use, despite techniques that reduce instrumentation overhead substantially [2]. Most systems in the second group, including prof [14], iprobe [11], Morph [5], the Vtune sampler [18], ss-run <ref> [19] </ref>, and our system, use statistical sampling to collect fine-grain information on program or system behavior. SGI SpeedShop's perfex gathers total numbers of events such as cycles, data cache misses, etc. over an entire program run [19]. <p> second group, including prof [14], iprobe [11], Morph [5], the Vtune sampler [18], ss-run <ref> [19] </ref>, and our system, use statistical sampling to collect fine-grain information on program or system behavior. SGI SpeedShop's perfex gathers total numbers of events such as cycles, data cache misses, etc. over an entire program run [19]. Other systems that use performance counters, including iprobe, the Vtune sampler, and SGI SpeedShop, share some of the characteristics of our system. <p> Since only a limited number of events can be monitored simultaneously (2 on the 21064 and 3 on the 21164), our system also supports time-multiplexing among different events at a very fine grain. (SGI's Speed-shop <ref> [19] </ref> provides a similar multiplexing capability.) 4.1.1 Sampling Period Performance counters can be configured to overflow at different values; legal settings vary on different Alpha processors. When monitoring CYCLES on the Alpha 21064, interrupts can be generated every 64K events or every 4K events.
References-found: 18

