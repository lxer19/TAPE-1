URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr94/tr94-027.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr94-abstracts.html
Root-URL: http://www.cis.ufl.edu
Email: email: davis@cis.ufl.edu  
Phone: phone: (719) 472-4470  phone: (904) 392-1481  
Title: Potential and Achievable Parallelism in Unsymmetric-Pattern Multifrontal LU Factorization  
Author: Steven M. Hadfield Timothy A. Davis 
Address: USA  Gainesville, Florida, USA  
Note: United States Air Force  April 19, 1994 This project is supported the National Science Foundation (ASC-9111263, DMS-9223088)  
Affiliation: Department of Mathematical Sciences  Academy Colorado Springs, Colorado,  Computer and Information Sciences Department University of Florida  
Abstract: Technical Report TR-94-027, Computer and Information Sciences Department, University of Florida 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. A. Davis. </author> <title> Users' guide for the unsymmetric-pattern multifrontal package (UMFPACK). </title> <type> Technical Report TR-93-020, </type> <institution> Computer and Information Sciences Department, University of Florida, </institution> <address> Gainesville, FL, </address> <month> June </month> <year> 1993. </year> <note> To obtain UMFPACK, send email to netlib@ornl.gov with the message: send umfpack.shar from misc. </note>
Reference-contexts: Finally, the results of these models were validated by an actual implementation of the method on the nCUBE 2. Importantly, the assembly DAGS used in this effort were produced using a sequential formulation of the unsymmetric-pattern multifrontal method <ref> [1] </ref>. Parallelism was not a specific objective in their derivation. Alternative formulations of these assembly DAGs are possible and could reveal significant additional parallelism. Such alternatives are currently under investigation. 8
Reference: [2] <author> T. A. Davis and I. S. Duff. </author> <title> An unsymmetric-pattern multifrontal method for parallel sparse LU factorization. </title> <type> Technical Report TR-93-018, </type> <institution> Computer and Info. Sci. Dept., University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1993. </year>
Reference-contexts: 1 Unsymmetric-Pattern Multifrontal Method Multifrontal methods for the factorization of sparse matrices <ref> [2, 6, 7, 12] </ref> decompose the sparse matrix into a set of overlapping dense submatrices called frontal matrices. Each frontal matrix contains one or more pivots and is partially factorized according to these pivots.
Reference: [3] <author> I. S. Duff. </author> <title> Parallel implementation of multifrontal schemes. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 193-204, </pages> <year> 1986. </year>
Reference-contexts: Furthermore, the factorization of 5 the dense frontal matrices provides a second level of parallelism because of the independence and regularity in the factorization for each pivot. Earlier efforts have shown that exploitation of both levels of parallelism is necessary for the best performance <ref> [3, 5] </ref>. 2 Unbounded Parallelism Models The unbounded parallelism models (UPMs) explore the amount of theoretical parallelism available using analytical techniques and assume an unbounded number of available processors. The underlying model of computation is a multiple instruction, multiple data (MIMD) parallel random access machine (PRAM).
Reference: [4] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford Science Publications, </publisher> <address> New York, NY, </address> <year> 1989. </year>
Reference: [5] <author> I. S. Duff and L. S. Johnsson. </author> <title> Node orderings and concurrency in structurally-symmetric sparse problems. </title> <editor> In Graham F. Carey, editor, </editor> <booktitle> Parallel Supercomputing: Methods, Algorithms, and Applications, </booktitle> <pages> pages 177-189. </pages> <publisher> John Wiley and Sons Ltd., </publisher> <address> New York, NY, </address> <year> 1989. </year>
Reference-contexts: Furthermore, the factorization of 5 the dense frontal matrices provides a second level of parallelism because of the independence and regularity in the factorization for each pivot. Earlier efforts have shown that exploitation of both levels of parallelism is necessary for the best performance <ref> [3, 5] </ref>. 2 Unbounded Parallelism Models The unbounded parallelism models (UPMs) explore the amount of theoretical parallelism available using analytical techniques and assume an unbounded number of available processors. The underlying model of computation is a multiple instruction, multiple data (MIMD) parallel random access machine (PRAM).
Reference: [6] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear systems. </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: 1 Unsymmetric-Pattern Multifrontal Method Multifrontal methods for the factorization of sparse matrices <ref> [2, 6, 7, 12] </ref> decompose the sparse matrix into a set of overlapping dense submatrices called frontal matrices. Each frontal matrix contains one or more pivots and is partially factorized according to these pivots.
Reference: [7] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric set of linear equations. </title> <journal> SIAM J. of Sci. and Stat. Computing, </journal> <volume> 5(3) </volume> <pages> 633-641, </pages> <year> 1984. </year>
Reference-contexts: 1 Unsymmetric-Pattern Multifrontal Method Multifrontal methods for the factorization of sparse matrices <ref> [2, 6, 7, 12] </ref> decompose the sparse matrix into a set of overlapping dense submatrices called frontal matrices. Each frontal matrix contains one or more pivots and is partially factorized according to these pivots.
Reference: [8] <author> K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <editor> In R. J. Plemmons, editor, </editor> <booktitle> Parallel Algorithms for Matrix Computations, </booktitle> <pages> pages 1-82. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: All of the processors in the frontal matrix's subcube then update their active columns using these multipliers. This is commonly refered to as a fan-out algorithm <ref> [8] </ref>. Node weights correspond to a predicted parallel execution time. This predicted time is based on an analytical model of the fan-out algorithm with specific parameters set according to results from an implementation and evaluation of this algorithm on the nCUBE 2.
Reference: [9] <author> G. A. Geist and M. Heath. </author> <title> Matrix factorization on a hypercube. </title> <editor> In M. Heath, editor, </editor> <booktitle> Hypercube Multiprocessors 1986, </booktitle> <pages> pages 161-180. </pages> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: This is consistent with similar results for Cholesky factorization [13]. 5 Implementation Results Within the implementation, factorization of the frontal matrices is done using a pipelined, fan-out algorithm similar to that used in the distributed memory model <ref> [9] </ref>. With pipelining, however, the processor owning the next pivot column will update only that column and compute and send the next pivot's multipliers before doing the rest of the updates for the current pivot. This allows the communication to be overlapped with computation.
Reference: [10] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive-Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference: [11] <author> Michael T. Heath, Esmond Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <editor> In R. J. Plemmons, editor, </editor> <booktitle> Parallel Algorithms for Matrix Computations, </booktitle> <pages> pages 83-124. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference: [12] <author> J. W. H. Liu. </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> Theory and practice. SIAM Review, </journal> <volume> 34(1) </volume> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: 1 Unsymmetric-Pattern Multifrontal Method Multifrontal methods for the factorization of sparse matrices <ref> [2, 6, 7, 12] </ref> decompose the sparse matrix into a set of overlapping dense submatrices called frontal matrices. Each frontal matrix contains one or more pivots and is partially factorized according to these pivots.
Reference: [13] <author> L. S. Ostrouchov, M. T. Heath, and C. H. Romine. </author> <title> Modeling speedup in parallel sparse matrix factorization. </title> <type> Technical Report ORNL/TM-11786, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1990. </year> <pages> 9 10 11 12 </pages>
Reference-contexts: The results of this distributed memory performance model are shown in Figure 3. These results indicate that about 25 percent of the theoretically available parallelism should be achieved on the nCUBE 2. This is consistent with similar results for Cholesky factorization <ref> [13] </ref>. 5 Implementation Results Within the implementation, factorization of the frontal matrices is done using a pipelined, fan-out algorithm similar to that used in the distributed memory model [9].
References-found: 13

