URL: http://www.research.att.com/~lewis/papers/cohen97.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Title: Approximating Matrix Multiplication for Pattern Recognition Tasks  
Author: Edith Cohen David D. Lewis 
Abstract: Many pattern recognition tasks, including estimation, classification, and the finding of similar objects, make use of linear models. The fundamental operation in such tasks is the computation of the dot product between a query vector and a large database of instance vectors. Often we are interested primarily in those instance vectors which have high dot products with the query. We present a random sampling based algorithm that enables us to identify, for any given query vector, those instance vectors which have large dot products, while avoiding explicit computation of all dot products. We provide experimental results that demonstrate considerable speedups for text retrieval tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Wu. </author> <title> An optimal algorithm for approximate nearest neighbor searching. </title> <booktitle> In Proc. 5th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 573-582. ACM-SIAM, </pages> <year> 1994. </year>
Reference-contexts: running time, number of samples used, the number of dot products computed (equals the number of objects with scores at least T ), and the number of objects found with match at least 0:55; 0:6; : : :; 0:85 (recall that we use normalized vectors, hence all matches are in <ref> [0; 1] </ref>.) The first row of the table (algorithm Full) contains the performance data for the naive algorithm that computes all dot products. Note that each dot product computation amounts to 320 float multiply-adds, whereas each sampling step amounts to a memory lookup and a counter increment.
Reference: [2] <author> K. E. Atkinson. </author> <title> Numerical Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: The most popular such approach is Latent Semantic Indexing (LSI) [7]. LSI is based on Singular Value Decomposition (see <ref> [2] </ref> for background). The sparse document vectors x i 2 R t + are replaced by dense d-dimensional vectors ^ x i 2 R d where d t t (typically, d is of order 10 2 ).
Reference: [3] <author> J. L. Bentley, B. W. Weide, and A. C. Yao. </author> <title> Optimal expected-time algorithms for closest point problems. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 6 </volume> <pages> 563-580, </pages> <year> 1980. </year>
Reference: [4] <author> Chris Buckley, Gerard Salton, and James Allan. </author> <title> The effect of adding relevance information in a relevance feedback environment. </title> <editor> In W. Bruce Croft and C. J. van Rijsbergen, editors, </editor> <booktitle> SIGIR 94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 292-300, </pages> <address> London, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This weighting formula has been shown to be effective in making proximity of document vectors correspond more closely to similarity of meaning <ref> [4] </ref>. The resulting matrix is sparse, with about 2 fi 10 6 nonzeros. The vector for each document is normalized to have Euclidean norm of 1.0, so that dot product between two document vectors is their cosine correlation.
Reference: [5] <author> W. B. Croft and D. J. Harper. </author> <title> Using probabilistic models of document retrieval without relevance feedback. </title> <journal> Journal of Documentation, </journal> <volume> 35(4) </volume> <pages> 285-295, </pages> <year> 1979. </year>
Reference-contexts: The user query can be treated as a sample document, whose similarity to database documents must be found [14, ch. 4], or it can be used as evidence toward setting the parameters of a probabilistic classification function, which is then applied to database documents <ref> [13, 5] </ref>. In both cases, linear models are widely used and continue to be developed [12].
Reference: [6] <author> Belur V. Dasarathy, </author> <title> editor. Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Relation to Euclidean proximity problems A variety of tasks involve finding examples in close proximity to other examples. This may be a goal in itself, or may be a means of performing classification <ref> [6] </ref> or regression [11]. Frequently seen proximity problems are closest pair, nearest neighbors, and bichromatic nearest neighbors. A variety of proximity measures can be used in such tasks. As mentioned earlier, the cosine correlation can be implemented directly as a dot product.
Reference: [7] <author> Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman. </author> <title> Indexing by latent semantic indexing. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The most popular such approach is Latent Semantic Indexing (LSI) <ref> [7] </ref>. LSI is based on Singular Value Decomposition (see [2] for background). The sparse document vectors x i 2 R t + are replaced by dense d-dimensional vectors ^ x i 2 R d where d t t (typically, d is of order 10 2 ).
Reference: [8] <author> Susan T. Dumais. </author> <title> Latent semantic indexing (lsi): TREC-3 report. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Third Text Retrieval Conference (TREC-3), </booktitle> <pages> pages 219-230, </pages> <address> Gaithersburg, MD, </address> <year> 1995. </year> <institution> U. S. Dept. of Commerce, National Institute of Standards and Technology. </institution>
Reference-contexts: There has been little progress in speeding up retrieval with LSI document representations, beyond the obvious expedients of using parallel hardware or using fewer latent dimensions <ref> [8] </ref>. To achieve our speedup of 5 to 10-fold by dropping dimensions would require using only 32-64 dimensions, which would substantially impact effectiveness.
Reference: [9] <author> W. Feller. </author> <title> An introduction to probability theory and its applications, volume 1. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: S j has a binomial distribution with parameters S and p j . The expected value of S j is Sp j . The probability that S j = t (j is sampled t times) is t p t (see, e.g. Feller <ref> [9] </ref> for background.) Using the normal approximation, ProbfS j T g = 1 Sp j (1p j ) ProbfS j &lt; T g = Sp j (1p j ) 3 Implementation issues For the applications discussed here we apply the sam pling algorithm for products of k = 2 nonnegative ma
Reference: [10] <author> G. Golub. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins U. Press, </publisher> <address> Baltimore, MD, </address> <year> 1989. </year>
Reference-contexts: Information retrieval systems typically compute the dot product of a query vector with a collection of sparse document vectors by making use of an inverted file [18] of document vectors. The inverted file is the appropriate storage for efficient sparse matrix multiplication (see <ref> [10] </ref>). For each indexing term, a list of the id's of all document with a nonzero weight for that term, plus the weights themselves, are stored.
Reference: [11] <author> T. J. Hastie and R. J. Tibshirani. </author> <title> Generalized Additive Models, </title> <booktitle> volume 43 of Monographs on Statistics and Applied Probability. </booktitle> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: Relation to Euclidean proximity problems A variety of tasks involve finding examples in close proximity to other examples. This may be a goal in itself, or may be a means of performing classification [6] or regression <ref> [11] </ref>. Frequently seen proximity problems are closest pair, nearest neighbors, and bichromatic nearest neighbors. A variety of proximity measures can be used in such tasks. As mentioned earlier, the cosine correlation can be implemented directly as a dot product.
Reference: [12] <author> David D. Lewis, Robert E. Schapire, James P. Callan, and Ron Papka. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In SIGIR '96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: In both cases, linear models are widely used and continue to be developed <ref> [12] </ref>. Applying a weight vector q to a database of instance vectors means computing the vector by matrix product q T A T , where A is a matrix whose rows fx 1 ; : : : ; x n g are the instance vectors.
Reference: [13] <author> S. E. Robertson and K. Sparck Jones. </author> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <pages> pages 129-146, </pages> <month> May-June </month> <year> 1976. </year>
Reference-contexts: The user query can be treated as a sample document, whose similarity to database documents must be found [14, ch. 4], or it can be used as evidence toward setting the parameters of a probabilistic classification function, which is then applied to database documents <ref> [13, 5] </ref>. In both cases, linear models are widely used and continue to be developed [12].
Reference: [14] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Documents and queries are represented as vectors of terms or features. The user query can be treated as a sample document, whose similarity to database documents must be found <ref> [14, ch. 4] </ref>, or it can be used as evidence toward setting the parameters of a probabilistic classification function, which is then applied to database documents [13, 5]. In both cases, linear models are widely used and continue to be developed [12].
Reference: [15] <author> R. H. Thompson and W. B. Croft. </author> <title> Support for browsing in an intelligent text retrieval system. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 30 </volume> <pages> 639-668, </pages> <year> 1989. </year>
Reference-contexts: This finds, for each document, its set of nearest neighbors, under a dot product based proximity function. In information retrieval, such sets of nearest neighbors can be used to support hypertext browsing <ref> [15] </ref> or in doing a cluster analysis of the data [17]. Finding nearest neighbors has similar computational characteristics to running large numbers of queries or other linear classifiers against the document database. Different methods of representing texts will give document matrices with very different properties.
Reference: [16] <author> Howard Turtle and James Flood. </author> <title> Query evaluation: Strategies and optimizations. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 31(6) </volume> <pages> 831-850, </pages> <year> 1995. </year>
Reference-contexts: We propose a random-sampling based algorithm that identifies high-valued entries of nonnegative matrix products, without full computation of the product. Our algorithm assigns scores to each entry of q T A T . In contrast to existing approximate scoring techniques (see <ref> [16] </ref>), the expected value of the scores is equal to the true value that would be obtained with the full computation. Furthermore, the variance of the scores is independent of the weight distribution of the instance vectors. <p> We deemphasize our sparse matrix results, since a wide variety of methods of speeding up inverted file based scoring are already known, particularly when only the top scoring documents are needed and exact results are not necessary <ref> [18, 16] </ref>. These include compressing inverted files (to reduce I/O), using lower precision arithmetic, ignoring some inverted lists or parts of inverted lists, and limiting the number of document score accumulators maintained.
Reference: [17] <author> Peter Willett. </author> <title> Recent trends in hierarchic document clustering: A critical review. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 577-598, </pages> <year> 1988. </year>
Reference-contexts: This finds, for each document, its set of nearest neighbors, under a dot product based proximity function. In information retrieval, such sets of nearest neighbors can be used to support hypertext browsing [15] or in doing a cluster analysis of the data <ref> [17] </ref>. Finding nearest neighbors has similar computational characteristics to running large numbers of queries or other linear classifiers against the document database. Different methods of representing texts will give document matrices with very different properties.
Reference: [18] <author> Ian H. Witten, Alistair Moffat, and Timothy C. Bell. </author> <title> Managing Gigabytes: Compressing and Indexing Documents and Images. </title> <publisher> Von Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: In text retrieval, for example, the number of instances (documents) can be 10 4 to 10 6 and the number of features (words or phrases) may be 100,000 or more. This is an expensive task even when utilizing sparse matrix and indexing techniques <ref> [18] </ref>. Dense instances with hundreds of features are also common, for example, with factor analytic text representations (Sec. 4.1) or in image retrieval. We propose a random-sampling based algorithm that identifies high-valued entries of nonnegative matrix products, without full computation of the product. <p> Information retrieval systems typically compute the dot product of a query vector with a collection of sparse document vectors by making use of an inverted file <ref> [18] </ref> of document vectors. The inverted file is the appropriate storage for efficient sparse matrix multiplication (see [10]). For each indexing term, a list of the id's of all document with a nonzero weight for that term, plus the weights themselves, are stored. <p> We deemphasize our sparse matrix results, since a wide variety of methods of speeding up inverted file based scoring are already known, particularly when only the top scoring documents are needed and exact results are not necessary <ref> [18, 16] </ref>. These include compressing inverted files (to reduce I/O), using lower precision arithmetic, ignoring some inverted lists or parts of inverted lists, and limiting the number of document score accumulators maintained.
References-found: 18

