URL: http://www.epm.ornl.gov/pvm/PVMvsMPI.ps
Refering-URL: http://www.epm.ornl.gov/pvm/pvm_home.html
Root-URL: 
Title: PVM and MPI: a Comparison of Features faster within a large multiprocessor. It has many
Author: G. A. Geist J. A. Kohl P. M. Papadopoulos 
Note: MPI is expected to be  
Date: May 30, 1996  
Abstract: This paper compares PVM and MPI features, pointing out the situations where one may be favored over the other. Application developers can determine where their application most likely will run and if it requires particular features supplied by only one or the other of the APIs. PVM is better when applications will be run over heterogeneous networks. It has good interoperability between different hosts. PVM allows the development of fault tolerant applications that can survive host or task failures. Because the PVM model is built around the virtual machine concept (not present in the MPI model), it provides a powerful set of dynamic resource manager and process control functions. Each API has its unique strengths and this will remain so into the foreseeable future. One area of future research is to study the feasibility of creating a programming environment that allows access to the virtual machine features of PVM and the message passing features of MPI. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Graham E. Fagg and Jack J. Dongarra. PVMPI: </author> <title> An integration of the PVM and MPI systems. </title> <journal> Calculateurs Paralleles, </journal> <volume> 2, </volume> <year> 1996. </year>
Reference-contexts: Future Research: PVMPI The Universitiy of Tennessee and Oak Ridge National Laboratory have recently begun investigating the feasibility of merging features of PVM and MPI. The project is called PVMPI <ref> [1] </ref> and involves creating a programming environment that allows access to the virtual machine features of PVM and the message passing features of MPI. PVMPI would perform three symbiotic functions: It would use vendor implementations of MPI when available on multiprocessors.
Reference: [2] <author> MPI Forum. </author> <title> MPI: A message-passing interface standard. </title> <journal> International Journal of Supercomputer Application, </journal> <volume> 8 (3/4):165 - 416, </volume> <year> 1994. </year>
Reference-contexts: 1. Introduction The recent emergence of the MPI (Message Passing Interface) specification <ref> [2] </ref> has caused many programmers to wonder whether they should write their applications in MPI or use PVM (Parallel Virtual Machine) [3]. PVM is the existing de facto standard for distributed computing and MPI is being touted as the future message passing standard.
Reference: [3] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine. </title> <publisher> MIT press, </publisher> <year> 1994. </year>
Reference-contexts: 1. Introduction The recent emergence of the MPI (Message Passing Interface) specification [2] has caused many programmers to wonder whether they should write their applications in MPI or use PVM (Parallel Virtual Machine) <ref> [3] </ref>. PVM is the existing de facto standard for distributed computing and MPI is being touted as the future message passing standard. A related concern of users is whether they fl This work was supported in part by the Applied Mathematical Sciences subprogram of the Office of Energy Research, U.S.
Reference: [4] <author> G. A. Geist and V. S. Sunderam. </author> <title> Network-based concurrent computing on the PVM system. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 4 (4):293 - 311, </volume> <year> 1992. </year>
Reference-contexts: They needed a framework to explore this new area and so developed the concept of a Parallel Virtual Machine (PVM) <ref> [8, 4] </ref>. In 1991, Bob Manchek (a research associate at the University of Tennessee) joined the research and implemented a portable, robust version of the PVM design (PVM 2.0). Jack Dongarra, who was also involved in our heterogeneous distributed computing research, was instrumental in making PVM 2.0 publically available.
Reference: [5] <author> Ken Koch and Harvey Wasserman. </author> <title> A message passing algorithm for Sn transport. </title> <booktitle> In Proceedings of 1996 PVM User Group meeting, </booktitle> <year> 1996. </year> <note> http://bay.lanl.gov/pvmug96. </note>
Reference-contexts: Given this design focus, MPI is expected to always be faster than PVM on MPP hosts. Even so, two recent comparison studies show that PVM and MPI have very comparable performance on the Cray T3D and IBM SP-2 <ref> [10, 5] </ref>.
Reference: [6] <author> Jim Pruyne and Miron Livny. </author> <title> Providing resource management ser vices to parallel applications. </title> <booktitle> In Proceedings of the Second Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <year> 1994. </year> <note> http://www.cs.wisc.edu/condor/publications.html. </note>
Reference-contexts: One example of a user-requested feature was the addition of interfaces that allow third-party debuggers and resource managers to be seamlessly incorporated into the virtual machine <ref> [9, 6] </ref>. Examples of technology driven changes include the ability for PVM to transparently utilize shared memory and high-speed networks like ATM to move data between clusters of shared-memory multiprocessors. <p> This information can be used to redistribute load or expand the computation to utilize the new resource. Several systems have been designed specifically for this purpose, including the WoDi system [7] which uses Condor <ref> [6] </ref> on top of PVM. There are several important issues to consider when providing a fault notification scheme. For example, a task might request notification of an event after it has already occurred. PVM immediately generates a notify message in response to any such "after-the-fact" request.
Reference: [7] <author> Jim Pruyne and Miron Livny. </author> <title> Parallel processing on dynamic resources with CARMI. </title> <booktitle> In Proceedings of IPPS'95, </booktitle> <year> 1995. </year> <note> http://www.cs.wisc.edu/condor/publications.html. </note>
Reference-contexts: When a new host computer is added to the virtual machine, tasks can be notified of this as well. This information can be used to redistribute load or expand the computation to utilize the new resource. Several systems have been designed specifically for this purpose, including the WoDi system <ref> [7] </ref> which uses Condor [6] on top of PVM. There are several important issues to consider when providing a fault notification scheme. For example, a task might request notification of an event after it has already occurred. PVM immediately generates a notify message in response to any such "after-the-fact" request.
Reference: [8] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2 (4), </volume> <year> 1990. </year> <month> 15 </month>
Reference-contexts: They needed a framework to explore this new area and so developed the concept of a Parallel Virtual Machine (PVM) <ref> [8, 4] </ref>. In 1991, Bob Manchek (a research associate at the University of Tennessee) joined the research and implemented a portable, robust version of the PVM design (PVM 2.0). Jack Dongarra, who was also involved in our heterogeneous distributed computing research, was instrumental in making PVM 2.0 publically available.
Reference: [9] <author> BBN ToolWorks. </author> <title> Totalview parallel debugger. </title> <address> http://www.bbn.com:80/tv. </address>
Reference-contexts: One example of a user-requested feature was the addition of interfaces that allow third-party debuggers and resource managers to be seamlessly incorporated into the virtual machine <ref> [9, 6] </ref>. Examples of technology driven changes include the ability for PVM to transparently utilize shared memory and high-speed networks like ATM to move data between clusters of shared-memory multiprocessors.
Reference: [10] <author> S. VanderWiel, D. Nathanson, and D. Lilja. </author> <title> Performance and program complexity in contemporary network-based parallel computing systems. </title> <type> Technical Report HPPC-96-02, </type> <institution> University of Minnesota, </institution> <year> 1996. </year> <month> 16 </month>
Reference-contexts: Given this design focus, MPI is expected to always be faster than PVM on MPP hosts. Even so, two recent comparison studies show that PVM and MPI have very comparable performance on the Cray T3D and IBM SP-2 <ref> [10, 5] </ref>.
References-found: 10

