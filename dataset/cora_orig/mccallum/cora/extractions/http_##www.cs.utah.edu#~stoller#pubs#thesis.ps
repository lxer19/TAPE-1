URL: http://www.cs.utah.edu/~stoller/pubs/thesis.ps
Refering-URL: http://www.cs.utah.edu/~stoller/pubs/pubs.html
Root-URL: 
Title: AN IMPLEMENTATION OF THE SCHIZOPHRENIC WORKSTATION  
Author: by Leigh B. Stoller 
Degree: A thesis submitted to the faculty of The University of Utah in partial fulfillment of the requirements for the degree of Master of Science  
Date: December 1993  
Affiliation: Department of Computer Science The University of Utah  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Barrera, J. S. </author> <title> A Fast Mach Network IPC Implementation. </title> <booktitle> In Proceedings of the Second Usenix Mach Symposium (1991), </booktitle> <pages> pp. 1-12. </pages>
Reference-contexts: The direct support entails changes to the microkernel such that a collection of microkernels, each running on a separate processor, presents a unified 11 port name space, a shared virtual memory system providing distributed shared memory <ref> [1] </ref>, and a mechanism to specify the processor on which a new task is created. The NORMA microkernel contains additional support for determining system configuration at system startup. Location and reference transparency for virtual memory relies on the unified port name space since memory objects are referred to via ports.
Reference: [2] <author> Comer, D. E. </author> <title> Internetworking with TCP/IP Vol I: Principles, Protocols, and Architecture. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year> <note> 2nd Edition. </note>
Reference-contexts: Messages that need to be forwarded to remote nodes must be transmitted via some network device. In the current NORMA implementation, Ethernet is used to transmit Unreliable Datagram Protocol (UDP) <ref> [2] </ref> packets. When a message is to be sent to a remote node, it is converted into network format, and then copied into one or more UDP packets.
Reference: [3] <author> Cooper, E. C., and Draves, R. P. </author> <title> C Threads. </title> <type> Tech. Rep. </type> <institution> CMU-CS-88-154, Carnegie Mellon, </institution> <year> 1988. </year>
Reference-contexts: Although it is possible to achieve parallel computation, the processes doing the computation are "heavyweight," incurring considerable CPU overhead to switch contexts. Mach, on the other hand, splits the process abstraction into two components: the task and the thread <ref> [3] </ref>. A Mach task is simply an address space and other system resources such as port rights, but it does not contain a flow of control. A Mach thread provides the flow of control.
Reference: [4] <author> Forin, A., Barrera, J., and Sanzi, R. </author> <title> The Shared Memory Server. </title> <booktitle> In USENIX Conference Proceedings (January 1989), </booktitle> <pages> pp. 229-243. </pages>
Reference-contexts: A reference to an address whose physical page is not resident on the local node is resolved via the in-kernel memory manager, resulting in a network transfer of the appropriate page. This approach is similar to earlier attempts which employed user-level external memory managers to resolve page faults <ref> [4] </ref>. A more in-depth treatment of the NORMA microkernel is given in Section 5.1. 3.3 Operating System Personality As mentioned above, the Mach microkernel provides only basic operating system support. <p> These tasks can either be on the same node, or they can be on different nodes by employing the network memory server <ref> [4] </ref> to handle the sharing. The primary drawback to using an external memory manager to share memory between nodes is performance. The server is a user process, which results in more context switches than a macrokernel implementation.
Reference: [5] <author> Gao, C., Liu, J., and Railey, M. </author> <title> Load Balancing Algorithms in Homogeneous Distributed Systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing (1984), </booktitle> <pages> pp. 302-306. </pages>
Reference-contexts: Whenever a process needs to be scheduled, a multicast query is sent over the network. Nodes that consider themselves candidates for accepting a new process respond with load information. A best choice selection is then made based on the information returned. Encompassing load sharing systems are load balancing systems <ref> [18, 20, 5] </ref>, which consider the system as a whole, attempting to remove imbalances in the system either by transferring new tasks as they arrive, or by migrating already running tasks. <p> As a result, these systems are often more complex and expensive to implement. As an example, consider the "Balancing Unfinished Work" algorithm presented in <ref> [5] </ref>. In this algorithm each node periodically updates all other nodes with load information. Using this information, the algorithm seeks to minimize the amount of unfinished work on each processor by sending jobs from overloaded nodes 5 to underloaded nodes.
Reference: [6] <author> Hennessy, J. L., and Patterson, D. A. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Integral to this sharing is the protection of the memory space, which prevents a process from accessing memory that is not in its address space, whether it be deliberate or a program error <ref> [6] </ref>. The virtual memory system provides the abstractions that allow processes to run in their own address space, potentially sharing data with other processes using shared memory.
Reference: [7] <author> Krueger, P., and R., C. </author> <title> The Stealth Distributed Scheduler. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems (May 1991), </booktitle> <pages> pp. 336-343. </pages>
Reference-contexts: RPC mechanisms are used to provide global scheduling information, and remote execution. Some distributed systems can also be described as either load sharing or load balancing. Load sharing systems employ local and/or global scheduling algorithms <ref> [7, 28, 22] </ref> to statically determine which workstation is the best candidate for a new task, or whether a newly arriving task should be transferred elsewhere.
Reference: [8] <author> Leffler, S., McKusick, M., Karels, M., and Quarterman, J. </author> <title> 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Designed as an operating system that could provide support tailored to multiprocessor machines, Mach is less "featurized" than traditional Unix <ref> [8] </ref> systems. Like Chorus, Mach has taken a "microkernel" approach, in which the core system provides only essential services, thus reducing its size and complexity. These basic services include virtual memory, process creation and scheduling, interprocess communication, and access to I/O devices.
Reference: [9] <author> Li, K., and Hudak, P. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems (November 1989), </journal> <pages> 321-359. </pages>
Reference-contexts: When the process restarts on another node, it faults in pages from the same paging file, which is available via Sprite's network filesystem. Yet another approach is the one taken by Clouds and the NORMA version of Mach. These systems provide a distributed shared memory <ref> [9] </ref> implementation that allows memory to span multiple nodes. Migration involves transferring process state to the destination node and then restarting; the memory is already available as a result of the DSM system. As the process continues to run, its pages are transferred lazily to the destination node. <p> The current version uses a strict consistency model; memory is always kept consistent between nodes by using a single-writer, multiple-reader protocol <ref> [9] </ref>. The implementation is an extension of the external pager mechanism discussed earlier; memory objects are implemented by pagers, which are referred to by ports. With global IPC, the pager backing an object can now reside on any node in the cluster.
Reference: [10] <author> Litzkow, M., Livny, M., and Mutka, M. </author> <title> Experience With the Condor Distributed Batch System. </title> <booktitle> In Proceedings of the IEEE Workshop on Experimental Distributed Systems (October 1990). </booktitle>
Reference-contexts: A different class of distributed system, known as network operating systems, seeks to combine networks of independent workstations by providing transparent access to remote resources, such as files and I/O devices. Locus [23], Utopia [28] and Condor <ref> [10] </ref> are examples of network operating systems that use remote access mechanisms to make nonlocal resources appear local. In Locus, remote files are operated upon by converting system calls into kernel to kernel RPC. <p> Another common aspect of distributed operating systems is whether the individual workstations retain their identity. Instead of being part of a processor pool, as in Amoeba [12], workstations in systems such as Sprite [15], Condor <ref> [10] </ref> and Butler [13] are both owned by someone and are participating in a load sharing system. Owners of those workstations expect them to be reliable, and to provide all the computing power they have come to expect, when they want it.
Reference: [11] <author> Milojicic, D., Zint, W., Dangel, A., and Giese, P. </author> <title> Task Migration on Top of the Mach Microkernel. </title> <booktitle> In Proceedings of the Third Usenix Mach Symposium (April 1993). </booktitle>
Reference-contexts: A new kernel to kernel RPC has been added that indicates which node is about to leave the cluster. This prevents applications, such as the Schizo server, from trying to contact the node after it has gone down. 5.2.3 Task Migration The task migration facility <ref> [11] </ref> used by Schizo was implemented as part of an independent research project at the University of Kaiserslautern. This section is intended to provide context for the discussion of task migration in the following chapter. <p> This ensures that processes are migrated from one remote node to another before ones on the core node are migrated. This prevents the second form of ping-ponging described above. 6.2.3 Migration Module The migration module was implemented by Milojicic <ref> [11] </ref>, and subsequently modified for Schizo to do eager migration in addition to lazy migration. 2 Other than the small modifications made to the microkernel (see Section 5.2.3), the migration module is implemented entirely at user level.
Reference: [12] <author> Mullender, S., Rossum, G., Tanenbaum, A., Renesse, R., and Staveren, H. v. </author> <title> Amoeba A Distributed Operating System for the 1990s. </title> <journal> IEEE Computer Magazine (May 1990), </journal> <pages> 44-52. 61 </pages>
Reference-contexts: Another common aspect of distributed operating systems is whether the individual workstations retain their identity. Instead of being part of a processor pool, as in Amoeba <ref> [12] </ref>, workstations in systems such as Sprite [15], Condor [10] and Butler [13] are both owned by someone and are participating in a load sharing system.
Reference: [13] <author> Nichols, D. A. </author> <title> Using Idle Workstations in a Shared Computing Environment. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems principles (November 1987), </booktitle> <pages> pp. 5-12. </pages>
Reference-contexts: Another common aspect of distributed operating systems is whether the individual workstations retain their identity. Instead of being part of a processor pool, as in Amoeba [12], workstations in systems such as Sprite [15], Condor [10] and Butler <ref> [13] </ref> are both owned by someone and are participating in a load sharing system. Owners of those workstations expect them to be reliable, and to provide all the computing power they have come to expect, when they want it.
Reference: [14] <institution> Open Software Foundation and Carnegie Mellon University. MACH 3 Kernel Interface. </institution> <address> Cambridge MA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Typically, the operating system personality is granted complete access to all devices, which can in turn pass that capability on to "trusted" programs. 3.2 NORMA Version The NORMA version of the microkernel is a separate version of Mach under development by the Open Software Foundation <ref> [14] </ref>, and is so called because it contains direct support for a single system image on NO Remote Memory Access multicomputers.
Reference: [15] <author> Osterhout, J., Cherenson, A. R., Douglis, F., Nelson, M., and Welch, B. B. </author> <title> The Sprite Network Operating System. </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: Another common aspect of distributed operating systems is whether the individual workstations retain their identity. Instead of being part of a processor pool, as in Amoeba [12], workstations in systems such as Sprite <ref> [15] </ref>, Condor [10] and Butler [13] are both owned by someone and are participating in a load sharing system. Owners of those workstations expect them to be reliable, and to provide all the computing power they have come to expect, when they want it.
Reference: [16] <author> Rashid, R. F. </author> <title> Threads of a New System. (Mach, a Multiprocessor Operating System). UNIX Review 4, </title> <month> 8 (August </month> <year> 1986), </year> <pages> 37-49. </pages>
Reference-contexts: An object can be reference transparent without being location transparent. The port abstraction of Chorus [17] and Mach <ref> [16] </ref> is an example of how transparency can be provided. An application simply sends a message to a port; the operating system is responsible for finding the receiver and delivering the message. <p> Although the Schizo design does not rely on any particular operating system personality, Schizo does use the OSF/1 Single Server, which is described below. 3.1 Microkernel Technology Mach <ref> [16] </ref> is a multiprocessor operating system that was originally developed at Carnegie Mellon University. Designed as an operating system that could provide support tailored to multiprocessor machines, Mach is less "featurized" than traditional Unix [8] systems.
Reference: [17] <author> Rozier, M., Abrossimov, V., Armand, F., Boule, I., Gien, M., Guillemont, M., Herrmann, F., Kaiser, C., Langlois, S., Leonard, P., and Neuhauser, W. </author> <title> Overview of the CHORUS Distributed Operating Systems. </title> <type> Tech Report 90-25, </type> <institution> Chorus Systemes, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Location transparency allows a program to function without knowing whether an object is local or remote, while reference transparency allows a program to access local and remote objects in the same way. An object can be reference transparent without being location transparent. The port abstraction of Chorus <ref> [17] </ref> and Mach [16] is an example of how transparency can be provided. An application simply sends a message to a port; the operating system is responsible for finding the receiver and delivering the message.
Reference: [18] <author> Schaar, M., Efe, K., Delcambre, L., and Bhuyan, L. N. </author> <title> Load Balancing with Network Cooperation. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems (May 1991), IEEE. </booktitle>
Reference-contexts: Whenever a process needs to be scheduled, a multicast query is sent over the network. Nodes that consider themselves candidates for accepting a new process respond with load information. A best choice selection is then made based on the information returned. Encompassing load sharing systems are load balancing systems <ref> [18, 20, 5] </ref>, which consider the system as a whole, attempting to remove imbalances in the system either by transferring new tasks as they arrive, or by migrating already running tasks.
Reference: [19] <author> Shapiro, M. </author> <title> A Fault-Tolerant, Scalable, Low-Overhead Distributed Garbage Detection Protocol. </title> <booktitle> In IEEE 10th Symposium on Reliable Distributed Systems (October 1991). </booktitle>
Reference-contexts: Send-once rights are slightly different in that using the send-once right destroys it, but reference count management is still simple to implement. Keeping track of this information in a distributed system is a complex problem. Further, no-senders notification requires that a garbage detection algorithm <ref> [19] </ref> be used, rather than automatic garbage collection. The semantics of no-senders is that the holder of the receive right is notified as soon as the count reaches zero. Therefore, the NORMA microkernel uses distributed reference counting to implement garbage detection.
Reference: [20] <author> Shivaratri, N., and Krueger, P. </author> <title> Two Adaptive Location Policies for Global Scheduling Algorithms. </title> <booktitle> In Proceedings of the 10th International Conference on Distributed Computing Systems (May-June 1990), </booktitle> <pages> pp. 502-509. </pages>
Reference-contexts: Whenever a process needs to be scheduled, a multicast query is sent over the network. Nodes that consider themselves candidates for accepting a new process respond with load information. A best choice selection is then made based on the information returned. Encompassing load sharing systems are load balancing systems <ref> [18, 20, 5] </ref>, which consider the system as a whole, attempting to remove imbalances in the system either by transferring new tasks as they arrive, or by migrating already running tasks.
Reference: [21] <author> Swanson, M., Stoller, L., Critchlow, T., and Kessler, R. </author> <title> The Design of the Schizophrenic Workstation System. </title> <booktitle> In Proceedings of the Third Usenix Mach Symposium (April 1993). </booktitle>
Reference-contexts: At other times, the workstation owner needs all of the power his or her workstation can provide, and possibly far more than it can provide. The Schizophrenic Workstation System <ref> [21] </ref> is a cycle harvesting distributed operating system that utilizes spare workstation resources to shift load from overburdened workstations to idle or underutilized ones. <p> As the results show, the degradation of the foreground task lessens. This indicates that depressing CPU priority helps in preventing Schizo processes from having an adverse effect on local tasks. However, using depressed CPU priority alone can cause page thrashing. Experiments conducted in <ref> [21] </ref> show that depressing virtual memory priority of Schizo tasks further reduces the amount of degradation in the foreground task.
Reference: [22] <author> Theimer, M., and Lantz, K. </author> <title> Finding Idle Machines in a Workstation-based Distributed System. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computing Systems (1988), </booktitle> <pages> pp. 112-122. </pages>
Reference-contexts: RPC mechanisms are used to provide global scheduling information, and remote execution. Some distributed systems can also be described as either load sharing or load balancing. Load sharing systems employ local and/or global scheduling algorithms <ref> [7, 28, 22] </ref> to statically determine which workstation is the best candidate for a new task, or whether a newly arriving task should be transferred elsewhere.
Reference: [23] <author> Walker, B., Popek, G., English, R., Kline, C., and Thiel, G. </author> <title> The LOCUS Distributed Operating System. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating Systems Principles (October 1983), </booktitle> <pages> pp. 49-70. </pages>
Reference-contexts: A different class of distributed system, known as network operating systems, seeks to combine networks of independent workstations by providing transparent access to remote resources, such as files and I/O devices. Locus <ref> [23] </ref>, Utopia [28] and Condor [10] are examples of network operating systems that use remote access mechanisms to make nonlocal resources appear local. In Locus, remote files are operated upon by converting system calls into kernel to kernel RPC.
Reference: [24] <author> Wilkenloh, C. J., and Ramachandran, U. </author> <title> The Clouds Experience: Building an Object-Based Distributed Operating System. </title> <booktitle> In USENIX, Distributed and Multiprocessor Workshop (October 1989), </booktitle> <pages> pp. 333-347. 62 </pages>
Reference-contexts: The port abstraction of Chorus [17] and Mach [16] is an example of how transparency can be provided. An application simply sends a message to a port; the operating system is responsible for finding the receiver and delivering the message. Another example is the RPC mechanism of Clouds <ref> [24] </ref>, which treats all services as objects to which invocations can either be local or remote.
Reference: [25] <author> Yih, B., Swanson, M., and Kessler, R. </author> <title> Persistent Immutable Shared Abstractions. </title> <note> In Second Workshop on Parallel Symbolic Computing (October 1992). to appear in Springer-Verlag Lecture Notes in Computer Science. </note>
Reference-contexts: More generally, any program performing large amounts of reading and writing and making frequent use of signals is an inappropriate application. The next experiment was performed using a ray tracer implemented in Scheme <ref> [25] </ref>. The ray tracer is a long-running, compute intensive, single-threaded program. Once it completes an initialization phase, it computes intensively; the length of the computation is easily adjustable by specifying the desired number of pixels.
Reference: [26] <author> Young, M. W. </author> <title> Exporting a User Interface to Memory Management from a Communication-Oriented Operating System. </title> <type> Tech. Rep. </type> <institution> CMU-CS-89-202, Carnegie Mellon University, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: The Mach microkernel provides a virtual memory implementation that addresses all of these issues. The key abstraction of the Mach virtual memory system is the memory object <ref> [26] </ref>, which like other Mach abstractions, is represented by a port. A memory object represents a region of memory that can be mapped by a task. The backing store for the region is provided by a memory manager, which is said to implement the memory object.
Reference: [27] <author> Zajcew, R., Roy, P., Black, D., Peak, C., Guedes, P., Kemp, B., LoVerso, J., Leibensperger, M., Barnett, M., Rabii, F., and Netterwala, D. </author> <title> An OSF/1 UNIX for Massively Parallel Multicomputers. </title> <booktitle> In USENIX Conference Proceedings (January 1993), </booktitle> <pages> pp. 449-468. </pages>
Reference-contexts: In addition to the standard OSF/1 server, there exists a new version under development by the Open Software Foundation and Locus Computing Corporation. This new version, called OSF/1 AD TNC <ref> [27] </ref>, is designed specifically for NORMA multicomputers. Its primary goal is to avoid bottlenecks by using a distributed file system and a distributed process management system that allows process management system calls to be handled on the same node as the process.
Reference: [28] <author> Zhou, S., Wang, J., Zheng, X., and Delisle, P. </author> <title> UTOPIA: A Load Sharing Facility for Large Heterogeneous Distributed Compter Systems. </title> <type> Tech. Rep. </type> <institution> CSRI-257, Computer Systems Research Institute, University of Toronto, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: A different class of distributed system, known as network operating systems, seeks to combine networks of independent workstations by providing transparent access to remote resources, such as files and I/O devices. Locus [23], Utopia <ref> [28] </ref> and Condor [10] are examples of network operating systems that use remote access mechanisms to make nonlocal resources appear local. In Locus, remote files are operated upon by converting system calls into kernel to kernel RPC. <p> RPC mechanisms are used to provide global scheduling information, and remote execution. Some distributed systems can also be described as either load sharing or load balancing. Load sharing systems employ local and/or global scheduling algorithms <ref> [7, 28, 22] </ref> to statically determine which workstation is the best candidate for a new task, or whether a newly arriving task should be transferred elsewhere.
References-found: 28

