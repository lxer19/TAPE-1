URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-95-32.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/biblio_ora?sort_by_author=yes&tailor=1&loc=0&format=ml/ml&keyword=Publications&keyword=WWW_ML&relop=/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: stefan@ai.univie.ac.at  
Title: Predicate Invention: A Comprehensive View 1  
Author: Stefan Kramer 
Keyword: Predicate Invention, Constructive Induction, Inductive Learning, Inductive Logic Programming  
Address: Schottengasse 3, A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: This paper discusses predicate invention (PI) from various, previously neglected viewpoints. First of all, we argue that predicate invention should build on existing work on constructive induction in propositional learning. We recall the major reasons for constructive induction in propositional languages, and give a brief overview of the frameworks for constructive induction by Matheus, Wnek and Michalski. We then apply these frameworks to predicate invention in order to categorize systems and to identify relevant aspects of PI. The discussion demonstrates that some relevant aspects are treated only implicitly, and some are largely neglected in many systems. Secondly, we review current criticism against constructive induction that also concerns predicate invention. In particular, we agree with Sutton's demand that constructive induction should be based on continuing learning, i.e. it should reuse representational "tricks" in a series of learning tasks. Thirdly, we discuss the advantages and disadvantages of fully-automatic vs. interactive predicate invention. The question is how to create meaningful new predicates. Since comprehensibility and syntactical complexity are not necessarily the same, human intervention may be required if humans shall make sense of the resulting theory. We try to attract more attention to important aspects that have not yet been recognized clearly, and still are present in the work of many authors. These aspects are illustrated by existing PI systems. 
Abstract-found: 1
Intro-found: 1
Reference: [Banerji 92] <author> Banerji R.B.: </author> <title> Learning Theoretical Terms, in Muggleton S.(ed.), Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> London, U.K., pp.93-112, </address> <year> 1992. </year>
Reference-contexts: Conj. 2 Lits. Disj. Rec. Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji <ref> [Banerji 92] </ref> fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi
Reference: [De Raedt & Bruynooghe 89] <author> De Raedt L., Bruynooghe M.: </author> <title> Constructive Induction by Analogy, </title> <booktitle> in Segre A.M.(ed.), Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, pp.476-477, </address> <year> 1989. </year>
Reference: [De Raedt 91] <author> De Raedt L.: </author> <title> Interactive Concept-Learning, </title> <type> Ph.D. Thesis, </type> <institution> Katholieke Univer-siteit Leuven, </institution> <year> 1991. </year>
Reference-contexts: Thus, constructive induction by clause refinement is also multi-strategy constructive induction (MCI). Schema-driven methods have to be discussed separately. CILP is the only system that performs data-driven constructive induction (DCI). In CLINT-CIA <ref> [De Raedt 91] </ref> the instantiation of a schema is data-driven, and the subsequent suggestion of the new predicate is accepted or rejected by a human expert. Since it is based on two sources of information, CLINT-CIA is a method for multi-strategy constructive induction (MCI), too.
Reference: [De Raedt & Bruynooghe 92] <author> De Raedt L., Bruynooghe M.: </author> <title> Interactive Concept-Learning and Constructive Induction by Analogy, </title> <journal> Machine Learning, </journal> <volume> 8(2), </volume> <year> 1992. </year>
Reference-contexts: Conj. 2 Lits. Disj. Rec. Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA <ref> [De Raedt & Bruynooghe 92] </ref> fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi <p> This and other restrictions determine which of the potential new predicates are included in the vocabulary. Schema-driven methods have to include selection, if several second-order schemata apply. In CLINT <ref> [De Raedt & Bruynooghe 92] </ref> the user is responsible for selection. In FOCL [Silverstein & Pazzani 93], selection takes place in two steps: First, the selection of candidates for cliches is based on the occurrence in an inductive hypothesis and on the information gain measure [Quinlan 90]. <p> There are two kinds of systems for predicate invention: 1. Systems like GOLEM [Muggleton 94] perform constructive induction without human intervention. Constructive induction takes over in order to improve the best "human" representation. 2. Systems like CLINT <ref> [De Raedt & Bruynooghe 92] </ref> and CIGOL [Muggleton & Buntine 88] perform interactive constructive induction, a kind of constructive induction that is not performed fully automatically. Instead, a constructive induction component proposes the application of constructive induction operators to an oracle.
Reference: [Flach 93] <author> Flach P.A.: </author> <title> Predicate Invention in Inductive Data Engineering, in Brazdil P.B.(ed.), Machine Learning: </title> <publisher> ECML-93, Springer, </publisher> <address> Berlin, pp.83-94, </address> <year> 1993. </year>
Reference-contexts: GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi fi fi SIERES [Wirth & O'Rorke 92] fi fi fi INDEX <ref> [Flach 93] </ref> fi extensional, Conj.-conjunctive, 2 Lits.-a conjunction of two literals, Disj.- disjunctive, Rec.-recursive, Deps.-dependencies among arguments.) In INDEX [Flach 93], new predicates are either functional respectively multi-valued dependencies or represent partitionings of tuples in a relation. * Partial Determination: Partial determinations are like functional depen dencies, but allow for exceptions. <p> fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi fi fi SIERES [Wirth & O'Rorke 92] fi fi fi INDEX <ref> [Flach 93] </ref> fi extensional, Conj.-conjunctive, 2 Lits.-a conjunction of two literals, Disj.- disjunctive, Rec.-recursive, Deps.-dependencies among arguments.) In INDEX [Flach 93], new predicates are either functional respectively multi-valued dependencies or represent partitionings of tuples in a relation. * Partial Determination: Partial determinations are like functional depen dencies, but allow for exceptions. <p> Summing up, we believe that compression is a major motivation for PI. Nevertheless, the goal of the overall system strongly affects the use of PI. For instance, the goal of interactive theory revision affects the way of PI in MOBAL [Wrobel 94]. Likewise, the goal of inductive data engineering <ref> [Flach 93] </ref> needs a different strategy for the invention of new predicates than employed by other systems. 5 Applying Constructive Induction Frame works to Predicate Invention In this section we will first show how predicate invention can be discussed within Matheus' framework for constructive induction: In the first subsections we review <p> These predicates are quite ad hoc, and they are not likely to capture significant and meaningful patterns in the data. INDEX <ref> [Flach 93] </ref> is not really comparable to other systems, since its goal is restructuring a relational database, and not concept learning. So there is no selective induction algorithm, and no detection if predicate invention is needed. <p> Basically, there are two possible directions of further research: First, the interactive approach could be further developed, aiming at the support of data engineering for inductive learning (which is explorative in nature) and for inductive data engineering <ref> [Flach 93] </ref>. For this purpose it would be promising to integrate compression-based and second-order learning approaches. The second challenge is to build autonomous machine discoverers, which are capable of creating and transforming intermediate concepts and theoretical terms according to their needs. These discoverers would act in a continuing learning situation.
Reference: [Franova & Kodratoff 92] <author> Franova M., Kodratoff Y.: </author> <title> Predicate Synthesis from Formal Specifications: Using Mathematical Induction for Finding the Preconditions of Theorems, </title> <institution> Rapport de Recherche No.781, L.R.I., Univ. de Paris-Sud, </institution> <year> 1992. </year>
Reference-contexts: Furthermore, the so-called transformation approaches (creating predicates from formal specifications) to PI (e.g. <ref> [Franova & Kodratoff 92, Le Blanc 94] </ref>) are not discussed, as they are only partially relevant in the inductive learning setting.
Reference: [Hamfelt & Nilsson 94] <author> Hamfelt A., Nilsson J.F.: </author> <title> Inductive Metalogic Programming, </title> <booktitle> in Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. 237, </booktitle> <address> pp.85-96, </address> <year> 1994. </year>
Reference-contexts: Several systems are able to utilize schemata (possibly acquired in another domain) for learning. In fact, using such schemata or rule models is the basic principle of MOBAL. Moreover, work on second-order learning (e.g. <ref> [Hamfelt & Nilsson 94] </ref>) and on schema-driven approaches to predicate invention deals with the problem of "retrieving" schemata. ILP methods are apparently well suited for this task, since a lot of structure is available for learning useful patterns in the representation.
Reference: [Holte 93] <author> Holte R.C.: </author> <title> Very Simple Classification Rules Perform Well on Most Commonly Used Datasets, </title> <journal> Machine Learning, </journal> <volume> 11(1), </volume> <year> 1993. </year>
Reference-contexts: It is a well-known fact that the results of machine learning algorithms strongly depend on how well the domain is understood. Whereas easy concepts can be learned quite well even by simple learners <ref> [Holte 93] </ref>, Rendell claims that hard concepts [Rendell & Seshu 90] mostly cannot even be learned by sophisticated learners. <p> In domains with incomplete information, we cannot hope to get beyond the accuracy of very simple benchmark algorithms such as Holte's <ref> [Holte 93] </ref>: The independent variables may influence the value of the dependent variable, but do not "determine" it | there is simply not enough information available. However, this is not a matter of all-or-nothing: The "causal relevance" of variables rather is gradually different.
Reference: [Kietz & Wrobel 92] <author> Kietz J.-U., Wrobel S.: </author> <title> Controlling the Complexity of Learning in Logic through Syntactic and Task-Oriented Models, in Muggleton S.(ed.), Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> London, U.K., pp.335-359, </address> <year> 1992. </year>
Reference: [Kijsirikul et al. 92] <author> Kijsirikul B., Numao M., Shimura M.: </author> <title> Discrimination-Based Constructive Induction of Logic Programs, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, pp.44-49, </address> <year> 1992. </year>
Reference-contexts: [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP <ref> [Kijsirikul et al. 92] </ref> fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi fi fi SIERES [Wirth & O'Rorke 92] fi fi fi INDEX [Flach 93] fi extensional, Conj.-conjunctive, 2 Lits.-a conjunction of two literals, Disj.- disjunctive, Rec.-recursive, Deps.-dependencies among arguments.) In INDEX [Flach 93], new predicates <p> It should be noted that an extensional definition of predicates can be turned into an intensional definition by a selective induction algorithm. This step is performed by the systems MOBAL [Morik 93, Wrobel 94], CHAMP <ref> [Kijsirikul et al. 92] </ref> and CWS [Srinivasan et al. 92]. Learning an intensional def inition is in fact generalization in the sense of Matheus. The definitions could further be generalized by dropping a condition of the definition. few systems are capable of inventing predicates of several types. <p> For pragmatic reasons, systems have to decide heuristically if the existing vocabulary is insufficient. Reformulation-based approaches perform a kind of look-ahead, searching for new predicates that make the theory more compact. Demand-driven systems like CWS [Srinivasan et al. 92] and CHAMP-DBC <ref> [Kijsirikul et al. 92] </ref> detect the need for predicate invention, if the top-down algorithm fails to discriminate the tuples covered by the clause. The over-general clauses are handed over to the algorithm for predicate invention. <p> If they were due to chance, inventing a new predicate would overfit the noise. As in feature construction, we have to control the complexity of the theory and the newly defined predicates. Correcting a clause in this way is often used together with top-down algorithms <ref> [Srinivasan et al. 92, Kijsirikul et al. 92] </ref>, but does not presuppose their use. For instance, the predicate invention component of GOLEM [Muggleton 94] works in a similar way, correcting bottom-up generated clauses. <p> The first task is to select the set of arguments for the new predicate. [Srinivasan et al. 92] select the same arguments as in the head of the over-general clause. CHAMP <ref> [Kijsirikul et al. 92] </ref> starts with all variables and drops variables as long as there are enough to discriminate the positive and the negative instances. As Stahl [Stahl 94] points out, searching for minimum sets of discriminating arguments has a subjective component, since there might be several such sets. <p> Kramer 95] could be more successfully applied to the problem of finding good sets of arguments in the presence of noise than rough set theory. 5.3 Generalizing the Definition of a New Predicate Only a few systems perform a generalization step after the initial definition of a new predicate: CHAMP <ref> [Kijsirikul et al. 92] </ref>, CWS [Srinivasan et al. 92] and MOBAL [Wrobel 94]. An extensional definition can easily be generalized by applying a relational learning algorithm to the tuples of the definition. CIGOL only generalizes its definitions if this step helps to compress the knowl 14 edge base.
Reference: [Kramer 94] <author> Kramer S.: CN2-MCI: </author> <title> A Two-Step Method for Constructive Induction, </title> <booktitle> Proceedings of the Workshop on Constructive Induction and Change of Representation, 11th International Conference on Machine Learning (ML-94/COLT-94), </booktitle> <address> New Brunswick, New Jersey, </address> <year> 1994. </year>
Reference-contexts: For instance, detection may be based on the initial set of training instances or on the analysis of a concept description. Another approach to detection is taken by CN2-MCI <ref> [Kramer 94] </ref>: The system basically performs a kind of look-ahead: it constructs new features and evaluates the results. If the quality of the hypothesis induced in the transformed representation is worse than the original hypothesis, CN2-MCI concludes there is no need for a representation change.
Reference: [Lapointe et al. 93] <author> Lapointe S., Ling C., Matwin S.: </author> <title> Constructive Inductive Logic Programming, in Bajcsy R.(ed.), </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1030-1036, </address> <year> 1993. </year>
Reference-contexts: Conjunction of n Literals: For instance, RINCON's [Wogulis & Langley 89] intermediate con cepts consist of a deliberate number of literals. 3. Disjunctive Definition: Systems using the intraconstruction operator [Muggleton & Buntine 88, Rouveirol 92, Wirth 89] construct predicates with disjunctive def initions. * Recursive Predicate: Several systems <ref> [Ling 91, Lapointe et al. 93, Ling 95] </ref> are designed to invent recursive new predicates. Ling states that recursive predicates are the only ones that are really necessary, because they cannot be eliminated without failing to learn certain concepts. <p> Conj. 2 Lits. Disj. Rec. Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP <ref> [Lapointe et al. 93] </ref> fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle
Reference: [Le Blanc 94] <author> Le Blanc G.: </author> <title> BMWk Revisited Generalization and Formalization of an Algorithm for Detecting Recursive Relations in Term Sequences, </title> <editor> in Bergadano F. & Raedt L.de(eds.), </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-94, </address> <publisher> Springer, </publisher> <address> Berlin, pp.183-197, </address> <year> 1994. </year>
Reference-contexts: Furthermore, the so-called transformation approaches (creating predicates from formal specifications) to PI (e.g. <ref> [Franova & Kodratoff 92, Le Blanc 94] </ref>) are not discussed, as they are only partially relevant in the inductive learning setting. <p> The over-general clauses are handed over to the algorithm for predicate invention. Note that the component for predicate invention can be used without other parts of the system. Among others, this scenario also highlights the relationship between predicate invention and theory revision <ref> [Le Blanc 94] </ref>: A theory with exceptions can be corrected by adding literals with new predicates to the over-general clauses in order to separate those instances that are incorrectly covered from those that are correctly covered.
Reference: [Ling 91] <author> Ling C.: </author> <title> Inventing Necessary Theoretical Terms in Scientific Discovery and Inductive Logic Programming, </title> <type> Report No. 302, </type> <institution> Dept. of Computer Science, University of Western Ontario, </institution> <address> London, Ontario, </address> <year> 1991. </year>
Reference-contexts: If the vocabulary is limited for a learning task at hand, predicate invention can be a means to successfully learn a concept that otherwise could not be learned. Predicates that make it possible to learn a given concept are called necessary <ref> [Ling 91] </ref>. Predicates that are not crucial but help to compress a theory are called useful. 4.2 Reformulation vs. Demand-Driven Approaches In her overview article on predicate invention [Stahl 93], Stahl distinguishes between reformulation approaches and demand-driven approaches to predicate invention. <p> Conjunction of n Literals: For instance, RINCON's [Wogulis & Langley 89] intermediate con cepts consist of a deliberate number of literals. 3. Disjunctive Definition: Systems using the intraconstruction operator [Muggleton & Buntine 88, Rouveirol 92, Wirth 89] construct predicates with disjunctive def initions. * Recursive Predicate: Several systems <ref> [Ling 91, Lapointe et al. 93, Ling 95] </ref> are designed to invent recursive new predicates. Ling states that recursive predicates are the only ones that are really necessary, because they cannot be eliminated without failing to learn certain concepts. <p> CHAMP and 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU [Rouveirol 92]), LFP [Wirth 88] (the predecessor of LFP2 [Wirth 89]), MENDEL <ref> [Ling 91] </ref> (the predecessor of INPP [Ling 95]), BLIP [Morik 89, Wrobel 89] (the predecessor of MOBAL [Morik 93, Wrobel 94]).
Reference: [Ling 95] <author> Ling C.: </author> <title> Introducing New Predicates to Model Scientific Revolution, to appear in: </title> <booktitle> International Studies in the Philosophy of Science, </booktitle> <volume> 9(2), </volume> <year> 1995. </year> <month> 23 </month>
Reference-contexts: Conjunction of n Literals: For instance, RINCON's [Wogulis & Langley 89] intermediate con cepts consist of a deliberate number of literals. 3. Disjunctive Definition: Systems using the intraconstruction operator [Muggleton & Buntine 88, Rouveirol 92, Wirth 89] construct predicates with disjunctive def initions. * Recursive Predicate: Several systems <ref> [Ling 91, Lapointe et al. 93, Ling 95] </ref> are designed to invent recursive new predicates. Ling states that recursive predicates are the only ones that are really necessary, because they cannot be eliminated without failing to learn certain concepts. <p> Conj. 2 Lits. Disj. Rec. Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP <ref> [Ling 95] </ref> fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi <p> CHAMP and 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU [Rouveirol 92]), LFP [Wirth 88] (the predecessor of LFP2 [Wirth 89]), MENDEL [Ling 91] (the predecessor of INPP <ref> [Ling 95] </ref>), BLIP [Morik 89, Wrobel 89] (the predecessor of MOBAL [Morik 93, Wrobel 94]). Furthermore, the so-called transformation approaches (creating predicates from formal specifications) to PI (e.g. [Franova & Kodratoff 92, Le Blanc 94]) are not discussed, as they are only partially relevant in the inductive learning setting.
Reference: [Ling & Narayan 91] <author> Ling C., Narayan M.A.: </author> <title> A Critical Comparison of Various Methods Based on Inverse Resolution, </title> <editor> in Birnbaum L.A. & Collins G.C.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.168-172, </address> <year> 1991. </year>
Reference: [Matheus & Rendell 89] <author> Matheus C.J., Rendell L.A.: </author> <title> Constructive Induction On Decision Trees, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, 645-650, </address> <year> 1989. </year>
Reference-contexts: In this report we try to apply the frameworks for constructive induction by Matheus <ref> [Matheus & Rendell 89, Matheus 91] </ref> and Wnek and Michal-ski [Wnek & Michalski 94] to predicate invention. In the next section, we recall the major reasons for constructive induction in propositional languages. In the third section we review the frameworks for constructive induction by Matheus, Wnek and Michalski. <p> Constructing binary features that represent relations among features makes feature interaction visible even to algorithms which greedily select features. 3 Constructive Induction Frameworks According to Matheus, there are four major questions that a system must answer when performing constructive induction <ref> [Matheus & Rendell 89, Matheus 91] </ref>: 1. When should new features/predicates be constructed ? (Detection) Detection is necessary, because the construction of irrelevant features/predicates usually affects learning like noise in the data [Wnek & Michalski 94].
Reference: [Matheus 91] <author> Matheus C.J.: </author> <title> The Need for Constructive Induction, </title> <editor> in Birnbaum L.A. & Collins G.C.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.173-177, </address> <year> 1991. </year>
Reference-contexts: In this report we try to apply the frameworks for constructive induction by Matheus <ref> [Matheus & Rendell 89, Matheus 91] </ref> and Wnek and Michal-ski [Wnek & Michalski 94] to predicate invention. In the next section, we recall the major reasons for constructive induction in propositional languages. In the third section we review the frameworks for constructive induction by Matheus, Wnek and Michalski. <p> More precisely, there are relations and dependencies between original features hidden in the data that may not be visible to the learner. 2.2 Algorithmic Bias The other major reason for constructive induction is the algorithmic bias of learning algorithms. For instance, the greedy selection <ref> [Matheus 91] </ref> of features is a kind of bias that is problematic when applied to datasets with high feature interaction. Experiments showed that very complex hypotheses are generated [Pagallo & Haussler 90], since a large number of conditions are required to express interrelationships among features in the dataset. <p> Constructing binary features that represent relations among features makes feature interaction visible even to algorithms which greedily select features. 3 Constructive Induction Frameworks According to Matheus, there are four major questions that a system must answer when performing constructive induction <ref> [Matheus & Rendell 89, Matheus 91] </ref>: 1. When should new features/predicates be constructed ? (Detection) Detection is necessary, because the construction of irrelevant features/predicates usually affects learning like noise in the data [Wnek & Michalski 94].
Reference: [Michalski 83] <author> Michalski R.S.: </author> <title> A Theory and Methodology of Inductive Learning, </title> <editor> in Michalski R.S., et al.(eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, CA, pp.83-134, </address> <year> 1983. </year>
Reference-contexts: Early work focussed on the use of constructive operators during induction <ref> [Michalski 83] </ref>. These so-called "constructive generalization rules...generate inductive assertions that use descriptors not present in the original observational statements". Later, researchers investigated feature construction, i.e. the construction of new features from existing features.
Reference: [Michalski 93] <author> Michalski R.S.: </author> <title> Inferential Theory of Learning as a Conceptual Basis for Multi-strategy Learning, </title> <journal> in Special Issue on Multistrategy Learning, Machine Learning, </journal> <volume> 11(2/3), </volume> <year> 1993. </year>
Reference-contexts: 1 Introduction <ref> [Michalski 93] </ref> distinguishes between selective induction and constructive induc tion as follows: "Empirical learning uses little domain knowledge, while constructive induction uses more domain knowledge.
Reference: [Mitchell 80] <author> Mitchell T.M.: </author> <title> The Need for Biases in Learning Generalizations, </title> <editor> in Shavlik J.W., Dietterich T.G.(eds.), </editor> <booktitle> Readings in Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Basically, a concept is hard if the 2 The term "bias" refers to any basis for excluding hypotheses from the search space other than strict completeness and consistency with the examples <ref> [Mitchell 80] </ref>. An important part of the bias is the hypothesis language: it restricts the range of expressible concepts. The language bias is given by the vocabulary (the available predicate, function and constant symbols) and the syntactic form of the potential target theories.
Reference: [Morik 89] <editor> Morik K.: </editor> <booktitle> Sloppy Modeling, in Morik K.(ed.), Knowledge Representation and Organization in Machine Learning, Vol. 347 of Lecture Notes in Artificial Intelligence, </booktitle> <publisher> Springer, </publisher> <address> Berlin, pp.107-134, </address> <year> 1989. </year>
Reference-contexts: CHAMP and 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU [Rouveirol 92]), LFP [Wirth 88] (the predecessor of LFP2 [Wirth 89]), MENDEL [Ling 91] (the predecessor of INPP [Ling 95]), BLIP <ref> [Morik 89, Wrobel 89] </ref> (the predecessor of MOBAL [Morik 93, Wrobel 94]). Furthermore, the so-called transformation approaches (creating predicates from formal specifications) to PI (e.g. [Franova & Kodratoff 92, Le Blanc 94]) are not discussed, as they are only partially relevant in the inductive learning setting.
Reference: [Morik 93] <author> Morik K.: </author> <title> Balanced Cooperative Modeling, </title> <journal> in Special Issue on Multistrategy Learning, Machine Learning, </journal> <volume> 11(2/3), </volume> <year> 1993. </year>
Reference-contexts: It should be noted that an extensional definition of predicates can be turned into an intensional definition by a selective induction algorithm. This step is performed by the systems MOBAL <ref> [Morik 93, Wrobel 94] </ref>, CHAMP [Kijsirikul et al. 92] and CWS [Srinivasan et al. 92]. Learning an intensional def inition is in fact generalization in the sense of Matheus. <p> 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU [Rouveirol 92]), LFP [Wirth 88] (the predecessor of LFP2 [Wirth 89]), MENDEL [Ling 91] (the predecessor of INPP [Ling 95]), BLIP [Morik 89, Wrobel 89] (the predecessor of MOBAL <ref> [Morik 93, Wrobel 94] </ref>). Furthermore, the so-called transformation approaches (creating predicates from formal specifications) to PI (e.g. [Franova & Kodratoff 92, Le Blanc 94]) are not discussed, as they are only partially relevant in the inductive learning setting. <p> However, the desired generality strongly depends on the particular context of the learning task. Thus, considering the generalization of definitions makes the learner more flexible in learning situations. 5.4 Evaluating the Set of Existing Predicates MOBAL <ref> [Morik 93] </ref> is the only system that also evaluates its newly defined theoretical vocabulary. Therefore, it is the only system that considers all four aspects of Matheus' framework.
Reference: [Muggleton & Feng 90] <author> Muggleton S., Feng C.: </author> <title> Efficient Induction of Logic Programs, </title> <booktitle> in Mug-gleton S.(ed.), Inductive Logic Programming, </booktitle> <publisher> Academic Press, </publisher> <address> London, U.K., pp.281-298, </address> <year> 1992. </year>
Reference: [Muggleton 87] <author> Muggleton S.: Duce, </author> <title> An Oracle-Based Approach to Constructive Induction, </title> <booktitle> in Proceedings of the 10th International Joint Conference on Artificial Intelligence (IJCAI-87), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, p.287-292, </address> <year> 1987. </year>
Reference-contexts: Obviously, there is a close correspondence between features in propositional languages and predicates in first-order languages. Consequently, there is a close correspondence between feature construction and predicate invention. However, there exists no systematic comparison between work done in both areas. Apart from one exception (DUCE <ref> [Muggleton 87] </ref> and CIGOL [Muggleton & Buntine 88]), work on predicate invention neither tries to extend ideas from propositional constructive induction to first-order logic, nor builds on existing work on feature construction. <p> Instead, a constructive induction component proposes the application of constructive induction operators to an oracle. The oracle rejects or accepts, and in the latter case has the opportunity to name the new feature or predicate. This procedure ensures that the new features/predicates are meaningful to the user <ref> [Muggleton 87] </ref>. The scenario for interactive constructive induction has a great potential for helping to learn hard concepts. It is exploratory in nature, and should help to gain a better understanding of poorly understood domains. <p> Furthermore, this scenario is best suited for reusing patterns of useful representations, much in the spirit of Sutton's idea of performing constructive induction in a sequence of learning tasks. The scenario was first realized by DUCE <ref> [Muggleton 87] </ref>, a system that transforms a propositional knowledge base via a number of operators. The system searches for the operation achieving the best compression. If the operation is not truth-preserving, it is proposed to an oracle, otherwise it is performed without asking.
Reference: [Muggleton 94] <author> Muggleton S.: </author> <title> Predicate Invention and Utilization, in Special Issue: Algorithmic Learning Theory, </title> <journal> JETAI Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6(1), </volume> <year> 1994. </year>
Reference-contexts: Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM <ref> [Muggleton 94] </ref> fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi fi fi SIERES [Wirth & O'Rorke 92] fi fi fi INDEX [Flach 93] <p> Correcting a clause in this way is often used together with top-down algorithms [Srinivasan et al. 92, Kijsirikul et al. 92], but does not presuppose their use. For instance, the predicate invention component of GOLEM <ref> [Muggleton 94] </ref> works in a similar way, correcting bottom-up generated clauses. CHILLIN [Zelle et al. 94] combines bottom-up and top-down techniques: First, it computes the least general generalization (lgg) of two clauses, then it tries to make the generalization consistent by specializing it. <p> In this section we want to discuss the advantages and disadvantages of autonomy in contrast to human intervention in predicate invention. There are two kinds of systems for predicate invention: 1. Systems like GOLEM <ref> [Muggleton 94] </ref> perform constructive induction without human intervention. Constructive induction takes over in order to improve the best "human" representation. 2. Systems like CLINT [De Raedt & Bruynooghe 92] and CIGOL [Muggleton & Buntine 88] perform interactive constructive induction, a kind of constructive induction that is not performed fully automatically.
Reference: [Muggleton 92] <author> Muggleton S.: </author> <title> Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> London, U.K., </address> <year> 1992. </year>
Reference-contexts: These so-called "constructive generalization rules...generate inductive assertions that use descriptors not present in the original observational statements". Later, researchers investigated feature construction, i.e. the construction of new features from existing features. In the early nineties, a new area called Inductive Logic Programming (ILP) <ref> [Muggleton 92] </ref> developed, dealing with learning theories in first-order logic. From the very beginning, ILP also was concerned with constructive induction in first-order logic, better known as predicate invention [Muggleton & Buntine 88]. Obviously, there is a close correspondence between features in propositional languages and predicates in first-order languages.
Reference: [Muggleton 92] <author> Muggleton S.: </author> <title> Inductive Logic Programming, in Muggleton S.(ed.), Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> London, U.K., </address> <year> 1992. </year>
Reference-contexts: These so-called "constructive generalization rules...generate inductive assertions that use descriptors not present in the original observational statements". Later, researchers investigated feature construction, i.e. the construction of new features from existing features. In the early nineties, a new area called Inductive Logic Programming (ILP) <ref> [Muggleton 92] </ref> developed, dealing with learning theories in first-order logic. From the very beginning, ILP also was concerned with constructive induction in first-order logic, better known as predicate invention [Muggleton & Buntine 88]. Obviously, there is a close correspondence between features in propositional languages and predicates in first-order languages.
Reference: [Muggleton & Buntine 88] <author> Muggleton S., Buntine W.: </author> <title> Machine Invention of First-Order Predicates by Inverting Resolution, in Laird J.(ed.), </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> Univ.of Michigan, Ann Arbor, June 12-14, </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.339-352, </address> <year> 1988. </year>
Reference-contexts: In the early nineties, a new area called Inductive Logic Programming (ILP) [Muggleton 92] developed, dealing with learning theories in first-order logic. From the very beginning, ILP also was concerned with constructive induction in first-order logic, better known as predicate invention <ref> [Muggleton & Buntine 88] </ref>. Obviously, there is a close correspondence between features in propositional languages and predicates in first-order languages. Consequently, there is a close correspondence between feature construction and predicate invention. However, there exists no systematic comparison between work done in both areas. <p> Obviously, there is a close correspondence between features in propositional languages and predicates in first-order languages. Consequently, there is a close correspondence between feature construction and predicate invention. However, there exists no systematic comparison between work done in both areas. Apart from one exception (DUCE [Muggleton 87] and CIGOL <ref> [Muggleton & Buntine 88] </ref>), work on predicate invention neither tries to extend ideas from propositional constructive induction to first-order logic, nor builds on existing work on feature construction. <p> This restriction is due to efficiency reasons. 2. Conjunction of n Literals: For instance, RINCON's [Wogulis & Langley 89] intermediate con cepts consist of a deliberate number of literals. 3. Disjunctive Definition: Systems using the intraconstruction operator <ref> [Muggleton & Buntine 88, Rouveirol 92, Wirth 89] </ref> construct predicates with disjunctive def initions. * Recursive Predicate: Several systems [Ling 91, Lapointe et al. 93, Ling 95] are designed to invent recursive new predicates. <p> Conj. 2 Lits. Disj. Rec. Deps. CIGOL <ref> [Muggleton & Buntine 88] </ref> fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS <p> Selection can be distinguished with respect to the method for predicate invention. Inverse resolution methods have many degrees of freedom in their choice of appropriate substitutions and inverse substitutions. Most systems a priori restrict the possibilities by simplifying assumptions. For instance, one operator of CIGOL <ref> [Muggleton & Buntine 88] </ref> is simplified by assuming that one input clause and the output clause have no common literal (separability assumption). This and other restrictions determine which of the potential new predicates are included in the vocabulary. Schema-driven methods have to include selection, if several second-order schemata apply. <p> There are two kinds of systems for predicate invention: 1. Systems like GOLEM [Muggleton 94] perform constructive induction without human intervention. Constructive induction takes over in order to improve the best "human" representation. 2. Systems like CLINT [De Raedt & Bruynooghe 92] and CIGOL <ref> [Muggleton & Buntine 88] </ref> perform interactive constructive induction, a kind of constructive induction that is not performed fully automatically. Instead, a constructive induction component proposes the application of constructive induction operators to an oracle. <p> If the application of a constructive induction operator is proposed, the oracle is asked to reject or accept the new feature, and, in the latter case, is asked to name it. In the same manner, CIGOL <ref> [Muggleton & Buntine 88] </ref> (the successor of DUCE) works in first-order logic. CIGOL was the first system based on inverse resolution, and employed three operators to compress a given knowledge base. In general, the fundamental question is if and how we can create meaningful new predicates.
Reference: [Pagallo & Haussler 90] <author> Pagallo G., Haussler D.: </author> <title> Boolean Feature Discovery in Empirical Learning, </title> <journal> Machine Learning, </journal> <volume> 5(1), </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: For instance, the greedy selection [Matheus 91] of features is a kind of bias that is problematic when applied to datasets with high feature interaction. Experiments showed that very complex hypotheses are generated <ref> [Pagallo & Haussler 90] </ref>, since a large number of conditions are required to express interrelationships among features in the dataset. From a theoretical point of view, the algorithmic bias is the only justification for feature construction in propositional learning 3 . <p> Experience from the propositional setting, however, suggests that pure HCI-methods <ref> [Pagallo & Haussler 90, Wnek & Michalski 94] </ref> are worth consideration, too. Successful work in this field should be extended to first-order logic.
Reference: [Pazzani & Kibler 92] <author> Pazzani M., Kibler D.: </author> <title> The Utility of Knowledge in Inductive Learning, </title> <booktitle> in Machine Learning, </booktitle> <volume> 9, </volume> <pages> 57-94, </pages> <year> 1992. </year> <month> 24 </month>
Reference-contexts: Other kinds of predicates are not necessary in the strict sense, but nevertheless useful. For example, cliches are useful to overcome the algorithmic bias of FOCL (its selection of literals) <ref> [Pazzani & Kibler 92, Silverstein & Pazzani 91] </ref>.
Reference: [Pfahringer 94] <author> Pfahringer B.: </author> <title> Controlling Constructive Induction in CiPF: An MDL Ap--proach, </title> <editor> in Bergadano F. & Raedt L.de(eds.), </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-94, </address> <publisher> Springer, </publisher> <address> Berlin, pp.242-256, </address> <year> 1994. </year>
Reference-contexts: Otherwise we would overfit the potential noise in the data by means of the language, a problem termed "language fitting" in <ref> [Pfahringer 94] </ref>. [Pfahringer 94] also suggests the usefulness of a function measuring both accuracy and complexity of a hypothesis and the definitions of the new features or predicates. <p> Otherwise we would overfit the potential noise in the data by means of the language, a problem termed "language fitting" in <ref> [Pfahringer 94] </ref>. [Pfahringer 94] also suggests the usefulness of a function measuring both accuracy and complexity of a hypothesis and the definitions of the new features or predicates. <p> We have to avoid what is called "language fitting" in <ref> [Pfahringer 94] </ref> . Robust measures are needed to avoid fitting insignificant patterns in the theory or in the data. Except for CWS [Srinivasan et al. 92], no system appears to account for noisy data. * Systems for PI tend to construct insignificant predicates.
Reference: [Pfahringer 95] <author> Pfahringer B.: </author> <title> Compression-Based Feature Subset Selection, </title> <booktitle> Proceedings of the IJCAI-95 Workshop on Data Engineering for Inductive Learning, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference-contexts: In the propositional case, this problem is subsumed by what now is discussed as feature subset selection (e.g. <ref> [Pfahringer 95] </ref>). 3. Shall the definitions be generalized ? (Generalization) Generalizing the definition might effect an improvement, if the newly defined term is too specific to be useful for the concept to be learned. Only few systems generalize the definitions of new features/predicates. 4.
Reference: [Pfahringer & Kramer 95] <author> Pfahringer B., Kramer S.: </author> <title> Compression-Based Evaluation of Partial Determinations, </title> <booktitle> Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: If exceptions are likely to arise in the presence of noise, partial determinations are a useful dependency model <ref> [Pfahringer & Kramer 95] </ref>. It should be noted that an extensional definition of predicates can be turned into an intensional definition by a selective induction algorithm. This step is performed by the systems MOBAL [Morik 93, Wrobel 94], CHAMP [Kijsirikul et al. 92] and CWS [Srinivasan et al. 92]. <p> The core of arguments is in any case necessary to discriminate the examples. Unfortunately, the core is empty if the noise level is sufficiently high. We therefore believe that a robust measure like the MDL measure in <ref> [Pfahringer & Kramer 95] </ref> could be more successfully applied to the problem of finding good sets of arguments in the presence of noise than rough set theory. 5.3 Generalizing the Definition of a New Predicate Only a few systems perform a generalization step after the initial definition of a new predicate:
Reference: [Quinlan 90] <author> Quinlan J.R.: </author> <title> Learning Logical Definitions from Relations, </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: In CLINT [De Raedt & Bruynooghe 92] the user is responsible for selection. In FOCL [Silverstein & Pazzani 93], selection takes place in two steps: First, the selection of candidates for cliches is based on the occurrence in an inductive hypothesis and on the information gain measure <ref> [Quinlan 90] </ref>. The next step is a complicated procedure to integrate the candidate cliche in a class hierarchy of existing cliches. During this second step further candidate cliches are dropped.
Reference: [Rendell & Seshu 90] <author> Rendell L.A., Seshu R.: </author> <title> Learning Hard Concepts through Constructive Induction: Framework and Rationale, </title> <journal> Computational Intelligence, </journal> <volume> 6, </volume> <pages> 247-270, </pages> <year> 1990. </year>
Reference-contexts: It is a well-known fact that the results of machine learning algorithms strongly depend on how well the domain is understood. Whereas easy concepts can be learned quite well even by simple learners [Holte 93], Rendell claims that hard concepts <ref> [Rendell & Seshu 90] </ref> mostly cannot even be learned by sophisticated learners. Basically, a concept is hard if the 2 The term "bias" refers to any basis for excluding hypotheses from the search space other than strict completeness and consistency with the examples [Mitchell 80]. <p> However, this is not a matter of all-or-nothing: The "causal relevance" of variables rather is gradually different. Only in a few extreme domains, the causal connection between the independent and the dependent variables seems very weak. Sidestepping philosophical problems concerning causality, <ref> [Rendell & Seshu 90] </ref> discuss this issue under the heading of intrinsic accuracy. In order to turn a hard problem into an easy one, useful abstractions have to be formed from the initial low-level features/predicates. One of the goals of constructive induction is to solve this problem.
Reference: [Rissanen 78] <author> Rissanen J.: </author> <title> Modeling by Shortest Data Description, </title> <journal> Automatica, </journal> <volume> 14, </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: This can be done by a measure based on the Minimum Description Length (MDL) principle <ref> [Rissanen 78] </ref> that measures both accuracy and complexity in a common currency, namely bits needed for encoding an inductive theory and the training examples, given the theory.
Reference: [Rouveirol & Puget 90] <author> Rouveirol C., Puget J.F.: </author> <title> Beyond Inversion of Resolution, </title> <editor> in Porter B.W. & Mooney R.(eds.), </editor> <booktitle> Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, pp.122-131, </address> <year> 1990. </year>
Reference-contexts: The definitions could further be generalized by dropping a condition of the definition. few systems are capable of inventing predicates of several types. CHAMP and 4 Not considered in this report are the predecessors of several systems: IRES <ref> [Rouveirol & Puget 90] </ref> (the predecessor of ITOU [Rouveirol 92]), LFP [Wirth 88] (the predecessor of LFP2 [Wirth 89]), MENDEL [Ling 91] (the predecessor of INPP [Ling 95]), BLIP [Morik 89, Wrobel 89] (the predecessor of MOBAL [Morik 93, Wrobel 94]).
Reference: [Rouveirol 92] <author> Rouveirol C.: ITOU: </author> <title> Induction of First-Order Theories, in Muggleton S.(ed.), Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> London, U.K., </address> <year> 1992. </year>
Reference-contexts: This restriction is due to efficiency reasons. 2. Conjunction of n Literals: For instance, RINCON's [Wogulis & Langley 89] intermediate con cepts consist of a deliberate number of literals. 3. Disjunctive Definition: Systems using the intraconstruction operator <ref> [Muggleton & Buntine 88, Rouveirol 92, Wirth 89] </ref> construct predicates with disjunctive def initions. * Recursive Predicate: Several systems [Ling 91, Lapointe et al. 93, Ling 95] are designed to invent recursive new predicates. <p> Conj. 2 Lits. Disj. Rec. Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 [Wirth 89] fi ITOU <ref> [Rouveirol 92] </ref> fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel <p> The definitions could further be generalized by dropping a condition of the definition. few systems are capable of inventing predicates of several types. CHAMP and 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU <ref> [Rouveirol 92] </ref>), LFP [Wirth 88] (the predecessor of LFP2 [Wirth 89]), MENDEL [Ling 91] (the predecessor of INPP [Ling 95]), BLIP [Morik 89, Wrobel 89] (the predecessor of MOBAL [Morik 93, Wrobel 94]).
Reference: [Schaffer 94] <author> Schaffer C.: </author> <title> A Conservation Law for Generalization Performance, </title> <booktitle> in Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.259-265, </address> <year> 1994. </year>
Reference-contexts: Schaffer <ref> [Schaffer 94] </ref> proved that the generalization performance of any inductive learning algorithm over all learning situations is null. Positive performance in some learning situations must be offset by an equal degree of negative performance in others.
Reference: [Silverstein & Pazzani 91] <author> Silverstein G., Pazzani M.J.: </author> <title> Relational Cliches: Constraining Constructive Induction During Relational Learning, </title> <editor> in Birnbaum L.A. & Collins G.C.(eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.203-207, </address> <year> 1991. </year>
Reference-contexts: Other kinds of predicates are not necessary in the strict sense, but nevertheless useful. For example, cliches are useful to overcome the algorithmic bias of FOCL (its selection of literals) <ref> [Pazzani & Kibler 92, Silverstein & Pazzani 91] </ref>.
Reference: [Silverstein & Pazzani 93] <author> Silverstein G., Pazzani M.J.: </author> <title> Learning Relational Cliches, </title> <editor> in Bergadano F., et al., </editor> <booktitle> Proceedings of the IJCAI-93 Workshop on Inductive Logic Programming, </booktitle> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: Concepts can be combined in numerous ways. In many cases constraints on meaningful combinations are beyond the "knowledge" that is available to the machine. Sometimes selecting the right and interesting combinations requires a thorough understanding of the particular domain. 1. Conjunction of 2 Literals: The predicates in <ref> [Silverstein & Pazzani 93] </ref> are restricted to conjunc tions of two literals. This restriction is due to efficiency reasons. 2. Conjunction of n Literals: For instance, RINCON's [Wogulis & Langley 89] intermediate con cepts consist of a deliberate number of literals. 3. <p> Conj. 2 Lits. Disj. Rec. Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL <ref> [Silverstein & Pazzani 93] </ref> fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi fi fi SIERES [Wirth & O'Rorke 92] <p> This and other restrictions determine which of the potential new predicates are included in the vocabulary. Schema-driven methods have to include selection, if several second-order schemata apply. In CLINT [De Raedt & Bruynooghe 92] the user is responsible for selection. In FOCL <ref> [Silverstein & Pazzani 93] </ref>, selection takes place in two steps: First, the selection of candidates for cliches is based on the occurrence in an inductive hypothesis and on the information gain measure [Quinlan 90]. <p> These ideas have yet to prove their usefulness. Strictly, only two existing systems implement this idea (learning across domain borders): FOCL extended by the capability to learn cliches <ref> [Silverstein & Pazzani 93] </ref>, and MOBAL-MAT [Thieme 89], a component of MOBAL that learns rule models from rules. Furthermore, CLINT-CIA could be used in this way, but it seems that it only has been tested in single domains. <p> aspects and building blocks of 21 predicate invention in this way, we are ready to select and combine features of these systems in order to put predicate invention to work. * The idea that CI should be based on continuing learning should be adapted by work on PI. (Interestingly, FOCL <ref> [Silverstein & Pazzani 93] </ref> anticipated this idea.) * Another lesson from propositional constructive induction is that we have to take care of the complexity. We have to avoid what is called "language fitting" in [Pfahringer 94] .
Reference: [Sommer 95] <author> Sommer E.: FENDER: </author> <title> An Approach to Theory Restructuring, </title> <editor> in Lavrac N. & Wrobel S.(eds.), </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-95, </address> <publisher> Springer, </publisher> <address> Berlin, pp.356-359, </address> <year> 1995. </year>
Reference-contexts: Furthermore, the so-called transformation approaches (creating predicates from formal specifications) to PI (e.g. [Franova & Kodratoff 92, Le Blanc 94]) are not discussed, as they are only partially relevant in the inductive learning setting. For the same reason we omitted the discussion of FENDER <ref> [Sommer 95] </ref>, a component of MOBAL that restructures a knowledge base by certain inverse resolution operators. 11 CHILLIN are the most flexible systems 5 with respect to the variety of types of predicates that can be invented. 4.6 Some Considerations on Predicate Invention PI is often triggered by the need to
Reference: [Srinivasan et al. 92] <author> Srinivasan A., Muggleton S., Bain M.: </author> <title> Distinguishing Exceptions from Noise in Non-Monotonic Learning, </title> <booktitle> Proceedings of the 2nd International Workshop on Inductive Logic Programming, </booktitle> <year> 1992. </year>
Reference-contexts: Unfortunately, this problem is undecidable in first-order Horn logic [Stahl 95]. However, Stahl proved that the problem is decidable in fixed-size languages, and she also demonstrated that PI is useful in this context. Nevertheless, most systems have to take a heuristic decision (e.g. <ref> [Srinivasan et al. 92] </ref>) for pragmatic reasons anyway. Stahl's classification is based on two different aspects of predicate invention: First, demand-driven and reformulation methods essentially differ in their approach to detection. Demand-driven systems need a reason for predicate invention, reformulation-based systems do not. <p> Before the actual refinement step, we have to determine the clause of the theory which is to blame for the incorrectly covered instances. This blame assignment can be simple as in <ref> [Srinivasan et al. 92] </ref> or more sophisticated as in [Wrobel 93]. The third type of methods cannot be found in [Stahl 93] | it is a type we identified besides inverse resolution and schemata. The kinds of methods differ in their use of negative examples. <p> & Buntine 88] fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS <ref> [Srinivasan et al. 92] </ref> fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi fi fi SIERES [Wirth & O'Rorke 92] fi fi fi INDEX [Flach 93] fi extensional, Conj.-conjunctive, 2 <p> It should be noted that an extensional definition of predicates can be turned into an intensional definition by a selective induction algorithm. This step is performed by the systems MOBAL [Morik 93, Wrobel 94], CHAMP [Kijsirikul et al. 92] and CWS <ref> [Srinivasan et al. 92] </ref>. Learning an intensional def inition is in fact generalization in the sense of Matheus. The definitions could further be generalized by dropping a condition of the definition. few systems are capable of inventing predicates of several types. <p> The task of recognizing the demand for predicate invention is nothing else than detection. For pragmatic reasons, systems have to decide heuristically if the existing vocabulary is insufficient. Reformulation-based approaches perform a kind of look-ahead, searching for new predicates that make the theory more compact. Demand-driven systems like CWS <ref> [Srinivasan et al. 92] </ref> and CHAMP-DBC [Kijsirikul et al. 92] detect the need for predicate invention, if the top-down algorithm fails to discriminate the tuples covered by the clause. The over-general clauses are handed over to the algorithm for predicate invention. <p> More sophisticated techniques from theory revision can be used to determine which clause is really to be blamed for an incorrect classification. A major task in this process is to decide if the incorrectly covered instances are due to noise, or if they are real exceptions <ref> [Srinivasan et al. 92] </ref>. If they were due to chance, inventing a new predicate would overfit the noise. As in feature construction, we have to control the complexity of the theory and the newly defined predicates. <p> If they were due to chance, inventing a new predicate would overfit the noise. As in feature construction, we have to control the complexity of the theory and the newly defined predicates. Correcting a clause in this way is often used together with top-down algorithms <ref> [Srinivasan et al. 92, Kijsirikul et al. 92] </ref>, but does not presuppose their use. For instance, the predicate invention component of GOLEM [Muggleton 94] works in a similar way, correcting bottom-up generated clauses. <p> During this second step further candidate cliches are dropped. For the case of clause-refinement methods, we have to face a theory that is too general and therefore has to be specialized with a new predicate. The first task is to select the set of arguments for the new predicate. <ref> [Srinivasan et al. 92] </ref> select the same arguments as in the head of the over-general clause. CHAMP [Kijsirikul et al. 92] starts with all variables and drops variables as long as there are enough to discriminate the positive and the negative instances. <p> successfully applied to the problem of finding good sets of arguments in the presence of noise than rough set theory. 5.3 Generalizing the Definition of a New Predicate Only a few systems perform a generalization step after the initial definition of a new predicate: CHAMP [Kijsirikul et al. 92], CWS <ref> [Srinivasan et al. 92] </ref> and MOBAL [Wrobel 94]. An extensional definition can easily be generalized by applying a relational learning algorithm to the tuples of the definition. CIGOL only generalizes its definitions if this step helps to compress the knowl 14 edge base. <p> We have to avoid what is called "language fitting" in [Pfahringer 94] . Robust measures are needed to avoid fitting insignificant patterns in the theory or in the data. Except for CWS <ref> [Srinivasan et al. 92] </ref>, no system appears to account for noisy data. * Systems for PI tend to construct insignificant predicates. New predicates are invented to complete a single clause, or they are used only once. Two exceptions are RINCON [Wogulis & Langley 89] and MOBAL [Wrobel 94].
Reference: [Stahl 94] <author> Stahl I.: </author> <title> On the Utility of Predicate Invention in Inductive Logic Programming, </title> <editor> in Bergadano F. & Raedt L.de(eds.), </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-94, </address> <publisher> Springer, </publisher> <address> Berlin, pp.272-286, </address> <year> 1994. </year>
Reference-contexts: CHAMP [Kijsirikul et al. 92] starts with all variables and drops variables as long as there are enough to discriminate the positive and the negative instances. As Stahl <ref> [Stahl 94] </ref> points out, searching for minimum sets of discriminating arguments has a subjective component, since there might be several such sets. The intersection of all minimal discriminating sets of arguments roughly corresponds to the core as defined in rough set theory [Ziarko 92].
Reference: [Stahl 93] <author> Stahl I.: </author> <title> Predicate Invention in ILP an Overview, in Brazdil P.B.(ed.), Machine Learning: </title> <publisher> ECML-93, Springer, </publisher> <address> Berlin, pp.313-322, </address> <year> 1993. </year>
Reference-contexts: Predicates that make it possible to learn a given concept are called necessary [Ling 91]. Predicates that are not crucial but help to compress a theory are called useful. 4.2 Reformulation vs. Demand-Driven Approaches In her overview article on predicate invention <ref> [Stahl 93] </ref>, Stahl distinguishes between reformulation approaches and demand-driven approaches to predicate invention. Reformulation approaches introduce new intermediate predicates as a reformulation of an existing theory in order to express it more compactly. This is done in any case, not only if learning fails in the given representation. <p> This blame assignment can be simple as in [Srinivasan et al. 92] or more sophisticated as in [Wrobel 93]. The third type of methods cannot be found in <ref> [Stahl 93] </ref> | it is a type we identified besides inverse resolution and schemata. The kinds of methods differ in their use of negative examples. Clause-refinement methods depend on the existence of negative examples, because they shall discriminate positive and negative instances.
Reference: [Stahl 94] <author> Stahl I.: </author> <title> The Arguments of Newly Invented Predicates in ILP, </title> <booktitle> in Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. 237, </booktitle> <address> pp.233-245, </address> <year> 1994. </year>
Reference-contexts: CHAMP [Kijsirikul et al. 92] starts with all variables and drops variables as long as there are enough to discriminate the positive and the negative instances. As Stahl <ref> [Stahl 94] </ref> points out, searching for minimum sets of discriminating arguments has a subjective component, since there might be several such sets. The intersection of all minimal discriminating sets of arguments roughly corresponds to the core as defined in rough set theory [Ziarko 92].
Reference: [Stahl 95] <author> Stahl I.: </author> <title> The Appropriateness of Predicate Invention as Bias Shift Operation in ILP, </title> <note> to appear in Machine Learning, </note> <year> 1995. </year>
Reference-contexts: In contrast to reformulation approaches, demand-driven systems try to detect situations where the given vocabulary is insufficient for the learning task at hand. Unfortunately, this problem is undecidable in first-order Horn logic <ref> [Stahl 95] </ref>. However, Stahl proved that the problem is decidable in fixed-size languages, and she also demonstrated that PI is useful in this context. Nevertheless, most systems have to take a heuristic decision (e.g. [Srinivasan et al. 92]) for pragmatic reasons anyway. <p> positive examples in a special way: They shall describe everything that the given examples have in common, namely their base-case and the repeated application of an operator. 4.4 Utility and Decidability of Predicate Invention In the discussion of the utility of predicate invention the language bias plays a central role <ref> [Stahl 95] </ref>. The assessment of the utility and appropriateness of 8 predicate invention is based on the question if a predicate is necessary to express a concept. <p> According to this view, predicate invention is useful only if it makes possible to express the given concept. Unfortunately, the question if predicate invention is necessary to successfully learn a theory is undecidable in first-order Horn logic <ref> [Stahl 95] </ref>. So in unconstrained or weakly constrained first-order Horn logic, predicate invention may be useful, but its necessity is undecidable. However, induction in weakly-constrained first-order languages is known not to be feasible anyway.
Reference: [Sutton 92] <author> Sutton R.S.: </author> <title> Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta, </title> <booktitle> in Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, pp.171-176, </address> <year> 1992. </year>
Reference-contexts: Sutton [Sutton 94] argues that constructive induction is doomed to effect only minor improvements with a single learning task at hand. As a solution he proposed a new methodology for constructive induction that is based on continuing learning (as outlined in <ref> [Sutton 92] </ref>). Constructive induction will effect major improvements when the learner is confronted with a sequence of learning tasks. These tasks may have different solutions, but supposedly often share the same useful representations.
Reference: [Sutton 94] <author> Sutton R.S.: </author> <title> Constructive Induction Needs a Methodology based on Continuing Learning, Panel of the Workshop on Constructive Induction and Change of Representation, </title> <booktitle> 11th International Conference on Machine Learning (ML-94/COLT-94), </booktitle> <address> New Brunswick, New Jersey, </address> <year> 1994. </year>
Reference-contexts: However, the theorem makes clear that constructive induction just makes learning algorithms more flexible, a flexibility that does not guarantee any improvement. Sutton <ref> [Sutton 94] </ref> argues that constructive induction is doomed to effect only minor improvements with a single learning task at hand. As a solution he proposed a new methodology for constructive induction that is based on continuing learning (as outlined in [Sutton 92]).
Reference: [Thagard & Nowak 90] <author> Thagard P., Nowak G.: </author> <title> The Conceptual Structure of the Geological Revolution, </title> <editor> in Shrager J., Langley P.(eds.): </editor> <title> Computational Models of Discovery and Theory Formation, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: The assumption of such a border can be seen as a hypothesis by itself. 9 * Intensional without Recursion: The predicates are intensionally defined, but without recursion. The definition describes the set of tuples in the relation. Intensional descriptions are what <ref> [Thagard & Nowak 90] </ref> call concept combinations. That is, a new concept is defined as a combination of existing concepts. Concepts can be combined in numerous ways. In many cases constraints on meaningful combinations are beyond the "knowledge" that is available to the machine.
Reference: [Thieme 89] <author> Thieme S.: </author> <title> The Acquisition of Model-Knowledge for a Model-Driven Machine Learning Approach, </title> <editor> in Morik K.(ed.), </editor> <booktitle> Knowledge Representation and Organization in Machine Learning, Vol. 347 of Lecture Notes in Artificial Intelligence, </booktitle> <publisher> Springer, </publisher> <address> Berlin, pp.177-191, </address> <year> 1989. </year>
Reference-contexts: These ideas have yet to prove their usefulness. Strictly, only two existing systems implement this idea (learning across domain borders): FOCL extended by the capability to learn cliches [Silverstein & Pazzani 93], and MOBAL-MAT <ref> [Thieme 89] </ref>, a component of MOBAL that learns rule models from rules. Furthermore, CLINT-CIA could be used in this way, but it seems that it only has been tested in single domains. Several systems are able to utilize schemata (possibly acquired in another domain) for learning.
Reference: [Weber & Tausend 94] <author> Weber I., Tausend B.: </author> <title> A Three-Tiered Confidence Model for Revising Logical Theories, </title> <booktitle> in Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. 237, </booktitle> <address> pp.391-402, </address> <year> 1994. </year>
Reference: [Wirth 88] <author> Wirth R.: </author> <title> Learning by Failure to Prove, </title> <booktitle> in Proceedings of the Third European Working Session on Learning (EWSL-88), </booktitle> <address> pp.237-251, </address> <year> 1988. </year>
Reference-contexts: The definitions could further be generalized by dropping a condition of the definition. few systems are capable of inventing predicates of several types. CHAMP and 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU [Rouveirol 92]), LFP <ref> [Wirth 88] </ref> (the predecessor of LFP2 [Wirth 89]), MENDEL [Ling 91] (the predecessor of INPP [Ling 95]), BLIP [Morik 89, Wrobel 89] (the predecessor of MOBAL [Morik 93, Wrobel 94]).
Reference: [Wirth 89] <author> Wirth R.: </author> <title> Completing Logic Programs by Inverse Resolution, </title> <booktitle> in Proceedings of the Fourth European Working Session on Learning (EWSL-89), </booktitle> <publisher> Pitman, </publisher> <address> London, pp.239-250, </address> <year> 1989. </year>
Reference-contexts: This restriction is due to efficiency reasons. 2. Conjunction of n Literals: For instance, RINCON's [Wogulis & Langley 89] intermediate con cepts consist of a deliberate number of literals. 3. Disjunctive Definition: Systems using the intraconstruction operator <ref> [Muggleton & Buntine 88, Rouveirol 92, Wirth 89] </ref> construct predicates with disjunctive def initions. * Recursive Predicate: Several systems [Ling 91, Lapointe et al. 93, Ling 95] are designed to invent recursive new predicates. <p> Conj. 2 Lits. Disj. Rec. Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 <ref> [Wirth 89] </ref> fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi <p> CHAMP and 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU [Rouveirol 92]), LFP [Wirth 88] (the predecessor of LFP2 <ref> [Wirth 89] </ref>), MENDEL [Ling 91] (the predecessor of INPP [Ling 95]), BLIP [Morik 89, Wrobel 89] (the predecessor of MOBAL [Morik 93, Wrobel 94]).
Reference: [Wirth & O'Rorke 92] <author> Wirth R., O'Rorke P.: </author> <title> Constraints on Predicate Invention, in Muggleton S.(ed.), Inductive Logic Programming, </title> <publisher> Academic Press, </publisher> <address> London, U.K., pp.299-318, </address> <year> 1992. </year>
Reference-contexts: 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi fi fi SIERES <ref> [Wirth & O'Rorke 92] </ref> fi fi fi INDEX [Flach 93] fi extensional, Conj.-conjunctive, 2 Lits.-a conjunction of two literals, Disj.- disjunctive, Rec.-recursive, Deps.-dependencies among arguments.) In INDEX [Flach 93], new predicates are either functional respectively multi-valued dependencies or represent partitionings of tuples in a relation. * Partial Determination: Partial determinations are <p> CHILLIN [Zelle et al. 94] combines bottom-up and top-down techniques: First, it computes the least general generalization (lgg) of two clauses, then it tries to make the generalization consistent by specializing it. If this fails, predicate invention is performed to make the clause consistent. SIERES <ref> [Wirth & O'Rorke 92] </ref> triggers predicate invention if no extension of the clause exists that is correct with respect to argument dependency graphs and critical terms, i.e. unused input or unbound output terms.
Reference: [Wnek & Michalski 94] <author> Wnek J., Michalski R.S.: </author> <title> Hypothesis-Driven Constructive Induction in AQ17-HCI: A Method and Experiments, in Special Issue on Evaluating and Changing Representation, </title> <journal> Machine Learning, </journal> <volume> 14(2), </volume> <year> 1994. </year>
Reference-contexts: In this report we try to apply the frameworks for constructive induction by Matheus [Matheus & Rendell 89, Matheus 91] and Wnek and Michal-ski <ref> [Wnek & Michalski 94] </ref> to predicate invention. In the next section, we recall the major reasons for constructive induction in propositional languages. In the third section we review the frameworks for constructive induction by Matheus, Wnek and Michalski. <p> Generally, the goal of constructive induction is an increase in accuracy and a decrease in complexity of a hypothesis <ref> [Wnek & Michalski 94] </ref>. It is important to include the complexity of the newly defined features or predicates in the complexity of the hypothesis | we have to pay a price for defining a large number of new features or predicates. <p> When should new features/predicates be constructed ? (Detection) Detection is necessary, because the construction of irrelevant features/predicates usually affects learning like noise in the data <ref> [Wnek & Michalski 94] </ref>. Moreover, the complexity of the definitions has to be added to the complexity of the hypothesis. Additionally, the complexity of the hypothesis space is increased by the definition of useless features/predicates, making search in this space error-prone and more costly in potential subsequent steps. <p> If the quality of the hypothesis induced in the transformed representation is worse than the original hypothesis, CN2-MCI concludes there is no need for a representation change. As we will see below, a similar kind of look-ahead is done by systems for predicate invention. <ref> [Wnek & Michalski 94] </ref> introduce a taxonomy of constructive induction systems according to the sources of information that are used by constructive induction operators. <p> Subsequently, we apply the framework of <ref> [Wnek & Michalski 94] </ref> to PI. Finally, we summarize the benefits of the application of those frameworks to predicate invention. 5 MOBAL is able to construct recursive predicates given the required rule models. <p> However, FOCL subsequently evaluates pairs of literals by their information gain, and this computation requires looking through the data. So FOCL is not a system for HCI, but also for MCI. The framework of <ref> [Wnek & Michalski 94] </ref> does not reveal fundamental differences and commonalities among the systems under consideration. The methods 15 primarily differ in how strongly parts of the hypothesis and subsets of examples influence the construction of a new predicate. <p> Experience from the propositional setting, however, suggests that pure HCI-methods <ref> [Pagallo & Haussler 90, Wnek & Michalski 94] </ref> are worth consideration, too. Successful work in this field should be extended to first-order logic.
Reference: [Wogulis & Langley 89] <author> Wogulis J., Langley P.: </author> <title> Improving Efficiency by Learning Intermediate Concepts, </title> <booktitle> in Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, 657-662, </address> <year> 1989. </year>
Reference-contexts: More precisely, several inverse resolution operators are theoretically possible. Two of them could be used for predicate invention, and mostly only one of them is used (as described above): intraconstruction. Only RINCON <ref> [Wogulis & Langley 89] </ref> employs the other operator usable for PI, namely interconstruction. In contrast to intraconstruction, inter-construction puts the commonalities between the input clauses into the definition of the new predicate. The differences are kept in the clauses which then make use of the newly defined predicate. <p> Conjunction of 2 Literals: The predicates in [Silverstein & Pazzani 93] are restricted to conjunc tions of two literals. This restriction is due to efficiency reasons. 2. Conjunction of n Literals: For instance, RINCON's <ref> [Wogulis & Langley 89] </ref> intermediate con cepts consist of a deliberate number of literals. 3. <p> Conj. 2 Lits. Disj. Rec. Deps. CIGOL [Muggleton & Buntine 88] fi LFP2 [Wirth 89] fi ITOU [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON <ref> [Wogulis & Langley 89] </ref> fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul <p> Except for CWS [Srinivasan et al. 92], no system appears to account for noisy data. * Systems for PI tend to construct insignificant predicates. New predicates are invented to complete a single clause, or they are used only once. Two exceptions are RINCON <ref> [Wogulis & Langley 89] </ref> and MOBAL [Wrobel 94]. RINCON chooses the one conjunctive concept that allows to rewrite as many other concept definitions as possible. MOBAL evaluates new predicates by a number of structural properties, including the usage of the new predicate in other clauses.
Reference: [Wrobel 89] <author> Wrobel S.: </author> <title> Demand-Driven Concept Formation, </title> <editor> in Morik K.(ed.), </editor> <booktitle> Knowledge Representation and Organization in Machine Learning, Vol. 347 of Lecture Notes in Artificial Intelligence, </booktitle> <publisher> Springer, </publisher> <address> Berlin, pp.289-319, </address> <year> 1989. </year>
Reference-contexts: CHAMP and 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU [Rouveirol 92]), LFP [Wirth 88] (the predecessor of LFP2 [Wirth 89]), MENDEL [Ling 91] (the predecessor of INPP [Ling 95]), BLIP <ref> [Morik 89, Wrobel 89] </ref> (the predecessor of MOBAL [Morik 93, Wrobel 94]). Furthermore, the so-called transformation approaches (creating predicates from formal specifications) to PI (e.g. [Franova & Kodratoff 92, Le Blanc 94]) are not discussed, as they are only partially relevant in the inductive learning setting.
Reference: [Wrobel 93] <author> Wrobel S.: </author> <title> On the Proper Definition of Minimality in Specialization and Theory Revision, in Brazdil P.B.(ed.), Machine Learning: </title> <publisher> ECML-93, Springer, </publisher> <address> Berlin, pp.65-82, </address> <year> 1993. </year>
Reference-contexts: Before the actual refinement step, we have to determine the clause of the theory which is to blame for the incorrectly covered instances. This blame assignment can be simple as in [Srinivasan et al. 92] or more sophisticated as in <ref> [Wrobel 93] </ref>. The third type of methods cannot be found in [Stahl 93] | it is a type we identified besides inverse resolution and schemata. The kinds of methods differ in their use of negative examples.
Reference: [Wrobel 94] <author> Wrobel S.: </author> <title> Concept Formation during Interactive Theory Revision, in Special Issue on Evaluating and Changing Representation, </title> <journal> Machine Learning, </journal> <volume> 14(2), </volume> <year> 1994. </year>
Reference-contexts: [Rouveirol 92] fi Banerji [Banerji 92] fi RINCON [Wogulis & Langley 89] fi INPP [Ling 95] fi CILP [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL <ref> [Wrobel 94] </ref> fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN [Zelle et al. 94] fi fi fi fi fi SIERES [Wirth & O'Rorke 92] fi fi fi INDEX [Flach 93] fi extensional, Conj.-conjunctive, 2 Lits.-a conjunction of two literals, Disj.- disjunctive, Rec.-recursive, Deps.-dependencies <p> It should be noted that an extensional definition of predicates can be turned into an intensional definition by a selective induction algorithm. This step is performed by the systems MOBAL <ref> [Morik 93, Wrobel 94] </ref>, CHAMP [Kijsirikul et al. 92] and CWS [Srinivasan et al. 92]. Learning an intensional def inition is in fact generalization in the sense of Matheus. <p> 4 Not considered in this report are the predecessors of several systems: IRES [Rouveirol & Puget 90] (the predecessor of ITOU [Rouveirol 92]), LFP [Wirth 88] (the predecessor of LFP2 [Wirth 89]), MENDEL [Ling 91] (the predecessor of INPP [Ling 95]), BLIP [Morik 89, Wrobel 89] (the predecessor of MOBAL <ref> [Morik 93, Wrobel 94] </ref>). Furthermore, the so-called transformation approaches (creating predicates from formal specifications) to PI (e.g. [Franova & Kodratoff 92, Le Blanc 94]) are not discussed, as they are only partially relevant in the inductive learning setting. <p> Summing up, we believe that compression is a major motivation for PI. Nevertheless, the goal of the overall system strongly affects the use of PI. For instance, the goal of interactive theory revision affects the way of PI in MOBAL <ref> [Wrobel 94] </ref>. <p> finding good sets of arguments in the presence of noise than rough set theory. 5.3 Generalizing the Definition of a New Predicate Only a few systems perform a generalization step after the initial definition of a new predicate: CHAMP [Kijsirikul et al. 92], CWS [Srinivasan et al. 92] and MOBAL <ref> [Wrobel 94] </ref>. An extensional definition can easily be generalized by applying a relational learning algorithm to the tuples of the definition. CIGOL only generalizes its definitions if this step helps to compress the knowl 14 edge base. Systems like CILP define recursive predicates without considering ex-tensional definitions as an alternative. <p> Except for CWS [Srinivasan et al. 92], no system appears to account for noisy data. * Systems for PI tend to construct insignificant predicates. New predicates are invented to complete a single clause, or they are used only once. Two exceptions are RINCON [Wogulis & Langley 89] and MOBAL <ref> [Wrobel 94] </ref>. RINCON chooses the one conjunctive concept that allows to rewrite as many other concept definitions as possible. MOBAL evaluates new predicates by a number of structural properties, including the usage of the new predicate in other clauses.
Reference: [Zelle et al. 94] <author> Zelle J.M., Mooney R.J., Konvisser J.B.: </author> <title> Combining Top-down and Bottom-up Techniques in Inductive Logic Programming, </title> <booktitle> in Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.343-351, </address> <year> 1994. </year> <month> 26 </month>
Reference-contexts: [Lapointe et al. 93] fi CLINT-CIA [De Raedt & Bruynooghe 92] fi FOCL [Silverstein & Pazzani 93] fi GOLEM [Muggleton 94] fi CWS [Srinivasan et al. 92] fi fi fi fi MOBAL [Wrobel 94] fi fi fi fi fi CHAMP [Kijsirikul et al. 92] fi fi fi fi fi CHILLIN <ref> [Zelle et al. 94] </ref> fi fi fi fi fi SIERES [Wirth & O'Rorke 92] fi fi fi INDEX [Flach 93] fi extensional, Conj.-conjunctive, 2 Lits.-a conjunction of two literals, Disj.- disjunctive, Rec.-recursive, Deps.-dependencies among arguments.) In INDEX [Flach 93], new predicates are either functional respectively multi-valued dependencies or represent partitionings of <p> Correcting a clause in this way is often used together with top-down algorithms [Srinivasan et al. 92, Kijsirikul et al. 92], but does not presuppose their use. For instance, the predicate invention component of GOLEM [Muggleton 94] works in a similar way, correcting bottom-up generated clauses. CHILLIN <ref> [Zelle et al. 94] </ref> combines bottom-up and top-down techniques: First, it computes the least general generalization (lgg) of two clauses, then it tries to make the generalization consistent by specializing it. If this fails, predicate invention is performed to make the clause consistent.
Reference: [Ziarko 92] <author> Ziarko W.: </author> <title> The Discovery, Analysis, and Representation of Data Dependen--cies in Databases, </title> <editor> in Piatetsky-Shapiro G.,Frawley W.J.(eds.), </editor> <title> Knowledge Discovery in Databases, </title> <publisher> AAAI Press, </publisher> <address> Palo Alto, CA, </address> <year> 1992. </year>
Reference-contexts: As Stahl [Stahl 94] points out, searching for minimum sets of discriminating arguments has a subjective component, since there might be several such sets. The intersection of all minimal discriminating sets of arguments roughly corresponds to the core as defined in rough set theory <ref> [Ziarko 92] </ref>. The core of arguments is in any case necessary to discriminate the examples. Unfortunately, the core is empty if the noise level is sufficiently high.
Reference: [Zytkow 93] <author> Zytkow J.M.: </author> <title> Introduction: </title> <booktitle> Cognitive Autonomy in Machine Discovery, in Special Issue on Machine Discovery, Machine Learning, </booktitle> <pages> 12(1-3), </pages> <year> 1993. </year> <month> 27 </month>
Reference-contexts: However, defining a new concept only pays off if we make use of the name many times. Normally, we cannot know beforehand if a new concept turns out to be useful in practice. This is the reason why Zytkow <ref> [Zytkow 93] </ref> metaphorically calls new terms "investments". These considerations obviously favor a process that is iterative rather than a process that stops after a single iteration of constructive induction. <p> In this scenario, the learner shall 17 find a suitable representation on his own. This amounts to a higher degree of autonomy, and turns a learner into a discoverer <ref> [Zytkow 93] </ref>. In this section we want to discuss the advantages and disadvantages of autonomy in contrast to human intervention in predicate invention. There are two kinds of systems for predicate invention: 1. Systems like GOLEM [Muggleton 94] perform constructive induction without human intervention.
References-found: 64

