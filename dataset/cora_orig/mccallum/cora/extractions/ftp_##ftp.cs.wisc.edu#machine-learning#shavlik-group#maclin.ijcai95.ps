URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/maclin.ijcai95.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/maclin.ijcai95.ps.abstract.html
Root-URL: 
Title: Combining the Predictions of Multiple Classifiers: Using Competitive Learning to Initialize Neural Networks  
Author: Richard Maclin Jude W. Shavlik 
Address: 1210 West Dayton Street Madison, WI 53706 USA  1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin  Computer Sciences Department University of Wisconsin  
Note: Appears in the Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95).  
Abstract: The primary goal of inductive learning is to generalize well that is, induce a function that accurately produces the correct output for future inputs. Hansen and Salamon showed that, under certain assumptions, combining the predictions of several separately trained neural networks will improve generalization. One of their key assumptions is that the individual networks should be independent in the errors they produce. In the standard way of performing backpropagation this assumption may be violated, because the standard procedure is to initialize network weights in the region of weight space near the origin. This means that backpropagation's gradient-descent search may only reach a small subset of the possible local minima. In this paper we present an approach to initializing neural networks that uses competitive learning to intelligently create networks that are originally located far from the origin of weight space, thereby potentially increasing the set of reachable local minima. We report experiments on two real-world datasets where combinations of networks initialized with our method generalize better than combina tions of networks initialized the traditional way.
Abstract-found: 1
Intro-found: 1
Reference: [ Baxt, 1992 ] <author> W. Baxt. </author> <title> Improving the accuracy of an artificial neural network using multiple differently trained networks. </title> <journal> Neural Comp., </journal> <volume> 4 </volume> <pages> 772-780, </pages> <year> 1992. </year>
Reference: [ Berenji and Khedkar, 1992 ] <author> H. Berenji and P. Khedkar. </author> <title> Learning and tuning fuzzy logic controllers through reinforcements. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3 </volume> <pages> 724-740, </pages> <year> 1992. </year>
Reference: [ Drucker et al., 1994 ] <author> H. Drucker, C. Cortes, L. Jackel, Y. Le Cun, and V. Vapnik. </author> <title> Boosting and other machine learning algorithms. </title> <booktitle> In Procs. of the 11th Int. Machine Learning Conf., </booktitle> <pages> pages 53-61, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference: [ Fahlman and Lebiere, 1990 ] <author> S. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Many methods have been introduced to solve these problems. Some researchers introduce penalty terms into the cost function that favor generalization [ Hinton, 1989 ] . Others "avoid" overfitting using stopping criteria like patience <ref> [ Fahlman and Lebiere, 1990 ] </ref> or by employing validation sets [ Lang et al., 1990 ] . Some research has focused on choosing an appropriate network by growing [ Fahlman and Lebiere, 1990 ] or shrinking [ Le Cun et al., 1989 ] a network topology, or by exploring a <p> Others "avoid" overfitting using stopping criteria like patience <ref> [ Fahlman and Lebiere, 1990 ] </ref> or by employing validation sets [ Lang et al., 1990 ] . Some research has focused on choosing an appropriate network by growing [ Fahlman and Lebiere, 1990 ] or shrinking [ Le Cun et al., 1989 ] a network topology, or by exploring a number of topologies [ Opitz and Shavlik, 1993 ] .
Reference: [ Fahlman, 1988 ] <author> S. Fahlman. </author> <title> Faster learning variations on back-propagation: An empirical study. </title> <booktitle> In Procs. of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 38-51, </pages> <address> Pittsburgh, PA, </address> <year> 1988. </year>
Reference-contexts: Our first approach was the obvious solution use a much larger random range for generating initial network weights. This works well in terms of producing large weights, but the resulting networks suffer from the flat-spot problem <ref> [ Fahlman, 1988 ] </ref> backpropagation is unable to refine these networks since the activation values of the hidden and output units tend to be very close to 0 or 1. 2 This occurs because the net input values for a network unit will often be highly positive or negative.
Reference: [ Ghosh et al., 1992 ] <author> J. Ghosh, L. Deuser, and S. Beck. </author> <title> Evidence combination techniques for robust classification of short-duration oceanic signals. </title> <booktitle> In SPIE Conf. on Adaptive and Learning Systems, </booktitle> <volume> volume 1706, </volume> <pages> pages 266-276, </pages> <address> Orlando, FL, </address> <year> 1992. </year>
Reference: [ Granger, 1989 ] <author> C. Granger. </author> <title> Combining forecasts - twenty years later. </title> <journal> J. of Forecasting, </journal> <volume> 8, </volume> <year> 1989. </year>
Reference: [ Hampshire and Waibel, 1990 ] <author> J. Hampshire and A. Waibel. </author> <title> A novel objective function for improved phoneme recognition using time-delay neural networks. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1 </volume> <pages> 216-228, </pages> <year> 1990. </year>
Reference: [ Hansen and Salamon, 1990 ] <author> L. Hansen and P. Salamon. </author> <title> Neural network ensembles. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 993-1001, </pages> <year> 1990. </year>
Reference-contexts: Figure 2 shows a framework for combining multiple networks. The choice of a function for combining predictions is important [ Kearns and Seung, 1995 ] . Examples of combination functions include voting schemes <ref> [ Hansen and Salamon, 1990 ] </ref> , simple averages [ Lincoln and Skrzypek, 1990 ] , weighted average schemes [ Perrone and Cooper, 1994; Rogova, 1994 ] , and schemes for training combiners [ Rost and Sander, 1993; Wolpert, 1992; Zhang et al., 1992 ] .
Reference: [ Hashem et al., 1993 ] <author> S. Hashem, Y. Yih, and B. Schmeiser. </author> <title> An efficient model for product allocation using optimal combinations of neural networks. </title> <editor> In C. Dagli, L. Burke, B. Femandez, and J. Ghosh, editors, </editor> <booktitle> Intelligent Engineering Systems through Artificial Neural Networks, </booktitle> <volume> volume 3. </volume> <publisher> ASME Press, </publisher> <year> 1993. </year>
Reference: [ Hecht-Nielsen, 1988 ] <author> R. Hecht-Nielsen. </author> <title> Applications of counterpropagation networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 1 </volume> <pages> 131-139, </pages> <year> 1988. </year>
Reference-contexts: Our work also relates to hybrid systems that mix levels of unsupervised and supervised learning in neural networks <ref> [ Hecht-Nielsen, 1988; Huang and Lippmann, 1988 ] </ref> . One difference in our work is that we perform our unsupervised learning among the categories separately. We also transform the results from competitive learning using our weight multiplier producing large initial weights.
Reference: [ Hinton, 1989 ] <author> G. Hinton. </author> <title> Connectionist learning procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 185-234, </pages> <year> 1989. </year>
Reference-contexts: A number of schemes have been introduced to address this ubiquitous problem, ranging from tree-pruning in decision trees [ Quinlan, 1987 ] to the use of complexity terms in neural networks <ref> [ Hinton, 1989 ] </ref> . In this paper we present a way to address the generalization problem for neural networks trained by backpropagation [ Rumelhart et al., 1986 ] . <p> Many methods have been introduced to solve these problems. Some researchers introduce penalty terms into the cost function that favor generalization <ref> [ Hinton, 1989 ] </ref> . Others "avoid" overfitting using stopping criteria like patience [ Fahlman and Lebiere, 1990 ] or by employing validation sets [ Lang et al., 1990 ] . <p> To mitigate overfitting, for the digit-recognition data we added a sum-of-weight-magnitudes complexity term to the cost function (which is equivalent to using weight decay <ref> [ Hinton, 1989 ] </ref> ), while for the protein-folding data we used validation sets [ Lang et al., 1990 ] (tests with weight decay produced poor results compared to the standard results for this data set). However, we did not use the cost function when evaluating the trained networks.
Reference: [ Huang and Lippmann, 1988 ] <author> W. Huang and R. Lipp-mann. </author> <title> Neural net and traditional classifiers. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 387-396. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Our work also relates to hybrid systems that mix levels of unsupervised and supervised learning in neural networks <ref> [ Hecht-Nielsen, 1988; Huang and Lippmann, 1988 ] </ref> . One difference in our work is that we perform our unsupervised learning among the categories separately. We also transform the results from competitive learning using our weight multiplier producing large initial weights.
Reference: [ Jacobs et al., 1991 ] <author> R. Jacobs, M. Jordan, S. Nowlan, and G. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference: [ Kearns and Seung, 1995 ] <author> M. Kearns and H. Seung. </author> <title> Learning from a population of hypotheses. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 255-276, </pages> <year> 1995. </year>
Reference-contexts: Figure 2 shows a framework for combining multiple networks. The choice of a function for combining predictions is important <ref> [ Kearns and Seung, 1995 ] </ref> .
Reference: [ Lang et al., 1990 ] <author> K. Lang, A. Waibel, and G. Hinton. </author> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43, </pages> <year> 1990. </year>
Reference-contexts: Many methods have been introduced to solve these problems. Some researchers introduce penalty terms into the cost function that favor generalization [ Hinton, 1989 ] . Others "avoid" overfitting using stopping criteria like patience [ Fahlman and Lebiere, 1990 ] or by employing validation sets <ref> [ Lang et al., 1990 ] </ref> . Some research has focused on choosing an appropriate network by growing [ Fahlman and Lebiere, 1990 ] or shrinking [ Le Cun et al., 1989 ] a network topology, or by exploring a number of topologies [ Opitz and Shavlik, 1993 ] . <p> To mitigate overfitting, for the digit-recognition data we added a sum-of-weight-magnitudes complexity term to the cost function (which is equivalent to using weight decay [ Hinton, 1989 ] ), while for the protein-folding data we used validation sets <ref> [ Lang et al., 1990 ] </ref> (tests with weight decay produced poor results compared to the standard results for this data set). However, we did not use the cost function when evaluating the trained networks.
Reference: [ Le Cun et al., 1989 ] <author> Y. Le Cun, J. Denker, and S. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Others "avoid" overfitting using stopping criteria like patience [ Fahlman and Lebiere, 1990 ] or by employing validation sets [ Lang et al., 1990 ] . Some research has focused on choosing an appropriate network by growing [ Fahlman and Lebiere, 1990 ] or shrinking <ref> [ Le Cun et al., 1989 ] </ref> a network topology, or by exploring a number of topologies [ Opitz and Shavlik, 1993 ] . Combining multiple networks 1 has the advantage of achieving good generalization without having to do a great deal of empirical exploration or complex coding.
Reference: [ Leng et al., 1994 ] <author> B. Leng, B. Buchanan, and H. Nicholas. </author> <title> Protein secondary structure prediction using two-level case-based reasoning. </title> <editor> In J. </editor> <booktitle> of Computational Biology, </booktitle> <pages> pages 25-38, </pages> <year> 1994. </year>
Reference-contexts: For this set of protein-folding data, the best reported error rates are 37.3% using standard neural networks [ Qian and Sejnowski, 1988 ] , 36.6% using a knowledge-based neural network [ Maclin and Shavlik, 1993 ] , and 30.7% using a case-based reasoning algorithm (and a somewhat larger data format) <ref> [ Leng et al., 1994 ] </ref> . Our results are better than all but the case-based reasoning results (which also used a different input encoding). In Figure 5 we report error as a function of the number of networks combined.
Reference: [ Lincoln and Skrzypek, 1990 ] <author> W. Lincoln and J. Skrzy-pek. </author> <title> Synergy of clustering multiple back propagation networks. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 650-659. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Figure 2 shows a framework for combining multiple networks. The choice of a function for combining predictions is important [ Kearns and Seung, 1995 ] . Examples of combination functions include voting schemes [ Hansen and Salamon, 1990 ] , simple averages <ref> [ Lincoln and Skrzypek, 1990 ] </ref> , weighted average schemes [ Perrone and Cooper, 1994; Rogova, 1994 ] , and schemes for training combiners [ Rost and Sander, 1993; Wolpert, 1992; Zhang et al., 1992 ] .
Reference: [ Maclin and Shavlik, 1993 ] <author> R. Maclin and J. Shavlik. </author> <title> Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 195-215, </pages> <year> 1993. </year>
Reference-contexts: For this set of protein-folding data, the best reported error rates are 37.3% using standard neural networks [ Qian and Sejnowski, 1988 ] , 36.6% using a knowledge-based neural network <ref> [ Maclin and Shavlik, 1993 ] </ref> , and 30.7% using a case-based reasoning algorithm (and a somewhat larger data format) [ Leng et al., 1994 ] . Our results are better than all but the case-based reasoning results (which also used a different input encoding).
Reference: [ Mani, 1991 ] <author> G. Mani. </author> <title> Lowering variance of decisions by using artificial neural network portfolios. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 484-486, </pages> <year> 1991. </year>
Reference-contexts: We chose as our method for combining networks the scheme of simply averaging the values of the corresponding output units. Results from portfolio theory suggest this method provides a good solution with minimal extra work <ref> [ Mani, 1991 ] </ref> . network predictions. Hansen and Salamon [ 1990 ] explored the value of combining multiple networks. They demonstrated that under certain assumptions, generalization will increase as more networks are combined.
Reference: [ Moody and Darken, 1988 ] <author> J. Moody and C. Darken. </author> <title> Learning with localized receptive fields. </title> <booktitle> In Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 133-143, </pages> <address> Pittsburgh, PA, </address> <year> 1988. </year>
Reference: [ Moody and Darken, 1989 ] <author> J. Moody and C. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference: [ Nguyen and Widrow, 1990 ] <author> D. Nguyen and B. Widrow. </author> <title> Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights. In Procs. </title> <journal> IJCNN-90, </journal> <volume> volume 3, </volume> <pages> pages 21-26, </pages> <address> San Diego, CA, </address> <year> 1990. </year>
Reference: [ Opitz and Shavlik, 1993 ] <author> D. Opitz and J. Shavlik. </author> <title> Heuristically expanding knowledge-based neural networks. </title> <booktitle> In Procs. IJCAI-93, </booktitle> <pages> pages 1360-1365, </pages> <address> Cham-bery, France, </address> <year> 1993. </year>
Reference-contexts: Some research has focused on choosing an appropriate network by growing [ Fahlman and Lebiere, 1990 ] or shrinking [ Le Cun et al., 1989 ] a network topology, or by exploring a number of topologies <ref> [ Opitz and Shavlik, 1993 ] </ref> . Combining multiple networks 1 has the advantage of achieving good generalization without having to do a great deal of empirical exploration or complex coding.
Reference: [ Perrone and Cooper, 1994 ] <author> M. Perrone and L. Cooper. </author> <title> When networks disagree: Ensemble method for neural networks. </title> <editor> In R. Mammone, editor, </editor> <title> Artificial Neural Networks for Speech and Vision. </title> <publisher> Chapman and Hall, </publisher> <year> 1994. </year>
Reference-contexts: The choice of a function for combining predictions is important [ Kearns and Seung, 1995 ] . Examples of combination functions include voting schemes [ Hansen and Salamon, 1990 ] , simple averages [ Lincoln and Skrzypek, 1990 ] , weighted average schemes <ref> [ Perrone and Cooper, 1994; Rogova, 1994 ] </ref> , and schemes for training combiners [ Rost and Sander, 1993; Wolpert, 1992; Zhang et al., 1992 ] . We chose as our method for combining networks the scheme of simply averaging the values of the corresponding output units.
Reference: [ Perrone, 1992 ] <author> M. Perrone. </author> <title> A soft-competitive splitting rule for adaptive tree-structured neural networks. </title> <booktitle> In Procs. IJCNN-92, </booktitle> <pages> pages 689-693, </pages> <address> Baltimore, MD, </address> <year> 1992. </year>
Reference: [ Qian and Sejnowski, 1988 ] <author> N. Qian and T. Sejnowski. </author> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> J. of Molecular Biology, </journal> <volume> 202 </volume> <pages> 865-884, </pages> <year> 1988. </year>
Reference-contexts: As a baseline for comparison, note that the best reported error rate for the digit-recognition task is 14.9%, using decision lists [ Shen, 1992 ] . For this set of protein-folding data, the best reported error rates are 37.3% using standard neural networks <ref> [ Qian and Sejnowski, 1988 ] </ref> , 36.6% using a knowledge-based neural network [ Maclin and Shavlik, 1993 ] , and 30.7% using a case-based reasoning algorithm (and a somewhat larger data format) [ Leng et al., 1994 ] .
Reference: [ Quinlan, 1987 ] <author> J. R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> Int. Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: The difficulty with achieving good generalization is that a learner cannot measure generalization directly; instead, the learner relies on its inductive bias to hopefully produce an accurate classifier. A number of schemes have been introduced to address this ubiquitous problem, ranging from tree-pruning in decision trees <ref> [ Quinlan, 1987 ] </ref> to the use of complexity terms in neural networks [ Hinton, 1989 ] . In this paper we present a way to address the generalization problem for neural networks trained by backpropagation [ Rumelhart et al., 1986 ] .
Reference: [ Reilly et al., 1987 ] <author> R. Reilly, C. Scofield, C. Elbaum, and L. Cooper. </author> <title> Learning system architectures composed of multiple learning modules. </title> <booktitle> In Procs. ICNN-87, </booktitle> <pages> pages 495-503, </pages> <address> San Diego, CA, </address> <year> 1987. </year>
Reference: [ Rogova, 1994 ] <author> G. Rogova. </author> <title> Combining the results of several neural-network classifiers. </title> <booktitle> Neural Networks, </booktitle> <volume> 7 </volume> <pages> 777-781, </pages> <year> 1994. </year>
Reference-contexts: The choice of a function for combining predictions is important [ Kearns and Seung, 1995 ] . Examples of combination functions include voting schemes [ Hansen and Salamon, 1990 ] , simple averages [ Lincoln and Skrzypek, 1990 ] , weighted average schemes <ref> [ Perrone and Cooper, 1994; Rogova, 1994 ] </ref> , and schemes for training combiners [ Rost and Sander, 1993; Wolpert, 1992; Zhang et al., 1992 ] . We chose as our method for combining networks the scheme of simply averaging the values of the corresponding output units.
Reference: [ Rost and Sander, 1993 ] <author> B. Rost and C. Sander. </author> <title> Prediction of protein secondary structure at better than 70% accuracy. </title> <journal> J. of Molecular Biology, </journal> <volume> 232 </volume> <pages> 584-599, </pages> <year> 1993. </year>
Reference: [ Rumelhart and Zipser, 1985 ] <author> D. Rumelhart and D. Zipser. </author> <title> Feature discovery by competitive learning. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 75-112, </pages> <year> 1985. </year>
Reference-contexts: We do this to cause backpropagation to seek local minima not normally encountered using standard initialization (see Figure 1). We explored a number of means to achieve our goal of a diverse set of starting points in weight space; the best one is a method employing competitive learning <ref> [ Rumelhart and Zipser, 1985 ] </ref> to produce class prototypes that are used as hidden units in a network in a method similar to Moody and Darken's [ 1988; 1989 ] . <p> Our solution makes use of the technique of competitive learning, which we outline next. 3 Competitive Learning Competitive learning <ref> [ Rumelhart and Zipser, 1985 ] </ref> is a simple unsupervised learning technique that can be used to partition a set of examples into classes. We start competitive learning by randomly creating a set of class seeds (which are simply points in a feature space).
Reference: [ Rumelhart et al., 1986 ] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart and J. McClel-land, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 1, </volume> <pages> pages 318-363. </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In this paper we present a way to address the generalization problem for neural networks trained by backpropagation <ref> [ Rumelhart et al., 1986 ] </ref> .
Reference: [ Schapire, 1990 ] <author> R. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference: [ Shen, 1992 ] <author> W.-M. Shen. </author> <title> Complementary discrimination learning with decision lists. </title> <booktitle> In Procs. AAAI-92, </booktitle> <pages> pages 153-158, </pages> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: An interesting point to note is that the combination results for our approach are better than the oracle results even though the oracle may seem to be ideal. As a baseline for comparison, note that the best reported error rate for the digit-recognition task is 14.9%, using decision lists <ref> [ Shen, 1992 ] </ref> .
Reference: [ Wolpert, 1992 ] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference: [ Zhang et al., 1992 ] <author> X. Zhang, J. Mesirov, and D. Waltz. </author> <title> Hybrid system for protein secondary structure prediction. </title> <journal> J. of Molecular Biology, </journal> <volume> 225 </volume> <pages> 1049-1063, </pages> <year> 1992. </year>
References-found: 38

