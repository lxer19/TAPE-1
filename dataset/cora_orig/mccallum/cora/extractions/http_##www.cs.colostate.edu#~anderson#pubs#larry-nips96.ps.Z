URL: http://www.cs.colostate.edu/~anderson/pubs/larry-nips96.ps.Z
Refering-URL: http://www.cs.colostate.edu/~anderson/pubs/pubs.html
Root-URL: 
Email: email: fpyeatt,howe,andersong@cs.colostate.edu  
Title: Learning Coordinated Behaviors for Control of a Simulated Race Car  
Author: Larry D. Pyeatt Adele E. Howe Charles W. Anderson 
Keyword: category: Control, navigation and planning  
Note: This work is not in submission or press to any other conference.  
Address: Ft. Collins, CO 80523  
Affiliation: Computer Science Dept, Colorado State Univ.,  
Abstract: The demands of rapid response and the complexity of many environments make it difficult to decompose, tune and coordinate reactive behaviors while ensuring consistency. We hypothesize that complex behaviors should be decomposed into separate behaviors resident in separate networks, coordinated through a higher level controller. To explore these issues, we have implemented a neural network architecture as the reactive component of a two layer control system for a simulated race car. By varying the architecture, we tested whether decomposing reactivity into separate behaviors leads to superior overall performance and learning convergence. Based on these results, we further modified the architecture to produce a race car that is competitive with publicly available solutions.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Charles W. Anderson. </author> <title> Strategy learning with multilayer connectionist representations. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 103-114, </pages> <year> 1989. </year>
Reference-contexts: Our basic architecture consists of two low level controllers and a high level coordinating mechanism. One low level controller is heuristic and the other is implemented as an actor-critic reinforcement learning architecture similar to those used by Barto, Sutton and Watkins [2], Anderson <ref> [1] </ref>, and Lin [6]. The high level coordinating mechanism switches vehicle control between the heuristic strategy and the reinforcement learning mechanism as appropriate. <p> The possible steering commands are to adjust steering to the left by 0.05 radians, do not adjust steering, or adjust steering to the right by 0.05 radians. The reinforcement learning network is an actor-critic architecture that uses two feed-forward neural networks, both trained with the back-propagation temporal difference algorithm <ref> [1, 9] </ref>. A stochastic action selector forces the action network to explore the space of possible actions by occasionally choosing an action that is not the one selected by the action network. The probability of choosing a random action is set to 0.01 for our experiments.
Reference: [2] <author> A.G. Barto, R.S. Sutton, and C.J.C.H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical report, COINS Technical Report 89-95, </type> <institution> Dept. of Computer and Information Science, University of Massachusetts, </institution> <year> 1989. </year>
Reference-contexts: Our basic architecture consists of two low level controllers and a high level coordinating mechanism. One low level controller is heuristic and the other is implemented as an actor-critic reinforcement learning architecture similar to those used by Barto, Sutton and Watkins <ref> [2] </ref>, Anderson [1], and Lin [6]. The high level coordinating mechanism switches vehicle control between the heuristic strategy and the reinforcement learning mechanism as appropriate.
Reference: [3] <author> Daniel Bullock, Stephen Grossberg, and Frank H. Guenther. </author> <title> A self-organizing neural network model for redundant sensory-motor control, motor equivalence, and tool use. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 91-96, </pages> <address> Baltimore, 1992. </address> <publisher> IEEE. </publisher>
Reference-contexts: The adaptability and noise rejection of a neural based robot controller was studied by Poo [8], who found that the neural based controller performed better than a standard model based control algorithm. Bullock <ref> [3] </ref> demonstrated a neural network system for positioning a robotic manipulator with various tools attached and under various hardware failure conditions. Although previous work has demonstrated that it is possible to use neural networks for robotic control, the question remains as to how to build a neural network controller.
Reference: [4] <author> R. F. Comoglio and A. S. Pandya. </author> <title> Using a cerebellar model arithmetic computer (cmac) neural network to control an autonomous underwater vehicle. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 781-786, </pages> <address> Baltimore, 1992. </address> <publisher> IEEE. </publisher>
Reference-contexts: Neural network approaches have been used for controlling autonomous robots for several years. Krishnaswamy [5] demonstrated a structured neural network approach to controlling a robotic manipulator. Liu [7] used neural networks to control grasping of a robotic hand. Comoglio <ref> [4] </ref> presented results of simulations for the control of an autonomous underwater vehicle using neural networks. The adaptability and noise rejection of a neural based robot controller was studied by Poo [8], who found that the neural based controller performed better than a standard model based control algorithm.
Reference: [5] <author> Gita Drishnaswamy, Marcelo H. Ang Jr., and Gerry B. Andeen. </author> <title> Structured neural-network approach to robot motion control. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 1059-1066. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: The neural network reactions use a form of reinforcement learning to improve performance. The high level coordination mechanism heuristically switches control between the low level behaviors and assigns reinforcements to the reinforcement learning components. Neural network approaches have been used for controlling autonomous robots for several years. Krishnaswamy <ref> [5] </ref> demonstrated a structured neural network approach to controlling a robotic manipulator. Liu [7] used neural networks to control grasping of a robotic hand. Comoglio [4] presented results of simulations for the control of an autonomous underwater vehicle using neural networks.
Reference: [6] <author> Long-H Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> 8(3/4):69-97, 1992. 
Reference-contexts: Our basic architecture consists of two low level controllers and a high level coordinating mechanism. One low level controller is heuristic and the other is implemented as an actor-critic reinforcement learning architecture similar to those used by Barto, Sutton and Watkins [2], Anderson [1], and Lin <ref> [6] </ref>. The high level coordinating mechanism switches vehicle control between the heuristic strategy and the reinforcement learning mechanism as appropriate.
Reference: [7] <author> Huan Liu, Thea Iberall, and George A. Beckey. </author> <title> Neural network architecture for robot hand control. </title> <booktitle> In Proceedings of IEEE International Conference on Neural Networks. IEEE, </booktitle> <month> July </month> <year> 1989. </year>
Reference-contexts: The high level coordination mechanism heuristically switches control between the low level behaviors and assigns reinforcements to the reinforcement learning components. Neural network approaches have been used for controlling autonomous robots for several years. Krishnaswamy [5] demonstrated a structured neural network approach to controlling a robotic manipulator. Liu <ref> [7] </ref> used neural networks to control grasping of a robotic hand. Comoglio [4] presented results of simulations for the control of an autonomous underwater vehicle using neural networks.
Reference: [8] <author> A.N. Poo, M.H. Ang Jr., C.L. Teo, and Qing Li. </author> <title> Performance of a neuro-model-based robot controller: adaptability and noise rejection. </title> <journal> Intelligent Systems Engineering, </journal> <volume> 1(1) </volume> <pages> 50-62, </pages> <year> 1992. </year>
Reference-contexts: Liu [7] used neural networks to control grasping of a robotic hand. Comoglio [4] presented results of simulations for the control of an autonomous underwater vehicle using neural networks. The adaptability and noise rejection of a neural based robot controller was studied by Poo <ref> [8] </ref>, who found that the neural based controller performed better than a standard model based control algorithm. Bullock [3] demonstrated a neural network system for positioning a robotic manipulator with various tools attached and under various hardware failure conditions.
Reference: [9] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: The possible steering commands are to adjust steering to the left by 0.05 radians, do not adjust steering, or adjust steering to the right by 0.05 radians. The reinforcement learning network is an actor-critic architecture that uses two feed-forward neural networks, both trained with the back-propagation temporal difference algorithm <ref> [1, 9] </ref>. A stochastic action selector forces the action network to explore the space of possible actions by occasionally choosing an action that is not the one selected by the action network. The probability of choosing a random action is set to 0.01 for our experiments.
Reference: [10] <author> Mitchell E. Timin. </author> <title> Robot Auto Racing Simulator. </title> <note> Anonymous ftp from ftp.ijs.com:/rars, </note> <year> 1995. </year>
References-found: 10

