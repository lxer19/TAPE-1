URL: http://www.tc.cornell.edu/~liao/thesis.ps.Z
Refering-URL: http://www.tc.cornell.edu/~liao/liao.html
Root-URL: http://www.tc.cornell.edu
Title: ALGORITHMS FOR LINEAR PROGRAMMING VIA WEIGHTED CENTERS  
Author: Ai-Ping Liao 
Degree: A Dissertation Presented to the Faculty of the Graduate School  in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy by  
Date: January 1992  
Affiliation: of Cornell University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> I. Adler, M. G. C. Resende, G. Veiga, and N. Karmarkar. </author> <title> An implementation of Karmarkar's algorithm for linear programming. </title> <journal> Mathematical Programming, </journal> <volume> 44 </volume> <pages> 297-335, </pages> <year> 1989. </year>
Reference-contexts: In 1984, Karmarkar [28] introduced a new algorithm which is polynomial-time bounded and seems to be a competitor to the simplex method in practice (see, for example, Adler, Resende, Veiga, and Karmarkar <ref> [1] </ref> and Monma and Morton [36]). The relations among these methods can be found in Todd [48]. Karmarkar's algorithm and other algorithms developed later on, for example, the path-following methods, generate points which are in the (relative) interior of the feasible region, and force them to approach the optimal solution. <p> Therefore, the solution set of (P f+h ) is (at least) fd = d + (1 ) dj 2 <ref> [0; 1] </ref>g. 2 We note that any point used as a "center" must be uniquely defined. Although the optimal solution set S f+h may not be a singleton, it turns out that x c (d fl ) is 55 unique, so it can be defined as a "center". <p> We need only to show that for any d; d 2 S f+h , we have ADA = A DA : By Lemma 3.2.13, det (A (D + "( D D))A ) = det (ADA ); 8" 2 <ref> [0; 1] </ref>: (3:15) On the other hand, det (A (D + "( D D))A T ) = det (ADA T + "A ( D D)A T ) 1 2 A ( D D)A T (ADA T ) 1 2 : Thus, (3.15) implies det (I + "(ADA ) 2 A ( <p> We therefore change the factors to n, dn, lk, and dl. The problems for the experimental design are as follows. 93 For given factors n, dn, lk, and dl, we take m = n + dn, then generate randomly, according to a uniform distribution on the interval <ref> [1; 1] </ref>, an n by m matrix A; then we generate, according the same distribution, a column and multiply this column by lk and take it as the lower bound l; the upper bound u is simply a column obtained by adding dl to each component of l.
Reference: [2] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1976. </year>
Reference-contexts: The theoretical significance of Khachiyan's result is that it resolved the question LP2 P which had puzzled theoretical computer scientists for many years. Readers unfamiliar with computational complexity may consult Aho, Hopcroft and Ullman <ref> [2] </ref>, Garey and Johnson [17], and Karp [29]. We only mention that classes P and N P are theoretical measures of the difficulty of solving certain problems.
Reference: [3] <author> M. </author> <title> Akgul. Topics in relaxation and ellipsoidal methods. </title> <publisher> Pitman Advanced Publishing Program, </publisher> <year> 1984. </year>
Reference-contexts: special case of the ellipsoids that we are concerned with, therefore, volume (E k+1 ) volume (E k ) volume ( ~ E k+1 ) volume (E k ) = n 1 1 n 2 1 2 1 ) with the last inequality following by, e.g., Corollary 3.7.3 of Akgul <ref> [3] </ref>. 2 The above theorem gives us the convergence rate of the algorithm measured by volume of corresponding ellipsoids.
Reference: [4] <author> D. Avis and V. Chvatal. </author> <title> Notes on Bland's pivoting rule. </title> <journal> Mathematical Programming Study, </journal> <volume> 8 </volume> <pages> 24-34, </pages> <year> 1978. </year>
Reference-contexts: The pulling technique is used in our MATLAB codes of Algorithm 4.3.1 attached in Appendix A. 116 Our limited test results are reported in Table 4.1. The test problems we use are those in Avis and Chvatal <ref> [4] </ref> but with upper bounds on each component of x, namely, max e T x 0 x 10e: N is an nfin matrix with integer elements chosen randomly in the range 1; : : : ; 1000.
Reference: [5] <author> M. Avriel. </author> <title> Nonlinear Programming: Analysis and Methods. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1976. </year>
Reference-contexts: There are many books devoted to nonlinear programming, such as Avriel <ref> [5] </ref>, Dennis and Schnabel [12], and Fletcher [15], to name a few. For a given x, a vector d is called a feasible direction at x if x+d 2 , 8 2 [0; ], for some &gt; 0. <p> Thus h n (d) is a positive linear function of each component d i ; on the other hand, f (d) is a convex function, hence so is f n . By theorem 6.9 of Avriel <ref> [5] </ref>, v n is a pseudo-convex function of each d i and d fl i minimizes v n (thus v) if @v = 0. 2 Since x k c violates the j-th constraint, q 0 (0) &lt; 0 and so @v (d k ) &lt; 0; on the other hand, from
Reference: [6] <author> R. M. Bethea, B. S. Duran, and T. L. Boullion. </author> <title> Statistical Methods. </title> <publisher> Marcel Dekker, Inc., </publisher> <year> 1985. </year>
Reference-contexts: Here we have 4 factors, so a suitable model is the Graeco-Latin square design. The idea of this design can be found, for example, in Devore [13] and Bethea, Duran and Boullion <ref> [6] </ref>. Usually, the use of the Graeco-Latin square design can obtain a tremendous saving in time and money in that the total number of observations is greatly reduced. But this design requires no interactions among the factors.
Reference: [7] <author> R. G. Bland, D. Goldfarb, and M. J. Todd. </author> <title> The ellipsoid method: a survey. </title> <journal> Operations Research, </journal> <volume> 29 </volume> <pages> 1039-1091, </pages> <year> 1981. </year>
Reference-contexts: method due to Dantzig [11], fast in practice, is not theoretically satisfactory (Klee and Minty [33]); on the other hand, the ellipsoid method [30], which was shown theoretically satisfactory for linear programming by Khachiyan [30] [31], is of less use in practice (see, for example, Bland, Goldfarb and Todd 3 <ref> [7] </ref>). In 1984, Karmarkar [28] introduced a new algorithm which is polynomial-time bounded and seems to be a competitor to the simplex method in practice (see, for example, Adler, Resende, Veiga, and Karmarkar [1] and Monma and Morton [36]). The relations among these methods can be found in Todd [48]. <p> The ellipsoid method has to be carried out to a very high precision so that the feasible solutions will not be lost during the constructing of the sequence of ellipsoids, otherwise the algorithm may lead to a wrong conclusion. There is an example in Bland, Goldfarb and Todd <ref> [7] </ref> showing that the convergence of the ellipsoid method can be very slow. Although the ellipsoid method is not efficient in practical use, it is a powerful tool in combinatorial optimization. <p> By scaling and changing variables we can assume, without loss of generality, AA T = I. Thus Algorithm 3.3.1 is a kind of "ball method" <ref> [7] </ref> for which Todd [46] and Goffin [18] show that an exponential number of iterations may be required. <p> This makes the code simpler yet will not affect the statistical analysis much, since we are mainly interested in the number of iterations and the time is a constant multiple of that using Cholesky factorization. There is an example in Bland, Goldfarb and Todd <ref> [7] </ref>, showing that convergence of the standard ellipsoid algorithm can be extremely slow. The example is A T = I , l = u = 0; by using our models, as well as Burrell and Todd's algorithm [8], 92 the solution can be obtained in just one iteration. <p> Since model I is closely related to the ellipsoid method it can be adapted to a polynomial algorithm for (LP) without getting the exact center, see, e.g., Bland, Goldfarb and Todd <ref> [7] </ref>. 105 We are now going to discuss a practical stopping criterion. <p> To overcome this disadvantage we replace h (d) by a strictly convex function B (d) which forms our second model, model II. Similarly, we propose two algorithms for solving this model: a coordinate descent algorithm and a Newton's 121 algorithm. The first one is actually a ball-like method <ref> [7] </ref> which has been shown to be not a polynomial time algorithm. The second algorithm is a combination of Newton's algorithm and a scaling technique, which works well in practice. The statistical analysis shows that this algorithm is very robust numerically. <p> Or in other words, if x c does not approach the optimal set, x c approaches the optimal set linearly. In addition to the pushing technique similar to that using by the sliding objective function method <ref> [7] </ref> and Renegar [39] we also propose a "pulling" technique: we let the lower bound corresponding to the objective be a very large negative number, i.e., l 0 ~ 1 (or u 0 ~ 1 for the maximizing linear program).
Reference: [8] <author> B. P. Burrell and M. J. Todd. </author> <title> The ellipsoid method generates dual variables. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10 </volume> <pages> 688-700, </pages> <year> 1985. </year>
Reference-contexts: In this dissertation, we introduce some new centers and develop from them new algorithms for solving feasibility problems as well as linear programs. In Chapter 2, we study the parallel-cut ellipsoid method proposed by Burrell and Todd <ref> [8] </ref>, and develop a related coordinate descent method using an "up and down" technique which simplifies their treatment. In Chapter 3, we study some new models which exploit homogeneity and introduce associated centers; statistical methods are used here to analyze the numerical behavior of these methods for feasibility problems. <p> We also denote r := l+u ul for convenience. In <ref> [8] </ref> Burrell and Todd proposed a parallel-cut ellipsoid algorithm based on the result of Todd [47]. <p> As a matter of fact, the "down and up" technique is also possible; that is, when we meet a non-canonical case, instead of seeking help in a higher dimensional space, we seek help in a lower dimensional space. This leads to the Burrell-Todd algorithm <ref> [8] </ref>. <p> Then take fl = argminfv (d k + e j ) : 0g where v (d) is defined with the new j-th bounds, and d k+1 = d k + fl e j . 2 Actually, Burrell and Todd <ref> [8] </ref> give even better updated bounds by considering the dual variables. This algorithm also decreases the volume of the corresponding ellipsoids by a factor of exp ( 1 2 (n+1) ) at each step. In the above algorithms, the significant decrease occurs in the so called "canonical case". <p> There is an example in Bland, Goldfarb and Todd [7], showing that convergence of the standard ellipsoid algorithm can be extremely slow. The example is A T = I , l = u = 0; by using our models, as well as Burrell and Todd's algorithm <ref> [8] </ref>, 92 the solution can be obtained in just one iteration. This comparison might be not appropriate, but it does show the difference between our model and the ellipsoid method. In the following we investigate the numerical behavior of model II by using Newton's method, i.e., Algorithm 3.3.2.. <p> 0: 101 If the input data are all integers, (ILP) is equivalent to min ~c T x (l =)0 x u: (ILPB) where u = 2 L e with L the input length of (ILP), which can be easily adapted to a form of (LP) (see, e.g., Burrell and Todd <ref> [8] </ref>). So, theoretically speaking, our form (LP) is not restrictive. <p> The dual of (LP) is: max l T y 1 u T y 2 y 1 ; y 2 0: By the results of Burrell and Todd <ref> [8] </ref>, (DP) is equivalent to max (y) := l T y + u T y (DP ) 1 where y := (maxf0; y i g), y + := (maxf0; y i g), and so y = y + y . <p> By taking y = (c (ADA ) c) 2 D (A z r) where z = x c c T (ADA T ) 1 c) 1 2 (ADA T ) 1 c and d is such that f (d) = 1, Burrell and Todd <ref> [8] </ref> show that (y) c T z = c T x c (c T (ADA T ) 1 c) 2 : They also give a technique to get a dual feasible solution ~y such that (~y) = (y). <p> 440 q = 10 3 &gt; 183 &gt; 228 &gt; 440 q = 10 5 215 183 294 q = 10 7 145 217 253 Chapter 5 Summary And Conclusions In this thesis we have analyzed, and tested, some generalized models, developed from Burrell and Todd's parallel cut ellipsoid method <ref> [8] </ref>, for linear systems and, later, proposed some algorithms for solving linear programming problems via these models. Foremost among these is the idea of generalizing Burrell and Todd's approach [8] which is closely related to the coordinate descent method so that Newton's method can be employed. <p> Conclusions In this thesis we have analyzed, and tested, some generalized models, developed from Burrell and Todd's parallel cut ellipsoid method <ref> [8] </ref>, for linear systems and, later, proposed some algorithms for solving linear programming problems via these models. Foremost among these is the idea of generalizing Burrell and Todd's approach [8] which is closely related to the coordinate descent method so that Newton's method can be employed. We explain Burrell and Todd's work [8] in Chapter 2 and propose a related coordinate descent method using an "up and down" technique which simplifies their treatment. <p> Foremost among these is the idea of generalizing Burrell and Todd's approach <ref> [8] </ref> which is closely related to the coordinate descent method so that Newton's method can be employed. We explain Burrell and Todd's work [8] in Chapter 2 and propose a related coordinate descent method using an "up and down" technique which simplifies their treatment. This simpler approach is an aid throughout this thesis. <p> We also explain why it is necessary to update the bounds in Burrell and Todd's algorithm <ref> [8] </ref> to get a polynomial bound. We develop our models in Chapter 3 with the analysis in Chapter 2. <p> The first model, model I, is thus established by using F (d) = f (d) + h (d) as the objective function. The advantage of this model is that it is a convex program as well as preserving the advantages of Burrell and Todd's approach <ref> [8] </ref>. We prove the existence and uniqueness of the smallest ellipsoid of the form (2.5) which contains the feasible region P . The center of this smallest ellipsoid is proved to be an interior point of P ; it is our first kind of center. <p> We thus hope the effects of this thesis fall into two main categories. By our work in Chapter 2, we hope to have made Burrell and Todd's algorithm <ref> [8] </ref> more clear 122 and accessible to other applications.
Reference: [9] <author> V. Chvatal. </author> <title> Linear Programming. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1980. </year>
Reference-contexts: So, theoretically speaking, our form (LP) is not restrictive. On the other hand, since "many linear programming problems involve explicit upper bounds on individual variables [in the standard form]" (Chvatal <ref> [9] </ref>) especially, the upper bounds may stand for limits on resources we thus often, in practice, are confronted with linear programming problems with the following form: min c T x (l =)0 x u which, again, can be converted to our form (LP).
Reference: [10] <author> T. F. Coleman and C. Van Loan. </author> <title> Handbook for Matrix Computations. </title> <publisher> SIAM, </publisher> <year> 1988. </year>
Reference-contexts: The third method is QR decomposition; using Householder orthogonalization, the QR decomposition of D 2 A T needs n 2 (m n 3 ) flops. Among these three possible choices the Cholesky Decomposition seems the best. Our computer code is written in MATLAB ([35], <ref> [10] </ref>) and uses LU decomposition because MATLAB finds x = Anb by using the LU decomposition for A.
Reference: [11] <author> G. B. Dantzig. </author> <title> Maximization of a linear function of variables subject to linear inequalities. </title> <editor> In Tj. C. Koopmans, editor, </editor> <booktitle> Activity Analysis of Production and Allocation, </booktitle> <pages> pages 339-347, </pages> <address> New York, 1951. </address> <publisher> Wiley. </publisher> <pages> 132 133 </pages>
Reference-contexts: Of course, a good model without an efficient way to solve it would be little use for practice. The extraordinary computational efficiency and robustness of the simplex method developed by G. B. Dantzig <ref> [11] </ref> in 1947, together with the availability of high-speed computers, allow linear programming problems to be solved quickly. Thus linear programming has become one of the most powerful and widely used models in applied mathematics. 1 2 We also mention that L. V. <p> But we note that being theoretically satisfactory is not equivalent to being satisfactory in practice. The simplex method due to Dantzig <ref> [11] </ref>, fast in practice, is not theoretically satisfactory (Klee and Minty [33]); on the other hand, the ellipsoid method [30], which was shown theoretically satisfactory for linear programming by Khachiyan [30] [31], is of less use in practice (see, for example, Bland, Goldfarb and Todd 3 [7]).
Reference: [12] <author> J. E. Dennis, Jr. and R. B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1983. </year>
Reference-contexts: There are many books devoted to nonlinear programming, such as Avriel [5], Dennis and Schnabel <ref> [12] </ref>, and Fletcher [15], to name a few. For a given x, a vector d is called a feasible direction at x if x+d 2 , 8 2 [0; ], for some &gt; 0. <p> For measuring the ultimate speed of "approaching" an optimal solution and generally determining the relative advantage of one algorithm over the other, we give some definitions of convergence rates <ref> [12] </ref> in the following.
Reference: [13] <author> J. L. Devore. </author> <title> Probability and Statistics. </title> <publisher> Brooks/Cole Publishing Company, </publisher> <year> 1991. </year>
Reference-contexts: Here we have 4 factors, so a suitable model is the Graeco-Latin square design. The idea of this design can be found, for example, in Devore <ref> [13] </ref> and Bethea, Duran and Boullion [6]. Usually, the use of the Graeco-Latin square design can obtain a tremendous saving in time and money in that the total number of observations is greatly reduced. But this design requires no interactions among the factors.
Reference: [14] <author> D. Z. Du and X. S. Zhang. </author> <title> Global convergence of Rosen's gradient projection method. </title> <journal> Mathematical Programming, </journal> <volume> 44 </volume> <pages> 357-366, </pages> <year> 1989. </year>
Reference-contexts: [41] once the iteration point hits the region := fd 2 R m + : d i *; for some ig, where * &gt; 0 is a small number and, under (A1), any cluster point of the sequence fd k g is optimal by the global convergence of Rosen's algorithm <ref> [14] </ref>. If the algorithm never hits , then the algorithm is a descent algorithm for the unconstrained problem and it is obvious that any cluster point d fl of the sequence fd k g is optimal and d fl &gt; 0. It is rather complicated to analyze its convergence rate.
Reference: [15] <author> R. Fletcher. </author> <title> Practical Methods of Optimization. </title> <publisher> John Wiley & Sons, Ltd, </publisher> <year> 1981. </year>
Reference-contexts: There are many books devoted to nonlinear programming, such as Avriel [5], Dennis and Schnabel [12], and Fletcher <ref> [15] </ref>, to name a few. For a given x, a vector d is called a feasible direction at x if x+d 2 , 8 2 [0; ], for some &gt; 0.
Reference: [16] <author> P. Gacs and L. Lovasz. </author> <title> Khachiyan's algorithm for linear programming. </title> <journal> Mathematical Programming Study, </journal> <volume> 14 </volume> <pages> 61-68, </pages> <year> 1981. </year>
Reference-contexts: A T x l 0 := l + 2 L 0 (2:17) which is equivalent to l 2 e =: l A x u := u + 2 e (2:18) and let P p be its solution set; then one can prove (see, e.g., Khachian [31] or Gacs and Lovasz <ref> [16] </ref>) that P = ; if and only if P p = ; and that, if P 6= ;, there is some y fl with B (y fl ; 2 2L 0 ) P p .
Reference: [17] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of N P -Completeness. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: The theoretical significance of Khachiyan's result is that it resolved the question LP2 P which had puzzled theoretical computer scientists for many years. Readers unfamiliar with computational complexity may consult Aho, Hopcroft and Ullman [2], Garey and Johnson <ref> [17] </ref>, and Karp [29]. We only mention that classes P and N P are theoretical measures of the difficulty of solving certain problems.
Reference: [18] <author> J. L. Goffin. </author> <title> On the non-polynomiality of the relaxation method for system of inequalities. </title> <type> Technical report, </type> <institution> Faculty of Management, McGill University, Montreal, Quebec (November), </institution> <year> 1979. </year>
Reference-contexts: By scaling and changing variables we can assume, without loss of generality, AA T = I. Thus Algorithm 3.3.1 is a kind of "ball method" [7] for which Todd [46] and Goffin <ref> [18] </ref> show that an exponential number of iterations may be required.
Reference: [19] <author> D. Goldfarb and M. J. Todd. </author> <title> Modifications and implementation of the ellipsoid algorithm for linear programming. </title> <journal> Mathematical Programming, </journal> <volume> 23 </volume> <pages> 1-19, </pages> <year> 1982. </year>
Reference-contexts: As for the convergence rate measured by the objective value, we have, similar to Theorem 6.1 in Goldfarb and Todd <ref> [19] </ref>, 104 Theorem 4.2.2 Suppose there is a ball B with a radius of contained in the feasible region P of (LP).
Reference: [20] <author> D. Goldfarb and M. J. Todd. </author> <title> Linear programming. </title> <editor> In G. L. Nemhauser, A. H. G. Rinnooy Kan, and M. J. Todd, editors, </editor> <booktitle> Optimization, </booktitle> <pages> pages 73-170, </pages> <address> Amsterdam, New York, Oxford, Tokyo, 1989. </address> <publisher> North-Holland. </publisher>
Reference-contexts: The concepts "primal" and "dual" themselves are a "dual-pair", in other words, the dual of the dual of the primal is the primal again. The close relationships between these two problems are described in the following theorems. (See, for example, Goldfarb and Todd <ref> [20] </ref>.) Theorem 1.2.1 (Weak Duality Theorem) If x is primal feasible and y is dual feasible, then b T y c T x.
Reference: [21] <author> G. H. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1983. </year>
Reference-contexts: For the general case, instead of calculating (ADA T ) 1 directly, an equivalent linear system is solved. There are many methods for solving linear systems, see, for example, Golub and Van Loan <ref> [21] </ref>. Usually, some decomposition methods are used to reduce the linear system to a simpler system and the solution of the original system can be obtained from that of this simpler system. We would like to mention here three possible decomposition methods.
Reference: [22] <author> M. Grotschel, L. Lovasz, and A. Schrijver. </author> <title> The ellipsoid method and its consequences in combinatorial optimization. </title> <journal> Combinatorica, </journal> <volume> 1 </volume> <pages> 169-197, </pages> <year> 1981. </year>
Reference-contexts: Although the ellipsoid method is not efficient in practical use, it is a powerful tool in combinatorial optimization. The readers interested in this topic are referred to Grotschel, Lovasz and Schrijver <ref> [22] </ref> [23]. 1.2 Linear Programming The linear programming problem is that of minimizing (or maximizing) a linear objective function subject to a linear system of equalities or inequalities. <p> Then the bound becomes 24n 5 (n + 1)L. Finally, we note that a feasible solution of (2.1) can be obtained in polynomial time from one of (2.18) using a rounding method (see Grotschel, Lovasz and Schrijver <ref> [22] </ref>). If l j ; u j do not satisfy fi 1 4 , the constant decrease factor might not be bounded away from 1.
Reference: [23] <author> M. Grotschel, L. Lovasz, and A. Schrijver. </author> <title> Geometric Algorithms and Combinatorial Optimization. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Although the ellipsoid method is not efficient in practical use, it is a powerful tool in combinatorial optimization. The readers interested in this topic are referred to Grotschel, Lovasz and Schrijver [22] <ref> [23] </ref>. 1.2 Linear Programming The linear programming problem is that of minimizing (or maximizing) a linear objective function subject to a linear system of equalities or inequalities.
Reference: [24] <author> R. A. Horn and C. R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <year> 1990. </year>
Reference-contexts: We first cite Cauchy's formula which is a special case of the Cauchy-Binet formula that can be found, for example, in Horn and Johnson <ref> [24] </ref>. Lemma 2.4.1 (Cauchy's Formula) Let A be an n by m and B be an m by n matrix, m &gt; n, and let C = AB. <p> The following result can be found, for example, in <ref> [24] </ref>. Lemma 3.2.4 If B and C are m by m positive semidefinite matrices, so is B ffi C . Proof. B is positive semidefinite, so there is a m by m matrix E such that B = EE T .
Reference: [25] <author> A. S. </author> <title> Householder. The Theory of Matrices in Numerical Analysis. </title> <address> Ginn(Blaisdell), </address> <year> 1964. </year>
Reference-contexts: Suppose that the j-th constraint is violated by the current center x c (d). By the argument in the previous section, it is enough to deal with case (i); we thus assume that fi 1 The following two rank-1 update formulae can be found, for example, in Householder <ref> [25] </ref>. Lemma 2.3.1 (Sherman-Morrison Formula) Let u; v be two n-vectors, and M be a nonsingular n fi n matrix. <p> For convenience, we call these assumptions (A2): 68 (A2) P 6= ; and any n columns of A are linearly independent. The following general Sherman-Morrison-Woodbury Formula can be found in Householder <ref> [25] </ref>.
Reference: [26] <author> F. John. </author> <title> Extremum problems with inequalities as subsidiary conditions. </title> <booktitle> In Studies and Essays(Courant Anniversary Volume), </booktitle> <publisher> Interscience, </publisher> <address> New York, </address> <year> 1948. </year> <month> 134 </month>
Reference-contexts: We note that John's result <ref> [26] </ref> shows that for every convex polytope P R n the minimum volume ellipsoid containing P exists and is unique.
Reference: [27] <author> L. V. </author> <title> Kantorovich. Mathematical Methods of Organizing and Planning Production. </title> <institution> Publication House of the Leningrad State University, </institution> <year> 1939. </year>
Reference-contexts: B. Dantzig [11] in 1947, together with the availability of high-speed computers, allow linear programming problems to be solved quickly. Thus linear programming has become one of the most powerful and widely used models in applied mathematics. 1 2 We also mention that L. V. Kantorovich <ref> [27] </ref>, a Russian mathematician, had proposed linear programming models for production planning and a rudimentary algorithm for their solution as early as 1939.
Reference: [28] <author> N. Karmarkar. </author> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Combinatorica, </journal> <volume> 4 </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: Through the years, new applications of linear programming arose. The many applications, especially present-day large-scale linear programming problems, stimulated further research for new efficient algorithms theoretically as well as practically; thus were developed the ellipsoid method [30], Karmarkar's projective algorithm <ref> [28] </ref>, and, arising from it, other interior point methods (see, for example, Todd [48]). Solving linear programming problems usually means finding a sequence fx k g such that x k goes to x fl , an optimal solution. <p> In 1984, Karmarkar <ref> [28] </ref> introduced a new algorithm which is polynomial-time bounded and seems to be a competitor to the simplex method in practice (see, for example, Adler, Resende, Veiga, and Karmarkar [1] and Monma and Morton [36]). The relations among these methods can be found in Todd [48].
Reference: [29] <author> R. M. Karp. </author> <title> On the computational complexity of combinatorial problems. </title> <journal> Networks, </journal> <volume> 5 </volume> <pages> 45-68, </pages> <year> 1975. </year>
Reference-contexts: The theoretical significance of Khachiyan's result is that it resolved the question LP2 P which had puzzled theoretical computer scientists for many years. Readers unfamiliar with computational complexity may consult Aho, Hopcroft and Ullman [2], Garey and Johnson [17], and Karp <ref> [29] </ref>. We only mention that classes P and N P are theoretical measures of the difficulty of solving certain problems.
Reference: [30] <author> L. G. Khachiyan. </author> <title> A polynomial algorithm for linear programming. </title> <journal> Doklady Akad. Nauk. USSR, </journal> <volume> 244(5) </volume> <pages> 1093-1096, </pages> <year> 1979. </year>
Reference-contexts: Through the years, new applications of linear programming arose. The many applications, especially present-day large-scale linear programming problems, stimulated further research for new efficient algorithms theoretically as well as practically; thus were developed the ellipsoid method <ref> [30] </ref>, Karmarkar's projective algorithm [28], and, arising from it, other interior point methods (see, for example, Todd [48]). Solving linear programming problems usually means finding a sequence fx k g such that x k goes to x fl , an optimal solution. <p> But we note that being theoretically satisfactory is not equivalent to being satisfactory in practice. The simplex method due to Dantzig [11], fast in practice, is not theoretically satisfactory (Klee and Minty [33]); on the other hand, the ellipsoid method <ref> [30] </ref>, which was shown theoretically satisfactory for linear programming by Khachiyan [30] [31], is of less use in practice (see, for example, Bland, Goldfarb and Todd 3 [7]). <p> The simplex method due to Dantzig [11], fast in practice, is not theoretically satisfactory (Klee and Minty [33]); on the other hand, the ellipsoid method <ref> [30] </ref>, which was shown theoretically satisfactory for linear programming by Khachiyan [30] [31], is of less use in practice (see, for example, Bland, Goldfarb and Todd 3 [7]). <p> This modified method is computationally implementable. They also point out that this ellipsoid method is a special case of Shor's algorithm [42] with space dilation in the direction of the subgradient. Shor [43] independently developed the ellipsoid method. Later, in 1979, Khachian <ref> [30] </ref> showed that the ellipsoid method is a polynomial time algorithm for solving linear programming problems. Renegar [39] uses the "analytical center" (see also Sonnevend [44]) to develop his algorithm for linear programming problems, which is polynomial time bounded. The analytical center is easier to approximate compared with the centroid.
Reference: [31] <author> L. G. Khachiyan. </author> <title> Polynomial algorithms for linear programming. </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 20 </volume> <pages> 53-72, </pages> <year> 1980. </year>
Reference-contexts: The simplex method due to Dantzig [11], fast in practice, is not theoretically satisfactory (Klee and Minty [33]); on the other hand, the ellipsoid method [30], which was shown theoretically satisfactory for linear programming by Khachiyan [30] <ref> [31] </ref>, is of less use in practice (see, for example, Bland, Goldfarb and Todd 3 [7]). <p> The ellipsoid method for linear programming is an outgrowth of the method of central sections of Levin [34] and Newman [38] and is based on the ellipsoid methods of Shor [43], Yudin and Nemirovskii [50] for convex programming. Khachiyan ([30] and <ref> [31] </ref>) showed that this method is a polynomial time algorithm for solving linear programming problems; it thus became the first polynomial algorithm in linear programming. The theoretical significance of Khachiyan's result is that it resolved the question LP2 P which had puzzled theoretical computer scientists for many years. <p> u + 2 L 0 A T x l 0 := l + 2 L 0 (2:17) which is equivalent to l 2 e =: l A x u := u + 2 e (2:18) and let P p be its solution set; then one can prove (see, e.g., Khachian <ref> [31] </ref> or Gacs and Lovasz [16]) that P = ; if and only if P p = ; and that, if P 6= ;, there is some y fl with B (y fl ; 2 2L 0 ) P p .
Reference: [32] <author> L. G. Khachiyan and M. J. Todd. </author> <title> On the complexity of approximating the maximal inscribed ellipsoid for a polytope. </title> <type> Technical Report 893, </type> <institution> School of Operations Research and Industrial Engineering, Cornell University, </institution> <year> 1990. </year>
Reference-contexts: However, the smallest ellipsoid containing P is hard to find in general, and so is its center. Tarasov, Khachiyan and Erlich [45] study the method of inscribed ellipsoids. Actually, the center of the maximal inscribed ellipsoid for a polytope can be regarded as a "center". In <ref> [32] </ref>, Khachiyan and Todd discuss the problem of approximating the maximal inscribed ellipsoid and related problems. They also propose algorithms for finding these ellipsoids. The "center" of P we propose here is the center of the ellipsoid with smallest volume among a certain set of ellipsoids that contain P .
Reference: [33] <author> V. Klee and G. J. Minty. </author> <title> How good is the simplex algorithm? In O. </title> <editor> Shisha, editor, </editor> <booktitle> Inequalities III, </booktitle> <pages> pages 159-175, </pages> <address> New York, 1972. </address> <publisher> Academic Press. </publisher>
Reference-contexts: But we note that being theoretically satisfactory is not equivalent to being satisfactory in practice. The simplex method due to Dantzig [11], fast in practice, is not theoretically satisfactory (Klee and Minty <ref> [33] </ref>); on the other hand, the ellipsoid method [30], which was shown theoretically satisfactory for linear programming by Khachiyan [30] [31], is of less use in practice (see, for example, Bland, Goldfarb and Todd 3 [7]).
Reference: [34] <author> A. Yu. Levin. </author> <title> On an algorithm for the minimization of convex functions. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 6 </volume> <pages> 286-290, </pages> <year> 1965. </year>
Reference-contexts: We note that E can be regarded as an affine transformation of the unit ball B (0; 1). The ellipsoid method for linear programming is an outgrowth of the method of central sections of Levin <ref> [34] </ref> and Newman [38] and is based on the ellipsoid methods of Shor [43], Yudin and Nemirovskii [50] for convex programming. Khachiyan ([30] and [31]) showed that this method is a polynomial time algorithm for solving linear programming problems; it thus became the first polynomial algorithm in linear programming. <p> In this chapter we will study two kinds of centers, and later, in Chapter 4, we will propose the associated interior point methods. The first kind of "center" that was used in optimization is the center of gravity or centroid used by Levin <ref> [34] </ref> in his algorithm, the method of central sections, for minimization of a convex function over a convex polytope P (see also Newman [38]). 39 40 In that paper the centroid is used as the test point. <p> For example, we can cut the current polytope by a hyper-plane through the current center and throw away the part that does not contain the optimal solution and repeat this procedure; we thus get a sequence of shrinking poly-topes which contain the optimal solution, as does Levin <ref> [34] </ref> as well as Newman [38]; or push the center towards the optimum by adjusting the appropriate bound as done by Renegar [39]. <p> We call this technique the "pulling" technique, since it pulls the current center to the optimal solution. Note that most "bound-update" methods, e.g., Levin <ref> [34] </ref>, Newman [38] and Renegar [39], decrease an appropriate bound so as to push the current center to approach the optimal solution. The pulling technique is used in our MATLAB codes of Algorithm 4.3.1 attached in Appendix A. 116 Our limited test results are reported in Table 4.1.
Reference: [35] <author> C. B. Moler, J. Little, S. Bangert, and S. Kleiman. </author> <title> Pro-Matlab User's Guide. The MathWorks, </title> <publisher> Inc., </publisher> <year> 1987. </year>
Reference: [36] <author> C. L. Monma and A. J. Morton. </author> <title> Computational experience with a dual affine variant of Karmarkar's method for linear programming. </title> <journal> Operations Research Letters, </journal> <volume> 6 </volume> <pages> 216-267, </pages> <year> 1987. </year>
Reference-contexts: In 1984, Karmarkar [28] introduced a new algorithm which is polynomial-time bounded and seems to be a competitor to the simplex method in practice (see, for example, Adler, Resende, Veiga, and Karmarkar [1] and Monma and Morton <ref> [36] </ref>). The relations among these methods can be found in Todd [48]. Karmarkar's algorithm and other algorithms developed later on, for example, the path-following methods, generate points which are in the (relative) interior of the feasible region, and force them to approach the optimal solution.
Reference: [37] <author> J. L. Nazareth. </author> <title> Pricing criteria in linear programming. </title> <editor> In N. Meggiddo, editor, </editor> <booktitle> Progress in Mathematical Programming, </booktitle> <pages> pages 105-130, </pages> <address> New York, 1989. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The data in the column corresponding to the simplex method are from Nazareth <ref> [37] </ref>; these are average numbers of iterations for a number of random problems. := f (d)c T (ADA T ) 1 c is the width of the ellipsoid in the direction c after the final iteration and it serves here as the stopping criterion (we terminate if 10 3 ).
Reference: [38] <author> D. J. Newman. </author> <title> Location of the maximum on unimodal surfaces. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 12 </volume> <pages> 395-398, </pages> <year> 1965. </year>
Reference-contexts: We note that E can be regarded as an affine transformation of the unit ball B (0; 1). The ellipsoid method for linear programming is an outgrowth of the method of central sections of Levin [34] and Newman <ref> [38] </ref> and is based on the ellipsoid methods of Shor [43], Yudin and Nemirovskii [50] for convex programming. Khachiyan ([30] and [31]) showed that this method is a polynomial time algorithm for solving linear programming problems; it thus became the first polynomial algorithm in linear programming. <p> The first kind of "center" that was used in optimization is the center of gravity or centroid used by Levin [34] in his algorithm, the method of central sections, for minimization of a convex function over a convex polytope P (see also Newman <ref> [38] </ref>). 39 40 In that paper the centroid is used as the test point. <p> we can cut the current polytope by a hyper-plane through the current center and throw away the part that does not contain the optimal solution and repeat this procedure; we thus get a sequence of shrinking poly-topes which contain the optimal solution, as does Levin [34] as well as Newman <ref> [38] </ref>; or push the center towards the optimum by adjusting the appropriate bound as done by Renegar [39]. <p> We call this technique the "pulling" technique, since it pulls the current center to the optimal solution. Note that most "bound-update" methods, e.g., Levin [34], Newman <ref> [38] </ref> and Renegar [39], decrease an appropriate bound so as to push the current center to approach the optimal solution. The pulling technique is used in our MATLAB codes of Algorithm 4.3.1 attached in Appendix A. 116 Our limited test results are reported in Table 4.1.
Reference: [39] <author> J. Renegar. </author> <title> A polynomial-time algorithm based on Newton's method for linear programming. </title> <journal> Mathematical Programming, </journal> <volume> 40 </volume> <pages> 59-93, </pages> <year> 1988. </year>
Reference-contexts: These interior points are kinds of centers. Usually, if we can find a kind of center easily, then a theoretically satisfactory method for solving linear programs can be obtained accordingly. Based on using the "analytical center", Renegar <ref> [39] </ref> proposed a new polynomial algorithm for linear programming; and Vaidya's "fast" algorithm [49] for feasibility problems makes use of the "volumetric center". In this dissertation, we introduce some new centers and develop from them new algorithms for solving feasibility problems as well as linear programs. <p> Shor [43] independently developed the ellipsoid method. Later, in 1979, Khachian [30] showed that the ellipsoid method is a polynomial time algorithm for solving linear programming problems. Renegar <ref> [39] </ref> uses the "analytical center" (see also Sonnevend [44]) to develop his algorithm for linear programming problems, which is polynomial time bounded. The analytical center is easier to approximate compared with the centroid. Actually, in his algorithm, Renegar showed that an "*-analytical center" is enough. <p> The analytical center is easier to approximate compared with the centroid. Actually, in his algorithm, Renegar showed that an "*-analytical center" is enough. Unlike the centroid, the analytical center is not analytically independent; it depends on the way in which the polytope P is represented. Renegar <ref> [39] </ref> makes use of this property to improve the convergence of his algorithm by adding some extra constraints. 41 In the paper [49] Vaidya introduces the "volumetric center" which is the center of the ellipsoid with largest volume among a certain set of ellipsoids that are contained in P and proposes <p> part that does not contain the optimal solution and repeat this procedure; we thus get a sequence of shrinking poly-topes which contain the optimal solution, as does Levin [34] as well as Newman [38]; or push the center towards the optimum by adjusting the appropriate bound as done by Renegar <ref> [39] </ref>. <p> We call this technique the "pulling" technique, since it pulls the current center to the optimal solution. Note that most "bound-update" methods, e.g., Levin [34], Newman [38] and Renegar <ref> [39] </ref>, decrease an appropriate bound so as to push the current center to approach the optimal solution. The pulling technique is used in our MATLAB codes of Algorithm 4.3.1 attached in Appendix A. 116 Our limited test results are reported in Table 4.1. <p> Or in other words, if x c does not approach the optimal set, x c approaches the optimal set linearly. In addition to the pushing technique similar to that using by the sliding objective function method [7] and Renegar <ref> [39] </ref> we also propose a "pulling" technique: we let the lower bound corresponding to the objective be a very large negative number, i.e., l 0 ~ 1 (or u 0 ~ 1 for the maximizing linear program).
Reference: [40] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <year> 1970. </year> <month> 135 </month>
Reference-contexts: Theorem 3.2.8 If l &lt; u, and A is of full rank, then P = ; () f (d) &lt; 0; for some d 2 D: Proof. " =)". First of all, we recall Helly's Theorem which can be found, e.g., in Rockafellar <ref> [40] </ref>. Let fC i ji 2 I g be a finite collection of convex sets in R n (not neces sarily closed). If every subcollection consisting of n + 1 or fewer sets has a non-empty intersection, then the entire collection has a non-empty intersection. <p> We have thus proved that ~ F has no recession direction in R m . Therefore, by Theorem 27.3 of Rockafellar <ref> [40] </ref>, ~ F attains its minimum in R m . Equivalently, F attains its minimum over D, i.e., S f +h is not empty. 2 According to Theorem 7.1 of [40], D e := fd : ~ F (d) ~ F (e)g = fd 2 D : F (d) F (e)g <p> Therefore, by Theorem 27.3 of Rockafellar <ref> [40] </ref>, ~ F attains its minimum in R m . Equivalently, F attains its minimum over D, i.e., S f +h is not empty. 2 According to Theorem 7.1 of [40], D e := fd : ~ F (d) ~ F (e)g = fd 2 D : F (d) F (e)g is compact, and D e R m + . <p> As in Theorem 3.2.9, what we need to do is 69 to show that f has no recession direction in R m + . In this case, by Theorem 27.3 of Rockafellar <ref> [40] </ref>, F attains its minimum, say at d fl , over D. However, according to Proposition 3.2.12, x c (d fl ) must be an interior point of P , i.e. int (P ) 6= ; which is a contradiction.
Reference: [41] <author> J. B. Rosen. </author> <title> The gradient projection method for nonlinear programming, part 1: Linear constraints. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 8 </volume> <pages> 181-217, </pages> <year> 1960. </year>
Reference-contexts: Usually this phenomenon occurs when the feasible direction method is used for linearly constrained problems. We can avoid jamming by simply shifting to Rosen's gradient projection algorithm <ref> [41] </ref> once the iteration point hits the region := fd 2 R m + : d i *; for some ig, where * &gt; 0 is a small number and, under (A1), any cluster point of the sequence fd k g is optimal by the global convergence of Rosen's algorithm [14].
Reference: [42] <author> N. Z. Shor. </author> <title> Utilization of the operation of space dilatation in the minimization of convex functions. </title> <journal> Cybernetics, </journal> <volume> 6 </volume> <pages> 7-15, </pages> <year> 1970. </year>
Reference-contexts: Yudin and Nemirovskii [50] discuss the computational difficulties of Levin's method and propose a modified method of centered cross-sections, using ellipsoids instead of polyhedra. This modified method is computationally implementable. They also point out that this ellipsoid method is a special case of Shor's algorithm <ref> [42] </ref> with space dilation in the direction of the subgradient. Shor [43] independently developed the ellipsoid method. Later, in 1979, Khachian [30] showed that the ellipsoid method is a polynomial time algorithm for solving linear programming problems.
Reference: [43] <author> N. Z. Shor. </author> <title> Cut-off method with space extension in convex programming problems. </title> <journal> Cybernetics, </journal> <volume> 13 </volume> <pages> 94-96, </pages> <year> 1977. </year>
Reference-contexts: We note that E can be regarded as an affine transformation of the unit ball B (0; 1). The ellipsoid method for linear programming is an outgrowth of the method of central sections of Levin [34] and Newman [38] and is based on the ellipsoid methods of Shor <ref> [43] </ref>, Yudin and Nemirovskii [50] for convex programming. Khachiyan ([30] and [31]) showed that this method is a polynomial time algorithm for solving linear programming problems; it thus became the first polynomial algorithm in linear programming. <p> This modified method is computationally implementable. They also point out that this ellipsoid method is a special case of Shor's algorithm [42] with space dilation in the direction of the subgradient. Shor <ref> [43] </ref> independently developed the ellipsoid method. Later, in 1979, Khachian [30] showed that the ellipsoid method is a polynomial time algorithm for solving linear programming problems. Renegar [39] uses the "analytical center" (see also Sonnevend [44]) to develop his algorithm for linear programming problems, which is polynomial time bounded. <p> Thus (LP) is not very restrictive in practice either. 4.2 Sliding Method Via Model I The sliding objective function method was first proposed by Yudin and Nemirovskii [50] and Shor <ref> [43] </ref>. The idea is to reduce the linear programming problem to a sequence of feasibility problems formed by letting the objective be an extra constraint and decreasing the bound corresponding to the objective function as long as it is possible. 102 Suppose we are to solve (LP).
Reference: [44] <author> G. Sonnevend. </author> <title> An analytical center for polyhedrons and new classes of global algorithms for linear (smooth, convex) programming. </title> <booktitle> In Lecture Notes in Control and Information Sciences, </booktitle> <volume> No. 84, </volume> <pages> pages 866-875, </pages> <address> Berlin, 1986. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Shor [43] independently developed the ellipsoid method. Later, in 1979, Khachian [30] showed that the ellipsoid method is a polynomial time algorithm for solving linear programming problems. Renegar [39] uses the "analytical center" (see also Sonnevend <ref> [44] </ref>) to develop his algorithm for linear programming problems, which is polynomial time bounded. The analytical center is easier to approximate compared with the centroid. Actually, in his algorithm, Renegar showed that an "*-analytical center" is enough.
Reference: [45] <author> S. P. Tarasov, L. G. Khachiyan, and I. I. Erlich. </author> <title> The method of inscribed ellipsoids. </title> <journal> Soviet Math. Dokl., </journal> <volume> 37(1), </volume> <year> 1988. </year>
Reference-contexts: Thus, the center of this ellipsoid can be used as a "center". However, the smallest ellipsoid containing P is hard to find in general, and so is its center. Tarasov, Khachiyan and Erlich <ref> [45] </ref> study the method of inscribed ellipsoids. Actually, the center of the maximal inscribed ellipsoid for a polytope can be regarded as a "center". In [32], Khachiyan and Todd discuss the problem of approximating the maximal inscribed ellipsoid and related problems. They also propose algorithms for finding these ellipsoids.
Reference: [46] <author> M. J. Todd. </author> <title> Some remarks on the relaxation method for linear inequalities. </title> <type> Technical Report 419, </type> <institution> School of Operations Research and Industrial Engineering, Cornell University, </institution> <year> 1979. </year>
Reference-contexts: By scaling and changing variables we can assume, without loss of generality, AA T = I. Thus Algorithm 3.3.1 is a kind of "ball method" [7] for which Todd <ref> [46] </ref> and Goffin [18] show that an exponential number of iterations may be required.
Reference: [47] <author> M. J. Todd. </author> <title> On minimum volume ellipsoids containing part of a given ellipsoid. </title> <journal> Mathematics of Operations Research, </journal> <volume> 7 </volume> <pages> 253-261, </pages> <year> 1980. </year>
Reference-contexts: We also denote r := l+u ul for convenience. In [8] Burrell and Todd proposed a parallel-cut ellipsoid algorithm based on the result of Todd <ref> [47] </ref>. <p> If the current center violates some constraint, say, l i a T i x u i , by the result of Todd <ref> [47] </ref> we can construct a new ellipsoid that contains that part of the previous one between the parallel hyperplanes a T i x = l i and a T i x = u i , and the volume of the ellipsoid decreases by a factor which is, at worst, exp ( <p> Thus, by the result of Todd <ref> [47] </ref>, the volume of the corresponding ellipsoid decreases by a factor of exp ( 1 In our algorithms we use the so called "up and down" technique. <p> Then we have volume (E k+1 ) volume (E k ) exp ( 2n Proof. Since the bounds l k 0 and u k 0 are tight regarding to the current ellipsoid E k , by the results in Todd <ref> [47] </ref>, we have volume ( ~ E k+1 ) volume (E k ) = n 1 1 n 2 1 2 where ~ E k+1 is the minimum volume ellipsoid that contains E k " fx 2 R n : l k 0 g (corresponding to ff = 0; fi = <p> volume ( ~ E k+1 ) volume (E k ) = n 1 1 n 2 1 2 where ~ E k+1 is the minimum volume ellipsoid that contains E k " fx 2 R n : l k 0 g (corresponding to ff = 0; fi = 1 in <ref> [47] </ref>).
Reference: [48] <author> M. J. Todd. </author> <title> Recent developments and new directions in linear programming. </title> <editor> In M. Iri and K. Tanabe, editors, </editor> <booktitle> Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 109-157. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: The many applications, especially present-day large-scale linear programming problems, stimulated further research for new efficient algorithms theoretically as well as practically; thus were developed the ellipsoid method [30], Karmarkar's projective algorithm [28], and, arising from it, other interior point methods (see, for example, Todd <ref> [48] </ref>). Solving linear programming problems usually means finding a sequence fx k g such that x k goes to x fl , an optimal solution. <p> In 1984, Karmarkar [28] introduced a new algorithm which is polynomial-time bounded and seems to be a competitor to the simplex method in practice (see, for example, Adler, Resende, Veiga, and Karmarkar [1] and Monma and Morton [36]). The relations among these methods can be found in Todd <ref> [48] </ref>. Karmarkar's algorithm and other algorithms developed later on, for example, the path-following methods, generate points which are in the (relative) interior of the feasible region, and force them to approach the optimal solution. These interior points are kinds of centers.
Reference: [49] <author> P. M. Vaidya. </author> <title> A new algorithm for minimizing convex functions over convex sets. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1989. </year>
Reference-contexts: Usually, if we can find a kind of center easily, then a theoretically satisfactory method for solving linear programs can be obtained accordingly. Based on using the "analytical center", Renegar [39] proposed a new polynomial algorithm for linear programming; and Vaidya's "fast" algorithm <ref> [49] </ref> for feasibility problems makes use of the "volumetric center". In this dissertation, we introduce some new centers and develop from them new algorithms for solving feasibility problems as well as linear programs. <p> Unlike the centroid, the analytical center is not analytically independent; it depends on the way in which the polytope P is represented. Renegar [39] makes use of this property to improve the convergence of his algorithm by adding some extra constraints. 41 In the paper <ref> [49] </ref> Vaidya introduces the "volumetric center" which is the center of the ellipsoid with largest volume among a certain set of ellipsoids that are contained in P and proposes an algorithm with a better global convergence rate and time complexity than the ellipsoid method.
Reference: [50] <author> D. B. Yudin and A. S. Nemirovskii. </author> <title> Informational complexity and efficient methods for the solution of convex extremal problems. </title> <journal> Matekon, </journal> <volume> 13(2) </volume> <pages> 3-25, </pages> <year> 1976. </year>
Reference-contexts: The ellipsoid method for linear programming is an outgrowth of the method of central sections of Levin [34] and Newman [38] and is based on the ellipsoid methods of Shor [43], Yudin and Nemirovskii <ref> [50] </ref> for convex programming. Khachiyan ([30] and [31]) showed that this method is a polynomial time algorithm for solving linear programming problems; it thus became the first polynomial algorithm in linear programming. <p> This method is very concise and 1 exp (1) is a guaranteed reduction of the volume of successive polytopes. The disadvantage is the difficulty of calculating the centroid. Yudin and Nemirovskii <ref> [50] </ref> discuss the computational difficulties of Levin's method and propose a modified method of centered cross-sections, using ellipsoids instead of polyhedra. This modified method is computationally implementable. <p> Thus (LP) is not very restrictive in practice either. 4.2 Sliding Method Via Model I The sliding objective function method was first proposed by Yudin and Nemirovskii <ref> [50] </ref> and Shor [43]. The idea is to reduce the linear programming problem to a sequence of feasibility problems formed by letting the objective be an extra constraint and decreasing the bound corresponding to the objective function as long as it is possible. 102 Suppose we are to solve (LP).
References-found: 50

