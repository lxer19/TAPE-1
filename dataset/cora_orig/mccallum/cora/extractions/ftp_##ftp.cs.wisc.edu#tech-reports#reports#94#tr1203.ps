URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/94/tr1203.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Email: Email: ferris@cs.wisc.edu  Email: jsp@vicp.mts.jhu.edu  
Title: Nondegenerate Solutions and Related Concepts in Affine Variational Inequalities  
Author: M.C. Ferris J.S. Pang 
Note: The work of this author was based on research supported by the National Science Foundation grant CCR-9157632 and the Air Force Office of Scientific Research grant F49620-94-1-0036. The work of this author was based on research supported by the National Science Foundation under grants DDM-9104078 and CCR-9213739 and by the Office of Naval Research under grant N00014-93-1-0228.  
Date: January 12th, 1994  
Address: Madison, Wisconsin 53706  Baltimore, Maryland 21218-2689  
Affiliation: Computer Sciences Department University of Wisconsin  Department of Mathematical Sciences The Johns Hopkins University  
Abstract: Dedication. This paper is dedicated to Professor O.L. Mangasarian on the occasion of his 60th birthday. We are happy to submit this paper for publication on Professor Mangasarian's birthday to a journal that he has been associated with for many years, the SIAM Journal on Control and Optimization. He has made many significant contributions to the topics addressed in this paper, namely error bounds, weak sharp minima, minimum principle sufficiency and complementarity problems. We are both indebted to him for his constant encouragement, advice and fruitful collaborations over many years. Without his help and guidance, this paper would not have been possible. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F.A. Al-Khayyal and J. Kyparisis, </author> <title> "Finite convergence of algorithms for nonlinear programs and variational inequalities", </title> <journal> Journal of Optimization and Applications 70 (1991), </journal> <pages> pp. 319-332. </pages>
Reference-contexts: Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11]. The usefulness of this concept in establishing the finite convergence of various iterative algorithms was discussed in several subsequent papers <ref> [12, 5, 1] </ref>. Among the classes of optimization problems that possess weak sharp minima are linear programs [32] and certain convex quadratic programs and monotone linear complementarity problems [5]. <p> Since both x and x are in FEA (q; M; C) , it follows that !(x + t (x x)) = ~!(x + t (x x)) for all t 2 <ref> [0; 1] </ref>. Hence, we have ! 0 (x; x x) = ~! 0 (x; x x) = minfu T M (x x) : u 2 ~ (x)g; where ~ (x) argminfz T (q + M x) : z 2 Gg is a nonempty, finite subset of (x). <p> are any two vectors in SOL (q; M; C), then g 0 (x; x x) = 0 and !(x) = !(x) + ! 0 (x; x x): (20) Proof Since SOL (q; M; C) is convex, x + t (x x) 2 SOL (q; M; C) for all t 2 <ref> [0; 1] </ref>. Hence for all such t , g (x + t (x x)) = 0; which easily implies g 0 (x; x x) = 0.
Reference: [2] <author> G. Auchmuty, </author> <title> "Variational principles for variational inequalities", </title> <booktitle> Numerical Functional Analysis and Optimization 10 (1989), </booktitle> <pages> pp. 863-874. </pages>
Reference-contexts: By Proposition 2, the function g is convex, piecewise quadratic, and possibly extended-valued; it is in general not Frechet differentiable. We should mention that recently, there have been several differentiable optimization problems introduced for the study of a monotone VI <ref> [2, 15, 34] </ref>; since the objective functions of the latter optimization problems are not known to be convex even for a monotone AVI, it is therefore not clear whether our results can be extended to these other (possibly nonconvex) optimization formulations of the AVI.
Reference: [3] <author> A. Auslander, </author> <note> Optimization Methodes Numeriques, Masson, Paris (1976). </note>
Reference-contexts: Hence y ^x = 0 = F (^x) z as required. For the final statement of the proposition, let z 2 SOL (F; C) be arbitrary. Since F is monotone, it follows that (see e.g. <ref> [3] </ref>) F (c) T (c z) 0; for all c 2 C; which implies, since ^x 2 C, that F (^x) T (^x z) 0. However, ^x also solves the VI (F; C), so implying F (^x) T (^x z) = 0.
Reference: [4] <author> J.V. Burke, </author> <title> "An exact penalization viewpoint of constrained optimization", </title> <journal> SIAM Journal on Control and Optimization 29 (1991), </journal> <pages> pp. 968-998. </pages>
Reference-contexts: Part of the importance of an error bound is that it provides the foundation for exact penalization of mathematical programs [24, 30]; this in turn is strongly connected to the theory of optimality conditions for nonlinear programs <ref> [4] </ref>. Error bounds play an important role in the convergence analysis (particu 3 larly in establishing the convergence rates) of many iterative algorithms for solving various mathematical programs.
Reference: [5] <author> J.V. Burke and M.C. Ferris, </author> <title> "Weak sharp minima in mathematical programming", </title> <note> SIAM Journal on Control and Optimization 31 (1993), pp.1340-1359. </note>
Reference-contexts: Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11]. The usefulness of this concept in establishing the finite convergence of various iterative algorithms was discussed in several subsequent papers <ref> [12, 5, 1] </ref>. Among the classes of optimization problems that possess weak sharp minima are linear programs [32] and certain convex quadratic programs and monotone linear complementarity problems [5]. <p> The usefulness of this concept in establishing the finite convergence of various iterative algorithms was discussed in several subsequent papers [12, 5, 1]. Among the classes of optimization problems that possess weak sharp minima are linear programs [32] and certain convex quadratic programs and monotone linear complementarity problems <ref> [5] </ref>. Finally, the minimum principle [29] is a well-known set of conditions that must be satisfied by any local minimum of a nonlinear program with a convex feasible region. <p> In what follows, we shall describe each of these concepts more formally. The notion of a weak sharp minimum was introduced in [11] and extensively analyzed in <ref> [5, 13] </ref>. The formal definition is as follows. Definition 1 Let f : R n ! R [ f1g and C R n . <p> That (c) is also equivalent to (a) or (b) is equally obvious because x solves (1) if and only if x 2 (x). Finally, the equivalence of (d) and the above statements was proved in <ref> [5, Theorem 4.2] </ref>. Remark. <p> That (c) is also equivalent to (a) or (b) is equally obvious because x solves (1) if and only if x 2 (x). Finally, the equivalence of (d) and the above statements was proved in [5, Theorem 4.2]. Remark. Theorem 4.2 in <ref> [5] </ref> shows that in the above proposition, (d) always implies (a) for an arbitrary closed convex set C; nevertheless, Example 4.3 shows that the polyhedrality of C is needed for the reverse implication. 3 Miscellaneous Preliminary Results We have now defined all the concepts we shall deal with in this paper.
Reference: [6] <author> J.V. Burke and J.J. </author> <title> More, "On the identification of active constraints", </title> <note> SIAM Journal on Numerical Analysis 15 (1988), pp.1197-1211. </note>
Reference-contexts: In recent years, the strict complementarity property was given a renewed emphasis in the analysis of many iterative algorithms for solving linear and nonlinear programs and complementarity problems. Dunn [10] and Burke-More <ref> [6] </ref> used a geometric definition of a strictly complementary solution to a nonlinear program and showed how such a solution was essential for the successful identification of active constraints in a broad class of gradient based methods for solving constrained optimization problems. <p> For an optimization problem of the form minimize f (x) subject to x 2 C; (1) where f : R n ! R is continuous and C R n is convex, different forms of nondegeneracy abound in the literature. Dunn [10], Burke and More <ref> [6] </ref> use the relative interior condition: rf (^x) 2 ri N C (^x) ; (2) to define an optimal solution ^x of (1) as being nondegenerate. <p> Throughout this paper, we will use the notation F (x) to denote the face of C which contains a vector x 2 C in its relative interior. The following result was established in <ref> [6] </ref>. Lemma 1 The normal cone to a polyhedral convex set C is constant for all x 2 ri F , where F is a face of C; henceforth labeled N F .
Reference: [7] <author> J.V. Burke and J.J. </author> <title> More, "Exposing constraints", </title> <type> preprint MCS-P308-0592, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne (1992). </institution>
Reference-contexts: If in addition, F is monotone, then SOL (F; C) F (^x). Proof The equivalence of (ii) and (iii) has been noted before. The equivalence of (ii) and (iv) is by <ref> [7, Theorem 2.4] </ref>. Since ^x 2 ri F and F (^x) 2 ri N F , it follows that ^x + F (^x) 2 ri F + ri (N F ) = ri (F N F ) which, as we have noted, has a nonempty interior. Thus (ii) implies (i). <p> However, ^x also solves the VI (F; C), so implying F (^x) T (^x z) = 0. Hence, z 2 fc 2 C j F (^x) T (c ^x) = 0g; which is F (^x) by <ref> [7, Theorem 2.4] </ref>. 7 In the remainder of this paper, we shall focus on the AVI (q; M; C). As stated before, our goal is to establish the equivalence of the existence of a nondegenerate solution to this problem and a number of related concepts.
Reference: [8] <author> R.W. Cottle, J.S. Pang, and R.E. Stone, </author> <title> The Linear Complementarity Problem, </title> <publisher> Academic Press, </publisher> <address> Boston (1992). </address>
Reference-contexts: Error bounds play an important role in the convergence analysis (particu 3 larly in establishing the convergence rates) of many iterative algorithms for solving various mathematical programs. These include the the matrix splitting methods for linear complementarity problems <ref> [8, Chapter 5] </ref> and affine variational inequalities [25], various descent methods for convex minimization problems [26, 27, 28], and interior-point methods for linear programs and extensions [23, 35, 42]. Error bounds can also be used to design inexact iterative methods [37, 16]. <p> The results in [13, 31] suggest that for a monotone linear complementarity problem and its "natural" convex quadratic program <ref> [8, Chapter 3] </ref>, all these concepts are equivalent (to be made precise later). In this paper, we shall extend the equivalences to a monotone affine variational inequality. By adding appropriate multipliers to the constraints of an affine variational inequality, this problem becomes equivalent to a linear complementarity problem [38].
Reference: [9] <author> V.F. Dem'yanov and L.V. Vasil'ev, </author> <title> Nondifferentiable Optimization, Optimization Software, </title> <publisher> Inc., </publisher> <address> New York (1985). </address>
Reference-contexts: The equivalence of (ii) and (iii) follows from the fact that (x) is the subdifferential of the support function of C at (M x + q) and <ref> [9, Lemma 5.3] </ref>. 4 The Main Result We are now ready to state the main result of this paper. This result gives various necessary and sufficient conditions for the existence of a nondegenerate solution for a monotone AVI.
Reference: [10] <author> J.C. Dunn, </author> <title> "On the convergence of projected gradient processes to singular critical points", </title> <note> Journal of Optimization Theory and Applications 55 (1987), pp.203-216. </note>
Reference-contexts: Robinson [40] has introduced a generalized notion of strict complementarity and considered its role in parametric nonlinear programming. In recent years, the strict complementarity property was given a renewed emphasis in the analysis of many iterative algorithms for solving linear and nonlinear programs and complementarity problems. Dunn <ref> [10] </ref> and Burke-More [6] used a geometric definition of a strictly complementary solution to a nonlinear program and showed how such a solution was essential for the successful identification of active constraints in a broad class of gradient based methods for solving constrained optimization problems. <p> For an optimization problem of the form minimize f (x) subject to x 2 C; (1) where f : R n ! R is continuous and C R n is convex, different forms of nondegeneracy abound in the literature. Dunn <ref> [10] </ref>, Burke and More [6] use the relative interior condition: rf (^x) 2 ri N C (^x) ; (2) to define an optimal solution ^x of (1) as being nondegenerate.
Reference: [11] <author> M.C. Ferris, </author> <title> Weak Sharp Minima and Penalty Functions in Mathematical Programming, </title> <type> Ph.D. thesis, </type> <institution> University of Cambridge (1988). </institution>
Reference-contexts: Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in <ref> [11] </ref>. The usefulness of this concept in establishing the finite convergence of various iterative algorithms was discussed in several subsequent papers [12, 5, 1]. Among the classes of optimization problems that possess weak sharp minima are linear programs [32] and certain convex quadratic programs and monotone linear complementarity problems [5]. <p> As stated before, our goal is to establish the equivalence of the existence of a nondegenerate solution to this problem and a number of related concepts. In what follows, we shall describe each of these concepts more formally. The notion of a weak sharp minimum was introduced in <ref> [11] </ref> and extensively analyzed in [5, 13]. The formal definition is as follows. Definition 1 Let f : R n ! R [ f1g and C R n .
Reference: [12] <author> M.C. Ferris, </author> <title> "Finite termination of the proximal point algorithm", </title> <booktitle> Mathematical Programming 50 (1991), </booktitle> <pages> pp. 359-366. </pages>
Reference-contexts: Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11]. The usefulness of this concept in establishing the finite convergence of various iterative algorithms was discussed in several subsequent papers <ref> [12, 5, 1] </ref>. Among the classes of optimization problems that possess weak sharp minima are linear programs [32] and certain convex quadratic programs and monotone linear complementarity problems [5].
Reference: [13] <author> M.C. Ferris and O.L. Mangasarian, </author> <title> "Minimum principle sufficiency", </title> <booktitle> Mathematical Programming 57 (1992), </booktitle> <address> pp.1-14. </address>
Reference-contexts: One way to state this principle is in terms of the gap function [20] of the nonlinear program; informally, this principle states that a local minimum of a nonlinear program must be a global minimizer of the gap function over the same convex feasible region of the program. In <ref> [13] </ref>, Ferris and Mangasarian studied the "converse" of this principle for the class of convex programs and coined the term "minimum principle sufficiency" when this converse was valid. <p> The present research is motivated by the desire to gain a better understanding of the concepts of strict complementarity, error bounds, weak sharp minima, and minimum principle sufficiency for various mathematical programs, and how these concepts are related. The results in <ref> [13, 31] </ref> suggest that for a monotone linear complementarity problem and its "natural" convex quadratic program [8, Chapter 3], all these concepts are equivalent (to be made precise later). In this paper, we shall extend the equivalences to a monotone affine variational inequality. <p> In this paper, we shall extend the equivalences to a monotone affine variational inequality. By adding appropriate multipliers to the constraints of an affine variational inequality, this problem becomes equivalent to a linear complementarity problem [38]. In view of the results available for the linear complementarity problem <ref> [13, 31] </ref>, this transformation therefore raises the question 4 of whether the intended generalized equivalences for the affine variational inequality are of any significant interest. <p> In what follows, we shall describe each of these concepts more formally. The notion of a weak sharp minimum was introduced in [11] and extensively analyzed in <ref> [5, 13] </ref>. The formal definition is as follows. Definition 1 Let f : R n ! R [ f1g and C R n . <p> Consequently, a necessary and sufficient condition for the existence of a weak sharp minimum for the problem (1) is the existence of an error bound of the type (5) where S is the set of minimizers of (1). The notion of minimum principle sufficiency was introduced in <ref> [13] </ref>. The minimum principle is a well-known necessary optimality condition for a program of the form (1), where C is convex; this principle states that, for a continuously differentiable function f , if x solves (1) then x 2 SOL (rf; C). <p> Thus by Proposition 3, the minimum principle sufficiency holds for (27). Next by using a similar proof technique as in <ref> [13, Theorem 13] </ref>, we will establish that the GLCP (p; N; K) has a nondegenerate solution. Proposition 4 will then imply that the AVI (q; M; C) has a nondegenerate solution. In what follows, let y (x; ); also let f (y) denote the objective function of (27).
Reference: [14] <author> A.V. </author> <title> Fiacco and G.P. McCormick, Nonlinear Programming: Sequential Unconstrained Minimization Technique, </title> <publisher> John Wiley, </publisher> <address> New York (1968). </address>
Reference-contexts: In general, the property of strict complementarity of a solution to an optimization or a complementarity problem plays an important role in many aspects of such a problem. Historically, Fiacco and McCormick <ref> [14] </ref> used this property to develop the first sensitivity theory of nonlinear programs under perturbation. Robinson [40] has introduced a generalized notion of strict complementarity and considered its role in parametric nonlinear programming.
Reference: [15] <author> M. Fukushima, </author> <title> "Equivalent differentiable optimization problems and descent methods for asymmetric variational inequality problems", </title> <booktitle> Mathematical Programming 53 (1992), </booktitle> <pages> pp. 99-110. </pages>
Reference-contexts: By Proposition 2, the function g is convex, piecewise quadratic, and possibly extended-valued; it is in general not Frechet differentiable. We should mention that recently, there have been several differentiable optimization problems introduced for the study of a monotone VI <ref> [2, 15, 34] </ref>; since the objective functions of the latter optimization problems are not known to be convex even for a monotone AVI, it is therefore not clear whether our results can be extended to these other (possibly nonconvex) optimization formulations of the AVI.
Reference: [16] <author> S.A. Gabriel and J.S. Pang, </author> <title> "An inexact NE/SQP method for solving the nonlinear complementarity problem", </title> <booktitle> Computational Optimization and Applications 1 (1992), </booktitle> <pages> pp. 67-92. 27 </pages>
Reference-contexts: Error bounds can also be used to design inexact iterative methods <ref> [37, 16] </ref>. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11]. The usefulness of this concept in establishing the finite convergence of various iterative algorithms was discussed in several subsequent papers [12, 5, 1].
Reference: [17] <author> A.J. Goldman and A.W. Tucker, </author> <title> "Theory of linear programming", </title> <editor> in H.W. Kuhn and A.W. Tucker, eds., </editor> <title> Linear Inequalities and Related Systems, </title> <publisher> Princeton University Press, Princeton (1956), </publisher> <pages> pp. 53-97. </pages>
Reference-contexts: 1 Introduction Strict complementarity is a familiar notion in the context of optimization problems and/or complementarity theory. A classical result proved in <ref> [17, Corollary 2A] </ref> shows that a solvable linear complementarity problem defined by a skew-symmetric matrix must possess a strictly complementary solution. In general, the property of strict complementarity of a solution to an optimization or a complementarity problem plays an important role in many aspects of such a problem. <p> We conclude this paper by giving an application of Theorem 1 that generalizes the classical result of Goldman and Tucker <ref> [17] </ref> mentioned in the opening of this paper. Corollary 1 Let q 2 R n be arbitrary, M 2 R nfin be positive semidefi-nite, and C R n be a polyhedral set.
Reference: [18] <author> M.S. Gowda and J.S. Pang, </author> <title> "On the boundedness and stability of solutions to the affine variational inequality problem", </title> <journal> SIAM Journal on Control and Optimization 32 (1994), </journal> <pages> pp. </pages>
Reference-contexts: for the latter problem is the extended-valued function g : R n ! R [ f1g given by g (x) x T F (x) !(x); for all x 2 R n ; (6) where !(x) inffz T F (x) : z 2 Cg: (7) The function ! was introduced in <ref> [18] </ref> where it was used for stability analysis of the AVI. Let (x) argminfz T F (x) : z 2 Cg; it is understood that if the minimum value in !(x) is not attained, then (x) is defined to be the empty set. <p> Motivation for using this function 10 g stems partly from statement (iii) in Proposition 2 which suggests that g is a likely candidate for a residual function for the AVI. This choice is also supported by some error bound results in <ref> [18] </ref> which are derived with the aid of some additional properties of the monotone AVI. In what follows, we shall summarize the relevant results for later use. <p> We shall assume that M is positive semidefinite and C is a polyhedral. We shall further assume that SOL (q; M; C) 6= ;. There are two important constants associated with the solution set of the monotone AVI (q; M; C). Indeed, by results in <ref> [18] </ref>, there exist a vector d 2 R n and a scalar 2 R + , both dependent on the data (q; M; C), such that d = (M + M T )x; = x T M x; (8) for all x 2 SOL (q; M; C).
Reference: [19] <author> O. Guler and Y. Ye, </author> <title> "Convergence behavior of interior-point algorithms", </title> <booktitle> Mathematical Programming 60 (1993), </booktitle> <pages> pp. 215-228. </pages>
Reference-contexts: Dunn [10] and Burke-More [6] used a geometric definition of a strictly complementary solution to a nonlinear program and showed how such a solution was essential for the successful identification of active constraints in a broad class of gradient based methods for solving constrained optimization problems. Guler and Ye <ref> [19] </ref> showed that many interior-point algorithms for linear programs generated a sequence of iterates whose limit points satisfied the strict complementarity condition; they also extended the result to a monotone linear complementarity problem having a strictly complementary solution.
Reference: [20] <author> D.W. Hearn, </author> <title> "The gap function of a convex program", </title> <note> Operations Research Letters 1 (1982), pp.67-71. </note>
Reference-contexts: Finally, the minimum principle [29] is a well-known set of conditions that must be satisfied by any local minimum of a nonlinear program with a convex feasible region. One way to state this principle is in terms of the gap function <ref> [20] </ref> of the nonlinear program; informally, this principle states that a local minimum of a nonlinear program must be a global minimizer of the gap function over the same convex feasible region of the program.
Reference: [21] <author> A.J. Hoffman, </author> <title> "On approximate solutions of systems of linear inequalities", </title> <booktitle> Journal of the National Bureau of Standards 49 (1952), </booktitle> <pages> pp. 263-265. </pages>
Reference-contexts: The theory of error bounds for inequality systems has in recent years become an active area of research within the field of mathematical programming. In this regard, Hoffman <ref> [21] </ref> obtained the first error bound for a system of finitely many linear inequalities. The generalizations of Hoffman's result are too numerous to be mentioned here. There are several factors that have motivated this proliferation of activities. <p> Thus x 2 SOL (q; M; C) as desired. Next, we prove (d) ) (c). The proof of this implication uses the following consequence of the famous Hoffman error bound for systems of linear inequalities <ref> [21] </ref>. Let P be a polyhedral set in R n , and let E and f be, respectively, a matrix and vector of compatible dimensions.
Reference: [22] <author> W. Li, </author> <title> "Error bounds for piecewise convex quadratic programs and applications", </title> <type> manuscript, </type> <institution> Department of Mathematics and Statistics, Old Dominion University, </institution> <address> Norfolk, Virginia (1994). </address>
Reference-contexts: With the above proposition, we can now discuss the extension of the minimum principle sufficiency to the nondifferentiable gap minimization problem (13), or equivalently, to (17). Some related work on error bounds for convex, piecewise quadratic minimization problems, of which (13) is a special case, can be found in <ref> [22] </ref>. The following result establishes two properties of solutions to the AVI (q; M; C). Proposition 7 Let q 2 R n be arbitrary, M 2 R nfin be positive semidefi-nite, and C R n be a polyhedral set.
Reference: [23] <author> Z.Q. Luo, </author> <title> "Convergence analysis of primal-dual interior-point algorithms for convex quadratic programs", </title> <type> manuscript, </type> <institution> Communications Research Laboratory, McMaster University, Hamilton (1992). </institution>
Reference-contexts: These include the the matrix splitting methods for linear complementarity problems [8, Chapter 5] and affine variational inequalities [25], various descent methods for convex minimization problems [26, 27, 28], and interior-point methods for linear programs and extensions <ref> [23, 35, 42] </ref>. Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11].
Reference: [24] <author> Z.Q. Luo, J.S. Pang, D. Ralph, and S.Q. Wu, </author> <title> "Exact penalization and stationarity conditions of mathematical programs with equilibrium constraints", </title> <type> manuscript, </type> <institution> Department of Mathematical Sciences, The Johns Hopkins University, </institution> <address> Baltimore (1993). </address>
Reference-contexts: Part of the importance of an error bound is that it provides the foundation for exact penalization of mathematical programs <ref> [24, 30] </ref>; this in turn is strongly connected to the theory of optimality conditions for nonlinear programs [4]. Error bounds play an important role in the convergence analysis (particu 3 larly in establishing the convergence rates) of many iterative algorithms for solving various mathematical programs.
Reference: [25] <author> Z.Q. Luo and P. Tseng, </author> <title> "Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem", </title> <journal> SIAM Journal on Optimization 2 (1992), </journal> <pages> pp. 43-54. </pages>
Reference-contexts: Error bounds play an important role in the convergence analysis (particu 3 larly in establishing the convergence rates) of many iterative algorithms for solving various mathematical programs. These include the the matrix splitting methods for linear complementarity problems [8, Chapter 5] and affine variational inequalities <ref> [25] </ref>, various descent methods for convex minimization problems [26, 27, 28], and interior-point methods for linear programs and extensions [23, 35, 42]. Error bounds can also be used to design inexact iterative methods [37, 16].
Reference: [26] <author> Z.Q. Luo and P. Tseng, </author> <title> "On the linear convergence of descent methods for convex essentially smooth minimization", </title> <journal> SIAM Journal on Control and Optimization 30 (1992), </journal> <pages> pp. 408-425. </pages>
Reference-contexts: These include the the matrix splitting methods for linear complementarity problems [8, Chapter 5] and affine variational inequalities [25], various descent methods for convex minimization problems <ref> [26, 27, 28] </ref>, and interior-point methods for linear programs and extensions [23, 35, 42]. Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11].
Reference: [27] <author> Z.Q. Luo and P. Tseng, </author> <title> "On the convergence rate of dual ascent methods for linearly constrained convex minimization", </title> <booktitle> Mathematics of Operations Research 18 (1993), </booktitle> <pages> pp. 846-867. 28 </pages>
Reference-contexts: These include the the matrix splitting methods for linear complementarity problems [8, Chapter 5] and affine variational inequalities [25], various descent methods for convex minimization problems <ref> [26, 27, 28] </ref>, and interior-point methods for linear programs and extensions [23, 35, 42]. Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11].
Reference: [28] <author> Z.Q. Luo and P. Tseng, </author> <title> "Error bounds and convergence analysis of feasible descent methods: A general approach", </title> <journal> Annals of Operations Research, forthcoming. </journal>
Reference-contexts: These include the the matrix splitting methods for linear complementarity problems [8, Chapter 5] and affine variational inequalities [25], various descent methods for convex minimization problems <ref> [26, 27, 28] </ref>, and interior-point methods for linear programs and extensions [23, 35, 42]. Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11].
Reference: [29] <author> O.L. Mangasarian, </author> <title> Nonlinear Programming, </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York (1969). </address>
Reference-contexts: Among the classes of optimization problems that possess weak sharp minima are linear programs [32] and certain convex quadratic programs and monotone linear complementarity problems [5]. Finally, the minimum principle <ref> [29] </ref> is a well-known set of conditions that must be satisfied by any local minimum of a nonlinear program with a convex feasible region.
Reference: [30] <author> O.L. Mangasarian, </author> <title> "Sufficiency of exact penalty minimization", </title> <journal> SIAM Journal on Control and Optimization 23 (1985), </journal> <pages> pp. 30-37. </pages>
Reference-contexts: Part of the importance of an error bound is that it provides the foundation for exact penalization of mathematical programs <ref> [24, 30] </ref>; this in turn is strongly connected to the theory of optimality conditions for nonlinear programs [4]. Error bounds play an important role in the convergence analysis (particu 3 larly in establishing the convergence rates) of many iterative algorithms for solving various mathematical programs.
Reference: [31] <author> O.L. Mangasarian, </author> <title> "Error bounds for nondegenerate monotone linear complementarity problems", </title> <booktitle> Mathematical Programming, Series B 48 (1990), </booktitle> <pages> pp. 437-445. </pages>
Reference-contexts: The present research is motivated by the desire to gain a better understanding of the concepts of strict complementarity, error bounds, weak sharp minima, and minimum principle sufficiency for various mathematical programs, and how these concepts are related. The results in <ref> [13, 31] </ref> suggest that for a monotone linear complementarity problem and its "natural" convex quadratic program [8, Chapter 3], all these concepts are equivalent (to be made precise later). In this paper, we shall extend the equivalences to a monotone affine variational inequality. <p> In this paper, we shall extend the equivalences to a monotone affine variational inequality. By adding appropriate multipliers to the constraints of an affine variational inequality, this problem becomes equivalent to a linear complementarity problem [38]. In view of the results available for the linear complementarity problem <ref> [13, 31] </ref>, this transformation therefore raises the question 4 of whether the intended generalized equivalences for the affine variational inequality are of any significant interest.
Reference: [32] <author> O.L. Mangasarian and R.R. Meyer, </author> <title> "Nonlinear perturbation of linear programs", </title> <note> SIAM Journal on Control and Optimization 17 (1979), pp.745-752. </note>
Reference-contexts: The usefulness of this concept in establishing the finite convergence of various iterative algorithms was discussed in several subsequent papers [12, 5, 1]. Among the classes of optimization problems that possess weak sharp minima are linear programs <ref> [32] </ref> and certain convex quadratic programs and monotone linear complementarity problems [5]. Finally, the minimum principle [29] is a well-known set of conditions that must be satisfied by any local minimum of a nonlinear program with a convex feasible region. <p> For statement (b), observe that if fl (x) 6= ; for some x, then !(x) is finite and equal to the optimal objective value of (x). By <ref> [32, Lemma A.1] </ref>, every solvable linear program has a nonempty set of weak sharp minima. A careful look at the proof of this result reveals that the constant associated with such a set of weak sharp minima is independent of the right-hand side in the constraints of the program.
Reference: [33] <author> O.L. Mangasarian and T.H. Shiau, </author> <title> "Lipschitz continuity of solutions of linear inequalities, programs and complementarity problems", </title> <journal> SIAM Journal on Control and Optimization 25 (1987), </journal> <pages> pp. 583-595. </pages>
Reference-contexts: Thus (b) follows. Statement (c) follows from the Lipschitzian property of the solutions to a parametric right-hand sided linear program as proved in <ref> [33, Theorem 2.4] </ref>. We shall associate the following optimization problem with the AVI (q; M; C): minimize g (x) subject to x 2 C (13) where g is the gap function defined in (6) with F (x) q + M x.
Reference: [34] <editor> O.L. Mangasarian and M.V. Solodov, </editor> <title> "Nonlinear complementarity as unconstrained and constrained minimization", </title> <booktitle> Mathematical Programming, Series B 62 (1992), </booktitle> <pages> pp. 277-298. </pages>
Reference-contexts: By Proposition 2, the function g is convex, piecewise quadratic, and possibly extended-valued; it is in general not Frechet differentiable. We should mention that recently, there have been several differentiable optimization problems introduced for the study of a monotone VI <ref> [2, 15, 34] </ref>; since the objective functions of the latter optimization problems are not known to be convex even for a monotone AVI, it is therefore not clear whether our results can be extended to these other (possibly nonconvex) optimization formulations of the AVI.
Reference: [35] <author> R.D.C. Monteiro and T. Tsuchiya, </author> <title> "Limiting behavior of the derivatives of certain trajectories associated with a monotone horizontal linear complementarity problem", </title> <type> manuscript, </type> <institution> Department of Systems and Industrial Engineering, University of Arizona, </institution> <address> Tucson (1992). </address>
Reference-contexts: These include the the matrix splitting methods for linear complementarity problems [8, Chapter 5] and affine variational inequalities [25], various descent methods for convex minimization problems [26, 27, 28], and interior-point methods for linear programs and extensions <ref> [23, 35, 42] </ref>. Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11].
Reference: [36] <author> R.D.C. Monteiro and S.J. Wright, </author> <title> "Local convergence of interior-point algorithms for degenerate LCP", </title> <type> preprint MCS-P357-0493, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne (1993). </institution>
Reference-contexts: Guler and Ye [19] showed that many interior-point algorithms for linear programs generated a sequence of iterates whose limit points satisfied the strict complementarity condition; they also extended the result to a monotone linear complementarity problem having a strictly complementary solution. Monteiro and Wright <ref> [36] </ref> demonstrated that the existence of a strictly complementary solution was essential for the fast convergence of these interior-point algorithms for a monotone linear complementarity problem. The theory of error bounds for inequality systems has in recent years become an active area of research within the field of mathematical programming.
Reference: [37] <author> J.S. Pang, </author> <title> "Inexact Newton methods for the nonlinear complementarity problems", </title> <booktitle> Mathematical Programming 34 (1986), </booktitle> <pages> pp. 56-71. </pages>
Reference-contexts: Error bounds can also be used to design inexact iterative methods <ref> [37, 16] </ref>. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11]. The usefulness of this concept in establishing the finite convergence of various iterative algorithms was discussed in several subsequent papers [12, 5, 1].
Reference: [38] <author> J.S. Pang, </author> <title> "Complementarity problems", </title> <editor> chapter in R. Horst and P. Pardalos, eds. </editor> <title> Handbook in Global Optimization, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston (1994), </address> <pages> pp. 29 </pages>
Reference-contexts: In this paper, we shall extend the equivalences to a monotone affine variational inequality. By adding appropriate multipliers to the constraints of an affine variational inequality, this problem becomes equivalent to a linear complementarity problem <ref> [38] </ref>. In view of the results available for the linear complementarity problem [13, 31], this transformation therefore raises the question 4 of whether the intended generalized equivalences for the affine variational inequality are of any significant interest. <p> (9) Since F N F K + K fl and both have full dimension, it follows from Proposition 1 that if ^y is a strictly complementary solution of the GLCP (q; M; K), then ^y + q + M ^y 2 int (K + K fl ): It is known <ref> [38] </ref> that the AVI (q; M; C) is equivalent to a mixed linear complementarity problem in higher dimensions. In what follows, we shall establish a connection between the nondegenerate solutions of these two problems.
Reference: [39] <author> J.A. Reinoza, </author> <title> A Degree for Generalized Equations, </title> <institution> Department of Industrial Engineering, University of Wisconsin, Madison (1979). </institution>
Reference-contexts: This observation will be used in the proof of the following proposition. We note that condition (iv) in this proposition has been used by Reinoza <ref> [39] </ref>. Proposition 1 Suppose ^x solves VI (F; C) and C is polyhedral. Let F = F (^x), so that F (^x) 2 N F .
Reference: [40] <author> S.M. Robinson, </author> <title> "Local structure of feasible sets in nonlinear programming, Part III: Stability and sensitivity", </title> <booktitle> Mathematical Programming Study 30 (1987), </booktitle> <pages> pp. 45-66. </pages>
Reference-contexts: In general, the property of strict complementarity of a solution to an optimization or a complementarity problem plays an important role in many aspects of such a problem. Historically, Fiacco and McCormick [14] used this property to develop the first sensitivity theory of nonlinear programs under perturbation. Robinson <ref> [40] </ref> has introduced a generalized notion of strict complementarity and considered its role in parametric nonlinear programming. In recent years, the strict complementarity property was given a renewed emphasis in the analysis of many iterative algorithms for solving linear and nonlinear programs and complementarity problems. <p> Robinson <ref> [40] </ref> uses the dual form: T C (^x) " rf (^x) ? is a subspace, where the tangent cone, T C (x), to C at x is the polar of the normal cone at x; i.e. <p> T C (x) fz 2 R n j z T y 0; for all y 2 N C (x)g: It is easy to show (see <ref> [40, Lemma 2.1] </ref> for a proof) that the definition (2) is equivalent to the subspace definition. In general, for a convex set S R n , the negative of the polar of S is the dual cone of S, which is denoted by S fl .
Reference: [41] <author> R.T. Rockafellar, </author> <title> Convex Analysis, </title> <publisher> Princeton University Press, </publisher> <address> Princeton (1970). </address>
Reference-contexts: We shall summarize these characterizations in Proposition 1 below. The additional characterizations rely heavily on the face structure of a polyhedral convex set. It is well known that the relative interiors of the faces of a convex set C form a partition of C <ref> [41, 18.2] </ref>. Throughout this paper, we will use the notation F (x) to denote the face of C which contains a vector x 2 C in its relative interior. The following result was established in [6]. <p> Hence, N F = fA T j 2 R m 12 From the theory of convex polyhedra, particularly <ref> [41, 6.6] </ref>, we have ri F = fx 2 F j (Ax &gt; b) i ; for all i 62 Ig Hence, according to Proposition 1, ^x is nondegenerate if and only if ^x 2 ri F and (q +M ^x) 2 ri N F .
Reference: [42] <author> S.J. Wright, </author> <title> "A path-following infeasible-interior-point algorithm for linear and quadratic problems", </title> <type> preprint MCS-P3??-0993, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> Ar-gonne </month> <year> (1993). </year> <month> 30 </month>
Reference-contexts: These include the the matrix splitting methods for linear complementarity problems [8, Chapter 5] and affine variational inequalities [25], various descent methods for convex minimization problems [26, 27, 28], and interior-point methods for linear programs and extensions <ref> [23, 35, 42] </ref>. Error bounds can also be used to design inexact iterative methods [37, 16]. The concept of a weak sharp minimum for a constrained optimization problem was introduced in [11].
References-found: 42

