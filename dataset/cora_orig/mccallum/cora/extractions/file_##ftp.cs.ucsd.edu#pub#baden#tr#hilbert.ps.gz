URL: file://ftp.cs.ucsd.edu/pub/baden/tr/hilbert.ps.gz
Refering-URL: http://www.cs.ucsd.edu/groups/hpcl/scg/tr.html
Root-URL: http://www.cs.ucsd.edu
Title: Dynamic Partitioning of Non-Uniform Structured Workloads with Spacefilling Curves  
Author: John R. Pilkington and Scott B. Baden 
Date: January 10, 1995  
Abstract: We discuss Inverse Spacefilling Partitioning (ISP), a partitioning strategy for nonuniform scientific computations running on distributed memory MIMD parallel computers. We consider the case of a dynamic workload distributed on a uniform mesh, and compare ISP against Orthogonal Recursive Bisection (ORB) and a Median of Medians variant of ORB, ORB-MM. We present two results. First, ISP and ORB-MM are superior to ORB in rendering balanced workloads|because they are more fine-grained|and incur communication overheads that are comparable to ORB. Second, ISP is more attractive than ORB-MM from a software engineering standpoint because it avoids elaborate bookkeeping. Whereas ISP partitionings can be described succinctly as logically contiguous segments of the line, ORB-MM's partitionings are inherently unstructured. We describe the general d-dimensional ISP algorithm and report empirical results with two- and three-dimensional, non-hierarchical particle methods. fl Scott B. Baden is supported by ONR contract N00014-93-1-0152. John R. Pilkington is supported in part by NSF contract number ASC-9106465. Pilkington and Baden are with the Computer Science and Engineering Department at the University of California, San Diego, CSE 0114, La Jolla, CA 92093-0114. Computer time on the San Diego Supercomputing Center's Intel Paragon was provided by a UCSD School of Engineering Block Grant. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles. </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: 1 Introduction Dynamic, non-uniform computations arise in diverse scientific applications including particle methods <ref> [1, 2] </ref>, adaptive mesh refinement [3, 4], and the finite element method [5]. Compared with uniform methods, they offer substantially increased accuracy for the same cost. Because they concentrate computational effort unpredictably and non-uniformly with position and time, a load balancing problem arises in parallel computer implementations. <p> We restrict our attention to the class of localized computations on a uniform d-dimensional mesh, in which the computational workload varies non-uniformly over the mesh as a function of position and time. Particle methods that employ a chaining mesh (or link-cells) to organize computation <ref> [1] </ref> are a typical member of this class, as are methods that employ a multi-level adaptive chaining mesh [6]. Our results may be extended to dynamic finite element meshes, so long as the range of length scales represented at each level is not too great.
Reference: [2] <author> J. Carrier, L. Greengard, and V. Rokhlin, </author> <title> "A fast adaptive multipole algorithm for particle simulations," </title> <note> SIAM J. </note> <institution> Sci. Stat. Comput., </institution> <month> Sept. </month> <year> 1988. </year> <month> 20 </month>
Reference-contexts: 1 Introduction Dynamic, non-uniform computations arise in diverse scientific applications including particle methods <ref> [1, 2] </ref>, adaptive mesh refinement [3, 4], and the finite element method [5]. Compared with uniform methods, they offer substantially increased accuracy for the same cost. Because they concentrate computational effort unpredictably and non-uniformly with position and time, a load balancing problem arises in parallel computer implementations.
Reference: [3] <author> M. J. Berger and J. Oliger, </author> <title> "Adaptive mesh refinement for hyperbolic partial differential equations," </title> <journal> J. Comput. Phys., </journal> <volume> vol. 53, </volume> <pages> pp. 484-512, </pages> <month> Mar. </month> <year> 1984. </year>
Reference-contexts: 1 Introduction Dynamic, non-uniform computations arise in diverse scientific applications including particle methods [1, 2], adaptive mesh refinement <ref> [3, 4] </ref>, and the finite element method [5]. Compared with uniform methods, they offer substantially increased accuracy for the same cost. Because they concentrate computational effort unpredictably and non-uniformly with position and time, a load balancing problem arises in parallel computer implementations.
Reference: [4] <author> M. J. Berger and P. Colella, </author> <title> "Local adaptive mesh refinement for shock hydrodynamics," </title> <journal> J. Comput. Phys., </journal> <volume> vol. 82, </volume> <pages> pp. 64-84, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Dynamic, non-uniform computations arise in diverse scientific applications including particle methods [1, 2], adaptive mesh refinement <ref> [3, 4] </ref>, and the finite element method [5]. Compared with uniform methods, they offer substantially increased accuracy for the same cost. Because they concentrate computational effort unpredictably and non-uniformly with position and time, a load balancing problem arises in parallel computer implementations. <p> Thus, the cost of this preprocessing step can be ignored. With 13 dynamic meshes (a case we do not consider) the cost will still be reasonable; dynamic mesh methods typically regenerate meshes periodically not every timestep or iteration <ref> [4, 6] </ref>. Our ORB algorithm alternates the cutting direction from one level of recursion to the next so that the longest side over all partitions will be approximately halved every other iteration.
Reference: [5] <author> C. Johnson, </author> <title> Numerical Solution of Partial Differential Equations by the Finite Element Method. </title> <publisher> Cambridge University Press, </publisher> <year> 1987. </year>
Reference-contexts: 1 Introduction Dynamic, non-uniform computations arise in diverse scientific applications including particle methods [1, 2], adaptive mesh refinement [3, 4], and the finite element method <ref> [5] </ref>. Compared with uniform methods, they offer substantially increased accuracy for the same cost. Because they concentrate computational effort unpredictably and non-uniformly with position and time, a load balancing problem arises in parallel computer implementations.
Reference: [6] <author> A. Almgren, T. Buttke, and P. Colella, </author> <title> "A fast vortex method in three dimensions," </title> <booktitle> in Proc. 10th AIAA Comput. Fluid Dyn. Conf., </booktitle> <address> (Honolulu, HI), </address> <pages> pp. 446-455, </pages> <month> Jun. </month> <year> 1991. </year>
Reference-contexts: Particle methods that employ a chaining mesh (or link-cells) to organize computation [1] are a typical member of this class, as are methods that employ a multi-level adaptive chaining mesh <ref> [6] </ref>. Our results may be extended to dynamic finite element meshes, so long as the range of length scales represented at each level is not too great. To solve the load balancing problem efficiently, we generally trade off the quality of the load balance against the cost. <p> Thus, the cost of this preprocessing step can be ignored. With 13 dynamic meshes (a case we do not consider) the cost will still be reasonable; dynamic mesh methods typically regenerate meshes periodically not every timestep or iteration <ref> [4, 6] </ref>. Our ORB algorithm alternates the cutting direction from one level of recursion to the next so that the longest side over all partitions will be approximately halved every other iteration. <p> However, performance tradeoffs depend on the ap-plication [36]. In calculations where the particle distributions exhibit a wide range of length scales, we should not use single level mesh, but rather, adaptively refined meshes <ref> [6] </ref>. ISP applies in this case; we partition each level separately. In practice, adaptive regridding is done every few time steps, so the cost of the ISP remapping phases should be reasonable. Ou et al. applied the Hilbert curve to finite element problems [15].
Reference: [7] <author> H. D. Simon, </author> <title> "Partitioning of unstructured problems for parallel processing," </title> <type> Tech. Rep. </type> <institution> RNR-91-008, NASA Ames Res. Ctr., </institution> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: To solve the load balancing problem efficiently, we generally trade off the quality of the load balance against the cost. At one end of the spectrum are techniques such as spectral bisection <ref> [7, 8] </ref> or physical optimization [9] that can obtain an optimal partitioning but at the cost of prohibitively long partitioning times. At the other end of the spectrum are a family of techniques based on orthogonal sectioning. <p> Physical optimiza-tion [9], minimizes an energy function derived from the communication and computation characteristics of the workload. Typical approaches are based on simulated annealing, neural networks, and genetic algorithms. Another optimization technique is spectral bisection (also quad- and octasection) <ref> [7, 8] </ref>. Approaches based on optimization are attractive because they balance workloads particularly well|they are fine-grained, mapping each point individually to a processor|and they also minimize communication. However, the benefits must be weighed against the cost: long and sometimes unpredictable running times.
Reference: [8] <author> B. Hendrickson and R. Leland, </author> <title> "An improved spectral graph partitioning algorithm for mapping parallel computations," </title> <type> Tech. Rep. </type> <institution> SAND92-1460, Sandia National Lab., </institution> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: To solve the load balancing problem efficiently, we generally trade off the quality of the load balance against the cost. At one end of the spectrum are techniques such as spectral bisection <ref> [7, 8] </ref> or physical optimization [9] that can obtain an optimal partitioning but at the cost of prohibitively long partitioning times. At the other end of the spectrum are a family of techniques based on orthogonal sectioning. <p> Physical optimiza-tion [9], minimizes an energy function derived from the communication and computation characteristics of the workload. Typical approaches are based on simulated annealing, neural networks, and genetic algorithms. Another optimization technique is spectral bisection (also quad- and octasection) <ref> [7, 8] </ref>. Approaches based on optimization are attractive because they balance workloads particularly well|they are fine-grained, mapping each point individually to a processor|and they also minimize communication. However, the benefits must be weighed against the cost: long and sometimes unpredictable running times.
Reference: [9] <author> N. Mansour and G. C. Fox, </author> <title> "A comparison of load balancing methods for parallel computations," </title> <type> Tech. Rep. </type> <institution> SCCS-154, Syracuse Ctr. for Computational Sci., </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: To solve the load balancing problem efficiently, we generally trade off the quality of the load balance against the cost. At one end of the spectrum are techniques such as spectral bisection [7, 8] or physical optimization <ref> [9] </ref> that can obtain an optimal partitioning but at the cost of prohibitively long partitioning times. At the other end of the spectrum are a family of techniques based on orthogonal sectioning. A well known member of this family is Orthogonal Recursive Bisection (ORB) [10, 11]. <p> Communication overhead generally varies according to the architecture. Here we will consider the case of distributed memory architectures, including shared memory and message passing variants. 4 Several heuristics are available for partitioning meshes non-uniformly. Physical optimiza-tion <ref> [9] </ref>, minimizes an energy function derived from the communication and computation characteristics of the workload. Typical approaches are based on simulated annealing, neural networks, and genetic algorithms. Another optimization technique is spectral bisection (also quad- and octasection) [7, 8].
Reference: [10] <author> M. J. Berger and S. H. Bokhari, </author> <title> "A partitioning strategy for nonuniform problems on multiprocessors," </title> <journal> IEEE Trans. on Comput., </journal> <volume> vol. C-36, </volume> <pages> pp. 570-580, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: At the other end of the spectrum are a family of techniques based on orthogonal sectioning. A well known member of this family is Orthogonal Recursive Bisection (ORB) <ref> [10, 11] </ref>. Orthogonal sectioning techniques are fast and often effective, but may fail under certain conditions due to geometric constraints on the partitioning process. In particular, they may fail when the workload is carried in a lower dimensional subspace of physical problem space.
Reference: [11] <author> S. B. Baden, </author> <title> "Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors," </title> <journal> SIAM J. on Sci. Stat. Comput., </journal> <volume> vol. 12, </volume> <pages> pp. 145-157, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: At the other end of the spectrum are a family of techniques based on orthogonal sectioning. A well known member of this family is Orthogonal Recursive Bisection (ORB) <ref> [10, 11] </ref>. Orthogonal sectioning techniques are fast and often effective, but may fail under certain conditions due to geometric constraints on the partitioning process. In particular, they may fail when the workload is carried in a lower dimensional subspace of physical problem space. <p> Assuming that the particles do not move quickly, the boundaries should only have to move a little. This is realistic owing to the customary time step constraints on the motion of the particles <ref> [11] </ref>, and may accelerate the partitioning process. As the particles move, each processor can maintain the change in its workload. Then, only the cells near the boundary need to be considered for exchanging with the neighboring processor to balance the load. <p> The choice of numerical simulation parameters for this computation is described elsewhere [28]. A second-order accurate time integration scheme is used, that performs 2 force evaluations per timestep. As previously reported, load balancing can be performed every few timesteps <ref> [11] </ref>, and a frequency of 2 timesteps 11 is assumed throughout our measurements presented here. This is consequence of the fact that particles move gradually through the mesh due to timestep constraints. <p> The simulator collected communication statistics in a machine-independent form described below, enabling us to extrapolate performance to a wide range of architectures including those with shared memory. The simulator also computed an estimate of load balancing efficiency which has been verified experimentally <ref> [11] </ref>. Each mesh point, fl i;j contains some number of particles fi i;j , and interacts with other mesh points within c mesh points along the Manhattan coordinate directions.
Reference: [12] <author> J. P. Singh, C. Holt, J. L. Hennessy, and A. Gupta, </author> <title> "A parallel adaptive fast multipole method," </title> <booktitle> in Proc. Supercomputing '93, </booktitle> <pages> pp. 54-65, </pages> <year> 1993. </year>
Reference-contexts: In particular, they may fail when the workload is carried in a lower dimensional subspace of physical problem space. A "Medians of Medians" variant of ORB, ORB-MM (also called ORB-Final <ref> [12] </ref>), can improve load imbalance dramatically, but at a cost of introducing highly unstructured parti-tionings, especially in three or more dimensions. <p> If the array is static, then the mapping phase can be considered a preprocessing step whose time can be amortized with the subsequent partitions. Each partition request entails subdividing the line (see Fig. 6), thereby implicitly partitioning the hyperspace. ISP techniques have been applied by Singh and Hennessy <ref> [12] </ref> and Warren and Salmon [14] to hierarchical N-body methods, and by Ou et al. to the Finite Element Method [15]. We provide empirical data for uniform-mesh based particle methods, and describe the general d-dimensional ISP algorithm, simplifying Singh and Hennessy's costzones technique [16]. <p> Hence, a cut in d-dimensions introduces a workload imbalance that is carried by a d 1 dimensional hyperplane. A way around the difficulty is to section the dividing hyperplane, too (Fig. 1d); this strategy is called ORB-MM, or ORB-"Median of Medians" (also called ORB-Final <ref> [12] </ref>). However, ORB-MM generates highly irregular partitionings in three or more space dimensions (see Fig. 2), which for all practical purposes are unstructured. Since unstructured partitionings do not have a compact representation, they require additional software bookkeeping to manage non-rectangular iteration spaces appearing in application software. <p> These properties help reduce interprocessor communication and follow directly from the construction of the spacefilling mapping. In general, only some spacefilling curves have this property (e.g. see <ref> [12, 14] </ref>). While there is a grey-coded interpretation of space-filling curves, we will use a geometric one instead. The 2-dimensional Hilbert curve [23] generalized here can be thought of as a U-shaped curve which visits each of the plane's four quadrants in the first level. <p> Partitionings based on inverse space filling curves have been previously employed in hierarchical N-body tree algorithms by Warren and Salmon [14] and by Singh and Hennessy using a technique called costzones <ref> [12] </ref>. Costzones identifies 32 different orientations that must be enumerated in three dimensions, and are described in lengthly tables [16]. By comparison, we describe an ISP algorithm for an arbitrary number of space dimensions, using a simple recursive algorithm. <p> One possible drawback of ISP is that the partitions are highly irregular in physical problem space. On shared memory architectures this may not be a concern, but it does introduce additional programming overhead on message passing architectures <ref> [12, 16] </ref>. Load balance is a prime concern in many non-uniform problems. Previous load balancing algorithms either impose software overheads or deliver inadequately balanced workloads. ISP creates highly efficient partitionings that are conceptually simpler, and is a viable technique for partitioning an important class of non-uniform problems.
Reference: [13] <author> A. Sussman, J. Saltz, R. Das, S. Gupta, D. Mavriplis, and R. Ponnusamy, </author> <title> "PARTI primitives for unstructured and block structured problems," </title> <booktitle> Computing Systems in Engineering, </booktitle> <year> 1992. </year> <month> 21 </month>
Reference-contexts: Such partitionings incur software overheads in the form of additional bookkeeping, as well as communication overheads to manage distributed mapping tables <ref> [13] </ref>. 2 We describe an alternative partitioning strategy called Inverse Spacefilling Partitioning (ISP) that retains the beneficial fine-grained partitioning property of ORB-MM but without the undesirable software complications. <p> In addition, distributed mapping tables must be maintained on message passing architectures, at the cost of additional communication overheads and a further increase in the application's complexity <ref> [13] </ref>. Inverse spacefilling partitioning (ISP) is an alternative technique that shares the desirable fine-grained load balancing characteristic of ORB-MM but with simplified bookkeeping. ISP is fast| it does not incur the heavy time penalties of optimization techniques|and is therefore appropriate for dynamic problems. <p> This table must be distributed across the processors since generally it will be too large to fit in the memory of a single processor. Thus, the added level of indirection incurs additional communication overhead <ref> [13] </ref>. This situation arises in ORB-MM in three or more space dimensions, because the partitions become highly unstructured (Fig. 2). Software issues aside, it is interesting to compare the effectiveness of ORB-MM against ISP.
Reference: [14] <author> M. S. Warren and J. K. Salmon, </author> <title> "A parallel hashed oct-tree n-body algorithm," </title> <booktitle> in Proc. Supercomputing '93, </booktitle> <pages> pp. 12-21, </pages> <year> 1993. </year>
Reference-contexts: Each partition request entails subdividing the line (see Fig. 6), thereby implicitly partitioning the hyperspace. ISP techniques have been applied by Singh and Hennessy [12] and Warren and Salmon <ref> [14] </ref> to hierarchical N-body methods, and by Ou et al. to the Finite Element Method [15]. We provide empirical data for uniform-mesh based particle methods, and describe the general d-dimensional ISP algorithm, simplifying Singh and Hennessy's costzones technique [16]. <p> These properties help reduce interprocessor communication and follow directly from the construction of the spacefilling mapping. In general, only some spacefilling curves have this property (e.g. see <ref> [12, 14] </ref>). While there is a grey-coded interpretation of space-filling curves, we will use a geometric one instead. The 2-dimensional Hilbert curve [23] generalized here can be thought of as a U-shaped curve which visits each of the plane's four quadrants in the first level. <p> This is true because ISP partitionings are logically contiguous in one space dimension|and can be succinctly described|whereas ORB-MM's partitionings are inherently unstructured. Partitionings based on inverse space filling curves have been previously employed in hierarchical N-body tree algorithms by Warren and Salmon <ref> [14] </ref> and by Singh and Hennessy using a technique called costzones [12]. Costzones identifies 32 different orientations that must be enumerated in three dimensions, and are described in lengthly tables [16]. By comparison, we describe an ISP algorithm for an arbitrary number of space dimensions, using a simple recursive algorithm.
Reference: [15] <author> C.-W. Ou, S. Ranka, and G. Fox, </author> <title> "Fast mapping and remapping algorithms for irregular and adaptive problems," </title> <booktitle> in 1993 ICPDS, </booktitle> <address> (Taipei, Taiwan), </address> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Each partition request entails subdividing the line (see Fig. 6), thereby implicitly partitioning the hyperspace. ISP techniques have been applied by Singh and Hennessy [12] and Warren and Salmon [14] to hierarchical N-body methods, and by Ou et al. to the Finite Element Method <ref> [15] </ref>. We provide empirical data for uniform-mesh based particle methods, and describe the general d-dimensional ISP algorithm, simplifying Singh and Hennessy's costzones technique [16]. The primary contributions of this paper are a comparative performance analysis of ISP, and an examination of the software implications of employing it in real applications. <p> ISP applies in this case; we partition each level separately. In practice, adaptive regridding is done every few time steps, so the cost of the ISP remapping phases should be reasonable. Ou et al. applied the Hilbert curve to finite element problems <ref> [15] </ref>. Load imbalance was found to be problematic when the range of length scales in the meshes was large. One possible drawback of ISP is that the partitions are highly irregular in physical problem space.
Reference: [16] <author> J. P. Singh, </author> <title> Parallel Hierarchical N-Body Methods and their Implications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford Univ., Dept. of Electrical Engineering, </institution> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: We provide empirical data for uniform-mesh based particle methods, and describe the general d-dimensional ISP algorithm, simplifying Singh and Hennessy's costzones technique <ref> [16] </ref>. The primary contributions of this paper are a comparative performance analysis of ISP, and an examination of the software implications of employing it in real applications. <p> Costzones identifies 32 different orientations that must be enumerated in three dimensions, and are described in lengthly tables <ref> [16] </ref>. By comparison, we describe an ISP algorithm for an arbitrary number of space dimensions, using a simple recursive algorithm. We have generalized the algorithm to non-hypercubic spaces, and to adaptive rectangular meshes with variable mesh spacings [34]. <p> One possible drawback of ISP is that the partitions are highly irregular in physical problem space. On shared memory architectures this may not be a concern, but it does introduce additional programming overhead on message passing architectures <ref> [12, 16] </ref>. Load balance is a prime concern in many non-uniform problems. Previous load balancing algorithms either impose software overheads or deliver inadequately balanced workloads. ISP creates highly efficient partitionings that are conceptually simpler, and is a viable technique for partitioning an important class of non-uniform problems.
Reference: [17] <author> S. B. Baden and S. R. Kohn, </author> <title> "A comparison of load balancing strategies for particle methods running on mimd multiprocessors," </title> <booktitle> in Proc. Fifth SIAM Conf. on Parallel Processing for Sci. </booktitle> <institution> Comput., (Houston, TX), </institution> <month> Mar. </month> <year> 1991. </year>
Reference-contexts: To this end we define the workload balance efficiency W 1 =(pW p ), where W 1 is the total workload, i.e. computation time on one processor, and W p is the maximum workload over all p processors <ref> [17] </ref>: W 1 = i=1 p i=1 In other words, W p is the parallel computation time on p processors, ignoring all overhead costs, and all computation is assumed to be parallelizable.
Reference: [18] <author> S. H. Bokhari, T. W. Crockett, and D. M. Nicol, </author> <title> "Parametric binary dissection," </title> <type> Tech. Rep. ICASE 93-39, </type> <institution> NASA Langley Res. Ctr., </institution> <month> Jul. </month> <year> 1993. </year>
Reference-contexts: Since we are interested in dynamic problems, we will not consider optimization methods any further. We can reduce the cost of load balancing by constraining the partitioning process, through a family of techniques we call orthogonal sectioning (see Fig. 1). Orthogonal Recursive Bisection (ORB)[10] and rectilinear partitioning <ref> [18] </ref> are typical techniques. Orthogonal sectioning algorithms reduce the higher dimensional partitioning problem to a set of simpler 1-dimensional partitioning problems, successively partitioning along orthogonal coordinate directions.
Reference: [19] <author> D. W. Walker, </author> <title> "The hierarchical spatial decomposition of three-dimensional particle-in-cell plasma simulations on mimd distributed memory multiprocessors," </title> <type> Tech. Rep. </type> <institution> ORNL/TM-12071, Oak Ridge Nat. Lab., </institution> <month> Jul. </month> <year> 1992. </year>
Reference-contexts: The cutting direction may be varied at each level of the recursion to avoid elongated partitions that could lead to poorly-balanced workloads. Another variant, called ORB-H or hierarchical ORB <ref> [19] </ref>, (Fig. 1b) splits the problem into p &gt; 2 strips (for pq processors) and recurses on each strip. Dynamic problems often exhibit temporal locality, whereby the solution changes gradually, for example, due to a timestep constraint.
Reference: [20] <author> R. v. Hanxleden and L. R. Scott, </author> <title> "Load balancing on message passing architectures," </title> <journal> J. Parallel Distr. Comput., </journal> <volume> vol. 13, </volume> <pages> pp. 312-324, </pages> <year> 1991. </year>
Reference-contexts: Dynamic problems often exhibit temporal locality, whereby the solution changes gradually, for example, due to a timestep constraint. An incremental approach to orthogonal sectioning may effectively exploit temporal locality by computing work gradients across the partition boundaries and shifting the boundaries accordingly <ref> [20] </ref>. However, complications arise when a boundary moves, since all other boundaries introduced later on in the recursion will be affected. In effect, we trade off temporal locality against spatial locality, and the trade-off may not be favorable.
Reference: [21] <author> D. M. Nicol, </author> <title> "Rectilinear partitioning of irregular data parallel computations," </title> <type> Tech. Rep. 91-55, </type> <institution> ICASE, NASA Langley Res. Ctr., </institution> <month> Jul. </month> <year> 1991. </year>
Reference-contexts: In effect, we trade off temporal locality against spatial locality, and the trade-off may not be favorable. An approach which exploits temporal locality without the drawback of disrupting spatial locality is Nicol's rectilinear partitioning strategy <ref> [21] </ref> 5 (Fig. 1c). This strategy avoids the difficulty of awkward workload adjustments by impos-ing partitionings with fixed connectivity: partitionings are formed by the tensor product of strips taken across orthogonal coordinate directions. Load balance is improved by iteratively adjusting cuts independently along each problem dimension as necessary.
Reference: [22] <author> L. K. Platzman and J. J. B. III, </author> <title> "Spacefilling curves and the planar travelling salesman problem," </title> <journal> J. ACM, </journal> <volume> vol. 36, </volume> <pages> pp. 719-737, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: of the space. 6 For any particular point in the space, the curve can come arbitrarily close with a finite level, hence the name "spacefilling." Spacefilling curves also preserve spatial locality in the higher dimensional space and have been employed previously to approximate a solution to the Traveling Salesman Problem <ref> [22] </ref>. ISP is parameterized by the type of spacefilling curve employed. The ISP partitioning strategy then, consists of a curve-dependent part|generating the mapping| and a curve-independent part|generating the partitioning. We next describe how to generate the mapping.
Reference: [23] <author> H. Abelson and A. A. diSessa, </author> <title> Turtle Geometry, The Computer as a Medium for Exploring Mathematics. </title> <publisher> The MIT Press, </publisher> <year> 1984. </year>
Reference-contexts: These properties help reduce interprocessor communication and follow directly from the construction of the spacefilling mapping. In general, only some spacefilling curves have this property (e.g. see [12, 14]). While there is a grey-coded interpretation of space-filling curves, we will use a geometric one instead. The 2-dimensional Hilbert curve <ref> [23] </ref> generalized here can be thought of as a U-shaped curve which visits each of the plane's four quadrants in the first level. Each subsequent level divides the previous level's quadrants into four finer quadrants which are visited by U shape curves themselves (Fig. 4).
Reference: [24] <author> C. Ozturan, B. Szymanski, and J. E. Flaherthy, </author> <title> "Adaptive methods and rectangular partitioning problem," </title> <booktitle> in Proc. 1992 Scalable High Perf. Comput. Conf., </booktitle> <address> (Williamsburg, VA), </address> <pages> pp. 409-415, </pages> <month> Mar. </month> <year> 1992. </year> <month> 22 </month>
Reference-contexts: This partitioning strategy is less than optimal in efficiency. There exists an optimal O (pn 3d ) algorithm which reduces the problem of partitioning a 1-dimensional grid to a shortest-path-like problem <ref> [24] </ref>. Although this is optimal for the mapped line, it may not be optimal for the hyperspace since there could be other mapped lines which have better partitions.
Reference: [25] <author> A. J. Chorin, </author> <title> "Numerical study of slightly viscous flow," </title> <journal> J. Fluid Mech., </journal> <volume> vol. 57, </volume> <pages> pp. </pages> <address> 785--796, </address> <year> 1973. </year>
Reference-contexts: Given current technological trends, scalability may be a moot point. 10 5 Evaluation In this section we compare the performance of ISP against two competitors: ORB and ORB-MM. We report empirical measurements for two particle applications: vortex dynamics <ref> [25] </ref> in two dimensions and smoothed particle hydrodynamics in three dimensions [26]. We make two major observations. First, ISP and ORB-MM do an excellent and comparable job of balancing workloads and are superior to ORB. <p> The equations are solved in vorticity-stream function form using Chorin's vortex blob method <ref> [25] </ref>. An N-body problem arises in the computation, and we solve this using a rapid N-body solver known as Anderson's Method of Local Corrections [27].
Reference: [26] <author> J. J. Monaghan, </author> <title> "Smoothed particle hydrodynamics," </title> <booktitle> Annual Review of Astronomy and Astrophysics, </booktitle> <volume> vol. 30, </volume> <pages> pp. 543-574, </pages> <year> 1992. </year>
Reference-contexts: Given current technological trends, scalability may be a moot point. 10 5 Evaluation In this section we compare the performance of ISP against two competitors: ORB and ORB-MM. We report empirical measurements for two particle applications: vortex dynamics [25] in two dimensions and smoothed particle hydrodynamics in three dimensions <ref> [26] </ref>. We make two major observations. First, ISP and ORB-MM do an excellent and comparable job of balancing workloads and are superior to ORB. Second, the communication overhead of all three algorithms is comparable, especially on systems that support fine-grain communication. <p> Surprisingly, ORB tends to incur the least communication volume during load balancing, though this effect is not so significant since relatively few particles are communicated during load balancing. 5.2 Smoothed Particle Hydrodynamics We consider a second application, smoothed particle hydrodynamics (SPH) in three dimensions <ref> [26] </ref>. SPH is similar to the vortex dynamics application, except that it computes short-range interactions only. The implementation of SPH is described elsewhere [33]. Again, we employed measurements from production runs and from trace-driven simulations. In this case the production runs were performed on the Intel Paragon.
Reference: [27] <author> C. R. Anderson, </author> <title> "A method of local corrections for computing the velocity field due to a distribution of vortex blobs," </title> <journal> J. Comput. Phys., </journal> <volume> vol. 62, </volume> <pages> pp. 111-123, </pages> <year> 1986. </year>
Reference-contexts: The equations are solved in vorticity-stream function form using Chorin's vortex blob method [25]. An N-body problem arises in the computation, and we solve this using a rapid N-body solver known as Anderson's Method of Local Corrections <ref> [27] </ref>. Anderson's method spends most of its time computing interactions among nearby particles, and the cost of updating a particle's position is proportional to the number of interacting neighbors. We ignore a separate phase which computes global interactions, as it is amenable to static, uniform partitioning.
Reference: [28] <author> S. B. Baden, </author> <title> "Very large vortex calculations in two dimensions," </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> vol. 1360, </volume> <pages> pp. 96-120, </pages> <year> 1988. </year>
Reference-contexts: We ignore a separate phase which computes global interactions, as it is amenable to static, uniform partitioning. The initial data consist of a regular array of approximately 13,000 particles confined to two disks centered about the origin. The choice of numerical simulation parameters for this computation is described elsewhere <ref> [28] </ref>. A second-order accurate time integration scheme is used, that performs 2 force evaluations per timestep. As previously reported, load balancing can be performed every few timesteps [11], and a frequency of 2 timesteps 11 is assumed throughout our measurements presented here. <p> The work assigned to a processor is the sum of all i;j for the mesh points assigned to it. We may adjust the size of the mesh to reduce load imbalance, and the interaction distance c scales with n. Here we chose c = n=64 <ref> [28] </ref>. For various n and p, the simulator partitioned each snapshot with ISP, ORB, and ORB-MM. It measured partitioning time, load balance efficiency, and communication, averaging each statistic over all 750 snapshots.
Reference: [29] <author> S. B. Baden and S. R. Kohn, </author> <title> "Portable parallel programming of numerical problems under the LPAR system," </title> <journal> J. </journal> <note> Parallel Distr. Comput., 1995. To appear. </note>
Reference-contexts: Production computations were performed on an iPSC/860. The application was written in C++ under the LPAR system <ref> [29] </ref>, and employed ORB partitioning only. Runs employed 12,848 particles and lasted 50 timesteps. Additional details about the computation have been reported elsewhere [30]. Simulations were conducted on a SPARC-10 workstation. The simulator used trace data from a production calculation run on a Cray X-MP.
Reference: [30] <author> S. R. Kohn and S. B. Baden, </author> <title> "An implementation of the LPAR parallel programming model for scientific computations," </title> <booktitle> in Proc. Sixth SIAM Conf. on Parallel Processing for Sci. </booktitle> <publisher> Comput., </publisher> <address> (Norfolk, VA), </address> <pages> pp. 759-766, </pages> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: Production computations were performed on an iPSC/860. The application was written in C++ under the LPAR system [29], and employed ORB partitioning only. Runs employed 12,848 particles and lasted 50 timesteps. Additional details about the computation have been reported elsewhere <ref> [30] </ref>. Simulations were conducted on a SPARC-10 workstation. The simulator used trace data from a production calculation run on a Cray X-MP. This calculation employed 12,788 particles and ran for 1500 timesteps. Trace data were sampled every other timestep for a total of 750 snapshots.
Reference: [31] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam, </author> <title> "The Stanford DASH multiprocessor," </title> <journal> IEEE Computer, </journal> <volume> vol. 25, </volume> <pages> pp. 63-79, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: While this characterization would appear to apply to message passing architectures only, in fact it also applies to distributed shared memory architectures, e.g. Stanford DASH <ref> [31] </ref>. Dependence information is transmitted between caches, and in the interests of enhancing locality, particles are transmitted between the private memories during load balancing and migration, just as they are in a message passing architecture implementation.
Reference: [32] <author> B. Falsafi, A. R. Lebeck, S. K. Reinhardt, I. Schoinas, M. D. Hill, J. R. Larus, A. Rogers, and D. A. Wood, </author> <title> "Application-specific protocols for user-level shared memory," </title> <booktitle> in Proc. Supercomputing '94, </booktitle> <address> (Wash., D.C.), </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: However, our analysis does not treat contention on the interconnect. We assume that communication employs a producer-consumer protocol in lieu of a costly request-reply protocol on both shared memory and message passing architectures, which also avoids a costly write-invalidate cycle on shared memory architectures <ref> [32] </ref>. A consequence of this is communication traffic on message passing and shared memory architectures looks the same. In all our communication measurements we assume 8-byte double precision numbers. Each particle consists of 3 values plus some temporary quantities required by time integration.
Reference: [33] <author> S. R. Kohn and S. B. Baden, </author> <title> "A robust parallel programming model for dynamic nonuniform scientific computations," </title> <booktitle> in Proc. 1994 Scalable High Perf. Comput. Conf., </booktitle> <address> (Knoxville, TN), </address> <month> May </month> <year> 1994. </year>
Reference-contexts: SPH is similar to the vortex dynamics application, except that it computes short-range interactions only. The implementation of SPH is described elsewhere <ref> [33] </ref>. Again, we employed measurements from production runs and from trace-driven simulations. In this case the production runs were performed on the Intel Paragon. The initial data consisted of 48,760 particles distributed uniformly within one thin disk. In effect, the workload is restricted to a two-dimensional subset of three-dimensional space.
Reference: [34] <author> J. R. Pilkington and S. B. Baden, </author> <title> "Partitioning with spacefilling curves," </title> <type> Tech. Rep. </type> <institution> CS94-349, Univ. of Calif., San Diego, Dept. of Computer Science and Engineering, </institution> <year> 1994. </year> <month> 23 </month>
Reference-contexts: By comparison, we describe an ISP algorithm for an arbitrary number of space dimensions, using a simple recursive algorithm. We have generalized the algorithm to non-hypercubic spaces, and to adaptive rectangular meshes with variable mesh spacings <ref> [34] </ref>. Ou and Ranka [35] have applied the Hilbert curve to a "free particle" method in which a chaining mesh is not used to organize the particles.
Reference: [35] <author> C.-W. Ou and S. Ranka, </author> <title> "Parallel remapping algorithms for adaptive problems," </title> <booktitle> in Frontiers '95, </booktitle> <address> (McLean, VA), </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: By comparison, we describe an ISP algorithm for an arbitrary number of space dimensions, using a simple recursive algorithm. We have generalized the algorithm to non-hypercubic spaces, and to adaptive rectangular meshes with variable mesh spacings [34]. Ou and Ranka <ref> [35] </ref> have applied the Hilbert curve to a "free particle" method in which a chaining mesh is not used to organize the particles. An advantage of employing a chaining mesh over free particles is that communication is coarse-grained, occurring in units mesh 19 boxes rather than individual particles.
Reference: [36] <author> S. M. Figueira and S. B. Baden, </author> <title> "Performance analysis of parallel strategies for localized n-body solvers," </title> <booktitle> in Proc. Sixth SIAM Conf. on Parallel Processing for Sci. </booktitle> <publisher> Comput., </publisher> <address> (San Francisco, CA), </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: An advantage of employing a chaining mesh over free particles is that communication is coarse-grained, occurring in units mesh 19 boxes rather than individual particles. However, performance tradeoffs depend on the ap-plication <ref> [36] </ref>. In calculations where the particle distributions exhibit a wide range of length scales, we should not use single level mesh, but rather, adaptively refined meshes [6]. ISP applies in this case; we partition each level separately.

References-found: 36

