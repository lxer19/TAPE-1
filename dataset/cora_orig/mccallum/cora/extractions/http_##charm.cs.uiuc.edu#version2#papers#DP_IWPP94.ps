URL: http://charm.cs.uiuc.edu/version2/papers/DP_IWPP94.ps
Refering-URL: http://charm.cs.uiuc.edu/version2/papers/DP_IWPP94.html
Root-URL: http://www.cs.uiuc.edu
Email: kornkven@cs.uiuc.edu kale@cs.uiuc.edu  
Title: Efficient Implementation of High Performance Fortran via Adaptive Scheduling-An Overview  
Author: Edward A. Kornkven Laxmikant V. Kale 
Address: Urbana, IL 61801 Urbana, IL 61801  
Affiliation: Department of Computer Science Department of Computer Science University Of Illinois University Of Illinois  
Abstract: We have been developing a compiler for a subset of High Performance Fortran. We have shown that generating message-driven code provides an opportunity for improved efficiency in the presence of communication. By utilizing a notation called Dagger, we are able to schedule work efficiently without relying on complicated and unreliable compile-time approaches. This paper gives an overview of our approach and reports on the project's status. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: To our knowledge, all of these efforts rely on statically scheduling program tasks. Finally, we note that previous work on dependence analysis <ref> [10, 1, 12] </ref> and computing static single assignment forms [3] has been helpful to our analysis.
Reference: [2] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF compiler for distributed memory MIMD computers. </title> <booktitle> In Proc. of Supercomputing '93, </booktitle> <pages> pages 351-360, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Details about DP and its implementation will be found in [9]. 5 Related Research Numerous efforts have been undertaken in recent years to compile languages based on data parallelism to MIMD machines. Among these are numerous Fortran variants <ref> [4, 15, 16, 2, 11] </ref>. Many of these efforts explicitly address the problem of generating efficient communication. What distinguishes our work is that the order in which tasks are performed is determined at run-time according to our adaptive schedule. <p> We know of no other machine-independent run-time system that offers the asynchronous, message-driven virtual machine to which we compile. Some examples of recent Fortran projects in which efforts are made to overlap communication and computation and/or improve load balance include Fortran D [15], Vienna Fortran [16], Fortran 90D/HPF <ref> [2] </ref>, Data Parallel Fortran [4], and the Crystallizing Fortran Project [11]. To our knowledge, all of these efforts rely on statically scheduling program tasks. Finally, we note that previous work on dependence analysis [10, 1, 12] and computing static single assignment forms [3] has been helpful to our analysis.
Reference: [3] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and F. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> TOPLAS, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: To our knowledge, all of these efforts rely on statically scheduling program tasks. Finally, we note that previous work on dependence analysis [10, 1, 12] and computing static single assignment forms <ref> [3] </ref> has been helpful to our analysis.
Reference: [4] <author> Pablo M. Elustondo, Lelia A. Vazquez, Os-valdo J. Nestares, Julio Sanchez Avalos, Guillermo A. Alvarez, Ching-Tien Ho, and Jorge L.C. Sanz. </author> <title> Data parallel Fortran. </title> <booktitle> In Proc. of Frontiers '92, </booktitle> <pages> pages 21-28, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Details about DP and its implementation will be found in [9]. 5 Related Research Numerous efforts have been undertaken in recent years to compile languages based on data parallelism to MIMD machines. Among these are numerous Fortran variants <ref> [4, 15, 16, 2, 11] </ref>. Many of these efforts explicitly address the problem of generating efficient communication. What distinguishes our work is that the order in which tasks are performed is determined at run-time according to our adaptive schedule. <p> Some examples of recent Fortran projects in which efforts are made to overlap communication and computation and/or improve load balance include Fortran D [15], Vienna Fortran [16], Fortran 90D/HPF [2], Data Parallel Fortran <ref> [4] </ref>, and the Crystallizing Fortran Project [11]. To our knowledge, all of these efforts rely on statically scheduling program tasks. Finally, we note that previous work on dependence analysis [10, 1, 12] and computing static single assignment forms [3] has been helpful to our analysis.
Reference: [5] <author> A. Gursoy and L.V. Kale. Dagger: </author> <title> Combining the benefits of synchronous and asynchronous communication styles. </title> <booktitle> In Proc. of IPPS '94, </booktitle> <pages> pages 590-596, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: There are no receive statements in the language. A chare that sends a message continues executing the entire entry point and then suspends, waiting for another message. In the meantime, other messages that have arrived are de-queued and execution continues, possibly in another chare. Dagger <ref> [5] </ref> runs on top of Charm and is a notation for expressing computations that can be represented by directed graphs. Dagger facilitates the expression of the local synchronization of threads of execution by specifying the condition under which a thread executes in terms of predecessor threads that must complete first. <p> The reader is referred to <ref> [5] </ref> for more details. The details of the design of the Dagger code generation algorithm may be found in [8]. 3 Performance Using a synthetic program consisting of eight WHERE statements separated by SUM calls, we conducted some experiments to determine the potential effectiveness of the scheduling method described here.
Reference: [6] <author> HPF Forum. </author> <title> High Performance Fortran Language Specification, </title> <address> 1.0 edition, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: We have implemented this technique in a compiler for a subset of High Performance Fortran (HPF) <ref> [6] </ref> called DP. DP supports operations on entire arrays such as arithmetic operations and assignment, an assortment of array- and scalar-oriented in-trinsics, and HPF's control structures: indexed DO, DO WHILE, FORALL and if statements.
Reference: [7] <author> L.V. Kale. </author> <title> The Chare Kernel parallel programming language and system. </title> <booktitle> In Proc. of ICPP '90, </booktitle> <volume> volume II, </volume> <pages> pages 17-25, </pages> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: Arrays are distributed among processors using HPF's array distribution directives, and each processor computes its own portion of an array. The schedule and computations are specified using a language called Dagger, which is part of the Charm parallel programming system <ref> [7] </ref>. Charm is an explicitly parallel language with a message-driven execution model. Since Charm is machine independent and has already been ported to numerous shared-memory and nonshared-memory parallel machines, the code produced by the compiler is machine independent by default.
Reference: [8] <author> E.A. Kornkven. </author> <title> Dynamic adaptive scheduling in an implementation of a data parallel language. </title> <type> Technical Report 92-10, </type> <institution> Parallel Programming Lab., Dept. of Computer Science, University of Illinois, </institution> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The reader is referred to [5] for more details. The details of the design of the Dagger code generation algorithm may be found in <ref> [8] </ref>. 3 Performance Using a synthetic program consisting of eight WHERE statements separated by SUM calls, we conducted some experiments to determine the potential effectiveness of the scheduling method described here. The characteristics of this example that make it useful for our purposes are the following: 1.
Reference: [9] <author> E.A. Kornkven. </author> <title> Exploiting Message-Driven Execution to Compile Data Parallel Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: We plan to explore these opportunities as well as extend and improve the DP compiler and further evaluate its performance. Details about DP and its implementation will be found in <ref> [9] </ref>. 5 Related Research Numerous efforts have been undertaken in recent years to compile languages based on data parallelism to MIMD machines. Among these are numerous Fortran variants [4, 15, 16, 2, 11]. Many of these efforts explicitly address the problem of generating efficient communication.
Reference: [10] <author> D.J. Kuck, R.H. Kuhn, D.A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proc. of POPL '81, </booktitle> <pages> pages 207-218, </pages> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: To our knowledge, all of these efforts rely on statically scheduling program tasks. Finally, we note that previous work on dependence analysis <ref> [10, 1, 12] </ref> and computing static single assignment forms [3] has been helpful to our analysis.
Reference: [11] <author> Jingke Li and Marina Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> TPDS, </journal> <volume> 2(3) </volume> <pages> 361-375, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Details about DP and its implementation will be found in [9]. 5 Related Research Numerous efforts have been undertaken in recent years to compile languages based on data parallelism to MIMD machines. Among these are numerous Fortran variants <ref> [4, 15, 16, 2, 11] </ref>. Many of these efforts explicitly address the problem of generating efficient communication. What distinguishes our work is that the order in which tasks are performed is determined at run-time according to our adaptive schedule. <p> Some examples of recent Fortran projects in which efforts are made to overlap communication and computation and/or improve load balance include Fortran D [15], Vienna Fortran [16], Fortran 90D/HPF [2], Data Parallel Fortran [4], and the Crystallizing Fortran Project <ref> [11] </ref>. To our knowledge, all of these efforts rely on statically scheduling program tasks. Finally, we note that previous work on dependence analysis [10, 1, 12] and computing static single assignment forms [3] has been helpful to our analysis.
Reference: [12] <author> William Pugh. </author> <title> A practical algorithm for exact dependence analysis. </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: To our knowledge, all of these efforts rely on statically scheduling program tasks. Finally, we note that previous work on dependence analysis <ref> [10, 1, 12] </ref> and computing static single assignment forms [3] has been helpful to our analysis.
Reference: [13] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors. </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: In simplified terms, when a block's predecessors in the graph have completed, the block is labelled as ready for execution by the run-time system (which also selects and initiates ready blocks). Execution begins with blocks which have no predecessors and continues in a macro-dataflow fashion <ref> [13] </ref> until execution is complete. In summary, our approach depends on three components: 1. A message-driven run-time substrate to provide the ability to execute a multi-threaded schedule that is adaptable to run-time conditions. 2. The ability to express the computation as a partial ordering of sub-computations. 3.
Reference: [14] <author> A.B. Sinha. </author> <title> Performance Analysis of Message Driven Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: We compared the multithreaded code generated by the DP compiler to a single-threaded version of the same program. The two versions of the program were run on a 32-processor CM-5 and analyzed using Projections <ref> [14] </ref>, a performance analysis tool for Charm. Using this tool, we are able to clearly see the performance characteristics of the two versions of the program by looking at overall system utilization.
Reference: [15] <author> C.W. Tseng, S. Hirananadani, and K. Kennedy. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proc. of Supercomputing '93, </booktitle> <pages> pages 338-350, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Details about DP and its implementation will be found in [9]. 5 Related Research Numerous efforts have been undertaken in recent years to compile languages based on data parallelism to MIMD machines. Among these are numerous Fortran variants <ref> [4, 15, 16, 2, 11] </ref>. Many of these efforts explicitly address the problem of generating efficient communication. What distinguishes our work is that the order in which tasks are performed is determined at run-time according to our adaptive schedule. <p> We know of no other machine-independent run-time system that offers the asynchronous, message-driven virtual machine to which we compile. Some examples of recent Fortran projects in which efforts are made to overlap communication and computation and/or improve load balance include Fortran D <ref> [15] </ref>, Vienna Fortran [16], Fortran 90D/HPF [2], Data Parallel Fortran [4], and the Crystallizing Fortran Project [11]. To our knowledge, all of these efforts rely on statically scheduling program tasks.
Reference: [16] <author> H. Zima, B. Chapman, H. Moritsch, and P. Mehrotra. </author> <title> Dynamic data distribution in Vi-enna Fortran. </title> <booktitle> In Proc. of Supercomputing '93, </booktitle> <pages> pages 284-293, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Details about DP and its implementation will be found in [9]. 5 Related Research Numerous efforts have been undertaken in recent years to compile languages based on data parallelism to MIMD machines. Among these are numerous Fortran variants <ref> [4, 15, 16, 2, 11] </ref>. Many of these efforts explicitly address the problem of generating efficient communication. What distinguishes our work is that the order in which tasks are performed is determined at run-time according to our adaptive schedule. <p> We know of no other machine-independent run-time system that offers the asynchronous, message-driven virtual machine to which we compile. Some examples of recent Fortran projects in which efforts are made to overlap communication and computation and/or improve load balance include Fortran D [15], Vienna Fortran <ref> [16] </ref>, Fortran 90D/HPF [2], Data Parallel Fortran [4], and the Crystallizing Fortran Project [11]. To our knowledge, all of these efforts rely on statically scheduling program tasks.
References-found: 16

