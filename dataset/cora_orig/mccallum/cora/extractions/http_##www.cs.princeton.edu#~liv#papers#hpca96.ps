URL: http://www.cs.princeton.edu/~liv/papers/hpca96.ps
Refering-URL: http://www.cs.princeton.edu/~felten/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Improving Release-Consistent Shared Virtual Memory using Automatic Update  
Author: Liviu Iftode, Cezary Dubnicki, Edward W. Felten and Kai Li 
Address: Princeton, NJ 08544  
Affiliation: Department of Computer Science Princeton University  
Date: February 1996.  
Note: In Proceedings of the 2nd International Symposium on High-Performance Comp uter Architecture,  
Abstract: Automatic update is a simple communication mechanism, implemented in the SHRIMP multicomputer, that forwards local writes to remote memory transparently. In this paper we propose a new lazy release consistency based protocol, called Automatic Update Release Consistency (AURC), that uses automatic update to propagate and merge shared memory modifications. We compare the performance of this protocol against a software-only LRC implementation on several Splash-2 applications and show that the AURC approach can substantially improve the performance of LRC. For 16 processors, the average speedup has increased from 5.9 under LRC, to 8.3 under AURC. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the IEEE COMPCON '93 Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes <ref> [5, 6, 14, 1, 23, 24, 14, 7] </ref>. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable. <p> Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7]. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency <ref> [1] </ref>, in which all shared data is explicitly associated with some synchronization variable. The TreadMarks library [13] is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses lazy release consistency and allows for multiple writers.
Reference: [2] <author> R. Bisiani and M. Ravishankar. </author> <title> Plus: A distributed shared-memory system. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Blizzard-S [25] rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing; for well-structured programs it imposes substantial overhead compared to more traditional shared virtual memory implementations. The PLUS <ref> [2] </ref>, Galactica Net [12], Merlin [22] and its successor SESAME [27], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. These systems do more in hardware, and thus are more expensive and complicated to build.
Reference: [3] <author> M. Blumrich, C. Dubnicki, E. Felten, K. Li, and M. Mesarina. </author> <title> Virtual memory mapped network interfaces. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 21-28, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The SHRIMP multicomputer, currently being built at Princeton, aims to reduce message-passing overhead by designing new network interface hardware. Our recent research shows that the "virtual memory mapped communication" technique used in SHRIMP can reduce the overhead of sending a message to a few user-level instructions <ref> [4, 3] </ref>. The computing nodes are Pentium PCs running the Linux operating system. This architecture allows selecting write-through or write-back caching mode per page basis. The interconnection is a comercially available network which delivers packets reliably in FIFO order.
Reference: [4] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The SHRIMP multicomputer, currently being built at Princeton, aims to reduce message-passing overhead by designing new network interface hardware. Our recent research shows that the "virtual memory mapped communication" technique used in SHRIMP can reduce the overhead of sending a message to a few user-level instructions <ref> [4, 3] </ref>. The computing nodes are Pentium PCs running the Linux operating system. This architecture allows selecting write-through or write-back caching mode per page basis. The interconnection is a comercially available network which delivers packets reliably in FIFO order.
Reference: [5] <author> L. Borrmann and M. Herdieckerhoff. </author> <title> A coherency model for virtually shared memory. </title> <booktitle> In 1990 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes <ref> [5, 6, 14, 1, 23, 24, 14, 7] </ref>. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable.
Reference: [6] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes <ref> [5, 6, 14, 1, 23, 24, 14, 7] </ref>. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable.
Reference: [7] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Ra-jamony, and W. Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Several recent studies indicate that a key to closing the performance gap is cooperation between the network interface and shared virtual memory software systems. Recent studies show that the overhead of coherence protocol messages can affect the performance of a "software-only" shared memory system significantly <ref> [14, 7] </ref>, and that on distributed memory architectures with remote memory reference capability, the performance of software cache coherence maintained at the virtual memory page level is competitive with that of hardware cache coherence schemes [23, 24, 15]. <p> It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes <ref> [5, 6, 14, 1, 23, 24, 14, 7] </ref>. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable.
Reference: [8] <author> S. Dwarkadas, P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: It then fetches the diffs (Figure 2) from other nodes and applies the diffs to its copy of the page in the proper causal order. Further details about lazy release consistency implementations can be found in <ref> [8] </ref> and [13]. 4 Automatic Update Release Consis tency In this section we describe three protocols which use automatic update to implement lazy release consistency.
Reference: [9] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The network interface generates and sends out a network packet while the CPU executes subsequent instructions. 3 Lazy Release Consistency Lazy release consistency [14] (LRC) is a variant of release consistency <ref> [9] </ref> (RC). Release consistency guarantees memory consistency only after explicit synchronization points, allowing the SVM system to hide the latency of update propagation by using either buffering or pipelining techniques. <p> It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models <ref> [9] </ref> allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7].
Reference: [10] <author> Richard Gillett. </author> <title> Memory channel network for pci. </title> <booktitle> In Proceedings of Hot Interconnects '95 Symposium, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: Memory Channel <ref> [10] </ref> network allows remote memory to be mapped into the local virtual address space but without a corresponding local memory mapping.
Reference: [11] <author> S.A. Herrod. </author> <title> TangoLite; A Multiprocessor Simulation Environment. </title> <institution> Computer Systems Laboratory, Stan-ford University, </institution> <year> 1994. </year>
Reference-contexts: To accomplish this task, we have developed a shared virtual memory coherence protocol based on the lazy release consistency model. To understand the performance implications of this approach, we have implemented both our method and the "software-only" shared virtual memory approach within the TangoLite simulation framework <ref> [11] </ref>. We conducted our simulation studies with the same architectural and systems configurations except that our approach uses the automatic update feature of the network interface. <p> In addition, while applying the Copyset-2 part of the protocol, eager release consistency can be maintained without page transfers. 5 Performance To evaluate the performance of our SVM protocols we implement them on a SHRIMP architecture simulator built within the TangoLite execution-driven multiprocessor simulation framework <ref> [11] </ref>. 5.1 The Architectural Model We consider a basic architectural model close to the SHRIMP multicomputer architecture with commodity PC Pentium boards as nodes (Figure 7).
Reference: [12] <author> Andrew W. Wilson Jr. Richard P. LaRowe Jr. and Marc J. Teller. </author> <title> Hardware assist for distributed shared memory. </title> <booktitle> In Proceedings of 13th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 246-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Blizzard-S [25] rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing; for well-structured programs it imposes substantial overhead compared to more traditional shared virtual memory implementations. The PLUS [2], Galactica Net <ref> [12] </ref>, Merlin [22] and its successor SESAME [27], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. These systems do more in hardware, and thus are more expensive and complicated to build.
Reference: [13] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: It then fetches the diffs (Figure 2) from other nodes and applies the diffs to its copy of the page in the proper causal order. Further details about lazy release consistency implementations can be found in [8] and <ref> [13] </ref>. 4 Automatic Update Release Consis tency In this section we describe three protocols which use automatic update to implement lazy release consistency. <p> Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7]. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable. The TreadMarks library <ref> [13] </ref> is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses lazy release consistency and allows for multiple writers. This implementation provides respectable performance in the absence of fine-grain sharing. Software-only techniques to control fine-grain accesses to shared memory have been proposed.
Reference: [14] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Several recent studies indicate that a key to closing the performance gap is cooperation between the network interface and shared virtual memory software systems. Recent studies show that the overhead of coherence protocol messages can affect the performance of a "software-only" shared memory system significantly <ref> [14, 7] </ref>, and that on distributed memory architectures with remote memory reference capability, the performance of software cache coherence maintained at the virtual memory page level is competitive with that of hardware cache coherence schemes [23, 24, 15]. <p> The network interface generates and sends out a network packet while the CPU executes subsequent instructions. 3 Lazy Release Consistency Lazy release consistency <ref> [14] </ref> (LRC) is a variant of release consistency [9] (RC). Release consistency guarantees memory consistency only after explicit synchronization points, allowing the SVM system to hide the latency of update propagation by using either buffering or pipelining techniques. <p> It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes <ref> [5, 6, 14, 1, 23, 24, 14, 7] </ref>. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable. <p> Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7]. New coherency protocols improving the performance of release consistency include lazy release consistency <ref> [14] </ref>, and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable. The TreadMarks library [13] is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses lazy release consistency and allows for multiple writers.
Reference: [15] <author> L.I. </author> <title> Kontothanassis and M.L. Scott. Software cache coherence for large scale multiprocessors. </title> <booktitle> In Proceedings of the First International Symposium on High Performance Computer Architecture, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: of coherence protocol messages can affect the performance of a "software-only" shared memory system significantly [14, 7], and that on distributed memory architectures with remote memory reference capability, the performance of software cache coherence maintained at the virtual memory page level is competitive with that of hardware cache coherence schemes <ref> [23, 24, 15] </ref>. These results suggest that appropriate architectural support may improve the performance of shared virtual memory substantially. The SHRIMP project at Princeton studies how to provide high-performance communication mechanisms to integrate unmodified, commodity desktops such as PCs and workstations into inexpensive, high-performance multicomputers.
Reference: [16] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocessor programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: Each node maintains a scalar local clock which always holds its current interval number. Every time a node executes a release or an acquire, a new interval begins and the clock is incremented. Acquire and release operations establish causal order <ref> [16] </ref> among intervals on different nodes. This order is kept by assigning to each interval a vector timestamp, with one entry for each node. The LRC protocol associates updates to shared pages with vector timestamps to minimize data transfers while ensuring the correctness of release consistency.
Reference: [17] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely-coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <note> Octo-ber 1986. Tech Report YALEU-RR-492. </note>
Reference-contexts: On the other hand, increased network traffic can artificially slow down the lock path, sometimes making the cost of synchronization a little higher in AURC (Water). 6 Related Work The concept of shared virtual memory (also called distributed shared memory) was proposed in Li's Ph.D. thesis in 1986 <ref> [17, 19] </ref>. It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing.
Reference: [18] <author> K. Li. Ivy: </author> <title> A shared virtual memory system for parallel computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, volume II Software, </booktitle> <pages> pages 94-101, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: It was first implemented on a network of workstations <ref> [18] </ref> and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7].
Reference: [19] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: On the other hand, increased network traffic can artificially slow down the lock path, sometimes making the cost of synchronization a little higher in AURC (Water). 6 Related Work The concept of shared virtual memory (also called distributed shared memory) was proposed in Li's Ph.D. thesis in 1986 <ref> [17, 19] </ref>. It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. <p> It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems <ref> [20, 19, 21, 29] </ref>. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7].
Reference: [20] <author> K. Li and R. Schaefer. </author> <title> A hypercube shared virtual memory. </title> <booktitle> In Proceedings of the 1989 International Parallel Processing Conference, volume Vol:I Architecture, </booktitle> <pages> pages 125-132, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems <ref> [20, 19, 21, 29] </ref>. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7].
Reference: [21] <author> Kai Li. </author> <title> Scalability issues on of shared virtual memory for multicomputers. </title> <editor> In M. Dubois and S.S. Thakkar, editors, </editor> <booktitle> Scalable Shared Memory Multiprocessors, </booktitle> <pages> pages 263-280. </pages> <publisher> Kluwer Academic Publishers, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems <ref> [20, 19, 21, 29] </ref>. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7].
Reference: [22] <author> Creve Maples. </author> <title> A high-performance, memory-based interconnection system for multicomputer environments. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 295-304, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Blizzard-S [25] rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing; for well-structured programs it imposes substantial overhead compared to more traditional shared virtual memory implementations. The PLUS [2], Galactica Net [12], Merlin <ref> [22] </ref> and its successor SESAME [27], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. These systems do more in hardware, and thus are more expensive and complicated to build.
Reference: [23] <author> K. Petersen and K. Li. </author> <title> Cache coherence for shared memory multiprocessors based on virtual memory support. </title> <booktitle> In Proceedings of the IEEE 7th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: of coherence protocol messages can affect the performance of a "software-only" shared memory system significantly [14, 7], and that on distributed memory architectures with remote memory reference capability, the performance of software cache coherence maintained at the virtual memory page level is competitive with that of hardware cache coherence schemes <ref> [23, 24, 15] </ref>. These results suggest that appropriate architectural support may improve the performance of shared virtual memory substantially. The SHRIMP project at Princeton studies how to provide high-performance communication mechanisms to integrate unmodified, commodity desktops such as PCs and workstations into inexpensive, high-performance multicomputers. <p> It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes <ref> [5, 6, 14, 1, 23, 24, 14, 7] </ref>. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable.
Reference: [24] <author> K. Petersen and K. Li. </author> <title> An evaluation of multiprocessor cache coherence based on virtual memory support. </title> <booktitle> In Proceedings of the IEEE 8th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: of coherence protocol messages can affect the performance of a "software-only" shared memory system significantly [14, 7], and that on distributed memory architectures with remote memory reference capability, the performance of software cache coherence maintained at the virtual memory page level is competitive with that of hardware cache coherence schemes <ref> [23, 24, 15] </ref>. These results suggest that appropriate architectural support may improve the performance of shared virtual memory substantially. The SHRIMP project at Princeton studies how to provide high-performance communication mechanisms to integrate unmodified, commodity desktops such as PCs and workstations into inexpensive, high-performance multicomputers. <p> It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems [20, 19, 21, 29]. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes <ref> [5, 6, 14, 1, 23, 24, 14, 7] </ref>. New coherency protocols improving the performance of release consistency include lazy release consistency [14], and entry consistency [1], in which all shared data is explicitly associated with some synchronization variable.
Reference: [25] <author> I. Schoinas, B. Falsafi, A.R. Lebeck, S.K. Reinhardt, J.R. Larus, and D.A. Wood. </author> <title> Fine-grain access for distributed shared memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: It uses lazy release consistency and allows for multiple writers. This implementation provides respectable performance in the absence of fine-grain sharing. Software-only techniques to control fine-grain accesses to shared memory have been proposed. Blizzard-S <ref> [25] </ref> rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing; for well-structured programs it imposes substantial overhead compared to more traditional shared virtual memory implementations.
Reference: [26] <author> Ch. A. Thekkath and H.M. Levy. </author> <title> Hardware and software support for efficient exception handling. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 110-121, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Global memory allocation is always aligned on a page boundary. but the SVM system processing and transfer is limited to only the allocated size. For both protocols, a fast trap mechanism <ref> [26] </ref> to handle notification on asynchronous message arrival is assumed. 5.3 Applications To evaluate the performance of our SVM protocols, we chose to run a subset of Splash-2 [28] applications and kernels, covering both regular (LU, Water and Ocean) and irregular (Cholesky) sharing.
Reference: [27] <author> Larry D. Wittie, Gudjon Hermannsson, and Ai Li. </author> <title> Eager sharing for efficient massive parallelism. </title> <booktitle> In Proceedings of the 1992 International Conference on Par-all el Processing, </booktitle> <pages> pages 251-255, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This technique works well in the presence of fine-grain sharing; for well-structured programs it imposes substantial overhead compared to more traditional shared virtual memory implementations. The PLUS [2], Galactica Net [12], Merlin [22] and its successor SESAME <ref> [27] </ref>, systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. These systems do more in hardware, and thus are more expensive and complicated to build.
Reference: [28] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: For both protocols, a fast trap mechanism [26] to handle notification on asynchronous message arrival is assumed. 5.3 Applications To evaluate the performance of our SVM protocols, we chose to run a subset of Splash-2 <ref> [28] </ref> applications and kernels, covering both regular (LU, Water and Ocean) and irregular (Cholesky) sharing. LU is a kernel which factors a 512 x 512 dense matrix into the product of a lower triangular and an upper triangular matrix. <p> Work is assigned statically by splitting the grid among processors. Nearest neighbor communication occurs between processors holding adjacent blocks of the grid. We chose to simulate an Ocean implementation with contiguous partition allocation using four-dimensional arrays for grid data storage to avoid false sharing <ref> [28] </ref>. 5.4 Speedups LRC protocols, for all four programs. In all cases, AURC outperforms LRC: by a factor of 1.15 for LU, 1.28 for Water, 1.57 for Cholesky and 2.5 for Ocean.
Reference: [29] <author> S. Zhou, M. Stumm, K. Li, and D. Wortman. </author> <title> Heterogeneous distributed shared memory: An experimental study. </title> <journal> IEEE Transactions on Parallel and Distributed Computing, </journal> <volume> 3(5) </volume> <pages> 540-554, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It was first implemented on a network of workstations [18] and then applied to large-scale multicomputer systems <ref> [20, 19, 21, 29] </ref>. Relaxed consistency models [9] allow shared virtual memory to reduce the cost of false sharing. Recent research in this area includes [5, 6, 14, 1, 23, 24, 14, 7].
References-found: 29

