URL: http://www.cs.utexas.edu/users/pclark/papers/ewsl87.ps
Refering-URL: http://www.cs.utexas.edu/users/pclark/papers/ewsl87.abs.html
Root-URL: 
Title: Induction in Noisy Domains  
Author: Peter Clark Tim Niblett 
Address: Glasgow, G1 2AD  
Affiliation: The Turing Institute  
Web: http://www.cs.utexas.edu/users/pclark/papers/ewsl87.ps  
Note: In: Proc. 2nd European Machine Learning Conference (EWSL-87), pp11-30, Eds: I. Bratko and N. Lavrac. Wilmslow,UK:Sigma (1987)  October 20, 1994  
Abstract: This paper examines the induction of classification rules from examples using real-world data. Real-world data is almost always characterized by two features, which are important for the design of an induction algorithm. Firstly, there is often noise present, for example, due to imperfect measuring equipment used to collect the data. Secondly the description language is often incomplete, such that examples with identical descriptions in the language will not always be members of the same class. Many induction systems make the `noiseless domain' assumption that the examples do not contain errors and the description language is complete, and consequently constrain their search for rules to those for which no counterexamples exist in the data used for induction. However, in real-world domains correlations between attributes and classes in a data set are rarely without exceptions. To locate such correlations and induce rules describing them it is also necessary to consider rules which may not classify all the training examples correctly. This paper firstly discusses some of the problems presented by noise and proposes a top-down induction algorithm for induction in real-world domains. Secondly, an experimental comparison of this algorithm with other induction systems is presented using three sets of real-world medical data. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Quinlan J. </author> <title> (1983) Learning efficient classification procedures and their application to chess end games, Machine Learning: an artificial intelligence approach Ed. </title> <editor> R.Michalski, J.Carbonell and T.Mitchell, </editor> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference-contexts: In particular, two families of systems based on the ID3 and AQ algorithms have been especially successful. ID3 has been successfully applied to the classification of chess endgames (for example <ref> [1] </ref>, [2], [3]) which were intractable to human experts because of the volume of data, and C4 (an ID3 descendant) to the diagnosis of thyroid diseases [4]. <p> Comparative Algorithms 3.1.1. Assistant Assistant [13] is a descendant of ID3 [24] and CLS [25]. Assistant induces rules in the form of decision trees. The entropy measure is used to guide the growth of the decision tree, as described in <ref> [1] </ref>. In addition, Assistant can apply a tree pruning method based on a technique of maximal classification precision. This technique detects the node at which additional branching would cause more errors than if the tree building process was stopped at this particular node.
Reference: 2. <author> Shapiro A., Niblett T. </author> <title> (1982) Automatic induction of classification rules for a chess endgame, </title> <booktitle> Advances in Computer Chess 3. </booktitle> <editor> Ed. Clarke M.R. </editor> <publisher> Oxford: Pergamon pp.73-91. </publisher>
Reference-contexts: In particular, two families of systems based on the ID3 and AQ algorithms have been especially successful. ID3 has been successfully applied to the classification of chess endgames (for example [1], <ref> [2] </ref>, [3]) which were intractable to human experts because of the volume of data, and C4 (an ID3 descendant) to the diagnosis of thyroid diseases [4].
Reference: 3. <author> O'Rorke P. </author> <title> (1982) A comparative study of inductive learning systems AQ11P and ID3 using a chess end-game test problem, </title> <institution> Urbana: Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science report (ISG 82-2). </institution>
Reference-contexts: In particular, two families of systems based on the ID3 and AQ algorithms have been especially successful. ID3 has been successfully applied to the classification of chess endgames (for example [1], [2], <ref> [3] </ref>) which were intractable to human experts because of the volume of data, and C4 (an ID3 descendant) to the diagnosis of thyroid diseases [4]. <p> The AQ rule generation algorithm is described well in the literature (for example <ref> [3] </ref>, [7], [8]) and only a brief overview will be presented here for comparison with CN2. Unlike CN2, AQR generates a decision rule for each class in turn. <p> These properties are not reviewed here, apart from making the observation that the systems were all able to induce rule sets in the domains tested in an `acceptable' time (less than 30 minutes run-time on a SUN-3). O'Rorke <ref> [3] </ref> and Jackson [27] provide detailed comparisons of time and memory requirements of ID3 and AQ11P in generating rules from examples using large chess endgame databases. 3.3.1. <p> For CN2 and AQR we measure complexity by the number of selectors in the final rule set. These measures reveal the gross features of the induced decision rules. More detailed measures of rule complexity have been done by O'Rorke <ref> [3] </ref> but are not used here. 3.3.2. Rule Complexity Assessment of the complexity of a Bayesian rule is more difficult. One measure would be to count the number of elements in the p (V j | C k ) matrix.
Reference: 4. <author> Quinlan J., Compton P.J., Horn K.A., Lazarus L. </author> <title> (1986) Inductive knowledge acquisition: </title> <booktitle> a case study Proceedings of the second Australian Conference on the Applications of Expert Systems Sydney: </booktitle> <institution> New South Wales Institute of Technology pp.183-204. </institution>
Reference-contexts: ID3 has been successfully applied to the classification of chess endgames (for example [1], [2], [3]) which were intractable to human experts because of the volume of data, and C4 (an ID3 descendant) to the diagnosis of thyroid diseases <ref> [4] </ref>. Systems based on the AQ algorithm, such as AQ11 [5] and GEM [6], have been successful in the fields of soya bean diagnosis ([6], [7]) and chess [6]. We consider the principle task of a real-world induction system to be assisting the expert in expressing his or her expertise. <p> Evaluation Criteria Ultimately, the evaluation of the performance of these systems as producing meaningful rules must be done by an expert in the application domain. However, it is difficult to quantify such an assessment. It has been shown on several occasions (for example <ref> [4] </ref>, [9], [14]), that the more quantifiable measures of classificational accuracy and rule simplicity are good general indicators of a rule's use to an expert. This empirical result is not surprising as we expect an expert to be able to classify examples himself or herself accurately also.
Reference: 5. <author> Michalski R., Larson J. </author> <title> (1983) Incremental generation of VL1 hypotheses : the underlying methodology and the description of program AQ11, </title> <institution> Urbana: Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science report (ISG 83-5). </institution>
Reference-contexts: ID3 has been successfully applied to the classification of chess endgames (for example [1], [2], [3]) which were intractable to human experts because of the volume of data, and C4 (an ID3 descendant) to the diagnosis of thyroid diseases [4]. Systems based on the AQ algorithm, such as AQ11 <ref> [5] </ref> and GEM [6], have been successful in the fields of soya bean diagnosis ([6], [7]) and chess [6]. We consider the principle task of a real-world induction system to be assisting the expert in expressing his or her expertise. <p> Generally, simple bottom-up data-driven approaches cope poorly with noise, for example the AQR system described later. However, more sophisticated data-driven approaches have been successful, such as AQ11 <ref> [5] </ref> and AQ15 [9]. These systems use techniques to ensure that reliance on those examples which produce poor and specific rules is removed through various selection and pruning methods, reviewed earlier in section 2.2. 3. Experimental Comparison CN2 was compared with four other algorithms in three medical domains. <p> AQR AQR is an induction system using the basic AQ algorithm to generate classification rules. The AQ algorithm is used in a variety of ways by many systems, for example AQ11 <ref> [5] </ref> and GEM [6]. Many such systems use this algorithm in a more sophisticated manner than AQR to improve predictive accuracy and rule simplicity (for example AQ11 uses a more complex rule interpretation method involving degrees of confirmation), hence AQR represents a relatively simple AQ-based system.
Reference: 6. <author> Reinke R.E. </author> <title> (1984) Knowledge acquisition and refinement tools for the ADVISE META-EXPERT system, </title> <institution> Urbana: Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science, </institution> <address> M.S.Thesis (ISG 84-4, UIUCDCS-F-84-921). </address>
Reference-contexts: Systems based on the AQ algorithm, such as AQ11 [5] and GEM <ref> [6] </ref>, have been successful in the fields of soya bean diagnosis ([6], [7]) and chess [6]. We consider the principle task of a real-world induction system to be assisting the expert in expressing his or her expertise. <p> Systems based on the AQ algorithm, such as AQ11 [5] and GEM <ref> [6] </ref>, have been successful in the fields of soya bean diagnosis ([6], [7]) and chess [6]. We consider the principle task of a real-world induction system to be assisting the expert in expressing his or her expertise. Consequently, we require that the induced rules are highly predictive and are easily comprehensible to the expert. <p> AQR AQR is an induction system using the basic AQ algorithm to generate classification rules. The AQ algorithm is used in a variety of ways by many systems, for example AQ11 [5] and GEM <ref> [6] </ref>. Many such systems use this algorithm in a more sophisticated manner than AQR to improve predictive accuracy and rule simplicity (for example AQ11 uses a more complex rule interpretation method involving degrees of confirmation), hence AQR represents a relatively simple AQ-based system.
Reference: 7. <author> Michalski R., Chilausky R. </author> <title> (1980) Learning by being told and learning from examples: an experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean diagnosis, Policy Analysis and Information Systems, </title> <journal> Vol.4 No.2 pp.125-160. </journal>
Reference-contexts: Systems based on the AQ algorithm, such as AQ11 [5] and GEM [6], have been successful in the fields of soya bean diagnosis ([6], <ref> [7] </ref>) and chess [6]. We consider the principle task of a real-world induction system to be assisting the expert in expressing his or her expertise. Consequently, we require that the induced rules are highly predictive and are easily comprehensible to the expert. <p> The AQ rule generation algorithm is described well in the literature (for example [3], <ref> [7] </ref>, [8]) and only a brief overview will be presented here for comparison with CN2. Unlike CN2, AQR generates a decision rule for each class in turn.
Reference: 8. <author> Michalski R., Larson J. </author> <title> (1978) Selection of most representative training examples and incremental generation of VL1 hypotheses : the underlying methodology and the description of programs ESEL and AQ11 Urbana: </title> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science (UIUCDCS-R-78-867). </institution>
Reference-contexts: Some of these methods are now reviewed. 2.2.1. Removal of Erroneous Training Examples One such technique is to perform induction using only representative training examples, as selected by the expert or automatically, as was done by the ESEL system <ref> [8] </ref> for AQ11's application to the task of soya bean diagnosis. 2.2.2. <p> The AQ rule generation algorithm is described well in the literature (for example [3], [7], <ref> [8] </ref>) and only a brief overview will be presented here for comparison with CN2. Unlike CN2, AQR generates a decision rule for each class in turn.
Reference: 9. <author> Michalski R., Mozetic I., Hong J., Lavrac N. </author> <title> (1986a) The AQ15 inductive learning system : an overview and experiments Proceedings of IMAL 1986, </title> <type> Orsay: </type> <institution> Universite de Paris-Sud. </institution>
Reference-contexts: This technique also used by AQ11 and is on of the important features of the AQ15 algorithm, see <ref> [9] </ref>, [10]. Misclassification by an erroneous rule may be overridden by other rules, whose conditions are nearly met and which have higher weight attached. 2.2.3. <p> The Use of Domain Knowledge Additional domain knowledge can be used to reduce problems of description language and noise. Explanation-based generalization, (for example [16], [17]), constrains generalization to be performed only where the generalization's validity can be proved. DISCIPLE [18] uses explanations to guide generalization. AQ15 <ref> [9] </ref> allows the user to provide background knowledge to assist in induction. This paper presents a description and empirical evaluation of a new induction system based on the 4th of these techniques, involving the relaxing of the requirement of complete consistency of rules with the training data during their generation. <p> Generally, simple bottom-up data-driven approaches cope poorly with noise, for example the AQR system described later. However, more sophisticated data-driven approaches have been successful, such as AQ11 [5] and AQ15 <ref> [9] </ref>. These systems use techniques to ensure that reliance on those examples which produce poor and specific rules is removed through various selection and pruning methods, reviewed earlier in section 2.2. 3. Experimental Comparison CN2 was compared with four other algorithms in three medical domains. <p> The data was obtained from the Institute of Oncology of the University Medical Center in Ljubljana, Yugosla-via [13]. This data is identical to that used to test AQ15 in <ref> [9] </ref> and [10], and are now described. 3.2.1. Lymphography Examples in this data set used 18 attributes, with four possible final diagnostic classes. 148 examples were available. The data was consistent, ie. examples of any two classes were always different. All the tested algorithms produced fairly simple and accurate rules. <p> Evaluation Criteria Ultimately, the evaluation of the performance of these systems as producing meaningful rules must be done by an expert in the application domain. However, it is difficult to quantify such an assessment. It has been shown on several occasions (for example [4], <ref> [9] </ref>, [14]), that the more quantifiable measures of classificational accuracy and rule simplicity are good general indicators of a rule's use to an expert. This empirical result is not surprising as we expect an expert to be able to classify examples himself or herself accurately also. <p> It is perhaps surprising that different methods of halting the rule specialization process, besides having the desirable effect of reducing rule complexity, do not greatly affect predictive accuracy. This effect has been reported in a number of papers, (for example <ref> [9] </ref>, [10], [13], [22]). Indeed it perhaps may be the case that any technique will have this effect, providing a certain maximum level of pruning is not exceeded. If this is the case then an algorithm should be preferred if it most closely estimates this maximum level.
Reference: 10. <author> Michalski R., Mozetic I., Hong J., Lavrac N. </author> <title> (1986b) The multi-purpose incremental learning system AQ15 and its testing application to three medical domains AAAI-86 Ca: </title> <publisher> Kauf-mann. </publisher>
Reference-contexts: This technique also used by AQ11 and is on of the important features of the AQ15 algorithm, see [9], <ref> [10] </ref>. Misclassification by an erroneous rule may be overridden by other rules, whose conditions are nearly met and which have higher weight attached. 2.2.3. <p> The data was obtained from the Institute of Oncology of the University Medical Center in Ljubljana, Yugosla-via [13]. This data is identical to that used to test AQ15 in [9] and <ref> [10] </ref>, and are now described. 3.2.1. Lymphography Examples in this data set used 18 attributes, with four possible final diagnostic classes. 148 examples were available. The data was consistent, ie. examples of any two classes were always different. All the tested algorithms produced fairly simple and accurate rules. <p> It is perhaps surprising that different methods of halting the rule specialization process, besides having the desirable effect of reducing rule complexity, do not greatly affect predictive accuracy. This effect has been reported in a number of papers, (for example [9], <ref> [10] </ref>, [13], [22]). Indeed it perhaps may be the case that any technique will have this effect, providing a certain maximum level of pruning is not exceeded. If this is the case then an algorithm should be preferred if it most closely estimates this maximum level.
Reference: 11. <author> Gascuel O. </author> <year> (1986) </year> <month> PLAGE: </month> <title> A way to give and use knowledge in learning Proceedings of EWSL 1986 Orsay: </title> <institution> Universite de Paris-Sud. </institution>
Reference-contexts: This allows induction to be halted in regions of the search space where there is little training data to guide the system and where further search is as often damaging as beneficial. Such learning algorithms use top-down, hypothesis driven methods for forming rules, for example PLAGE <ref> [11] </ref> and CALM [12]. The pruning of decision trees is also an example of this technique, as is done by Assistant [13] and C4 [14]. Quinlan [15] presents a detailed empirical study of the effect of tree pruning in noisy domains. 2.2.5. <p> In noisy domains, top-down approaches generally work well because the early stages of rule formation spread their dependence on training examples over a large proportion of the training set. Such top-down searches have been used in other induction systems, such as CALM [12], its derivative SEQUOIA [20], PLAGE <ref> [11] </ref> and by King [21]. These systems use different techniques for halting specialization, based on classificational accuracy on training examples and rule coverage.
Reference: 12. <author> Quinqueton J., Sallantin J. </author> <year> (1986) </year> <month> CALM: </month> <title> contestation for argumentative learning machine Machine Learning: A guide to current research Ed: </title> <editor> Mitchell T.M., Carbonell J.G., Michal-ski R.S. </editor> <address> Boston: </address> <publisher> Kluwer. </publisher>
Reference-contexts: This allows induction to be halted in regions of the search space where there is little training data to guide the system and where further search is as often damaging as beneficial. Such learning algorithms use top-down, hypothesis driven methods for forming rules, for example PLAGE [11] and CALM <ref> [12] </ref>. The pruning of decision trees is also an example of this technique, as is done by Assistant [13] and C4 [14]. Quinlan [15] presents a detailed empirical study of the effect of tree pruning in noisy domains. 2.2.5. <p> In noisy domains, top-down approaches generally work well because the early stages of rule formation spread their dependence on training examples over a large proportion of the training set. Such top-down searches have been used in other induction systems, such as CALM <ref> [12] </ref>, its derivative SEQUOIA [20], PLAGE [11] and by King [21]. These systems use different techniques for halting specialization, based on classificational accuracy on training examples and rule coverage.
Reference: 13. <author> Kononenko I., Bratko I., Roskar E. </author> <title> (1984) Experiments in automatic learning of medical diagnostic rules. </title> <booktitle> Presented at the International School for the Synthesis of Expert Knowledge Workshop 1984, </booktitle> <address> Bled, Yugoslavia. </address> <note> Also published as a Technical Report, </note> <institution> Faculty of Electrical Engineering, E. Kardelj University, Ljubljana, </institution> <address> Yugoslavia 1984. </address>
Reference-contexts: Such learning algorithms use top-down, hypothesis driven methods for forming rules, for example PLAGE [11] and CALM [12]. The pruning of decision trees is also an example of this technique, as is done by Assistant <ref> [13] </ref> and C4 [14]. Quinlan [15] presents a detailed empirical study of the effect of tree pruning in noisy domains. 2.2.5. The Use of Domain Knowledge Additional domain knowledge can be used to reduce problems of description language and noise. <p> Similarly, the top-down ID3 algorithm can use such thresholding or `pruning' techniques, such as was done in Assistant <ref> [13] </ref>, C4 [14] and by Niblett and Bratko [22]. CN2 is characterized by its use of entropy to guide rule specialization, and the use of a significance test to halt specialization. <p> Experimental Comparison CN2 was compared with four other algorithms in three medical domains. Firstly, we give a brief description of the algorithms used for comparison. Secondly, details of the medical domain are given and evaluation criteria presented. 3.1. Comparative Algorithms 3.1.1. Assistant Assistant <ref> [13] </ref> is a descendant of ID3 [24] and CLS [25]. Assistant induces rules in the form of decision trees. The entropy measure is used to guide the growth of the decision tree, as described in [1]. <p> Test Domains The above algorithms were tested on three sets of medical data from the domains lymphog-raphy, prognosis of breast cancer recurrence and location of primary tumor. The data was obtained from the Institute of Oncology of the University Medical Center in Ljubljana, Yugosla-via <ref> [13] </ref>. This data is identical to that used to test AQ15 in [9] and [10], and are now described. 3.2.1. Lymphography Examples in this data set used 18 attributes, with four possible final diagnostic classes. 148 examples were available. <p> It is perhaps surprising that different methods of halting the rule specialization process, besides having the desirable effect of reducing rule complexity, do not greatly affect predictive accuracy. This effect has been reported in a number of papers, (for example [9], [10], <ref> [13] </ref>, [22]). Indeed it perhaps may be the case that any technique will have this effect, providing a certain maximum level of pruning is not exceeded. If this is the case then an algorithm should be preferred if it most closely estimates this maximum level.
Reference: 14. <author> Quinlan J. </author> <title> (1986) Induction of Decision Trees Machine Learning vol. 1 no. </title> <address> 1 Boston: </address> <publisher> Kluwer. </publisher>
Reference-contexts: Such learning algorithms use top-down, hypothesis driven methods for forming rules, for example PLAGE [11] and CALM [12]. The pruning of decision trees is also an example of this technique, as is done by Assistant [13] and C4 <ref> [14] </ref>. Quinlan [15] presents a detailed empirical study of the effect of tree pruning in noisy domains. 2.2.5. The Use of Domain Knowledge Additional domain knowledge can be used to reduce problems of description language and noise. <p> If we now remove unchanged elements in this set we obtain -abc, abd, acd, bcd-. delineate both upper and lower bounds. Following Quinlan's discussion of techniques for dealing with unknown attribute values in <ref> [14] </ref>, we use the simple and effective method of replacing unknown values with the most commonly occurring value (or range, in the case of numeric attributes) for that attribute in the training data. A Prolog implementation of this algorithm is given in the appendix. <p> Similarly, the top-down ID3 algorithm can use such thresholding or `pruning' techniques, such as was done in Assistant [13], C4 <ref> [14] </ref> and by Niblett and Bratko [22]. CN2 is characterized by its use of entropy to guide rule specialization, and the use of a significance test to halt specialization. <p> Evaluation Criteria Ultimately, the evaluation of the performance of these systems as producing meaningful rules must be done by an expert in the application domain. However, it is difficult to quantify such an assessment. It has been shown on several occasions (for example [4], [9], <ref> [14] </ref>), that the more quantifiable measures of classificational accuracy and rule simplicity are good general indicators of a rule's use to an expert. This empirical result is not surprising as we expect an expert to be able to classify examples himself or herself accurately also.
Reference: 15. <author> Quinlan J. </author> <title> (1986) Learning from noisy data, Machine Learning vol. </title> <type> 2 Ed. </type> <institution> R.Michalski, J.Carbonell and T.Mitchell, </institution> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference-contexts: Such learning algorithms use top-down, hypothesis driven methods for forming rules, for example PLAGE [11] and CALM [12]. The pruning of decision trees is also an example of this technique, as is done by Assistant [13] and C4 [14]. Quinlan <ref> [15] </ref> presents a detailed empirical study of the effect of tree pruning in noisy domains. 2.2.5. The Use of Domain Knowledge Additional domain knowledge can be used to reduce problems of description language and noise.
Reference: 16. <author> DeJong G. </author> <title> (1981) Generalizations based on explanations IJCAI-81 Vancouver, </title> <booktitle> B.C.: </booktitle> <pages> Kauf-mann pp. 67-69. </pages>
Reference-contexts: Quinlan [15] presents a detailed empirical study of the effect of tree pruning in noisy domains. 2.2.5. The Use of Domain Knowledge Additional domain knowledge can be used to reduce problems of description language and noise. Explanation-based generalization, (for example <ref> [16] </ref>, [17]), constrains generalization to be performed only where the generalization's validity can be proved. DISCIPLE [18] uses explanations to guide generalization. AQ15 [9] allows the user to provide background knowledge to assist in induction.
Reference: 17. <author> Mitchell T.M., Keller R.M., Kedar-Cabelli S.T. </author> <title> (1986) Explanation-Based Generalization : A unifying view. </title> <journal> Machine Learning vol. </journal> <volume> 1 no. </volume> <pages> 1 pp. 47-80. </pages>
Reference-contexts: Quinlan [15] presents a detailed empirical study of the effect of tree pruning in noisy domains. 2.2.5. The Use of Domain Knowledge Additional domain knowledge can be used to reduce problems of description language and noise. Explanation-based generalization, (for example [16], <ref> [17] </ref>), constrains generalization to be performed only where the generalization's validity can be proved. DISCIPLE [18] uses explanations to guide generalization. AQ15 [9] allows the user to provide background knowledge to assist in induction.
Reference: 18. <editor> Kodratoff Y., Tecuci G. </editor> <booktitle> (1986) Rule Learning in DISCIPLE Proceedings of EWSL 1986 Orsay: </booktitle> <institution> Universite de Paris-Sud. </institution>
Reference-contexts: The Use of Domain Knowledge Additional domain knowledge can be used to reduce problems of description language and noise. Explanation-based generalization, (for example [16], [17]), constrains generalization to be performed only where the generalization's validity can be proved. DISCIPLE <ref> [18] </ref> uses explanations to guide generalization. AQ15 [9] allows the user to provide background knowledge to assist in induction.
Reference: 19. <author> Kalbfleish J. </author> <title> (1979) Probability and Statistical Inference II, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: If so, we consider that the complex is likely to reflect a genuine correlation between attributes and classes. To test significance we use the likelihood ratio statistic <ref> [19] </ref>, given by 2 S f i log ( f i /e i ) where the distribution F = ( f 1 , . . . , f n ) is the observed frequency distribution of examples amongst classes satisfying the complex in question and E = (e 1 , .
Reference: 20. <author> Haiech J., Quinqueton J., Sallantin J. </author> <title> (1986) Concept formation from sequential data Proceedings of EWSL 1986 Orsay: </title> <institution> Universite de Paris-Sud. </institution>
Reference-contexts: In noisy domains, top-down approaches generally work well because the early stages of rule formation spread their dependence on training examples over a large proportion of the training set. Such top-down searches have been used in other induction systems, such as CALM [12], its derivative SEQUOIA <ref> [20] </ref>, PLAGE [11] and by King [21]. These systems use different techniques for halting specialization, based on classificational accuracy on training examples and rule coverage. <p> Percentage accuracies and coverages cannot discern between these two cases, whereas a significance test can identify small but significant trends. Indeed, the authors of both SEQUOIA and CALM note that a 2 significance measure could be used as an alternative method for thresholding (see <ref> [20] </ref> and [23]). Generally, simple bottom-up data-driven approaches cope poorly with noise, for example the AQR system described later. However, more sophisticated data-driven approaches have been successful, such as AQ11 [5] and AQ15 [9].
Reference: 21. <author> King R. </author> <title> (1987) An inductive learning approach to the problem of predicting a protein's secondary structure from its amino acid sequence Proceedings EWSL 1987 Wilmslow, </title> <address> UK: </address> <note> Sigma (to be published). </note>
Reference-contexts: Such top-down searches have been used in other induction systems, such as CALM [12], its derivative SEQUOIA [20], PLAGE [11] and by King <ref> [21] </ref>. These systems use different techniques for halting specialization, based on classificational accuracy on training examples and rule coverage.
Reference: 22. <author> Niblett T., Bratko I. </author> <title> (1986) Learning decision rules in noisy domains. </title> <note> Presented at Expert Systems 86 Brighton, 15-18 Dec. and published in Conference Proceedings. </note>
Reference-contexts: Similarly, the top-down ID3 algorithm can use such thresholding or `pruning' techniques, such as was done in Assistant [13], C4 [14] and by Niblett and Bratko <ref> [22] </ref>. CN2 is characterized by its use of entropy to guide rule specialization, and the use of a significance test to halt specialization. <p> It is perhaps surprising that different methods of halting the rule specialization process, besides having the desirable effect of reducing rule complexity, do not greatly affect predictive accuracy. This effect has been reported in a number of papers, (for example [9], [10], [13], <ref> [22] </ref>). Indeed it perhaps may be the case that any technique will have this effect, providing a certain maximum level of pruning is not exceeded. If this is the case then an algorithm should be preferred if it most closely estimates this maximum level. <p> CN2 makes an estimate of the utility of attempting rule specialization (using the significance test as described in section 2). Post-pruning techniques actually perform such specialization and then having done so can evaluate the performance exactly. Post-pruning techniques have been used in AQ15 as mentioned earlier, on ID3 trees <ref> [22] </ref> and may also be applicable to the CN2 rule generation procedure. Acknowledgments This work was supported by the Office of Naval Research under contract N00014-85-G-0243. We would like to thank Professor Donald Michie for his careful reading and valuable com ments on earlier drafts of this paper.
Reference: 23. <author> Quinqueton J., Sallantin J. </author> <title> (1983) Algorithms for learning logical formulas IJCAI-83, </title> <publisher> pp.476-478. </publisher>
Reference-contexts: Percentage accuracies and coverages cannot discern between these two cases, whereas a significance test can identify small but significant trends. Indeed, the authors of both SEQUOIA and CALM note that a 2 significance measure could be used as an alternative method for thresholding (see [20] and <ref> [23] </ref>). Generally, simple bottom-up data-driven approaches cope poorly with noise, for example the AQR system described later. However, more sophisticated data-driven approaches have been successful, such as AQ11 [5] and AQ15 [9].
Reference: 24. <author> Quinlan J. </author> <title> (1979) Discovering rules by induction from large collections of examples Introductory readings in expert systems Ed. </title> <editor> D. Michie, </editor> <publisher> London: Gordon and Breach pp.33-46. </publisher>
Reference-contexts: Experimental Comparison CN2 was compared with four other algorithms in three medical domains. Firstly, we give a brief description of the algorithms used for comparison. Secondly, details of the medical domain are given and evaluation criteria presented. 3.1. Comparative Algorithms 3.1.1. Assistant Assistant [13] is a descendant of ID3 <ref> [24] </ref> and CLS [25]. Assistant induces rules in the form of decision trees. The entropy measure is used to guide the growth of the decision tree, as described in [1]. In addition, Assistant can apply a tree pruning method based on a technique of maximal classification precision.
Reference: 25. <author> Hunt E.B., Marin J., Stone P.T. </author> <title> (1966) Experiments in Induction New York: </title> <publisher> Academic Press. </publisher>
Reference-contexts: Firstly, we give a brief description of the algorithms used for comparison. Secondly, details of the medical domain are given and evaluation criteria presented. 3.1. Comparative Algorithms 3.1.1. Assistant Assistant [13] is a descendant of ID3 [24] and CLS <ref> [25] </ref>. Assistant induces rules in the form of decision trees. The entropy measure is used to guide the growth of the decision tree, as described in [1]. In addition, Assistant can apply a tree pruning method based on a technique of maximal classification precision.
Reference: 26. <author> Wald A. </author> <title> (1947) Sequential Analysis New York: </title> <publisher> Wiley. </publisher>
Reference-contexts: A Bayesian Classifier In addition, the performance of a simple Bayesian classifier was examined and compared to the other algorithms. It should be noted that more sophisticated applications of the Bayes rule also exist in which the attribute tests are ordered <ref> [26] </ref>. The classifier used represents its `decision rule' as a matrix of probabilities p (v j | C k ) of the occurrence of each attribute value given each class in turn.
Reference: 27. <author> Jackson J.A. </author> <title> (1985) Economics of automatic generation of rules from examples in a chess end-game, </title> <institution> Urbana: Univ. of Illinois at Urbana-Champaign, Dept. of Computer Science (UIUCDCS-F-85-932). </institution>
Reference-contexts: These properties are not reviewed here, apart from making the observation that the systems were all able to induce rule sets in the domains tested in an `acceptable' time (less than 30 minutes run-time on a SUN-3). O'Rorke [3] and Jackson <ref> [27] </ref> provide detailed comparisons of time and memory requirements of ID3 and AQ11P in generating rules from examples using large chess endgame databases. 3.3.1.
References-found: 27

