URL: http://www.cs.umn.edu/Users/dept/users/kumar/interior-point.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: A Parallel Formulation of Interior Point Algorithms  
Author: George Karypis, Anshul Gupta, and Vipin Kumar 
Keyword: linear programming, interior point methods, Cholesky factorization, sparse linear systems.  
Address: Minneapolis, MN 55455  
Affiliation: Computer Science Department University of Minnesota,  
Pubnum: Technical Report  
Email: karypis@cs.umn.edu agupta@cs.umn.edu kumar@cs.umn.edu  
Date: 94-20  
Abstract: In recent years, interior point algorithms have been used successfully for solving medium-to large-size linear programming (LP) problems. In this paper we describe a highly parallel formulation of the interior point algorithm. A key component of the interior point algorithm is the solution of a sparse system of linear equations using Cholesky factorization. The performance of parallel Cholesky factorization is determined by (a) the communication overhead incurred by the algorithm, and (b) the load imbalance among the processors. In our parallel interior point algorithm, we use our recently developed parallel multifrontal algorithm that has the smallest communication overhead over all parallel algorithms for Cholesky factorization developed to date. The computation imbalance depends on the shape of the elimination tree associated with the sparse system reordered for factorization. To balance the computation, we implemented and evaluated four different ordering algorithms. Among these algorithms, Kernighan-Lin and spectral nested dissection yield the most balanced elimination trees and greatly increase the amount of parallelism that can be exploited. Our preliminary implementation achieves a speedup as high as 108 on 256-processor nCUBE 2 on moderate-size problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ashcraft, S. C. Eisenstat, J. W.-H. Liu, and A. H. Sherman. </author> <title> A comparison of three column based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1990. </year> <booktitle> Also appears in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference: [2] <author> M. S. Bazaraa, J. J. Jarvis, and H. D. Sherali. </author> <title> Linear Programming and Network Flows. </title> <publisher> Wiley, </publisher> <year> 1990. </year>
Reference-contexts: Linear programming (LP) deals with the problem of minimizing or maximizing a linear function in the presence of linear equality and or inequality constraints. Linear programming is extensively used to model large and complex problems in many fields <ref> [11, 2, 49] </ref>.
Reference: [3] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: This mapping is very different than the mapping used by many standard matrix-matrix and matrix-vector multiplication algorithms. It is well known <ref> [30, 3] </ref> that parallel matrix-matrix and matrix-vector multiplication algorithms are more efficient when a two-dimensional distribution of the matrix is used. Therefore, we have implemented both the matrix-matrix and the matrix-vector multiplication using a two-dimensional distribution of M (and consequently A). This distribution is described in Section 4.2.1.
Reference: [4] <author> R. H. Bisseling, T. M. Doup, and L. Loyens. </author> <title> A parallel Interior Point algorithm for linear programming on a network of transputers. </title> <journal> Annals of Operations Research, </journal> <volume> 43 </volume> <pages> 51-86, </pages> <year> 1993. </year> <month> 20 </month>
Reference-contexts: Housos et al. [22] and Saltzman [58] developed parallel formulations of interior point methods for shared-memory architectures, and report good speedups on up to eight processors. Bisseling et al. <ref> [4] </ref> developed a parallel formulation for the dual affine interior point algorithm on a transputer network. They obtained speedup up to 88 on 400 processors for certain classes of LP problems (scheduling problems in oil refineries), and speedup up to 60 on 400 processors for general problems.
Reference: [5] <author> I. C. Choi, C. L. Monma, and D. F. Shanno. </author> <title> Further Development of a Primal-Dual Interior Point Method. </title> <journal> ORSA Journal on Computing, </journal> <volume> 2(4) </volume> <pages> 304-311, </pages> <month> Fall </month> <year> 1990. </year>
Reference-contexts: Karmarkar's algorithm is iterative in nature and has polynomial run time. It starts from a point within the feasible set and approaches the optimal solution by moving within this set. Since the development of this first algorithm, several variants have been proposed <ref> [45, 5, 46, 44, 42, 41, 64] </ref>. Because these algorithms approach the optimal solution while staying within the feasible set, they are called interior point algorithms.
Reference: [6] <author> G. B. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1963. </year>
Reference-contexts: The emergence of parallel computers provides the computational power that can be used to solve large linear programming problems within reasonable time periods. Traditionally, linear programming problems have been solved using the simplex method <ref> [6] </ref>. Even though this method performs well in practice, its run time is not polynomially bound. In 1984, Karmarkar [26] developed a radically different algorithm for solving linear programming problems, called projective scaling. Karmarkar's algorithm is iterative in nature and has polynomial run time.
Reference: [7] <author> I. S. Duff, M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> UK, </address> <year> 1990. </year>
Reference-contexts: Note that M is symmetric positive definite, and thus d y can be obtained by first factoring M using Cholesky factorization <ref> [16, 7] </ref> and then solving two triangular systems. Cholesky factorization is the most expensive part of the entire computation. In our experiments, around 90 percent of the time is spent in solving Equation 3. <p> The total number of multiplications required to compute AA T can be computed from the outer-product formulation of AA T <ref> [7] </ref>, where AA T = k=0 k;* : In the outer-product formulation, the kth column of A is multiplied with the kth row of A T . Since each column has c A nonzero elements, the kth row of A T is multiplied by a nonzero element c A times.
Reference: [8] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> (9):302-325, 1983. 
Reference-contexts: The main reason for that is that the amount of computation relative to the size of the matrix being factored is very small and the communication overhead is relatively high. We have recently developed [19] a parallel sparse Cholesky factorization algorithm based on the multifrontal sequential algorithm <ref> [8, 38] </ref>. <p> They are row Cholesky, column Cholesky, and submatrix Cholesky. Each method has its advantages and disadvantages depending on the memory access pattern, vectorization, and other considerations. The multifrontal method <ref> [8, 38] </ref> is a form of submatrix Cholesky, in which single elimination steps are performed on a sequence of small, dense frontal matrices, which are summed to L.
Reference: [9] <author> J. Eckstein, L. C. Polymenakos, R. Qi, V. I. Ragulin, and S. A. Zenios. </author> <title> Data-Parallel Implementations of Dense Linear Programming Algorithms. </title> <type> Technical Report TMC-230, </type> <institution> Thinking Machines Corporations, </institution> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Note that the amount of computation performed in each iteration of interior point algorithms is considerably higher than that for the simplex method; however, interior point algorithms require substantially fewer iterations. Earlier efforts at developing parallel formulations of linear programming algorithms concentrated on dense or nearly dense problems <ref> [9, 61, 48, 21, 28] </ref>. Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems. <p> Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems. For dense matrices, there are efficient parallel formulations for both rank-one updates <ref> [9] </ref> and Cholesky factorizations [12, 30], making it easy to develop highly scalable formulations for dense LP problems [28, 9]. In contrast, attempts to develop general parallel algorithms for sparse linear programming problems have had limited success. <p> Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems. For dense matrices, there are efficient parallel formulations for both rank-one updates [9] and Cholesky factorizations [12, 30], making it easy to develop highly scalable formulations for dense LP problems <ref> [28, 9] </ref>. In contrast, attempts to develop general parallel algorithms for sparse linear programming problems have had limited success. Shu and Wu [60] developed parallel formulations for both the product form of inverses and the LU variant of the revised simplex method on a shared-memory computer.
Reference: [10] <author> J. Eckstein. </author> <title> Large-Scale Parallel Computing, Optimization, and Operations Research: A survey. </title> <journal> ORSA CSTS Newsletter, </journal> <volume> 14(2), </volume> <month> Fall </month> <year> 1993. </year>
Reference-contexts: Earlier efforts at developing parallel formulations of linear programming algorithms concentrated on dense or nearly dense problems [9, 61, 48, 21, 28]. Eckstein <ref> [10] </ref> provides a good survey of work on parallel algorithms for dense linear programming problems. For dense matrices, there are efficient parallel formulations for both rank-one updates [9] and Cholesky factorizations [12, 30], making it easy to develop highly scalable formulations for dense LP problems [28, 9].
Reference: [11] <author> S.-C. Fang and S. Puthenpura. </author> <title> Linear Optimization and Extensions. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1993. </year>
Reference-contexts: Linear programming (LP) deals with the problem of minimizing or maximizing a linear function in the presence of linear equality and or inequality constraints. Linear programming is extensively used to model large and complex problems in many fields <ref> [11, 2, 49] </ref>.
Reference: [12] <author> K. A. Gallivan, R. J. Plemons, and A. H. Sameh. </author> <title> Parallel Algorithms for Dense Linear Algebra Computations. </title> <booktitle> In Parallel Algorithms for Matrix Computations, </booktitle> <pages> pages 1-82. </pages> <publisher> SIAM, </publisher> <year> 1990. </year>
Reference-contexts: Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems. For dense matrices, there are efficient parallel formulations for both rank-one updates [9] and Cholesky factorizations <ref> [12, 30] </ref>, making it easy to develop highly scalable formulations for dense LP problems [28, 9]. In contrast, attempts to develop general parallel algorithms for sparse linear programming problems have had limited success.
Reference: [13] <author> D. M. Gay. </author> <title> Electronic Mail Distribution of Linear Programming Test Problems. </title> <journal> &lt;mathematical Programming Society COAL Newsletter, </journal> <month> December </month> <year> 1985. </year>
Reference-contexts: Therefore, the total execution time of a matrix-vector multiplication is fi nc A p log p + fi n p ! 6 Test Problems In all the experiments reported in this paper, we used real linear programming models that are available through NETLIB <ref> [13] </ref>. Table 2 shows the characteristics of our experimental models.
Reference: [14] <author> G. A. Geist and E. G.-Y. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year>
Reference: [15] <author> A. George, M. T. Heath, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Sparse Cholesky Factorization on a local memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference: [16] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: We have implemented a variety of existing ordering algorithms and their variants and evaluated their ability to minimize the work load imbalance and the amount of fill-in. Among the orderings we evaluated are minimum degree <ref> [16, 20, 17] </ref>, minimum degree with constraints [36, 35], spectral nested dissection [53], and a nested dissection scheme based on Kernighan-Lin's edge separators [29]. The rest of this paper is organized as follows. Section 2 describes the sequential dual affine algorithm. <p> Note that M is symmetric positive definite, and thus d y can be obtained by first factoring M using Cholesky factorization <ref> [16, 7] </ref> and then solving two triangular systems. Cholesky factorization is the most expensive part of the entire computation. In our experiments, around 90 percent of the time is spent in solving Equation 3. <p> The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete [66], therefore a number of heuristic algorithms for ordering have been developed. In particular, minimum degree ordering <ref> [16, 20, 17] </ref> is found to have low fill-in. Even though the values of matrix D change in each iteration, matrix A remains the same, and thus, the sparsity structure of M also remains the same.
Reference: [17] <author> A. George and J. W.-H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31(1) </volume> <pages> 1-19, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: We have implemented a variety of existing ordering algorithms and their variants and evaluated their ability to minimize the work load imbalance and the amount of fill-in. Among the orderings we evaluated are minimum degree <ref> [16, 20, 17] </ref>, minimum degree with constraints [36, 35], spectral nested dissection [53], and a nested dissection scheme based on Kernighan-Lin's edge separators [29]. The rest of this paper is organized as follows. Section 2 describes the sequential dual affine algorithm. <p> The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete [66], therefore a number of heuristic algorithms for ordering have been developed. In particular, minimum degree ordering <ref> [16, 20, 17] </ref> is found to have low fill-in. Even though the values of matrix D change in each iteration, matrix A remains the same, and thus, the sparsity structure of M also remains the same.
Reference: [18] <author> J. R. Gilbert and R. Schreiber. </author> <title> Highly Parallel Sparse Cholesky Factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13 </volume> <pages> 1151-1172, </pages> <year> 1992. </year>
Reference: [19] <author> A. Gupta and V. Kumar. </author> <title> A Scalable Parallel Algorithm for Sparse Matrix Factorization. </title> <type> Technical Report 94-19, </type> <institution> Computer Science Department, University of Minnesota, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: However, attempts to obtain efficient and highly parallel formulations have had limited success so far. The main reason for that is that the amount of computation relative to the size of the matrix being factored is very small and the communication overhead is relatively high. We have recently developed <ref> [19] </ref> a parallel sparse Cholesky factorization algorithm based on the multifrontal sequential algorithm [8, 38]. The analysis presented in [19] and our experiments show that good speedup can be achieved by increasing the degree of parallelism and at the same time minimizing the communication overhead. 2 Parallel Cholesky factorization exploits the <p> We have recently developed <ref> [19] </ref> a parallel sparse Cholesky factorization algorithm based on the multifrontal sequential algorithm [8, 38]. The analysis presented in [19] and our experiments show that good speedup can be achieved by increasing the degree of parallelism and at the same time minimizing the communication overhead. 2 Parallel Cholesky factorization exploits the sparsity of the matrix to perform more than one elimination at the same time. <p> For the numerical Cholesky factorization step we use a highly parallel and scalable formulation of the multifrontal algorithm that we developed recently <ref> [19] </ref>. A brief description of parallel algorithm is as follows: Consider a p-processors hypercube-connected computer. Before the beginning of the algorithm, the elimination tree is converted to a binary tree using an algorithm described in [27]. <p> Furthermore, we will assume that the parallel computer has a hypercube interconnection network and cut-through routing. Cholesky Factorization Analyzing, the communication overhead of the algorithm for non-planar graphs is particularly hard. The performance of our parallel Cholesky factorization algorithm has been analyzed in <ref> [19] </ref>. It is shown there that, for matrices whose corresponding graphs are planar, the time spent by each processor for communication is fi (m= p p). <p> This overhead is smaller than the overheads of other schemes for parallel Cholesky factorization [39, 40, 1, 54, 55, 62, 15, 14, 23, 20, 59, 65, 47, 18, 57]. The experimental results in <ref> [19] </ref> suggest that our scheme is superior to other existing schemes even for non-planar graphs. But the communication overhead for non-planar graphs should be somewhat higher than fi (m= p p). Hence fi (m= p p) can be taken as a lower bound.
Reference: [20] <author> M. T. Heath, E. Ng, and B. W. Payton. </author> <title> Parallel Algorithms for Sparse Linear Systems. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 420-460, </pages> <year> 1990. </year>
Reference-contexts: We have implemented a variety of existing ordering algorithms and their variants and evaluated their ability to minimize the work load imbalance and the amount of fill-in. Among the orderings we evaluated are minimum degree <ref> [16, 20, 17] </ref>, minimum degree with constraints [36, 35], spectral nested dissection [53], and a nested dissection scheme based on Kernighan-Lin's edge separators [29]. The rest of this paper is organized as follows. Section 2 describes the sequential dual affine algorithm. <p> The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete [66], therefore a number of heuristic algorithms for ordering have been developed. In particular, minimum degree ordering <ref> [16, 20, 17] </ref> is found to have low fill-in. Even though the values of matrix D change in each iteration, matrix A remains the same, and thus, the sparsity structure of M also remains the same. <p> Having determined an ordering for M , the next step is to numerically factor M . Depending upon how the nonzero elements of the Cholesky factor are stored and accessed, there are several algorithms to perform Cholesky factorization <ref> [20, 25] </ref>. They are row Cholesky, column Cholesky, and submatrix Cholesky. Each method has its advantages and disadvantages depending on the memory access pattern, vectorization, and other considerations. <p> The performance of any parallel sparse Cholesky factorization algorithm is highly dependent upon the ordering algorithm. Due to sparsity, many columns of the matrix can be factored concurrently; thus, a good ordering must maximize the concurrency in the factorization processes, in addition to minimizing the fill-in <ref> [32, 31, 24, 51, 20, 30] </ref>. The computation associated with distinct subtrees of a node in the elimination tree can be performed in parallel; thus, the shape of the elimination tree determines the degree of concurrency available in the factorization.
Reference: [21] <author> H. F. Ho, G. H. Chen, S. H. Lin, and J. P. Sheu. </author> <title> Solving Linear Programming on Fixed-Size Hypercubes. </title> <booktitle> In International Conference on Parallel Processesing, </booktitle> <pages> pages 112-116, </pages> <year> 1988. </year>
Reference-contexts: Note that the amount of computation performed in each iteration of interior point algorithms is considerably higher than that for the simplex method; however, interior point algorithms require substantially fewer iterations. Earlier efforts at developing parallel formulations of linear programming algorithms concentrated on dense or nearly dense problems <ref> [9, 61, 48, 21, 28] </ref>. Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems.
Reference: [22] <author> E. C. Housos, C. C. Huang, and J.-M. Liu. </author> <title> Parallel Algorithms for the AT&T KORBX System. </title> <journal> AT&T Technical Journal, </journal> <volume> 68(3) </volume> <pages> 37-47, </pages> <year> 1989. </year>
Reference-contexts: For the former they obtained speedup of 4 on 32 processors, and for the latter, got a speedup of 2 on 8 processors. Housos et al. <ref> [22] </ref> and Saltzman [58] developed parallel formulations of interior point methods for shared-memory architectures, and report good speedups on up to eight processors. Bisseling et al. [4] developed a parallel formulation for the dual affine interior point algorithm on a transputer network.
Reference: [23] <author> L. Hulbert and E. Zmijewski. </author> <title> Limiting communication in parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(5) </volume> <pages> 1184-1197, </pages> <month> September </month> <year> 1991. </year>
Reference: [24] <author> J. Jess and H. Kees. </author> <title> A data structure for parallel L/U decomposition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31:231-239, </volume> <year> 1982. </year>
Reference-contexts: The performance of any parallel sparse Cholesky factorization algorithm is highly dependent upon the ordering algorithm. Due to sparsity, many columns of the matrix can be factored concurrently; thus, a good ordering must maximize the concurrency in the factorization processes, in addition to minimizing the fill-in <ref> [32, 31, 24, 51, 20, 30] </ref>. The computation associated with distinct subtrees of a node in the elimination tree can be performed in parallel; thus, the shape of the elimination tree determines the degree of concurrency available in the factorization. <p> Also, the height of the elimination tree is determined by the relative size of the subgraphs in which the original graph is partitioned. If the subgraphs have roughly the same size, then the height of the elimination tree is minimized <ref> [24, 34] </ref>. We implemented these three ordering algorithms and evaluated their suitability for our parallel formulation of the multifrontal algorithm for the sparse matrices arising in LP problems (Section 7).
Reference: [25] <author> H.-W. Jung, R. E. Marsten, and M. J. Saltzman. </author> <title> Numerical Factorization Methods for Interior Point Algorithms. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(1) </volume> <pages> 94-105, </pages> <month> Winter </month> <year> 1994. </year> <month> 21 </month>
Reference-contexts: Having determined an ordering for M , the next step is to numerically factor M . Depending upon how the nonzero elements of the Cholesky factor are stored and accessed, there are several algorithms to perform Cholesky factorization <ref> [20, 25] </ref>. They are row Cholesky, column Cholesky, and submatrix Cholesky. Each method has its advantages and disadvantages depending on the memory access pattern, vectorization, and other considerations.
Reference: [26] <author> N. Karmarkar. </author> <title> A New Polynomial-Time Algorithm for Linear Programming. </title> <journal> Combinatorica, </journal> <volume> 4(8):373--395, </volume> <year> 1984. </year>
Reference-contexts: Traditionally, linear programming problems have been solved using the simplex method [6]. Even though this method performs well in practice, its run time is not polynomially bound. In 1984, Karmarkar <ref> [26] </ref> developed a radically different algorithm for solving linear programming problems, called projective scaling. Karmarkar's algorithm is iterative in nature and has polynomial run time. It starts from a point within the feasible set and approaches the optimal solution by moving within this set.
Reference: [27] <author> G. Karypis, A. Gupta, and V. Kumar. </author> <title> Ordering and load balancing for parallel factorization of sparse matrices. </title> <note> Technical Report (in preparation), </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: A brief description of parallel algorithm is as follows: Consider a p-processors hypercube-connected computer. Before the beginning of the algorithm, the elimination tree is converted to a binary tree using an algorithm described in <ref> [27] </ref>. This is a preprocessing step applied before the execution of the parallel dual affine algorithm. In order to factor the sparse matrix in parallel, portions of the elimination tree are assigned to processors using the standard subtree-to-subcube assignment strategy.
Reference: [28] <author> G. Karypis and V. Kumar. </author> <title> Performance and Scalability of Parallel Formulations of the Simplex Method for Dense Linear Programming Problems. </title> <type> Technical report, </type> <institution> Computer Science Department, University of Minnesota, Minneapolis, MN, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Note that the amount of computation performed in each iteration of interior point algorithms is considerably higher than that for the simplex method; however, interior point algorithms require substantially fewer iterations. Earlier efforts at developing parallel formulations of linear programming algorithms concentrated on dense or nearly dense problems <ref> [9, 61, 48, 21, 28] </ref>. Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems. <p> Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems. For dense matrices, there are efficient parallel formulations for both rank-one updates [9] and Cholesky factorizations [12, 30], making it easy to develop highly scalable formulations for dense LP problems <ref> [28, 9] </ref>. In contrast, attempts to develop general parallel algorithms for sparse linear programming problems have had limited success. Shu and Wu [60] developed parallel formulations for both the product form of inverses and the LU variant of the revised simplex method on a shared-memory computer.
Reference: [29] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <year> 1970. </year>
Reference-contexts: Among the orderings we evaluated are minimum degree [16, 20, 17], minimum degree with constraints [36, 35], spectral nested dissection [53], and a nested dissection scheme based on Kernighan-Lin's edge separators <ref> [29] </ref>. The rest of this paper is organized as follows. Section 2 describes the sequential dual affine algorithm. Section 3 reviews the Cholesky factorization process and describes the multifrontal algorithm. Section 4 describes the various parallel algorithms involved in our parallel dual affine algorithm, including multifrontal, matrix-matrix, and matrix-vector multiplication. <p> Several ordering algorithms based on graph partitioning have been developed including minimum degree with constraints [36], spectral nested dissection [53], and Kernighan-Lee <ref> [29] </ref>. These algorithms operate in the undirected graph G = (V; E) constructed from M as described in Appendix A. The basic step in all these algorithms is the partitioning of G by removing a set of nodes S. <p> 310040 341322 338217 pilot87 455200 541250 558701 572659 pds-06 573263 688704 988000 1493175 maros-r7 1252577 1649706 1370906 1357307 Table 3: The number of nonzeros in the Cholesky factor for four ordering algorithms. section (MND), the spectral nested dissection [53] (SND), and a nested dissection scheme based on Kernighan-Lin's edge separators <ref> [29] </ref> (KLND). Table 3 shows the number of nonzero elements in the Cholesky factor L for the test problems of Table 2. The amount of fill-in produced by MD is in most cases smaller than that produced by the other orderings.
Reference: [30] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems. For dense matrices, there are efficient parallel formulations for both rank-one updates [9] and Cholesky factorizations <ref> [12, 30] </ref>, making it easy to develop highly scalable formulations for dense LP problems [28, 9]. In contrast, attempts to develop general parallel algorithms for sparse linear programming problems have had limited success. <p> The performance of any parallel sparse Cholesky factorization algorithm is highly dependent upon the ordering algorithm. Due to sparsity, many columns of the matrix can be factored concurrently; thus, a good ordering must maximize the concurrency in the factorization processes, in addition to minimizing the fill-in <ref> [32, 31, 24, 51, 20, 30] </ref>. The computation associated with distinct subtrees of a node in the elimination tree can be performed in parallel; thus, the shape of the elimination tree determines the degree of concurrency available in the factorization. <p> This mapping is very different than the mapping used by many standard matrix-matrix and matrix-vector multiplication algorithms. It is well known <ref> [30, 3] </ref> that parallel matrix-matrix and matrix-vector multiplication algorithms are more efficient when a two-dimensional distribution of the matrix is used. Therefore, we have implemented both the matrix-matrix and the matrix-vector multiplication using a two-dimensional distribution of M (and consequently A). This distribution is described in Section 4.2.1. <p> This is crucial because the amount of computation performed during the matrix-matrix multiplication in step 1 depends on the nonzero elements of M assigned to each processor. A widely used distribution strategy that tends to evenly distribute sparse matrices is the cyclic mapping (CM) <ref> [30] </ref>. In this mapping, element m i;j is assigned to processor (i mod p p We implemented this mapping. But as p increased, the percentage difference ((maxmin)/min) in the amount of nonzero elements of M assigned to each processor increased significantly. <p> We use a two-dimensional partitioning of A T to perform this operation. Each processor has a block of A T and it performs a subset of the dot product computations for a group of elements of u <ref> [30] </ref>. As discussed in Section 4.2.2, each processor stores the rows of A that are required during the multiplication. Specifically, for each element m i;j assigned to it, each processor stores row i and row j of A. <p> The only communication step involved in the matrix-matrix multiplication algorithm is the broadcast of the vector z. As described in Section 4.2.2 this is done in two steps. The first step takes fi ((n= p p) log p) time, while the second step takes fi (n) time <ref> [30] </ref>. Therefore, the total run time of a matrix-matrix multiplication is fi nc 2 p + fi n p ! Matrix-Vector Multiplication In the matrix-vector algorithm described in Section 4.2.3, each processor is responsible for computing partial dot-products for n= p p elements.
Reference: [31] <author> J. W.-H. Liu. </author> <title> Computational models and task scheduling for parallel sparse Cholesky factorization. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 327-342, </pages> <year> 1986. </year>
Reference-contexts: The performance of any parallel sparse Cholesky factorization algorithm is highly dependent upon the ordering algorithm. Due to sparsity, many columns of the matrix can be factored concurrently; thus, a good ordering must maximize the concurrency in the factorization processes, in addition to minimizing the fill-in <ref> [32, 31, 24, 51, 20, 30] </ref>. The computation associated with distinct subtrees of a node in the elimination tree can be performed in parallel; thus, the shape of the elimination tree determines the degree of concurrency available in the factorization.
Reference: [32] <author> J. W.-H. Liu. </author> <title> Reordering sparse matrices for parallel elimination. </title> <journal> Parallel Computing, </journal> <volume> 11 </volume> <pages> 73-91, </pages> <year> 1989. </year>
Reference-contexts: The performance of any parallel sparse Cholesky factorization algorithm is highly dependent upon the ordering algorithm. Due to sparsity, many columns of the matrix can be factored concurrently; thus, a good ordering must maximize the concurrency in the factorization processes, in addition to minimizing the fill-in <ref> [32, 31, 24, 51, 20, 30] </ref>. The computation associated with distinct subtrees of a node in the elimination tree can be performed in parallel; thus, the shape of the elimination tree determines the degree of concurrency available in the factorization.
Reference: [33] <author> J. W.-H. Liu. </author> <title> The Role of Elimination Trees in Sparse Factorization. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 11 </volume> <pages> 134-172, </pages> <year> 1990. </year>
Reference: [34] <author> J. W. H. Liu. </author> <title> Equivalent Sparse Matrix Reorderings by Elimination Tree Rotations. </title> <journal> Siam Journal on Scientific and Statistical Computing, </journal> <volume> 9(3) </volume> <pages> 424-444, </pages> <year> 1988. </year>
Reference-contexts: Also, the height of the elimination tree is determined by the relative size of the subgraphs in which the original graph is partitioned. If the subgraphs have roughly the same size, then the height of the elimination tree is minimized <ref> [24, 34] </ref>. We implemented these three ordering algorithms and evaluated their suitability for our parallel formulation of the multifrontal algorithm for the sparse matrices arising in LP problems (Section 7).
Reference: [35] <author> J. W. H. Liu. </author> <title> A Graph Partitioning Algorithm by Node Separators. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15(3) </volume> <pages> 198-219, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: We have implemented a variety of existing ordering algorithms and their variants and evaluated their ability to minimize the work load imbalance and the amount of fill-in. Among the orderings we evaluated are minimum degree [16, 20, 17], minimum degree with constraints <ref> [36, 35] </ref>, spectral nested dissection [53], and a nested dissection scheme based on Kernighan-Lin's edge separators [29]. The rest of this paper is organized as follows. Section 2 describes the sequential dual affine algorithm. Section 3 reviews the Cholesky factorization process and describes the multifrontal algorithm. <p> Table 2 shows the characteristics of our experimental models. The models shown is this table were pre-processed to remove empty constraints [41]. 7 Experimental Evaluation of Orderings We experimentally evaluated the following ordering algorithms: minimum degree (MD), the minimum degree ordering with constraints <ref> [36, 35] </ref>, which we call minimum degree based nested dis 13 Name Constraints m Variables n Nonzeros in A Nonzeros in AA T woodw 1098 8418 37487 20421 cycle 1093 3371 21234 27714 ken-11 14695 21349 70354 33880 pilot 1441 4860 44375 61538 gosh 3790 13451 101876 10117 pilot87 2030 6465
Reference: [36] <author> J. W. H. Liu. </author> <title> The Minimum Degree Ordering With Constraints. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10(8) </volume> <pages> 1136-1145, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: We have implemented a variety of existing ordering algorithms and their variants and evaluated their ability to minimize the work load imbalance and the amount of fill-in. Among the orderings we evaluated are minimum degree [16, 20, 17], minimum degree with constraints <ref> [36, 35] </ref>, spectral nested dissection [53], and a nested dissection scheme based on Kernighan-Lin's edge separators [29]. The rest of this paper is organized as follows. Section 2 describes the sequential dual affine algorithm. Section 3 reviews the Cholesky factorization process and describes the multifrontal algorithm. <p> However, minimum degree produces or-derings that are highly unbalanced, which limits the speedup that can be obtained from parallel Cholesky factorization algorithms such as the parallel multifrontal algorithm we developed. Several ordering algorithms based on graph partitioning have been developed including minimum degree with constraints <ref> [36] </ref>, spectral nested dissection [53], and Kernighan-Lee [29]. These algorithms operate in the undirected graph G = (V; E) constructed from M as described in Appendix A. The basic step in all these algorithms is the partitioning of G by removing a set of nodes S. <p> Table 2 shows the characteristics of our experimental models. The models shown is this table were pre-processed to remove empty constraints [41]. 7 Experimental Evaluation of Orderings We experimentally evaluated the following ordering algorithms: minimum degree (MD), the minimum degree ordering with constraints <ref> [36, 35] </ref>, which we call minimum degree based nested dis 13 Name Constraints m Variables n Nonzeros in A Nonzeros in AA T woodw 1098 8418 37487 20421 cycle 1093 3371 21234 27714 ken-11 14695 21349 70354 33880 pilot 1441 4860 44375 61538 gosh 3790 13451 101876 10117 pilot87 2030 6465
Reference: [37] <author> J. W. H. Liu. </author> <title> The Role of Elimination Trees in Sparse Factorization. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 11(1) </volume> <pages> 134-172, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Thus, the algorithm for finding a good ordering needs to be applied only once, and the same ordering can be used in each iteration of the dual affine algorithm. For a given ordering of sparse matrix M , there exists an elimination tree <ref> [37] </ref>, which shows the precedence relations among columns with respect to Cholesky factorization. The elimination tree of a matrix M contains a node for each column of M .
Reference: [38] <author> J. W. H. Liu. </author> <title> The Multifrontal Method for Sparse Matrix Solution: </title> <journal> Theory and Practice. SIAM Review, </journal> <volume> 34(1) </volume> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: The main reason for that is that the amount of computation relative to the size of the matrix being factored is very small and the communication overhead is relatively high. We have recently developed [19] a parallel sparse Cholesky factorization algorithm based on the multifrontal sequential algorithm <ref> [8, 38] </ref>. <p> They are row Cholesky, column Cholesky, and submatrix Cholesky. Each method has its advantages and disadvantages depending on the memory access pattern, vectorization, and other considerations. The multifrontal method <ref> [8, 38] </ref> is a form of submatrix Cholesky, in which single elimination steps are performed on a sequence of small, dense frontal matrices, which are summed to L. <p> Our parallel Cholesky factorization algorithm is based on the multifrontal method. In the next section we briefly describe the multifrontal method. For further details the reader should refer to the excellent tutorial by Liu 5 <ref> [38] </ref>. 3.1 Multifrontal Method Let M be an m fi m symmetric positive definite matrix and L be its Cholesky factor. Let T be its elimination tree and define T [i] to represent the set of descendants of the node i in the elimination tree T .
Reference: [39] <author> R. F. Lucas. </author> <title> Solving planar systems of equations on distributed-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <address> Palo Alto, CA, </address> <year> 1987. </year> <journal> Also see IEEE Transactions on Computer Aided Design, </journal> <volume> 6 </volume> <pages> 981-991, </pages> <year> 1987. </year>
Reference: [40] <author> R. F. Lucas, T. Blank, and J. J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference: [41] <author> I. J. Lustig, R. E. Marssten, and D. F. Shanno. </author> <title> Interior Point Methods for Linear Programming: Computational State of the Art. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(1) </volume> <pages> 1-14, </pages> <month> Winter </month> <year> 1994. </year>
Reference-contexts: Karmarkar's algorithm is iterative in nature and has polynomial run time. It starts from a point within the feasible set and approaches the optimal solution by moving within this set. Since the development of this first algorithm, several variants have been proposed <ref> [45, 5, 46, 44, 42, 41, 64] </ref>. Because these algorithms approach the optimal solution while staying within the feasible set, they are called interior point algorithms. <p> Research over the past decade has produced evidence that interior point algorithms provide a viable alternative to the simplex method for most medium- to large-size problems, and can be many times faster than the simplex method as the problem size increases <ref> [41, 64] </ref>. Most recent computational research has concentrated on the primal-dual logarithmic barrier method [42], and in particular, on the predictor-corrector variant [41, 43, 46] of this method. <p> Most recent computational research has concentrated on the primal-dual logarithmic barrier method [42], and in particular, on the predictor-corrector variant <ref> [41, 43, 46] </ref> of this method. The bulk of the computation performed in each iteration of the interior point algorithms is the solution of a symmetric positive definite system. <p> Table 2 shows the characteristics of our experimental models. The models shown is this table were pre-processed to remove empty constraints <ref> [41] </ref>. 7 Experimental Evaluation of Orderings We experimentally evaluated the following ordering algorithms: minimum degree (MD), the minimum degree ordering with constraints [36, 35], which we call minimum degree based nested dis 13 Name Constraints m Variables n Nonzeros in A Nonzeros in AA T woodw 1098 8418 37487 20421 cycle <p> Furthermore, the serial execution time for all these problems is less than one minute. For problems of these size, even sequential interior point algorithms may not be the best way to solve them, and the simplex method may be faster <ref> [41] </ref>. <p> However, the size of even these problems is relatively small compared to the problems for which sequential interior point algorithms excel. Significantly larger problems (more than 5-100 million nonzeros in the Cholesky factor) are solved using interior point methods <ref> [41] </ref>. For these bigger problems, we expect to obtain significantly better speedup. From our experiments we have seen that there are two sources of overhead in our parallel Cholesky factorization. The first overhead is due to load imbalance that can limit the maximum achievable efficiency.
Reference: [42] <author> I. J. Lustig, R. E. Marsten, and D. F. Shanno. </author> <title> Computational experience with a primal-dual interior-point method for linear programming. </title> <journal> Journal of Linear Algebra and Its Applications, </journal> <volume> 152 </volume> <pages> 191-222, </pages> <year> 1991. </year>
Reference-contexts: Karmarkar's algorithm is iterative in nature and has polynomial run time. It starts from a point within the feasible set and approaches the optimal solution by moving within this set. Since the development of this first algorithm, several variants have been proposed <ref> [45, 5, 46, 44, 42, 41, 64] </ref>. Because these algorithms approach the optimal solution while staying within the feasible set, they are called interior point algorithms. <p> Most recent computational research has concentrated on the primal-dual logarithmic barrier method <ref> [42] </ref>, and in particular, on the predictor-corrector variant [41, 43, 46] of this method. The bulk of the computation performed in each iteration of the interior point algorithms is the solution of a symmetric positive definite system.
Reference: [43] <author> I. J. Lustig, R. E. Marsten, and D. F. Shanno. </author> <title> On Implementing Mehrotra's Predictor-Corrector Interior-Point Method for Linear Programming. </title> <journal> SIAM J. Optimization, </journal> <volume> 2(3) </volume> <pages> 435-449, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Most recent computational research has concentrated on the primal-dual logarithmic barrier method [42], and in particular, on the predictor-corrector variant <ref> [41, 43, 46] </ref> of this method. The bulk of the computation performed in each iteration of the interior point algorithms is the solution of a symmetric positive definite system.
Reference: [44] <author> R. E. Marsten, M. J. Saltzman, D. F. Shanno, G. S. Pierce, and J. F. Ballintizn. </author> <title> Implementation of a Dual Affine Interior Point Algorithm for Linear Programming. </title> <journal> ORSA Journal on Computing, </journal> <volume> 1(4) </volume> <pages> 287-297, </pages> <year> 1989. </year>
Reference-contexts: Karmarkar's algorithm is iterative in nature and has polynomial run time. It starts from a point within the feasible set and approaches the optimal solution by moving within this set. Since the development of this first algorithm, several variants have been proposed <ref> [45, 5, 46, 44, 42, 41, 64] </ref>. Because these algorithms approach the optimal solution while staying within the feasible set, they are called interior point algorithms. <p> Let Z be an n fi n diagonal matrix, with elements z i;i = z i , let D = Z 1 , and let x be a vector of primal estimates of length n. Following <ref> [44] </ref>, an iteration of the dual affine interior point algorithm is shown in Program 2.1. The algorithm starts with an interior point (y; z) of the dual problem Equation 2.
Reference: [45] <author> K. A. McShane, C. L. Monma, and D. Shanno. </author> <title> An Implementation of a Primal-Dual Interior Point Method for Linear Programming. </title> <journal> ORSA Journal on Computing, </journal> <volume> 1(2) </volume> <pages> 70-83, </pages> <month> Spring </month> <year> 1989. </year> <month> 22 </month>
Reference-contexts: Karmarkar's algorithm is iterative in nature and has polynomial run time. It starts from a point within the feasible set and approaches the optimal solution by moving within this set. Since the development of this first algorithm, several variants have been proposed <ref> [45, 5, 46, 44, 42, 41, 64] </ref>. Because these algorithms approach the optimal solution while staying within the feasible set, they are called interior point algorithms.
Reference: [46] <author> S. Mehrotra. </author> <title> On the Implementation of a Primal-Dual Interior Point Method. </title> <journal> SIAM J. Optimization, </journal> <volume> 2(4) </volume> <pages> 575-601, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Karmarkar's algorithm is iterative in nature and has polynomial run time. It starts from a point within the feasible set and approaches the optimal solution by moving within this set. Since the development of this first algorithm, several variants have been proposed <ref> [45, 5, 46, 44, 42, 41, 64] </ref>. Because these algorithms approach the optimal solution while staying within the feasible set, they are called interior point algorithms. <p> Most recent computational research has concentrated on the primal-dual logarithmic barrier method [42], and in particular, on the predictor-corrector variant <ref> [41, 43, 46] </ref> of this method. The bulk of the computation performed in each iteration of the interior point algorithms is the solution of a symmetric positive definite system.
Reference: [47] <author> M. Mu and J. R. Rice. </author> <title> A grid-based subtree-subcube assignment strategy for solving partial differential equations on hypercubes. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13(3) </volume> <pages> 826-839, </pages> <month> May </month> <year> 1992. </year>
Reference: [48] <author> K. Onaga and H. Nagayasu. </author> <title> A Wavefront-Driven Algorithm for Linear Programming on Dataflow Processor-Arrays. </title> <booktitle> In Proceedings of International Computer Symposium, </booktitle> <pages> pages 739-746, </pages> <year> 1984. </year>
Reference-contexts: Note that the amount of computation performed in each iteration of interior point algorithms is considerably higher than that for the simplex method; however, interior point algorithms require substantially fewer iterations. Earlier efforts at developing parallel formulations of linear programming algorithms concentrated on dense or nearly dense problems <ref> [9, 61, 48, 21, 28] </ref>. Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems.
Reference: [49] <author> C. H. Papadimitriou and K. Steiglitz. </author> <title> Combinatorial Optimization. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1982. </year>
Reference-contexts: Linear programming (LP) deals with the problem of minimizing or maximizing a linear function in the presence of linear equality and or inequality constraints. Linear programming is extensively used to model large and complex problems in many fields <ref> [11, 2, 49] </ref>.
Reference: [50] <author> B. N. Parlett, H. Simon, and L. M. Stringer. </author> <title> On Estimating the Largest Eigenvalue With the Lanczos Algorithm. </title> <journal> Mathematics of Computation, </journal> <volume> 38(157) </volume> <pages> 153-165, </pages> <month> January </month> <year> 1982. </year>
Reference: [51] <author> F. Peters. </author> <title> Parallel pivoting algorithms for sparse symmetric matrices. </title> <journal> Parallel Computing, </journal> <volume> 1 </volume> <pages> 99-110, </pages> <year> 1984. </year>
Reference-contexts: The performance of any parallel sparse Cholesky factorization algorithm is highly dependent upon the ordering algorithm. Due to sparsity, many columns of the matrix can be factored concurrently; thus, a good ordering must maximize the concurrency in the factorization processes, in addition to minimizing the fill-in <ref> [32, 31, 24, 51, 20, 30] </ref>. The computation associated with distinct subtrees of a node in the elimination tree can be performed in parallel; thus, the shape of the elimination tree determines the degree of concurrency available in the factorization.
Reference: [52] <author> A. Pothen and C.-J. Fan. </author> <title> Computing the block triangular form of a sparse matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <year> 1990. </year>
Reference: [53] <author> A. Pothen, H. D. Simon, and K.-P. Liou. </author> <title> Partitioning Sparse Matrices With Eigenvectors of Graphs. </title> <journal> SIAM J. on Matrix Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: We have implemented a variety of existing ordering algorithms and their variants and evaluated their ability to minimize the work load imbalance and the amount of fill-in. Among the orderings we evaluated are minimum degree [16, 20, 17], minimum degree with constraints [36, 35], spectral nested dissection <ref> [53] </ref>, and a nested dissection scheme based on Kernighan-Lin's edge separators [29]. The rest of this paper is organized as follows. Section 2 describes the sequential dual affine algorithm. Section 3 reviews the Cholesky factorization process and describes the multifrontal algorithm. <p> However, minimum degree produces or-derings that are highly unbalanced, which limits the speedup that can be obtained from parallel Cholesky factorization algorithms such as the parallel multifrontal algorithm we developed. Several ordering algorithms based on graph partitioning have been developed including minimum degree with constraints [36], spectral nested dissection <ref> [53] </ref>, and Kernighan-Lee [29]. These algorithms operate in the undirected graph G = (V; E) constructed from M as described in Appendix A. The basic step in all these algorithms is the partitioning of G by removing a set of nodes S. <p> ken-11 133650 152074 222168 212109 pilot 192871 228245 242596 236911 gosh 267775 310040 341322 338217 pilot87 455200 541250 558701 572659 pds-06 573263 688704 988000 1493175 maros-r7 1252577 1649706 1370906 1357307 Table 3: The number of nonzeros in the Cholesky factor for four ordering algorithms. section (MND), the spectral nested dissection <ref> [53] </ref> (SND), and a nested dissection scheme based on Kernighan-Lin's edge separators [29] (KLND). Table 3 shows the number of nonzero elements in the Cholesky factor L for the test problems of Table 2.
Reference: [54] <author> A. Pothen and C. Sun. </author> <title> Distributed multifrontal factorization using clique trees. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 34-40, </pages> <year> 1991. </year>
Reference: [55] <author> R. Pozo and S. L. Smith. </author> <title> Performance evaluation of the parallel multifrontal method in a distributed-memory environment. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 453-456, </pages> <year> 1993. </year>
Reference: [56] <author> D. Rose. </author> <title> A graph-theoretic study of numerical solution of parse symmetric positive definite systems of linear equations. In Graph Theory and Computing. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference: [57] <author> E. Rothberg and A. Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <year> 1992. </year>
Reference: [58] <author> M. J. Saltzman. </author> <title> Implementation of an Interior Point LP Algorithm on a Shared-Memory Vector Multiprocessor. </title> <editor> In R. Sharda, O. Balci, and S. A. Zenios, editors, </editor> <booktitle> Computer Science and Operations Research: New Developments in their Interface. </booktitle> <publisher> Pergamon Press, </publisher> <year> 1992. </year>
Reference-contexts: For the former they obtained speedup of 4 on 32 processors, and for the latter, got a speedup of 2 on 8 processors. Housos et al. [22] and Saltzman <ref> [58] </ref> developed parallel formulations of interior point methods for shared-memory architectures, and report good speedups on up to eight processors. Bisseling et al. [4] developed a parallel formulation for the dual affine interior point algorithm on a transputer network.
Reference: [59] <author> R. Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <type> Technical Report RIACS TR 92.13, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> May </month> <year> 1992. </year> <note> Also appears in A. </note> <editor> George, John R. Gilbert, and J. W.- H. Liu, editors, </editor> <title> Sparse Matrix Computations: Graph Theory Issues and Algorithms (An IMA Workshop Volume). </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference: [60] <author> W. Shu and M.-Y. Wu. </author> <title> Sparse Implementation of Revised Simplex Algorithm on Parallel Computers. </title> <booktitle> In SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: In contrast, attempts to develop general parallel algorithms for sparse linear programming problems have had limited success. Shu and Wu <ref> [60] </ref> developed parallel formulations for both the product form of inverses and the LU variant of the revised simplex method on a shared-memory computer. For the former they obtained speedup of 4 on 32 processors, and for the latter, got a speedup of 2 on 8 processors.
Reference: [61] <author> C. B. Stunkel and D. A. Reed. </author> <title> Hypercube Implementation of the Simplex Algorithm. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> volume ||, pages 1473-1482, </pages> <year> 1988. </year>
Reference-contexts: Note that the amount of computation performed in each iteration of interior point algorithms is considerably higher than that for the simplex method; however, interior point algorithms require substantially fewer iterations. Earlier efforts at developing parallel formulations of linear programming algorithms concentrated on dense or nearly dense problems <ref> [9, 61, 48, 21, 28] </ref>. Eckstein [10] provides a good survey of work on parallel algorithms for dense linear programming problems.
Reference: [62] <author> C. Sun. </author> <title> Efficient parallel solutions of large sparse SPD systems on distributed-memory multiprocessors. </title> <type> Technical report, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1993. </year>
Reference: [63] <author> R. J. Vanderbei. </author> <title> LOQO User's Manual. </title> <institution> Princeton University, Princeton, NJ, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: On architectures with smaller t s , such as CM-5 (with active messages) and Cray T3D we expect the speedup to be significantly better. To evaluate the quality of our serial implementation we compared the performance obtained by our interior point code to that of LOQO <ref> [63] </ref>, running on a Sun Sparc-2 workstation. LOQO is a publicly available solver for linear programming problems based on interior point methods. Our 256-processor nCUBE 2 implementation is 8 times faster for woodw, 21 times faster for pilot87, and 44 times faster for maros-r7.
Reference: [64] <author> R. J. Vanderbei. </author> <title> ALPO: Another Linear Program Optimizer. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(2) </volume> <pages> 134-146, </pages> <month> Spring </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: Karmarkar's algorithm is iterative in nature and has polynomial run time. It starts from a point within the feasible set and approaches the optimal solution by moving within this set. Since the development of this first algorithm, several variants have been proposed <ref> [45, 5, 46, 44, 42, 41, 64] </ref>. Because these algorithms approach the optimal solution while staying within the feasible set, they are called interior point algorithms. <p> Research over the past decade has produced evidence that interior point algorithms provide a viable alternative to the simplex method for most medium- to large-size problems, and can be many times faster than the simplex method as the problem size increases <ref> [41, 64] </ref>. Most recent computational research has concentrated on the primal-dual logarithmic barrier method [42], and in particular, on the predictor-corrector variant [41, 43, 46] of this method.
Reference: [65] <author> S. Venugopal and V. K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Supercomputing '91 Proceedings, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference: [66] <author> M. Yannakakis. </author> <title> Computing the minimum fill-in is NP-complete. </title> <journal> SIAM J. Algebraic Discrete Methods, </journal> <volume> 2 </volume> <pages> 77-79, </pages> <year> 1981. </year>
Reference-contexts: Hence, it is desirable to find a good ordering (i.e., a permutation matrix P so that the Cholesky factor L of P M P T has small fill-in). The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete <ref> [66] </ref>, therefore a number of heuristic algorithms for ordering have been developed. In particular, minimum degree ordering [16, 20, 17] is found to have low fill-in.
References-found: 66

