URL: http://robotics.stanford.edu/~ronnyk/c45ap.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: ronnyk@CS.Stanford.EDU  gjohn@CS.Stanford.EDU  
Title: Automatic Parameter Selection by Minimizing Estimated Error  
Author: Ron Kohavi George H. John 
Note: In Armand Prieditis Stuart Russell, eds., Machine Learning: Proceedings of the Twelfth International Conference, Morgan Kaufmann Publishers,  
Web: http://robotics.stanford.edu/~ronnyk  http://robotics.stanford.edu/~gjohn  
Address: Stanford, CA 94305  Stanford, CA 94305  San Francisco, CA.  
Affiliation: Computer Science Dept. Stanford University  Computer Science Dept. Stanford University  
Abstract: We address the problem of finding the parameter settings that will result in optimal performance of a given learning algorithm using a particular dataset as training data. We describe a "wrapper" method, considering determination of the best parameters as a discrete function optimization problem. The method uses best-first search and cross-validation to wrap around the basic induction algorithm: the search explores the space of parameter values, running the basic algorithm many times on training and holdout sets produced by cross-validation to get an estimate of the expected error of each parameter setting. Thus, the final selected parameter settings are tuned for the specific induction algorithm and dataset being studied. We report experiments with this method on 33 datasets selected from the UCI and StatLog collections using C4.5 as the basic induction algorithm. At a 90% confidence level, our method improves the performance of C4.5 on nine domains, degrades performance on one, and is statistically indistinguishable from C4.5 on the rest. On the sample of datasets used for comparison, our method yields an average 13% relative decrease in error rate. We expect to see similar performance improvements when using our method with other machine learning al gorithms.
Abstract-found: 1
Intro-found: 1
Reference: <author> Brazdil, P., Gama, J. & Henery, B. </author> <year> (1994), </year> <title> Characterizing the applicability of classification algorithms using meta-level learning, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> "Machine Learning: ECML-94. European Conference on Machine Learning", Lectures Notes in Artificial Intelligence, </booktitle> <publisher> Springer-Verlag, Catania,Italy, </publisher> <pages> pp. 83-102. </pages>
Reference-contexts: The user wants to choose the algorithm and parameters that result in the best future performance. Although the former problem of selecting a learning algorithm for a particular task is recognized as an important issue in machine learning <ref> (Brazdil, Gama & Henery 1994, Schaffer 1993) </ref>, the latter problem of finding the best parameter values has not been systematically studied.
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994), </year> <title> Greedy attribute selection, </title> <booktitle> in Hirsh & Cohen (1994), </booktitle> <pages> pp. 28-37. </pages>
Reference: <author> Casella, G. & Berger, R. L. </author> <year> (1990), </year> <title> Statistical Inference, </title> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference-contexts: We believe that best first search conducts a reasonably thorough search of the space, and therefore we conjecture that the results of C4.5* cannot be significantly improved upon for any settings of C4.5 parameters. For each dataset in Table 4, we used a one-tailed paired t-test <ref> (Casella & Berger 1990) </ref> to test the hypothesis that the accuracy of the C4.5-AP algorithm is higher than the C4.5 algorithm (versus the null hypothesis that the algorithms perform equally). When we run 10-fold cross-validation, we get ten accuracy estimates that we average to give the final estimated accuracy.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993), </year> <title> Learning symbolic rules using artificial neural networks, </title> <editor> in P. Utgoff, ed., </editor> <booktitle> "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 73-80. </pages>
Reference: <author> Finnoff, W., Hergert, F. & Zimmerman, H. G. </author> <year> (1993), </year> <title> "Improving model selection by nonconvergent methods", </title> <booktitle> Neural Networks 6(6), </booktitle> <pages> 771-783. </pages>
Reference: <author> Gasser, T., Kneip, A. & Kohler, W. </author> <year> (1991), </year> <title> "A flexible and fast method for automatic smoothing", </title> <journal> Journal of the American Statistical Association 86(415), </journal> <pages> 643-652. </pages>
Reference: <author> Ginsberg, M. L. </author> <year> (1993), </year> <booktitle> Essentials of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We chose to view the problem as state-space search (Kohavi 1994). We chose the best-first search algorithm <ref> (Ginsberg 1993, Nilsson 1980) </ref>, which works by repeatedly expanding the most promising unexpanded state (Table 2). (Note that this is not simply hill-climbing.) Search problems can be characterized by five distinct components.
Reference: <author> Hirsh, H. & Cohen, W., </author> <booktitle> eds (1994), Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> "Very simple classification rules perform well on most commonly used datasets", </title> <booktitle> Machine Learning 11(1), </booktitle> <pages> 63-90. </pages>
Reference: <author> John, G. H. </author> <year> (1994), </year> <title> Cross-validated C4.5: Using error estimation for automatic parameter selection, </title> <type> Technical Report STAN-CS-TN-94-12, </type> <institution> Computer Science Department, Stanford University. </institution> <note> Available by anonymous ftp from starry.Stanford. EDU:pub/gjohn/papers/cvc45.ps. </note>
Reference-contexts: When the estimated standard deviation of err cv is larger than 1%, we run cross-validation again, up to a maximum of three times. We used a 10% trimmed mean <ref> (John 1994) </ref>; that is, if we run 10-fold CV three times, then out of the 30 resulting estimates we remove the lowest and highest 3 and take the average of the rest. * Operators For the binary parameters, we try the opposite values.
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in Hirsh & Cohen (1994), </booktitle> <pages> pp. 121-129. </pages> <note> Available by anonymous ftp in starry.Stanford.EDU: pub/gjohn/papers/relevance4.ps. </note>
Reference-contexts: When the estimated standard deviation of err cv is larger than 1%, we run cross-validation again, up to a maximum of three times. We used a 10% trimmed mean <ref> (John 1994) </ref>; that is, if we run 10-fold CV three times, then out of the 30 resulting estimates we remove the lowest and highest 3 and take the average of the rest. * Operators For the binary parameters, we try the opposite values.
Reference: <author> Kohavi, R. </author> <year> (1994), </year> <title> Feature subset selection as search with probabilistic estimates, </title> <note> in "AAAI Fall Symposium on Relevance". Available by anonymous ftp in starry.Stanford.EDU: pub/ronnyk/aaaiSymposium94.ps. </note>
Reference-contexts: We chose to view the problem as state-space search <ref> (Kohavi 1994) </ref>. We chose the best-first search algorithm (Ginsberg 1993, Nilsson 1980), which works by repeatedly expanding the most promising unexpanded state (Table 2). (Note that this is not simply hill-climbing.) Search problems can be characterized by five distinct components.
Reference: <author> Kohavi, R. </author> <year> (1995a), </year> <title> The power of decision tables, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> "Machine Learning: ECML-95 (Proc. European Conf. on Machine Learning)", </booktitle> <publisher> Spriner Verlag, </publisher> <pages> pp. 174 - 189. </pages>
Reference: <institution> Available by anonymous ftp from starry.Stanford.EDU:pub/ronnyk/tables.ps. </institution>
Reference: <author> Kohavi, R. </author> <year> (1995b), </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection, </title> <booktitle> in "Proceedings of the 14th International Joint Conference on Artificial Intelligence". </booktitle>
Reference: <institution> Available by anonymous ftp from starry.Stan-ford.EDU:pub/ronnyk/accEst-long.ps. </institution>
Reference: <author> Lang, K. J., Waibel, A. H. & Hinton, G. E. </author> <year> (1990), </year> <title> "A time-delay neural network architecture for isolated word recognition", </title> <booktitle> Neural Networks 3, </booktitle> <pages> 23-43. </pages>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994), </year> <title> Induction of selective bayesian classifiers, </title> <booktitle> in "Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence", </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Seattle, WA, </address> <pages> pp. 399-406. </pages>
Reference: <author> Linhart, H. & Zucchini, W. </author> <year> (1986), </year> <title> Model Selection, </title> <publisher> Wiley. </publisher>
Reference-contexts: For example, on the dna dataset (which took the most time) C4.5 took just under 2 minutes, while C4.5-AP took 6.8 hours. 6 Related Work In statistics, model selection refers to the general problem of selecting a learning algorithm A for a particular task <ref> (Linhart & Zucchini 1986) </ref>. Though the problems are equivalent in the abstract, model selection typically refers to the choice of one from a small set of algorithms, while different issues arise in parameter selection because of the large space to be searched.
Reference: <author> Mangasarian, O. L. & Wolberg, W. H. </author> <year> (1990), </year> <title> "Cancer diagnosis via linear programming", </title> <journal> SIAM News 23(5), 1,18. </journal>
Reference: <author> Michie, D., Spiegelhalter, D. J. & Taylor, C. C. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: We first describe our methodology for testing this hypothesis and then present the results and our analysis. 5.1 Methodology To test our hypothesis, we compared the performance of the C4.5 and C4.5-AP algorithms on 33 datasets gathered from the UCI (Murphy & Aha 1994) and StatLog <ref> (Michie, Spiegelhalter & Taylor 1994) </ref> collections.
Reference: <author> Mitchell, T. M. </author> <year> (1982), </year> <title> "Generalization as search", </title> <booktitle> Artificial Intelligence 18, </booktitle> <pages> 203-266. </pages>
Reference-contexts: To determine the parameter setting, the selection method must take into account the interaction between the biases of the induction algorithm <ref> (Mitchell 1982) </ref> and the particular training set available. Intuitively, an algorithm should be able to make use of the information in the training data to guide it in a search for the best parameters.
Reference: <author> Moore, A. & Lee, M. </author> <year> (1994), </year> <title> Efficient algorithms for minimizing cross-validation error, </title> <booktitle> in Hirsh & Co-hen (1994), </booktitle> <pages> pp. 190-198. </pages>
Reference: <author> Moore, A. W., Hill, D. J. & Johnson, M. P. </author> <year> (1992), </year> <title> An empirical investigation of brute force to choose features, smoothers and function approximators, </title> <booktitle> in "Computational Learning Theory and Natural Learning Systems Conference". </booktitle>
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> "UCI repository of machine learning databases", Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference-contexts: We first describe our methodology for testing this hypothesis and then present the results and our analysis. 5.1 Methodology To test our hypothesis, we compared the performance of the C4.5 and C4.5-AP algorithms on 33 datasets gathered from the UCI <ref> (Murphy & Aha 1994) </ref> and StatLog (Michie, Spiegelhalter & Taylor 1994) collections.
Reference: <author> Nilsson, N. </author> <year> (1980), </year> <booktitle> Principles of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T. & Flannery, B. P. </author> <year> (1992), </year> <title> Numerical Recipes in C: The Art of Scientific Computing, second edition, </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: All function optimization methods must specify some stopping criterion, and ours is similar to those used in numerical optimization methods <ref> (Press, Teukolsky, Vetterling & Flannery 1992) </ref>. 4 Automatic Parameter Selection for C4.5 To make our ideas more concrete, we describe the application of our method to the C4.5 decision tree algorithm (Quinlan 1993). C4.5 is an extremely robust algorithm that performs well on a wide variety of domains.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: function optimization methods must specify some stopping criterion, and ours is similar to those used in numerical optimization methods (Press, Teukolsky, Vetterling & Flannery 1992). 4 Automatic Parameter Selection for C4.5 To make our ideas more concrete, we describe the application of our method to the C4.5 decision tree algorithm <ref> (Quinlan 1993) </ref>. C4.5 is an extremely robust algorithm that performs well on a wide variety of domains. It is very difficult to consistently outperform C4.5 on a variety of datasets. Thus, improving C4.5 should yield an interesting learning algorithm.
Reference: <author> Schaffer, C. </author> <year> (1993), </year> <title> "Selecting a classification method by cross-validation", </title> <booktitle> Machine Learning 13, </booktitle> <pages> 135-143. </pages>
Reference: <author> Schaffer, C. </author> <year> (1994), </year> <title> A conservation law for generalization performance, </title> <booktitle> in Hirsh & Cohen (1994), </booktitle> <pages> pp. 259-267. </pages>
Reference-contexts: The user wants to choose the algorithm and parameters that result in the best future performance. Although the former problem of selecting a learning algorithm for a particular task is recognized as an important issue in machine learning <ref> (Brazdil, Gama & Henery 1994, Schaffer 1993) </ref>, the latter problem of finding the best parameter values has not been systematically studied.
Reference: <author> Stone, M. </author> <year> (1974), </year> <title> "Cross-validatory choice and assessment of statistical predictions", </title> <journal> Journal of the Royal Statistical Society B 36, </journal> <pages> 111-147. </pages>
Reference: <author> Thrun, S. B. et al. </author> <year> (1991), </year> <title> The monk's problems - a performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> CMU School of Computer Science. </institution>
Reference-contexts: The datasets represent all of the available StatLog datasets except the Shuttle database (which was too large), all of the UCI datasets used by Holte (1993), all of the Monks datasets <ref> (Thrun et al. 1991) </ref>, and Corral which is an artificial dataset presented in John et al. (1994). For some datasets, a single test set was specified by the contributors of the dataset (which includes the entire space for the artificial datasets).
Reference: <author> Utans, J. & Moody, J. </author> <year> (1991), </year> <title> Selecting neural network architectures via the prediction risk: Application to corporate bond rating prediction, </title> <booktitle> in "The First International Conference on Artificial Intelligence Applications on Wall Street", </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Wahba, G. </author> <year> (1990), </year> <title> Spline Functions for Observational Data, </title> <booktitle> CBMS-NSF Regional Conference Series, </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia. </address>
Reference: <author> Weigend, A. S. </author> <year> (1994), </year> <title> On overfitting and the effective number of hidden units, </title> <editor> in M. C. Mozer, P. Smolensky, D. S. Touretzky, J. L. Elman & A. S. Weigend, eds, </editor> <booktitle> "Proceedings of the 1993 Connectionist Models Summer School", </booktitle> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <pages> pp. 335-342. </pages>
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> (1991), </year> <title> Computer Systems that Learn, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA. </address>
Reference: <author> Wolpert, D. H. </author> <year> (1994), </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework, </title> <type> Technical report, </type> <institution> The Santa Fe Institute, </institution> <address> Santa Fe, NM. </address>
Reference: <author> Zwitter, M. & Soklic, M. </author> <year> (1994a), </year> <title> "Ljubljana breast cancer database", </title> <note> Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference: <author> Zwitter, M. & Soklic, M. </author> <year> (1994b), </year> <note> "Lymphography database", Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
References-found: 40

