URL: http://www.cs.cornell.edu/Info/People/aguilera/HBPartitionable.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/aguilera/HBPartitionable.html
Root-URL: 
Email: aguilera,weichen,sam@cs.cornell.edu  
Title: Quiescent Reliable Communication and Quiescent Consensus in Partitionable Networks  
Author: Marcos Kawazoe Aguilera Wei Chen Sam Toueg 
Date: July 1997  
Address: Upson Hall,  Ithaca, NY 14853-7501, USA.  
Affiliation: Department of Computer Science  Cornell University  
Abstract: We consider partitionable networks with process crashes and lossy links, and focus on the problems of reliable communication and consensus for such networks. For both problems we seek algorithms that are quiescent, i.e., algorithms that eventually stop sending messages. We first tackle the problem of reliable communication for partitionable networks by extending the results of [ACT97a]. In particular, we generalize the specification of the heartbeat failure detector HB, show how to implement it, and show how to use it to achieve quiescent reliable communication. We then turn our attention to the problem of consensus for partitionable networks. We first show that, even though this problem can be solved using a natural extension of failure detector 3S, such solutions are not quiescent in other words, 3S alone is not sufficient to achieve quiescent consensus in partitionable networks. We then solve this problem using 3S and the quiescent reliable communication primitives that we developed in the first part of the paper.
Abstract-found: 1
Intro-found: 1
Reference: [ACT97a] <author> Marcos Kawazoe Aguilera, Wei Chen, and Sam Toueg. Heartbeat: </author> <title> a timeout-free failure detector for quiescent reliable communication. </title> <type> Technical Report 97-1631, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: None of the partitions is isolated. For example, processes in D may receive messages from processes in C and are able to send messages to processes in B. There is no fair path from C to A, or from D to C, etc. <ref> [ACT97a] </ref> shows that without the help of failure detectors it is impossible to achieve quiescent reliable communication in the presence of process crashes and lossy links even if one assumes that the network never partitions. In order to overcome this problem, [ACT97a] introduces the heartbeat failure detector (denoted HB), and shows <p> C to A, or from D to C, etc. <ref> [ACT97a] </ref> shows that without the help of failure detectors it is impossible to achieve quiescent reliable communication in the presence of process crashes and lossy links even if one assumes that the network never partitions. In order to overcome this problem, [ACT97a] introduces the heartbeat failure detector (denoted HB), and shows how it can be implemented, and how it can be used to achieve quiescent reliable communication. All these results are for networks that do not partition. 2 In this paper, we extend the above results to partitionable networks. <p> If p ! q 62 crashed (F L ), we say that p ! q is fair in F L . A failure pattern F = (F P ; F L ) combines a process failure pattern and a link failure pattern. 2.2 Connectivity In contrast to <ref> [ACT97a] </ref>, the network is partitionable: there may be two correct processes p and q such that q is not reachable from p (Fig. 1). Intuitively, a partition is a maximal set of processes that are mutually reachable from each other. <p> Finite Receipt implies that if a link crashes then it eventually stops transporting messages. 3 The Heartbeat Failure Detector HB for Partitionable Networks One of our goals is to achieve quiescent reliable communication in partitionable networks with process crashes and message losses. In <ref> [ACT97a] </ref> it is shown that without failure detectors this is impossible, even if one assumes that the network does not partition. In order to circumvent this impossibility result, [ACT97a] introduces the heartbeat failure detector, denoted HB, for non-partitionable networks. <p> In <ref> [ACT97a] </ref> it is shown that without failure detectors this is impossible, even if one assumes that the network does not partition. In order to circumvent this impossibility result, [ACT97a] introduces the heartbeat failure detector, denoted HB, for non-partitionable networks. In this section, we generalize the definition of HB to partitionable networks. We then show how to implement it in Section 6. <p> This stronger property is not necessary in this paper. 5 In <ref> [ACT97a] </ref>, the output of D at p is an array with one nonnegative integer for each neighbor of p. 6 * HB-Accuracy: At each process p, the heartbeat sequence of every process is nondecreasing. <p> of reliable broadcast is quiescent if it sends only a finite number of messages when broadcast is invoked a finite number of times. 7 4.2 Reliable Broadcast: Algorithm Using HB The quiescent implementation of reliable broadcast for partitionable network that we give here is identical to the one given in <ref> [ACT97a] </ref> for non-partitionable networks. However, the network assumptions, the reliable broadcast requirements, and the failure detector properties are different, and so its proof of correctness and quiescence changes. <p> Roughly speaking, some processes propose a value and must 12 This specification is a generalization of the one for non-partitionable networks given in <ref> [ACT97a] </ref>. 12 decide on one of the proposed values [FLP85]. More precisely, consensus is defined in terms of two primitives, propose (v) and decide (v), where v is a value drawn from a set of possible proposed values. When a process invokes propose (v), we say that it proposes v. <p> Moreover, if the largest partition contains a majority of processes, then it also satisfies Termination. 6 Implementation of HB for Partitionable Networks We now show how to implement HB for partitionable networks. Our implementation (Fig. 4) is a minor modification of the one given in <ref> [ACT97a] </ref> for non-partitionable networks. Every process p executes two concurrent tasks. In the first task, p periodically increments its own heartbeat value, and sends the message (HEARTBEAT; p) to all its neighbors. The second task handles the receipt of messages of the form (HEARTBEAT; path). <p> Therefore p and q are in the same partition a contradiction. 2 By Corollary 26 and the above lemma, we have: Theorem 30 Figure 4 implements HB for partitionable networks. 21 7 Related Work Regarding reliable communication, the works that are closest to ours are <ref> [BCBT96, ACT97a] </ref>. Both of these works, however, consider only non-partitionable networks. <p> However, the communication algorithms that they give are not quiescent (and do not use failure detectors). <ref> [ACT97a] </ref> was the first paper to study the problem of achieving quiescent reliable communication by using failure detectors in a system with process crashes and lossy links. Regarding consensus, the works that are closest to ours are [FKM + 95, CHT96a, DFKM96, GS96].
Reference: [ACT97b] <author> Marcos Kawazoe Aguilera, Wei Chen, and Sam Toueg. </author> <title> On the weakest failure detector for quiescent reliable communication. </title> <type> Technical Report 97-1640, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: HB can be used to solve the problem of quiescent reliable communication and it is implementable, but its counters are unbounded. Can we solve this problem using a failure detector that is both implementable and has bounded output? The answer is no: in <ref> [ACT97b] </ref> we show that a failure detector with bounded output size is either (a) too weak to achieve quiescent reliable communication, or (b) not implementable.
Reference: [BCBT96] <author> Anindya Basu, Bernadette Charron-Bost, and Sam Toueg. </author> <title> Simulating reliable links with unreliable links in the presence of process crashes. </title> <booktitle> In Proceedings of the 10th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science, </booktitle> <pages> pages 105-122. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Therefore p and q are in the same partition a contradiction. 2 By Corollary 26 and the above lemma, we have: Theorem 30 Figure 4 implements HB for partitionable networks. 21 7 Related Work Regarding reliable communication, the works that are closest to ours are <ref> [BCBT96, ACT97a] </ref>. Both of these works, however, consider only non-partitionable networks. <p> Both of these works, however, consider only non-partitionable networks. In <ref> [BCBT96] </ref>, Basu et al. pose the following question: given a problem that can be solved in asynchronous systems with process crashes only, can this problem still be solved if links can also fail by losing messages? They show that the answer is yes if the problem is correct-restricted [BN92, Gop92] 17
Reference: [BDM97] <author> Ozalp Babao glu, Renzo Davoli, and Alberto Montresor. </author> <title> Partitionable group membership: specification and algorithms. </title> <type> Technical Report UBLCS-97-1, </type> <institution> Dept. of Computer Science, University of Bologna, Bologna, Italy, </institution> <month> January </month> <year> 1997. </year>
Reference-contexts: The underlying model of failures and failure detectors is also significantly different from the one proposed in this paper. Another model of failure detectors for partitionable networks is given in <ref> [BDM97] </ref>. <p> The underlying model of failures and failure detectors is also significantly different from the one proposed in this paper. Another model of failure detectors for partitionable networks is given in [BDM97]. We compare models in the next section. 8 Comparison with other Models In <ref> [DFKM96, BDM97] </ref>, network connectivity is defined in terms of the messages exchanged in a run in particular, it depends on whether the algorithm being executed sends a message or not, on the times these messages are sent, and on whether these messages are received. <p> Clearly, network connectivity depends on the messages of the run. In <ref> [BDM97] </ref>, process q is partitioned from p at time t if the last message that p sent to q by time t 0 t is never received by q. This particular way of defining network connectivity in terms of messages is problematic for our purposes, as the following example shows. <p> Suppose that at time t, p sends m to q, and this message is lost (it is never received by q). By the definition in <ref> [BDM97] </ref>, q is partitioned from p at time t. Suppose that the failure detector module at p now tells p (correctly) that q is partitioned from p. At this point, p stops sending messages to q until the failure detector says that q has become reachable again. <p> The proof of correctness of an algorithm (such as the one in the simple example above) should refer only to the abstract properties of the failure detector that it uses, and not to any aspects of its implementation. As a final remark, the model of <ref> [BDM97] </ref> is not suitable for our results because of the following. Consider a completely connected network in which all links are bidirectional and fair. <p> Consider a completely connected network in which all links are bidirectional and fair. Let R be any run in which every 23 link p ! q loses messages from time to time (but every message repeatedly sent is eventually received). In run R, by the definitions in <ref> [BDM97] </ref>: (a) neither q remains partitioned from p, nor q remains reachable from p, and (b) an Eventually Perfect failure detector 3P is allowed to behave arbitrarily. Therefore, with the definitions in [BDM97], 3P cannot be used to solve consensus in such a network. <p> In run R, by the definitions in <ref> [BDM97] </ref>: (a) neither q remains partitioned from p, nor q remains reachable from p, and (b) an Eventually Perfect failure detector 3P is allowed to behave arbitrarily. Therefore, with the definitions in [BDM97], 3P cannot be used to solve consensus in such a network. Our model was designed to deal with fair links explicitly 19 , and consensus can be solved even with 3S.
Reference: [BN92] <author> Rida Bazzi and Gil Neiger. </author> <title> Simulating crash failures with many faulty processors. </title> <booktitle> In Proceedings of the 6th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science, </booktitle> <pages> pages 166-184. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: In [BCBT96], Basu et al. pose the following question: given a problem that can be solved in asynchronous systems with process crashes only, can this problem still be solved if links can also fail by losing messages? They show that the answer is yes if the problem is correct-restricted <ref> [BN92, Gop92] </ref> 17 or if more than half of the processes do not crash.
Reference: [Cha97] <author> Tushar Deepak Chandra, </author> <month> April </month> <year> 1997. </year> <title> Private Communication. </title>
Reference-contexts: We then show how to implement it in Section 6. HB is different from the failure detectors defined in [CT96], or those currently in use in many systems (even though some existing systems, such as Ensemble and Phoenix, use the same name heartbeat in their failure detector implementations <ref> [vR97, Cha97] </ref>). In contrast to existing failure detectors, HB is implementable in asynchronous systems, without the use of timeouts (see Section 6). A heartbeat failure detector D (for partitionable networks) has the following features. <p> Thus, HB should not be confused with existing implementations of failure detectors (some of which, such as those in Ensemble and Phoenix, have modules that are also called heartbeat <ref> [vR97, Cha97] </ref>).
Reference: [CHT96a] <author> Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg, </author> <month> March </month> <year> 1996. </year> <title> Private Communication to the authors of [FKM + 95]. </title>
Reference-contexts: In Section 6, we show how to implement HB in partitionable networks. We conclude with a brief discussion of related work (Section 7) and of our model (Section 8). 2 The consensus algorithms for partitionable networks given in <ref> [FKM + 95, CHT96a, DFKM96] </ref> are not quiescent. 3 2 Model We consider asynchronous message-passing distributed systems in which there are no timing assumptions. In particular, we make no assumptions on the time it takes to deliver a message, or on relative process speeds. <p> use 3S and 3S LP to refer to an arbitrary member of the respective class. 5.3 Quiescent Consensus for Partitionable Networks Cannot be Achieved using 3S Although consensus for partitionable networks can be solved using 3S, we now show that any such solution is not quiescent (the consensus algorithms in <ref> [CHT96a, DFKM96] </ref> do not contradict this result because they are not quiescent). Theorem 14 In partitionable networks with 5 or more processes, consensus has no quiescent implementation using 3S. <p> Regarding consensus, the works that are closest to ours are <ref> [FKM + 95, CHT96a, DFKM96, GS96] </ref>. In [GS96], as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. Roughly speaking, only a subset G of the processes are required to satisfy some accuracy property. <p> However, their model assumes that the network is completely connected and links between correct processes do not lose messages thus, no permanent partition is possible. The first paper to consider the consensus problem in partitionable networks is [FKM + 95], but the algorithms described in that paper had errors <ref> [CHT96a] </ref>. Correct algorithms can be found in [CHT96a, DFKM96]. 18 All these algorithms use a variant of 3S, but in contrast to the one given in this paper they do not use HB and are not quiescent: processes in minority partitions may send messages forever. <p> The first paper to consider the consensus problem in partitionable networks is [FKM + 95], but the algorithms described in that paper had errors [CHT96a]. Correct algorithms can be found in <ref> [CHT96a, DFKM96] </ref>. 18 All these algorithms use a variant of 3S, but in contrast to the one given in this paper they do not use HB and are not quiescent: processes in minority partitions may send messages forever. <p> In particular, the link failure pattern 17 I.e., its specification refers only to the behavior of non-faulty processes. 18 Actually, the specification of consensus considered in <ref> [FKM + 95, CHT96a] </ref> only requires that one correct process in the largest partition eventually decides.
Reference: [CHT96b] <author> Tushar Deepak Chandra, Vassos Hadzilacos, and Sam Toueg. </author> <title> The weakest failure detector for solving consensus. </title> <journal> Journal of the ACM, </journal> <volume> 43(4) </volume> <pages> 685-722, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: We also generalize the definition of 3S the weakest failure detector for solving consensus in networks that do not partition <ref> [CHT96b] </ref>. We show that, although 3S can be used to solve consensus for partitionable networks, any such solution is not quiescent: Thus, 3S alone is not sufficient to solve quiescent consensus for partitionable networks. We then show that this problem can be solved using 3S together with HB. <p> The system can experience both process failures and link failures. Processes can fail by crashing, and links can fail by crashing, or by intermittently dropping messages (while remaining fair). Failures may cause permanent network partitions. The model, based on the one in <ref> [CHT96b] </ref>, is described next. A network is a directed graph G = (P; L) where P = f1; : : : ; ng is the set of processes, and L P fi P is the set of links. <p> To overcome this problem, Chandra and Toueg introduced unreliable failure detectors in [CT96]. In this paper, we focus on the class of eventually strong failure detectors (the weakest one for solving consensus in non-partitionable networks <ref> [CHT96b] </ref>), and extend it to partitionable networks. 13 At each process p, an eventually strong failure detector outputs a set of processes. In [CT96], these are the processes that p suspects to have crashed. In our case, these are the processes that p suspects to be outside its partition.
Reference: [CT96] <author> Tushar Deepak Chandra and Sam Toueg. </author> <title> Unreliable failure detectors for reliable distributed systems. </title> <journal> Journal of the ACM, </journal> <volume> 43(2) </volume> <pages> 225-267, </pages> <month> March </month> <year> 1996. </year> <title> 19 We do not want to hide the retransmissions that fair links require, since our main goal is to design algorithms that ensure that such retransmissions eventually subside. </title> <type> 24 </type>
Reference-contexts: We then show that this problem can be solved using 3S together with HB. In fact, our quiescent consensus algorithm for partitionable networks is identical to the one given in <ref> [CT96] </ref> for non-partitionable networks with reliable links: we simply replace the communication primitives used by the algorithm in [CT96] with the quiescent reliable communication primitives that we derive in this paper (the proof of correctness, however, is different). <p> We then show that this problem can be solved using 3S together with HB. In fact, our quiescent consensus algorithm for partitionable networks is identical to the one given in <ref> [CT96] </ref> for non-partitionable networks with reliable links: we simply replace the communication primitives used by the algorithm in [CT96] with the quiescent reliable communication primitives that we derive in this paper (the proof of correctness, however, is different). An important remark on the use of failure detectors to achieve quiescence is now in order. <p> In order to circumvent this impossibility result, [ACT97a] introduces the heartbeat failure detector, denoted HB, for non-partitionable networks. In this section, we generalize the definition of HB to partitionable networks. We then show how to implement it in Section 6. HB is different from the failure detectors defined in <ref> [CT96] </ref>, or those currently in use in many systems (even though some existing systems, such as Ensemble and Phoenix, use the same name heartbeat in their failure detector implementations [vR97, Cha97]). <p> To overcome this problem, Chandra and Toueg introduced unreliable failure detectors in <ref> [CT96] </ref>. In this paper, we focus on the class of eventually strong failure detectors (the weakest one for solving consensus in non-partitionable networks [CHT96b]), and extend it to partitionable networks. 13 At each process p, an eventually strong failure detector outputs a set of processes. In [CT96], these are the processes <p> unreliable failure detectors in <ref> [CT96] </ref>. In this paper, we focus on the class of eventually strong failure detectors (the weakest one for solving consensus in non-partitionable networks [CHT96b]), and extend it to partitionable networks. 13 At each process p, an eventually strong failure detector outputs a set of processes. In [CT96], these are the processes that p suspects to have crashed. In our case, these are the processes that p suspects to be outside its partition. <p> Formally: 8F; 8H 2 D (F ); 8P 2 Partitions F ; 9t 2 T ; 8p 62 P; 8q 2 P; 8t 0 t : p 2 H (q; t 0 ) 13 The other classes of eventual failure detectors introduced in <ref> [CT96] </ref> can be generalized in a similar way. 13 * Eventual Weak Accuracy: For every partition P , there is a time after which some process in P is permanently trusted by every process in P . <p> Note that the behavior of the failure detector in each of the above three runs is compatible with 3S. 2 5.4 Quiescent Consensus for Partitionable Networks using 3S LP and HB To solve consensus using 3S LP and HB in partitionable networks, we take the rotating coordinator consensus algorithm of <ref> [CT96] </ref>, we replace its communication primitives with the corresponding ones defined in Sections 4.3 and 4.1, namely, qr-send, qr-receive, broadcast and deliver, and then we plug in the quiescent implementations of these primitives given in Section 4.2 (these implementations use HB). <p> Although this algorithm is almost identical to the one given in <ref> [CT96] </ref> for non-partitionable networks, the network assumptions, the consensus requirements, and the failure detector properties are different, and so its proof of correctness and quiescence changes. The rotating coordinator algorithm is shown in Fig. 3 (the code consisting of lines 39-41 is executed atomically). Processes proceed in asynchronous rounds. <p> Consequently, c reliably broadcasts a request to decide estimate c . At any time, if a process delivers such a request, it decides accordingly. We next prove that the algorithm is correct and quiescent. Our proof is similar to the one in <ref> [CT96] </ref>, except for the proofs of Termination and Quiescence. The main difficulty in these proofs stems from the fact that we do not assume that partitions are eventually isolated: it is possible for processes in one partition to receive messages from outside this partition, forever. <p> If p is correct then the result follows from the Agreement property of reliable broadcast. 2 We omit the proof of the next lemma because it is almost identical to the one of Lemma 6.2.1 in <ref> [CT96] </ref>. Lemma 18 (Uniform Agreement) No two processes (whether in the same partition or not) decide differently. We now show the termination and quiescence properties of the implementation.
Reference: [DFKM96] <author> Danny Dolev, Roy Friedman, Idit Keidar, and Dahlia Malkhi. </author> <title> Failure detectors in omission failure environments. </title> <type> Technical Report TR96-1608, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: In Section 6, we show how to implement HB in partitionable networks. We conclude with a brief discussion of related work (Section 7) and of our model (Section 8). 2 The consensus algorithms for partitionable networks given in <ref> [FKM + 95, CHT96a, DFKM96] </ref> are not quiescent. 3 2 Model We consider asynchronous message-passing distributed systems in which there are no timing assumptions. In particular, we make no assumptions on the time it takes to deliver a message, or on relative process speeds. <p> A weaker class of failure detectors, denoted 3S LP , is obtained by defining the largest partition as in Section 5.1, and replacing For every partition P with For the largest partition P in the two properties above (this definition is similar to one given in <ref> [DFKM96] </ref>). <p> use 3S and 3S LP to refer to an arbitrary member of the respective class. 5.3 Quiescent Consensus for Partitionable Networks Cannot be Achieved using 3S Although consensus for partitionable networks can be solved using 3S, we now show that any such solution is not quiescent (the consensus algorithms in <ref> [CHT96a, DFKM96] </ref> do not contradict this result because they are not quiescent). Theorem 14 In partitionable networks with 5 or more processes, consensus has no quiescent implementation using 3S. <p> Regarding consensus, the works that are closest to ours are <ref> [FKM + 95, CHT96a, DFKM96, GS96] </ref>. In [GS96], as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. Roughly speaking, only a subset G of the processes are required to satisfy some accuracy property. <p> The first paper to consider the consensus problem in partitionable networks is [FKM + 95], but the algorithms described in that paper had errors [CHT96a]. Correct algorithms can be found in <ref> [CHT96a, DFKM96] </ref>. 18 All these algorithms use a variant of 3S, but in contrast to the one given in this paper they do not use HB and are not quiescent: processes in minority partitions may send messages forever. <p> The underlying model of failures and failure detectors is also significantly different from the one proposed in this paper. Another model of failure detectors for partitionable networks is given in [BDM97]. We compare models in the next section. 8 Comparison with other Models In <ref> [DFKM96, BDM97] </ref>, network connectivity is defined in terms of the messages exchanged in a run in particular, it depends on whether the algorithm being executed sends a message or not, on the times these messages are sent, and on whether these messages are received. <p> Ensuring that all correct processes in the largest partition decide can be subsequently achieved by a (quiescent) reliable broadcast of the decision value. 22 is intended to model the physical condition of each link independent of the particular messages sent by the algorithm being executed. In <ref> [DFKM96] </ref>, two processes p and q are permanently connected in a given run if they do not crash and there is a time after which every message that p sends to q is received by q, and vice-versa. Clearly, network connectivity depends on the messages of the run.
Reference: [FKM + 95] <author> Roy Friedman, Idit Keidar, Dahlia Malkhi, Ken Birman, and Danny Dolev. </author> <title> Deciding in par-titionable networks. </title> <type> Technical Report TR95-1554, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: In Section 6, we show how to implement HB in partitionable networks. We conclude with a brief discussion of related work (Section 7) and of our model (Section 8). 2 The consensus algorithms for partitionable networks given in <ref> [FKM + 95, CHT96a, DFKM96] </ref> are not quiescent. 3 2 Model We consider asynchronous message-passing distributed systems in which there are no timing assumptions. In particular, we make no assumptions on the time it takes to deliver a message, or on relative process speeds. <p> Regarding consensus, the works that are closest to ours are <ref> [FKM + 95, CHT96a, DFKM96, GS96] </ref>. In [GS96], as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. Roughly speaking, only a subset G of the processes are required to satisfy some accuracy property. <p> However, their model assumes that the network is completely connected and links between correct processes do not lose messages thus, no permanent partition is possible. The first paper to consider the consensus problem in partitionable networks is <ref> [FKM + 95] </ref>, but the algorithms described in that paper had errors [CHT96a]. <p> In particular, the link failure pattern 17 I.e., its specification refers only to the behavior of non-faulty processes. 18 Actually, the specification of consensus considered in <ref> [FKM + 95, CHT96a] </ref> only requires that one correct process in the largest partition eventually decides.
Reference: [FLP85] <author> Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. </author> <title> Impossibility of distributed consensus with one faulty process. </title> <journal> Journal of the ACM, </journal> <volume> 32(2) </volume> <pages> 374-382, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: Roughly speaking, some processes propose a value and must 12 This specification is a generalization of the one for non-partitionable networks given in [ACT97a]. 12 decide on one of the proposed values <ref> [FLP85] </ref>. More precisely, consensus is defined in terms of two primitives, propose (v) and decide (v), where v is a value drawn from a set of possible proposed values. When a process invokes propose (v), we say that it proposes v. <p> of the correct processes actually propose a value (the others may not wish to run consensus). 5.2 3S for Partitionable Networks It is well known that consensus cannot be solved in asynchronous systems, even if at most one process may crash and the network is completely connected with reliable links <ref> [FLP85] </ref>. To overcome this problem, Chandra and Toueg introduced unreliable failure detectors in [CT96].
Reference: [Gop92] <author> Ajei Gopal. </author> <title> Fault-Tolerant Broadcasts and Multicasts: The Problem of Inconsistency and Contamination. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: In [BCBT96], Basu et al. pose the following question: given a problem that can be solved in asynchronous systems with process crashes only, can this problem still be solved if links can also fail by losing messages? They show that the answer is yes if the problem is correct-restricted <ref> [BN92, Gop92] </ref> 17 or if more than half of the processes do not crash.
Reference: [GS96] <author> Rachid Guerraoui and Andr e Schiper. </author> <title> Gamma-Accurate failure detectors. </title> <booktitle> In Proceedings of the 10th International Workshop on Distributed Algorithms, Lecture Notes on Computer Science, </booktitle> <pages> pages 269-286. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Regarding consensus, the works that are closest to ours are <ref> [FKM + 95, CHT96a, DFKM96, GS96] </ref>. In [GS96], as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. Roughly speaking, only a subset G of the processes are required to satisfy some accuracy property. <p> Regarding consensus, the works that are closest to ours are [FKM + 95, CHT96a, DFKM96, GS96]. In <ref> [GS96] </ref>, as a first step towards partitionable networks, Guerraoui and Schiper define G-accurate failure detectors. Roughly speaking, only a subset G of the processes are required to satisfy some accuracy property.
Reference: [HT94] <author> Vassos Hadzilacos and Sam Toueg. </author> <title> A modular approach to fault-tolerant broadcasts and related problems. </title> <type> Technical Report TR 94-1425, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: These fields make every message unique. We say that q delivers message m if q returns from the invocation of deliver (m). Primitives broadcast and deliver satisfy the following properties: 6 6 This specification is a generalization of the one for non-partitionable networks given in <ref> [HT94] </ref>. 7 * Validity: If a correct process broadcasts a message m, then it eventually delivers m. * Agreement: If a correct process p delivers a message m, then all processes in the partition of p eventually deliver m. * Uniform Integrity: For every message m, every process delivers m at
Reference: [vR97] <author> Robbert van Renesse, </author> <month> April </month> <year> 1997. </year> <title> Private Communication. </title> <type> 25 </type>
Reference-contexts: We then show how to implement it in Section 6. HB is different from the failure detectors defined in [CT96], or those currently in use in many systems (even though some existing systems, such as Ensemble and Phoenix, use the same name heartbeat in their failure detector implementations <ref> [vR97, Cha97] </ref>). In contrast to existing failure detectors, HB is implementable in asynchronous systems, without the use of timeouts (see Section 6). A heartbeat failure detector D (for partitionable networks) has the following features. <p> Thus, HB should not be confused with existing implementations of failure detectors (some of which, such as those in Ensemble and Phoenix, have modules that are also called heartbeat <ref> [vR97, Cha97] </ref>).
References-found: 16

