URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-12.ps.Z
Refering-URL: 
Root-URL: 
Title: The Linear Convergence of a Successive Linear Programming Algorithm  
Author: Michael C. Ferris Sergei K. Zavriev 
Date: December 3, 1996  
Abstract: We present a successive linear programming algorithm for solving constrained nonlinear optimization problems. The algorithm employs an Armijo procedure for updating a trust region radius. We prove the linear convergence of the method by relating the solutions of our sub-problems to standard trust region and gradient projection subproblems and adapting an error bound analysis due to Luo and Tseng. Computational results are provided for polyhedrally constrained nonlinear programs.
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method <ref> [2, 16] </ref>) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm. <p> Note that this is important in the example of Figure 1 when r &gt; r fl . Furthermore, in that example, note that r 0 (x; a) = 1 for any a 2 <ref> [1=2; 2] </ref> but that a 0 (x; 1) = 1=2 is well defined. The main properties of the function a 0 (x; ) : R + ! R + [ f1g are presented in the following lemma. Lemma 2.9 For every x 2 X the following properties hold. i.
Reference: [3] <author> D. P. Bertsekas. </author> <title> Nonlinear Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: There have been many attempts to generate robust successive linear programming algorithms for nonlinear programming (see, for example [1, 14, 15, 19, 24, 26, 28]). This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method <ref> [3] </ref> (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm. <p> The only known linear rate of convergence is given in [12, 13], but this is for problems where the constraints are not polyhedral, but have "positive curvature"; there are even simple examples <ref> [3, p. 199] </ref> fl This work is partially supported by the National Science Foundation under grant CCR-9157632. y Computer Sciences Department, University of Wisconsin, Madison, WI 53706 z Operations Research Department, Faculty of Computational Mathematics and Cybernetics, Moscow State University, Moscow, Russia, 119899 1 that show the conditional gradient method has
Reference: [4] <author> I. Bongartz, A. R. Conn, N. Gould, and Ph. L. Toint. CUTE: </author> <title> Con strained and unconstrained testing environment. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 21 </volume> <pages> 123-160, </pages> <year> 1995. </year>
Reference-contexts: We have implemented SLP in Matlab [23], using the Cplex [11] linear programming code for the trust region subproblems LP (x; r). Using the interface between Cute <ref> [4] </ref> and Matlab described in [6], we have tested the algorithm on a reasonable class of linearly constrained test problems. The results are summarized in Tables 1 and 2, which arbitrarily separates the problems in the Hock-Schittkowski collection [18] from the remaining ones we tested from the Cute collection.
Reference: [5] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <type> Mathematical Programming Technical Report 95-21, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1995. </year>
Reference-contexts: Such a procedure would undoubtedly improve the performance of the algorithm on problems HS268 and S268. Similar finitely terminating successive linearization algorithms have been used for machine learning problems <ref> [5, 22] </ref>
Reference: [6] <author> M. A. </author> <title> Branch. Getting CUTE with Matlab. </title> <institution> Cornell Theory Center Report CTC94TR194, Cornell University, </institution> <address> Ithaca, New York, </address> <year> 1994. </year>
Reference-contexts: We have implemented SLP in Matlab [23], using the Cplex [11] linear programming code for the trust region subproblems LP (x; r). Using the interface between Cute [4] and Matlab described in <ref> [6] </ref>, we have tested the algorithm on a reasonable class of linearly constrained test problems. The results are summarized in Tables 1 and 2, which arbitrarily separates the problems in the Hock-Schittkowski collection [18] from the remaining ones we tested from the Cute collection.
Reference: [7] <author> J. V. Burke. </author> <title> A robust trust region method for constrained nonlin ear programming problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 </volume> <pages> 325-347, </pages> <year> 1992. </year>
Reference-contexts: Conditions relating stationary points to solutions of P are well known. 2 Our method resembles a trust region method (see <ref> [7, 9] </ref>) by virtue of the fact that the subproblems are of the form: LP (x; r) : subject to x + h 2 X; khk fl r; where kk fl represents an arbitrary norm on R n and r is the trust region radius.
Reference: [8] <author> J. V. Burke and J. J. </author> <title> More. On the identification of active constraints. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 25 </volume> <pages> 1197-1211, </pages> <year> 1988. </year>
Reference-contexts: Furthermore, conditions under which this method identifies the correct active set after a finite number of iterations could be derived in a similar fashion to that given by Burke and More <ref> [8] </ref>. Such a procedure would undoubtedly improve the performance of the algorithm on problems HS268 and S268. Similar finitely terminating successive linearization algorithms have been used for machine learning problems [5, 22]
Reference: [9] <author> J. V. Burke, J. J. More, and G. Toraldo. </author> <title> Convergence properties of trust region methods for linear and convex constraints. </title> <journal> Mathematical Programming, </journal> <volume> 47 </volume> <pages> 305-336, </pages> <year> 1990. </year>
Reference-contexts: Conditions relating stationary points to solutions of P are well known. 2 Our method resembles a trust region method (see <ref> [7, 9] </ref>) by virtue of the fact that the subproblems are of the form: LP (x; r) : subject to x + h 2 X; khk fl r; where kk fl represents an arbitrary norm on R n and r is the trust region radius.
Reference: [10] <author> P. H. Calamai and J. J. </author> <title> More. Projected gradient methods for linearly constrained problems. </title> <journal> Mathematical Programming, </journal> <volume> 39 </volume> <pages> 93-116, </pages> <year> 1987. </year>
Reference-contexts: We collect some well known properties of the function r 0 (x; ) : R + ! R + in the following lemma. The proofs can be found in <ref> [10, 17, 27] </ref>. Lemma 2.5 For every x 2 X the following properties hold: i. r 0 (x; ) 2 C (R + ); iii. 8a 0 a &gt; 0, a 0 a . iv. 8a &gt; 0; r 0 (x; a) = 0 () x 2 X stat .
Reference: [11] <author> CPLEX Optimization Inc., </author> <title> Incline Village, Nevada. Using the CPLEX(TM) Linear Optimizer and CPLEX(TM) Mixed Integer Optimizer (Version 2.0), </title> <year> 1992. </year>
Reference-contexts: We have implemented SLP in Matlab [23], using the Cplex <ref> [11] </ref> linear programming code for the trust region subproblems LP (x; r). Using the interface between Cute [4] and Matlab described in [6], we have tested the algorithm on a reasonable class of linearly constrained test problems.
Reference: [12] <author> J. C. Dunn. </author> <title> Rates of convergence for conditional gradient algorithms near singular and nonsingular extremals. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 17 </volume> <pages> 187-211, </pages> <year> 1979. </year>
Reference-contexts: We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm. The only known linear rate of convergence is given in <ref> [12, 13] </ref>, but this is for problems where the constraints are not polyhedral, but have "positive curvature"; there are even simple examples [3, p. 199] fl This work is partially supported by the National Science Foundation under grant CCR-9157632. y Computer Sciences Department, University of Wisconsin, Madison, WI 53706 z Operations
Reference: [13] <author> J. C. Dunn and E. Sachs. </author> <title> The effect of perturbations on the con vergence rates of optimization algorithms. </title> <journal> Applied Mathematics and Optimization, </journal> <volume> 10 </volume> <pages> 143-157, </pages> <year> 1983. </year> <month> 27 </month>
Reference-contexts: We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm. The only known linear rate of convergence is given in <ref> [12, 13] </ref>, but this is for problems where the constraints are not polyhedral, but have "positive curvature"; there are even simple examples [3, p. 199] fl This work is partially supported by the National Science Foundation under grant CCR-9157632. y Computer Sciences Department, University of Wisconsin, Madison, WI 53706 z Operations
Reference: [14] <author> M. C. Ferris. </author> <title> Iterative linear programming solution of convex programs. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 65 </volume> <pages> 53-65, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction In this paper we develop a first order technique for constrained optimization problems based on subproblems with a linear objective function. There have been many attempts to generate robust successive linear programming algorithms for nonlinear programming (see, for example <ref> [1, 14, 15, 19, 24, 26, 28] </ref>). This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm. <p> Note however that the method will only give linear convergence and thus once an active set has been identified, the algorithm should apply a New-ton process for solving the resulting reduced problem, such as the linear programming based technique given in <ref> [14] </ref>. Furthermore, conditions under which this method identifies the correct active set after a finite number of iterations could be derived in a similar fashion to that given by Burke and More [8]. Such a procedure would undoubtedly improve the performance of the algorithm on problems HS268 and S268.
Reference: [15] <author> R. Fletcher and E. Sainz de la Maza. </author> <title> Nonlinear programming and nonsmooth optimization by successive linear programming. Numerical Analysis Report NA/100, </title> <institution> Department of Mathematical Sciences, The University, </institution> <address> Dundee, Scotland, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction In this paper we develop a first order technique for constrained optimization problems based on subproblems with a linear objective function. There have been many attempts to generate robust successive linear programming algorithms for nonlinear programming (see, for example <ref> [1, 14, 15, 19, 24, 26, 28] </ref>). This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm.
Reference: [16] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method <ref> [2, 16] </ref>) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm.
Reference: [17] <author> E. M. Gafni and D. P. Bertsekas. </author> <title> Two-metric projection methods for constrained optimization. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 22 </volume> <pages> 936-964, </pages> <year> 1984. </year>
Reference-contexts: We collect some well known properties of the function r 0 (x; ) : R + ! R + in the following lemma. The proofs can be found in <ref> [10, 17, 27] </ref>. Lemma 2.5 For every x 2 X the following properties hold: i. r 0 (x; ) 2 C (R + ); iii. 8a 0 a &gt; 0, a 0 a . iv. 8a &gt; 0; r 0 (x; a) = 0 () x 2 X stat .
Reference: [18] <author> W. Hock and K. Schittkowski. </author> <title> Test Examples for Nonlinear Program ming Codes, </title> <booktitle> volume 187 of Lecture Notes in Economics and Mathematical Systems. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1981. </year>
Reference-contexts: Using the interface between Cute [4] and Matlab described in [6], we have tested the algorithm on a reasonable class of linearly constrained test problems. The results are summarized in Tables 1 and 2, which arbitrarily separates the problems in the Hock-Schittkowski collection <ref> [18] </ref> from the remaining ones we tested from the Cute collection. In the tables, the number of general 23 linear constraints is denoted by "m" and "n" is used to denote the number of variables.
Reference: [19] <author> L. S. Lasdon, A. Waren, S. Sarkar, and F. Palacios. </author> <title> Solving the pool ing problem using generalized reduced gradient and successive linear programming algorithms. </title> <journal> ACM SIGMAP Bulletin, </journal> <volume> 27 </volume> <pages> 9-15, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction In this paper we develop a first order technique for constrained optimization problems based on subproblems with a linear objective function. There have been many attempts to generate robust successive linear programming algorithms for nonlinear programming (see, for example <ref> [1, 14, 15, 19, 24, 26, 28] </ref>). This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm.
Reference: [20] <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bounds and convergence analysis of feasible descent methods: A general approach. </title> <journal> Annals of Operations Research, </journal> <volume> 46 </volume> <pages> 157-178, </pages> <year> 1993. </year>
Reference-contexts: It also guarantees that every subproblem we generate has an optimal solution. We develop an adaptive mechanism to update the trust region parameter that guarantees the linear convergence of the method under the standard assumptions given by Luo and Tseng <ref> [20] </ref>. Our analysis is somewhat involved and relies upon estimating descent properties of the directions generated by our algorithm in terms of standard quadratic subproblems arising for example in gradient projection algorithms. To our knowledge, every linearly convergent technique for such problems is based on such quadratic subproblems. <p> But this is a contradiction to the first statement of the theorem. Our convergence analysis is based in part of that given by Luo and Tseng <ref> [20] </ref>. The following assumptions are part of that work and will be used in our main convergence result. Assumption A1. <p> Assumption A2. For every - f opt the set f (fx 2 X stat j f (x) -g) R 1 is finite. Theorem 2.1 of <ref> [20] </ref> gives sufficient conditions to guarantee that the above assumptions hold. To obtain more precise convergence properties of SLP, we need the following lemma. This result essentially provides a local error bound. 17 Lemma 4.3 Let Assumption A1 hold. <p> Combining (30) and (31) yields f (x) f ( X stat (x)) ( 1 + 2 )r 2 : Now the proof of the main convergence theorem is similar to that given by Luo and Tseng <ref> [20, Theorem 3.1] </ref> for the linear convergence of a class of descent techniques. Theorem 4.5 Let A1, A2 hold. Every infinite sequence of iterates produced by SLP converges to a point of x fl 2 X stat .
Reference: [21] <author> O. L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year> <note> SIAM Classics in Applied Mathematics 10, SIAM, Philadelphia, </note> <year> 1994. </year>
Reference-contexts: We shall relate this to the following problem: T P (x; 1) : subject to x + h 2 X: Note that it is possible for v (x; 1) = 1 and H (x; 1) = ;. The standard optimality conditions (see <ref> [21, 25] </ref>) for the problem T P (x; r) provide the following lemma. Lemma 2.1 i.
Reference: [22] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave mini mization. </title> <editor> In H. Fischer, B. Riedmueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Hei-delberg, </address> <year> 1996. </year>
Reference-contexts: Such a procedure would undoubtedly improve the performance of the algorithm on problems HS268 and S268. Similar finitely terminating successive linearization algorithms have been used for machine learning problems <ref> [5, 22] </ref>
Reference: [23] <author> MATLAB. </author> <title> User's Guide. The MathWorks, </title> <publisher> Inc., </publisher> <year> 1992. </year>
Reference-contexts: We have implemented SLP in Matlab <ref> [23] </ref>, using the Cplex [11] linear programming code for the trust region subproblems LP (x; r). Using the interface between Cute [4] and Matlab described in [6], we have tested the algorithm on a reasonable class of linearly constrained test problems.
Reference: [24] <author> F. Palacios-Gomez, L. Lasdon, and M. Engquist. </author> <title> Nonlinear optimiza tion by successive linear programming. </title> <journal> Management Science, </journal> <volume> 28 </volume> <pages> 1106-1120, </pages> <year> 1982. </year> <month> 28 </month>
Reference-contexts: 1 Introduction In this paper we develop a first order technique for constrained optimization problems based on subproblems with a linear objective function. There have been many attempts to generate robust successive linear programming algorithms for nonlinear programming (see, for example <ref> [1, 14, 15, 19, 24, 26, 28] </ref>). This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm.
Reference: [25] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, Prince ton, </publisher> <address> New Jersey, </address> <year> 1970. </year>
Reference-contexts: We shall relate this to the following problem: T P (x; 1) : subject to x + h 2 X: Note that it is possible for v (x; 1) = 1 and H (x; 1) = ;. The standard optimality conditions (see <ref> [21, 25] </ref>) for the problem T P (x; r) provide the following lemma. Lemma 2.1 i.
Reference: [26] <author> T.-H. Shiau. </author> <title> Iterative Linear Programming for Linear Complementarity and Related Problems. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1983. </year> <type> Technical Report 507. </type>
Reference-contexts: 1 Introduction In this paper we develop a first order technique for constrained optimization problems based on subproblems with a linear objective function. There have been many attempts to generate robust successive linear programming algorithms for nonlinear programming (see, for example <ref> [1, 14, 15, 19, 24, 26, 28] </ref>). This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm.
Reference: [27] <author> Ph. L. Toint. </author> <title> Global convergence of a class of trust region methods for nonconvex minimization in a Hilbert space. </title> <journal> IMA Journal of Numerical Analysis, </journal> <volume> 8 </volume> <pages> 231-252, </pages> <year> 1988. </year>
Reference-contexts: We collect some well known properties of the function r 0 (x; ) : R + ! R + in the following lemma. The proofs can be found in <ref> [10, 17, 27] </ref>. Lemma 2.5 For every x 2 X the following properties hold: i. r 0 (x; ) 2 C (R + ); iii. 8a 0 a &gt; 0, a 0 a . iv. 8a &gt; 0; r 0 (x; a) = 0 () x 2 X stat .
Reference: [28] <author> J. Zhang, N.-H. Kim, and L. Lasdon. </author> <title> An improved successive linear programming algorithm. </title> <journal> Management Science, </journal> <volume> 31 </volume> <pages> 1312-1331, </pages> <year> 1985. </year> <month> 29 </month>
Reference-contexts: 1 Introduction In this paper we develop a first order technique for constrained optimization problems based on subproblems with a linear objective function. There have been many attempts to generate robust successive linear programming algorithms for nonlinear programming (see, for example <ref> [1, 14, 15, 19, 24, 26, 28] </ref>). This paper specifies a new algorithm that has the same convergence properties as the gradient projection method. We modify the conditional gradient method [3] (sometimes called the Frank-Wolfe method [2, 16]) for polyhedrally constrained optimization problems to obtain a linearly convergent algorithm.
References-found: 27

