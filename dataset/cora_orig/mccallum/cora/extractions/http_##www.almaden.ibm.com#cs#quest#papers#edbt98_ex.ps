URL: http://www.almaden.ibm.com/cs/quest/papers/edbt98_ex.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Discovery-driven Exploration of OLAP Data Cubes  
Author: Sunita Sarawagi Rakesh Agrawal Nimrod Megiddo 
Address: 650 Harry Road, San Jose, CA 95120, USA  
Affiliation: IBM Almaden Research Center,  
Abstract: Analysts predominantly use OLAP data cubes to identify regions of anomalies that may represent problem areas or new opportunities. The current OLAP systems support hypothesis-driven exploration of data cubes through operations such as drill-down, roll-up, and selection. Using these operations, an analyst navigates unaided through a huge search space looking at large number of values to spot exceptions. We propose a new discovery-driven exploration paradigm that mines the data for such exceptions and summarizes the exceptions at appropriate levels in advance. It then uses these exceptions to lead the analyst to interesting regions of the cube during navigation. We present the statistical foundation underlying our approach. We then discuss the computational issue of finding exceptions in data and making the process efficient on large multidimensional data bases.
Abstract-found: 1
Intro-found: 1
Reference: [AAD + 96] <author> S. Agarwal, R. Agrawal, P.M. Deshpande, A. Gupta, J.F. Naughton, R. Ramakrishnan, and S. Sarawagi. </author> <title> On the computation of multidimensional aggregates. </title> <booktitle> In Proc. of the 22nd Int'l Conference on Very Large Databases, </booktitle> <pages> pages 506-521, </pages> <address> Mumbai (Bombay), India, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: The model allows exceptions to be found at all levels of aggregation. We present computation techniques that make the process of finding exceptions efficient for large OLAP datasets. Our techniques use the same kind of data scan operations as required for cube aggregate computation <ref> [AAD + 96] </ref> and thus enables overlap of exception finding with routine aggregate precomputation. These techniques recognize that the data may be too large to fit in main memory and intermediate results may have to be written to disk requiring careful optimization. Paper layout The paper is organized as follows. <p> The first phase involves the computation of the aggregate values (as specified by the user-provided aggregate function) over which exceptions will be found at each group-by of the cube. This is essentially the problem of cube computation and efficient computation techniques for this problem have been developed in <ref> [AAD + 96] </ref>. 2. The next phase is model fitting, i.e., finding the coefficients of the model equation and using them to find the residuals as discussed in Section 3.1. 3. The final phase involves summarizing exceptions found in the second phase as discussed in Section 3.6. <p> for each of the 2 3 1 = 7 group-bys of the cube by starting from the ABC group-by and computing the average at AB, AC and BC from ABC, computing the average at A from one of AB or AC and so on, using the cube computation methods of <ref> [AAD + 96] </ref>. We then compute the coefficient starting from ALL.
Reference: [AGS97] <author> Rakesh Agrawal, Ashish Gupta, and Sunita Sarawagi. </author> <title> Modeling multidimensional databases. </title> <booktitle> In Proc. of the 13th Int'l Conference on Data Engineering, </booktitle> <address> Birmingham, U.K., </address> <month> April </month> <year> 1997. </year>
Reference: [Arb] <institution> Arbor Software Corporation. </institution> <note> Application Manager User's Guide, Essbase version 4.0. http://www.arborsoft.com. </note>
Reference-contexts: This prototype uses the Microsoft Excel spreadsheet, extended with appropriate macros, as the front-end for user-interaction. The backend is the well-known OLAP product, Arbor Essbase <ref> [Arb] </ref> that computes and stores the exceptions using the techniques we present in Sections 3 and 4. To keep the example brief, we will consider a three-dimensional data cube with dimensions Product, Market, and Time. There is a hierarchy Market ! Region ! ALL on the Market dimension. <p> To keep the example brief, we will consider a three-dimensional data cube with dimensions Product, Market, and Time. There is a hierarchy Market ! Region ! ALL on the Market dimension. The data for this cube is taken from a sample OLAP database distributed with Essbase <ref> [Arb] </ref>. We annotate every cell in all possible aggregations of a data cube with a value that indicates the degree of "surprise" that the quantity in the cell holds. The surprise value captures how anomalous a quantity in a cell is with respect to other cells.
Reference: [BFH75] <author> Y. Bishop, S. Fienberg, and P. Holland. </author> <title> Discrete Multivariate Analysis theory and practice. </title> <publisher> The MIT Press, </publisher> <year> 1975. </year>
Reference-contexts: The variance (square of standard deviation) is estimated as the sum of squares of the residuals divided by the number of entries. We found that this method provides poor fits on OLAP data. In the analysis of contingency tables <ref> [BFH75] </ref>, where cell entries represent counts, the Poisson distribution is assumed. This assumption implies that the variance is equal to the mean. When the entries are not counts (e.g., large dollar values), this typically leads to an underestimate of the variance.
Reference: [CL86] <author> William W. Cooley and Paul R Lohnes. </author> <title> Multivariate data analysis. </title> <editor> Robert E. </editor> <publisher> Krieger publishers, </publisher> <year> 1986. </year>
Reference-contexts: Other (more complex) functional forms for f are also possible | most of them involving different mixtures of additive and multiplicative terms [HMJ88]. A significantly different approach in this category is the one suggested in [Man71] where factor analytic models like the singular value decomposition <ref> [CL86] </ref> are used to fit a model based on a mixture of additive and multiplicative terms. The main demerit of these models is the high overhead of computing them and the lack of generalizations of the models to more than 2-3 dimensions and hierarchies. <p> We model the variance as a power of the mean value ^y i 1 :::i n as: i 1 i 2 :::i n = (^y i 1 i 2 :::i n ) : To calculate we use the maximum likelihood principle <ref> [CL86] </ref> on data assumed to be distributed normally with the mean value ^y i 1 i 2 :::i n .
Reference: [Col95] <author> George Colliat. </author> <title> OLAP, relational, and multidimensional database systems. </title> <type> Technical report, </type> <institution> Arbor Software Corporation, </institution> <address> Sunnyvale, CA, </address> <year> 1995. </year>
Reference-contexts: OLAP software helps analysts and managers gain insight into the performance of an enterprise through a wide variety of views of data organized to reflect the multidimensional nature of enterprise data <ref> [Col95] </ref>. An increasingly popular data model for OLAP applications is the multidimensional database [OLA96][AGS97], also known as the data cube [GBLP96]. A data cube consists of two kinds of attributes: measures and dimensions. <p> This "hypothesis-driven" exploration for anomalies has several shortcomings. The search space is very large | typically, a cube has 5-8 dimensions, each dimension has a hierarchy that is 2-8 levels deep and each level of the hierarchy has ten to hundreds of members <ref> [Col95] </ref>. Simply looking at data aggregated at various levels of details to hunt down an anomaly that could be one of several million values hidden in detailed data is a daunting task.
Reference: [GBLP96] <author> J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. </author> <title> Data cube: A relational aggregation operator generalizing group-by, </title> <booktitle> cross-tabs and sub-totals. In Proc. of the 12th Int'l Conference on Data Engineering, </booktitle> <pages> pages 152-159, </pages> <year> 1996. </year>
Reference-contexts: An increasingly popular data model for OLAP applications is the multidimensional database [OLA96][AGS97], also known as the data cube <ref> [GBLP96] </ref>. A data cube consists of two kinds of attributes: measures and dimensions. The set of dimensions consists of attributes like product names and store names that together form a key. The measures are typically numeric attributes like sales volumes and profit.
Reference: [HMJ88] <author> D. Hoaglin, F. Mosteller, and Tukey. J. </author> <title> Exploring data tables, trends and shapes. Wiley series in probability, </title> <year> 1988. </year>
Reference-contexts: We call a value an exception if it differs significantly from the anticipated value calculated using a model that takes into account all aggregates (group-bys) in which the value participates. This model was inspired by the table analysis methods <ref> [HMJ88] </ref> used in the statistical literature. <p> Multiplicative: the function f returns the product of its arguments. Other (more complex) functional forms for f are also possible | most of them involving different mixtures of additive and multiplicative terms <ref> [HMJ88] </ref>. A significantly different approach in this category is the one suggested in [Man71] where factor analytic models like the singular value decomposition [CL86] are used to fit a model based on a mixture of additive and multiplicative terms. <p> This helps provide easy grasp of why certain numbers are marked exceptions. 2. Other robust estimates: The main shortcoming of the mean-based approach is that it is not robust in the presence of extremely large outliers. Therefore, a number of methods including the median polish method <ref> [HMJ88] </ref> and the square combining method [HMJ88] have been proposed. These are all based on using robust estimates of central tendency like "median" or "trimmed-mean" instead of "mean" for calculating the coefficients. <p> Other robust estimates: The main shortcoming of the mean-based approach is that it is not robust in the presence of extremely large outliers. Therefore, a number of methods including the median polish method <ref> [HMJ88] </ref> and the square combining method [HMJ88] have been proposed. These are all based on using robust estimates of central tendency like "median" or "trimmed-mean" instead of "mean" for calculating the coefficients.
Reference: [HMT83] <author> D.C. Hoaglin, F. Mosteller, and J.W. Tukey. </author> <title> Understanding Robust and Exploratory Data Analysis. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Two possible approaches are: 1. Mean-based estimates: For deriving these estimates we assume the logarithms of the values are distributed normally with the same variance. The following approach yields the least-squares estimates in that case <ref> [HMT83] </ref>: - fl = ` +:::+ which is the grand mean or average.
Reference: [Man71] <author> J. Mandel. </author> <title> A new analysis of variance model for non-additive data. </title> <journal> Tech-nometrics, </journal> <volume> 13 </volume> <pages> 1-18, </pages> <year> 1971. </year>
Reference-contexts: Multiplicative: the function f returns the product of its arguments. Other (more complex) functional forms for f are also possible | most of them involving different mixtures of additive and multiplicative terms [HMJ88]. A significantly different approach in this category is the one suggested in <ref> [Man71] </ref> where factor analytic models like the singular value decomposition [CL86] are used to fit a model based on a mixture of additive and multiplicative terms.
Reference: [Mon91] <author> D.G. Montgomery. </author> <title> Design and Analysis of Experiments, chapter 13. </title> <publisher> John Wiley & sons, </publisher> <address> third edition, </address> <year> 1991. </year>
Reference-contexts: We used the 75% trimmed-mean where 25% of the extreme values are trimmed off and the mean is taken of the middle 75% numbers. By dropping 25% of the extreme numbers, we make the method robust to outliers. 3.5 Estimating standard deviation In classical Analysis of Variance (ANOVA) methods <ref> [Mon91] </ref>, the standard deviation for all the cells is assumed to be identical. The variance (square of standard deviation) is estimated as the sum of squares of the residuals divided by the number of entries. We found that this method provides poor fits on OLAP data.
Reference: [OLA96] <author> The OLAP Council. </author> <title> MD-API the OLAP Application Program Interface Version 0.5 Specification, </title> <month> September </month> <year> 1996. </year>
Reference: [SAM98] <author> Sunita Sarawagi, Rakesh Agrawal, and Nimrod Megiddo. </author> <title> Discovery-driven exploration of OLAP data cubes. </title> <type> Research Report RJ 10102 (91918), </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, CA 95120, </address> <month> January </month> <year> 1998. </year> <title> Available from http://www.almaden.ibm.com/cs/quest. This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: The exploration typically starts at the highest level of hierarchies of the cube dimension. Further, navigation of the cube is done using a sequence of "drill-down" ? This is an abridged version of the full paper that appears in <ref> [SAM98] </ref>. (zooming in to more detailed levels of hierarchies), "roll-up" (zooming out to less detailed levels) and "selection" (choosing a subset of dimension members) operations. <p> In Section 2 we demonstrate a scenario of the use of our proposed method. Section 3 gives the statistical model we use to compute the anticipated value of a cell and the rationale for choosing this model. Computation techniques are discussed in Section 4. Refer to <ref> [SAM98] </ref> for some performance results and experience with real-life datasets that illustrates the effectiveness of the proposed approach. We conclude with a summary and directions for future work in Section 5. 2 An Illustrative Example We illustrate our proposed method using an example session with our prototype implementation. <p> There could be several interpretations of this notion. We present the approach we use. In <ref> [SAM98] </ref> we discuss the alternatives we considered before deciding on our approach. Figure5. Change in sales over Time for Product "Birch-B" Our choice of exception model was motivated by the following desiderata: 1. <p> The main demerit of these models is the high overhead of computing them and the lack of generalizations of the models to more than 2-3 dimensions and hierarchies. In our experience with OLAP datasets, the multiplicative form provided better fit than the additive form. (See <ref> [SAM98] </ref> for an intuitive reason for this.) For ease of calculation, we transform the multiplicative form to a linear additive form by taking a log of original data values. <p> According to the latter, one can derive that the estimated value of must satisfy: X (y i 1 i 2 :::i n ^y i 1 i 2 :::i n ) 2 X The method we used for solving the equation to find is discussed in <ref> [SAM98] </ref>. 3.6 Summarizing exceptions As discussed in Section 2, we need to summarize exceptions in lower levels of the cube as single values at higher levels of cube. We present concise definitions of the SelfExp,InExp and PathExp quantities we associate with each cell for this purpose. In [SAM98] more formal definitions <p> is discussed in <ref> [SAM98] </ref>. 3.6 Summarizing exceptions As discussed in Section 2, we need to summarize exceptions in lower levels of the cube as single values at higher levels of cube. We present concise definitions of the SelfExp,InExp and PathExp quantities we associate with each cell for this purpose. In [SAM98] more formal definitions appear. SelfExp: denotes the exception value of the cell. This quantity is defined as the scaled absolute value of the residual defined in Eq. 2 with a cut-off threshold of t . <p> The final phase involves summarizing exceptions found in the second phase as discussed in Section 3.6. Computationally, this phase is similar to phase 1 with a few differences as discussed in <ref> [SAM98] </ref>. 4.1 Model fitting In general, we need to fit separate equations for different group-bys of the cube as discussed in Section 3.2. We will first consider the scenario where a single equation is fit on the base level data. <p> This is equivalent to joining the n-attribute group-by with 2 n 1 other group-bys. When the size of these group-bys is large, computing so many multi-attribute joins per group-by can incur large sorting and comparison costs. This straightforward computation can be improved further as discussed in <ref> [SAM98] </ref>. Rewriting We now discuss further ways of speeding up computation by rewriting Eq. 3. <p> Equations 3 and 5 yield the same set of residuals when the cube contains no missing data. [Proof appears in <ref> [SAM98] </ref>.] When a cube does contain missing data, the residuals could differ depending on the number of missing values. One should evaluate the coefficients iteratively ([HMJ88], chapter 4) for producing accurate least squares fit in such cases. <p> As the user typically navigates the data cube top-down, this enables the user to very naturally capture the context in which the value was declared an exception. In <ref> [SAM98] </ref> we present how our model handles hierarchies and ordered dimensions like time. We devised methods of efficiently computing exceptions. Novel rewriting techniques are used to reduce the cost of model fitting and modifying the computation flow so as to mesh exception finding with cube computation. Our experiments (detailed in [SAM98]) <p> <ref> [SAM98] </ref> we present how our model handles hierarchies and ordered dimensions like time. We devised methods of efficiently computing exceptions. Novel rewriting techniques are used to reduce the cost of model fitting and modifying the computation flow so as to mesh exception finding with cube computation. Our experiments (detailed in [SAM98]) show that these techniques yield almost a factor of three to four performance improvement. We have applied our technique on several real-life OLAP datasets with interesting results. In [SAM98] we report some of these findings. <p> Our experiments (detailed in <ref> [SAM98] </ref>) show that these techniques yield almost a factor of three to four performance improvement. We have applied our technique on several real-life OLAP datasets with interesting results. In [SAM98] we report some of these findings. Future work in the area should incorporate methods for model selection and user customization of the definition of exceptions.
References-found: 13

