URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/iconip96.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Title: An EM Algorithm for Asynchronous Input/Output Hidden Markov Models  
Author: Samy Bengioy, Yoshua Bengioz INRS-Telecommunications, 
Address: Ile-des-Soeurs, Qc, H3E 1H6, CANADA  Montreal, Qc, H3C 3J7, CANADA  
Affiliation: 16, Place du Commerce,  Dept. IRO, Universite de Montreal,  
Abstract: In learning tasks in which input sequences are mapped to output sequences, it is often the case that the input and output sequences are not synchronous. For example, in speech recognition, acoustic sequences are longer than phoneme sequences. Input/Output Hidden Markov Models have already been proposed to represent the distribution of an output sequence given an input sequence of the same length. We extend here this model to the case of asynchronous sequences, and show an Expectation-Maximization algorithm for training such models. 
Abstract-found: 1
Intro-found: 1
Reference: [Bengio and Bengio, 1996] <author> Bengio, Y. and Bengio, S. </author> <year> (1996). </year> <title> Training asynchronous input/output hidden markov models. </title> <type> Technical Report #1013, </type> <institution> Departement d'Informatique et de Recherche Operationnelle, Universite de Montreal, Montreal (QC) Canada. </institution>
Reference-contexts: The derivation of this recursion using the independence assumptions can be found in <ref> [Bengio and Bengio, 1996] </ref>. 3 An EM Algorithm for Asynchronous IOHMMs The learning algorithm we propose is based on the maximum likelihood principle, i.e., here, maximizing the conditional likelihood of the training data. <p> The proof of correctness of this recursion (using the Markovian independence assumptions) is given in <ref> [Bengio and Bengio, 1996] </ref>. We can now express g i;s;t and h i;j;t in terms of ff (i; s; t) and fi (i; s; t) (see derivations in [Bengio and Bengio, 1996]): h i;j;t = L 0 B B P (e i =1) s=1 +P (e i =0) s=1 1 C <p> The proof of correctness of this recursion (using the Markovian independence assumptions) is given in <ref> [Bengio and Bengio, 1996] </ref>. We can now express g i;s;t and h i;j;t in terms of ff (i; s; t) and fi (i; s; t) (see derivations in [Bengio and Bengio, 1996]): h i;j;t = L 0 B B P (e i =1) s=1 +P (e i =0) s=1 1 C C (20) g i;s;t = P (e i =1) L 3.2 The Maximization Step After each estimation step, one has to maximize Q (fi; ^ fi). <p> This variable can be computed recursively by dynamic programming (the derivation is given in <ref> [Bengio and Bengio, 1996] </ref>): V (i; t) = P (e i =1) max j j At the end of the sequence, the best final state i fl which maximizes V (i; T ) is picked within the set of final states F .
Reference: [Bengio and Frasconi, 1995] <author> Bengio, Y. and Frasconi, P. </author> <year> (1995). </year> <title> An input/output HMM architecture. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 427-434. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Another example is a recently proposed recurrent mixture of experts connectionist architecture which has an interpretation as a probabilistic model, called Input/Output Hidden Markov Model (IOHMM) <ref> [Bengio and Frasconi, 1996, Bengio and Frasconi, 1995] </ref>. <p> The output sequence distribution is decomposed into conditional emission distributions for individual outputs (given a state and an input at time t) and conditional transition distributions (given a previous state and an input at time t). This is an extension of the already proposed IOHMMs <ref> [Bengio and Frasconi, 1996, Bengio and Frasconi, 1995] </ref> that allows input and output sequences to be asynchronous. The parameters of the model can be estimated with an EM or GEM algorithm (depending of the form of the emission and transition distributions).
Reference: [Bengio and Frasconi, 1996] <author> Bengio, Y. and Frasconi, P. </author> <year> (1996). </year> <title> Input/Output HMMs for sequence processing. </title> <note> to appear in IEEE Transactions on Neural Networks. </note>
Reference-contexts: Another example is a recently proposed recurrent mixture of experts connectionist architecture which has an interpretation as a probabilistic model, called Input/Output Hidden Markov Model (IOHMM) <ref> [Bengio and Frasconi, 1996, Bengio and Frasconi, 1995] </ref>. <p> The output sequence distribution is decomposed into conditional emission distributions for individual outputs (given a state and an input at time t) and conditional transition distributions (given a previous state and an input at time t). This is an extension of the already proposed IOHMMs <ref> [Bengio and Frasconi, 1996, Bengio and Frasconi, 1995] </ref> that allows input and output sequences to be asynchronous. The parameters of the model can be estimated with an EM or GEM algorithm (depending of the form of the emission and transition distributions).
Reference: [Dempster et al., 1977] <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: We will show here how an iterative optimization to a local maximum can be achieved using the Expectation-Maximization (EM) algorithm. EM is an iterative procedure to maximum likelihood estimation, originally formalized in <ref> [Dempster et al., 1977] </ref>. Each iteration is composed of two steps: an estimation step and a maximization step. The basic idea is to introduce an additional variable, X, which, if it were known, would greatly simplify the optimization problem. This additional variable is known as the missing or hidden data.
Reference: [Levinson et al., 1983] <author> Levinson, S., Rabiner, L., and Sondhi, M. </author> <year> (1983). </year> <title> An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition. </title> <journal> Bell System Technical Journal, </journal> <volume> 64(4) </volume> <pages> 1035-1074. </pages>
Reference-contexts: This model represents the distribution of an output sequence when given an input sequence of the same length, using a hidden state variable and a Markovian independence assumption, as in Hidden Markov Models (HMMs) <ref> [Levinson et al., 1983, Rabiner, 1989] </ref>, in order to simplify the distribution. IOHMMs are a form of probabilistic transducers [Pereira et al., 1994, Singer, 1996], with input and output variables which can be discrete as well as continuous-valued.
Reference: [Pereira et al., 1994] <author> Pereira, F., Riley, M., and Sproat, R. </author> <year> (1994). </year> <title> Weighted rational transductions and their application to human language processing. </title> <booktitle> In ARPA Natural Language Processing Workshop. </booktitle>
Reference-contexts: IOHMMs are a form of probabilistic transducers <ref> [Pereira et al., 1994, Singer, 1996] </ref>, with input and output variables which can be discrete as well as continuous-valued. However, in many sequential problems where one tries to map an input sequence to an output sequence, the length of the input and output sequences may not be equal.
Reference: [Rabiner, 1989] <author> Rabiner, L. R. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286. </pages>
Reference-contexts: This model represents the distribution of an output sequence when given an input sequence of the same length, using a hidden state variable and a Markovian independence assumption, as in Hidden Markov Models (HMMs) <ref> [Levinson et al., 1983, Rabiner, 1989] </ref>, in order to simplify the distribution. IOHMMs are a form of probabilistic transducers [Pereira et al., 1994, Singer, 1996], with input and output variables which can be discrete as well as continuous-valued.
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. and McClelland, J., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 8, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: 1 Introduction Supervised learning algorithms for sequential data minimize a training criterion that depends on pairs of input and output sequences. It is often assumed that input and output sequences are synchronized, i.e., that each input sequence has the same length as the corresponding output sequence. For instance, recurrent networks <ref> [Rumelhart et al., 1986] </ref> can be used to map input sequences to output sequences, for example minimizing at each time step the squared difference between the actual output and the desired output.
Reference: [Singer, 1996] <author> Singer, Y. </author> <year> (1996). </year> <title> Adaptive mixtures of probabilistic transducers. </title> <editor> In Mozer, M., Touret-zky, D., and Perrone, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: IOHMMs are a form of probabilistic transducers <ref> [Pereira et al., 1994, Singer, 1996] </ref>, with input and output variables which can be discrete as well as continuous-valued. However, in many sequential problems where one tries to map an input sequence to an output sequence, the length of the input and output sequences may not be equal.
References-found: 9

