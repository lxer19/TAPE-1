URL: http://www.cs.iastate.edu/~gannadm/Bib/TR95-01.ps
Refering-URL: http://www.cs.iastate.edu/~gannadm/homepage.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Evolutionary Design of Neural Architectures A Preliminary Taxonomy and Guide to Literature  
Author: Karthik Balakrishnan Vasant Honavar 
Date: January 1995  
Affiliation: Artificial Intelligence Research Group  
Pubnum: CS TR #95-01  
Abstract-found: 0
Intro-found: 1
Reference: [Balakrishnan & Honavar, 1995a] <author> Balakrishnan, K. & Honavar, V. </author> <title> Analysis of Genetic Representations and Operators With Applications in Evolutionary Design of Neural Architectures. </title> <note> In preparation. </note>
Reference-contexts: Other categorizations of encoding schemes are possible including: deterministic versus stochastic, hierarchical versus non-hierarchical, modular versus non-modular, etc. The interested reader is refered to <ref> [Balakrishnan & Honavar, 1995a] </ref> for a characterization of the properties of genetic representations of neural architectures. 3.2 Network Topology or Structure of the Phenotypes It is well known that the success of a neural architecture in solving a particular problem (or class of problems) critically depends on the network topology (or
Reference: [Balakrishnan & Honavar, 1995b] <author> Balakrishnan, K. & Honavar, V. </author> <title> Evolutionary Design of Neural Architectures. </title> <note> In preparation. </note>
Reference-contexts: This report is an attempt to put together a fairly up-to-date bibliography and a tentative taxonomy of current research on the evolutionary design of neural architectures. It does not provide a comprehensive review of the vast amount of research in this area. The reader is referred to <ref> [Weiss, 1990, Yao, 1993, Honavar, 1995, Balakrishnan & Honavar, 1995b] </ref> for such reviews.
Reference: [Fogel et al, 1966] <author> Fogel, L., Owens, A. & Walsh, M. </author> <title> Artificial Intelligence through Simulated Evolution. </title> <address> New York, NY: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: At an abstract level, the neural network design problem is essentially an instance of the program synthesis task not very different from that explored by researchers in artificial intelligence [Uhr, 1963, Uhr, 1973] and genetic programming <ref> [Koza, 1992, Fogel et al, 1966] </ref>. Such techniques offer an attractive (and often, the only) approach to finding solutions to complex problems where neither the shape (detailed structure) nor the size of the solutions is known in advance.
Reference: [Gallant, 1993] <author> Gallant, S. </author> <title> Neural Networks and Expert Systems. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Motivated by this, some researchers have recently begun to investigate constructive or generative neural network learning algorithms that extend the search for the desired input-output mapping to the space of appropriately constrained network topologies by incrementally constructing the required network <ref> [Gallant, 1993, Honavar & Uhr, 1993, Honavar, 1994] </ref>. Against this background, a logical next step is the exploration of more powerful techniques for efficiently searching the space of network architectures. <p> used to classify different approaches to the evolutionary design of neural architectures. 3.3 Variables of Evolution Neural architectures are typically specified in terms of the topology (or connectivity pattern), functions computed by the neurons (e.g., threshold, sigmoid, etc.) and the connection weights (or, a learning algorithm that sets the weights) <ref> [Rumelhart & McClelland, 1986, Kung, 1993, Gallant, 1993, Haykin, 1994] </ref>. A more complete description of a neural architecture requires the specification of coordination and control structures and learning structures (among other things) [Honavar, 1990, Uhr, 1990, Honavar & Uhr, 1990, Honavar, 1994, Honavar, 1994, Honavar, 1995].
Reference: [Goldberg, 1989] <author> Goldberg, D. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Central to such evolutionary systems is the idea of a population of genotypes that are elements of a high dimensional search space. For example, in simple genetic algorithms <ref> [Goldberg, 1989] </ref>, genotypes are binary strings of some fixed length (say n) that code for points in an n-dimensional Boolean search space.
Reference: [Grossberg, 1988] <editor> Grossberg, S. (Ed). </editor> <booktitle> Neural Networks and Natural Intelligence. </booktitle> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: [Haykin, 1994] <author> Haykin, S. </author> <title> Neural Networks. </title> <address> New York, NY: </address> <publisher> Macmillan. </publisher>
Reference-contexts: Even when a suitable setting of parameters can be found using such an approach, the ability of the resulting network to generalize on data not seen during learning <ref> [Kung, 1993, Haykin, 1994] </ref> or the cost of the hardware realization or the cost of using the network (as measured by its size, power consumption etc.) may be far from optimal. These factors make the process of ANN design difficult. <p> used to classify different approaches to the evolutionary design of neural architectures. 3.3 Variables of Evolution Neural architectures are typically specified in terms of the topology (or connectivity pattern), functions computed by the neurons (e.g., threshold, sigmoid, etc.) and the connection weights (or, a learning algorithm that sets the weights) <ref> [Rumelhart & McClelland, 1986, Kung, 1993, Gallant, 1993, Haykin, 1994] </ref>. A more complete description of a neural architecture requires the specification of coordination and control structures and learning structures (among other things) [Honavar, 1990, Uhr, 1990, Honavar & Uhr, 1990, Honavar, 1994, Honavar, 1994, Honavar, 1995].
Reference: [Holland, 1975] <author> Holland, J. </author> <booktitle> Adaptation in Natural and Artificial Systems. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press. </publisher>
Reference: [Honavar, 1990] <author> Honavar, V. </author> <title> Generative Learning Structures for Generalized Connectionist Networks. </title> <type> Ph.D. Dissertation. </type> <institution> University of Wisconsin, Madison, Wiscon-sin. </institution>
Reference: [Honavar, 1994] <author> Honavar, V. </author> <title> Toward Learning Systems That Use Multiple Strategies and Representations. In: Artificial Intelligence and Neural Networks: Steps Toward Principled Integration. Honavar, </title> <editor> V. & Uhr, L. (Ed.). </editor> <address> San Diego, CA: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Motivated by this, some researchers have recently begun to investigate constructive or generative neural network learning algorithms that extend the search for the desired input-output mapping to the space of appropriately constrained network topologies by incrementally constructing the required network <ref> [Gallant, 1993, Honavar & Uhr, 1993, Honavar, 1994] </ref>. Against this background, a logical next step is the exploration of more powerful techniques for efficiently searching the space of network architectures. <p> Artificial neural networks represent a particularly attractive model of computation for a broad range of problems in artificial intelligence and cognitive modeling <ref> [Honavar & Uhr, 1994, Honavar, 1994, Honavar & Uhr, 1995] </ref>.
Reference: [Honavar, 1994] <author> Honavar, V. </author> <title> Symbolic Artificial Intelligence and Numeric Artificial Neural Networks: Toward a Resolution of the Dichotomy. In: Computational Architectures Integrating Symbolic and Neural Processes. Sun, </title> <editor> R. & Bookman, L. (Ed.) </editor> <address> New York: </address> <publisher> Kluwer. </publisher>
Reference-contexts: Motivated by this, some researchers have recently begun to investigate constructive or generative neural network learning algorithms that extend the search for the desired input-output mapping to the space of appropriately constrained network topologies by incrementally constructing the required network <ref> [Gallant, 1993, Honavar & Uhr, 1993, Honavar, 1994] </ref>. Against this background, a logical next step is the exploration of more powerful techniques for efficiently searching the space of network architectures. <p> Artificial neural networks represent a particularly attractive model of computation for a broad range of problems in artificial intelligence and cognitive modeling <ref> [Honavar & Uhr, 1994, Honavar, 1994, Honavar & Uhr, 1995] </ref>.
Reference: [Honavar, 1995] <author> Honavar, V. </author> <title> Evolutionary Design of Neural Architectures. </title> <note> Book in preparation. </note>
Reference-contexts: This report is an attempt to put together a fairly up-to-date bibliography and a tentative taxonomy of current research on the evolutionary design of neural architectures. It does not provide a comprehensive review of the vast amount of research in this area. The reader is referred to <ref> [Weiss, 1990, Yao, 1993, Honavar, 1995, Balakrishnan & Honavar, 1995b] </ref> for such reviews.
Reference: [Honavar, 1995b] <author> Honavar, V. & Uhr, L. </author> <booktitle> Symbolic Artificial Intelligence, Artificial Neural Networks, and Beyond. In: Hybrid Intelligent Systems. </booktitle> <editor> Goonatilake, S. & Khebbal, S. (Ed.) </editor> <publisher> London: Wiley. </publisher> <pages> 12 </pages>
Reference: [Honavar & Uhr, 1989] <author> Honavar, V. & Uhr, L. </author> <title> Brain-Structured Networks That Perceive and Learn. </title> <note> Connection Science 1 139-160. </note>
Reference-contexts: Additional requirements on the structure of the solution to a neural network design problem may be dictated by cost and/or performance considerations in the context of a particular technology. For example, a VLSI designer would prefer locally connected, fault-tolerant, highly modular neural network structures over globally connected, non-modular ones <ref> [Mead, 1989, Uhr, 1984, Honavar & Uhr, 1989, Kung, 1993] </ref>.
Reference: [Honavar & Uhr, 1990] <author> Honavar, V. & Uhr, L. </author> <title> Coordination and Control Structures and Processes: Possibilities for Connectionist Networks. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence 2 277-302. </journal>
Reference: [Honavar & Uhr, 1993] <author> Honavar, V. & Uhr, L. </author> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <journal> Information Sciences (Special Issue on Artificial Intelligence and Neural Networks) 70 75-108. </journal>
Reference-contexts: Clearly, in order for this approach to succeed, the desired setting of parameters must in fact exist within the space being searched (which in turn is constrained by the choice of network topology) and the search algorithm used must in fact be able to find it <ref> [Honavar & Uhr, 1993] </ref>. <p> Motivated by this, some researchers have recently begun to investigate constructive or generative neural network learning algorithms that extend the search for the desired input-output mapping to the space of appropriately constrained network topologies by incrementally constructing the required network <ref> [Gallant, 1993, Honavar & Uhr, 1993, Honavar, 1994] </ref>. Against this background, a logical next step is the exploration of more powerful techniques for efficiently searching the space of network architectures.
Reference: [Honavar & Uhr, 1994] <author> Honavar, V. & Uhr, L. (Ed.). </author> <title> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration. </title> <address> San Diego, CA: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Artificial neural networks represent a particularly attractive model of computation for a broad range of problems in artificial intelligence and cognitive modeling <ref> [Honavar & Uhr, 1994, Honavar, 1994, Honavar & Uhr, 1995] </ref>.
Reference: [Honavar & Uhr, 1995] <author> Honavar, V. & Uhr, L. </author> <title> Symbolic Artificial Intelligence, Connectionist Networks, and Beyond. In: Intelligent Hybrid Systems. </title> <editor> Goonatilake, S. & Khebbal, S. (Ed.) </editor> <publisher> London: Wiley. </publisher>
Reference-contexts: Artificial neural networks represent a particularly attractive model of computation for a broad range of problems in artificial intelligence and cognitive modeling <ref> [Honavar & Uhr, 1994, Honavar, 1994, Honavar & Uhr, 1995] </ref>.
Reference: [Koza, 1992] <editor> Koza, J. </editor> <booktitle> Genetic Programming. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: At an abstract level, the neural network design problem is essentially an instance of the program synthesis task not very different from that explored by researchers in artificial intelligence [Uhr, 1963, Uhr, 1973] and genetic programming <ref> [Koza, 1992, Fogel et al, 1966] </ref>. Such techniques offer an attractive (and often, the only) approach to finding solutions to complex problems where neither the shape (detailed structure) nor the size of the solutions is known in advance.
Reference: [Kung, 1993] <author> Kung, S. </author> <title> Digital Neural Networks. </title> <address> New York, NY: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: Even when a suitable setting of parameters can be found using such an approach, the ability of the resulting network to generalize on data not seen during learning <ref> [Kung, 1993, Haykin, 1994] </ref> or the cost of the hardware realization or the cost of using the network (as measured by its size, power consumption etc.) may be far from optimal. These factors make the process of ANN design difficult. <p> Additional requirements on the structure of the solution to a neural network design problem may be dictated by cost and/or performance considerations in the context of a particular technology. For example, a VLSI designer would prefer locally connected, fault-tolerant, highly modular neural network structures over globally connected, non-modular ones <ref> [Mead, 1989, Uhr, 1984, Honavar & Uhr, 1989, Kung, 1993] </ref>. <p> used to classify different approaches to the evolutionary design of neural architectures. 3.3 Variables of Evolution Neural architectures are typically specified in terms of the topology (or connectivity pattern), functions computed by the neurons (e.g., threshold, sigmoid, etc.) and the connection weights (or, a learning algorithm that sets the weights) <ref> [Rumelhart & McClelland, 1986, Kung, 1993, Gallant, 1993, Haykin, 1994] </ref>. A more complete description of a neural architecture requires the specification of coordination and control structures and learning structures (among other things) [Honavar, 1990, Uhr, 1990, Honavar & Uhr, 1990, Honavar, 1994, Honavar, 1994, Honavar, 1995].
Reference: [McCulloch & Pitts, 1943] <author> McCullcoh, W. S., & Pitts, W. H. </author> <title> A Logical Calculus of the Ideas Immanent in Nervous Activity. </title> <note> Bulletin of Mathematical Biophysics 5 115-133. </note>
Reference-contexts: It is well known that virtually any program can be realized by an artificial neural network (even a rather simple one such as that built from threshold logic units or McCulloch-Pitts neurons <ref> [McCulloch & Pitts, 1943] </ref>). Artificial neural networks represent a particularly attractive model of computation for a broad range of problems in artificial intelligence and cognitive modeling [Honavar & Uhr, 1994, Honavar, 1994, Honavar & Uhr, 1995].
Reference: [Mead, 1989] <author> Mead, C. </author> <title> Analog VLSI and Neural Systems. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Additional requirements on the structure of the solution to a neural network design problem may be dictated by cost and/or performance considerations in the context of a particular technology. For example, a VLSI designer would prefer locally connected, fault-tolerant, highly modular neural network structures over globally connected, non-modular ones <ref> [Mead, 1989, Uhr, 1984, Honavar & Uhr, 1989, Kung, 1993] </ref>.
Reference: [Michalewicz, 1992] <author> Michalewicz, Z. </author> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference: [Miller et al, 1989] <author> Miller, G., Todd, P. & Hegde, S. </author> <title> Designing Neural Networks Using Genetic Algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms 379-384. </booktitle>
Reference-contexts: Our example is drawn from <ref> [Miller et al, 1989] </ref>.
Reference: [Rudnick, 1990] <author> Rudnick, M. </author> <title> A Bibliography: The Intersection of Genetic Search and Artificial Neural Networks. </title> <type> Technical Report CSE-90-001, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Institute. </institution>
Reference-contexts: We have also drawn on earlier bibliographies on this topic <ref> [Rudnick, 1990, Schaffer, 1994] </ref>. We are grateful to National Science Foundation for its support of our research through a grant (IRI-9409580) to Vasant Honavar. 1 GANN: Genetic Algorithms and Neural Networks Mailing List. To subscribe, send e-mail to | gann-request@cs.iastate.edu with "subject" subscribe. 11
Reference: [Rumelhart & McClelland, 1986] <editor> Rumelhart, D. & McClelland, J. (Ed.). </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: used to classify different approaches to the evolutionary design of neural architectures. 3.3 Variables of Evolution Neural architectures are typically specified in terms of the topology (or connectivity pattern), functions computed by the neurons (e.g., threshold, sigmoid, etc.) and the connection weights (or, a learning algorithm that sets the weights) <ref> [Rumelhart & McClelland, 1986, Kung, 1993, Gallant, 1993, Haykin, 1994] </ref>. A more complete description of a neural architecture requires the specification of coordination and control structures and learning structures (among other things) [Honavar, 1990, Uhr, 1990, Honavar & Uhr, 1990, Honavar, 1994, Honavar, 1994, Honavar, 1995].
Reference: [Schaffer, 1994] <institution> ENCORE Bibliography on Combinations of Genetic Algorithms and Neural Networks (COGANN). Maintained at EClair sites. </institution> <month> 13 </month>
Reference-contexts: We have also drawn on earlier bibliographies on this topic <ref> [Rudnick, 1990, Schaffer, 1994] </ref>. We are grateful to National Science Foundation for its support of our research through a grant (IRI-9409580) to Vasant Honavar. 1 GANN: Genetic Algorithms and Neural Networks Mailing List. To subscribe, send e-mail to | gann-request@cs.iastate.edu with "subject" subscribe. 11
Reference: [Uhr, 1963] <author> Uhr, L. & Vossler, C. </author> <title> A Pattern Recognition Program That Generates, Eval--uates, and Adjusts its Own Operators. In: Computers and Thought. </title> <editor> Feigenbaum, E. & Feldman, J. (Ed.) </editor> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: At an abstract level, the neural network design problem is essentially an instance of the program synthesis task not very different from that explored by researchers in artificial intelligence <ref> [Uhr, 1963, Uhr, 1973] </ref> and genetic programming [Koza, 1992, Fogel et al, 1966]. Such techniques offer an attractive (and often, the only) approach to finding solutions to complex problems where neither the shape (detailed structure) nor the size of the solutions is known in advance.
Reference: [Uhr, 1973] <author> Uhr, L. </author> <title> Pattern Recognition, Learning, </title> <booktitle> and Thought. </booktitle> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: At an abstract level, the neural network design problem is essentially an instance of the program synthesis task not very different from that explored by researchers in artificial intelligence <ref> [Uhr, 1963, Uhr, 1973] </ref> and genetic programming [Koza, 1992, Fogel et al, 1966]. Such techniques offer an attractive (and often, the only) approach to finding solutions to complex problems where neither the shape (detailed structure) nor the size of the solutions is known in advance.
Reference: [Uhr, 1984] <author> Uhr, L. </author> <title> Algorithm-Structured Computer Arrays and Networks. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Additional requirements on the structure of the solution to a neural network design problem may be dictated by cost and/or performance considerations in the context of a particular technology. For example, a VLSI designer would prefer locally connected, fault-tolerant, highly modular neural network structures over globally connected, non-modular ones <ref> [Mead, 1989, Uhr, 1984, Honavar & Uhr, 1989, Kung, 1993] </ref>.
Reference: [Uhr, 1990] <author> Uhr, L. </author> <title> Increasing the Power of Connectionist Networks by Improving Structures, Processes and Learning. </title> <note> Connection Science 2 179-193. </note>
Reference: [Weiss, 1990] <author> Weiss, G. </author> <title> Combining Neural and Evolutionary Learning: Aspects and Approaches. </title> <type> Technical Report FKI-132-90, </type> <institution> Technical University of Munich, West Germany. </institution>
Reference-contexts: This report is an attempt to put together a fairly up-to-date bibliography and a tentative taxonomy of current research on the evolutionary design of neural architectures. It does not provide a comprehensive review of the vast amount of research in this area. The reader is referred to <ref> [Weiss, 1990, Yao, 1993, Honavar, 1995, Balakrishnan & Honavar, 1995b] </ref> for such reviews.
Reference: [Yao, 1993] <author> Yao, X. </author> <title> A Review of Evolutionary Artificial Neural Networks. </title> <journal> In International Journal of Intelligent Systems 8(4) 539-567. </journal> <volume> 14 </volume>
Reference-contexts: This report is an attempt to put together a fairly up-to-date bibliography and a tentative taxonomy of current research on the evolutionary design of neural architectures. It does not provide a comprehensive review of the vast amount of research in this area. The reader is referred to <ref> [Weiss, 1990, Yao, 1993, Honavar, 1995, Balakrishnan & Honavar, 1995b] </ref> for such reviews.
References-found: 33

