URL: http://www.cs.rochester.edu/u/si/papers/TCCA.97.ps.gz
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.06/docs-name.html
Root-URL: 
Email: e-mail: cashmere@cs.rochester.edu  
Title: Efficient Use of Memory-Mapped Network Interfaces for Shared Memory Computing  
Author: Nikolaos Hardavellas, Galen C. Hunt, Sotiris Ioannidis, Robert Stets, Sandhya Dwarkadas, Leonidas Kontothanassis, and Michael L. Scott 
Address: Rochester  
Affiliation: Department of Computer Science University of  
Abstract: Memory-mapped network interfaces provide users with fast and cheap access to remote memory on clusters of workstations. Software distributed shared memory (DSM) protocols built on top of these networks can take advantage of fast messaging to improve performance. The low latencies and remote memory access capabilities of these networks suggest the need to re-evaluate the assumptions underlying the design of DSM protocols. This paper describes some of the approaches currently being used to support shared memory efficiently on such networks. We discuss other possible design options for DSM systems on a memory-mapped network interface and propose methods by which the interface can best be used to implement coherent shared memory in software. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Amza, A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: AURC [10] is an interval-based multi-writer protocol designed for the Shrimp network interface [3]. Shasta [21] uses a single-writer directory protocol with in-line protocol operations to support variable-size coherence blocks. Finally, TreadMarks <ref> [1] </ref> uses a multi-writer interval-based protocol to provide DSM on message-passing networks; on a memory-mapped network it uses the extra functionality only for fast messages. These protocols represent only a small subset of choices in a very large design space. <p> Processors kept track of what modifications they had made on a page by making a copy of the page before starting to write it (called twinning), and then comparing the page to its twin (called diffing). TreadMarks <ref> [1] </ref> uses a lazy implementation of release consistency [11], which further limits communication to only those processes that synchronize with one another. Recent advances in network technology have narrowed the gap in communication performance between single-chassis systems and clusters of workstations. <p> Software DSM systems built on top of these very fast networks are an attractive cost-efficient alternative to full hardware coherence. In the rest of this section, we focus on four software DSM systems implemented on a memory mapped network interface. 2.2 Software DSMs on memory mapped network interfaces TreadMarks <ref> [1] </ref> is a distributed shared memory system based on lazy release consistency (LRC) [11]. Lazy release consistency guarantees memory consistency only at synchronization points and permits multiple writers per coherence block. Time on each node is divided into intervals delineated by remote acquire synchronization operations.
Reference: [2] <author> C. Amza, A. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Software DSM Protocols that Adapt between Single Writer and Multiple Writer. </title> <booktitle> In Proc. of the 3rd Intl. Symp. on High Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Requests for updates to migratory data are automatically directed to the modifying processor. This localization of updates comes at the expense of additional computation for diffing and twinning. This overhead can be considerably reduced by combining single and multi-writer protocols <ref> [2] </ref>. 3.4 Update Collection Mechanism Cashmere and AURC avoid the computation overheads of diffing and twinning by writing data through to a home node.
Reference: [3] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proc. of the 21st Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 142-153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Recent technological advances have led to the commercial availability of inexpensive workstation networks on which a processor can access the memory of a remote node safely from user space <ref> [3, 9, 5] </ref>. The This work is supported in part by the National Science Foundation under Grants CDA-9401142, and CCR-9319445. latency of access is two to three orders of magnitude lower than that of traditional message passing. <p> We begin with a summary of existing protocols and implementations. Cashmere [12] employs a directory-based multi-writer protocol that exploits the write-through capabilities of its network in order to merge updates by multiple processors. AURC [10] is an interval-based multi-writer protocol designed for the Shrimp network interface <ref> [3] </ref>. Shasta [21] uses a single-writer directory protocol with in-line protocol operations to support variable-size coherence blocks. Finally, TreadMarks [1] uses a multi-writer interval-based protocol to provide DSM on message-passing networks; on a memory-mapped network it uses the extra functionality only for fast messages. <p> At still lower cost, memory-mapped network interfaces without cache coherence allow messages (typically triggered by ordinary loads and stores) to be sent from user space with microsecond latencies; examples here include the Princeton Shrimp <ref> [3] </ref>, DEC Memory Channel [9], and HP Hamlyn [5] networks. Software DSM systems built on top of these very fast networks are an attractive cost-efficient alternative to full hardware coherence. <p> AURC [10] is a multi-writer protocol designed for the Princeton Shrimp <ref> [3] </ref>. Like TreadMarks, AURC uses distributed information in the form of timestamps and write notices to maintain sharing information. Like Cashmere, it relies on (remote) write-through to merge changes into a home copy of each page.
Reference: [4] <author> W. J. Bolosky, M. L. Scott, R. P. Fitzgerald, R. J. Fowler, and A. L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proc. of the 4th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 212-221, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: The large granularity of the coherence blocks and the sequential consistency memory model used often resulted in page thrashing without real data sharing at the application level. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors <ref> [4, 14] </ref>. Relaxed consistency models result in considerable improvements in DSM performance. Munin [6] was the first DSM system to adopt a release consistency model and to allow multiple processors to concurrently write the same coherence block.
Reference: [5] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Proc. of the 2nd Symp. on Operating Systems Design and Implementation, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Recent technological advances have led to the commercial availability of inexpensive workstation networks on which a processor can access the memory of a remote node safely from user space <ref> [3, 9, 5] </ref>. The This work is supported in part by the National Science Foundation under Grants CDA-9401142, and CCR-9319445. latency of access is two to three orders of magnitude lower than that of traditional message passing. <p> At still lower cost, memory-mapped network interfaces without cache coherence allow messages (typically triggered by ordinary loads and stores) to be sent from user space with microsecond latencies; examples here include the Princeton Shrimp [3], DEC Memory Channel [9], and HP Hamlyn <ref> [5] </ref> networks. Software DSM systems built on top of these very fast networks are an attractive cost-efficient alternative to full hardware coherence.
Reference: [6] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc. of the 13th ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors [4, 14]. Relaxed consistency models result in considerable improvements in DSM performance. Munin <ref> [6] </ref> was the first DSM system to adopt a release consistency model and to allow multiple processors to concurrently write the same coherence block.
Reference: [7] <author> J. B. Carter, A. Davis, R. Kuramkote, and M. Swan-son. </author> <title> The Avalanche Multiprocessor: An Overview. </title> <booktitle> In 6th Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <address> Boston, MA, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Several other academic and commercial projects are developing special-purpose adaptors that extend cache coherence (at somewhat lower performance, but potentially lower cost) across a collection of SMP workstations on a commodity network; these include the Dolphin SCI adaptor [16], the Avalanche project <ref> [7] </ref> at the University of Utah, and the Typhoon project at the University of Wis-consin [20].
Reference: [8] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. L. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proc. of the 17th Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: We restrict ourselves to a discussion of systems that support more-or-less "generic" shared-memory programs, such as might run on a machine with hardware coherence. The memory model presented to the user is release consistency <ref> [8] </ref>, with explicit synchronization operations visible to the run-time system. We begin with a summary of existing protocols and implementations. Cashmere [12] employs a directory-based multi-writer protocol that exploits the write-through capabilities of its network in order to merge updates by multiple processors.
Reference: [9] <author> R. Gillett. </author> <title> Memory Channel: An Optimized Cluster Interconnect. </title> <journal> IEEE Micro, </journal> <volume> 16(2), </volume> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Recent technological advances have led to the commercial availability of inexpensive workstation networks on which a processor can access the memory of a remote node safely from user space <ref> [3, 9, 5] </ref>. The This work is supported in part by the National Science Foundation under Grants CDA-9401142, and CCR-9319445. latency of access is two to three orders of magnitude lower than that of traditional message passing. <p> At still lower cost, memory-mapped network interfaces without cache coherence allow messages (typically triggered by ordinary loads and stores) to be sent from user space with microsecond latencies; examples here include the Princeton Shrimp [3], DEC Memory Channel <ref> [9] </ref>, and HP Hamlyn [5] networks. Software DSM systems built on top of these very fast networks are an attractive cost-efficient alternative to full hardware coherence.
Reference: [10] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li. </author> <title> Improving Release-Consistent Shared Virtual Memory Using Automatic Update. </title> <booktitle> In Proc. of the 2nd Intl. Symp. on High Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: We begin with a summary of existing protocols and implementations. Cashmere [12] employs a directory-based multi-writer protocol that exploits the write-through capabilities of its network in order to merge updates by multiple processors. AURC <ref> [10] </ref> is an interval-based multi-writer protocol designed for the Shrimp network interface [3]. Shasta [21] uses a single-writer directory protocol with in-line protocol operations to support variable-size coherence blocks. <p> AURC <ref> [10] </ref> is a multi-writer protocol designed for the Princeton Shrimp [3]. Like TreadMarks, AURC uses distributed information in the form of timestamps and write notices to maintain sharing information. Like Cashmere, it relies on (remote) write-through to merge changes into a home copy of each page.
Reference: [11] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proc. of the 19th Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Processors kept track of what modifications they had made on a page by making a copy of the page before starting to write it (called twinning), and then comparing the page to its twin (called diffing). TreadMarks [1] uses a lazy implementation of release consistency <ref> [11] </ref>, which further limits communication to only those processes that synchronize with one another. Recent advances in network technology have narrowed the gap in communication performance between single-chassis systems and clusters of workstations. <p> In the rest of this section, we focus on four software DSM systems implemented on a memory mapped network interface. 2.2 Software DSMs on memory mapped network interfaces TreadMarks [1] is a distributed shared memory system based on lazy release consistency (LRC) <ref> [11] </ref>. Lazy release consistency guarantees memory consistency only at synchronization points and permits multiple writers per coherence block. Time on each node is divided into intervals delineated by remote acquire synchronization operations.
Reference: [12] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using Memory-Mapped Network Interfaces to Improve the Performance of Distributed Shared Memory. </title> <booktitle> In Proc. of the 2nd Intl. Symp. on High Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The memory model presented to the user is release consistency [8], with explicit synchronization operations visible to the run-time system. We begin with a summary of existing protocols and implementations. Cashmere <ref> [12] </ref> employs a directory-based multi-writer protocol that exploits the write-through capabilities of its network in order to merge updates by multiple processors. AURC [10] is an interval-based multi-writer protocol designed for the Shrimp network interface [3]. <p> We do not currently use broadcast or remote memory access for either synchronization or protocol data structures, nor do we place shared memory in Memory Channel space. Cashmere <ref> [12] </ref> is a software coherence system expressly designed for memory-mapped network interfaces. It was inspired by Petersen's work on coherence for small-scale, non-hardware-coherent multiprocessors [19]. Cashmere maintains coherence information using a distributed directory data structure.
Reference: [13] <author> L. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S. Parthasarathy, W. Meira, S. Dwarkadas, and M. L. Scott. </author> <title> VM-Based Shared Memory on Low-Latency, Remote-Memory-Access Networks. </title> <booktitle> In Proc. of the 24th Intl. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Small coherence blocks have also been explored in the Blizzard system [22], but with a much less aggressive, sequentially-consistent protocol. 3 Performance Trade-Offs On the whole, the TreadMarks protocol with polling provides the best performance, though the Cashmere protocol comes close in several cases <ref> [13] </ref>. In general, the Cashmere protocol suffers from the overhead of write doubling, thrashing in the L1 cache, and the lack of bandwidth for write-throughs.
Reference: [14] <author> R. P. LaRowe Jr. and C. S. Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: The large granularity of the coherence blocks and the sequential consistency memory model used often resulted in page thrashing without real data sharing at the application level. Several groups employed similar techniques to migrate and replicate pages in early, cache-less shared-memory multiprocessors <ref> [4, 14] </ref>. Relaxed consistency models result in considerable improvements in DSM performance. Munin [6] was the first DSM system to adopt a release consistency model and to allow multiple processors to concurrently write the same coherence block.
Reference: [15] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: DSM systems provide an avenue for affordable, easy-to-use supercomputing for computationally demanding applications in a variety of problem domains. The original idea of using the virtual memory system on top of simple messaging to implement software coherence on networks dates from Kai Li's thesis work on Ivy <ref> [15] </ref>. A host of other systems were built following Li's early work; Nitzberg and Lo [18] provide a survey of early VM-based systems. Many of these systems often exhibited poor performance due to false sharing.
Reference: [16] <author> O. Lysne, S. Gjessing, and K. Lochsen. </author> <title> Running the SCI Protocol over HIC Networks. </title> <booktitle> In Second Intl. Workshop on SCI-based Low-cost/High-performance Computing, </booktitle> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: Several other academic and commercial projects are developing special-purpose adaptors that extend cache coherence (at somewhat lower performance, but potentially lower cost) across a collection of SMP workstations on a commodity network; these include the Dolphin SCI adaptor <ref> [16] </ref>, the Avalanche project [7] at the University of Utah, and the Typhoon project at the University of Wis-consin [20].
Reference: [17] <author> M. Marchetti, L. Kontothanassis, R. Bianchini, and M. L. Scott. </author> <title> Using Simple Page Placement Policies to Reduce the Cost of Cache Fills in Coherent Shared-Memory Systems. </title> <booktitle> In Proc. of the 9th Intl. Parallel Processing Symp., </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Ideally, a page should be placed at the node that accesses it the most. At the very least, it should be placed at a node that accesses it some. Cashmere currently uses a "first touch after initialization" policy, resulting in reasonably good home node placement <ref> [17] </ref>. A fixed choice of home node may lead to poor performance for migratory data in Cashmere and AURC, because of high write-through traffic.
Reference: [18] <author> B. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <journal> Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: The original idea of using the virtual memory system on top of simple messaging to implement software coherence on networks dates from Kai Li's thesis work on Ivy [15]. A host of other systems were built following Li's early work; Nitzberg and Lo <ref> [18] </ref> provide a survey of early VM-based systems. Many of these systems often exhibited poor performance due to false sharing. The large granularity of the coherence blocks and the sequential consistency memory model used often resulted in page thrashing without real data sharing at the application level.
Reference: [19] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proc. of the 7th Intl. Parallel Processing Symp., </booktitle> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: Cashmere [12] is a software coherence system expressly designed for memory-mapped network interfaces. It was inspired by Petersen's work on coherence for small-scale, non-hardware-coherent multiprocessors <ref> [19] </ref>. Cashmere maintains coherence information using a distributed directory data structure. For each shared page in the system, a single directory entry indicates one of three possible page states: un-cached, read-shared, or write-shared.
Reference: [20] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level Shared-Memory. </title> <booktitle> In Proc. of the 21st Intl. Symp. on Computer Architecture, </booktitle> <pages> pp. 325-336, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: developing special-purpose adaptors that extend cache coherence (at somewhat lower performance, but potentially lower cost) across a collection of SMP workstations on a commodity network; these include the Dolphin SCI adaptor [16], the Avalanche project [7] at the University of Utah, and the Typhoon project at the University of Wis-consin <ref> [20] </ref>. At still lower cost, memory-mapped network interfaces without cache coherence allow messages (typically triggered by ordinary loads and stores) to be sent from user space with microsecond latencies; examples here include the Princeton Shrimp [3], DEC Memory Channel [9], and HP Hamlyn [5] networks.
Reference: [21] <author> D. J. Scales, K. Gharachorloo, and C. A. Thekkath. </author> <title> Shasta: A Low Overhead, Software-Only Approach for Supporting Fine-Grain Shared Memory. </title> <booktitle> In Proc. of the 7th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: We begin with a summary of existing protocols and implementations. Cashmere [12] employs a directory-based multi-writer protocol that exploits the write-through capabilities of its network in order to merge updates by multiple processors. AURC [10] is an interval-based multi-writer protocol designed for the Shrimp network interface [3]. Shasta <ref> [21] </ref> uses a single-writer directory protocol with in-line protocol operations to support variable-size coherence blocks. Finally, TreadMarks [1] uses a multi-writer interval-based protocol to provide DSM on message-passing networks; on a memory-mapped network it uses the extra functionality only for fast messages. <p> Experimental results for AURC are currently based on simulation; implementation results await the completion of a large-scale Shrimp testbed. Shasta <ref> [21] </ref>, developed at DEC WRL, employs a single-writer relaxed consistency protocol with variable-size coherence blocks. Like TreadMarks, Shasta uses the Memory Channel only for fast messaging and for an inexpensive implementation of polling for remote requests.
Reference: [22] <author> I. Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proc. of the 6th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 297-306, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: If different data ranges within an application display different access patterns, Shasta can use a different coherence granularity for each, thereby allowing the user to customize the protocol. Small coherence blocks have also been explored in the Blizzard system <ref> [22] </ref>, but with a much less aggressive, sequentially-consistent protocol. 3 Performance Trade-Offs On the whole, the TreadMarks protocol with polling provides the best performance, though the Cashmere protocol comes close in several cases [13]. <p> Of the four systems outlined in Section 2, three|TreadMarks, Cashmere, and AURC|use page faults and therefore maintain coherence at the granularity of pages. In some systems it is possible to generate faults at a finer granularity using the ECC bits in memory <ref> [22] </ref>. Shasta checks a directory in software before shared loads and stores. If the check reveals an inconsistent state the program branches into a software handler that performs the necessary protocol actions.
References-found: 22

