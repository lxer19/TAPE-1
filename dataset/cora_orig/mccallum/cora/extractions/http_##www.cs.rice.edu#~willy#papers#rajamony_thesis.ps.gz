URL: http://www.cs.rice.edu/~willy/papers/rajamony_thesis.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/theses.html
Root-URL: 
Title: Copyright  
Author: Ramakrishnan Rajamony 
Date: 1998  
Abstract-found: 0
Intro-found: 1
Reference: [AALT95] <author> S. P. Amarasinghe, J. M. Anderson, M. S. Lam, and C. W. Tseng. </author> <title> The SUIF compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the 7th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Their base compiler inserts a barrier after each expression evaluation. Hence, programs analyzed by their implementation perform very well compared to the original compiler. Tseng has implemented a barrier elimination algorithm [Tse95] in the SUIF compiler <ref> [AALT95] </ref>. Two optimizations are used to reduce overhead and synchronization. By combining adjacent SPMD regions, the overhead of starting up parallel tasks is reduced. By augmenting dependence analysis with communication analysis [HKT92], the compile-time compute partitioning is taken into account when checking whether a barrier is needed.
Reference: [ACDZ94] <author> S.V. Adve, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Replacing locks by higher-level primitives. </title> <type> Technical Report TR94-237, </type> <institution> Rice University, </institution> <year> 1994. </year>
Reference-contexts: As in Figure 4.6 (a), it may be used to ensure that updates to a data structure are carried out atomically. In this case, the synchronization is used only to ensure atomicity. These are the non-synchronizing atomic (ns-atomic) operators described by Adve et. al. <ref> [ACDZ94] </ref>. The lock and unlock operation of a critical section may also be used to impose an ordering on accesses made outside the critical section. Consider a task queue where processes execute the following actions in a loop (see Figure 4.6 (b)). <p> In contrast, we use her framework to determine excess synchronization or synchronization that is stronger than necessary. Adve et. al. have looked at the benefits of relaxing the strict semantics of the system-provided lock and unlock synchronization constructs in a lazy release consistent system <ref> [ACDZ94] </ref>. They look at three constructs using which a programmer can communicate the intended use of a critical section. Their run-time system then uses this information to implement a weaker and more efficient form of the critical 52 section.
Reference: [Adv93] <author> S.V. Adve. </author> <title> Designing Memory Consistency Models for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <month> Decem-ber </month> <year> 1993. </year>
Reference-contexts: However, 4 most of their algorithms are tailored to detect and handle excess barriers introduced during compilation. y In contrast, we present algorithms to detect and transform barriers, pairwise synchronization, and critical sections. We explain these algorithms using a framework introduced by Adve <ref> [Adv93] </ref> for reasoning about memory models. We also present new algorithms to reduce synchronization and data communication by restructuring computation. These algorithms are based on two new computation transformations we have developed: postponing and relocating computation. Contribution 3 A set of low-overhead techniques to collect run-time information for performance tuning. <p> Several of the algorithms presented in this chapter have been explained using a framework for reasoning about memory models introduced by Adve <ref> [Adv93] </ref>. Adve has used this framework to explore system optimizations that do not violate sequential consistency, based on information supplied by the programmer [Adv96]. In contrast, we use her framework to determine excess synchronization or synchronization that is stronger than necessary. <p> With the exception of the first, these terms were originally introduced and defined by Adve <ref> [Adv93] </ref>. Unless otherwise stated, all definitions are being presented from Adve's work [Adv96]. A.1 Preliminary Definitions Definition 1: Program order: For a given execution, the program text imposes an order on the operations of an individual process. The union of these per-process orders is the program order. <p> exists, then for the set of ordering paths to Y from writes between (by execution order) W 0 and Y , one ordering path that ends in a program order edge is in the critical set (if such a path exists). fl More general forms of synchronization loops are possible <ref> [Adv93, GAG + 92] </ref>. <p> A read-modify-write, as used in Definition 7, is defined as a read followed by a write to the same location; further, the system guarantees that there is no other conflicting write between the read and write of a read-modify-write (as ordered by the execution order) <ref> [Adv93] </ref>. 114 For every execution, we consider one specific critical set, and call the paths in that set as critical paths.
Reference: [Adv96] <author> Sarita V. Adve. </author> <title> Using information from the programmer to implement system optimizations without violating sequential consistency. </title> <type> Technical Report Technical report TR 9603 (Submitted for publication), </type> <institution> Department of Electrical and Computer Engineering, Rice University, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: We also use the term "same synchronization variable" to imply that the locations accessed by two lock or unlock operations are the same. The following discussion makes use of the terms "critical path", "sender", and "receiver". These terms have been defined by Adve <ref> [Adv96] </ref>. The definitions are reproduced in Appendix A. However, for the class of programs that we consider (see Section 4.2), these terms can be explained as follows. release and subsequent acquire of the synchronization variable a. R x and W x denote a read and write to memory location x. <p> Thus, they ensure that the critical sections are not being used to order data accesses that lie outside them. For example, these conditions would exclude critical sections used to update a task queue. These two conditions have been specified by Adve <ref> [Adv96] </ref>. The third condition excludes cases where a value obtained from an 40 1. The lock does not receive for any operations following the critical section. 2. The unlock does not send for any operations preceding the critical section. 3. <p> Informally, the first two conditions state that a ns-atomic critical section must be a "critical section that is used only for atomicity" <ref> [Adv96] </ref>. This is because they ensure that the lock and unlock of the critical sections are not acting as communicators for accesses outside the critical section. <p> Several of the algorithms presented in this chapter have been explained using a framework for reasoning about memory models introduced by Adve [Adv93]. Adve has used this framework to explore system optimizations that do not violate sequential consistency, based on information supplied by the programmer <ref> [Adv96] </ref>. In contrast, we use her framework to determine excess synchronization or synchronization that is stronger than necessary. Adve et. al. have looked at the benefits of relaxing the strict semantics of the system-provided lock and unlock synchronization constructs in a lazy release consistent system [ACDZ94]. <p> With the exception of the first, these terms were originally introduced and defined by Adve [Adv93]. Unless otherwise stated, all definitions are being presented from Adve's work <ref> [Adv96] </ref>. A.1 Preliminary Definitions Definition 1: Program order: For a given execution, the program text imposes an order on the operations of an individual process. The union of these per-process orders is the program order. <p> The first operation on the conflict order edge is called a sender, and the second, a receiver. z y The term candidate critical path has not been defined by Adve. z While Adve defines these for critical paths <ref> [Adv96] </ref>, we define them also for candidate critical paths. 115
Reference: [AH90] <author> S. Adve and M. Hill. </author> <title> Weak ordering: A new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The programming model assumed by our algorithms is sequential consistency [Lam79]. Since we require programs to be free of data races, our algorithms can also be applied to programs written under weaker consistency models such as release consistency [GLL + 90], lazy release consistency [KCZ92], and data-race-free-0 <ref> [AH90] </ref>. In the absence of data races, these weaker models guarantee sequential consistency. It must be noted that the algorithms described in this chapter only reduce synchronization and communication.
Reference: [AH92] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Sufficient conditions for implementing the data-race-free-1 memory model. </title> <type> Technical Report 1107, </type> <institution> University Wisconsin, </institution> <year> 1992. </year>
Reference-contexts: Definition 4: Conflict order ( co ! ): Let X and Y be two memory operations in an execution. X is ordered before Y by conflict order (X co ! Y ) iff X and Y conflict and X executes before Y <ref> [AH92] </ref>.
Reference: [AH93] <author> S.V. Adve and M.D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year> <month> 116 </month>
Reference: [AKPW83] <author> J. R. Allen, Ken Kennedy, Carrie Porterfield, and Joe Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In POPL83, </booktitle> <month> January </month> <year> 1983. </year>
Reference-contexts: This stack grows as the program makes calls, much like the data stack. 73 Our approach has some similarities with "if-conversion", developed by Allen et. al. for vectorizing loops with control flow <ref> [AKPW83] </ref>. The difference is that instead of explicitly converting control dependences into data dependences statically (as Allen et. al. do), we do the conversion dynamically, as required for a particular execution.
Reference: [AL90] <author> T. E. Anderson and E. D. Lazowska. Quartz: </author> <title> A tool for tuning parallel program performance. </title> <booktitle> In Proceedings of the International Conference on Measurement and Modeling of Computer Systems (Sigmetrics '90), </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Other performance debuggers, such as the ones listed below, perform at least a partial analysis of run-time information in order to detect performance problems. Quartz <ref> [AL90] </ref> uses the normalized processor time metric to rank the contribution of individual procedures in the program to the overall execution. This metric normalizes the execution time of a procedure to the concurrent parallelism present during its execution.
Reference: [AMCA + 95] <author> V. S. Adve, J. Mellor-Crummey, M. Anderson, K. Kennedy, J. Wang, and D. Reed. </author> <title> An integrated compilation and performance analysis environment for data parallel programs. </title> <booktitle> In Proceedings Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The user has to interpret this data. The issue of correlating low-level information with the source program has been examined for the parallelizing compiler environment. For instance, Adve et al. describe an integrated compilation and performance analysis environment that provides source-level performance analysis of data-parallel programs <ref> [AMCA + 95] </ref>. Their compiler provides information to the run-time system to enable it to correlate execution statistics with the source code. In contrast, our focus is on prescribing source-level transformations that address performance problems. In contrast to the tools and methods outlined above, our goal is very different.
Reference: [Bau92] <author> Barr E. Bauer. </author> <title> Practical parallel programming. </title> <publisher> Academic Press, </publisher> <address> San Diego, California, </address> <year> 1992. </year>
Reference-contexts: As a result, it can determine that the access to ptra need not ever be protected by the critical section. This form of programmer involvement is not new. For instance, the SGI Power compiler provides pragmas based on which the compiler parallelizes loops <ref> [Bau92] </ref>. 22 Sun Microsystem's lock_lint tool [Sun95] uses programmer annotations extensively to check for data races and improperly used locks in multithreaded programs. Static debugging (see Bourdoncle's work [Bou93] for example) also has a history of using programmer supplied invariants.
Reference: [BBLS93] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report 103863, </type> <institution> NASA, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Barnes-Hut, Water, and Gauss are benchmark programs. The first two are from the SPLASH-2 suite [WOT + 95], while the third is locally derived. Conjugate Gradient and Multigrid were written by first year graduate students in a parallel programming course, using specifications from the NAS benchmarking document <ref> [BBLS93] </ref>. Shallow, also based on the NAS specifications, was provided to us by a source from industry, with a specific request to improve its performance. The applications range from fine-grained programs using locks, to coarse-grained programs that use only barriers. <p> Each process computes a portion of the result vector, for which it needs the entire input vector. These two steps are used in different phases of the program. The phases are separated by barriers. This application was developed using the NAS parallel benchmark specifications <ref> [BBLS93] </ref>, as part of a Rice Comp520 course project [LNW94]. R x detected three instances of excess synchronization in this program. First, in the dot-product step, it detected an unnecessary critical section executed only by process 0 to clear the location where the dot product would be accumulated. <p> During this phase, the boundary elements of the grids are copied to facilitate sharing between the processes. Synchronization is achieved within this phase using barriers. This application was developed using the NAS parallel benchmark specifications <ref> [BBLS93] </ref>, as part of a Rice Comp520 course project [LNW94]. R x detected two instances of excess synchronization in this program. First, it found a barrier that does not enforce any dependence, permitting it to be removed.
Reference: [Ber66] <author> A.J. Bernstein. </author> <title> Analysis of programs for parallel processing. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <pages> pages 757-763, </pages> <month> October </month> <year> 1966. </year>
Reference-contexts: These are an adaptation of Bernstein's conditions for determining whether the concurrent execution of a set of tasks is determinate <ref> [Ber66] </ref>. 4.4.1 Barriers The beginning and end of the program execution, and the barriers in between, divide a program into what we call b-intervals.
Reference: [Bix92] <author> R. Bixby. </author> <title> Implementing the simplex method: The initial basis. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4(3), </volume> <year> 1992. </year>
Reference-contexts: An optimal solution can be found by formulating this as an integer programming problem and solving it using one of the many solvers (such as CPLEX <ref> [Bix92] </ref>) that are currently available. The second step is to correlate this feedback with the source program, so that it can be provided in terms of the data structure names used by the programmer. <p> An optimal solution can be found by formulating this as an integer programming problem and solving it using one of the many solvers (such as CPLEX <ref> [Bix92] </ref>) that are currently available. The next step is to correlate this feedback with the source program, so that it can be provided in terms of the data structure names used by the programmer.
Reference: [Bou93] <author> Fran~cois Bourdoncle. </author> <title> Abstract debugging of higher-order imperative languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 46-55, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: For instance, the SGI Power compiler provides pragmas based on which the compiler parallelizes loops [Bau92]. 22 Sun Microsystem's lock_lint tool [Sun95] uses programmer annotations extensively to check for data races and improperly used locks in multithreaded programs. Static debugging (see Bourdoncle's work <ref> [Bou93] </ref> for example) also has a history of using programmer supplied invariants. When the supplied information is insufficient for the debugger to infer the correctness of the transformations, we proceed in the second direction.
Reference: [BPR96] <author> D. Badouel, Thierry Priol, and Luc Renambot. SVMview: </author> <title> A performance tuning tool for DSM-based parallel computers. </title> <booktitle> In Euro-Par'96, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Thus, it can be used easily by inexperienced parallel programmers. 1.2 Contributions Contribution 1 A new approach for designing performance tools, en abling the prescription of source-level changes to improve performance. Tools currently used by programmers to improve application performance suffer from two problems. First, existing tools <ref> [BPR96, RAN + 93] </ref> only describe performance information gathered at run-time, placing the onus on the user to deduce the cause for the performance problems. This requires users to understand how the program is mapped onto the parallel machine and also forces them to reason about machine-level execution statistics. <p> Typically, these tools provide feedback about the program execution, such as the time spent in different routines, and the factors that slow down progress, such as cache misses. However, the nature of this feedback is far from satisfactory. First, existing tools only describe the performance information gathered at run-time <ref> [BPR96, RAN + 93] </ref>. For example, a performance tool applied to a program that suffers from poor cache behavior could say: 70% of all cache misses occur in function foo () The onus is then on the user to infer the cause for the performance problems.
Reference: [CFR + 91] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year> <month> 117 </month>
Reference-contexts: Moreover, all accesses made inside doit (), and those inside the functions doit () might call, depend on v too. The first step in converting control to data dependences is to compute the control flow graph (CFG) <ref> [CFR + 91] </ref> of each function in the program. This is a directed graph with nodes representing basic blocks, and edges, the possible flow of control. We use the assembly output to compute this graph. <p> We use the assembly output to compute this graph. Next, we use the graph to com 70 x = y + z doit (t) g pute the immediate postdominator of each basic block in a function. The immediate postdominator is defined as follows <ref> [CFR + 91] </ref>: If X and Y are CFG nodes, and X appears on every path from Y to the exit node, X postdominates Y . If X postdominates Y , but X 6= Y , then X strictly postdominates Y .
Reference: [Col92] <author> W.W. Collier. </author> <title> Reasoning about Parallel Architectures. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Definition 3: Execution order: With atomic memory, there is a total order on all memory operations of an execution, such that a read returns the value of the last write to the same location in this total order. This order is called the execution order <ref> [Col92] </ref>. We say an operation O 1 executes before (or after) an operation O 2 if O 1 is before (or after) O 2 by execution order. Definition 4: Conflict order ( co ! ): Let X and Y be two memory operations in an execution.
Reference: [CT91] <author> K. M. Chandy and S. Taylor. </author> <title> Introduction to Parallel Programming. </title> <publisher> Jones & Bartlett, </publisher> <year> 1991. </year>
Reference-contexts: The minimum number of synchronization barriers required is given by the maximum number of dependences encountered along any path in the graph. For nodes that are not on the maximal path, there is some freedom in terms of where the corresponding x A definition variable <ref> [CT91] </ref> (DV) is a data structure associated with a queue. Requests are queued until the data structure is initialized, and are then satisfied.
Reference: [DBKF90] <author> J. Dongarra, O. Brewer, J. A. Kohl, and S. Fineberg. </author> <title> A tool to aid in the design, implementation, and understanding of matrix algorithms for parallel programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: A prescriptive tool can be developed only if this process can be automated. 2.4 Related Work Techniques for run-time performance tuning focus primarily on two aspects. A number of performance tools concentrate on the presentation of collected infor 12 mation using differently formatted displays <ref> [LSV + 89, DBKF90, RAN + 93] </ref>, animation [FLK + 91, PU89], and auralization [FJ93]. These tools perform none or very little analysis of the data and leave the reasoning required to detect the performance problem and its solution entirely to the programmer.
Reference: [DR96] <author> Pedro Diniz and Martin Rinard. </author> <title> Lock coarsening: Eliminating lock overhead in automatically parallelized object-based programs. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <volume> volume 1239, </volume> <pages> pages 285-299, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Consequently, scalar optimizations may be applied around synchronization primitives. Diniz and Rinard present three transformations that allow a compiler to decrease the overhead of critical sections used for mutual exclusion in object-based programs <ref> [DR96, DR97] </ref>. Lock cancellation eliminates acquires and releases that access the same lock, and have no intervening computation. Lock movement moves acquire and release nodes across computation. Together, these two transformation permit lock elimination, which coarsens a critical section by increasing the computation performed inside it.
Reference: [DR97] <author> Pedro Diniz and Martin Rinard. </author> <title> Synchronization transformations for parallel computing. </title> <booktitle> In POPL97, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: We provide three examples to illustrate how this approach can be used to develop prescriptive tools. Chapter 3 then introduces R x , a prescriptive tool for reducing synchronization y The only exception to this that we are aware of is the work by Diniz and Rinard <ref> [DR97] </ref>, which we describe in Chapter 4. 5 and data communication in shared-memory parallel programs. R x demonstrates our prescriptive approach to performance tuning. Chapter 4 describes a set of algorithms for detecting excess synchronization in shared-memory programs. <p> Consequently, scalar optimizations may be applied around synchronization primitives. Diniz and Rinard present three transformations that allow a compiler to decrease the overhead of critical sections used for mutual exclusion in object-based programs <ref> [DR96, DR97] </ref>. Lock cancellation eliminates acquires and releases that access the same lock, and have no intervening computation. Lock movement moves acquire and release nodes across computation. Together, these two transformation permit lock elimination, which coarsens a critical section by increasing the computation performed inside it. <p> If applied to such a program, the transformations implemented by Diniz and Rinard will violate program correctness. In contrast, our algorithms do not restrict the synchronization semantics. Second, their compile-time analysis is able to arrive at the critical section transformations primarily because the programs being considered are object-based <ref> [DR97] </ref>. It is unknown how much the efficacy of this approach will be affected when applied to non-object-based programs, for which the static analysis is likely to be less precise.
Reference: [FJ93] <author> Joan M. Francioni and Jay A. Jackson. </author> <title> Breaking the silence: Auraliza-tion of parallel program behavior. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(2), </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: A number of performance tools concentrate on the presentation of collected infor 12 mation using differently formatted displays [LSV + 89, DBKF90, RAN + 93], animation [FLK + 91, PU89], and auralization <ref> [FJ93] </ref>. These tools perform none or very little analysis of the data and leave the reasoning required to detect the performance problem and its solution entirely to the programmer.
Reference: [FLK + 91] <author> Mark Friedell, Mark LaPolla, Sandeep Kochhar, Steve Sistare, and Janusz Juda. </author> <title> Visualizing the behavior of massively parallel programs. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: A number of performance tools concentrate on the presentation of collected infor 12 mation using differently formatted displays [LSV + 89, DBKF90, RAN + 93], animation <ref> [FLK + 91, PU89] </ref>, and auralization [FJ93]. These tools perform none or very little analysis of the data and leave the reasoning required to detect the performance problem and its solution entirely to the programmer.
Reference: [GAG + 92] <author> K. Gharachorloo, S. V. Adve, A. Gupta, J. L. Hennessy, and M. D. Hill. </author> <title> Programming for different memory consistency models. </title> <journal> J. Para. and Dist. Comp., </journal> <volume> 15(4) </volume> <pages> 339-407, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Definition 7: A synchronization loop <ref> [GAG + 92] </ref> is a sequence of instructions that satisfies the following. (i) The sequence executes one or more reads, called exit reads. <p> Other operations are essential <ref> [GAG + 92] </ref>. Definition 9: Two conflicting operations in an execution, X and Y , are called consecutive iff there is no ordering path between X and Y that has another write on it that conflicts with X and Y . <p> exists, then for the set of ordering paths to Y from writes between (by execution order) W 0 and Y , one ordering path that ends in a program order edge is in the critical set (if such a path exists). fl More general forms of synchronization loops are possible <ref> [Adv93, GAG + 92] </ref>.
Reference: [GH93] <author> Aaron Golberg and John Hennessy. </author> <title> MTOOL: An integrated system for performance debugging shared memory multiprocessor applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 28-40, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Changing this layout can reduce conflict misses and also increase implicit prefetching by improving spatial locality. Consequently, the application performance can be improved. Existing tools that target memory performance <ref> [GH93, LW94, MGA95, SB94] </ref> present only descriptive information, such as cache statistics, to the user. In contrast, a prescriptive tool can treat this as an optimization problem where the goal is to optimally arrange the program data structures, and directly specify the best layout to use. <p> The problem with this approach is that performance bottlenecks cannot often be attributed to any one point in the code. For instance, the performance degradation might be caused by accesses to a particular data structure whose references are distributed through the program. Mtool <ref> [GH93] </ref> analyzes memory and synchronization bottlenecks in parallel programs. Basic blocks and loops are instrumented to collect timing information. The difference between the actual time spent in a code section and that spent when the memory subsystem imposes no delays is reported as the memory overhead of that section. <p> Changing this layout can reduce 110 conflict misses and also increase implicit prefetching by improving spatial locality. Consequently, the application performance can be improved. Existing tools that target memory performance <ref> [GH93, LW94, MGA95, SB94] </ref> present only descriptive information, such as cache statistics, to the user. In contrast, a tool based on our approach can directly specify the best layout to use.
Reference: [GJ79] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractibil-ity: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year> <month> 118 </month>
Reference-contexts: O'Boyle and Bodin describe a similar approach for reducing the number of barriers when compiling into SPMD code [OB95]. Once loop and data partitioning have been completed, they use dependence analysis to insert the minimal barriers required to protect inter-processor dependences. This translates into the maximal cut problem <ref> [GJ79] </ref>; they hence use a heuristic solution to insert the barriers. They also examine whether barriers within loops can be replaced with wait/post synchronization. They compare their compiler implementation against KAP and provide results for two kernels on a 32-processor KSR.
Reference: [GLL + 90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The programming model assumed by our algorithms is sequential consistency [Lam79]. Since we require programs to be free of data races, our algorithms can also be applied to programs written under weaker consistency models such as release consistency <ref> [GLL + 90] </ref>, lazy release consistency [KCZ92], and data-race-free-0 [AH90]. In the absence of data races, these weaker models guarantee sequential consistency. It must be noted that the algorithms described in this chapter only reduce synchronization and communication. <p> Between subgraphs, synchronization operations are used to create directed edges. An edge is drawn from the release of a synchronization variable to its subsequent acquire, as observed during the execution. A release is a synchronization write and an acquire, a synchronization read <ref> [GLL + 90] </ref>. The synchronization operations divide the execution on a process into intervals. Let S0 and S1 denote two synchronization operations on one process. Then we denote the interval between them by (S0; S1 ). This graph enables us to evaluate the happens-before-1 relation ( hb1 - )[AH93].
Reference: [Gol93] <author> S.R. Goldschmidt. </author> <title> Simulation of multiprocessors, speed and accuracy. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: MemSpy [MGA95] also presents data-oriented 13 statistics, for tuning the memory performance of parallel programs. At compile-time, instrumentation is added into the program which calls an event simulator at interesting events. The program is then simulated using Tango Lite <ref> [Gol93] </ref>, an address reference generator. MemSpy measures three types of cache misses viz. replacement misses (typically caused by cache interference), first-reference misses (typically caused by poor spatial locality) and invalidation misses (potentially caused by false sharing). It then correlates this information to code and data structures in the program.
Reference: [Gup89] <author> R. Gupta. </author> <title> The fuzzy barrier: A mechanism for high speed synchronization of processors. </title> <booktitle> Proceedings of the 3rd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 54-63, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The workers then use this new partition in the next phase of the computation. This also corresponds to cases 4 and 5 in Table 4.1. Using pairwise synchronization as explained above provides the same benefits as using a fuzzy barrier <ref> [Gup89] </ref>. In a fuzzy barrier, the barrier releases and acquires are separated, permitting computation to be carried out between the two. Finally, when a barrier is present only to enforce anti-dependences, it can be replaced with a weaker form of synchronization that does not make the shared address space consistent.
Reference: [HKT92] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Tseng has implemented a barrier elimination algorithm [Tse95] in the SUIF compiler [AALT95]. Two optimizations are used to reduce overhead and synchronization. By combining adjacent SPMD regions, the overhead of starting up parallel tasks is reduced. By augmenting dependence analysis with communication analysis <ref> [HKT92] </ref>, the compile-time compute partitioning is taken into account when checking whether a barrier is needed. In addition, the communication that needs to occur at a barrier (obtained using communication analysis) is matched against some simple patterns corresponding to nearest neighbor, scatter or gather operations.
Reference: [KCZ92] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The programming model assumed by our algorithms is sequential consistency [Lam79]. Since we require programs to be free of data races, our algorithms can also be applied to programs written under weaker consistency models such as release consistency [GLL + 90], lazy release consistency <ref> [KCZ92] </ref>, and data-race-free-0 [AH90]. In the absence of data races, these weaker models guarantee sequential consistency. It must be noted that the algorithms described in this chapter only reduce synchronization and communication.
Reference: [KDCZ94] <author> P. Keleher, S. Dwarkadas, A. L. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Hence, the barrier can be replaced by a special synchronization construct that ensures that all reads preceding it (by program order) will be performed before the synchronization access will be performed. We call this construct a control barrier. Software DSM systems such as TreadMarks <ref> [KDCZ94] </ref>, which make the address space consistent lazily, can take advantage of control barriers. <p> Hence, for these applications, the instrumentation did not distort the synchronization order. 76 5.8 Run-time System After instrumenting the assembled source program, we compile it and link with the mirror functions. Our run-time system is the TreadMarks software distributed shared memory system <ref> [KDCZ94] </ref>. TreadMarks provides a shared memory abstraction on distributed memory (message-passing) machines. In our implementation, we limit the number of [process,interval] tuples used for recording operand information. When a location depends on more [process,interval] tuples than we can accommodate, we discard the oldest interval and keep the rest. <p> Our hardware shared memory environment consists of an 8-processor Sun Enterprise server on which we use the Posix threads (pthreads) package. The software shared memory platform consists of an 8-processor IBM SP2 machine running TreadMarks <ref> [KDCZ94] </ref>. We first discuss the impact of R x feedback when the optimized applications were executed on the hardware shared memory system. For Gauss, the debugger findings had no impact on performance. R x feedback slightly improved the performance of 89 Shallow and Multigrid.
Reference: [KY95] <author> Arvind Krishnamurthy and Katherine Yelick. </author> <title> Optimizing parallel programs with explicit synchronization. </title> <booktitle> In Proceedings of the ACM SIG-PLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: As we explain in the next chapter, by basing our analysis on run-time information, our approach can provide feedback on how to reduce synchronization and communication in complex, pointer-ridden applications. Krishnamurthy and Yelick present compiler optimizations for explicitly parallel shared-memory programs <ref> [KY95] </ref>. In such programs, code motion optimizations require consideration of inter-process dependences. In order to guarantee sequential consistency after reordering transformations have been applied, analysis introduced by Shasha and Snir known as cycle detection [SS88] is required.
Reference: [Lam79] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Data races may cause the behavior of a program to be time-dependent. Their presence may therefore cause programs to execute 27 incorrectly. Therefore, it is not unreasonable to require that the program being analyzed be free of data races. The programming model assumed by our algorithms is sequential consistency <ref> [Lam79] </ref>. Since we require programs to be free of data races, our algorithms can also be applied to programs written under weaker consistency models such as release consistency [GLL + 90], lazy release consistency [KCZ92], and data-race-free-0 [AH90]. In the absence of data races, these weaker models guarantee sequential consistency. <p> Depending on whether the value returned by each read belongs to one of specified values, called exit values, the construct either terminates or repeats the above. (ii) The loop terminates in every sequentially consistent <ref> [Lam79] </ref> execution.
Reference: [Lar90] <author> J. R. Larus. </author> <title> Abstract execution: A technique for efficiently tracing programs. </title> <journal> Software Practices and Experience, </journal> 20(12) 1241-1258, December 1990. <volume> 119 </volume>
Reference-contexts: Instrumentation that we insert at the entry and exit of every function ensures that the mirror functions operate on the proper register set. 75 Using mirror functions to carry out the run-time processing shares some similarities with abstract execution <ref> [Lar90] </ref>, which expands a small set of events recorded at run-time into a full trace. However, while our mirror functions can be considered abstract versions of the original basic blocks, we execute them in conjunction with the original program to efficiently compute operand information.
Reference: [LAS85] <author> Zhiyuan Li and Walid Abu-Sufah. </author> <title> A technique for reducing sunchro-nization overhead in large scale multiprocessors. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 284-291, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: Data dependence analysis [PW86] forms the core of all compile-time data transformations. Li and Abu-sufah have presented a technique to remove redundant synchronization interlocks (for instance, enforced with a post and wait condition) from doacross loops <ref> [LAS85] </ref>. An interlock is required in the presence of loop-carried flow- and anti-dependences. They detect simple cases where synchronization inserted to satisfy a dependence satisfies another as well, and implement their technique as a pass in the Parafrase source-to-source restructurer.
Reference: [LNW94] <author> Honghui Lu, Nenad Nedeljkovic, and Edmar Wienskoski. </author> <title> Performance comparison of the treadmarks distributed shared memory system and the parallel virtual machine. </title> <institution> Rice COMP520 Course Project Report, </institution> <year> 1994. </year>
Reference-contexts: These two steps are used in different phases of the program. The phases are separated by barriers. This application was developed using the NAS parallel benchmark specifications [BBLS93], as part of a Rice Comp520 course project <ref> [LNW94] </ref>. R x detected three instances of excess synchronization in this program. First, in the dot-product step, it detected an unnecessary critical section executed only by process 0 to clear the location where the dot product would be accumulated. <p> During this phase, the boundary elements of the grids are copied to facilitate sharing between the processes. Synchronization is achieved within this phase using barriers. This application was developed using the NAS parallel benchmark specifications [BBLS93], as part of a Rice Comp520 course project <ref> [LNW94] </ref>. R x detected two instances of excess synchronization in this program. First, it found a barrier that does not enforce any dependence, permitting it to be removed. In addition, it found two barriers invoked in the global-communicate phase to be stronger than necessary.
Reference: [LS95] <author> J. R. Larus and E Schnarr. EEL: </author> <title> Machine-indepent executable editing. </title> <booktitle> In Proceedings of the ACM SIGPLAN 95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Although we could have used an instrumentation system such as ATOM [SE94] for collecting the per-basic block address trace, or EEL <ref> [LS95] </ref> which also provides control flow information, we would need to extend them to convert control to data dependences and to examine the basic blocks to obtain the operand information.
Reference: [LSV + 89] <author> T. Lehr, Z. Segall, D. F. Vraslovic, E. Caplan, A. L. Chung, and C. E. Fineman. </author> <title> Visualizing performance debugging. </title> <journal> IEEE Computer, </journal> <volume> 22(10), </volume> <month> October </month> <year> 1989. </year>
Reference-contexts: A prescriptive tool can be developed only if this process can be automated. 2.4 Related Work Techniques for run-time performance tuning focus primarily on two aspects. A number of performance tools concentrate on the presentation of collected infor 12 mation using differently formatted displays <ref> [LSV + 89, DBKF90, RAN + 93] </ref>, animation [FLK + 91, PU89], and auralization [FJ93]. These tools perform none or very little analysis of the data and leave the reasoning required to detect the performance problem and its solution entirely to the programmer.
Reference: [LW94] <author> A. R. Lebeck and D. A. Wood. </author> <title> Cache profiling and the SPEC benchmarks: a case study. </title> <journal> IEEE Computer, </journal> <volume> 27(10), </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: Changing this layout can reduce conflict misses and also increase implicit prefetching by improving spatial locality. Consequently, the application performance can be improved. Existing tools that target memory performance <ref> [GH93, LW94, MGA95, SB94] </ref> present only descriptive information, such as cache statistics, to the user. In contrast, a prescriptive tool can treat this as an optimization problem where the goal is to optimally arrange the program data structures, and directly specify the best layout to use. <p> Mtool's main drawback is that it provides only descriptive feedback. In addition, the level of detail at which information is provided does not easily explain the performance problems. Cprof <ref> [LW94] </ref> is a cache profiling system for sequential programs that presents statistics in terms of both code and data structures. It categorizes cache misses obtained from a simulation of the program into compulsory, capacity or conflict misses and presents them the programmer. <p> Changing this layout can reduce 110 conflict misses and also increase implicit prefetching by improving spatial locality. Consequently, the application performance can be improved. Existing tools that target memory performance <ref> [GH93, LW94, MGA95, SB94] </ref> present only descriptive information, such as cache statistics, to the user. In contrast, a tool based on our approach can directly specify the best layout to use.
Reference: [MCC + 95] <author> B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, K. Kunchithapadam, and T. Newhall. </author> <title> The para-dyn parallel performance measurement tools. </title> <journal> IEEE Computer, </journal> <volume> 28(11), </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: This metric normalizes the execution time of a procedure to the concurrent parallelism present during its execution. A listing of the "importance" of different procedures to the execution is then produced. The programmer uses this listing a la gprof for performance tuning. Paradyn <ref> [MCC + 95] </ref> is a performance measurement tool that dynamically instruments the program being traced, and searches the execution for bottlenecks. Paradyn uses the time spent in various operations, such as synchronization or the time wasted blocking on message receives, as a metric.
Reference: [Mes95] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard, </title> <note> version 1.1, </note> <month> June </month> <year> 1995. </year>
Reference-contexts: With the increasing complexity of message-passing interfaces, the task of choosing the appropriate communication primitive is not easy. For instance, MPI <ref> [Mes95] </ref>, a widely implemented message-passing interface standard, specifies around 100 primitives for communication, including several complex ones for group communication. The large selection of communication primitives makes it difficult for a novice programmer to choose the best primitive from the communication library. <p> Optimizing Message-passing Programs Our approach can also be used to design tools that facilitate the development and porting of message passing programs. With the increasing complexity of message-passing interfaces, the task of choosing the appropriate communication primitive is 111 not easy. For instance, MPI <ref> [Mes95] </ref>, a widely implemented message-passing interface standard, specifies around 100 primitives for communication, including several complex ones for group communication. The large selection of communication primitives makes it difficult for a novice programmer to choose the best primitive from the communication library.
Reference: [MGA95] <author> M. Martonosi, A. Gupta, and T. E. Anderson. </author> <title> Tuning memory performance of sequential and parallel programs. </title> <journal> IEEE Computer, </journal> <volume> 28(4), </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: Changing this layout can reduce conflict misses and also increase implicit prefetching by improving spatial locality. Consequently, the application performance can be improved. Existing tools that target memory performance <ref> [GH93, LW94, MGA95, SB94] </ref> present only descriptive information, such as cache statistics, to the user. In contrast, a prescriptive tool can treat this as an optimization problem where the goal is to optimally arrange the program data structures, and directly specify the best layout to use. <p> Cprof [LW94] is a cache profiling system for sequential programs that presents statistics in terms of both code and data structures. It categorizes cache misses obtained from a simulation of the program into compulsory, capacity or conflict misses and presents them the programmer. MemSpy <ref> [MGA95] </ref> also presents data-oriented 13 statistics, for tuning the memory performance of parallel programs. At compile-time, instrumentation is added into the program which calls an event simulator at interesting events. The program is then simulated using Tango Lite [Gol93], an address reference generator. <p> Changing this layout can reduce 110 conflict misses and also increase implicit prefetching by improving spatial locality. Consequently, the application performance can be improved. Existing tools that target memory performance <ref> [GH93, LW94, MGA95, SB94] </ref> present only descriptive information, such as cache statistics, to the user. In contrast, a tool based on our approach can directly specify the best layout to use.
Reference: [MP87] <author> S.P. Midkiff and D.A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1485-1495, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: They detect simple cases where synchronization inserted to satisfy a dependence satisfies another as well, and implement their technique as a pass in the Parafrase source-to-source restructurer. Midkiff and Padua extend this to eliminate redundant synchronization operations from loops with arbitrary control flow <ref> [MP87] </ref>. Both algorithms can be applied to reduce the synchronization added when paral-lelizing loops. However, we have shown that the synchronization added by these algorithms is not optimal [RC97a].
Reference: [Net91] <author> Robert H. B. Netzer. </author> <title> Race condition detection for debugging shared-memory parallel programs. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madi-son, </institution> <month> August </month> <year> 1991. </year> <month> 120 </month>
Reference-contexts: In a shared-memory program, if conflicting accesses (read-write or write-write accesses to the same memory location) [SS88] are not ordered by synchronization, time-dependent bugs may be introduced. Consequently, the program behavior can be unexpected or even incorrect. Faced with the possibility of these race conditions <ref> [Net91] </ref>, most inexperienced parallel programmers add more synchronization than necessary to their applications. By over-synchronizing, the programmer can often ensure that the application is free of data races without an in-depth understanding of its sharing pattern. <p> In the presence of these race conditions <ref> [Net91] </ref>, the program behavior can be unexpected or even incorrect. Faced with this possibility, most inexperienced parallel programmers add more synchronization than necessary to their applications. This is especially true when the programmer is not writing the application from scratch, but is instead parallelizing or porting it. <p> Synchronization is used for this purpose. Specifically, conflicting accesses (read-write or write-write accesses to the same memory location) [SS88] need to be ordered using synchronization. If this ordering is not enforced, time-dependent bugs known as race conditions <ref> [Net91] </ref> may be introduced into the program. Consequently, the program behavior can be unexpected or even incorrect. By using synchronization to order accesses, the program can be made to behave as expected.
Reference: [OB95] <author> M. O'Boyle and Francois Bodin. </author> <title> Compiler reduction of synchronization in shared virtual memory systems. </title> <booktitle> In Proceedings of the 1995 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: We present a set of algorithms to detect excess synchronization ranging from pairwise synchronization constructs such as flags, to global synchronization constructs such as barriers. The parallelizing compiler community has been addressing the problem of reducing synchronization in parallel programs <ref> [OB95, PH95, Tse95] </ref>. However, 4 most of their algorithms are tailored to detect and handle excess barriers introduced during compilation. y In contrast, we present algorithms to detect and transform barriers, pairwise synchronization, and critical sections. <p> The barrier is replaced by counters or flags in this case. The performance of the implementation is compared against that of the original compiler; modest improvements are reported. O'Boyle and Bodin describe a similar approach for reducing the number of barriers when compiling into SPMD code <ref> [OB95] </ref>. Once loop and data partitioning have been completed, they use dependence analysis to insert the minimal barriers required to protect inter-processor dependences. This translates into the maximal cut problem [GJ79]; they hence use a heuristic solution to insert the barriers.
Reference: [PDB93] <author> S. Prakash, M. Dhagat, and R. Bagrodia. </author> <title> Synchronization issues in dat-aparallel languages. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Quinn, Hatcher and Seevers describe an algorithm to insert the minimal number of barriers when compiling a synchronous data parallel program [QHS91]. Prakash et al. address reducing synchronization costs when compiling UC, a synchronous data parallel extension of C <ref> [PDB93] </ref>. They describe a set of transformations to reduce synchronization such as eliminating barriers and replacing it with point-to-point communication. They also describe how definition variables, x array renaming, and array alignment can be used to replace barriers with explicit communication.
Reference: [PH95] <author> M. Philipsen and E. A. Heinz. </author> <title> Automatic synchronization elimination for synchronous foralls. </title> <booktitle> In Proceedings of the 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: We present a set of algorithms to detect excess synchronization ranging from pairwise synchronization constructs such as flags, to global synchronization constructs such as barriers. The parallelizing compiler community has been addressing the problem of reducing synchronization in parallel programs <ref> [OB95, PH95, Tse95] </ref>. However, 4 most of their algorithms are tailored to detect and handle excess barriers introduced during compilation. y In contrast, we present algorithms to detect and transform barriers, pairwise synchronization, and critical sections. <p> The drawback of this work is that they just stop at describing the transformations. Specifically, they do not give any algorithms for arriving at these transformations. In addition, their evaluation is based on a simplistic analytical model. Philippsen and Heinz present an algorithm <ref> [PH95] </ref> for eliminating redundant barriers from synchronous FORALL loops in Modula-2*, a parallel extension of Modula-2. They first construct a graph representing both dependence information and expression trees (as used in intermediate representations during compilation).
Reference: [PK96] <author> Dejan Perkovic and Pete Keleher. </author> <title> Online data-race detection via coherency guarantees. </title> <booktitle> In Proceedings of the Second USENIX Symposium on Operating System Design and Implementation, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: Since most programs make use of system-supplied libraries for synchronization, this requirement is met by most programs without taking any special steps. * The program must be free of data races. In the absence of synchronization between conflicting accesses, a program is said to exhibit a data race <ref> [PK96] </ref>. Data races may cause the behavior of a program to be time-dependent. Their presence may therefore cause programs to execute 27 incorrectly. Therefore, it is not unreasonable to require that the program being analyzed be free of data races. <p> Hence, their presence is often indicative of incorrect program execution. Therefore, it is not unreasonable to require that the program being analyzed be free of data races. Several techniques can be used to assist in the detection of data races; the work of Perkovic and Keleher <ref> [PK96] </ref> is an example. 5.3 Conflict Analysis The presence or absence of conflicting accesses is a key factor that determines whether synchronization can be removed. Synchronization is needed between two intervals only if the accesses made within them conflict.
Reference: [PU89] <author> Cherri M. Pancake and Sue Utter. </author> <title> Models for visualization in parallel debuggers. </title> <booktitle> In Proceedings Supercomputing '89, </booktitle> <month> November </month> <year> 1989. </year>
Reference-contexts: A number of performance tools concentrate on the presentation of collected infor 12 mation using differently formatted displays [LSV + 89, DBKF90, RAN + 93], animation <ref> [FLK + 91, PU89] </ref>, and auralization [FJ93]. These tools perform none or very little analysis of the data and leave the reasoning required to detect the performance problem and its solution entirely to the programmer.
Reference: [PW86] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> De-cember </month> <year> 1986. </year>
Reference-contexts: Typically, synchronization is inserted conservatively during parallelization, to preserve dependences. A final pass then examines the program and removes unnecessary synchronization. Excess 49 synchronization is detected by analyzing the data access behavior of the program. Data dependence analysis <ref> [PW86] </ref> forms the core of all compile-time data transformations. Li and Abu-sufah have presented a technique to remove redundant synchronization interlocks (for instance, enforced with a post and wait condition) from doacross loops [LAS85]. An interlock is required in the presence of loop-carried flow- and anti-dependences.
Reference: [QHS91] <author> M. Quinn, P. Hatcher, and B. Seevers. </author> <title> Implementing a data parallel language on a tighly coupled multiprocessor. </title> <editor> In A. Nicolau, D. Gelernter, T. Gross, and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing. </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Both algorithms can be applied to reduce the synchronization added when paral-lelizing loops. However, we have shown that the synchronization added by these algorithms is not optimal [RC97a]. Quinn, Hatcher and Seevers describe an algorithm to insert the minimal number of barriers when compiling a synchronous data parallel program <ref> [QHS91] </ref>. Prakash et al. address reducing synchronization costs when compiling UC, a synchronous data parallel extension of C [PDB93]. They describe a set of transformations to reduce synchronization such as eliminating barriers and replacing it with point-to-point communication.
Reference: [RAN + 93] <author> D. A. Reed, R. A. Aydt, R. J. Noe, P. C. Roth, K. A. Shields, B. Schwartz, , and L. F. Tavera. </author> <title> Scalable performance analysis: The pablo performance analysis environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <year> 1993. </year>
Reference-contexts: Thus, it can be used easily by inexperienced parallel programmers. 1.2 Contributions Contribution 1 A new approach for designing performance tools, en abling the prescription of source-level changes to improve performance. Tools currently used by programmers to improve application performance suffer from two problems. First, existing tools <ref> [BPR96, RAN + 93] </ref> only describe performance information gathered at run-time, placing the onus on the user to deduce the cause for the performance problems. This requires users to understand how the program is mapped onto the parallel machine and also forces them to reason about machine-level execution statistics. <p> Typically, these tools provide feedback about the program execution, such as the time spent in different routines, and the factors that slow down progress, such as cache misses. However, the nature of this feedback is far from satisfactory. First, existing tools only describe the performance information gathered at run-time <ref> [BPR96, RAN + 93] </ref>. For example, a performance tool applied to a program that suffers from poor cache behavior could say: 70% of all cache misses occur in function foo () The onus is then on the user to infer the cause for the performance problems. <p> A prescriptive tool can be developed only if this process can be automated. 2.4 Related Work Techniques for run-time performance tuning focus primarily on two aspects. A number of performance tools concentrate on the presentation of collected infor 12 mation using differently formatted displays <ref> [LSV + 89, DBKF90, RAN + 93] </ref>, animation [FLK + 91, PU89], and auralization [FJ93]. These tools perform none or very little analysis of the data and leave the reasoning required to detect the performance problem and its solution entirely to the programmer.
Reference: [RC96] <author> R. Rajamony and A. L. Cox. </author> <title> A performance debugger for eliminating excess synchronization in shared-memory parallel programs. </title> <booktitle> In Proceedings of the Fourth International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, </booktitle> <pages> pages 250-256, </pages> <month> February </month> <year> 1996. </year> <month> 121 </month>
Reference-contexts: If the distribution will be skewed, we do not not carry out the relocation. 4.7 Related Work To the best of our knowledge, we are the first to address the detection and elimination of excess synchronization from explicitly parallel shared-memory programs <ref> [RC96] </ref>. Parallelizing compilers have long examined the issue of reducing the amount of synchronization inserted during the parallelization process. Typically, synchronization is inserted conservatively during parallelization, to preserve dependences. A final pass then examines the program and removes unnecessary synchronization.
Reference: [RC97a] <author> R. Rajamony and A. L. Cox. </author> <title> Optimally synchronizing DOACROSS loops on shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1997 International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: A description of the integer programming formulation can be found in our earlier work <ref> [RC97a] </ref>. For the example in Figure 4.2, we obtain the following equation: total deps Y ffi i The minimal set of synchronization is a, c, and e. <p> This brings down the size of the right hand side. If the equation does have a large number of terms, we resort to integer programming. A description of the integer programming formulation can be found in our earlier work <ref> [RC97a] </ref>. 4.6 Restructuring Computation Even if a program contains no excess synchronization as written, it may be possible to reduce synchronization and data communication by changing its computation structure. z Before we solve the equation, we weight each critical section term with the number of times it is invoked in the <p> Midkiff and Padua extend this to eliminate redundant synchronization operations from loops with arbitrary control flow [MP87]. Both algorithms can be applied to reduce the synchronization added when paral-lelizing loops. However, we have shown that the synchronization added by these algorithms is not optimal <ref> [RC97a] </ref>. Quinn, Hatcher and Seevers describe an algorithm to insert the minimal number of barriers when compiling a synchronous data parallel program [QHS91]. Prakash et al. address reducing synchronization costs when compiling UC, a synchronous data parallel extension of C [PDB93]. <p> In earlier work, we have shown that a similar problem is encountered in a compile-time context, when minimizing synchronization that must be added while parallelizing loops <ref> [RC97a] </ref>. We also described how to reduce the time spent inside critical sections, by moving outside, those operations that do not have to be performed inside the critical section. Our second method for reducing synchronization and communication involves transforming critical sections in barrier-separated intervals of the program.
Reference: [RC97b] <author> R. Rajamony and A. L. Cox. </author> <title> Performance debugging shared memory parallel programs using run-time dependence analysis. </title> <booktitle> In Proceedings of the 1997 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: When a shared location is written, we set WRITE to P curr . This permits us to later determine the shared locations written by P curr in this interval, which we fl WRITE and READ subsume CREAT and D in our earlier work <ref> [RC97b] </ref>. 67 (a) On every load to a shared location, set READ for the location to P curr . (b) On every store i. If the location is in shared memory, set WRITE for the location to P curr . ii. <p> All times are in seconds. fl We erroneously reported this as 16384 in our earlier work <ref> [RC97b] </ref>. x Note that in spite of this dilation, the perturbation effects can be reduced so that they do not skew the results obtained from the performance debugging process.
Reference: [RZ97] <author> M. Ronsse and W. Zwaenepoel. </author> <title> Execution replay for treadmarks. </title> <booktitle> In Proceedings of the Fifth EUROMICRO Workshop on Parallel and Distributed Processing, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: As long as the program is deterministic and free of data races, this results in the same access ordering as during the "record" phase. This technique and the perturbation it introduces has been researched by Ronsse and Zwaenepoel <ref> [RZ97] </ref>. We have not yet implemented this technique. The applications we discuss in this dissertation use only barriers and locks. In these applications, the locks are used only to enforce mutual exclusion when computing reductions.
Reference: [Sad75] <author> R. Sadourny. </author> <title> The dynamics of finite-difference models of the shallow-water equations. </title> <journal> Journal of Atmospheric Sciences, </journal> <volume> 32(4), </volume> <month> April </month> <year> 1975. </year>
Reference-contexts: Again, this barrier can be replaced by a flag that synchronizes only the relevant processes. 7.3.5 Shallow Shallow is an implementation of the shallow water benchmark from the National Center for Atmospheric Research <ref> [Sad75] </ref>. This code solves a set of difference equations on a two dimensional grid for the purpose of weather prediction. The main data structures are a set of arrays, with each process being assigned the computation on a band of rows.
Reference: [SB94] <author> Evan Speight and John K. Bennett. Paraview: </author> <title> Performance debugging of shared-memory parallel programs. </title> <type> Technical Report ELEC TR 9403, </type> <institution> Rice University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Changing this layout can reduce conflict misses and also increase implicit prefetching by improving spatial locality. Consequently, the application performance can be improved. Existing tools that target memory performance <ref> [GH93, LW94, MGA95, SB94] </ref> present only descriptive information, such as cache statistics, to the user. In contrast, a prescriptive tool can treat this as an optimization problem where the goal is to optimally arrange the program data structures, and directly specify the best layout to use. <p> For instance, the presence of a large number of invalidation misses to a data structure could indicate the presence of false sharing, or that the sharing is true and cannot be avoided. This deduction is left to the programmer. ParaView <ref> [SB94] </ref> performs some analysis on trace information collected from a cycle-level simulation of the program, and displays it in various formats to the programmer. Time spent in computation, synchronization and the memory hierarchy along with cache access information is presented. Miss rates are broken into categories as in MemSpy. <p> Changing this layout can reduce 110 conflict misses and also increase implicit prefetching by improving spatial locality. Consequently, the application performance can be improved. Existing tools that target memory performance <ref> [GH93, LW94, MGA95, SB94] </ref> present only descriptive information, such as cache statistics, to the user. In contrast, a tool based on our approach can directly specify the best layout to use.
Reference: [SE94] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the ACM SIGPLAN 94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Instead, when analyzing the program source, we generate specialized functions that when executed at run-time, provide the necessary information. This considerably speeds up the collection of run-time data. This method can also be used to speed up program analysis tools such as ATOM <ref> [SE94] </ref>, which instrument an application to collect run-time information. 1.3 Dissertation Overview This dissertation is organized as follows. Chapter 2 describes our new approach for designing performance tools. We provide three examples to illustrate how this approach can be used to develop prescriptive tools. <p> However, while our mirror functions can be considered abstract versions of the original basic blocks, we execute them in conjunction with the original program to efficiently compute operand information. Although we could have used an instrumentation system such as ATOM <ref> [SE94] </ref> for collecting the per-basic block address trace, or EEL [LS95] which also provides control flow information, we would need to extend them to convert control to data dependences and to examine the basic blocks to obtain the operand information. <p> As is done in most simulators, we could have interpreted the statically pre-computed information. Instead, we generate code which when executed, directly carries out the desired run-time processing. This significantly speeds up execution. This technique can also be used to speed up program analysis tools such as ATOM <ref> [SE94] </ref>, which instrument an application to collect run-time information.
Reference: [Smi91] <author> M. Smith. </author> <title> Tracing with pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stan-ford, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: The disadvantage is an increase in the code size. We can address this problem by first profiling the code to find out the basic blocks that are executed most often. Tools such as pixie <ref> [Smi91] </ref> can be used to do this. Then, we can use the mirror functions only for the most frequently executed basic blocks, leaving the others to be handled by a run-time interpreter. We have not yet implemented this.
Reference: [SPA94] <author> SPARC International, Inc. </author> <title> The SPARC Architecture Manual, </title> <type> Version 9, </type> <year> 1994. </year>
Reference-contexts: Removing this critical section eliminates the serialization of the execution of this part of the program. y A control barrier can be implemented using the membar instructions provided by the SPARC v9 instruction set <ref> [SPA94] </ref>. 98 Reducing synchronization in Gauss and Multigrid has little impact on the performance because of the small cost of synchronization on this platform. Compared to the running time, both programs have so few barriers that converting them to flags has negligible effect on the performance.
Reference: [SS88] <author> D. Shasha and M. Snir. </author> <title> Efficient and correct execution of parallel programs that share memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 232-312, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Contribution 2 A set of algorithms to detect excess synchronization and some forms of excess data communication in explicitly parallel shared memory programs. In a shared-memory program, if conflicting accesses (read-write or write-write accesses to the same memory location) <ref> [SS88] </ref> are not ordered by synchronization, time-dependent bugs may be introduced. Consequently, the program behavior can be unexpected or even incorrect. Faced with the possibility of these race conditions [Net91], most inexperienced parallel programmers add more synchronization than necessary to their applications. <p> We provide three examples in this section to illustrate the general approach. 2.3.1 Reducing Synchronization and Communication In a shared-memory program, time-dependent bugs may be introduced if conflicting accesses (read-write or write-write accesses to the same memory location) <ref> [SS88] </ref> are not ordered by synchronization. In the presence of these race conditions [Net91], the program behavior can be unexpected or even incorrect. Faced with this possibility, most inexperienced parallel programmers add more synchronization than necessary to their applications. <p> Processors communicate by reading and writing locations in the shared memory. Since the same location can potentially be accessed simultaneously by multiple processors, a mechanism to order accesses is required. Synchronization is used for this purpose. Specifically, conflicting accesses (read-write or write-write accesses to the same memory location) <ref> [SS88] </ref> need to be ordered using synchronization. If this ordering is not enforced, time-dependent bugs known as race conditions [Net91] may be introduced into the program. Consequently, the program behavior can be unexpected or even incorrect. <p> 19 perspective, a significant advantage of R x over other tools is that it prescribes source-level transformations. 3.3 Why Use a Run-Time Approach? At the heart of the methods to detect excess synchronization and data communication is the detection of conflicting accesses (read-write or write-write accesses to the same location) <ref> [SS88] </ref>. A prerequisite for detecting conflicting accesses is the accurate determination of access sets, i.e., the set of data read or written in different parts of the program. <p> In other words, it is required only between conflicting data accesses (read-write or write 28 write accesses to the same memory location) <ref> [SS88] </ref>. In this section, we discuss how to determine whether a particular synchronization operation is required or not. Consider Figure 4.1 which shows a portion of the partial-order graph for an ex ecution on two processes, P a and P b . <p> Krishnamurthy and Yelick present compiler optimizations for explicitly parallel shared-memory programs [KY95]. In such programs, code motion optimizations require consideration of inter-process dependences. In order to guarantee sequential consistency after reordering transformations have been applied, analysis introduced by Shasha and Snir known as cycle detection <ref> [SS88] </ref> is required. Krishnamurthy and Yelick improve the accuracy of cycle detection by incorporating additional informa 51 tion available from the explicit synchronization in the program. Consequently, scalar optimizations may be applied around synchronization primitives. <p> The union of these per-process orders is the program order. Definition 2: Conflicting operations: Two operations conflict if they access the same locations and at least one of them is a write <ref> [SS88] </ref>. Definition 3: Execution order: With atomic memory, there is a total order on all memory operations of an execution, such that a read returns the value of the last write to the same location in this total order. This order is called the execution order [Col92].
Reference: [Sun95] <author> Sun Microsystems Inc. SPARCworks/iMPact: </author> <title> Tools for Multithreaded Programming, 1995. Catalog number: </title> <journal> 802-3542-10. </journal> <volume> 122 </volume>
Reference-contexts: As a result, it can determine that the access to ptra need not ever be protected by the critical section. This form of programmer involvement is not new. For instance, the SGI Power compiler provides pragmas based on which the compiler parallelizes loops [Bau92]. 22 Sun Microsystem's lock_lint tool <ref> [Sun95] </ref> uses programmer annotations extensively to check for data races and improperly used locks in multithreaded programs. Static debugging (see Bourdoncle's work [Bou93] for example) also has a history of using programmer supplied invariants.
Reference: [SWG91] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: This relocation also improves the load balance of the program. 95 7.3.6 Water Water, from the original SPLASH suite <ref> [SWG91] </ref>, is a molecular dynamics simulation. The molecules are divided equally among the processes. There are two key phases in each timestep.
Reference: [TLH94] <author> Joseph Torrellas, Monica S. Lam, and John L. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Once a performance problem has been detected, a prescriptive tool must also determine a solution to it. The exact methods used to carry out this step depend on the problem domain in which the tool is being applied. Consider a tool being developed to reduce false sharing <ref> [TLH94] </ref> in shared-memory programs. False sharing can be detected by analyzing the memory access trace from the application. Once false sharing has been detected, a possible solution is to rearrange the layout of the data structures in the address space, enabling better use of the cache hierarchy. 2. <p> We call this construct a control barrier. Software DSM systems such as TreadMarks [KDCZ94], which make the address space consistent lazily, can take advantage of control barriers. In such systems, by using a control barrier, substantial benefits can be obtained by eliminating the false sharing <ref> [TLH94] </ref> that may be caused by making the address space consistent eagerly. 4.4.2 Pairwise Synchronization Acquires and releases constitute the general form of synchronization that can be used to impose an access ordering.
Reference: [Tse95] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the 5th Symposium on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: We present a set of algorithms to detect excess synchronization ranging from pairwise synchronization constructs such as flags, to global synchronization constructs such as barriers. The parallelizing compiler community has been addressing the problem of reducing synchronization in parallel programs <ref> [OB95, PH95, Tse95] </ref>. However, 4 most of their algorithms are tailored to detect and handle excess barriers introduced during compilation. y In contrast, we present algorithms to detect and transform barriers, pairwise synchronization, and critical sections. <p> Their base compiler inserts a barrier after each expression evaluation. Hence, programs analyzed by their implementation perform very well compared to the original compiler. Tseng has implemented a barrier elimination algorithm <ref> [Tse95] </ref> in the SUIF compiler [AALT95]. Two optimizations are used to reduce overhead and synchronization. By combining adjacent SPMD regions, the overhead of starting up parallel tasks is reduced.
Reference: [WOT + 95] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> Splash-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: We also show the viability of our approach through a detailed examination of the overheads imposed while performance debugging these applications. Our application suite includes programs from a wide variety of sources. Barnes-Hut, Water, and Gauss are benchmark programs. The first two are from the SPLASH-2 suite <ref> [WOT + 95] </ref>, while the third is locally derived. Conjugate Gradient and Multigrid were written by first year graduate students in a parallel programming course, using specifications from the NAS benchmarking document [BBLS93]. <p> 5 iters p p p CG LARGE p p Gauss 2000 fi 2000 p Multigrid 128 fi 128, 20 iters p Shallow 800 fi 800, 20 iters p No change Water 1728 molecules, 5 iters p p p Table 7.2 R x findings 7.3.1 Barnes-Hut Barnes-Hut, from the SPLASH-2 suite <ref> [WOT + 95] </ref>, is a simulation of a system of bodies influenced by gravitational forces. A body is represented as a point mass that exerts forces on all other bodies. The algorithm uses a hierarchical oct-tree representation of space in three dimensions. Space is broken into cells. <p> We then executed the original and optimized versions on both a hardware and a software shared memory platform. Both versions used the data sets described in Table 7.2. The data sets fit into the main memory of both systems. fl This change appears in the SPLASH-2 versions of Water <ref> [WOT + 95] </ref>. 96 7.4.1 Hardware Shared Memory The hardware shared memory platform is an 8-processor Sun Enterprise server, with 248 MHz UltraSparc processors and 1 Gbyte of memory. We used Posix threads (pthreads) to create and synchronize threads. Basic synchronization costs on this platform are given in Table 7.3.
References-found: 69

