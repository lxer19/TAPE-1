URL: http://www.research.att.com/~lewis/papers/lewis91c.ps
Refering-URL: http://www.csi.uottawa.ca/~debruijn/irbib.html
Root-URL: 
Title: Evaluating Text Categorization  
Author: David D. Lewis 
Address: Amherst, MA 01003  
Affiliation: Computer and Information Science Dept. University of Massachusetts  
Note: Appeared (with same pagination) in Proceedings of the Speech and Natural Language Workshop, Asilomar, Feb. 1991. Morgan Kaufmann, San Mateo, CA, pp. 312-318.  
Abstract: While certain standard procedures are widely used for evaluating text retrieval systems and algorithms, the same is not true for text categorization. Omission of important data from reports is common and methods of measuring effectiveness vary widely. This has made judging the relative merits of techniques for text categorization difficult and has disguised important research issues. In this paper I discuss a variety of ways of evaluating the effectiveness of text categorization systems, drawing both on reported categorization experiments and on methods used in evaluating query-driven retrieval. I also consider the extent to which the same evaluation methods may be used with systems for text extraction, a more complex task. In evaluating either kind of system, the purpose for which the output is to be used is crucial in choosing appropriate evaluation methods. 
Abstract-found: 1
Intro-found: 1
Reference: [BB64] <author> Harold Borko and Myrna Bernick. </author> <title> Automatic document classification part II. additional experiments. </title> <journal> J. Association for Computing Machinery, </journal> <volume> 11(2) </volume> <pages> 138-151, </pages> <month> April </month> <year> 1964. </year>
Reference-contexts: However, the numerical measures described above provide a useful standard for understanding the differences between methods under a variety of conditions. Comparison between categorization methods would be aided by the use of common testsets, something which has rarely been done. (An exception is <ref> [BB64] </ref>.) Development of standard collections would be an important first step to better understanding of text categorization. Categorization is an important facet of many kinds of text processing systems. The effectiveness measures defined above may be useful for evaluating some aspects of these systems.
Reference: [Bor64] <author> Harold Borko. </author> <title> Measuring the reliability of subject classification by men and machines. </title> <journal> American Documentation, </journal> <pages> pages 268-273, </pages> <month> October </month> <year> 1964. </year>
Reference-contexts: Otherwise, it may be necessary to have human indexers categorize some texts specifically for the purposes of the experiment. Many studies have found that even professional bibliographic indexers disagree on a substantial proportion of categorization decisions <ref> [Bor64, Fie75, HZ80] </ref>. This calls into question the validity of human category assignment as a standard against which to judge mechanical assignment. One approach to this problem has been to have an especially careful indexing done [Fie75, HZ80]. Sometimes evaluation is done against several indexings [Fie75, HKC88].
Reference: [CH88] <author> Paul R. Cohen and Adele E. Howe. </author> <title> How evaluation guides AI research. </title> <journal> AI Magazine, </journal> <pages> pages 35-43, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: Effectiveness refers to the ability of a categorization to supply information to a system or user that wants to access the texts. Measuring effectiveness is just one of several kinds of evaluation that should be considered <ref> [Spa81a, CH88, PF90] </ref>. After considering effectiveness evaluation for text categorization we will turn to a related task, text extraction, and consider what role the effectiveness measures discussed for categorization have there.
Reference: [Fie75] <author> B. J. </author> <title> Field. Towards automatic indexing: Automatic assignment of controlled-language indexing and classification from free indexing. </title> <journal> J. Documentation, </journal> <volume> 31(4) </volume> <pages> 246-265, </pages> <month> De-cember </month> <year> 1975. </year>
Reference-contexts: Field uses the strategy of assigning the top k categories to a document, but unlike the above studies does this without knowledge of the proper number of categories for any particular document. He then plots the recall value achieved for variations in the number of categories assigned <ref> [Fie75] </ref>. Fuhr and Knorz plot a curve showing tradeoff between recall and precision as a category assignment threshold varies [FK84]. <p> Otherwise, it may be necessary to have human indexers categorize some texts specifically for the purposes of the experiment. Many studies have found that even professional bibliographic indexers disagree on a substantial proportion of categorization decisions <ref> [Bor64, Fie75, HZ80] </ref>. This calls into question the validity of human category assignment as a standard against which to judge mechanical assignment. One approach to this problem has been to have an especially careful indexing done [Fie75, HZ80]. Sometimes evaluation is done against several indexings [Fie75, HKC88]. <p> This calls into question the validity of human category assignment as a standard against which to judge mechanical assignment. One approach to this problem has been to have an especially careful indexing done <ref> [Fie75, HZ80] </ref>. Sometimes evaluation is done against several indexings [Fie75, HKC88]. Another approach is to accept that there will always be some degree of inconsistency in human categorization, and that this imposes an upper limit on the effectiveness of machine categorization. <p> This calls into question the validity of human category assignment as a standard against which to judge mechanical assignment. One approach to this problem has been to have an especially careful indexing done [Fie75, HZ80]. Sometimes evaluation is done against several indexings <ref> [Fie75, HKC88] </ref>. Another approach is to accept that there will always be some degree of inconsistency in human categorization, and that this imposes an upper limit on the effectiveness of machine categorization.
Reference: [FK84] <author> N. Fuhr and G. E. Knorz. </author> <title> Retrieval test evaluation of a rule based automatic indexing (AIR/PHYS). </title> <editor> In C. J. van Rijsbergen, editor, </editor> <booktitle> Research and Development in Information Retrieval, </booktitle> <pages> pages 391-408, </pages> <address> Cambridge. </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: He then plots the recall value achieved for variations in the number of categories assigned [Fie75]. Fuhr and Knorz plot a curve showing tradeoff between recall and precision as a category assignment threshold varies <ref> [FK84] </ref>. When categories are completely disjoint and a catego-rizer always assigns exactly 1 of the M categories to a text, we really have a single M -ary decision, rather than M binary decisions. <p> How an indirect evaluation is done depends on the kind of system using the categorized text. Most categorizers have been intended to index documents for query-driven text retrieval. Despite this, there have been surprisingly few studies <ref> [Har82, FK84] </ref> comparing text retrieval performance under different automatic category assignments. The focus on manual categorization as a standard appears to have led categorization researchers to ignore some promising research directions.
Reference: [Hal90] <author> Peter C. Halverson. </author> <title> MUC scoring system: user's manual. Edition 1.5, General Electric Corporate Research and Development, </title> <address> Schenectady, NY, </address> <month> November 2 </month> <year> 1990. </year>
Reference-contexts: Two perspectives are taken|one focusing on the type of data extracted and the other focusing on the purpose for which extraction is done. Types of Extracted Data Extracted data can include binary or M -ary categorizations, quantities, and templates or database records with atomic or structured fillers <ref> [Sun89, McC90, Hal90] </ref>. The number of desired records per text may depend on text content, and cross references between fillers of record fields may be required. <p> The MUC-3 evaluation <ref> [Hal90] </ref> has taken the approach of retaining the contingency table measures but redefining the set of possible decisions.
Reference: [Har82] <author> P. Harding. </author> <title> Automatic Indexing and Classification for Mechanised Information Retrieval. BLRDD Report No. </title> <type> 5723, </type> <institution> British Library R & D Department, </institution> <address> London, </address> <month> February </month> <year> 1982. </year>
Reference-contexts: How an indirect evaluation is done depends on the kind of system using the categorized text. Most categorizers have been intended to index documents for query-driven text retrieval. Despite this, there have been surprisingly few studies <ref> [Har82, FK84] </ref> comparing text retrieval performance under different automatic category assignments. The focus on manual categorization as a standard appears to have led categorization researchers to ignore some promising research directions.
Reference: [HKC88] <author> Philip J. Hayes, Laura E. Knecht, and Monica J. Cellio. </author> <title> A news story categorization system. </title> <booktitle> In Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 9-17, </pages> <year> 1988. </year>
Reference-contexts: Presenting effectiveness measures averaged over category groups defined by frequency in the training set would be extremely informative, but does not appear to have been done in any published study. If the number of categories is small enough, effectiveness can be presented separately for each category <ref> [HKC88] </ref>. Subsets of the set of test documents can be defined as well, particularly if the behavior of the system on texts of different kinds is of interest. <p> This calls into question the validity of human category assignment as a standard against which to judge mechanical assignment. One approach to this problem has been to have an especially careful indexing done [Fie75, HZ80]. Sometimes evaluation is done against several indexings <ref> [Fie75, HKC88] </ref>. Another approach is to accept that there will always be some degree of inconsistency in human categorization, and that this imposes an upper limit on the effectiveness of machine categorization. <p> Much more could be said even about evaluating categorization systems. In particular, I have focused entirely on numerical measures. Carefully chosen examples, examined in detail, can also be quite revealing <ref> [HKC88] </ref>. However, the numerical measures described above provide a useful standard for understanding the differences between methods under a variety of conditions.
Reference: [Hay90] <author> Philip J. Hayes. </author> <title> Intelligent high-volume text processing using shallow, domain-specific techniques. </title> <editor> In P. S. Jacobs, editor, </editor> <booktitle> Text-Based Intelligent Systems, </booktitle> <pages> pages 70-74, </pages> <address> Sch-enectady, NY, </address> <year> 1990. </year> <title> GE R & D Center. Report Number 90CRD198. </title>
Reference-contexts: Categorization of documents may be desired for other purposes than supporting query-driven retrieval. Separation of a text stream by category may allowing packaging of the text stream as different products <ref> [Hay90] </ref>. Some comparison of average retrieval effectiveness across text streams might be an appropriate measure in this case. Categorization may also be used to select a subset of texts for more sophisticated processing, such as extraction of information or question answering [JR90].
Reference: [Hol69] <author> Ole R. Holsti. </author> <title> Content Analysis for the Social Sciences and Humanities. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1969. </year>
Reference: [HZ80] <author> Karen A. Hamill and Antonio Zamora. </author> <title> The use of titles for automatic document classification. </title> <journal> J. American Society for Information Science, </journal> <pages> pages 396-402, </pages> <year> 1980. </year>
Reference-contexts: Otherwise, it may be necessary to have human indexers categorize some texts specifically for the purposes of the experiment. Many studies have found that even professional bibliographic indexers disagree on a substantial proportion of categorization decisions <ref> [Bor64, Fie75, HZ80] </ref>. This calls into question the validity of human category assignment as a standard against which to judge mechanical assignment. One approach to this problem has been to have an especially careful indexing done [Fie75, HZ80]. Sometimes evaluation is done against several indexings [Fie75, HKC88]. <p> This calls into question the validity of human category assignment as a standard against which to judge mechanical assignment. One approach to this problem has been to have an especially careful indexing done <ref> [Fie75, HZ80] </ref>. Sometimes evaluation is done against several indexings [Fie75, HKC88]. Another approach is to accept that there will always be some degree of inconsistency in human categorization, and that this imposes an upper limit on the effectiveness of machine categorization.
Reference: [JR90] <author> Paul S. Jacobs and Lisa F. Rau. SCISOR: </author> <title> Extracting information from on-line news. </title> <journal> Communications of the ACM, </journal> <volume> 33(11) </volume> <pages> 88-97, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Some comparison of average retrieval effectiveness across text streams might be an appropriate measure in this case. Categorization may also be used to select a subset of texts for more sophisticated processing, such as extraction of information or question answering <ref> [JR90] </ref>. Evaluating the quality of the extracted information may give some insight into categorization performance, though the connection can be distant here. 315 There are drawbacks to indirect evaluation, of course. Tague questions why any particular set of queries should serve as a test of an indexing. <p> When categorization is used as a component in a complex language understanding system, that system itself may be difficult to evaluate <ref> [JR90] </ref> or differences in categorization quality may be hard to discern from overall system behavior. A single categorization may also be intended to serve several purposes, some possibly not yet defined. Using both direct and indirect evaluation will be the best approach, when practical.
Reference: [KW75] <author> B. Gautam Kar and L. J. White. </author> <title> A distance measure for automatic sequential document classification. </title> <type> Technical Report OSU-CISRC-TR-75-7, </type> <institution> Computer and Information Science Research Center; Ohio State Univ., Columbus, Ohio, </institution> <month> August </month> <year> 1975. </year>
Reference-contexts: One Category or Many? Evaluations of systems which assign multiple categories to a document have often been flawed, particularly for cate-gorizers which use statistical techniques. For instance, some of the results in [Mar61] and <ref> [KW75] </ref> were obtained under assumptions equivalent to the categorizer knowing in advance how many correct categories each test document has. This knowledge is not available in an operational setting. Better attempts to both produce and evaluate multiple category assignments are found in work by Fuhr and Knorz, and by Field.
Reference: [Lew91] <author> David D. Lewis. </author> <title> Evaluating text classification systems. </title> <note> In preparation., </note> <year> 1991. </year>
Reference-contexts: The appropriate partitions to make will depend on many factors that cannot be anticipated here. A crucial point to stress, however, is that care should be taken to partition supporting data on the task and system in the same fashion <ref> [Lew91] </ref>. For instance, if effectiveness measures are presented for subsets of documents, then statistics such as average number of words per document, etc. should be given for the same groups of documents. Arithmetic Anomalies The above discussion assumed that computing the effectiveness measures is always straightforward. <p> I also discussed how some of the work done by text extraction systems can be viewed as categorization and evaluated in a similar fashion, though new measures are needed as well. Acknowledgments The author is preparing a longer article on this topic <ref> [Lew91] </ref> so comments on the above would be most welcome. This research has already greatly benefited from discussions with Laura Balcom, Nancy Chinchor, Bruce Croft, Ralph Grishman, Jerry Hobbs, Adele Howe, Lisa Rau, Penelope Sibun, Beth Sundheim, and Carl Weir.
Reference: [Mar61] <author> M. E. Maron. </author> <title> Automatic indexing: An experimental inquiry. </title> <journal> J. of the Association for Computing Machinery, </journal> <volume> 8 </volume> <pages> 404-417, </pages> <year> 1961. </year>
Reference-contexts: Maron grouped documents on the basis of the amount of evidence they provided for making a categorization decision, and showed that effectiveness increased in proportion to the amount of evidence <ref> [Mar61] </ref>. Finally, it is sometimes appropriate to partition results by degree of correctness of a category/document pair. While the contingency table model assumes that an assignment decision is either correct or incorrect, the standard they are being tested against may actually have gradations of correctness. <p> One Category or Many? Evaluations of systems which assign multiple categories to a document have often been flawed, particularly for cate-gorizers which use statistical techniques. For instance, some of the results in <ref> [Mar61] </ref> and [KW75] were obtained under assumptions equivalent to the categorizer knowing in advance how many correct categories each test document has. This knowledge is not available in an operational setting. <p> The focus on manual categorization as a standard appears to have led categorization researchers to ignore some promising research directions. For instance, I know of no study that has evaluated weighted assignment of categories to documents, despite early recognition of the potential of this technique <ref> [Mar61] </ref> and the strong evidence that weighting free text terms in documents improves retrieval performance [Sal86]. Categorization of documents may be desired for other purposes than supporting query-driven retrieval. Separation of a text stream by category may allowing packaging of the text stream as different products [Hay90].
Reference: [McC90] <author> Rita McCardell. </author> <title> Evaluating natural language generated database records. </title> <booktitle> In Proceedings of Speech and Natural Language Workshop, </booktitle> <pages> pages 64-70. </pages> <publisher> Defense Advanced Research Projects Agency, Morgan Kaufmann, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Two perspectives are taken|one focusing on the type of data extracted and the other focusing on the purpose for which extraction is done. Types of Extracted Data Extracted data can include binary or M -ary categorizations, quantities, and templates or database records with atomic or structured fillers <ref> [Sun89, McC90, Hal90] </ref>. The number of desired records per text may depend on text content, and cross references between fillers of record fields may be required.
Reference: [PF90] <author> Martha Palmer and Tim Finin. </author> <booktitle> Workshop on the evaluation of natural language processing systems. Computational Linguistics, </booktitle> <volume> 16 </volume> <pages> 175-181, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Effectiveness refers to the ability of a categorization to supply information to a system or user that wants to access the texts. Measuring effectiveness is just one of several kinds of evaluation that should be considered <ref> [Spa81a, CH88, PF90] </ref>. After considering effectiveness evaluation for text categorization we will turn to a related task, text extraction, and consider what role the effectiveness measures discussed for categorization have there. <p> A single categorization may also be intended to serve several purposes, some possibly not yet defined. Using both direct and indirect evaluation will be the best approach, when practical. Other Issues The evaluation of natural language processing (NLP) systems is an area of active research <ref> [PF90] </ref>, and a great deal remains to be learned. Much more could be said even about evaluating categorization systems. In particular, I have focused entirely on numerical measures. Carefully chosen examples, examined in detail, can also be quite revealing [HKC88].
Reference: [Sal72] <author> G. Salton. </author> <title> The "generality" effect and the retrieval evaluation for large collections. </title> <journal> J. American Society for Information Science, </journal> <pages> pages 11-22, </pages> <month> January-February </month> <year> 1972. </year>
Reference-contexts: However, in doing so they capture different properties of the categorization. In the context of query-driven retrieval, Salton has pointed out how systems which maintain constant precision react differently to increasing numbers of documents than those which maintain constant fallout <ref> [Sal72] </ref>. Similar effects can arise for categorizers as the number or nature of categories changes. Table 2 shows the hypothetical performance of catego-rizer X as the category set is expanded to include new topics.
Reference: [Sal86] <author> Gerard Salton. </author> <title> Another look at automatic text-retrieval systems. </title> <journal> Communications of the ACM, </journal> <volume> 29(7) </volume> <pages> 648-656, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: For instance, I know of no study that has evaluated weighted assignment of categories to documents, despite early recognition of the potential of this technique [Mar61] and the strong evidence that weighting free text terms in documents improves retrieval performance <ref> [Sal86] </ref>. Categorization of documents may be desired for other purposes than supporting query-driven retrieval. Separation of a text stream by category may allowing packaging of the text stream as different products [Hay90]. Some comparison of average retrieval effectiveness across text streams might be an appropriate measure in this case.
Reference: [Spa81a] <editor> Karen Sparck Jones, editor. </editor> <title> Information Retrieval Experiment. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Effectiveness refers to the ability of a categorization to supply information to a system or user that wants to access the texts. Measuring effectiveness is just one of several kinds of evaluation that should be considered <ref> [Spa81a, CH88, PF90] </ref>. After considering effectiveness evaluation for text categorization we will turn to a related task, text extraction, and consider what role the effectiveness measures discussed for categorization have there. <p> A common theme is the need to consider in an evaluation the purpose for which information is generated from the text. I will have occasion in the following to repeatedly refer to a chapter by Tague [Tag81] in Sparck Jones' collection on information retrieval experimentation <ref> [Spa81a] </ref>. This collection discusses a wide range of evaluation issues, and is an important resource for anyone interested in the evaluation of text-based systems.
Reference: [Spa81b] <author> Karen Sparck Jones. </author> <title> Retrieval system tests 1958-1978. </title> <editor> In Karen Sparck Jones, editor, </editor> <title> Information Retrieval Experiment, chapter 12. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Strongly held intuitions about the relative effectiveness of indexing languages and indexing methods for supporting document retrieval have often been shown by experiment to be incorrect <ref> [Spa81b] </ref>. If the primary purpose of extracted information is to support querying, then indirect evaluation, i.e. testing with actual queries, is very important. 317 CONCLUSION Text categorization plays a variety of roles in text-based systems.
Reference: [Sun89] <author> Beth M. Sundheim. </author> <title> Plans for task-oriented evaluation of natural language understanding systems. </title> <booktitle> In Proceedings of the Speech and Natural Language Workshop, </booktitle> <pages> pages 197-202. </pages> <publisher> Defense Advanced Research Projects Agency, Morgan Kaufmann, </publisher> <month> February </month> <year> 1989. </year>
Reference-contexts: In the next section we consider the evaluation of text extraction systems from this standpoint. IMPLICATIONS FOR EVALUATING TEXT EXTRACTION Systems for text extraction generate formatted data from natural language text. Some forms of extraction, for instance specifying the highest level of action in a naval report <ref> [Sun89] </ref>, are in fact categorization decisions. Other forms of extraction are very different, and do not fit well into the contingency table model. In the following I briefly consider evaluation of text extraction systems using the effectiveness measures described for categorization. <p> Two perspectives are taken|one focusing on the type of data extracted and the other focusing on the purpose for which extraction is done. Types of Extracted Data Extracted data can include binary or M -ary categorizations, quantities, and templates or database records with atomic or structured fillers <ref> [Sun89, McC90, Hal90] </ref>. The number of desired records per text may depend on text content, and cross references between fillers of record fields may be required.
Reference: [Swe64] <editor> John A. Swets, editor. </editor> <title> Signal Detection and Recognition by Human Observers. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1964. </year>
Reference-contexts: Given the contingency table, three important measures of the system's effectiveness are: (1) recall = a=(a + c) (2) precision = a=(a + b) (3) fallout = b=(b + d) Measures equivalent to recall and fallout made their first appearance in signal detection theory <ref> [Swe64] </ref>, where they play a central role. Recall and precision are ubiquitous in information retrieval, where they measure the proportion of relevant documents retrieved and the proportion of retrieved documents which are relevant, respectively. Fallout measures the proportion of nonrelevant documents which are retrieved, and has also seen considerable use. <p> The contingency table model provides one way of summarizing M -ary decision effectiveness, but other approaches, such as confusion matrices <ref> [Swe64] </ref>, may be more revealing. Standard of Correctness The effectiveness measures described above require that correct categorizations are known for a set of test documents.
Reference: [Tag81] <author> Jean M. Tague. </author> <title> The pragmatics of information retrieval experimentation. </title> <editor> In Karen Sparck Jones, editor, </editor> <title> Information Retrieval Experiment, chapter 5. </title> <publisher> Butterworths, </publisher> <address> Lon-don, </address> <year> 1981. </year> <month> 318 </month>
Reference-contexts: A common theme is the need to consider in an evaluation the purpose for which information is generated from the text. I will have occasion in the following to repeatedly refer to a chapter by Tague <ref> [Tag81] </ref> in Sparck Jones' collection on information retrieval experimentation [Spa81a]. This collection discusses a wide range of evaluation issues, and is an important resource for anyone interested in the evaluation of text-based systems.
References-found: 24

