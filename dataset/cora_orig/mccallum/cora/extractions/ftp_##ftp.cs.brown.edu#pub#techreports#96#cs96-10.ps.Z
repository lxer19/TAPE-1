URL: ftp://ftp.cs.brown.edu/pub/techreports/96/cs96-10.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-96-10.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Andrew G. Barto, Richard S. Sutton, and Christopher J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report 89-95, </type> <institution> Department of Computer and Information Science, University of Massachusetts, Amherst, Massachusetts, </institution> <year> 1989. </year> <title> Also published in Learning and Computational Neuroscience: Foundations of Adaptive Networks, </title> <editor> Michael Gabriel and John Moore, editors. </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: A discount parameter 0 fl &lt; 1 controls the degree to which future rewards are significant compared to immediate rewards. The theory of Markov decision processes can be used as a theoretical foundation for important results concerning this reinforcement-learning scenario <ref> [1] </ref>. A (finite) Markov decision process (mdp) [18] is defined by the tuple hS; A; P; Ri, where S represents a finite set of states, A a finite set of actions, P a transition function, and R a reward function. <p> Also, if R t = R and P t = P for all t, this result implies that real-time dynamic programming converges to the optimal value function <ref> [1] </ref>. 12 5 CONCLUSIONS In this paper, we presented a generalized model of Markov decision processes, and proved the convergence of several reinforcement-learning algorithms in the generalized model.
Reference: [2] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: These simultaneous equations, known as the Bellman equations, can be solved using a variety of techniques ranging from successive approximation <ref> [2] </ref> to linear programming [6]. In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions.
Reference: [3] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Theorem 1 therefore implies that this generalized Q-learning algorithm converges to the optimal Q function with probability 1 uniformly over X fi A. The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for undiscounted "all-policies-proper" mdps <ref> [3] </ref>, a soft state aggregation learning rule [24], and a "spreading" learning rule [19] are given in an extended version of this paper [28]. 4.2 Q-LEARNING FOR MARKOV GAMES Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step.
Reference: [4] <author> Justin A. Boyan. </author> <title> Modular neural networks for learning context-dependent game strategies. </title> <type> Master's thesis, </type> <institution> Department of Engineering and Computer Laboratory, University of Cambridge, </institution> <address> Cambridge, UK, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. A great deal of reinforcement-learning research has been directed to the problem of solving two-player games <ref> [29, 30, 21, 4] </ref>, for example, and the reinforcement-learning algorithms for solving mdps and their convergence proofs do not apply directly to games. <p> Markov games <ref> [4] </ref> max a or min a f (x; a) P risk-sensitive mdps [8] max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) exploration-sens. mdps [11] max 2P 0 P P Markov games [13] max A min b P P information-state mdp [16] max a f (x; a) P
Reference: [5] <author> Anne Condon. </author> <title> The complexity of stochastic games. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 203-224, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: In the zero-sum games we consider, the rewards to player 2 4 (the minimizer) are simply the additive inverse of the rewards for player 1 (the maximizer). Markov decision processes are a special case of alternating Markov games in which S 2 = ;; Condon <ref> [5] </ref> proves this and the other unattributed results in this section. A common optimality criterion for alternating Markov games is discounted minimax optimality. Under this criterion, the maximizer should choose actions so as to maximize its reward in the event that the minimizer chooses the best possible counter-policy.
Reference: [6] <author> Cyrus Derman. </author> <title> Finite State Markovian Decision Processes. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year> <note> Volume 67 is Mathematics in Science and Engineering, edited by Richard Bellman. </note>
Reference-contexts: These simultaneous equations, known as the Bellman equations, can be solved using a variety of techniques ranging from successive approximation [2] to linear programming <ref> [6] </ref>. In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions.
Reference: [7] <author> Vijaykumar Gullapalli and Andrew G. Barto. </author> <title> Convergence of indirect adaptive asynchronous value iteration algorithms. </title> <editor> In S. J. Hanson, J. D Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 695-702. </pages> <publisher> Morgan Kaufmann, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: Both model-free (direct) methods, such as Q-learning [33], and model-based (indirect) methods, such as prioritized sweeping [15] and DYNA [26], have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref> [33, 31, 10, 7] </ref>. As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. <p> In model-based reinforcement learning, R and P are estimated on-line, and the value function is updated according to the approximate dynamic-programming operator derived from these estimates; this algorithm converges to the optimal value function under a wide variety of choices of the order states are updated <ref> [7] </ref>. The method of Q-learning [32] uses experience to estimate the optimal value function without ever explicitly approximating R and P .
Reference: [8] <author> Matthias Heger. </author> <title> Consideration of risk in reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 105-111, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this paper, we introduce a generalized Markov decision process model with applications to reinforcement learning, and list some of the important results concerning the model. Generalized mdps provide a foundation for the use of reinforcement learning in mdps and games, as well in risk-sensitive reinforcement learning <ref> [8] </ref>, exploration-sensitive reinforcement learning [11], reinforcement learning in simultaneous-action games [13], and other models. <p> Markov games [4] max a or min a f (x; a) P risk-sensitive mdps <ref> [8] </ref> max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) exploration-sens. mdps [11] max 2P 0 P P Markov games [13] max A min b P P information-state mdp [16] max a f (x; a) P Table 1: Some reinforcement-learning scenarios and their specification as generalized Markov decision <p> ff t (x t ; a t ; b t ) @ r t + fl a;b 1 where O a;b 2 (A) b2B a2A The results of the previous section prove that this rule converges to the optimal Q function under the proper conditions. 10 4.3 RISK-SENSITIVE MODELS Heger <ref> [8] </ref> described an optimality criterion for mdps in which only the worst possible value of the next state makes a contribution to the value of a state.
Reference: [9] <author> Matthias Heger. </author> <title> The loss from imperfect value functions in expectation-based and minimax-based tasks. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> In preparation. </note>
Reference-contexts: Other Results We have derived a collection of results [28] for the generalized mdp model that demonstrate its general applicability: the Bellman equations can be solved by value iteration; a myopic policy with respect to an approximately optimal value function gives an approximately optimal policy <ref> [34, 9] </ref>; when N has a particular "maximization" property, policy iteration converges to the optimal value function; and, for models with finite state and action spaces, both value iteration and policy iteration identify optimal policies in pseudopolynomial time.
Reference: [10] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6), </volume> <month> November </month> <year> 1994. </year>
Reference-contexts: Both model-free (direct) methods, such as Q-learning [33], and model-based (indirect) methods, such as prioritized sweeping [15] and DYNA [26], have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref> [33, 31, 10, 7] </ref>. As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. <p> Q-learning converges to the optimal Q function under the proper conditions <ref> [33, 31, 10] </ref>. 2.2 ALTERNATING MARKOV GAMES In alternating Markov games, two players take turns issuing actions to try to maximize their own expected discounted total reward. <p> Related Work The work presented here is closely related to several previous research efforts. Szepesvari [27] described a related generalized reinforcement-learning model, and presented conditions under which there is an optimal (stationary) policy that is myopic with respect to the optimal value function. Jaakkola, Jordan, and Singh <ref> [10] </ref> and Tsitsiklis [31] developed the connection between stochastic-approximation theory and reinforcement learning in mdps. Our work is similar in spirit to that of Jaakkola, et al.
Reference: [11] <author> George H. John. </author> <title> When the best move isn't optimal: Q-learning with exploration. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1995. </year> <note> Available on the web. </note>
Reference-contexts: Generalized mdps provide a foundation for the use of reinforcement learning in mdps and games, as well in risk-sensitive reinforcement learning [8], exploration-sensitive reinforcement learning <ref> [11] </ref>, reinforcement learning in simultaneous-action games [13], and other models. <p> Markov games [4] max a or min a f (x; a) P risk-sensitive mdps [8] max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) exploration-sens. mdps <ref> [11] </ref> max 2P 0 P P Markov games [13] max A min b P P information-state mdp [16] max a f (x; a) P Table 1: Some reinforcement-learning scenarios and their specification as generalized Markov decision processes. <p> The proof is based on estimating the Q-learning algorithm from above by an appropriate process where the Q function is updated only if the received experience tuple is an extremity according to the optimality equation; details are given in the extended paper [28]. 4.4 EXPLORATION-SENSITIVE MODELS John <ref> [11] </ref> considered the implications of insisting that reinforcement-learning agents keep exploring forever; he found that better learning performance can be achieved if the Q-learning rule is changed to incorporate the condition of persistent exploration.
Reference: [12] <author> R. E. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 189-211, </pages> <year> 1990. </year>
Reference-contexts: Like Q-learning, this learning algorithm is a generalization of Korf's <ref> [12] </ref> LRTA* algorithm for stochastic environments.
Reference: [13] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 14 </pages>
Reference-contexts: Generalized mdps provide a foundation for the use of reinforcement learning in mdps and games, as well in risk-sensitive reinforcement learning [8], exploration-sensitive reinforcement learning [11], reinforcement learning in simultaneous-action games <ref> [13] </ref>, and other models. <p> Markov games [4] max a or min a f (x; a) P risk-sensitive mdps [8] max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) exploration-sens. mdps [11] max 2P 0 P P Markov games <ref> [13] </ref> max A min b P P information-state mdp [16] max a f (x; a) P Table 1: Some reinforcement-learning scenarios and their specification as generalized Markov decision processes. <p> An equivalent set of equations can be written with a stochastic choice for the minimizer, and also with the roles of the maximizer and minimizer reversed. The Q-learning update rule for Markov games <ref> [13] </ref> given step t experience hx t ; a t ; b t ; y t ; r t i has the form Q t+1 (x t ; a t ; b t ) := (1 ff t (x t ; a t ; b t ))Q t (x t ;
Reference: [14] <author> Sridhar Mahadevan. </author> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: The results in this paper primarily concern reinforcement-learning in contractive models (fl &lt; 1 or all-policies-proper), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion <ref> [22, 14] </ref>. It would be interesting to develop a TD () algorithm [25] for generalized mdps; this has already been done for mdps [17].
Reference: [15] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <year> 1993. </year>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Both model-free (direct) methods, such as Q-learning [33], and model-based (indirect) methods, such as prioritized sweeping <ref> [15] </ref> and DYNA [26], have been explored and many have been shown to converge to optimal value functions under the proper conditions [33, 31, 10, 7]. As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. <p> Although Q-learning shows that optimal value functions can be estimated without ever explicitly learning R and P , learning R and P makes more efficient use of experience at the expense of additional storage and computation <ref> [15] </ref>. The parameters of R and P can be learned from experience by keeping statistics for each state-action pair on the expected reward and the proportion of transitions to each next state.
Reference: [16] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Markov games [4] max a or min a f (x; a) P risk-sensitive mdps [8] max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) exploration-sens. mdps [11] max 2P 0 P P Markov games [13] max A min b P P information-state mdp <ref> [16] </ref> max a f (x; a) P Table 1: Some reinforcement-learning scenarios and their specification as generalized Markov decision processes.
Reference: [17] <author> Jing Peng and Ronald J. Williams. </author> <title> Incremental multi-step Q-learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 226-232, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It would be interesting to develop a TD () algorithm [25] for generalized mdps; this has already been done for mdps <ref> [17] </ref>. Theorem 1 is not restricted to finite state spaces, and it might be valuable to prove the convergence of a reinforcement-learning algorithm for a infinite state-space model.
Reference: [18] <author> Martin L. Puterman. </author> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: A discount parameter 0 fl &lt; 1 controls the degree to which future rewards are significant compared to immediate rewards. The theory of Markov decision processes can be used as a theoretical foundation for important results concerning this reinforcement-learning scenario [1]. A (finite) Markov decision process (mdp) <ref> [18] </ref> is defined by the tuple hS; A; P; Ri, where S represents a finite set of states, A a finite set of actions, P a transition function, and R a reward function. <p> These results are well established; proofs of the unattributed claims can be found in Puterman's mdp book <ref> [18] </ref>. The ultimate target of learning is an optimal policy. A policy is some function that tells the agent which actions should be chosen under which circumstances.
Reference: [19] <author> Carlos Ribeiro and Csaba Szepesvari. </author> <title> Q-learning with a spreading activiation rule. </title> <note> Submitted to ML'96, </note> <year> 1996. </year>
Reference-contexts: The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for undiscounted "all-policies-proper" mdps [3], a soft state aggregation learning rule [24], and a "spreading" learning rule <ref> [19] </ref> are given in an extended version of this paper [28]. 4.2 Q-LEARNING FOR MARKOV GAMES Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step.
Reference: [20] <author> H. Robbins and S. Monro. </author> <title> A stochastic approximation method. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference-contexts: For example, the theorem makes the convergence of Q-learning a consequence of the classical Robbins-Monro theorem <ref> [20] </ref>. 4 APPLICATIONS This section makes use of Theorem 1 to prove the convergence of various reinforcement learning algorithms. 8 4.1 GENERALIZED Q-LEARNING FOR EXPECTED VALUE MODELS Consider the family of finite state and action generalized mdps defined by the Bellman equations V fl (x) = a R (x; a) + <p> equal to R (x t ; a t ), * the learning rates are decayed so that P P x; a t = a)ff t (x; a) 2 &lt; 1 with probability 1 uniformly over X fi A 2 , then a standard result from the theory of stochastic approximation <ref> [20] </ref> states that T t approximates T with probability 1 uniformly over X fi A. That is, this method of using a decayed, exponentially weighted average correctly computes the average one-step reward.
Reference: [21] <author> Nicol N. Schraudolph, Peter Dayan, and Terrence J. Sejnowski. </author> <title> Using the TD() algorithm to learn an evaluation function for the game of Go. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. A great deal of reinforcement-learning research has been directed to the problem of solving two-player games <ref> [29, 30, 21, 4] </ref>, for example, and the reinforcement-learning algorithms for solving mdps and their convergence proofs do not apply directly to games.
Reference: [22] <author> Anton Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 298-305, </pages> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The results in this paper primarily concern reinforcement-learning in contractive models (fl &lt; 1 or all-policies-proper), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion <ref> [22, 14] </ref>. It would be interesting to develop a TD () algorithm [25] for generalized mdps; this has already been done for mdps [17].
Reference: [23] <author> L.S. Shapley. </author> <title> Stochastic games. </title> <booktitle> Proceedings of the National Academy of Sciences of the United States of America, </booktitle> <volume> 39 </volume> <pages> 1095-1100, </pages> <year> 1953. </year>
Reference-contexts: When 0 fl &lt; 1, these equations have a unique solution and can be solved by successive-approximation methods <ref> [23] </ref>. In addition, 2 we show in this paper that the natural extension of several reinforcement-learning algorithms for mdps converge to optimal value functions in two-player games. <p> The basic model was developed by Shap-ley <ref> [23] </ref> and is defined by the tuple hS; A; B; P; Ri and discount factor fl.
Reference: [24] <author> S.P. Singh, T. Jaakkola, and M.I. Jordan. </author> <title> Reinforcement learning with soft state aggregation. </title> <booktitle> In Proceedings of Neural Information Processing Systems, </booktitle> <year> 1995. </year>
Reference-contexts: The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for undiscounted "all-policies-proper" mdps [3], a soft state aggregation learning rule <ref> [24] </ref>, and a "spreading" learning rule [19] are given in an extended version of this paper [28]. 4.2 Q-LEARNING FOR MARKOV GAMES Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step.
Reference: [25] <author> Richard S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: When 0 fl &lt; 1 and N L are non-expansions, the generalized Bellman equations have a unique optimal solution, and therefore, the optimal value function is well defined. 5 model/example reference N L disc. exp. mdps [33] max a f (x; a) P exp. return of <ref> [25] </ref> P P alt. <p> The results in this paper primarily concern reinforcement-learning in contractive models (fl &lt; 1 or all-policies-proper), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion [22, 14]. It would be interesting to develop a TD () algorithm <ref> [25] </ref> for generalized mdps; this has already been done for mdps [17]. Theorem 1 is not restricted to finite state spaces, and it might be valuable to prove the convergence of a reinforcement-learning algorithm for a infinite state-space model.
Reference: [26] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, TX, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Both model-free (direct) methods, such as Q-learning [33], and model-based (indirect) methods, such as prioritized sweeping [15] and DYNA <ref> [26] </ref>, have been explored and many have been shown to converge to optimal value functions under the proper conditions [33, 31, 10, 7]. As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps.
Reference: [27] <author> Csaba Szepesvari. </author> <title> General framework for reinforcement learning. </title> <booktitle> In Proceedings of ICANN'95 Paris, </booktitle> <year> 1995. </year>
Reference-contexts: Related Work The work presented here is closely related to several previous research efforts. Szepesvari <ref> [27] </ref> described a related generalized reinforcement-learning model, and presented conditions under which there is an optimal (stationary) policy that is myopic with respect to the optimal value function. Jaakkola, Jordan, and Singh [10] and Tsitsiklis [31] developed the connection between stochastic-approximation theory and reinforcement learning in mdps.
Reference: [28] <author> Csaba Szepesvari and Michael L. Littman. </author> <title> Generalized markov decision processes: </title> <journal> Dynamic-programming and reinforcement-learning algorithms. </journal> <note> In preparation, 1996. 15 </note>
Reference-contexts: The theorem is proven in an extended version of this paper <ref> [28] </ref>. We next describe some of the intuition behind the statement of the theorem and its conditions. <p> The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for undiscounted "all-policies-proper" mdps [3], a soft state aggregation learning rule [24], and a "spreading" learning rule [19] are given in an extended version of this paper <ref> [28] </ref>. 4.2 Q-LEARNING FOR MARKOV GAMES Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step. The basic model was developed by Shap-ley [23] and is defined by the tuple hS; A; B; P; Ri and discount factor fl. <p> The proof is based on estimating the Q-learning algorithm from above by an appropriate process where the Q function is updated only if the received experience tuple is an extremity according to the optimality equation; details are given in the extended paper <ref> [28] </ref>. 4.4 EXPLORATION-SENSITIVE MODELS John [11] considered the implications of insisting that reinforcement-learning agents keep exploring forever; he found that better learning performance can be achieved if the Q-learning rule is changed to incorporate the condition of persistent exploration. <p> Other Results We have derived a collection of results <ref> [28] </ref> for the generalized mdp model that demonstrate its general applicability: the Bellman equations can be solved by value iteration; a myopic policy with respect to an approximately optimal value function gives an approximately optimal policy [34, 9]; when N has a particular "maximization" property, policy iteration converges to the optimal
Reference: [29] <author> Gerald Tesauro. </author> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 58-67, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. A great deal of reinforcement-learning research has been directed to the problem of solving two-player games <ref> [29, 30, 21, 4] </ref>, for example, and the reinforcement-learning algorithms for solving mdps and their convergence proofs do not apply directly to games.
Reference: [30] <author> Sebastian Thrun. </author> <title> Learning to play the game of chess. </title> <booktitle> In Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. A great deal of reinforcement-learning research has been directed to the problem of solving two-player games <ref> [29, 30, 21, 4] </ref>, for example, and the reinforcement-learning algorithms for solving mdps and their convergence proofs do not apply directly to games.
Reference: [31] <author> John N. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3), </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: Both model-free (direct) methods, such as Q-learning [33], and model-based (indirect) methods, such as prioritized sweeping [15] and DYNA [26], have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref> [33, 31, 10, 7] </ref>. As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. <p> Q-learning converges to the optimal Q function under the proper conditions <ref> [33, 31, 10] </ref>. 2.2 ALTERNATING MARKOV GAMES In alternating Markov games, two players take turns issuing actions to try to maximize their own expected discounted total reward. <p> Szepesvari [27] described a related generalized reinforcement-learning model, and presented conditions under which there is an optimal (stationary) policy that is myopic with respect to the optimal value function. Jaakkola, Jordan, and Singh [10] and Tsitsiklis <ref> [31] </ref> developed the connection between stochastic-approximation theory and reinforcement learning in mdps. Our work is similar in spirit to that of Jaakkola, et al.
Reference: [32] <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK, </address> <year> 1989. </year>
Reference-contexts: The method of Q-learning <ref> [32] </ref> uses experience to estimate the optimal value function without ever explicitly approximating R and P .
Reference: [33] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Both model-free (direct) methods, such as Q-learning <ref> [33] </ref>, and model-based (indirect) methods, such as prioritized sweeping [15] and DYNA [26], have been explored and many have been shown to converge to optimal value functions under the proper conditions [33, 31, 10, 7]. As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. <p> Both model-free (direct) methods, such as Q-learning [33], and model-based (indirect) methods, such as prioritized sweeping [15] and DYNA [26], have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref> [33, 31, 10, 7] </ref>. As we mentioned before, not all reinforcement-learning scenarios of interest can be modeled as mdps. <p> Q-learning converges to the optimal Q function under the proper conditions <ref> [33, 31, 10] </ref>. 2.2 ALTERNATING MARKOV GAMES In alternating Markov games, two players take turns issuing actions to try to maximize their own expected discounted total reward. <p> The N operator defines how an optimal agent should choose actions. When 0 fl &lt; 1 and N L are non-expansions, the generalized Bellman equations have a unique optimal solution, and therefore, the optimal value function is well defined. 5 model/example reference N L disc. exp. mdps <ref> [33] </ref> max a f (x; a) P exp. return of [25] P P alt.
Reference: [34] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> November </month> <year> 1993. </year> <month> 16 </month>
Reference-contexts: Other Results We have derived a collection of results [28] for the generalized mdp model that demonstrate its general applicability: the Bellman equations can be solved by value iteration; a myopic policy with respect to an approximately optimal value function gives an approximately optimal policy <ref> [34, 9] </ref>; when N has a particular "maximization" property, policy iteration converges to the optimal value function; and, for models with finite state and action spaces, both value iteration and policy iteration identify optimal policies in pseudopolynomial time.
References-found: 34

