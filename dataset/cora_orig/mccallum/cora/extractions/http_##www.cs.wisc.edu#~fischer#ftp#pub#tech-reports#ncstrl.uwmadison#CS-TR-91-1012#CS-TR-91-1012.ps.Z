URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-91-1012/CS-TR-91-1012.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-91-1012/
Root-URL: http://www.cs.wisc.edu
Title: Comparison of Hardware and Software Cache Coherence Schemes  
Author: Sarita V. Adve, Vikram S. Adve, Mark D. Hill, Mary K. Vernon 
Address: Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: We use mean value analysis models to compare representative hardware and software cache coherence schemes for a large-scale shared-memory system. Our goal is to identify the workloads for which either of the schemes is significantly better. Our methodology improves upon previous analytical studies and complements previous simulation studies by developing a common high-level workload model that is used to derive separate sets of low-level workload parameters for the two schemes. This approach allows an equitable comparison of the two schemes for a specific workload. Our results show that software schemes are comparable (in terms of processor efficiency) to hardware schemes for a wide class of programs. The only cases for which software schemes perform significantly worse than hardware schemes are when there is a greater than 15% reduction in hit rate due to inaccurate prediction of memory access conflicts, or when there are many writes in the program that are not executed at runtime. For relatively well-structured and deterministic programs, on the other hand, software schemes perform significantly better than hardware schemes. Keywords: hardware cache coherence, software cache coherence, mean value analysis, workload model 
Abstract-found: 1
Intro-found: 1
Reference: [ASH88] <author> A. AGARWAL, R. SIMONI, M. HOROWITZ and J. HENNESSY, </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> Proc. 15th Annual Intl. Symp. on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> 280-289. </pages>
Reference-contexts: 1. Introduction In shared-memory systems that allow shared data to be cached, some mechanism is required to keep the caches coherent. Hardware snooping protocols [ArB86] are impractical for large systems because they rely on a broadcast medium to maintain coherence. Hardware directory protocols <ref> [ASH88] </ref> can be used with a large number of processors, but they are complex to design and implement. An alternative to hardware cache coherence is the use of software techniques to keep caches coherent, as in Cedar [KDL86] and RP3 [BMW85]. <p> We compare a software coherence scheme similar to one proposed by Cytron et al. [CKM88] to a hardware directory-based Dir i B protocol <ref> [ASH88] </ref> for large-scale systems. Our conclusions also hold for the version control and timestamp schemes, as discussed in Sections 5 and 6. <p> and processor allocation. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c For hardware coherence, we assume a simple directory-based Dir i B protocol similar to the ones described by Agarwal et al. <ref> [ASH88] </ref>.
Reference: [ArB86] <author> J. ARCHIBALD and J. BAER, </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model, </title> <journal> ACM Trans. on Computer Systems 4, </journal> <month> 4 (November </month> <year> 1986), </year> <pages> 273-298. </pages>
Reference-contexts: 1. Introduction In shared-memory systems that allow shared data to be cached, some mechanism is required to keep the caches coherent. Hardware snooping protocols <ref> [ArB86] </ref> are impractical for large systems because they rely on a broadcast medium to maintain coherence. Hardware directory protocols [ASH88] can be used with a large number of processors, but they are complex to design and implement. <p> However, they assume perfect compile-time analysis of memory dependencies, including correct prediction of all conditional branches, which is optimistic for the software scheme. Owicki and Agarwal [OwA89] used an analytical model to compare a software scheme [CKM88] against the Dragon hardware snooping protocol <ref> [ArB86] </ref> for bus-based systems. They conclude that the software scheme generally shows lower processor efficiencies than the hardware scheme and is more sensitive to the amount of sharing in the workload.
Reference: [BMW85] <author> W. C. BRANTLEY, K. P. MCAULIFFE and J. WEISS, </author> <title> RP3 Process-Memory Element, </title> <booktitle> Intl. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1985, </year> <pages> 772-781. </pages>
Reference-contexts: Hardware directory protocols [ASH88] can be used with a large number of processors, but they are complex to design and implement. An alternative to hardware cache coherence is the use of software techniques to keep caches coherent, as in Cedar [KDL86] and RP3 <ref> [BMW85] </ref>. Software cache coherence is attractive because the overhead of detecting stale data is transferred from runtime to compile time, and the design complexity is transferred from hardware to software.
Reference: [ChV88] <author> J. CHEONG and A. V. VEIDENBAUM, </author> <title> A Cache Coherence Scheme With Fast Selective Invalidation, </title> <booktitle> Proc. of the 15th Annual Intl. Symp. on Computer Architecture 16, </booktitle> <month> 2 (June </month> <year> 1988), </year> <pages> 299-307. - 18 </pages> - - -- 
Reference-contexts: Although we have specifically modeled the scheme described by Cytron et al., we believe our results apply equally to the Fast Selective Invalidation scheme <ref> [ChV88] </ref> and to the timestamp based [MiB90b] and version control schemes [Che90]. The Fast Selective Invalidation scheme has been shown to be very similar to the Cytron et al. scheme in terms of compile time analysis and exploiting temporal locality.
Reference: [Che90] <author> H. CHEONG, </author> <title> Compiler-Directed Cache Coherence Strategies for Large-Scale Shared-Memory Multiprocessor Systems, </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Electrical Engineering, University of Illinois, Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: In a previous study comparing the performance of hardware and software coherence, Cheong and Veiden baum used a parallelizing compiler to implement three different software coherence schemes <ref> [Che90] </ref>. For hhhhhhhhhhhhhhhhhh This work is supported in part by the National Science Foundation (DCR-8451405, MIPS-8957278 and CCR-8902536), A.T.& T. <p> Although we have specifically modeled the scheme described by Cytron et al., we believe our results apply equally to the Fast Selective Invalidation scheme [ChV88] and to the timestamp based [MiB90b] and version control schemes <ref> [Che90] </ref>. The Fast Selective Invalidation scheme has been shown to be very similar to the Cytron et al. scheme in terms of compile time analysis and exploiting temporal locality.
Reference: [CKM88] <author> R. CYTRON, S. KARLOVSKY and K. P. MCAULIFFE, </author> <title> Automatic Management of Programmable Caches, </title> <booktitle> Proc. 1988 Intl. Conf. on Parallel Processing, </booktitle> <address> University Park PA, </address> <month> August </month> <year> 1988, </year> <month> II-229-238. </month>
Reference-contexts: They also report comparable hit ratios for the two schemes. However, they assume perfect compile-time analysis of memory dependencies, including correct prediction of all conditional branches, which is optimistic for the software scheme. Owicki and Agarwal [OwA89] used an analytical model to compare a software scheme <ref> [CKM88] </ref> against the Dragon hardware snooping protocol [ArB86] for bus-based systems. They conclude that the software scheme generally shows lower processor efficiencies than the hardware scheme and is more sensitive to the amount of sharing in the workload. <p> From the high-level workload model, we derive two sets of low-level parameters that are used as inputs to queueing network models of the systems with hardware and software coherence. We compare a software coherence scheme similar to one proposed by Cytron et al. <ref> [CKM88] </ref> to a hardware directory-based Dir i B protocol [ASH88] for large-scale systems. Our conclusions also hold for the version control and timestamp schemes, as discussed in Sections 5 and 6. <p> Since the use of RFO could significantly change the performance of Dir i B relative to software coherence, we model Dir i B without and with RFO. For software coherence, we model a scheme similar to the one proposed by Cytron et al. <ref> [CKM88] </ref>. The compiler inserts an invalidate instruction before each potential access to stale data, causing the data to be retrieved from main memory.
Reference: [EgK88] <author> S. J. EGGERS and R. H. KATZ, </author> <title> A Characterization of Sharing in Parallel Programs and its Application to Coherency Protocol Evaluation, </title> <booktitle> Proc. 15th Annual Intl. Conf. on Computer Architecture, </booktitle> <address> Honolulu, HA, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: The system parameters (except cons) are held fixed throughout our experiments, and the values are given in Table 4.1. The values of f data and f pvt were chosen to reflect the findings of previous work characterizing parallel applications. <ref> [EgK88, OwA89] </ref>. Except for loc hw and loc sw , we believe that varying the other parameters will not affect the conclusions of our study.
Reference: [EgK89] <author> S. J. EGGERS and R. H. KATZ, </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs, </title> <booktitle> Proc. 3rd Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: Furthermore, they assume the same miss ratio (0.4-2.4%) for private and shared data accesses in the hardware scheme, which is an optimistic assumption as shown in studies of sharing behavior of parallel programs <ref> [EgK89, WeG89] </ref>. Our analysis improves on the work by Owicki and Agarwal and complements the simulation studies by quantifying coherence protocol performance as a function of parameters that characterize parallel program behavior and compile-time analysis.
Reference: [KEW85] <author> R. H. KATZ, S. J. EGGERS, D. A. WOOD, C. L. PERKINS and R. G. SHELDON, </author> <title> Implementing a Cache Consistency Protocol, </title> <booktitle> Proc. 12th Annual Intl. Symp. on Computer Architecture, </booktitle> <address> Boston, </address> <month> June </month> <year> 1985, </year> <pages> 276-283. </pages>
Reference-contexts: This performance difference can be reduced if hardware supports a Read-For-Ownership (RFO) operation <ref> [KEW85] </ref>. RFO is a read operation that procures the requested line in modified state in the processor cache to avoid a directory access on a subsequent write.
Reference: [KDL86] <author> D. J. KUCK, E. S. DAVIDSON, D. H. LAWRIE and A. H. SAMEH, </author> <title> Parallel Supercomputing Today and the Cedar Approach, </title> <note> Science 231(28 February 1986), </note> . 
Reference-contexts: Hardware directory protocols [ASH88] can be used with a large number of processors, but they are complex to design and implement. An alternative to hardware cache coherence is the use of software techniques to keep caches coherent, as in Cedar <ref> [KDL86] </ref> and RP3 [BMW85]. Software cache coherence is attractive because the overhead of detecting stale data is transferred from runtime to compile time, and the design complexity is transferred from hardware to software.
Reference: [MeSar] <author> J. M. MELLOR-CRUMMEY and M. L. SCOTT, </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <note> to appear. </note>
Reference-contexts: Our workload model includes parameters to account for this factor. Finally, all the software coherence schemes proposed so far require synchronization variables be uncache-able, whereas many hardware schemes allows such variables to be cached. In the future, the effects of this difference can be mitigated by software techniques <ref> [MeSar] </ref> that make locks appear more like ordinary shared data. For this reason, we do not model synchronization directly. 3. The High-Level Workload Model Our high-level workload model partitions shared data objects into classes very similar to those defined by Weber and Gupta [WeG89]. <p> Table 3.1 summarizes the high-level workload parameters. (The column of values gives the ranges used in our experiments.) As discussed earlier, we do not model synchronization objects separately, but expect them to behave like ordinary shared data once contention-reducing techniques have been applied <ref> [MeSar] </ref>. The parameters for mostly-read, frequently read-written and migratory data are further discussed below. These parameters are designed to capture the sharing behavior of the particular data class, so as to reflect the performance considera tions discussed in Section 2. hhhhhhhhhhhhhhhhhh 1.
Reference: [MiB90a] <author> S. L. MIN and J. BAER, </author> <title> A Performance Comparison of Directory-based and Timestamp-based Cache Coherence Schemes, </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <year> 1990, </year> <month> I305-I311. </month>
Reference-contexts: Min and Baer <ref> [MiB90a] </ref> simulated a timestamp-based software scheme and a hardware directory scheme using traces from three programs. They also report comparable hit ratios for the two schemes. However, they assume perfect compile-time analysis of memory dependencies, including correct prediction of all conditional branches, which is optimistic for the software scheme.
Reference: [MiB90b] <author> S. L. MIN and J. BAER, </author> <title> Design and Analysis of a Scalable Cache Coherence Scheme Based on Clocks and Timestamps, </title> <note> Submitted for Publication, </note> <year> 1990. </year>
Reference-contexts: Although we have specifically modeled the scheme described by Cytron et al., we believe our results apply equally to the Fast Selective Invalidation scheme [ChV88] and to the timestamp based <ref> [MiB90b] </ref> and version control schemes [Che90]. The Fast Selective Invalidation scheme has been shown to be very similar to the Cytron et al. scheme in terms of compile time analysis and exploiting temporal locality.
Reference: [OwA89] <author> S. OWICKI and A. AGARWAL, </author> <title> Evaluating the Performance of Software Cache Coherency, </title> <booktitle> Proc. 3rd Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: They also report comparable hit ratios for the two schemes. However, they assume perfect compile-time analysis of memory dependencies, including correct prediction of all conditional branches, which is optimistic for the software scheme. Owicki and Agarwal <ref> [OwA89] </ref> used an analytical model to compare a software scheme [CKM88] against the Dragon hardware snooping protocol [ArB86] for bus-based systems. They conclude that the software scheme generally shows lower processor efficiencies than the hardware scheme and is more sensitive to the amount of sharing in the workload. <p> The system parameters (except cons) are held fixed throughout our experiments, and the values are given in Table 4.1. The values of f data and f pvt were chosen to reflect the findings of previous work characterizing parallel applications. <ref> [EgK88, OwA89] </ref>. Except for loc hw and loc sw , we believe that varying the other parameters will not affect the conclusions of our study.
Reference: [VLZ88] <author> M. K. VERNON, E. D. LAZOWSKA and J. ZAHORJAN, </author> <title> An Accurate and Efficient Performance Analysis Technique for Multiprocessor Snooping Cache-Consistency Protocols, </title> <booktitle> Proc. 15th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: However, software schemes may perform poorly because compile-time analysis may need to be conservative, leading to unnecessary cache misses and main memory updates. In this paper, we use approximate Mean Value Analysis <ref> [VLZ88] </ref> to compare the performance of a representative software scheme with a directory-based hardware scheme on a large-scale shared-memory sys tem. In a previous study comparing the performance of hardware and software coherence, Cheong and Veiden baum used a parallelizing compiler to implement three different software coherence schemes [Che90]. <p> These parameters are derived from the high-level workload model as explained in Section 4.2. The MVA models used to calculate system performance are similar to models developed by others for the analysis of different types of processor-memory interconnects <ref> [VLZ88, WiE90] </ref>. The detailed equations of the model are given in Appendix B. These models can be solved very quickly and have been shown to have high - 8 - - -- accuracy for studying similar design issues.
Reference: [WeG89] <author> W. WEBER and A. GUPTA, </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors, </title> <booktitle> Proc. 3rd Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: Furthermore, they assume the same miss ratio (0.4-2.4%) for private and shared data accesses in the hardware scheme, which is an optimistic assumption as shown in studies of sharing behavior of parallel programs <ref> [EgK89, WeG89] </ref>. Our analysis improves on the work by Owicki and Agarwal and complements the simulation studies by quantifying coherence protocol performance as a function of parameters that characterize parallel program behavior and compile-time analysis. <p> For this reason, we do not model synchronization directly. 3. The High-Level Workload Model Our high-level workload model partitions shared data objects into classes very similar to those defined by Weber and Gupta <ref> [WeG89] </ref>. We use five classes, namely, passively-shared objects, mostly-read objects, frequently read-written objects, migratory objects, and synchronization objects. Passively-shared objects include read-only data as well as the portions of shared read-write objects that are exclusively accessed by a single processor . <p> Furthermore, we assume that n MR is large enough that broadcast is required for invalidations. This is consistent with Weber and Gupta's findings, which showed that writes to mostly-read data caused an average of 3 to 4 invalidates even for 16 processor systems <ref> [WeG89] </ref>. Frequently Read-Written Data. The contribution of this class to the probability of read and write misses is calculated in the same manner as for mostly-read data (when RFO is not included).
Reference: [WiE90] <author> D. L. WILLICK and D. L. EAGER, </author> <title> An Analytic Model of Multistage Interconnection Networks, </title> <booktitle> Proc. ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems 18, </booktitle> <month> 1 (May </month> <year> 1990), </year> <pages> 192-202. </pages> - -- 
Reference-contexts: These parameters are derived from the high-level workload model as explained in Section 4.2. The MVA models used to calculate system performance are similar to models developed by others for the analysis of different types of processor-memory interconnects <ref> [VLZ88, WiE90] </ref>. The detailed equations of the model are given in Appendix B. These models can be solved very quickly and have been shown to have high - 8 - - -- accuracy for studying similar design issues.
References-found: 17

