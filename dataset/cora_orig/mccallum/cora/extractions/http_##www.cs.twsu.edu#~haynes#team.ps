URL: http://www.cs.twsu.edu/~haynes/team.ps
Refering-URL: http://adept.cs.twsu.edu/~thomas/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: [haynes,sandip,dschoen,rogerw]@euler.mcs.utulsa.edu  
Title: Evolving a Team  
Author: Thomas Haynes, Sandip Sen, Dale Schoenefeld Roger Wainwright 
Address: Tulsa  
Affiliation: Department of Mathematical Computer Sciences, The University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Peter J. Angeline and Jordan B. Pollack. </author> <title> Competitive environments evolve better solutions for complex tasks. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 264-278. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: In this work we examine the rise of cooperation strategies without implicit communication. This is achieved by having each predator agent being controlled by its own program. Such a system solves a cooperative co-evolution problem as opposed to a competitive co-evolution problem as described in <ref> [1, 7, 14] </ref>. 1 We believe that cooperative co-evolution provides opportunities to produce solutions to problems that can not be solved with implicit communication.
Reference: [2] <author> M. Benda, V. Jagannathan, and R. Dodhiawalla. </author> <title> On optimal cooperation of knowledge sources. </title> <type> Technical Report BCS-G2010-28, </type> <institution> Boeing AI Center, Boeing Computer Services, Bellevue, WA, </institution> <month> August </month> <year> 1985. </year>
Reference-contexts: We can then measure their efficiency and effectiveness by some criteria relevant to the domain. Populations of such structures are evolved to produce increasingly efficient coordination strategies. We have used the predator-prey pursuit game <ref> [2] </ref> to test our hypothesis that useful coordination strategies can be evolved using the STGP paradigm for non-trivial problems. This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it.
Reference: [3] <editor> Alan H. Bond and Les Gasser. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: In effect, we want to evolve behavioral strategies that guide the actions of agents in a given domain. The identification, design, and implementation of strategies for coordination is a central research issue in the field of Distributed Artificial Intelligence (DAI) <ref> [3] </ref>. Current research techniques in developing coordination strategies are mostly off-line mechanisms that use extensive domain knowledge to design from scratch the most appropriate cooperation strategy. It is nearly impossible to identify or even prove the existence of the best coordination strategy.
Reference: [4] <author> Kenneth A. DeJong. </author> <title> Genetic-algorithm-based learning. </title> <editor> In Y. Kodratoff and R.S. Michalski, editors, </editor> <booktitle> Machine Learning, Volume III. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Alamos, CA, </address> <year> 1990. </year>
Reference-contexts: Each team member always participates in the same team. Thus all of the points it is awarded, for both its individual contribution and the teams contribution, are correctly apportioned to the entire team. This approach is similar to "the Pitt approach" used for evolving Genetic-Based Machine Learning systems <ref> [4] </ref>.
Reference: [5] <author> Les Gasser, Nicolas Rouquette, Randall W. Hill, and John Lieb. </author> <title> Representing and using organizational knowledge in DAI systems. </title> <editor> In Les Gasser and Michael N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence, </booktitle> <pages> pages 55-78. </pages> <publisher> Pitman, </publisher> <year> 1989. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [5, 10, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies.
Reference: [6] <author> John Grefenstette. </author> <title> Credit assignment in rule discovery systems. </title> <journal> Machine Learning, </journal> 3(2/3):225-246, 1988. 
Reference-contexts: it fair to evenly divide the score? Assuming k members to a team, if the actions of one individual accounted for a large share of the team's score, why should it only get 1 k th of the score? This problem is the same as the credit assignment problem in <ref> [6] </ref>. A modification of this strategy is to deterministically split the population into k sized teams. Thus the first k individuals would always form the first team. The problem with this is that it imposes an artificial ordering on the population.
Reference: [7] <author> Thomas Haynes and Sandip Sen. </author> <title> Evolving behavioral strategies in predators and prey. </title> <booktitle> In IJCAI-95 Workshop on Adaptation and Learning in Multiagent Systems, </booktitle> <year> 1995. </year>
Reference-contexts: In this work we examine the rise of cooperation strategies without implicit communication. This is achieved by having each predator agent being controlled by its own program. Such a system solves a cooperative co-evolution problem as opposed to a competitive co-evolution problem as described in <ref> [1, 7, 14] </ref>. 1 We believe that cooperative co-evolution provides opportunities to produce solutions to problems that can not be solved with implicit communication.
Reference: [8] <author> Thomas Haynes, Roger Wainwright, Sandip Sen, and Dale Schoenefeld. </author> <title> Strongly typed genetic programming in evolving cooperation strategies. </title> <booktitle> In Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <year> 1995. </year> <note> (accepted for publication). </note>
Reference-contexts: It is nearly impossible to identify or even prove the existence of the best coordination strategy. In most cases a coordination strategy is chosen if it is reasonably good. In <ref> [8] </ref>, we presented a new approach for developing coordination strategies for multi-agent problem solving situations, which is different from most of the existing techniques for constructing coordination strategies in two ways: * Strategies for coordination are incrementally constructed by repeatedly solving problems in the domain, i.e., on-line. * We rely on <p> The approach proposed in <ref> [8] </ref> for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm [13], which is an extension of genetic programming (GP) [11].
Reference: [9] <author> Leslie Knight and Sandip Sen. </author> <title> Please: A prototype learning system using genetic algorithms. </title> <booktitle> In Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <year> 1995. </year> <note> (accepted for publication). </note>
Reference-contexts: The Pitt approach bypasses the credit assignment problem, in that rules are only evaluated in the context of a ruleset. A similar mechanism as proposed in this paper has been used to successfully co-evolve a set of prototypes for supervised concept classification problems <ref> [9] </ref>.
Reference: [10] <author> Richard E. Korf. </author> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 183-194, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [5, 10, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies.
Reference: [11] <author> John R. Koza. </author> <title> Genetic Programming, On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The approach proposed in [8] for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm [13], which is an extension of genetic programming (GP) <ref> [11] </ref>. To use the STGP approach for evolving coordination strategies, the strategies are encoded as symbolic expressions (S-expressions) and an evaluation criterion is chosen for evaluating arbitrary S-expressions.
Reference: [12] <author> Ran Levy and Jeffrey S. Rosenschein. </author> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 195-213, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [5, 10, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies.
Reference: [13] <author> David J. Montana. </author> <title> Strongly typed genetic programming. </title> <type> Technical Report 7866, </type> <institution> Bolt Beranek and Newman, Inc., </institution> <month> March 25, </month> <year> 1994. </year>
Reference-contexts: The approach proposed in [8] for developing coordination strategies for multi-agent problems is completely domain independent, and uses the strongly typed genetic programming (STGP) paradigm <ref> [13] </ref>, which is an extension of genetic programming (GP) [11]. To use the STGP approach for evolving coordination strategies, the strategies are encoded as symbolic expressions (S-expressions) and an evaluation criterion is chosen for evaluating arbitrary S-expressions.
Reference: [14] <author> Craig W. Reynolds. </author> <title> Competition, coevolution and the game of tag. </title> <booktitle> In Artificial Life IV. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: In this work we examine the rise of cooperation strategies without implicit communication. This is achieved by having each predator agent being controlled by its own program. Such a system solves a cooperative co-evolution problem as opposed to a competitive co-evolution problem as described in <ref> [1, 7, 14] </ref>. 1 We believe that cooperative co-evolution provides opportunities to produce solutions to problems that can not be solved with implicit communication.
Reference: [15] <author> Larry M. Stephens and Matthias B. Merx. </author> <title> Agent organization as an effector of dai system performance. </title> <booktitle> In Working Papers of the 9th International Workshop on Distributed Artificial Intelligence, </booktitle> <month> September </month> <year> 1989. </year>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [5, 10, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies.
Reference: [16] <author> Larry M. Stephens and Matthias B. Merx. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop, </booktitle> <month> October </month> <year> 1990. </year> <month> 5 </month>
Reference-contexts: This domain involves multiple predator agents trying to capture a prey agent in a grid world by surrounding it. The predator-prey problem has been widely used to test new coordination schemes <ref> [5, 10, 12, 15, 16] </ref>. The problem is easy to describe, but extremely difficult to solve; the performances of even the best manually generated coordination strategies are less than satisfactory. We showed that STGP evolved coordination strategies perform competitively with the best available manually generated strategies.
References-found: 16

