URL: http://theory.lcs.mit.edu/~alex/concur.ps
Refering-URL: http://theory.lcs.mit.edu/~alex/concur.html
Root-URL: 
Phone: 2  
Title: Efficient parallelism vs reliable distribution: a trade-off for concurrent computations  
Author: Paris C. Kanellakis Dimitrios Michailidis and Alex A. Shvartsman 
Address: Box 1910 Providence, RI 02912-1910, USA  30 Porter Road, LJ02/I11 Littleton, MA 01460-1446, USA  
Affiliation: 1 Department of Computer Science, Brown University,  Digital Equipment Corporation,  
Abstract: Concurrent computations should combine efficiency with reliability, where efficiency is usually associated with parallel and reliability with distributed computing. Such a desirable combination is not always possible, because of an intuitive trade-off: efficiency requires removing redundancy from computations whereas reliability requires some redundancy. We survey a spectrum of algorithmic models (from fail-stop, synchronous to asynchronous and from approximate to exact computations) in which reliability is guaranteed with small trade-offs in efficiency. We illustrate a number of cases where optimal trade-offs are achievable. A basic property of all these models, which is of some interest in the study of concurrency, is that "true" read/write concurrency is necessary for fault tolerance. In particular, we outline (from [14]) how algorithms can be designed so that, in each execution, the total "true" concurrency used can be closely related to the faults that can be tolerated.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Ajtai, J. Aspnes, C. Dwork, O. Waarts, </author> <title> "The Competitive Analysis of Wait-Free Algorithms and its Application to the Cooperative Collect Problem", </title> <note> to appear in PODC, </note> <year> 1994. </year>
Reference-contexts: Future developments in asynchronous parallel computation will employ randomization as well as the array of deterministic techniques surveyed here. The work on fault-tolerant and efficient parallel shared memory models has also been applied to distributed message passing models; see <ref> [1, 9, 10] </ref>. In Section 3 we examine an array of algorithms for the Write-All problem. These employ a variety of deterministic techniques and are extensible to the computation of other functions (see Section 6).
Reference: 2. <author> G. B. Adams III, D. P. Agrawal, H. J. Seigel, </author> <title> "A Survey and Comparison of Fault-tolerant Multistage Interconnection Networks", </title> <journal> IEEE Computer, </journal> <volume> 20, 6, </volume> <pages> pp. 14-29, </pages> <year> 1987. </year>
Reference-contexts: Processors and memory are interconnected via a synchronous network [33]. A combining interconnection network well suited for implementing synchronous concurrent reads and writes is in [23] and can be made more reliable by employing redundancy <ref> [2] </ref>. 2.2 Measures of Efficiency We use a generalization of the standard Parallel-time fiProcessors product to measure work of an algorithm when the number of processors performing work fluctuates due to failures or delays [15, 16].
Reference: 3. <author> R. Anderson, H. Woll, </author> <title> "Wait-Free Parallel Algorithms for the Union-Find Problem", </title> <booktitle> Proc. of the 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 370-380, </pages> <year> 1991. </year>
Reference-contexts: The undetectable restart case is equivalent to the most general general model of asynchrony that has received a fair amount of attention in the literature. An elegant deterministic solution for Write-All in this case appeared in <ref> [3] </ref>. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to [7, 8, 13, 21, 22, 25, 26, 27, 29]. <p> The undetectable restart case is equivalent to the most general general model of asynchrony that has received a fair amount of attention in the literature. An elegant deterministic solution for Write-All in this case appeared in <ref> [3] </ref>. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to [7, 8, 13, 21, 22, 25, 26, 27, 29]. <p> In the study of fail-stop prams, we consider three main types of failure-inducing adversaries. These form a hierarchy, based on their power. Note that, each adversary is more powerful than the preceding ones and that the last case can be used to simulate fully asynchronous processors <ref> [3] </ref>. Fail-stop failures: adversary causes processors to stop during the computa tion; there are no restarts. Fail-stop failures, detectable restarts: adversary causes stop failures; subsequently to a failure, the adversary might restart a processor and a restarted processor "knows" of the restart. <p> For example, the model used in [6, 16] can be realized this way. The undetectable restarts adversary can also be realized in a similar way by making the algorithm weaker. For undetectable restarts algorithms have to have identical red and green parts. For example, the fully asynchronous model of <ref> [3] </ref> can be realized this way. The abstract model we are studying can be realized in the architecture in Fig. 1. There are P fail-stop processors [32]. There are Q shared memory cells. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques [31]. <p> One problem with the above approach is that there could be a large number of restarts and a large amount of work. Algorithm V can be combined with algorithm X of the next section or with the asymptotically better algorithm of <ref> [3] </ref> to provide better bounds on work. 3.3 Dynamic Faults, Undetected Restarts, Algorithms X and Y When the failures cannot be detected, it is still possible to achieve sub-quadratic upper bound for any dynamic failure/restart pattern. <p> We present Write-All algorithm X with S = O (N P log 3 2 ) = N P 0:59 [6, 16]. This simple algorithm can be improved to S = O (N P * ) using the method in <ref> [3] </ref>. We present X for its simplicity and in the next section a (possible) deterministic version of [3]. Algorithm X utilizes a progress tree of size N that is traversed by the processors independently, not in synchronized phases. <p> This simple algorithm can be improved to S = O (N P * ) using the method in <ref> [3] </ref>. We present X for its simplicity and in the next section a (possible) deterministic version of [3]. Algorithm X utilizes a progress tree of size N that is traversed by the processors independently, not in synchronized phases. This reflects the local nature of the processor assignment as opposed to the global assignments used in algorithms V and W. <p> Algorithm X could also be useful for the case without restarts, even though its worst-case performance without restarts is no better than algorithm W. A family of randomized Write-All algorithms was presented by Anderson and Woll <ref> [3] </ref>. The main technique in these algorithms is abstracted in Fig. 4. The basic algorithm in [3] is obtained by randomly choosing the permutation in line 03. <p> A family of randomized Write-All algorithms was presented by Anderson and Woll <ref> [3] </ref>. The main technique in these algorithms is abstracted in Fig. 4. The basic algorithm in [3] is obtained by randomly choosing the permutation in line 03. <p> Prior to starting a parallel step simulation, a processor uses binary search to find the newest simulated step. When reading, a processor linearly searches past generations of memory to find the latest written value. In the result below we use the existential algorithm <ref> [3] </ref>. Theorem 21. Any N -processor, log O (1) N -time, Q-memory pram alg. can be determin. executed on a fail-stop P processor crcw pram (P N ) with undetectable restarts, and using shared memory Q log O (1) N .
Reference: 4. <author> Y. Aumann and M.O. Rabin, </author> <title> "Clock Construction in Fully Asynchronous Parallel Systems and PRAM Simulation", </title> <booktitle> in Proc. of the 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 147-156, </pages> <year> 1992. </year>
Reference-contexts: It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to [7, 8, 13, 21, 22, 25, 26, 27, 29]. In the last three years, randomized asynchronous computation has been examined in depth in <ref> [4, 5, 20] </ref>. These analyses involve randomness in a central way. They are mostly about off-line or oblivious adversaries, which cause faults during the computation but pick the times of these faults before the computation. <p> The fail-stop model with undetectable restarts is clearly the most challenging model, both from a technical and from a practical point of view. The simulation result above is considerably weaker than what is achievable for the other models. This is an area where the application of randomized techniques (e.g., <ref> [4, 5, 20] </ref>) is most promising.
Reference: 5. <author> Y. Aumann, Z.M. Kedem, K.V. Palem, M.O. Rabin, </author> <title> "Highly Efficient Asynchronous Execution of Large-Grained Parallel Programs", </title> <booktitle> in Proc. of the 34th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 271-280, </pages> <year> 1993. </year>
Reference-contexts: It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to [7, 8, 13, 21, 22, 25, 26, 27, 29]. In the last three years, randomized asynchronous computation has been examined in depth in <ref> [4, 5, 20] </ref>. These analyses involve randomness in a central way. They are mostly about off-line or oblivious adversaries, which cause faults during the computation but pick the times of these faults before the computation. <p> The fail-stop model with undetectable restarts is clearly the most challenging model, both from a technical and from a practical point of view. The simulation result above is considerably weaker than what is achievable for the other models. This is an area where the application of randomized techniques (e.g., <ref> [4, 5, 20] </ref>) is most promising.
Reference: 6. <author> J. Buss, P.C. Kanellakis, P. Ragde, A.A. Shvartsman, </author> <title> "Parallel algorithms with processor failures and delays", </title> <institution> Brown Univ. TR CS-91-54, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: This synthesis includes most of the models proposed to date (see [18]). It links the work on fail-stop no-restart errors, to fail-stop errors with restarts, where both detectable and undetectable restarts are considered. The detectable restart case has been examined, using a slightly different formalism in <ref> [6, 16] </ref>. It represents an intermediate level on asynchrony for which we have many interesting algorithmic techniques. The undetectable restart case is equivalent to the most general general model of asynchrony that has received a fair amount of attention in the literature. <p> The green part gets executed under normal conditions. If a processor fails then all memory remains intact, but in the subsequent restart the next instruction red part is executed instead of the green part. For example, the model used in <ref> [6, 16] </ref> can be realized this way. The undetectable restarts adversary can also be realized in a similar way by making the algorithm weaker. For undetectable restarts algorithms have to have identical red and green parts. For example, the fully asynchronous model of [3] can be realized this way. <p> We present Write-All algorithm X with S = O (N P log 3 2 ) = N P 0:59 <ref> [6, 16] </ref>. This simple algorithm can be improved to S = O (N P * ) using the method in [3]. We present X for its simplicity and in the next section a (possible) deterministic version of [3]. <p> The performance of algorithm X is characterized as follows: Theorem 9 <ref> [6, 16] </ref>. Algorithm X with P processors solves the Write-All problem of size N (P N ) in the fail-stop restartable model with work S = O (N P log 3 2 ). <p> In addition, there is an adversary that forces algorithm X to perform S = (N P log 3 2 ) work. The algorithm views undetected restarts as delays, and it can be used in the asynchronous model where it has the same work <ref> [6] </ref>. Algorithm X could also be useful for the case without restarts, even though its worst-case performance without restarts is no better than algorithm W. A family of randomized Write-All algorithms was presented by Anderson and Woll [3]. The main technique in these algorithms is abstracted in Fig. 4.
Reference: 7. <author> R. Cole and O. Zajicek, </author> <title> "The APRAM: Incorporating Asynchrony into the PRAM Model," </title> <booktitle> in Proc. of the 1989 ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 170-178, </pages> <year> 1989. </year>
Reference-contexts: An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way.
Reference: 8. <author> R. Cole and O. Zajicek, </author> <title> "The Expected Advantage of Asynchrony," </title> <booktitle> in Proc. 2nd ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 85-94, </pages> <year> 1990. </year>
Reference-contexts: An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way.
Reference: 9. <author> R. DePrisco, A. Mayer, M. Yung, </author> <title> "Time-Optimal Message-Efficient Work Perfor--mance in the Presence of Faults," </title> <note> to appear in PODC, </note> <year> 1994. </year>
Reference-contexts: Future developments in asynchronous parallel computation will employ randomization as well as the array of deterministic techniques surveyed here. The work on fault-tolerant and efficient parallel shared memory models has also been applied to distributed message passing models; see <ref> [1, 9, 10] </ref>. In Section 3 we examine an array of algorithms for the Write-All problem. These employ a variety of deterministic techniques and are extensible to the computation of other functions (see Section 6).
Reference: 10. <author> C. Dwork, J. Halpern, O. Waarts, </author> <title> "Accomplishing Work in the Presence of Failures" in Proc. </title> <booktitle> 11th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 91-102, </pages> <year> 1992. </year>
Reference-contexts: Future developments in asynchronous parallel computation will employ randomization as well as the array of deterministic techniques surveyed here. The work on fault-tolerant and efficient parallel shared memory models has also been applied to distributed message passing models; see <ref> [1, 9, 10] </ref>. In Section 3 we examine an array of algorithms for the Write-All problem. These employ a variety of deterministic techniques and are extensible to the computation of other functions (see Section 6).
Reference: 11. <author> D. Eppstein and Z. Galil, </author> <title> "Parallel Techniques for Combinatorial Computation", </title> <booktitle> Annual Computer Science Review, 3 (1988), </booktitle> <pages> pp. 233-83. </pages>
Reference-contexts: of fail-stop models of parallel computation, define relevant complexity measures and characterize robust algorithms. 2.1 Fail-Stop PRAMs The parallel random access machine (pram) of Fortune and Wyllie [12] combines the simplicity of a ram with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys <ref> [11, 19] </ref> for the rationale behind this model and the fundamental algorithms. We build our models of fail-stop prams as extensions of the pram model. 1. There are Q shared memory cells, and the input of size N Q is stored in the first N cells.
Reference: 12. <author> S. Fortune and J. Wyllie, </author> <title> "Parallelism in Random Access Machines", </title> <booktitle> Proc. the 10th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 114-118, </pages> <year> 1978. </year>
Reference-contexts: it was somewhat surprising when in [15] we demonstrated that it is possible to combine efficiency and fault tolerance for ? Research supported in part by ONR grant N00014-91-J-1613 and by ONR contract N00014-91-J-4052 ARPA Order 8225. many basic algorithms expressed as concurrent-read concurrent-write parallel (crcw) random access machines (prams <ref> [12] </ref>). The [15] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. The fault model was applied to all crcw prams in [22, 34]. <p> present a number of open questions, which delimit the present state-of-the-art. 2 Fault-Tolerant Parallel Computation Models In this section we detail a hierarchy of fail-stop models of parallel computation, define relevant complexity measures and characterize robust algorithms. 2.1 Fail-Stop PRAMs The parallel random access machine (pram) of Fortune and Wyllie <ref> [12] </ref> combines the simplicity of a ram with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys [11, 19] for the rationale behind this model and the fundamental algorithms. We build our models of fail-stop prams as extensions of the pram model. 1. <p> Each processor "knows" its pid and the value of P , i.e., these can be part of its finite state control. 3. The processors that are active all execute synchronously as in the standard pram model <ref> [12] </ref>. <p> The basic assumption has been that: The (P ) auxiliary shared memory is cleared or initialized to some known value. While this is consistent with definitions of pram such as <ref> [12] </ref>, it is nevertheless a requirement that fault-tolerant systems ought to be able to do without. Interestingly there is an efficient deterministic procedure that solves the Write-All problem even when the shared memory is contaminated, i.e., contains arbitrary values, see [36]. Interconnect Issues.
Reference: 13. <author> P. Gibbons, </author> <title> "A More Practical PRAM Model," </title> <booktitle> in Proc. of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 158-168, </pages> <year> 1989. </year>
Reference-contexts: An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way.
Reference: 14. <author> P. C. Kanellakis, D. Michailidis, A. A. Shvartsman, </author> <title> "Controlling Memory Access Concurrency in Efficient Fault-Tolerant Parallel Algorithms", </title> <booktitle> 7th Intl Workshop on Distributed Algorithms, </booktitle> <pages> pp. 99-114, </pages> <year> 1993. </year> <note> An extended version appears as TR CS-94-23, </note> <institution> Brown University. </institution>
Reference-contexts: The fault model was applied to all crcw prams in [22, 34]. It was extended in [16] to include processor restarts, and in [36] to include arbitrary static memory faults, i.e., arbitrary memory initialization, and in <ref> [14] </ref> to include restricted memory access patterns through controlled memory access. Concurrency of reads and writes is an essential feature that accounts for the necessary redundancy so it can be restricted but not eliminated. <p> We assume: 1. Processor faults do not affect memory. 2. Processors can read and write memory concurrently, crcw. These two assumptions are essential for deterministic algorithms with dynamic faults. Static or initial memory can be contaminated [36]. Static or initial processor faults can be handled with erew algorithms <ref> [14] </ref>. For dynamic behavior there is a trade-off between concurrency and fault tolerance, which we overview here (see [14] for the full details). A central algorithmic primitive in our work is the Write-All operation [15]. <p> These two assumptions are essential for deterministic algorithms with dynamic faults. Static or initial memory can be contaminated [36]. Static or initial processor faults can be handled with erew algorithms <ref> [14] </ref>. For dynamic behavior there is a trade-off between concurrency and fault tolerance, which we overview here (see [14] for the full details). A central algorithmic primitive in our work is the Write-All operation [15]. Iterated Write-All forms the basis for the algorithm simulation techniques of [22, 34] and for the memory initialization of [36]. Therefore, improved Write-All solutions lead to improved simulations and memory clearing techniques. <p> Looking at another variation of Write-All , <ref> [14] </ref> consider the case of static processor faults. This is the situation where all the processor faults occur at the beginning of the execution of the Write-All algorithm and no processors are allowed to fail once the algorithm has begun executing. <p> It turns out that this is a much simpler case that does not require any concurrent memory accesses. Algorithm E of <ref> [14] </ref> solves the Write-All problem with static faults optimally with work S = O (N +P 0 log P ), where P 0 P is the number of live processors at the beginning of the execution. Like Z this algorithm makes no assumptions about the initial memory contents. <p> However, while most known solutions for the Write-All problem indeed make heavy use of concurrency, the goal of minimizing concurrent access to shared memory is attainable. 5.1 Minimizing Concurrency: Processor Priority Trees We gave a Write-All algorithm in <ref> [14] </ref> in which we bound the total amount of concurrency used in terms of the number of dynamic processor faults of the actual algorithm run. <p> Whereas unrestricted concurrent writes by p P processors take only one step to update T , a PPT requires as many steps as there are levels in the tree, i.e., O (log p). The following lemma is instrumental in bounding the write concurency. Lemma 14 <ref> [14] </ref>. If algorithm CW is executed in the presence of a failure pattern F , then its write concurrency ! is no more than the number of failures jF j. A Write-All algorithm with controlled write concurrency can be obtained by incorporating PPTs into the four phases of algorithm W. <p> We allow blog P c + 1 steps for each execution of CW, thus slowing down each iteration of the original algorithm W by a O (log P ) factor. The resulting algorithm is known as W CW <ref> [14] </ref>. PPTs are organized and maintained as follows. At the outset of the algorithm and at the beginning of each execution of phase W1 each processor forms a PPT by itself. <p> Merging and compacting two PPTs. Further details of algorithm W CW are provided in <ref> [14] </ref>. The following theorem describes its performance. Theorem 15 [14]. <p> Merging and compacting two PPTs. Further details of algorithm W CW are provided in <ref> [14] </ref>. The following theorem describes its performance. Theorem 15 [14]. <p> This allows processors to communicate without requiring them to know each other's identity. These arrays can be stored in a segment of memory of size P for all the PPTs (see <ref> [14] </ref>). Each processor on levels 0; : : :; i1 is associated with exactly one processor on each of levels i; i + 1; : : :. <p> Algorithm W CR=W : Incorporating algorithms CR/W, CR1, and CR2 along with the techniques of PPT compaction and merging into W results in a Write-All algorithm that controls both read and write concurrency. The details of this algorithm, known as W CR=W , can be found in <ref> [14] </ref>. The following theorem provides bounds on the performance of W CR=W . Theorem 16 [14]. <p> The details of this algorithm, known as W CR=W , can be found in <ref> [14] </ref>. The following theorem provides bounds on the performance of W CR=W . Theorem 16 [14]. Algorithm W CR=W is a robust algorithm for the Write-All problem with S = O (N log N log P +P log P log 2 N= log log N ), read concurrency 7 jF j log N , and write concurrency ! jF j, where 1 P N . <p> By taking advantage of parallel slackness and by clustering the input data into groups of size log N log P , we can obtain a modification of algorithm W CR=W that has a non trivial processor optimality range. The modified algorithm, known as W CR=W , is described in <ref> [14] </ref>. Its performance is characterized by the following theorem: Theorem 17 [14]. <p> The modified algorithm, known as W CR=W , is described in <ref> [14] </ref>. Its performance is characterized by the following theorem: Theorem 17 [14]. Algorithm W opt CR=W is a robust algorithm for the Write-All problem with S = O (N + P log 2 N log 2 P log log N ), write concurrency ! jF j, and read concurrency 7 jF j logN , where 1 P N . <p> The algorithm can be further extended to handle arbitrary initial memory contents <ref> [14] </ref>. It is also possible to reduce the maximum per step memory access concurrency by polylogarithmic factors by utilizing a general pipelining tech-nique. Finally, [14] established a lower bound showing that no robust Write-All algorithm exists with write concurrency ! jF j " for 0 " &lt; 1. 6 General Computations <p> The algorithm can be further extended to handle arbitrary initial memory contents <ref> [14] </ref>. It is also possible to reduce the maximum per step memory access concurrency by polylogarithmic factors by utilizing a general pipelining tech-nique. Finally, [14] established a lower bound showing that no robust Write-All algorithm exists with write concurrency ! jF j " for 0 " &lt; 1. 6 General Computations with Minimal Concurrency In this section we will work our way from the simplest to the most complicated functions with robust solutions. 6.1 Constants, <p> By using algorithm W opt CR=W we obtain efficient PRAM simulations on fail-stop PRAMs whose concurrency depends on the number of failures. In particular, we obtain the following: Theorem 19 <ref> [14] </ref>.
Reference: 15. <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Efficient Parallel Algorithms Can Be Made Robust", </title> <journal> Distributed Computing, </journal> <volume> vol. 5, no. 4, </volume> <pages> pp. 201-217, </pages> <year> 1992; </year> <month> prelim. </month> <title> vers. </title> <booktitle> in Proc. of the 8th ACM PODC, </booktitle> <pages> pp. 211-222, </pages> <year> 1989. </year>
Reference-contexts: Thus, even allowing for some abstraction in the model of parallel computation, it is not obvious that there are any non-trivial fault models that allow near-linear speed-ups. So it was somewhat surprising when in <ref> [15] </ref> we demonstrated that it is possible to combine efficiency and fault tolerance for ? Research supported in part by ONR grant N00014-91-J-1613 and by ONR contract N00014-91-J-4052 ARPA Order 8225. many basic algorithms expressed as concurrent-read concurrent-write parallel (crcw) random access machines (prams [12]). The [15] fault model allows any <p> somewhat surprising when in <ref> [15] </ref> we demonstrated that it is possible to combine efficiency and fault tolerance for ? Research supported in part by ONR grant N00014-91-J-1613 and by ONR contract N00014-91-J-4052 ARPA Order 8225. many basic algorithms expressed as concurrent-read concurrent-write parallel (crcw) random access machines (prams [12]). The [15] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. The fault model was applied to all crcw prams in [22, 34]. <p> Concurrency of reads and writes is an essential feature that accounts for the necessary redundancy so it can be restricted but not eliminated. Also, as shown in <ref> [15] </ref>, it suffices to consider common crcw prams (all concurrent writes are identical) in which the atomically written words need only contain a constant number of bits. The work we survey makes two key assumptions. <p> Static or initial processor faults can be handled with erew algorithms [14]. For dynamic behavior there is a trade-off between concurrency and fault tolerance, which we overview here (see [14] for the full details). A central algorithmic primitive in our work is the Write-All operation <ref> [15] </ref>. Iterated Write-All forms the basis for the algorithm simulation techniques of [22, 34] and for the memory initialization of [36]. Therefore, improved Write-All solutions lead to improved simulations and memory clearing techniques. <p> The Write-All completes in all algorithms presented here. Under dynamic failures, efficient deterministic solutions to Write-All , i.e., increasing the fault-free O (N ) work by small polylog (N ) factors, are desirable. The first such solution was algorithm W of <ref> [15] </ref> which has (to date) the best worst-case work bound O (N + P log 2 N= log log N ) for 1 P N . <p> concurrent reads and writes is in [23] and can be made more reliable by employing redundancy [2]. 2.2 Measures of Efficiency We use a generalization of the standard Parallel-time fiProcessors product to measure work of an algorithm when the number of processors performing work fluctuates due to failures or delays <ref> [15, 16] </ref>. In the measure we account for the available processor steps and we do not charge for time steps during which a processor was unavailable due to a failure. Definition 1. <p> However, a more severe difference exists between crcw and crew prams (and thus also erew prams) when the processors are subject to failures. The choice of crcw (concurrent read, concurrent write) model used here is justified because of a lower bound <ref> [15] </ref> that shows that the crew (concurrent read, exclusive write) model does not admit efficient fault-tolerant algorithms. Theorem 5 [15]. <p> The choice of crcw (concurrent read, concurrent write) model used here is justified because of a lower bound <ref> [15] </ref> that shows that the crew (concurrent read, exclusive write) model does not admit efficient fault-tolerant algorithms. Theorem 5 [15]. Given any deterministic or randomized N -processor crew pram algorithm for the Write-All problem (where P = N ), the adversary can force fail-stop errors that result in (N 2 ) steps being performed, even if the processors can read and locally process all shared memory at unit cost. <p> Thus our measures capture one of the key distinctions among the erew, crew and crcw memory access disciplines. 3 Exact Write-All Algorithms 3.1 Dynamic Faults and Algorithm W Algorithm W of <ref> [15] </ref> is an efficient fail-stop Write-All solution (Fig. 2) when the failures are dynamically determined by an on-line adversary. It uses full binary trees for processor enumeration, processor allocation, and progress measurement. Active processors synchronously iterate through the following four phases: W1: Processor enumeration. <p> By optimality we mean that for a range of processors the work is O (N ). A complete description of the algorithm can be found in <ref> [15] </ref>. Martel [24] gave a tight analysis of algorithm W. Theorem 7 [15, 24]. <p> By optimality we mean that for a range of processors the work is O (N ). A complete description of the algorithm can be found in [15]. Martel [24] gave a tight analysis of algorithm W. Theorem 7 <ref> [15, 24] </ref>. <p> Note that, in a "snapshot" model, where all memory can be read and locally processed in unit time but where writes are accounted as before, algorithm W is optimal in fi (N + P log N= log log N ) <ref> [15, 16] </ref>. <p> It is an open question whether there is a Las Vegas randomized algorithm (i.e., one that always succeeds) with similar performance bounds. 5 Minimizing Concurrency Among the key lower bound results is the fact that no efficient fault-tolerant crew pram Write-All algorithms exist <ref> [15] </ref> | if the adversary is dynamic then any P -processor solution for theWrite-All problem of size N will have (deterministic) work (N P ). Thus memory access concurrency is necessary to combine efficiency and fault tolerance. <p> A timestamp in this context is just a sequence number that need not exceed N (see <ref> [15] </ref>). <p> One of the known lower bounds applies to the model with memory snapshots, i.e., processors can read and process the entire shared memory in unit time <ref> [15] </ref>. For the snapshot model there is a sharp separation between Write-All and boolean functions. <p> Assume each of the N scalar components of f can be computed in O (1) sequential time. This is the general parallel assignment problem. forall processors P ID = 1::N parbegin shared integer array x [1::N ]; x [P ID] := f (P ID; x [1::N ]) parend In <ref> [15] </ref> a general technique was shown for making this operation robust using the same work as required by Write-All .
Reference: 16. <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Efficient Parallel Algorithms On Restartable Fail-Stop Processors", </title> <booktitle> in Proc. of the 10th ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1991. </year>
Reference-contexts: The [15] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. The fault model was applied to all crcw prams in [22, 34]. It was extended in <ref> [16] </ref> to include processor restarts, and in [36] to include arbitrary static memory faults, i.e., arbitrary memory initialization, and in [14] to include restricted memory access patterns through controlled memory access. <p> This synthesis includes most of the models proposed to date (see [18]). It links the work on fail-stop no-restart errors, to fail-stop errors with restarts, where both detectable and undetectable restarts are considered. The detectable restart case has been examined, using a slightly different formalism in <ref> [6, 16] </ref>. It represents an intermediate level on asynchrony for which we have many interesting algorithmic techniques. The undetectable restart case is equivalent to the most general general model of asynchrony that has received a fair amount of attention in the literature. <p> The green part gets executed under normal conditions. If a processor fails then all memory remains intact, but in the subsequent restart the next instruction red part is executed instead of the green part. For example, the model used in <ref> [6, 16] </ref> can be realized this way. The undetectable restarts adversary can also be realized in a similar way by making the algorithm weaker. For undetectable restarts algorithms have to have identical red and green parts. For example, the fully asynchronous model of [3] can be realized this way. <p> concurrent reads and writes is in [23] and can be made more reliable by employing redundancy [2]. 2.2 Measures of Efficiency We use a generalization of the standard Parallel-time fiProcessors product to measure work of an algorithm when the number of processors performing work fluctuates due to failures or delays <ref> [15, 16] </ref>. In the measure we account for the available processor steps and we do not charge for time steps during which a processor was unavailable due to a failure. Definition 1. <p> Note that, in a "snapshot" model, where all memory can be read and locally processed in unit time but where writes are accounted as before, algorithm W is optimal in fi (N + P log N= log log N ) <ref> [15, 16] </ref>. <p> Therefore we produce a modified version of algorithm W, that we call V. To avoid a restatement of the details, the reader is referred to <ref> [16] </ref>. V uses the optimized algorithm W data structures for progress estimation and processor allocation. The processors iterate through the following three phases based on the phases W2, W3 and W4 of algorithm W: V1: Processors are allocated as in the phase W2, but using the permanent pids. <p> The model assumes re-synchronization on the instruction level, and a wraparound counter based on the pram clock implements synchronization with re spect to the phases after detected failures <ref> [16] </ref>. The work and the overhead ratio of the algorithm are as follows: Theorem 8 [16]. <p> The model assumes re-synchronization on the instruction level, and a wraparound counter based on the pram clock implements synchronization with re spect to the phases after detected failures <ref> [16] </ref>. The work and the overhead ratio of the algorithm are as follows: Theorem 8 [16]. Algorithm V using P N processors subject to an arbitrary failure and restart pattern F of size M has work S = O (N +P log 2 N +M log N ) and overhead ratio = O (log 2 N ). <p> We present Write-All algorithm X with S = O (N P log 3 2 ) = N P 0:59 <ref> [6, 16] </ref>. This simple algorithm can be improved to S = O (N P * ) using the method in [3]. We present X for its simplicity and in the next section a (possible) deterministic version of [3]. <p> The performance of algorithm X is characterized as follows: Theorem 9 <ref> [6, 16] </ref>. Algorithm X with P processors solves the Write-All problem of size N (P N ) in the fail-stop restartable model with work S = O (N P log 3 2 ). <p> Proof. To obtain this result we use algorithm W CR=W with log N elements clustered at each leaf of the progress tree and then apply the analysis of algorithm V <ref> [16] </ref>. This algorithm differs from W CR=W for the no-restarts model in that a failed processor upon restarting waits until the beginning of the next W1 phase.
Reference: 17. <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Robust Computing with Fail-Stop Processors", </title> <booktitle> in Proc. of the Second Annual ONR Workshop on Ultradependable Mul-ticomputers, Office of Naval Research, </booktitle> <pages> pp. 55-60, </pages> <year> 1991. </year>
Reference-contexts: A high level view of the algorithm Y. We propose the following way of determinizing the algorithm (see <ref> [17] </ref>): Given P = N , we choose the smallest prime m such that P &lt; m. Primes are sufficiently dense, so that there is at least one prime between P and 2P , so that the complexity of the algorithms is not distorted when P is not a prime.
Reference: 18. <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Fault Tolerance and Efficiency in Massively Parallel Algorithms", </title> <booktitle> in Foundations of Dependable Computing, </booktitle> <volume> vol. II, </volume> <editor> chapter 2.2, G. Koob, C. Lau (editors), </editor> <address> Kluwer, </address> <note> 1994 (to appear). </note>
Reference-contexts: Let us now describe the contents of this survey, with some pointers to the literature. We focus on upper bounds. In Section 2 we present a synthesis of parallel computation and fault models. This synthesis includes most of the models proposed to date (see <ref> [18] </ref>). It links the work on fail-stop no-restart errors, to fail-stop errors with restarts, where both detectable and undetectable restarts are considered. The detectable restart case has been examined, using a slightly different formalism in [6, 16]. <p> As discussed above we would like to initialize at least (1 ")N array locations to 1. Surprisingly, algorithm W has the desired property: Theorem 10 <ref> [18] </ref>. Given any constant " such that 0 &lt; " &lt; 1 2 , algorithm W solves the AWA (") problem with S = O (N log N ) using N processors. <p> Thus we can get even closer to solving the Write-All problem: Theorem 11 <ref> [18] </ref>.
Reference: 19. <author> R. M. Karp and V. Ramachandran, </author> <title> "A Survey of Parallel Algorithms for Shared-Memory Machines", </title> <note> in Handbook of Theoretical Computer Science (ed. </note> <editor> J. van Leeuwen), </editor> <volume> vol. 1, </volume> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: of fail-stop models of parallel computation, define relevant complexity measures and characterize robust algorithms. 2.1 Fail-Stop PRAMs The parallel random access machine (pram) of Fortune and Wyllie [12] combines the simplicity of a ram with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys <ref> [11, 19] </ref> for the rationale behind this model and the fundamental algorithms. We build our models of fail-stop prams as extensions of the pram model. 1. There are Q shared memory cells, and the input of size N Q is stored in the first N cells. <p> Interconnect Issues. In the absence of failures, any P -processor crew (concurrent read exclusive write) or erew (exclusive read exclusive write) pram can simulate a P -processor crcw pram with only a factor of O (log P ) more parallel work <ref> [19] </ref>. However, a more severe difference exists between crcw and crew prams (and thus also erew prams) when the processors are subject to failures. <p> This algorithmic method has been used in [25, 27]. (For a definition of Monte--Carlo probabilistic algorithms see <ref> [19] </ref>). Let us first present an analysis without faults. Lemma 12. Let N be the size of an array initialized to 0 and let P N be the number of processors of a fault-free CRCW PRAM. <p> Alternatively, we can maintain the same concurrency bounds at the expense of increasing the work by a logarithmic factor by first converting the original algorithm into an equivalent EREW algorithm <ref> [19] </ref>. When the full range of simulating processors is used (P = N ) optimality is not achievable.
Reference: 20. <author> Z. M. Kedem, K. V. Palem, M. O. Rabin, A. Raghunathan, </author> <title> "Efficient Program Transformations for Resilient Parallel Computation via Randomization," </title> <booktitle> in Proc. 24th ACM Symp. on Theory of Comp., </booktitle> <pages> pp. 306-318, </pages> <year> 1992. </year>
Reference-contexts: It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to [7, 8, 13, 21, 22, 25, 26, 27, 29]. In the last three years, randomized asynchronous computation has been examined in depth in <ref> [4, 5, 20] </ref>. These analyses involve randomness in a central way. They are mostly about off-line or oblivious adversaries, which cause faults during the computation but pick the times of these faults before the computation. <p> The fail-stop model with undetectable restarts is clearly the most challenging model, both from a technical and from a practical point of view. The simulation result above is considerably weaker than what is achievable for the other models. This is an area where the application of randomized techniques (e.g., <ref> [4, 5, 20] </ref>) is most promising.
Reference: 21. <author> Z. M. Kedem, K. V. Palem, A. Raghunathan, and P. Spirakis, </author> <title> "Combining Tentative and Definite Executions for Dependable Parallel Computing," </title> <booktitle> in Proc 23d ACM. Symposium on Theory of Computing, </booktitle> <pages> pp. 381-390, </pages> <year> 1991. </year>
Reference-contexts: The first such solution was algorithm W of [15] which has (to date) the best worst-case work bound O (N + P log 2 N= log log N ) for 1 P N . This bound was first shown in <ref> [21] </ref> for a different version of the algorithm and in [24] the basic argument was adapted to algorithm W. The strongest lower bound to date for Write-All was derived in [21], namely (N + P log N ) work is required for 1 P N . <p> This bound was first shown in <ref> [21] </ref> for a different version of the algorithm and in [24] the basic argument was adapted to algorithm W. The strongest lower bound to date for Write-All was derived in [21], namely (N + P log N ) work is required for 1 P N . Let us now describe the contents of this survey, with some pointers to the literature. We focus on upper bounds. In Section 2 we present a synthesis of parallel computation and fault models. <p> An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way. <p> A high level view of algorithm W. The above bound is tight for algorithm W. This upper bound was first shown in <ref> [21] </ref> for a different algorithm. The data structuring technique [21] might lead to even better bounds for Write-All . The best lower bound to date is (N + P log N ). <p> A high level view of algorithm W. The above bound is tight for algorithm W. This upper bound was first shown in <ref> [21] </ref> for a different algorithm. The data structuring technique [21] might lead to even better bounds for Write-All . The best lower bound to date is (N + P log N ). <p> Although much is known about handling static faults, an interesting open problem remains. Open Problem D: Prove the (N + P log N ) lower bound on Write-All on a crcw pram with initial faults only (the previous lower bounds make use of dynamic faults <ref> [21] </ref>). 4 Approximate Write-All Algorithms So far we have required that all array locations be set to 1; this is the exact Write-All problem. In this section we relax the requirement that all locations be set to 1 and instead require that only a fraction of them be written into. <p> Consider the majority relation M: Given a binary array x [1::N ], x 2 M when jfx [i] : x [i] = 1gj &gt; 1 2 N . Dwork observed that the (N log N ) lower bound of <ref> [21] </ref> on solving exact Write-All using N processors also applies to determining membership in M in the presence of failures. It turns out that O (N log N ) work is also sufficient to compute a member of the majority relation. <p> As shown in <ref> [21] </ref>, deterministic Write-All solutions have a work lower bound of (N log N ) when the number of active processors is N . This is true even for approximate Write-All .
Reference: 22. <author> Z. M. Kedem, K. V. Palem, and P. Spirakis, </author> <title> "Efficient Robust Parallel Computations," </title> <booktitle> Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pp. 138-148, </pages> <year> 1990. </year>
Reference-contexts: The [15] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. The fault model was applied to all crcw prams in <ref> [22, 34] </ref>. It was extended in [16] to include processor restarts, and in [36] to include arbitrary static memory faults, i.e., arbitrary memory initialization, and in [14] to include restricted memory access patterns through controlled memory access. <p> For dynamic behavior there is a trade-off between concurrency and fault tolerance, which we overview here (see [14] for the full details). A central algorithmic primitive in our work is the Write-All operation [15]. Iterated Write-All forms the basis for the algorithm simulation techniques of <ref> [22, 34] </ref> and for the memory initialization of [36]. Therefore, improved Write-All solutions lead to improved simulations and memory clearing techniques. The Write-All problem is: using P processors write 1s into all locations of an array of size N , where P N . <p> Requiring completion of a Write-All algorithm is critical if one wishes to iterate it, as pointed out in <ref> [22] </ref> which uses a certification bit to separate the various iterations of (Certified) Write-All . The Write-All completes in all algorithms presented here. Under dynamic failures, efficient deterministic solutions to Write-All , i.e., increasing the fault-free O (N ) work by small polylog (N ) factors, are desirable. <p> An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way. <p> The asymptotic work complexities of solving the general parallel assignment problem and the Write-All problem are equal. 6.3 Any PRAM Step The original motivation for studying the Write-All problem was that it captured the essence of a single pram step computation. It was shown in <ref> [22, 34] </ref> how to use the Write-All paradigm in implementing general pram simulations. The generality of this result is somewhat surprising. Fail-stop faults: An approach to such simulations is given in Fig. 8. <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the general parallel assignment style, e.g., using algorithm W. Using such techniques it was shown in <ref> [22, 34] </ref> that if S w (N; P ) is the efficiency of solving a Write-All instance of size N using P processors, and if a linear amount of clear memory is available, then any N -processor pram step can be deterministically simulated using P fail-stop processors and work S w
Reference: 23. <author> C. P. Kruskal, L. Rudolph, M. Snir, </author> <title> "Efficient Synchronization on Multiprocessors with Shared Memory," </title> <journal> in ACM Trans. on Programming Languages and Systems, </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. </pages> <month> 579-601 </month> <year> 1988. </year>
Reference-contexts: There are Q shared memory cells. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques [31]. Processors and memory are interconnected via a synchronous network [33]. A combining interconnection network well suited for implementing synchronous concurrent reads and writes is in <ref> [23] </ref> and can be made more reliable by employing redundancy [2]. 2.2 Measures of Efficiency We use a generalization of the standard Parallel-time fiProcessors product to measure work of an algorithm when the number of processors performing work fluctuates due to failures or delays [15, 16].
Reference: 24. <author> C. </author> <title> Martel, </title> <type> personal communication, </type> <month> March, </month> <year> 1991. </year>
Reference-contexts: This bound was first shown in [21] for a different version of the algorithm and in <ref> [24] </ref> the basic argument was adapted to algorithm W. The strongest lower bound to date for Write-All was derived in [21], namely (N + P log N ) work is required for 1 P N . <p> By optimality we mean that for a range of processors the work is O (N ). A complete description of the algorithm can be found in [15]. Martel <ref> [24] </ref> gave a tight analysis of algorithm W. Theorem 7 [15, 24]. <p> By optimality we mean that for a range of processors the work is O (N ). A complete description of the algorithm can be found in [15]. Martel [24] gave a tight analysis of algorithm W. Theorem 7 <ref> [15, 24] </ref>.
Reference: 25. <author> C. Martel, A. Park, and R. Subramonian, </author> <title> "Work-optimal Asynchronous Algorithms for Shared Memory Parallel Computers," </title> <journal> SIAM Journal on Computing, </journal> <volume> vol. 21, </volume> <pages> pp. 1070-1099, </pages> <year> 1992 </year>
Reference-contexts: An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way. <p> If the number of locations that must be written is N 0 = ffN for some constant 0 &lt; ff &lt; 1 this method results in a CRCW Monte-Carlo algorithm with linear expected work. This algorithmic method has been used in <ref> [25, 27] </ref>. (For a definition of Monte--Carlo probabilistic algorithms see [19]). Let us first present an analysis without faults. Lemma 12. Let N be the size of an array initialized to 0 and let P N be the number of processors of a fault-free CRCW PRAM. <p> When the full range of simulating processors is used (P = N ) optimality is not achievable. In this case customized transformations of parallel algorithms (such as prefix and list ranking algorithms <ref> [25, 35] </ref>) may improve on the oblivious simulations. 01 forall processors PID=1..P parbegin Simulate N fault-prone processors 02 The PRAM program for N processors is in shared memory (read-only) 03 Shared memory has two generations: current and future; 04 Initialize N simulated instruction counters to start at the first instruction 05
Reference: 26. <author> C. Martel and R. Subramonian, </author> <title> "On the Complexity of Certified Write-All Algorithms", to appear in Journal of Algorithms (a prel. </title> <booktitle> version in the Proc. of the 12th Conference on Foundations of Software Technology and Theoretical Computer Science, </booktitle> <address> New Delhi, India, </address> <month> December </month> <year> 1992). </year>
Reference-contexts: An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way.
Reference: 27. <author> C. Martel, R. Subramonian, and A. Park, </author> <title> "Asynchronous PRAMs are (Almost) as Good as Synchronous PRAMs," </title> <booktitle> in Proc. 32d IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 590-599, </pages> <year> 1990. </year>
Reference-contexts: An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way. <p> If the number of locations that must be written is N 0 = ffN for some constant 0 &lt; ff &lt; 1 this method results in a CRCW Monte-Carlo algorithm with linear expected work. This algorithmic method has been used in <ref> [25, 27] </ref>. (For a definition of Monte--Carlo probabilistic algorithms see [19]). Let us first present an analysis without faults. Lemma 12. Let N be the size of an array initialized to 0 and let P N be the number of processors of a fault-free CRCW PRAM.
Reference: 28. <author> J. Naor, R.M. Roth, </author> <title> "Constructions of Permutation Arrays for Ceratin Scheduling Cost Measures", </title> <type> manuscript, </type> <year> 1993. </year>
Reference-contexts: An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in <ref> [28] </ref>. For some important early work on asynchronous prams we refer to [7, 8, 13, 21, 22, 25, 26, 27, 29]. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way.
Reference: 29. <author> N. Nishimura, </author> <title> "Asynchronous Shared Memory Parallel Computation," </title> <booktitle> in Proc. 3rd ACM Symp. on Parallel Algor. and Architect., </booktitle> <pages> pp. 76-84, </pages> <year> 1990. </year>
Reference-contexts: An elegant deterministic solution for Write-All in this case appeared in [3]. The proof in [3] is existential, because it uses a counting argument. It has recently been made constructive in [28]. For some important early work on asynchronous prams we refer to <ref> [7, 8, 13, 21, 22, 25, 26, 27, 29] </ref>. In the last three years, randomized asynchronous computation has been examined in depth in [4, 5, 20]. These analyses involve randomness in a central way.
Reference: 30. <author> M.O. Rabin, </author> <title> "Efficient Dispersal of Information for Security, Load Balancing and Fault Tolerance", </title> <journal> J. of ACM, </journal> <volume> vol. 36, no. 2, </volume> <pages> pp. 335-348, </pages> <year> 1989. </year>
Reference-contexts: Although, we will not survey this interesting subject here we would like to point-out that one very promising direction involves combining techniques of randomized asynchronous computation with randomized information dispersal <ref> [30] </ref>. Randomized algorithms often achieve better practical performance than deterministic ones, even when their analytical bounds are similar. Future developments in asynchronous parallel computation will employ randomization as well as the array of deterministic techniques surveyed here.
Reference: 31. <author> D. B. Sarrazin and M. Malek, </author> <title> "Fault-Tolerant Semiconductor Memories", </title> <journal> IEEE Computer, </journal> <volume> vol. 17, no. 8, </volume> <pages> pp. 49-56, </pages> <year> 1984. </year>
Reference-contexts: The abstract model we are studying can be realized in the architecture in Fig. 1. There are P fail-stop processors [32]. There are Q shared memory cells. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques <ref> [31] </ref>. Processors and memory are interconnected via a synchronous network [33].
Reference: 32. <author> R. D. Schlichting and F. B. Schneider, </author> <title> "Fail-Stop Processors: an Approach to Designing Fault-tolerant Computing Systems", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 222-238, </pages> <year> 1983. </year>
Reference-contexts: For undetectable restarts algorithms have to have identical red and green parts. For example, the fully asynchronous model of [3] can be realized this way. The abstract model we are studying can be realized in the architecture in Fig. 1. There are P fail-stop processors <ref> [32] </ref>. There are Q shared memory cells. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques [31]. Processors and memory are interconnected via a synchronous network [33].
Reference: 33. <author> J. T. Schwartz, </author> <title> "Ultracomputers", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 484-521, </pages> <year> 1980. </year>
Reference-contexts: There are P fail-stop processors [32]. There are Q shared memory cells. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques [31]. Processors and memory are interconnected via a synchronous network <ref> [33] </ref>.
Reference: 34. <author> A. A. Shvartsman, </author> <title> "Achieving Optimal CRCW PRAM Fault Tolerance", </title> <journal> Information Processing Letters, </journal> <volume> vol. 39, no. 2, </volume> <pages> pp. 59-66, </pages> <year> 1991. </year>
Reference-contexts: The [15] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. The fault model was applied to all crcw prams in <ref> [22, 34] </ref>. It was extended in [16] to include processor restarts, and in [36] to include arbitrary static memory faults, i.e., arbitrary memory initialization, and in [14] to include restricted memory access patterns through controlled memory access. <p> For dynamic behavior there is a trade-off between concurrency and fault tolerance, which we overview here (see [14] for the full details). A central algorithmic primitive in our work is the Write-All operation [15]. Iterated Write-All forms the basis for the algorithm simulation techniques of <ref> [22, 34] </ref> and for the memory initialization of [36]. Therefore, improved Write-All solutions lead to improved simulations and memory clearing techniques. The Write-All problem is: using P processors write 1s into all locations of an array of size N , where P N . <p> The asymptotic work complexities of solving the general parallel assignment problem and the Write-All problem are equal. 6.3 Any PRAM Step The original motivation for studying the Write-All problem was that it captured the essence of a single pram step computation. It was shown in <ref> [22, 34] </ref> how to use the Write-All paradigm in implementing general pram simulations. The generality of this result is somewhat surprising. Fail-stop faults: An approach to such simulations is given in Fig. 8. <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the general parallel assignment style, e.g., using algorithm W. Using such techniques it was shown in <ref> [22, 34] </ref> that if S w (N; P ) is the efficiency of solving a Write-All instance of size N using P processors, and if a linear amount of clear memory is available, then any N -processor pram step can be deterministically simulated using P fail-stop processors and work S w
Reference: 35. <author> A. A. Shvartsman, </author> <title> Fault-Tolerant and Efficient Parallel Computation, </title> <type> Ph.D. dissertation, </type> <institution> Brown University, </institution> <type> Tech. Rep. </type> <institution> CS-92-23, </institution> <year> 1992. </year>
Reference-contexts: When the full range of simulating processors is used (P = N ) optimality is not achievable. In this case customized transformations of parallel algorithms (such as prefix and list ranking algorithms <ref> [25, 35] </ref>) may improve on the oblivious simulations. 01 forall processors PID=1..P parbegin Simulate N fault-prone processors 02 The PRAM program for N processors is in shared memory (read-only) 03 Shared memory has two generations: current and future; 04 Initialize N simulated instruction counters to start at the first instruction 05
Reference: 36. <author> A. A. Shvartsman, </author> <title> "Efficient Write-All Algorithm for Fail-Stop PRAM Without Initialized Memory", </title> <journal> Information Processing Letters, </journal> <volume> vol. 44, no. 6, </volume> <pages> pp. 223-231, </pages> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The [15] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. The fault model was applied to all crcw prams in [22, 34]. It was extended in [16] to include processor restarts, and in <ref> [36] </ref> to include arbitrary static memory faults, i.e., arbitrary memory initialization, and in [14] to include restricted memory access patterns through controlled memory access. Concurrency of reads and writes is an essential feature that accounts for the necessary redundancy so it can be restricted but not eliminated. <p> We assume: 1. Processor faults do not affect memory. 2. Processors can read and write memory concurrently, crcw. These two assumptions are essential for deterministic algorithms with dynamic faults. Static or initial memory can be contaminated <ref> [36] </ref>. Static or initial processor faults can be handled with erew algorithms [14]. For dynamic behavior there is a trade-off between concurrency and fault tolerance, which we overview here (see [14] for the full details). A central algorithmic primitive in our work is the Write-All operation [15]. <p> A central algorithmic primitive in our work is the Write-All operation [15]. Iterated Write-All forms the basis for the algorithm simulation techniques of [22, 34] and for the memory initialization of <ref> [36] </ref>. Therefore, improved Write-All solutions lead to improved simulations and memory clearing techniques. The Write-All problem is: using P processors write 1s into all locations of an array of size N , where P N . <p> Interestingly there is an efficient deterministic procedure that solves the Write-All problem even when the shared memory is contaminated, i.e., contains arbitrary values, see <ref> [36] </ref>. Interconnect Issues. In the absence of failures, any P -processor crew (concurrent read exclusive write) or erew (exclusive read exclusive write) pram can simulate a P -processor crcw pram with only a factor of O (log P ) more parallel work [19]. <p> It turns out that this assumption is not critical for the existence of efficient Write-All algorithms. Algorithm Z of <ref> [36] </ref> is an efficient Write-All algorithm for the case that the global memory may be contaminated (i.e., set to arbitrary initial values). It works by combining a bootstrap approach with algorithm W. The algorithm makes a number of iterations.
References-found: 36

