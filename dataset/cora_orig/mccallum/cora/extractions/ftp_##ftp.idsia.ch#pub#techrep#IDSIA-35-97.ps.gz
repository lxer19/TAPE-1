URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-35-97.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Title: WHAT'S INTERESTING?  (Two souls, alas! are dwelling in my breast!)  
Author: Jurgen Schmidhuber Johann Wolfgang Goethe 
Keyword: randomness, triviality, surprise, confidence, curiosity, creativity, co-evolution, success-story algorithm, incremental self-improvement  
Note: Zwei Seelen wohnen, ach, in meiner Brust!  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Pubnum: Technical Report IDSIA-35-97, Version 1.0  
Email: juergen@idsia.ch  
Web: http://www.idsia.ch/~juergen  
Date: July 14, 1997  
Abstract: Interestingness depends on the observer's current knowledge and computational abilities. Things are boring if either too much or too little is known about them | if they appear either trivial or random. Interesting are unexpected regularities that seem easy to figure out. I attempt to implement these ideas in a "curious", "creative" explorer with two coevolving "brains". It executes a lifelong sequence of instructions whose modifiable probabilities are conditioned on both brains | both must agree on each instruction. There are special instructions for comparing computational results. The brains can predict outcomes of such comparisons. If their opinions differ, then the winner will get rewarded, the loser punished. Hence each brain wants to lure the other into agreeing upon instruction subsequences involving comparisons that surprise it. The surprised brain adapts. In turn, the other loses a source of reward | an incentive to shift the focus of interest. Both brains deal with the complex credit assignment problem using the recent Incremental Self-Improvement paradigm. Extensive simulations include an example where curiosity helps to speed up external reward. We can learn only what we already almost know. Patrick Winston 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. B. Baum. </author> <title> Neural nets that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 5-19, </pages> <year> 1991. </year>
Reference-contexts: The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., <ref> [6, 1, 9, 13, 15, 4] </ref>. For goal-oriented RL-based exploration see, e.g., [31, 33, 5, 25]. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise.
Reference: [2] <author> G.J. Chaitin. </author> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159, </pages> <year> 1969. </year>
Reference-contexts: Details of noise, for instance, cannot be expected confidently | so they tend to get uninteresting. Examples of learnable regularities. If the instruction set is universal and allows for arbitrary computations, then arbitrary regularities <ref> [10, 2, 29, 11] </ref> may be exploited by both brains to generate or avoid surprises. <p> It can move about in a two-dimensional environment (details of the environment will be described in Section 4). The agent's current position is given by a pair of real values. Its current direction is given by variable Direction with range <ref> [0; : : : ; 2] </ref>. MoveAgent (): Move the agent 12 unit lengths in direction Direction, unless an obstacle or the environment's boundary blocks the path (the value 12 was chosen arbitrarily | neither it nor other arbitrarily chosen constants mentioned below have ever been changed during the experiments). <p> There may be RL schemes even more general than IS, but this is beyond the scope of this paper. Other notions of regularity. Note that my notion of "simple regularities" differs from, e.g., Kolmogorov complexity theory's <ref> [10, 2, 29, 11, 21] </ref>. There an object is called simple relative to current knowledge x if the size of the shortest algorithm computing it from x is small. The algorithm's computation time is ignored, as are constant factors reflecting Kolmogorov complexity's machine independence.
Reference: [3] <author> A. C. Clarke. </author> <title> The ghost from the grand banks. </title> <publisher> Orbit books, </publisher> <year> 1991. </year>
Reference: [4] <author> D. A. Cohn. </author> <title> Neural network exploration using optimal experiment design. </title> <editor> In J. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 679-686. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., <ref> [6, 1, 9, 13, 15, 4] </ref>. For goal-oriented RL-based exploration see, e.g., [31, 33, 5, 25]. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise.
Reference: [5] <author> P. Dayan and T. J. Sejnowski. </author> <title> Exloration bonuses and dual control. </title> <journal> Machine Learning, </journal> <volume> 25 </volume> <pages> 5-22, </pages> <year> 1996. </year>
Reference-contexts: Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., [6, 1, 9, 13, 15, 4]. For goal-oriented RL-based exploration see, e.g., <ref> [31, 33, 5, 25] </ref>. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise. I will try to offer a more acceptable alternative.
Reference: [6] <author> V. V. Fedorov. </author> <title> Theory of optimal experiments. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., <ref> [6, 1, 9, 13, 15, 4] </ref>. For goal-oriented RL-based exploration see, e.g., [31, 33, 5, 25]. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise.
Reference: [7] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: This behavior leads to about 5 visits of GOAL1 per 10 million time steps. Initial bias. Following Geman and Bienenstock <ref> [7] </ref>, the learning system's bias towards solving the task is low, while variance is high. There is a confusing choice of 2 fi 576 = 1152 brain columns (many more than needed for solving the task), all being potential candidates for modifications.
Reference: [8] <author> D. Hillis. </author> <title> Co-evolving parasites improve simulated evolution as an optimization procedure. In C.G. </title> <editor> Langton, C. Taylor, J. D. Farmer, and S. Rasmussen, editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 313-324. </pages> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1992. </year>
Reference-contexts: History teaches us, however, that it is hard to decide which math will be useless forever. What's old. This paper's approach draws inspiration from several sources. For instance, the two-brain system is based on two co-evolving modules. Co-evolution of competing strategies, however, is nothing new. See, e.g., <ref> [8, 16] </ref> for interesting examples. Also, the idea of improving a 18 after simulation 1. Grey scales indicate probability magnitudes (white = close to 0, black = close to 1). The probability mass of many (but not all) columns is concentrated in a single value.
Reference: [9] <author> J. Hwang, J. Choi, S. Oh, and R. J. Marks II. </author> <title> Query-based learning applied to partially trained multilayer perceptrons. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 131-136, </pages> <year> 1991. </year> <month> 21 </month>
Reference-contexts: The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., <ref> [6, 1, 9, 13, 15, 4] </ref>. For goal-oriented RL-based exploration see, e.g., [31, 33, 5, 25]. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise.
Reference: [10] <author> A.N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11, </pages> <year> 1965. </year>
Reference-contexts: Details of noise, for instance, cannot be expected confidently | so they tend to get uninteresting. Examples of learnable regularities. If the instruction set is universal and allows for arbitrary computations, then arbitrary regularities <ref> [10, 2, 29, 11] </ref> may be exploited by both brains to generate or avoid surprises. <p> There may be RL schemes even more general than IS, but this is beyond the scope of this paper. Other notions of regularity. Note that my notion of "simple regularities" differs from, e.g., Kolmogorov complexity theory's <ref> [10, 2, 29, 11, 21] </ref>. There an object is called simple relative to current knowledge x if the size of the shortest algorithm computing it from x is small. The algorithm's computation time is ignored, as are constant factors reflecting Kolmogorov complexity's machine independence.
Reference: [11] <author> M. Li and P. M. B. Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: Details of noise, for instance, cannot be expected confidently | so they tend to get uninteresting. Examples of learnable regularities. If the instruction set is universal and allows for arbitrary computations, then arbitrary regularities <ref> [10, 2, 29, 11] </ref> may be exploited by both brains to generate or avoid surprises. <p> There may be RL schemes even more general than IS, but this is beyond the scope of this paper. Other notions of regularity. Note that my notion of "simple regularities" differs from, e.g., Kolmogorov complexity theory's <ref> [10, 2, 29, 11, 21] </ref>. There an object is called simple relative to current knowledge x if the size of the shortest algorithm computing it from x is small. The algorithm's computation time is ignored, as are constant factors reflecting Kolmogorov complexity's machine independence.
Reference: [12] <author> L.J. Lin. </author> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: For instance, [25] describes two agents A and B living in a partially observable 600 fi 500 environment with obstacles. They learn to solve a complex task that could not be solved by various TD () Q-learning variants <ref> [12] </ref>.
Reference: [13] <author> D. J. C. MacKay. </author> <title> Information-based objective functions for active data selection. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 550-604, </pages> <year> 1992. </year>
Reference-contexts: The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., <ref> [6, 1, 9, 13, 15, 4] </ref>. For goal-oriented RL-based exploration see, e.g., [31, 33, 5, 25]. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise.
Reference: [14] <author> F. Nake. </author> <title> Asthetik als Informationsverarbeitung. </title> <publisher> Springer, </publisher> <year> 1974. </year>
Reference-contexts: The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise. I will try to offer a more acceptable alternative. It is inspired by Nake <ref> [14] </ref> and others who suggested that maximally interesting and aesthetically pleasing input data exhibits an ideal ratio between expected and unexpected information. Things are considered boring if they are either too random or too predictable. <p> No prewired ratio between expected and unexpected info. As the explorer's knowledge about its environment and computational abilities expands, it keeps balancing on the thin, dynamically changing line between the subjectively random and the subjectively trivial. Unlike Nake and other authors he cites <ref> [14] </ref>, I do not suggest a predefined optimal ratio between known and unknown information. Instead, the two cooperating/competing brains dynamically, implicitly determine this ratio as they keep trying to surprise each other. Interestingness and "beauty".
Reference: [15] <author> M. Plutowski, G. Cottrell, and H. White. </author> <title> Learning Mackey-Glass from 25 examples, plus or minus 2. </title> <editor> In J. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 1135-1142. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., <ref> [6, 1, 9, 13, 15, 4] </ref>. For goal-oriented RL-based exploration see, e.g., [31, 33, 5, 25]. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise.
Reference: [16] <author> J. B. Pollack and A. D. Blair. </author> <title> Why did TD-Gammon work? In M. </title> <editor> C. Mozer, M. I. Jordan, and S. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <pages> pages 10-16. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1997. </year>
Reference-contexts: History teaches us, however, that it is hard to decide which math will be useless forever. What's old. This paper's approach draws inspiration from several sources. For instance, the two-brain system is based on two co-evolving modules. Co-evolution of competing strategies, however, is nothing new. See, e.g., <ref> [8, 16] </ref> for interesting examples. Also, the idea of improving a 18 after simulation 1. Grey scales indicate probability magnitudes (white = close to 0, black = close to 1). The probability mass of many (but not all) columns is concentrated in a single value.
Reference: [17] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3 </volume> <pages> 210-229, </pages> <year> 1959. </year>
Reference-contexts: Compare Figure 10. learner by letting it play against itself is ancient. See, e.g., <ref> [17, 32] </ref>. Even the idea of unsupervised learning through co-evolution of predictors and modules trying to escape the predictions is nothing new | it has been used extensively in our previous work on unsupervised sensory coding [20, 27, 24, 23].
Reference: [18] <author> J. Schmidhuber. </author> <booktitle> Curious model-building control systems. In Proc. International Joint Conference on Neural Networks, Singapore, </booktitle> <volume> volume 2, </volume> <pages> pages 1458-1463. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: To draw the line between goal-oriented and pure exploration I will use a reinforcement learning (RL) framework and say that exploration involves a pure component if there is explicit reward for knowledge growth, besides possible environmental reward. 1 Previous work. Existing RL approaches to pure exploration <ref> [19, 18, 30] </ref> use adaptive predic-tors to predict the next input given previous inputs and actions. The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. <p> Finally, co-evolutionary methods translating mismatches between reality and expectations into reward for "curious", exploring agents are not new either | see our previous work on "pure" RL-based exploration <ref> [19, 18, 30] </ref>. So, what's new? 19 0 1000 2000 3000 "PLAIN" "CURIOUS" intervals of 10 7 steps (averaged over 10 simulations). Initial advantages of curiosity are lost as more and more goal-oriented training examples become available. Compare Figure 9. Competitors collectively decide. <p> Generality. Another novel aspect is the general setting. Instead of being limited to Markovian domains and simple reactive strategies such as approaches in <ref> [18, 30] </ref>, this paper's set-up allows for quite arbitrary domains and computations. This is made possible by the recent IS paradigm [25, 26]. There is no essential limit (besides computability) to the nature of the regularities that may be exploited to generate surprises.
Reference: [19] <author> J. Schmidhuber. </author> <title> A possibility for implementing curiosity and boredom in model-building neural controllers. </title> <editor> In J. A. Meyer and S. W. Wilson, editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats, </booktitle> <pages> pages 222-227. </pages> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year>
Reference-contexts: To draw the line between goal-oriented and pure exploration I will use a reinforcement learning (RL) framework and say that exploration involves a pure component if there is explicit reward for knowledge growth, besides possible environmental reward. 1 Previous work. Existing RL approaches to pure exploration <ref> [19, 18, 30] </ref> use adaptive predic-tors to predict the next input given previous inputs and actions. The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. <p> Finally, co-evolutionary methods translating mismatches between reality and expectations into reward for "curious", exploring agents are not new either | see our previous work on "pure" RL-based exploration <ref> [19, 18, 30] </ref>. So, what's new? 19 0 1000 2000 3000 "PLAIN" "CURIOUS" intervals of 10 7 steps (averaged over 10 simulations). Initial advantages of curiosity are lost as more and more goal-oriented training examples become available. Compare Figure 9. Competitors collectively decide.
Reference: [20] <author> J. Schmidhuber. </author> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879, </pages> <year> 1992. </year>
Reference-contexts: See, e.g., [17, 32]. Even the idea of unsupervised learning through co-evolution of predictors and modules trying to escape the predictions is nothing new | it has been used extensively in our previous work on unsupervised sensory coding <ref> [20, 27, 24, 23] </ref>. Finally, co-evolutionary methods translating mismatches between reality and expectations into reward for "curious", exploring agents are not new either | see our previous work on "pure" RL-based exploration [19, 18, 30].
Reference: [21] <author> J. Schmidhuber. </author> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <year> 1997. </year> <note> In press. </note>
Reference-contexts: A single SSA call, however, may undo many SLMs if necessary. What has been described for Left analogously holds for Right. The next section will describe a particular implementation based on an assembler-like programming language similar to those used in <ref> [25, 21] </ref>. 3 Implementation In what follows I will describe details of the system used in the experiments. Architecture. <p> There may be RL schemes even more general than IS, but this is beyond the scope of this paper. Other notions of regularity. Note that my notion of "simple regularities" differs from, e.g., Kolmogorov complexity theory's <ref> [10, 2, 29, 11, 21] </ref>. There an object is called simple relative to current knowledge x if the size of the shortest algorithm computing it from x is small. The algorithm's computation time is ignored, as are constant factors reflecting Kolmogorov complexity's machine independence.
Reference: [22] <author> J. Schmidhuber. </author> <title> Low-complexity art. </title> <journal> Leonardo, Journal of the International Society for the Arts, Sciences, and Technology, </journal> <volume> 30(2) </volume> <pages> 97-103, </pages> <year> 1997. </year>
Reference-contexts: Instead, the two cooperating/competing brains dynamically, implicitly determine this ratio as they keep trying to surprise each other. Interestingness and "beauty". A recent paper attempts at explaining "beauty" with the 20 help of complexity theory concepts <ref> [22] </ref>. It argues that something "beautiful" needs not be "inter-esting". Its formalism predicts that the "most beautiful" object from a set of objects satisfying certain specifications is the one that can be most easily computed from the subjective observer's input coding scheme.
Reference: [23] <author> J. Schmidhuber, M. Eldracher, and B. Foltin. </author> <title> Semilinear predictability minimization produces well-known feature detectors. </title> <journal> Neural Computation, </journal> <volume> 8(4) </volume> <pages> 773-786, </pages> <year> 1996. </year>
Reference-contexts: See, e.g., [17, 32]. Even the idea of unsupervised learning through co-evolution of predictors and modules trying to escape the predictions is nothing new | it has been used extensively in our previous work on unsupervised sensory coding <ref> [20, 27, 24, 23] </ref>. Finally, co-evolutionary methods translating mismatches between reality and expectations into reward for "curious", exploring agents are not new either | see our previous work on "pure" RL-based exploration [19, 18, 30].
Reference: [24] <author> J. Schmidhuber and D. Prelinger. </author> <title> Discovering predictable classifications. </title> <journal> Neural Computation, </journal> <volume> 5(4) </volume> <pages> 625-635, </pages> <year> 1993. </year>
Reference-contexts: For instance, raw inputs may be too noisy to be precisely predictable. Still, there may be internally computable, predictable, informative input transformations <ref> [24] </ref>. E.g., hearing the first two words of the sentence "John eats chips" does not make the word "chips" predictable, but at least it is likely that the third word will represent something edible. <p> See, e.g., [17, 32]. Even the idea of unsupervised learning through co-evolution of predictors and modules trying to escape the predictions is nothing new | it has been used extensively in our previous work on unsupervised sensory coding <ref> [20, 27, 24, 23] </ref>. Finally, co-evolutionary methods translating mismatches between reality and expectations into reward for "curious", exploring agents are not new either | see our previous work on "pure" RL-based exploration [19, 18, 30].
Reference: [25] <author> J. Schmidhuber, J. Zhao, and N. Schraudolph. </author> <title> Reinforcement learning with self-modifying policies. </title> <editor> In S. Thrun and L. Pratt, editors, </editor> <title> Learning to learn. </title> <publisher> Kluwer, </publisher> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., [6, 1, 9, 13, 15, 4]. For goal-oriented RL-based exploration see, e.g., <ref> [31, 33, 5, 25] </ref>. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise. I will try to offer a more acceptable alternative. <p> This does not reflect a universal law though: in general there is no guarantee that curiosity will not turn out to be harmful (e.g., by "killing the cat" [30]). Outline. Section 2 will provide formal details of what has been said above, and explain how Incremental Self-Improvement <ref> [25, 26] </ref> can be used to deal with both brains' complex credit assignment problems (though other machine learning paradigms may also be appropriate). Section 3 will describe details of a concrete implementation. Section 4 will describe experiments with and without external and surprise rewards. <p> In principle, both can learn by executing subsequences of instructions that include PLAs. How can we ensure that each brain indeed improves by accelerating its reward intake? Incremental Self-Improvement (IS). In this paper I will use the recent Incremental Self-Improvement paradigm (IS) <ref> [26, 25] </ref> to deal with both brains' complex spatio-temporal credit assignment problem. This does not necessarily mean that IS is the best way of doing so. Other RL paradigms may be appropriate, too | this paper's basic ideas are independent of the choice of RL method. <p> each brain modification recursively 4 depends on the success of all later modifications for which it is setting the stage. (3) Its objective takes into account the entire time consumed by lifelong learning itself. (4) It is designed for quite general non-Markovian credit assignment problems in lifelong learning situations (see <ref> [26, 25] </ref> for recent IS applications). Following [25], the remainder of this section will describe Left's IS-based learning process. Right's is analogous. Checkpoints. Left's entire life-time can be partitioned into time intervals separated by special times called checkpoints. <p> Following <ref> [25] </ref>, the remainder of this section will describe Left's IS-based learning process. Right's is analogous. Checkpoints. Left's entire life-time can be partitioned into time intervals separated by special times called checkpoints. Checkpoints are computed dynamically during the learner's life by certain instructions in A executed according to the brains themselves. <p> Implementing SSA. Using stack-based backtracking methods such as those described in <ref> [26, 25] </ref> and section 3, one can guarantee that SSC will be satisfied right after each new SLM-start, despite interference from S, E, and Right. <p> A single SSA call, however, may undo many SLMs if necessary. What has been described for Left analogously holds for Right. The next section will describe a particular implementation based on an assembler-like programming language similar to those used in <ref> [25, 21] </ref>. 3 Implementation In what follows I will describe details of the system used in the experiments. Architecture. <p> For instance, suppose that Left's current advantage depends on supporting some IncProbRight instruction. An equal Right opponent should strongly support IncProbLeft instead. 4 Experiments. Previous IS experiments. It has already been shown that IS by itself can solve interesting tasks. For instance, <ref> [25] </ref> describes two agents A and B living in a partially observable 600 fi 500 environment with obstacles. They learn to solve a complex task that could not be solved by various TD () Q-learning variants [12]. <p> This agent's reward is 5.0; the other's is 3.0. In the beginning, the goal is found only every 300,000 basic cycles. Through IS, however, within 130,000 trials the average trial length decreases by a factor of 60 | both agents learn to cooperate to accelerate reward intake <ref> [25] </ref>. Outline. This section's purpose is not to elaborate on how IS can solve difficult tasks. Instead IS is used as a particular vehicle to implement the two-brain idea for preliminary attempts at studying "inquisitive" explorers. Subsection 4.1 will describe empirically observed system behavior in absence of external rewards. <p> Many additional experiments are necessary, however, to understand whether the above is a typical result or not. 5 Final Remarks Previous work on IS (e.g., <ref> [25, 26] </ref>) implicitly focused on goal-oriented exploration | in principle IS can learn to change its exploration strategy if this turns out to accelerate external reward in the long run. This paper's IS implementation, however, involves an additional pure exploration component besides the goal-oriented one. <p> Generality. Another novel aspect is the general setting. Instead of being limited to Markovian domains and simple reactive strategies such as approaches in [18, 30], this paper's set-up allows for quite arbitrary domains and computations. This is made possible by the recent IS paradigm <ref> [25, 26] </ref>. There is no essential limit (besides computability) to the nature of the regularities that may be exploited to generate surprises. Neither is there an essential limit to the nature of the learning processes that can make formerly surprising regularities predictable and boring.
Reference: [26] <author> J. Schmidhuber, J. Zhao, and M. Wiering. </author> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 105-130, </pages> <year> 1997. </year> <note> In press. </note>
Reference-contexts: This does not reflect a universal law though: in general there is no guarantee that curiosity will not turn out to be harmful (e.g., by "killing the cat" [30]). Outline. Section 2 will provide formal details of what has been said above, and explain how Incremental Self-Improvement <ref> [25, 26] </ref> can be used to deal with both brains' complex credit assignment problems (though other machine learning paradigms may also be appropriate). Section 3 will describe details of a concrete implementation. Section 4 will describe experiments with and without external and surprise rewards. <p> In principle, both can learn by executing subsequences of instructions that include PLAs. How can we ensure that each brain indeed improves by accelerating its reward intake? Incremental Self-Improvement (IS). In this paper I will use the recent Incremental Self-Improvement paradigm (IS) <ref> [26, 25] </ref> to deal with both brains' complex spatio-temporal credit assignment problem. This does not necessarily mean that IS is the best way of doing so. Other RL paradigms may be appropriate, too | this paper's basic ideas are independent of the choice of RL method. <p> each brain modification recursively 4 depends on the success of all later modifications for which it is setting the stage. (3) Its objective takes into account the entire time consumed by lifelong learning itself. (4) It is designed for quite general non-Markovian credit assignment problems in lifelong learning situations (see <ref> [26, 25] </ref> for recent IS applications). Following [25], the remainder of this section will describe Left's IS-based learning process. Right's is analogous. Checkpoints. Left's entire life-time can be partitioned into time intervals separated by special times called checkpoints. <p> Implementing SSA. Using stack-based backtracking methods such as those described in <ref> [26, 25] </ref> and section 3, one can guarantee that SSC will be satisfied right after each new SLM-start, despite interference from S, E, and Right. <p> Use backtracking and the information in StackLeft to undo as many of the most recent Left-modifications as necessary to achieve SSC | see inequality (1) in section 2.2. Pop off the corresponding blocks in StackLeft. This procedure guarantees that SSC will eventually be satisfied | see, e.g., <ref> [26] </ref>. 3. Push t and RL (t) onto StackLeft. They are the first two elements of the next block to be pushed. <p> Many additional experiments are necessary, however, to understand whether the above is a typical result or not. 5 Final Remarks Previous work on IS (e.g., <ref> [25, 26] </ref>) implicitly focused on goal-oriented exploration | in principle IS can learn to change its exploration strategy if this turns out to accelerate external reward in the long run. This paper's IS implementation, however, involves an additional pure exploration component besides the goal-oriented one. <p> Generality. Another novel aspect is the general setting. Instead of being limited to Markovian domains and simple reactive strategies such as approaches in [18, 30], this paper's set-up allows for quite arbitrary domains and computations. This is made possible by the recent IS paradigm <ref> [25, 26] </ref>. There is no essential limit (besides computability) to the nature of the regularities that may be exploited to generate surprises. Neither is there an essential limit to the nature of the learning processes that can make formerly surprising regularities predictable and boring.
Reference: [27] <author> N. Schraudolph and T. J. Sejnowski. </author> <title> Unsupervised discrimination of clustered data via optimization of binary information gain. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 499-506. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1993. </year>
Reference-contexts: See, e.g., [17, 32]. Even the idea of unsupervised learning through co-evolution of predictors and modules trying to escape the predictions is nothing new | it has been used extensively in our previous work on unsupervised sensory coding <ref> [20, 27, 24, 23] </ref>. Finally, co-evolutionary methods translating mismatches between reality and expectations into reward for "curious", exploring agents are not new either | see our previous work on "pure" RL-based exploration [19, 18, 30].
Reference: [28] <author> C. E. Shannon. </author> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, </journal> <volume> XXVII:379-423, </volume> <year> 1948. </year>
Reference-contexts: Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., [6, 1, 9, 13, 15, 4]. For goal-oriented RL-based exploration see, e.g., [31, 33, 5, 25]. The "ideal ratio". According to traditional information theory <ref> [28] </ref> noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise. I will try to offer a more acceptable alternative. <p> The learner is "interested" in "creative" computations leading to unexpected results, while simultaneously trying to make formerly surprising things predictable and boring. It does not care much for irregular noise rich with Shannon-information <ref> [28] </ref>. Instead it prefers easily learnable regularities, taking into account the costs of gaining information in an RL framework. From each brain's perspective an instruction subsequence is "novel" as long as its outcome surprises the other.
Reference: [29] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, </pages> <year> 1964. </year>
Reference-contexts: Details of noise, for instance, cannot be expected confidently | so they tend to get uninteresting. Examples of learnable regularities. If the instruction set is universal and allows for arbitrary computations, then arbitrary regularities <ref> [10, 2, 29, 11] </ref> may be exploited by both brains to generate or avoid surprises. <p> There may be RL schemes even more general than IS, but this is beyond the scope of this paper. Other notions of regularity. Note that my notion of "simple regularities" differs from, e.g., Kolmogorov complexity theory's <ref> [10, 2, 29, 11, 21] </ref>. There an object is called simple relative to current knowledge x if the size of the shortest algorithm computing it from x is small. The algorithm's computation time is ignored, as are constant factors reflecting Kolmogorov complexity's machine independence.
Reference: [30] <author> J. Storck, S. Hochreiter, and J. Schmidhuber. </author> <title> Reinforcement driven information acquisition in nondeterministic environments. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Paris, </booktitle> <volume> volume 2, </volume> <pages> pages 159-164. </pages> <address> EC2 & Cie, Paris, </address> <year> 1995. </year>
Reference-contexts: To draw the line between goal-oriented and pure exploration I will use a reinforcement learning (RL) framework and say that exploration involves a pure component if there is explicit reward for knowledge growth, besides possible environmental reward. 1 Previous work. Existing RL approaches to pure exploration <ref> [19, 18, 30] </ref> use adaptive predic-tors to predict the next input given previous inputs and actions. The action-generating module gets rewarded in case of predictor failures. Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. <p> In fact, later I will present an example where a curious system indeed outperforms a noncurious one. This does not reflect a universal law though: in general there is no guarantee that curiosity will not turn out to be harmful (e.g., by "killing the cat" <ref> [30] </ref>). Outline. Section 2 will provide formal details of what has been said above, and explain how Incremental Self-Improvement [25, 26] can be used to deal with both brains' complex credit assignment problems (though other machine learning paradigms may also be appropriate). <p> Finally, co-evolutionary methods translating mismatches between reality and expectations into reward for "curious", exploring agents are not new either | see our previous work on "pure" RL-based exploration <ref> [19, 18, 30] </ref>. So, what's new? 19 0 1000 2000 3000 "PLAIN" "CURIOUS" intervals of 10 7 steps (averaged over 10 simulations). Initial advantages of curiosity are lost as more and more goal-oriented training examples become available. Compare Figure 9. Competitors collectively decide. <p> Generality. Another novel aspect is the general setting. Instead of being limited to Markovian domains and simple reactive strategies such as approaches in <ref> [18, 30] </ref>, this paper's set-up allows for quite arbitrary domains and computations. This is made possible by the recent IS paradigm [25, 26]. There is no essential limit (besides computability) to the nature of the regularities that may be exploited to generate surprises. <p> Note that PLAs can be almost anything: neural net algorithms, Bayesian analysis algorithms, etc. All of this may help to understand under which conditions curiosity can actually improve goal-directed learning. It will always be possible, however, to design environments where "curiosity kills the cat" <ref> [30] </ref>, or at least has negative influence on external performance.
Reference: [31] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning and reacting based on dynamic programming. </title> <booktitle> In Machine Learning: Proceedings of the Seventh International Workshop, </booktitle> <year> 1990. </year> <month> 22 </month>
Reference-contexts: Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., [6, 1, 9, 13, 15, 4]. For goal-oriented RL-based exploration see, e.g., <ref> [31, 33, 5, 25] </ref>. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise. I will try to offer a more acceptable alternative.
Reference: [32] <author> G. Tesauro. </author> <title> TD-gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 215-219, </pages> <year> 1994. </year>
Reference-contexts: Compare Figure 10. learner by letting it play against itself is ancient. See, e.g., <ref> [17, 32] </ref>. Even the idea of unsupervised learning through co-evolution of predictors and modules trying to escape the predictions is nothing new | it has been used extensively in our previous work on unsupervised sensory coding [20, 27, 24, 23].
Reference: [33] <author> S. Thrun and K. Moller. </author> <title> Active exploration in dynamic environments. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 531-538. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year> <month> 23 </month>
Reference-contexts: Thus it prefers to go to states where the predictor still can learn something. Essentially there is reward for classic information gain. For related work outside the RL context see, e.g., [6, 1, 9, 13, 15, 4]. For goal-oriented RL-based exploration see, e.g., <ref> [31, 33, 5, 25] </ref>. The "ideal ratio". According to traditional information theory [28] noise is very informative. That is why certain previous pure explorers tend to be interested in intuitively uninteresting noise. I will try to offer a more acceptable alternative.
References-found: 33

