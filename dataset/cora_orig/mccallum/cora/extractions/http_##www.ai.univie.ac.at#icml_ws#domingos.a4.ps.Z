URL: http://www.ai.univie.ac.at/icml_ws/domingos.a4.ps.Z
Refering-URL: http://www.ai.univie.ac.at/icml_ws/program.html
Root-URL: 
Email: pedrod@ics.uci.edu  
Title: Exploiting Context in Feature Selection  
Author: Pedro Domingos 
Web: http://www.ics.uci.edu/~pedrod  
Address: Irvine, California 92717, U.S.A.  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: Most widely-used feature selection methods assume that features are either relevant in the whole instance space or irrelevant throughout. However, it can often be the case that features are relevant only in the context of other features (e.g., feature Y is relevant if feature X = 1, but irrelevant if X = 0). RC is a new feature selection algorithm that takes this into account, by potentially selecting a different set of relevant features for each training instance. When applied to an instance-based learner, it produces higher accuracies than forward and backward sequential selection on a large number of domains, and its advantage increases with increasing context dependency.
Abstract-found: 1
Intro-found: 1
Reference: [Aha, 1989] <author> D. W. Aha. </author> <title> Incremental, instance-based learning of independent and graded concept descriptions. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 387-391, </pages> <address> Ithaca, NY, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More flexible approaches employ one weight per feature value [Nosofsky et al., 1989, Stanfill and Waltz, 1986], one weight per feature per class <ref> [Aha, 1989] </ref>, or a combination of the two [Creecy et al., 1992], and thus exhibit a moderate degree of context sensitivity.
Reference: [Aha and Bankert, 1994] <author> D. W. Aha and R. L. Bankert. </author> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <booktitle> In Proceedings of the 1994 AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> pages 106-112, </pages> <address> Seattle, WA, 1994. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: RC was also found to be faster than BSS and FSS in all domains, sometimes by a large factor [Domingos, in press]. 4 RELATED WORK Variations of FSS and BSS are described and evaluated in <ref> [Aha and Bankert, 1994] </ref>.
Reference: [Aha and Goldstone, 1992] <author> D. W. Aha and R. L. Goldstone. </author> <title> Concept learning and flexible weighting. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 534-539, </pages> <address> Evanston, IL, 1992. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: The most elaborate algorithms have in effect one weight per feature per instance, and are consequently fully context-sensitive; these weights can be assigned at classification time [Atkeson et al., in press] or at learn ing time <ref> [Aha and Goldstone, 1992] </ref>. Seen as a 0-1 fea-ture weighting algorithm, RC falls into this last category. 5 CONCLUSION This paper showed that exploiting context in feature selection can lead to consistent gains in accuracy.
Reference: [Aha et al., 1991] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: 1 INTRODUCTION Feature selection is often a crucial part of an inductive learning application [John et al., 1994], and perhaps never more so than when instance-based learning, or IBL, is used <ref> [Aha et al., 1991] </ref>. A typical feature selection algorithm, like forward sequential selection (FSS) or backward sequential selection (BSS) [Devijver and Kittler, 1982], will consider adding or deleting a feature at a time, and make a decision based on the impact that this has on the learning algorithm's accuracy.
Reference: [Almuallim and Dietterich, 1991] <author> H. Almuallim and T. G. Dietterich. </author> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 547-552, </pages> <address> Menlo Park, CA, 1991. </address> <publisher> AAAI Press. </publisher>
Reference: [Atkeson et al., in press] <author> C. G. Atkeson, A. W. Moore, and S. Schaal. </author> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review. </journal> <note> In press. </note>
Reference-contexts: The most elaborate algorithms have in effect one weight per feature per instance, and are consequently fully context-sensitive; these weights can be assigned at classification time <ref> [Atkeson et al., in press] </ref> or at learn ing time [Aha and Goldstone, 1992]. Seen as a 0-1 fea-ture weighting algorithm, RC falls into this last category. 5 CONCLUSION This paper showed that exploiting context in feature selection can lead to consistent gains in accuracy.
Reference: [Cain et al., 1991] <author> T. Cain, M. J. Pazzani, and G. Sil-verstein. </author> <title> Using domain knowledge to influence similarity judgments. </title> <booktitle> In Proceedings of the Case-Based Reasoning Workshop, </booktitle> <pages> pages 191-199, </pages> <address> Washington, DC, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Cardie, 1993] <author> C. Cardie. </author> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 25-32, </pages> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Caruana and Freitag, 1994] <author> R. Caruana and D. Fre-itag. </author> <title> Greedy attribute selection. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 28-36, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Creecy et al., 1992] <author> R. H. Creecy, B. M. Masand, S. J. Smith, and D. L. Waltz. </author> <title> Trading MIPS and memory for knowledge engineering. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 48-63, </pages> <year> 1992. </year>
Reference-contexts: More flexible approaches employ one weight per feature value [Nosofsky et al., 1989, Stanfill and Waltz, 1986], one weight per feature per class [Aha, 1989], or a combination of the two <ref> [Creecy et al., 1992] </ref>, and thus exhibit a moderate degree of context sensitivity. In the case of continuous features, it is also possible to take into account the relative values of the feature in the instance and the example being classified, resulting in directional weights [Ricci and Avesani, 1995].
Reference: [DeGroot, 1986] <author> M. H. </author> <title> DeGroot. Probability and Statistics. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, 2nd edition, </address> <year> 1986. </year>
Reference-contexts: BSS have only a probability of occurrence of 1/1000. This results in very high confidence that RC is a more accurate algorithm than FSS and BSS on the population of domains from which the 24 used are drawn. Line four shows the result of a Wilcoxon signed-ranks test <ref> [DeGroot, 1986] </ref>, a more sensitive procedure that also takes into account the relative magnitudes of the differences observed, though not their absolute values; a large difference in accuracy is considered more significant than a small one.
Reference: [Devijver and Kittler, 1982] <author> P. A. Devijver and J. Kit-tler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <address> Prentice/Hall, Englewood Cliffs, N.J., </address> <year> 1982. </year>
Reference-contexts: 1 INTRODUCTION Feature selection is often a crucial part of an inductive learning application [John et al., 1994], and perhaps never more so than when instance-based learning, or IBL, is used [Aha et al., 1991]. A typical feature selection algorithm, like forward sequential selection (FSS) or backward sequential selection (BSS) <ref> [Devijver and Kittler, 1982] </ref>, will consider adding or deleting a feature at a time, and make a decision based on the impact that this has on the learning algorithm's accuracy. Features are either wholly removed from consideration, or kept and used at all times. <p> RC was also found to be faster than BSS and FSS in all domains, sometimes by a large factor [Domingos, in press]. 4 RELATED WORK Variations of FSS and BSS are described and evaluated in [Aha and Bankert, 1994]. Beyond the pattern recognition approaches surveyed in [Kittler, 1986] and <ref> [Devijver and Kittler, 1982] </ref>, many methods for feature selection have been proposed in the artificial intelligence literature in recent years [Kibler and Aha, 1987, Almuallim and Dietterich, 1991, Kira and Ren-dell, 1992, Cardie, 1993, Schlimmer, 1993, Vafaie and DeJong, 1993, Caruana and Freitag, 1994, John et al., 1994, Langley and Sage,
Reference: [Domingos, 1995] <author> P. Domingos. </author> <title> Rule induction and instance-based learning: A unified approach. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1226-1232, </pages> <address> Montreal, Canada, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An instance-based learner that assigns each test example to the class of the nearest training example, using Euclidean distance for numeric features and a simplified version of Stanfill and Waltz's value difference metric for symbolic features <ref> [Stanfill and Waltz, 1986, Domingos, 1995] </ref>, was used. 2 CONTEXT-SENSITIVE FEATURE SELECTION This section describes RC (Relevance in Context), a context-sensitive feature selection algorithm. RC makes local, instance-specific decisions on feature relevance, as opposed to global ones.
Reference: [Domingos, in press] <author> P. Domingos. </author> <title> Context-sensitive feature selection for lazy learners. </title> <journal> Artificial Intelligence Review. </journal> <note> In press. </note>
Reference-contexts: This reflects a simplicity bias; it is discussed further in <ref> [Domingos, in press] </ref>. Avoiding redundant computations, RC's worst-case time complexity is O (N 2 F 2 ), where N is the number of training examples and F is the initial number of features. <p> This is O (F ) faster than a naive implementation of forward/backward selection, and similar to an optimized version of them (the latter, however, is not always possible). See <ref> [Domingos, in press] </ref> for further details. 3 EMPIRICAL STUDY An empirical study was carried out to test the hypothesis that RC will produce higher accuracies than FSS and BSS when feature relevance is significantly context-dependent. For this purpose, a measure of context dependency is necessary. <p> Unfortunately, in real-world domains the "true" degree of context dependency for a target concept is necessarily unknown. One way to circumvent this problem is to carry out studies in artificial domains, where the context dependency can be predetermined by the experimenter, and this is described in <ref> [Domingos, in press] </ref>. Another approach is to find an empirical measure that is thought to correlate positively with context dependency. One possibility is to find out how far RC strays from selecting the same features for all instances (i.e, from doing the same as FSS and BSS). <p> If RC, BSS and FSS all incorporate the "best" features, then their accuracies should not be expected to differ by more than this amount. RC was also found to be faster than BSS and FSS in all domains, sometimes by a large factor <ref> [Domingos, in press] </ref>. 4 RELATED WORK Variations of FSS and BSS are described and evaluated in [Aha and Bankert, 1994].
Reference: [Holte, 1993] <author> R. C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91, </pages> <year> 1993. </year>
Reference: [John et al., 1994] <author> G. H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 121-129, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: 1 INTRODUCTION Feature selection is often a crucial part of an inductive learning application <ref> [John et al., 1994] </ref>, and perhaps never more so than when instance-based learning, or IBL, is used [Aha et al., 1991].
Reference: [Kelly and Davis, 1991] <author> J. D. Kelly and L. Davis. </author> <title> A hybrid genetic algorithm for classification. </title> <booktitle> In Proceedings of the Twelfth International Joint Confer ence on Artificial Intelligence, </booktitle> <pages> pages 645-650, </pages> <address> Syd--ney, Australia, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Kibler and Aha, 1987] <author> D. Kibler and D. W. Aha. </author> <title> Learning representative exemplars of concepts: An initial case study. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pages 24-30, </pages> <address> Irvine, CA, 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Kira and Rendell, 1992] <author> K. Kira and L. A. Rendell. </author> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 249-256, </pages> <address> Aberdeen, Scotland, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Kittler, 1986] <author> J. Kittler. </author> <title> Feature selection and extraction. </title> <editor> In T. Y. Young and K. S. Fu, editors, </editor> <booktitle> Handbook of Pattern Recognition and Image Processing. </booktitle> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1986. </year>
Reference-contexts: RC was also found to be faster than BSS and FSS in all domains, sometimes by a large factor [Domingos, in press]. 4 RELATED WORK Variations of FSS and BSS are described and evaluated in [Aha and Bankert, 1994]. Beyond the pattern recognition approaches surveyed in <ref> [Kittler, 1986] </ref> and [Devijver and Kittler, 1982], many methods for feature selection have been proposed in the artificial intelligence literature in recent years [Kibler and Aha, 1987, Almuallim and Dietterich, 1991, Kira and Ren-dell, 1992, Cardie, 1993, Schlimmer, 1993, Vafaie and DeJong, 1993, Caruana and Freitag, 1994, John et al., 1994,
Reference: [Langley and Sage, 1994] <author> P. Langley and S. Sage. </author> <title> Oblivious decision trees and abstract cases. </title> <booktitle> In Proceedings of the 1994 AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> pages 113-117, </pages> <address> Seattle, CA, 1994. </address> <publisher> AAAI Press. </publisher>
Reference: [Lee, 1994] <author> C. Lee. </author> <title> An instance-based learning method for databases: An information theoretic approach. </title> <booktitle> In Proceedings of the Ninth European Conference on Machine Learning, </booktitle> <pages> pages 387-390, </pages> <address> Cata-nia, Italy, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference: [Mohri and Tanaka, 1994] <author> T. Mohri and H. Tanaka. </author> <title> An optimal weighting criterion of case indexing for both numeric and symbolic attributes. </title> <booktitle> In Proceedings of the 1994 AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> pages 123-127, </pages> <address> Seattle, WA, 1994. </address> <publisher> AAAI Press. </publisher>
Reference: [Murphy and Aha, 1995] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. Machine-readable data repository, </title> <institution> Department of Information and Computer Science, University of California at Irvine, </institution> <address> Irvine, CA, </address> <year> 1995. </year>
Reference-contexts: The core of the study that follows will thus be to correlate the feature difference D with the differential accuracy of RC and the context-free algorithms. Twenty-four datasets from the UCI repository were used <ref> [Murphy and Aha, 1995] </ref>. Twenty runs were carried out for each, with the training set being composed of two-thirds of the examples, chosen at random, and the remainder being used as test examples.
Reference: [Nosofsky et al., 1989] <author> R. M. Nosofsky, S. E. Clark, and H. J. Shin. </author> <title> Rules and exemplars in categorization, identification, and recognition. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 15 </volume> <pages> 282-304, </pages> <year> 1989. </year>
Reference-contexts: In the representationally simplest schemes, there is one weight per feature, and they are therefore completely context-free [Kelly and Davis, 1991, Salzberg, 1991, Lee, 1994, Mohri and Tanaka, 1994]. More flexible approaches employ one weight per feature value <ref> [Nosofsky et al., 1989, Stanfill and Waltz, 1986] </ref>, one weight per feature per class [Aha, 1989], or a combination of the two [Creecy et al., 1992], and thus exhibit a moderate degree of context sensitivity.
Reference: [Ricci and Avesani, 1995] <author> F. Ricci and P. Avesani. </author> <title> Learning a local similarity metric for case-based reasoning. </title> <booktitle> In Proceedings of the First International Conference on Case-Based Reasoning, </booktitle> <address> Sesim-bra, Portugal, 1995. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In the case of continuous features, it is also possible to take into account the relative values of the feature in the instance and the example being classified, resulting in directional weights <ref> [Ricci and Avesani, 1995] </ref>. The most elaborate algorithms have in effect one weight per feature per instance, and are consequently fully context-sensitive; these weights can be assigned at classification time [Atkeson et al., in press] or at learn ing time [Aha and Goldstone, 1992].
Reference: [Salzberg, 1991] <author> S. Salzberg. </author> <title> A nearest hyperrectan-gle learning method. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 251-276, </pages> <year> 1991. </year>
Reference: [Schlimmer, 1993] <author> J. C. Schlimmer. </author> <title> Efficiently inducing determinations: A complete and systematic search algorithm that uses optimal pruning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 284-290, </pages> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Skalak, 1992] <author> D. B. Skalak. </author> <title> Representing cases as knowledge sources that apply local similarity met-rics. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 325-330, </pages> <address> Evanston, IL, 1992. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: [Skalak, 1994] <author> D. B. Skalak. </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 293-301, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: [Stanfill and Waltz, 1986] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference-contexts: An instance-based learner that assigns each test example to the class of the nearest training example, using Euclidean distance for numeric features and a simplified version of Stanfill and Waltz's value difference metric for symbolic features <ref> [Stanfill and Waltz, 1986, Domingos, 1995] </ref>, was used. 2 CONTEXT-SENSITIVE FEATURE SELECTION This section describes RC (Relevance in Context), a context-sensitive feature selection algorithm. RC makes local, instance-specific decisions on feature relevance, as opposed to global ones. <p> In the representationally simplest schemes, there is one weight per feature, and they are therefore completely context-free [Kelly and Davis, 1991, Salzberg, 1991, Lee, 1994, Mohri and Tanaka, 1994]. More flexible approaches employ one weight per feature value <ref> [Nosofsky et al., 1989, Stanfill and Waltz, 1986] </ref>, one weight per feature per class [Aha, 1989], or a combination of the two [Creecy et al., 1992], and thus exhibit a moderate degree of context sensitivity.
Reference: [Vafaie and DeJong, 1993] <author> H. Vafaie and K. DeJong. </author> <title> Robust feature selection algorithms. </title> <booktitle> In Proceedings of the Fifth IEEE International Conference on Tools for Artificial Intelligence, </booktitle> <pages> pages 356-363, </pages> <address> Boston, MA, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
References-found: 32

