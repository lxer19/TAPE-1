URL: http://www.neci.nj.nec.com/homepages/pny/papers/mlnm/mlnm.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/mlnm/main.html
Root-URL: 
Title: Metric Learning via Normal Mixtures  
Author: Peter N. Yianilos 
Keyword: Nearest Neighbor Search, Metric Learning, Normal/Gaussian Mix ture Densities, Unsupervised Learning, Neural Network, Encoder Network.  
Address: Independence Way, Princeton, NJ 08540  
Affiliation: NEC Research Institute  4  
Pubnum: Technical Memorandum:  
Email: pny@research.nj.nec.com  
Date: October 4, 1995  
Abstract: Natural learners rarely have access to perfectly labeled data motivating the study of unsupervised learning in an attempt to assign labels. An alternative viewpoint, which avoids the issue of labels entirely, has as the learner's goal the discovery of an effective metric with which similarity judgments can be made. We refer to this paradigm as metric learning. Effective classification, for example, then becomes a consequence rather than the direct purpose of learning. Consider the following setting: a database made up of exactly one observation of each of many different objects. This paper shows that, under admittedly strong assumptions, there exists a natural prescription for metric learning in this data starved case. Our outlook is stochastic, and the metric we learn is represented by a joint probability density estimated from the observed data. We derive a closed-form expression for the value of this density starting from an explanation of the data as a Gaussian Mixture. Our framework places two known classification techniques of statistical pattern recognition at opposite ends of a spectrum and describes new intermediate possibilities. The notion of a stochastic equivalence predicate is introduced and striking differences between its behavior and that of conventional metrics are illuminated. As a result one of the basic tenets of nearest-neighbor-based classification is challenged. NEC Research Institute Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1973. </year> <note> 19 NEC Research Institute Technical Report </note>
Reference-contexts: The observations we consider are represented by vectors from a finite dimensional real vector space. To put this work in perspective we begin by reviewing two common approaches to pattern classification. In the statistical pattern recognition outlook <ref> [1] </ref>, one typically assumes that patterns are generated by some process of change operating on one or more starting points. Commonly these starting points represent vector means of object classes, and the process of change is assumed to be modeled by normal densities about these means.
Reference: [2] <author> R. D. Short and K. Fukunaga, </author> <title> "The optimal distance measure for nearest neighbor classification," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 27, </volume> <month> September </month> <year> 1981. </year>
Reference-contexts: Humans for example can classify numeric digits long before they've seen enough examples so that pixel by pixel correlation yields a satisfactory result. Improved metrics for nearest neighbor classification were proposed by Fukanaga et al in <ref> [2] </ref> and [3], and later in [4] and [5]. But again, given very few members of each class, or in the entirely unsupervised case, their results do not apply and it is not at all clear what if anything can be done to choose a good metric.
Reference: [3] <author> K. Fukunaga and T. E. Flick, </author> <title> "An optimal global nearest neighbor metric," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 6, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: Humans for example can classify numeric digits long before they've seen enough examples so that pixel by pixel correlation yields a satisfactory result. Improved metrics for nearest neighbor classification were proposed by Fukanaga et al in [2] and <ref> [3] </ref>, and later in [4] and [5]. But again, given very few members of each class, or in the entirely unsupervised case, their results do not apply and it is not at all clear what if anything can be done to choose a good metric.
Reference: [4] <author> J. H. Friedman, </author> <title> "Flexible metric nearest neighbor classification," </title> <type> tech. rep., </type> <institution> Stanford University, Dept. of Statistics, </institution> <year> 1994. </year>
Reference-contexts: Humans for example can classify numeric digits long before they've seen enough examples so that pixel by pixel correlation yields a satisfactory result. Improved metrics for nearest neighbor classification were proposed by Fukanaga et al in [2] and [3], and later in <ref> [4] </ref> and [5]. But again, given very few members of each class, or in the entirely unsupervised case, their results do not apply and it is not at all clear what if anything can be done to choose a good metric.
Reference: [5] <author> D. G. Lowe, </author> <title> "Similarity metric learning for a variable-kernal classifier," </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 72-85, </pages> <year> 1995. </year>
Reference-contexts: Humans for example can classify numeric digits long before they've seen enough examples so that pixel by pixel correlation yields a satisfactory result. Improved metrics for nearest neighbor classification were proposed by Fukanaga et al in [2] and [3], and later in [4] and <ref> [5] </ref>. But again, given very few members of each class, or in the entirely unsupervised case, their results do not apply and it is not at all clear what if anything can be done to choose a good metric.
Reference: [6] <author> T. Kanade, </author> <title> Computer Recognition of Human Faces. </title> <publisher> Birkhauser Verlag, </publisher> <address> Stuttgart Germany, </address> <year> 1977. </year>
Reference-contexts: Addressing the first problem is the subject of the remainder of this section, leading eventually to our first main result. The case we've just considered corresponds to a generalization of the technique used for example in <ref> [6] </ref> where inverse variances are used to weight Euclidean distance providing similarity judgments. This amounts to assuming a diagonal covariance matrix, then forming the vector QY and selecting the Y that maximizes the probability of this difference given the zero mean normal density arising from this diagonal matrix.
Reference: [7] <author> I. J. Cox, J. Ghosn, and P. N. Yianilos, </author> <title> "Experiments on feature-based face recognition using gaussian models and mixtures," </title> <type> tech. rep., </type> <institution> The NEC Research Institute, Princeton, </institution> <address> New Jersey, </address> <year> 1995. </year>
Reference-contexts: In a sense, the complex earlier sections are the author's attempt to deal with the conceptual problems associated with these early discoveries. It is worth noting that however flawed conceptually, these forms perform very well in limited experiments <ref> [7] </ref>. <p> Over short ranges however we expect that Pr (Y jQ) Pr (QjY ) and therefore tolerate this reformulation of the problem. The next practical expedient consists of recording for each database element the component k for which this quantity is maximized and assuming all others to be zero <ref> [7] </ref>. This amounts to precomputed hard vector quantization of the database. We are then left to deal with only a single mixture component for each database record. <p> If M (n) denotes a mixture model with n elements, then this amounts to computing: Pr (Q Y ) = n In one experiment <ref> [7] </ref>, this blended distance performed as well as the single best value. <p> One multiplies the off-diagonal entries of by some scaling factor s 2 [0; 1). Avoiding unity has the additional advantage of preventing ill-conditioned matrices (assuming none of the variances are near zero). This is discussed in greater detail in <ref> [7] </ref>. It should be mentioned that given enough data one might attempt to assign different values of s to each mixture component. Intuitively this makes sense since some components may have many members while others have relatively few. <p> Our joint work on face recognition from feature vectors [11] provided much needed concrete examples from which the author drew the intuition needed to complete this work. This work now continues with the authors above as well as Joumana Ghosn <ref> [7] </ref>. I thank Adam Grove and Steve Omohundro for helpful discussions and comments on earlier drafts. I also thank Eric S. Ristad. Our joint work on handwriting recognition and stochastic modeling contributed greatly to my thinking in this area.
Reference: [8] <author> L. R. Liporace, </author> <title> "Maximum likelihood estimation for multivariate observations of markov sources," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 729-734, </pages> <year> 1982. </year>
Reference-contexts: Liporace <ref> [8] </ref> demonstrates that this introduces only small complications for a somewhat general class of densities. 7 A Simple Example Consider a two dimensional distribution made up of two zero mean normal densities, of equal probability, but with covariance matrices which result in ellipsoidal contours that are very different.
Reference: [9] <author> D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, </author> <title> "A learning algorithm for boltzmann machines," </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 147-169, </pages> <year> 1985. </year>
Reference-contexts: Here our discrete mixture would attempt to approximate this continuous variation by covering various regions with different normal densities. 8 A Neural Network Application Encoder nets 8 <ref> [9] </ref> are feed-forward neural networks which include a highly constricted layer forcing the network to somehow compress/encode its inputs and then decompress/decode them to arrive at an output which approximates the inputs. This well known design approach is an effective way to avoid over-training.
Reference: [10] <author> L. Bottou, J. Denker, H. Drucker, I. Guyon, L. Jackel, Y. LeCun, U. Muller, E. Sackinger, P. Simard, and V. Vapnik, </author> <title> "Comparison of classifier methods: A case study in handwritten digit recognition," </title> <booktitle> in Proc. 12th IAPR International Conference on Pattern Recognition, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: This distance can then be used to effect classification via nearest neighbor search of some labeled dataset. Thus after training, the final stages of the network can essentially be discarded. There is some evidence that this general approach is effective. In <ref> [10] </ref> a neural net the authors call LeNet-4 was trained to achieve 1.1% classification error for the problem of handwritten digit recognition. The authors realized that a particular 50-unit layer might be thought of as a feature vector, and built a pattern classifier using the Euclidean distance metric.
Reference: [11] <author> I. J. Cox, P. N. Yianilos, S. L. Hingorani, and D. W. Jacobs, </author> <title> "Experiments on feature-based face recognition," </title> <type> tech. rep., </type> <institution> The NEC Research Institute, Princeton, </institution> <address> New Jersey, </address> <year> 1995. </year> <note> 20 NEC Research Institute Technical Report </note>
Reference-contexts: Many other applications might benefit from our techniques. These include almost any problem for which nearest neighbor search is somewhat effective. 10 Acknowledgments The author thanks Ingemar Cox and David Jacobs for helpful discussions. Our joint work on face recognition from feature vectors <ref> [11] </ref> provided much needed concrete examples from which the author drew the intuition needed to complete this work. This work now continues with the authors above as well as Joumana Ghosn [7]. I thank Adam Grove and Steve Omohundro for helpful discussions and comments on earlier drafts.
References-found: 11

