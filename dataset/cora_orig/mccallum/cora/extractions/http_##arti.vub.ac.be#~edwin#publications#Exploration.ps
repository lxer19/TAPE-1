URL: http://arti.vub.ac.be/~edwin/publications/Exploration.ps
Refering-URL: http://euler.mcs.utulsa.edu/~sandip/wshop/97/
Root-URL: 
Email: edwin@arti.vub.ac.be  
Title: An Accumulative Exploration Method for Reinforcement Learning  
Author: Edwin de Jong 
Address: Pleinlaan 2, B-1050 Brussels  
Affiliation: Artificial Intelligence Laboratory Vrije Universiteit Brussel  
Abstract: Agents in Multi Agent Systems can coordinate their actions by communicating. We investigate a minimal form of communication, where the signals that agents send represent evaluations of the behavior of the receiving agent. Learning to act according to these signals is a typical Reinforcement Learning problem. The backpropagation neural network has been used to predict rewards that will follow an action. The first results made clear that a mechanism for balancing between exploitation and exploration was needed. We introduce the Exploration Buckets algorithm, a method that favors both actions with high prediction errors and actions that have been ignored for some time. The algorithm's scope is not restricted to a single learning algorithm, and its main characterics are its insensibility to large (or even continuous) state spaces and its appropriateness for online learning; the Exploration/Exploitation balance does not depend on properties external to the system, such as time. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Benda, M., Jagannathan, V., and Dodhiawalla, R. </author> <year> (1988). </year> <title> On optimal cooperation of knowledge sources. </title> <type> Technical Report BCS-G2010-28, </type> <institution> Boeing AI Center. </institution>
Reference-contexts: See (Kaelbling et al., 1996) for a review. The scope of this design method is limited by the power of existing learning algorithms. As a testcase, it has been applied to the Pursuit Problem, first described by <ref> (Benda et al., 1988) </ref>. The first task, defining appropriate coordination signals, has been accomplished satisfactorily, see (De Jong, 1997) for an account. Here, we describe the problem that had to be faced for solving the second task, i.e. implementing a reinforcement learning agent.
Reference: <author> Cohn, D. A. </author> <year> (1994). </year> <title> Neural network exploration using op-timal experiment design. </title> <note> Technical Report AI Memo 1491 and CBCL Paper 99, </note> <institution> MIT AI Lab and Center for Biological and Computational Learning Department of Brain and Cognitive Sciences. </institution>
Reference-contexts: This interesting recency-based strategy is restricted to problems with small numbers of states and actions. In <ref> (Cohn, 1994) </ref>, an algorithm based on Federov's Theory of Optimal Experiment Design, see (Federov, 1972), is described. This algorithm can be very useful for choosing expensive experiments, such as drilling oil wells, but will generally be too slow for online learning.
Reference: <author> Dayan, P. and Sejnowski, T. J. </author> <year> (1996). </year> <title> Exploration bonuses and dual control. </title> <journal> Machine Learning, </journal> <volume> 25. </volume>
Reference-contexts: In (Cohn, 1994), an algorithm based on Federov's Theory of Optimal Experiment Design, see (Federov, 1972), is described. This algorithm can be very useful for choosing expensive experiments, such as drilling oil wells, but will generally be too slow for online learning. In <ref> (Dayan and Sejnowski, 1996) </ref>, an estimation of the Bayesian balance between exploration and exploitation is computed. This method is limited to systems that keep a model of state transitions. We introduce the Exploration Buckets algorithm, which meets all of the above requirements.
Reference: <author> De Jong, E. D. </author> <year> (1997). </year> <title> Multi-agent coordination by communication of evaluations. </title> <booktitle> In Proceedings of Modeling Autonomous Agents in a Multi Agent World MAAMAW '97. </booktitle>
Reference-contexts: Introduction When designing Multi Agent Systems (MAS), diminishing complex communication increases generality. Agents that use complex communication inherently depend on mutual knowledge. In previous work <ref> (De Jong, 1997) </ref>, a framework for MAS design has been described in which a minimal form of communication is used; the only messages that agents send are numbers between 0 and 1 that represent evaluations of the recipient's behavior. <p> The scope of this design method is limited by the power of existing learning algorithms. As a testcase, it has been applied to the Pursuit Problem, first described by (Benda et al., 1988). The first task, defining appropriate coordination signals, has been accomplished satisfactorily, see <ref> (De Jong, 1997) </ref> for an account. Here, we describe the problem that had to be faced for solving the second task, i.e. implementing a reinforcement learning agent.
Reference: <author> Federov, V. </author> <year> (1972). </year> <title> Theory of Optimal Experiments. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: This interesting recency-based strategy is restricted to problems with small numbers of states and actions. In (Cohn, 1994), an algorithm based on Federov's Theory of Optimal Experiment Design, see <ref> (Federov, 1972) </ref>, is described. This algorithm can be very useful for choosing expensive experiments, such as drilling oil wells, but will generally be too slow for online learning. In (Dayan and Sejnowski, 1996), an estimation of the Bayesian balance between exploration and exploitation is computed.
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Later on (not shown), the influence of exploration drops. predictions of action 'move left' are quickly increased. this make this learning problem a rather difficult one. These first experiences with the Exploration Buckets algorithm show that it performed better than Roulette exploration, see <ref> (Goldberg, 1989) </ref>, which was used as a comparison. With Roulette exploration, the outputs of the network are used as relative probabilities to choose the corresponding action. Using a delayed rewards algorithm, see (Sutton, 1988), could improve this.
Reference: <author> Gullapalli, V. </author> <year> (1990). </year> <title> A stochastic reinforcement algorithm for learning real-valued functions. </title> <journal> Neural Netw., </journal> <volume> 3 </volume> <pages> 671-692. </pages>
Reference-contexts: Similarly, dependency of this balance on the running time is to be avoided when designing an online learning agent. This excludes the popular exploration mechanisms that depend on the slowly decaying influence of randomness , such as Gullapalli's SRV unit <ref> (Gullapalli, 1990) </ref>, which implements exploration by adding random numbers from a zero-mean Gaussian distribution to a neuron's output and gradually decreasing the variance of this distribution over time.
Reference: <author> Kaelbling, L. P., Littman, M. L., and Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4. </volume>
Reference-contexts: In RL, the learner chooses actions which affect the environment and yield rewards, which are high if the action is considered appropriate. Since these rewards do not contain information about the direction of the error in the action vector, RL refers to a class of semi-supervised learning problems. See <ref> (Kaelbling et al., 1996) </ref> for a review. The scope of this design method is limited by the power of existing learning algorithms. As a testcase, it has been applied to the Pursuit Problem, first described by (Benda et al., 1988). <p> In a single-state environment, this problem reduces to the k-armed bandit problem, for which formally justified solutions exist; in the general multi-state, delayed-reinforcement case that we are concerned with here, no theoretically guaranteed methods can be applied <ref> (Kaelbling et al., 1996) </ref>. The Pursuit Problem In our version of the pursuit problem, based on (Stephens and Merx, 1990) one prey and several predators are placed in a 30 x 30 grid. The prey is captured when the four orthogonal positions around it are occupied by predators.
Reference: <editor> Rumelhart, D., McClelland, J., and et. al. </editor> <booktitle> (1987). Parallel distributed processing; explorations in the microstructure of cognition, </booktitle> <volume> volume 1-2. </volume> <publisher> MIT Press. </publisher>
Reference-contexts: This scheme is not investigated here, since its assumptions (i.e. a good action will always result in an increase of the reward) make it less generally applicable. As a learning algorithm, we use the common backpropagation network, see <ref> (Rumelhart et al., 1987) </ref>. The inputs are the polar positions of all predators (including the learning agent) and the cartesian position of the prey, and a bias input neuron. The network has one hidden layer of 20 neurons. The output layer contains 5 neurons, each representing an action.
Reference: <author> Sandholm, T. W. and Crites, R. H. </author> <year> (1995). </year> <title> On multia-gent q-learning in a semi-competitive domain. </title> <editor> In G. Weiss, S. S., editor, </editor> <booktitle> Adaption and Learning in multi-agent systems, </booktitle> <pages> pages 191-205, </pages> <address> Berlin, Heidelberg. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Finding a Suitable Reinforcement Learning Algorithm A common algorithm for reinforcement learning problems is Q-learning (Watkins, 1989). This technique found its way into the domain of multi agent systems, see e.g. (Sen and Sekaran, 1995), <ref> (Sandholm and Crites, 1995) </ref>. Q-learning involves storing valuations for each possible action in every state. This is only feasible in problems with a small number of possible states and actions.
Reference: <author> Sen, S. and Sekaran, M. </author> <year> (1995). </year> <title> Multiagent coordination with learning classifier systems. </title> <editor> In G. Weiss, S. S., editor, </editor> <booktitle> Adaption and Learning in multi-agent systems, </booktitle> <pages> pages 218-233, </pages> <address> Berlin, Heidelberg. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Finding a Suitable Reinforcement Learning Algorithm A common algorithm for reinforcement learning problems is Q-learning (Watkins, 1989). This technique found its way into the domain of multi agent systems, see e.g. <ref> (Sen and Sekaran, 1995) </ref>, (Sandholm and Crites, 1995). Q-learning involves storing valuations for each possible action in every state. This is only feasible in problems with a small number of possible states and actions.
Reference: <author> Stephens, L. M. and Merx, M. B. </author> <year> (1990). </year> <title> The effect of agent control strategy on the performance of a dai pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop. </booktitle>
Reference-contexts: The Pursuit Problem In our version of the pursuit problem, based on <ref> (Stephens and Merx, 1990) </ref> one prey and several predators are placed in a 30 x 30 grid. The prey is captured when the four orthogonal positions around it are occupied by predators. We use Stephens and Merx's efficiency measures.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <type> Technical Report TR87-509.1, </type> <institution> GTE Laboratories. </institution>
Reference-contexts: These first experiences with the Exploration Buckets algorithm show that it performed better than Roulette exploration, see (Goldberg, 1989), which was used as a comparison. With Roulette exploration, the outputs of the network are used as relative probabilities to choose the corresponding action. Using a delayed rewards algorithm, see <ref> (Sutton, 1988) </ref>, could improve this. The network using the Exploration Buckets algorithm that has been presented can thus successfully be used for finding a balance between exploration and exploitation for reinforcement learning problems with direct rewards. Conclusions We investigated the possibilities for exploration in a particular a reinforcement learning problem.
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh Int. Conf. on Machince Learning, </booktitle> <pages> pp. 314-321, pages 216-224. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A C E + + = = = Actions Exploration buckets Eligibilities x x x B D Many approaches to exploration feature some of these requirements, but none was encountered that satisfies all. <ref> (Sutton, 1990) </ref>, introduces exploration bonuses that keep a memory for each state x and each action a of "the number of time steps that have elapsed since a was tried in x in a real experience".
Reference: <author> Sutton, R. S. </author> <year> (1993). </year> <title> Online learning with random representations. </title> <booktitle> In Proceedings of the Tenth Int. Conf. on Machince Learning, </booktitle> <pages> pp. 314-321, pages 314-321. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We are interested in finding an exploration mechanism that possesses the following properties: * It can be used for online learning (Thrun, 1992) describes several algorithms that distinguish between the learning phase and the performing phase. As <ref> (Sutton, 1993) </ref> notes, in order to obtain learning systems instead of merely learned ones, online learning is required. This refers to learning that is done during the operational phase of a system.
Reference: <author> Sutton, R. S. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 8 </volume> <pages> 1038-1044. </pages>
Reference-contexts: Although the number of possible actions is only 5 (moving in the compass directions and staying), the number of combinations of state and action already amounts to 5 900! 895! = = 2:9 10 15 1 To avoid this combinatorial explosion, several methods for generalization exist. In <ref> (Sutton, 1996) </ref>, a function approximator is applied to discretize a continuous state space, so that algorithms based on storing information about combinations of actions and states can be used all the same.
Reference: <author> Thrun, S. </author> <year> (1992). </year> <title> The role of exploration in learning control. </title> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: But when no attention is paid to exploring actions with lower expectated rewards, underrated actions will never be recognized as such. The task of an Exploration / Exploition algorithm is to find a good balance between these. According to <ref> (Thrun, 1992) </ref>, exploration mechanisms can be divided into undirected and directed strategies. Within the class of directed strategies, which are generally more successful than undi-rected ones such as random or stochastic exploration, three types are encountered: stategies based on counters, on recency and on errors. <p> We are interested in finding an exploration mechanism that possesses the following properties: * It can be used for online learning <ref> (Thrun, 1992) </ref> describes several algorithms that distinguish between the learning phase and the performing phase. As (Sutton, 1993) notes, in order to obtain learning systems instead of merely learned ones, online learning is required. This refers to learning that is done during the operational phase of a system.
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Ph.D. Thesis. </type>
Reference-contexts: Finding a Suitable Reinforcement Learning Algorithm A common algorithm for reinforcement learning problems is Q-learning <ref> (Watkins, 1989) </ref>. This technique found its way into the domain of multi agent systems, see e.g. (Sen and Sekaran, 1995), (Sandholm and Crites, 1995). Q-learning involves storing valuations for each possible action in every state. This is only feasible in problems with a small number of possible states and actions.
Reference: <author> Wilson, S. W. </author> <year> (1996). </year> <title> Explore / exploit strategies in autonomy. </title> <editor> In et.al., P. M., editor, </editor> <booktitle> Proceedings of the fourth international conference on simulation of adaptive behavior. From animals to animats 4. </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: This refers to learning that is done during the operational phase of a system. Online learning agents that have to operate in a changing environment should never stop learning, and should therefore avoid to distinguish between learning and operating phases. <ref> (Wilson, 1996) </ref> gives a systematic account of the variables on which the balance between exploration and exploitation may depend. Similarly, dependency of this balance on the running time is to be avoided when designing an online learning agent.
References-found: 19

