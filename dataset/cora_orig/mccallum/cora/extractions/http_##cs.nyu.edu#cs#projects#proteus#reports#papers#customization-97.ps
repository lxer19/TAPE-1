URL: http://cs.nyu.edu/cs/projects/proteus/reports/papers/customization-97.ps
Refering-URL: http://cs.nyu.edu/cs/projects/proteus/reports/index.html
Root-URL: http://www.cs.nyu.edu
Title: Customization of Information Extraction Systems  
Author: Roman Yangarber and Ralph Grishman 
Address: New York University  
Affiliation: Computer Science Department  
Abstract: Information Extraction (IE) is becoming an increasingly fast and accurate technology for extracting information about specific relationships and events from free natural language text. However, adapting an IE system to a new class of events remains a time-consuming and expensive task. We describe a suite of tools for rapidly adapting a system to new domains by allowing the user to provide examples of the events of interest, and of the associated database entries to be created. From these examples, the tool automatically creates appropriate patterns for text analysis.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, MD, </institution> <address> Novem-ber 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Furthermore, they allow a reasonable measurement of performance and progress in the field. Indeed, over the last decade, competing IE systems have undergone a series of formal evaluations theMessage Understanding Conferences (MUCs), <ref> [1, 8] </ref>. The results of these evaluations to date show that, although IE is a promising technology, it requires much further research.
Reference: [2] <author> Douglas Appelt, Jerry Hobbs, John Bear, David Israel, Megumi Kameyama, Andy Kehler, David Martin, Karen Meyers, and Mabry Tyson. </author> <title> SRI International FASTUS system: MUC-6 test results and analysis. </title> <booktitle> In Proc. Sixth Message Understanding Conf. </booktitle> <address> (MUC-6), Columbia, MD, November 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Pattern matching is thus a form of deterministic, bottom-up partial parsing. At present, it is the most widely used strategy in the IE community, superseding full syntactic analysis of earlier IE systems <ref> [3, 2] </ref>. The name recognition module identifies proper names in the text by using local contextual cues, such as capitalization, personal titles ("Mr.", "Esq."), and company suffixes ("Inc.", "Co."). Name recognition is a well-researched topic, with the best available systems today reaching 97% accuracy. <p> The new pattern can then be matched against the rest of the training corpus, to refine its operation and to reduce overgeneration. Syntactic generalization: Actually, the pattern base would acquire much more than the basic pattern that the user accepted. The system applies built-in meta-rules <ref> [2, 6] </ref>, to automatically produce a set of syntactic transformations from a basic, active clause pattern.
Reference: [3] <author> Douglas Appelt, Jerry Hobbs, John Bear, David Israel, and Mabry Tyson. FASTUS: </author> <title> A finite-state processor for information extraction from real-world text. </title> <booktitle> In Proc. 13th Int'l Joint Conf. Artificial Intelligence (IJCAI-93), </booktitle> <pages> pages 1172-1178, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Pattern matching is thus a form of deterministic, bottom-up partial parsing. At present, it is the most widely used strategy in the IE community, superseding full syntactic analysis of earlier IE systems <ref> [3, 2] </ref>. The name recognition module identifies proper names in the text by using local contextual cues, such as capitalization, personal titles ("Mr.", "Esq."), and company suffixes ("Inc.", "Co."). Name recognition is a well-researched topic, with the best available systems today reaching 97% accuracy.
Reference: [4] <author> Daniel Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. Nymble: </author> <title> a high-performance learning name-finder. </title> <booktitle> In Proc. Fifth Applied Natural Language Processing Conf., </booktitle> <address> Washington, DC, </address> <month> April </month> <year> 1997. </year> <note> Assn. for Computational Linguistics. </note>
Reference-contexts: is a collection of documents, with an associated "answer key", a set of templates filled manually by human annotators.) Unsupervised learning has served some IE tasks well: in identifying named entities, BBN has recently used similar techniques to achieve scores within a few points of the best manually constructed systems <ref> [4] </ref>. However, unsupervised learning is known to be dependent on the availability of a large amount of manually prepared training data. The UMass system performed well when there were thousands of manually prepared messages to train on.
Reference: [5] <author> David Fisher, Stephen Soderland, Joseph McCarthy, Fangfang Feng, and Wendy Lehnert. </author> <title> Description of the UMass system as used for MUC-6. </title> <booktitle> In Proc. Sixth Message Understanding Conf. </booktitle> <address> (MUC-6), Columbia, MD, November 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When the size of the training corpus was closer to what one could reasonably expect human annotators to provide (100 messages, at MUC-6), the sparseness of coverage caused the system's performance to drop, cf. <ref> [5] </ref>. 4.2 Solution Before we describe Proteus's example-based strategy for pattern building, let us examine the organization of the pattern base in closer detail. The pattern base is organized in layers, corresponding to different levels of processing.
Reference: [6] <author> Ralph Grishman. </author> <title> The NYU system for MUC-6 or where's the syntax? In Proc. </title> <booktitle> Sixth Message Understanding Conf. </booktitle> <address> (MUC-6), Columbia, MD, </address> <month> November </month> <year> 1995. </year> <note> Morgan Kaufmann. 10 </note>
Reference-contexts: processing will appear either as mark-up, when we bracket or attach semantic labels to textual items in the document in situ, or as templates when more complex logical structures are extracted from the text and output separately, according to a predetermined format. 3 Architecture of the Proteus IE System see <ref> [6, 7] </ref>. The system consists of a cascade of modules with their attendant knowledge bases, each of the modules applied to and transforming the input text document. The first module, lexical analysis is responsible for breaking up the document into sentences, and the sentences into tokens. <p> The new pattern can then be matched against the rest of the training corpus, to refine its operation and to reduce overgeneration. Syntactic generalization: Actually, the pattern base would acquire much more than the basic pattern that the user accepted. The system applies built-in meta-rules <ref> [2, 6] </ref>, to automatically produce a set of syntactic transformations from a basic, active clause pattern.
Reference: [7] <author> Ralph Grishman. </author> <title> Information extraction: Techniques and challenges. </title> <editor> In Maria Teresa Pazienza, editor, </editor> <title> Information Extraction. Springer-Verlag, </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <address> Rome, </address> <year> 1997. </year>
Reference-contexts: processing will appear either as mark-up, when we bracket or attach semantic labels to textual items in the document in situ, or as templates when more complex logical structures are extracted from the text and output separately, according to a predetermined format. 3 Architecture of the Proteus IE System see <ref> [6, 7] </ref>. The system consists of a cascade of modules with their attendant knowledge bases, each of the modules applied to and transforming the input text document. The first module, lexical analysis is responsible for breaking up the document into sentences, and the sentences into tokens.
Reference: [8] <author> Ralph Grishman and Beth Sundheim. </author> <title> Message understanding conference - 6: A brief history. </title> <booktitle> In Proc. 16th Int'l Conf. on Computational Linguistics (COLING 96), </booktitle> <address> Copenhagen, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Furthermore, they allow a reasonable measurement of performance and progress in the field. Indeed, over the last decade, competing IE systems have undergone a series of formal evaluations theMessage Understanding Conferences (MUCs), <ref> [1, 8] </ref>. The results of these evaluations to date show that, although IE is a promising technology, it requires much further research.
Reference: [9] <author> Maria Teresa Pazienza, </author> <title> editor. Information Extraction. Springer-Verlag, </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <address> Rome, </address> <year> 1997. </year>
Reference-contexts: of performance: the top scores in MUC-6, the latest of the MUCs, were all below the 60 percent mark. 2 Other, less obvious 1 This definition of IE is narrower than some; for a review of the fuller range of systems falling under this category, the reader is referred to <ref> [9] </ref>. 2 in terms of the so-called F-measure, a weighted mean of the usual recall and precision measures, used in evaluation of information retrieval. 1 ...
Reference: [10] <author> W. Soderland, D. Fisher, J. Aseltine, and W. Lenhert. </author> <title> CRYSTAL: Inducing a conceptual dictionary. </title> <booktitle> In Proc. Int'l Joint Conf. Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1314-1319, </pages> <address> Montreal, Canada, </address> <year> 1995. </year> <month> 11 </month>
Reference-contexts: The goal here is to avoid engaging the user at the lower levels of the system, and to keep the interaction as high-level as possible. One example is the system developed at University of Massachusetts at Amherst <ref> [10] </ref>, which uses unsupervised training methods: the system learns patterns from the MUC training corpus. (The training corpus is a collection of documents, with an associated "answer key", a set of templates filled manually by human annotators.) Unsupervised learning has served some IE tasks well: in identifying named entities, BBN has
References-found: 10

