URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS94-16.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: (martin@cs.ucsb.edu)  (pedro@cs.ucsb.edu)  
Title: Commutativity Analysis: A New Technique for Automatically Parallelizing Serial Programs  
Author: Martin C. Rinard Pedro Diniz 
Keyword: POPL95 Topic: Program Analysis, Parallelization  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: This paper introduces a new analysis technique, commutativity analysis, for automatically parallelizing programs written in sequential, imperative programming languages. Commutativity analysis aggregates both data and computation into larger grain units. It then analyzes the computation at this granularity to discover when pieces of the computation commute (i.e. generate the same result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. This approach differs from standard approaches based on data dependence analysis in that it generates parallel programs that may violate the data dependences of the original serial program. Commutativity analysis can therefore automatically parallelize programs that are inherently beyond the reach of any compiler that preserves the data dependences. In this paper we formally define a set of conditions that the compiler can use to automatically detect commuting operations, prove that if operations meet these conditions then their executions commute and show how to exploit the commutativity information to automatically generate parallel code.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Current parallelizing compilers preserve the semantics of the original serial program by preserving the data dependences <ref> [1, 8] </ref>. These compilers attempt to identify independent pieces of computation (two pieces of computation are independent if neither writes a piece of data that the other accesses), then generate code that executes independent pieces concurrently. <p> We use the applied object partial order v to induce a corresponding partial order v on seq (A): a 1 ffi ffi a k v a 0 1 ffi ffi a 0 k 0 if there exists a strictly increasing, one to one function h : <ref> [1; k] </ref> ! [1; k 0 ] such that 8i 2 [1; k] : a i v a 0 h (i) . <p> We use the applied object partial order v to induce a corresponding partial order v on seq (A): a 1 ffi ffi a k v a 0 1 ffi ffi a 0 k 0 if there exists a strictly increasing, one to one function h : [1; k] ! <ref> [1; k 0 ] </ref> such that 8i 2 [1; k] : a i v a 0 h (i) . <p> v to induce a corresponding partial order v on seq (A): a 1 ffi ffi a k v a 0 1 ffi ffi a 0 k 0 if there exists a strictly increasing, one to one function h : <ref> [1; k] </ref> ! [1; k 0 ] such that 8i 2 [1; k] : a i v a 0 h (i) . We use a function g : M fi A ! seq (A); g 2 G to model the invoked operations. g (m; r-&gt;op (o)) is the sequence of operations invoked when r-&gt;op (o) executes in memory m.
Reference: [2] <author> P. Barth, R. Nikhil, and Arvind. M-structures: </author> <title> Extending a parallel, non-strict, functional language with state. </title> <booktitle> In Proceedings of the Fifth ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 538-568. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Finally, preserving the data dependences for programs that periodically update shared data structures can artificially limit the amount of exposed concurrency, since tasks must delay updates until they are sure that each update will not change the relative order of reads and writes to the shared data structure <ref> [2, 11] </ref>. 1 This paper presents a new analysis technique, commutativity analysis, that eliminates many of the limitations of existing data-dependence based approaches. Instead of preserving the relative order of individual reads and writes to single words of memory, commutativity analysis aggregates both data and computation into larger grain units. <p> It is therefore important for the programmer be able to control whether the compiler treats adding numbers into a running sum as commuting operations. 5 4 Related Work Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [15, 2, 14] </ref>. These systems, however, focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The techniques presented in this paper, on the other hand, automatically detect commuting operations.
Reference: [3] <author> D. Callahan. </author> <title> Recognizing and parallelizing bounded recurrences. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The techniques presented in this paper, on the other hand, automatically detect commuting operations. Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [4, 10, 3] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [4] <author> A. Fisher and A. Ghuloum. </author> <title> Parallelizing complex scans and reductions. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year> <month> 14 </month>
Reference-contexts: The techniques presented in this paper, on the other hand, automatically detect commuting operations. Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [4, 10, 3] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [5] <author> W.L. Harrison. </author> <title> The interprocedural analysis and automatic parallelization of Scheme programs. </title> <journal> Lisp and Symbolic Computation, </journal> 2(3/4):179-396, October 1989. 
Reference-contexts: While researchers have been able to develop reasonably effective algorithms for loop nests that manipulate dense matrices using affine access functions [9], there has been little progress towards the successful automatic analysis of programs that manipulate pointer-based data structures. Researchers have attempted to build totally automatic systems <ref> [5] </ref>, but the most promising approaches require the programmer to provide annotations that specify information about the global topology of the manipulated data structures [6]. A second, more fundamental limitation of data-dependence based approaches is an inability to parallelize computations that manipulate graph-like data structures.
Reference: [6] <author> L. Hendren, J. Hummel, and A. Nicolau. </author> <title> Abstractions for recursive pointer data structures: improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Researchers have attempted to build totally automatic systems [5], but the most promising approaches require the programmer to provide annotations that specify information about the global topology of the manipulated data structures <ref> [6] </ref>. A second, more fundamental limitation of data-dependence based approaches is an inability to parallelize computations that manipulate graph-like data structures. The aliases inherently present in these data structures preclude the static discovery of independent pieces of code, forcing the compiler to generate serial code.
Reference: [7] <author> G. Huet. </author> <title> Confluent reductions: Abstract properties and applications to term rewriting systems. </title> <journal> Journal of the ACM, </journal> <volume> 27(4) </volume> <pages> 797-821, </pages> <year> 1980. </year>
Reference-contexts: ) commute, then hm; fr-&gt;op (o)gi ) ) hm 0 1 ; ;i and hm; fr-&gt;op (o)gi ) ) hm 0 2 ; ;i implies m 0 1 = m 0 Proof Sketch: If all of the invoked operations commute then the transition system is confluent, which guarantees deterministic execution <ref> [7] </ref>. 8 5.2 Program Semantics We next show, given a program that defines a set of operations, how to extract the functions f and g that determine the behavior of the operations.
Reference: [8] <author> D. Kuck, Y. Muraoka, and S. Chen. </author> <title> On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21(12):1293-1310, </volume> <month> December </month> <year> 1972. </year>
Reference-contexts: 1 Introduction Current parallelizing compilers preserve the semantics of the original serial program by preserving the data dependences <ref> [1, 8] </ref>. These compilers attempt to identify independent pieces of computation (two pieces of computation are independent if neither writes a piece of data that the other accesses), then generate code that executes independent pieces concurrently.
Reference: [9] <author> D. Maydan, J. Hennessy, and M. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: A significant limitation of this approach is the difficulty of performing dependence analysis that is precise enough to expose substantial amounts of concurrency. While researchers have been able to develop reasonably effective algorithms for loop nests that manipulate dense matrices using affine access functions <ref> [9] </ref>, there has been little progress towards the successful automatic analysis of programs that manipulate pointer-based data structures.
Reference: [10] <author> S. Pinter and R. Pinter. </author> <title> Program optimization and parallelization using idioms. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: The techniques presented in this paper, on the other hand, automatically detect commuting operations. Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [4, 10, 3] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [11] <author> M. Rinard. </author> <title> The Design, Implementation and Evaluation of Jade, a Portable, Implicitly Parallel Programming Language. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: Finally, preserving the data dependences for programs that periodically update shared data structures can artificially limit the amount of exposed concurrency, since tasks must delay updates until they are sure that each update will not change the relative order of reads and writes to the shared data structure <ref> [2, 11] </ref>. 1 This paper presents a new analysis technique, commutativity analysis, that eliminates many of the limitations of existing data-dependence based approaches. Instead of preserving the relative order of individual reads and writes to single words of memory, commutativity analysis aggregates both data and computation into larger grain units.
Reference: [12] <author> D. Scales and M. S. Lam. </author> <title> An efficient shared memory system for distributed memory machines. </title> <type> Technical Report CSL-TR-94-627, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Programmers who parallelize computations by hand have always used commuting operations. For example, four (Water, MP3D, LocusRoute and Cholesky) of the six parallel applications in the SPLASH benchmark suite [13] and three of the four parallel applications described in <ref> [12] </ref> violate the data dependences of the original serial program and rely on commuting operations for their correct execution.
Reference: [13] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Programmers who parallelize computations by hand have always used commuting operations. For example, four (Water, MP3D, LocusRoute and Cholesky) of the six parallel applications in the SPLASH benchmark suite <ref> [13] </ref> and three of the four parallel applications described in [12] violate the data dependences of the original serial program and rely on commuting operations for their correct execution.
Reference: [14] <author> J. Solworth and B. Reagan. </author> <title> Arbitrary order operations on trees. </title> <booktitle> In Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: It is therefore important for the programmer be able to control whether the compiler treats adding numbers into a running sum as commuting operations. 5 4 Related Work Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [15, 2, 14] </ref>. These systems, however, focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The techniques presented in this paper, on the other hand, automatically detect commuting operations.
Reference: [15] <author> G. Steele. </author> <title> Making asynchronous parallelism safe for the world. </title> <booktitle> In Proceedings of the Seventeenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: It is therefore important for the programmer be able to control whether the compiler treats adding numbers into a running sum as commuting operations. 5 4 Related Work Other researchers have recognized the value of including support for commuting operations in parallel computing systems <ref> [15, 2, 14] </ref>. These systems, however, focus on exploiting commuting operations and rely on some external mechanism, typically the programmer, to specify when the operations actually commute. The techniques presented in this paper, on the other hand, automatically detect commuting operations.
Reference: [16] <author> W. Weihl. </author> <title> Commutativity-based concurrency control for abstract data types. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1488-1505, </pages> <month> December </month> <year> 1988. </year> <month> 15 </month>
Reference-contexts: Knowledge of the algebraic properties therefore enhances the effectiveness of, rather than enables, the presented techniques. Research performed in the context of database concurrency control has shown that it is possible to take advantage of commuting operations to increase the amount of concurrency available in transaction processing systems <ref> [16] </ref>. The presented approach is based on using abstract specifications of the behavior of the operations to expose the commutativity. It is the responsibility of the programmer to ensure that the actual implementation preserves the semantics of the specification.
References-found: 16

