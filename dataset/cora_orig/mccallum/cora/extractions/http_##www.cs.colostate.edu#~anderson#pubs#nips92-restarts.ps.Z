URL: http://www.cs.colostate.edu/~anderson/pubs/nips92-restarts.ps.Z
Refering-URL: http://www.cs.colostate.edu/~anderson/pubs/pubs.html
Root-URL: 
Title: Q-Learning with Hidden-Unit Restarting  
Author: Charles W. Anderson 
Address: Fort Collins, CO 80523  
Affiliation: Department of Computer Science Colorado State University  
Abstract: Platt's resource-allocation network (RAN) (Platt, 1991a, 1991b) is modified for a reinforcement-learning paradigm and to "restart" existing hidden units rather than adding new units. After restarting, units continue to learn via back-propagation. The resulting restart algorithm is tested in a Q-learning network that learns to solve an inverted pendulum problem. Solutions are found faster on average with the restart algorithm than without it.
Abstract-found: 1
Intro-found: 1
Reference: <author> C. W. Anderson. </author> <year> (1987). </year> <title> Strategy learning with multilayer connectionist representations. </title> <type> Technical Report TR87-509.3, </type> <institution> GTE Laboratories, </institution> <address> Waltham, MA, </address> <year> 1987. </year> <note> Corrected version of article that was published in Proceedings of the Fourth International Workshop on Machine Learning, pp. 103-114, </note> <month> June, </month> <year> 1987. </year>
Reference: <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <year> (1991). </year> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <month> Aug. </month>
Reference-contexts: For Q-learning, the following temporal-difference error (Sutton, 1988) e t = r t+1 + fl max [Q (x t+1 ; a t+1 )] Q (x t ; a t ): is derived by using max a t+1 [Q (x t+1 ; a t+1 )] as an approximation to P 1 <ref> (Barto, Bradtke, and Singh, 1991) </ref> for further discussion of the relationships between reinforcement learning and dynamic programming. 4 Q-Learning Network For the inverted pendulum experiments reported here, a neural network with a single hidden layer was used to learn the Q (x; a) function.
Reference: <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846. </pages> <note> Reprinted in J. </note> <editor> A. Anderson and E. Rosen-feld, Neurocomputing: </editor> <booktitle> Foundations of Research, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: <author> M. I. Jordan and R. A. Jacobs. </author> <year> (1990). </year> <title> Learning to control an unstable system with forward modeling. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 324-331. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> A. H. Klopf and E. Gose. </author> <year> (1969). </year> <title> An evolutionary pattern recognition network. </title> <journal> IEEE Transactions on Systems, Science, and Cybernetics, </journal> <volume> 15 </volume> <pages> 247-250. </pages>
Reference: <author> L.-J. Lin. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> 8(3/4):293-321. 
Reference: <author> M. C. Mozer and P. Smolensky. </author> <year> (1989). </year> <title> Skeltonization: A technique for trimming the fat from a network via relevance assessment. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 107-115. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference: <author> J. C. Platt. </author> <year> (1991a). </year> <title> Learning by combining memorization and gradient descent. </title>
Reference-contexts: An initially local representation is also advantageous when the learning component is operating in parallel with a conventional, fixed controller. A learning experience should not generalize widely; the conventional controller should be preferred for inputs that have not yet been experienced. Platt's resource-allocation network (RAN) <ref> (Platt, 1991a, 1991b) </ref> combines gradient search and memorization. RAN uses locally tuned (gaussian) units in the hidden layer.
Reference: <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 714-720. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference: <author> J. C. Platt. </author> <title> (1991b) A resource-allocating network for function interpolation. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 213-225. </pages>
Reference: <author> R. S. Sutton. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: Watkins (1989) presents a number of algorithms for adjusting the parameters of Q. Here we focus on using error back-propagation to train a neural network to learn the Q function. For Q-learning, the following temporal-difference error <ref> (Sutton, 1988) </ref> e t = r t+1 + fl max [Q (x t+1 ; a t+1 )] Q (x t ; a t ): is derived by using max a t+1 [Q (x t+1 ; a t+1 )] as an approximation to P 1 (Barto, Bradtke, and Singh, 1991) for further
Reference: <author> C. J. C. H. Watkins. </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cam-bridge University Psychology Department. </institution>
Reference-contexts: Platt demonstrated RAN on the supervised learning task of predicting values in the Mackey-Glass time series. We have integrated Platt's ideas with the reinforcement-learning algorithm called Q-learning <ref> (Watkins, 1989) </ref>. One major modification is that the network has a fixed number of hidden units, all in a single-layer, all of which are trained on every step.
Reference: <author> C. J. C. H. Watkins and P. Dayan. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> 8(3/4):279-292. 
Reference: <author> D. Whitley, S. Dominic, R. Das, and C. Anderson. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference: <author> B. Widrow and F. W. Smith. </author> <year> (1964). </year> <title> Pattern-recognizing control systems. </title> <booktitle> In Proceedings of the 1963 Computer and Information Sciences (COINS) Symposium, </booktitle> <pages> pages 288-317, </pages> <address> Washington, DC. </address> <publisher> Spartan. </publisher>
References-found: 15

