URL: http://www.eecs.umich.edu/~marios/papers/tr96.ps
Refering-URL: http://www.eecs.umich.edu/~marios/pubs.html
Root-URL: http://www.cs.umich.edu
Email: lalgudi@cs.yale.edu  marios@eecs.umich.edu  miodrag@cs.ucla.edu  
Title: Optimizing Computations for Effective Block-Processing  
Author: Kumar N. Lalgudi Marios C. Papaefthymiou Miodrag Potkonjak 
Keyword: Categories and subject descriptors: C.3 Special-purpose and application-based systems Signal processing systems; F.2 Analysis of algorithms and problem complexity. General terms: Algorithms, design, performance. Keywords and phrases: Combinatorial optimization, computation dataflow graphs, embedded systems, high-level synthesis, integer linear programming, retiming, schedul ing, vectorization.  
Address: Hillsboro, OR 97124  Ann Arbor, MI 48109  Los Angeles, CA 90095  
Affiliation: Intel Corporation  EECS Department University of Michigan  CS Department University of California  
Abstract: Block-processing is a powerful and popular technique for increasing computation speed by simultaneously processing several samples of data. In comparison with conventional computations, block-processed computations have reduced hardware and power dissipation requirements and result in less expensive implementations on uniprocessor and parallel programmable platforms. The effectiveness of block-processing is often reduced, however, due to suboptimal placement of delays in the dataflow graph of a computation. In this paper we investigate an application of the retiming transformation for improving the effectiveness of block-processing in computation structures. Specifically, we consider the k-delay problem in which we wish to restructure any given computation by relocating its delays so that for any given k, the resulting computation can process k data samples simultaneously in a fully regular manner. Our first contribution is the formulation of the k-delay problem as an integer linear program. Our main contribution is the formulation of a set of necessary conditions which we use to develop a branch-and-bound algorithm for the k-delay problem that runs efficiently in practice. Specifically, for a given computation dataflow graph (CDFG) and a given positive integer k, our algorithm computes a retimed CDFG that can process k samples simultaneously, or determines that such a retiming is infeasible. We provide extensive experimental results which demonstrate the effectiveness of our optimization and the efficiency of our algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Berkelaar. </author> <title> lp solve: A mixed-integer linear programming solver. </title> <note> Available by anonymous ftp from ftp://ftp.es.ele.tue.nl/pub/lp solve. </note>
Reference-contexts: This number equals the maximum number of delays that can be placed on any edge and is bounded from above by F . Thus, k max can be determined by a binary search over the integers in the range <ref> [1; F ] </ref>. The feasibility of each value is checked using Algorithm SolveKDP. 6 Experimental Results We have developed three programs for computing the optimal block-processing factor k max . In all three programs k max is determined by a binary search. <p> KDP that we described in Section 3. It first generates the ILP constraints and then solves the integer program separately using lp solve, a public-domain mixed-integer linear pro gramming solver <ref> [1] </ref>. Our second program (NC-ILP) first checks the necessary conditions given in Section 4 to screen out infeasible problems. The problems that satisfy the neces sary conditions are then solved by an ILP formulation which is fed to lp solve.
Reference: [2] <author> R. E. Blahut. </author> <title> Fast Algorithms for Digital Signal Processing. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: Enhanced regularity reduces the effort in software switching and address calculation, and improved locality improves the effectiveness of code-size reduction methods [13]. Moreover, block-processing enables the efficient utilization of pipelines and efficient implementations of vector-based algorithms such as FFT-based filtering and error-correction codes <ref> [2] </ref>. In general, block-processing is beneficial in all cases where the net cost of processing n samples individually is higher than the net cost of processing n samples simultaneously. Typical cost measures include processing time, memory requirements, and energy dissipation per sample.
Reference: [3] <author> B. Cherkassky, A. Goldberg, and T. Radzik. </author> <title> Shortest paths algorithms: theory and experimental evaluation. </title> <type> Technical Report STAN-CS-93-1480, </type> <institution> Stanford, </institution> <year> 1993. </year>
Reference-contexts: In order to evaluate the efficiency of our implementations, we experimented with large synthetic graphs in addition to the real DSP programs. The synthetic graphs in our test suite were generated using the sprand function of the random graph generator described in <ref> [3] </ref>. All the graphs generated using sprand were connected and had integer edge weights chosen uniformly in the range [0, 5]. Given the number of vertices and edges desired, sprand generates graphs by randomly placing edges between vertices and by randomly assigning weights from the specified range.
Reference: [4] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: All for loops in Algorithm AddEdges-2 execute in O (V 2 ) steps since we have at most O (V 2 ) vertex pairs. Step 15 takes O (V E + V 2 lg V ) time using Johnson's algorithm for computing the all-pairs shortest paths <ref> [4] </ref>.
Reference: [5] <author> W. W. Gibbs. </author> <title> Software's chronic crisis. </title> <publisher> Scientific American, </publisher> <pages> pages 86-95, </pages> <year> 1994. </year> <month> 23 </month>
Reference-contexts: The block-processing factor of the retimed graph in Part (b) is 3. doubling every year, the processing power of hardware is doubling only every three years <ref> [5] </ref>. Furthermore, new applications requirements such as low power dissipation impose additional design constraints which often further add to the gap between the speed of hardware primitives and the rate of incoming data. <p> The synthetic graphs in our test suite were generated using the sprand function of the random graph generator described in [3]. All the graphs generated using sprand were connected and had integer edge weights chosen uniformly in the range <ref> [0, 5] </ref>. Given the number of vertices and edges desired, sprand generates graphs by randomly placing edges between vertices and by randomly assigning weights from the specified range. The size of these computation dataflow graphs was between 10 to 300 vertices and 20 to 750 edges.
Reference: [6] <author> L. M. Guerra, M. Potkonjak, and J. Rabaey. </author> <title> System-level design guidance using al-gorithm properties. </title> <booktitle> In Proc. of the VLSI Signal Processing Workshop, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This approach, known as block-processing or vectorization, is widely used to satisfy throughput requirements through the use of parallelism and pipelining. Block-processing enhances both regularity and locality in computations, thus greatly facilitating their efficient implementation on many hardware platforms <ref> [6, 9] </ref>. Enhanced regularity reduces the effort in software switching and address calculation, and improved locality improves the effectiveness of code-size reduction methods [13]. Moreover, block-processing enables the efficient utilization of pipelines and efficient implementations of vector-based algorithms such as FFT-based filtering and error-correction codes [2].
Reference: [7] <author> A. T. Ishii, C. E. Leiserson, and M. C. Papaefthymiou. </author> <title> Optimizing two-phase, </title> <booktitle> level-clocked circuitry. In Advanced Research in VLSI and Parallel Systems: Proc. of the 1992 Brown/MIT Conference, </booktitle> <pages> pages 245-264. </pages> <publisher> MIT Press, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: However, this technique may not uniformly increase the block-processing factor for all computational blocks. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other architectural transformation techniques that have targeted high-level synthesis [8, 17], retiming has been used traditionally for clock period minimization <ref> [7, 10, 12] </ref> and for logic synthesis [14, 15]. 2 dataflow graph (CDFG) in this figure has three computation blocks A, B, and C, and three delays. An input stream is coming into block A, and an output stream is generated by A.
Reference: [8] <author> D. C. Ku and G. De Micheli. </author> <title> Relative scheduling under timing constraints: Algorithms for high-level synthesis of digital circuits. </title> <journal> IEEE Transactions on CAD of Integrated Circuits and Systems, </journal> <volume> 11(6) </volume> <pages> 696-717, </pages> <year> 1992. </year>
Reference-contexts: However, this technique may not uniformly increase the block-processing factor for all computational blocks. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other architectural transformation techniques that have targeted high-level synthesis <ref> [8, 17] </ref>, retiming has been used traditionally for clock period minimization [7, 10, 12] and for logic synthesis [14, 15]. 2 dataflow graph (CDFG) in this figure has three computation blocks A, B, and C, and three delays.
Reference: [9] <author> S. Y. Kung. </author> <title> VLSI Array Processors. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1988. </year>
Reference-contexts: This approach, known as block-processing or vectorization, is widely used to satisfy throughput requirements through the use of parallelism and pipelining. Block-processing enhances both regularity and locality in computations, thus greatly facilitating their efficient implementation on many hardware platforms <ref> [6, 9] </ref>. Enhanced regularity reduces the effort in software switching and address calculation, and improved locality improves the effectiveness of code-size reduction methods [13]. Moreover, block-processing enables the efficient utilization of pipelines and efficient implementations of vector-based algorithms such as FFT-based filtering and error-correction codes [2].
Reference: [10] <author> K. N. Lalgudi and M. C. Papaefthymiou. </author> <title> DelaY: An efficient tool for retiming with realistic delay modeling. </title> <booktitle> In Proceedings of the 32th ACM/IEEE Design Automation Conference, </booktitle> <pages> pages 304-309, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: However, this technique may not uniformly increase the block-processing factor for all computational blocks. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other architectural transformation techniques that have targeted high-level synthesis [8, 17], retiming has been used traditionally for clock period minimization <ref> [7, 10, 12] </ref> and for logic synthesis [14, 15]. 2 dataflow graph (CDFG) in this figure has three computation blocks A, B, and C, and three delays. An input stream is coming into block A, and an output stream is generated by A. <p> In this section we rely on the notion of the "companion graph" that was described in <ref> [10] </ref> to express Problem KDP as an Integer Linear Program (ILP).
Reference: [11] <author> K. N. Lalgudi and M. C. Papaefthymiou. </author> <title> Computing strictly-second shortest paths. </title> <journal> Information Processing Letters, </journal> <note> 1997. To appear. </note>
Reference-contexts: Although the problem of computing the strictly-second shortest simple path between a given vertex pair is NP-complete, the corresponding problem without the simple path requirement can be solved in polynomial time <ref> [11] </ref>. For graphs that satisfy Inequality (15) in Lemma 4, it is straightforward to show that if the strictly-second shortest path is non-simple, then its delay count exceeds that of the shortest path by at least k. <p> Proof. Steps 1-2 take O (V 2 ) time. Since the all-pairs second-shortest paths can be computed in O (V E + V 2 lg V ) + O (V 2 E) time <ref> [11] </ref>, Step 4 takes O (V 2 E) time. Steps 5-10 take O (V 2 ) time to complete. Thus, Algorithm AddEdges-1 terminates in O (V 2 E) steps. Lemma 5 captures only explicit delay requirements and not implicit or hidden requirements.
Reference: [12] <author> C. E. Leiserson and J. B. Saxe. </author> <title> Retiming synchronous circuitry. </title> <journal> Algorithmica, </journal> <volume> 6(1) </volume> <pages> 1-27, </pages> <year> 1991. </year>
Reference-contexts: However, this technique may not uniformly increase the block-processing factor for all computational blocks. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other architectural transformation techniques that have targeted high-level synthesis [8, 17], retiming has been used traditionally for clock period minimization <ref> [7, 10, 12] </ref> and for logic synthesis [14, 15]. 2 dataflow graph (CDFG) in this figure has three computation blocks A, B, and C, and three delays. An input stream is coming into block A, and an output stream is generated by A.
Reference: [13] <author> S. Liao, S. Devadas, K. Keutzer, S. Tjiang, and A. Wang. </author> <title> Storage assignment to decrease code size. </title> <booktitle> In Proc. of the ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 186-195, </pages> <year> 1995. </year>
Reference-contexts: Block-processing enhances both regularity and locality in computations, thus greatly facilitating their efficient implementation on many hardware platforms [6, 9]. Enhanced regularity reduces the effort in software switching and address calculation, and improved locality improves the effectiveness of code-size reduction methods <ref> [13] </ref>. Moreover, block-processing enables the efficient utilization of pipelines and efficient implementations of vector-based algorithms such as FFT-based filtering and error-correction codes [2].
Reference: [14] <author> S. Malik, E. Sentovich, R. K. Brayton, and A. Sangiovanni-Vincentelli. </author> <title> Retiming and resynthesis: Optimizing sequential networks with combinational techniques. </title> <booktitle> In Proc. of the Hawaii International Conference on System Sciences, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other architectural transformation techniques that have targeted high-level synthesis [8, 17], retiming has been used traditionally for clock period minimization [7, 10, 12] and for logic synthesis <ref> [14, 15] </ref>. 2 dataflow graph (CDFG) in this figure has three computation blocks A, B, and C, and three delays. An input stream is coming into block A, and an output stream is generated by A.
Reference: [15] <author> G. De Micheli. </author> <title> Synchronous logic synthesis: Algorithms for cycle-time minimization. </title> <journal> IEEE Transactions on CAD of Integrated Circuits and Systems, </journal> <volume> 10(1) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other architectural transformation techniques that have targeted high-level synthesis [8, 17], retiming has been used traditionally for clock period minimization [7, 10, 12] and for logic synthesis <ref> [14, 15] </ref>. 2 dataflow graph (CDFG) in this figure has three computation blocks A, B, and C, and three delays. An input stream is coming into block A, and an output stream is generated by A.
Reference: [16] <author> S. Ritz, M. Pankert, V. Zivojnovic, and H. Meyr. </author> <title> Optimum vectorization of scalable synchronous dataflow graphs. </title> <booktitle> In Proc. of the International Conference on Application-Specific Array Processors, </booktitle> <pages> pages 285-296, </pages> <month> October </month> <year> 1993. </year> <month> 24 </month>
Reference-contexts: Recently, retiming has been studied in the context of optimum vectorization for a class of DSP programs <ref> [16, 18] </ref>. Specifically, a technique for linear vectorization of DSP programs using retiming has been presented in [18]. This technique involves the redistribution of delays in the CDFG representation of a DSP program in a way that maximizes the concentration of delays on the edges.
Reference: [17] <author> R. A. Walker and D. E. Thomas. </author> <title> Behavioral transformations for algorithmic level IC design. </title> <journal> IEEE Transactions on CAD of Integrated Circuits and Systems, </journal> <volume> 8(10) </volume> <pages> 1115-1127, </pages> <year> 1989. </year>
Reference-contexts: However, this technique may not uniformly increase the block-processing factor for all computational blocks. Another transformation that can be used for increasing the block-processing factor is retiming. Contrary to other architectural transformation techniques that have targeted high-level synthesis <ref> [8, 17] </ref>, retiming has been used traditionally for clock period minimization [7, 10, 12] and for logic synthesis [14, 15]. 2 dataflow graph (CDFG) in this figure has three computation blocks A, B, and C, and three delays.
Reference: [18] <author> V. Zivojnovic, S. Ritz, and H. Meyr. </author> <title> Retiming of DSP programs for optimum vec-torization. </title> <booktitle> In Proc. of the International Conference on Acoustic, Speech, and Signal Processing, </booktitle> <year> 1994. </year> <month> 25 </month>
Reference-contexts: Recently, retiming has been studied in the context of optimum vectorization for a class of DSP programs <ref> [16, 18] </ref>. Specifically, a technique for linear vectorization of DSP programs using retiming has been presented in [18]. This technique involves the redistribution of delays in the CDFG representation of a DSP program in a way that maximizes the concentration of delays on the edges. <p> Recently, retiming has been studied in the context of optimum vectorization for a class of DSP programs [16, 18]. Specifically, a technique for linear vectorization of DSP programs using retiming has been presented in <ref> [18] </ref>. This technique involves the redistribution of delays in the CDFG representation of a DSP program in a way that maximizes the concentration of delays on the edges. However, fully regular vectorization cannot be achieved using the linear vectorization approach in that paper. <p> In order to explore the computational speedup possible with block-processing, we applied our k-delay optimization to the computation dataflow graphs of four real DSP programs. Our test suite comprised an adaptive voice echo canceler, an adaptive video coder, and two examples from <ref> [18] </ref>. The size of the CDFGs of these DSP programs ranged from 10 to 25 nodes. The results of our speedup experiments are given in the table of Figure 10. These data have been obtained for uniprocessor implementations.
References-found: 18

