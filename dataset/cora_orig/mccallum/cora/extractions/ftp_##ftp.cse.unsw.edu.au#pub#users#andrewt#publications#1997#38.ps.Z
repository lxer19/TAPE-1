URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1997/38.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1997/SCSE_publications.html
Root-URL: 
Email: Email fericm,arung@cse.unsw.edu.au  
Title: ILP with Noise and Fixed Example Size: A Bayesian Approach  
Author: Eric McCreath and Arun Sharma 
Address: Sydney NSW 2052, Australia  
Affiliation: Department of Artificial Intelligence School of Computer Science and Engineering The University of New South whales  
Abstract: Current inductive logic programming systems are limited in their handling of noise, as they employ a greedy covering approach to constructing the hypothesis one clause at a time. This approach also causes difficulty in learning recursive predicates. Additionally, many current systems have an implicit expectation that the cardinality of the positive and negative examples reflect the "proportion" of the concept to the instance space. A framework for learning from noisy data and fixed example size is presented. A Bayesian heuristic for finding the most probable hypothesis in this general framework is derived. This approach evaluates a hypothesis as a whole rather than one clause at a time. The heuristic, which has nice theoretical properties, is incorporated in an ILP system, Lime. Experimental results show that Lime handles noise better than FOIL and PROGOL. It is able to learn recursive definitions from noisy data on which other systems do not perform well. Lime is also capable of learning from only positive data and also from only negative data.
Abstract-found: 1
Intro-found: 1
Reference: [ Angluin and Laird, 1987 ] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 343-370, </pages> <year> 1987. </year>
Reference-contexts: zero (B); zero (C): 16=24 2:5% 44:53 sort (A; B) components (A; C; D); insert (C; E; B); sort (D; E): sort (A; B) empty (A); empty (B): 162=153 0:0% 578:63 Table 1: Some recursive logic programs 5 Discussion Another approach to modeling noise in learning systems is due to <ref> [ Angluin and Laird, 1987 ] </ref> . Their noise level parameter measures the percentage of data with the incorrect sign, that is, elements of the concept being mislabeled as negative data and vice versa.
Reference: [ Laird, 1988 ] <author> P. Laird. </author> <title> Learning from Good and Bad Data. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Future work will attempt to derive stochastic convergence in the limit results for the noise model presented in this paper in the style of Laird's <ref> [ Laird, 1988 ] </ref> result for the Angluin-Laird noise model. Another direction would be to do adapt the predicative error analysis of [ Mug-gleton, 1996 ] for the Bayesian heuristic with noise and fixed example size. On the empirical front, applicability of Lime on additional real-world domains will be investigated.
Reference: [ Lavrac and Dzeroski, 1992 ] <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive learning of relations from noisy examples. </title> <editor> In S Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 495-516. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Noise handling in LINUS <ref> [ Lavrac and Dzeroski, 1992 ] </ref> is more about taking advantage of noise handling techniques from attribute value learning. the learner matches the true "proportion" of the under-lying concept to the instance space. However, in many learning situations this assumption is unjustified.
Reference: [ Lavrac et al., 1996 ] <author> N. Lavrac, S. Dzeroski, and I. Bratko. </author> <title> Handling imperfect data in inductive logic programming. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming, </booktitle> <pages> pages 48-64. </pages> <publisher> IOS Press, </publisher> <year> 1996. </year>
Reference-contexts: Again, an approach that evaluates complete hypotheses instead of individual clauses does a better job of overcoming such deficiencies in the data. Apart from the above mentioned difficulties, many applications of MDL/MML like heuristic have an implicit expectation that the distribution of examples received by 1 mFOIL <ref> [ Lavrac et al., 1996 ] </ref> has an improved noise handling capability, but it still suffers from the greedy covering approach.
Reference: [ McCreath and Sharma, 1995 ] <author> E. McCreath and A. Sharma. </author> <title> Extraction of meta-knowledge to restrict the hypothesis space for ILP systems. </title> <editor> In X. Yao, editor, </editor> <booktitle> Proceedings of the Eighth Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 75-82. </pages> <publisher> World Scientific, </publisher> <month> November </month> <year> 1995. </year>
Reference-contexts: Lime's functioning may be described in three stages: (a) preprocessing of the background knowledge, (b) generation of candidate clauses, and (c) search for the most probable hypothesis. Lime preprocesses the backgound knowledge to identify functional dependencies, type information, and redundancies <ref> [ McCreath and Sharma, 1995 ] </ref> . This step is very helpful in reducing the search space. Most ILP systems require such information to be explicitly provided together with data.
Reference: [ Muggleton and Feng, 1990 ] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, Tokyo, </booktitle> <pages> pages 368-381. </pages> <publisher> Ohmsa Publishers, </publisher> <year> 1990. </year> <note> Reprinted by Ohmsa Springer-Verlag. </note>
Reference-contexts: 1 Introduction Most ILP systems like GOLEM <ref> [ Muggleton and Feng, 1990 ] </ref> and FOIL [ Quinlan, 1990 ] employ a greedy covering heuristic to build hypotheses. They try to find the clause that covers maximum number of positive examples without covering any or few negative examples. <p> While this approach has lead to efficient learning in many applications, there are situations in which it fails to perform well. Consider the problem of noise handling. GOLEM <ref> [ Muggleton and Feng, 1990 ] </ref> has a rudimentary noise handling facility as each clause is allowed to cover a fixed number of negative examples in addition to as many positive examples as possible.
Reference: [ Muggleton, 1996 ] <author> S. Muggleton. </author> <title> Learning from positive data. </title> <booktitle> In Proceedings of the Inductive Logic Programming Workshop, </booktitle> <year> 1996. </year>
Reference-contexts: On the empirical front, applicability of Lime on additional real-world domains will be investigated. To this end we would like to note that initial experiments with Lime on protein secondary structure data show comparable results to GOLEM. Acknowledgements We would like to thank M. Bain for bringing <ref> [ Muggleton, 1996 ] </ref> to our attention. We would also like to thank the referees for several valuable comments that have improved the paper.
Reference: [ Quinlan, 1990 ] <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Most ILP systems like GOLEM [ Muggleton and Feng, 1990 ] and FOIL <ref> [ Quinlan, 1990 ] </ref> employ a greedy covering heuristic to build hypotheses. They try to find the clause that covers maximum number of positive examples without covering any or few negative examples.
Reference: [ Rissanen, 1978 ] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Auotomatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: Inflexibility aside, handling of noise at the clause level, in many cases, appears to result in a poor overall hypothesis, mainly because of overfitting. FOIL [ Quin-lan, 1990 ] employs an MDL/MML <ref> [ Rissanen, 1978; Wallace and Freeman, 1987 ] </ref> like approach to noise handling.
Reference: [ Wallace and Freeman, 1987 ] <author> C. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society (B), </journal> <volume> 49 </volume> <pages> 240-265, </pages> <year> 1987. </year>
Reference-contexts: Inflexibility aside, handling of noise at the clause level, in many cases, appears to result in a poor overall hypothesis, mainly because of overfitting. FOIL [ Quin-lan, 1990 ] employs an MDL/MML <ref> [ Rissanen, 1978; Wallace and Freeman, 1987 ] </ref> like approach to noise handling.
References-found: 10

