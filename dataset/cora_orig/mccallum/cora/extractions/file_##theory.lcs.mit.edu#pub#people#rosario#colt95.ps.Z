URL: file://theory.lcs.mit.edu/pub/people/rosario/colt95.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~rosario/research.html
Root-URL: 
Email: email address: sed@das.harvard.edu  rosario@theory.lcs.mit.edu  
Title: On Learning from Noisy and Incomplete Examples  
Author: Scott E. Decatur Rosario Gennaro 
Note: Author was supported by an NDSEG Fellowship and by NSF Grant CCR-92-00884. Author's  Author was supported by NSF Grant 9121466-CCR and a graduate fellowship from the Consiglio  
Address: Cambridge, MA 02138  Cambridge, MA 02139  Italy. Author's email address:  
Affiliation: Aiken Computation Laboratory Harvard University  Laboratory for Computer Science Massachusetts Institute of Technology  Nazionale delle Ricerche,  
Abstract: We investigate learnability in the PAC model when the data used for learning, attributes and labels, is either corrupted or incomplete. In order to prove our main results, we define a new complexity measure on statistical query (SQ) learning algorithms. The view of an SQ algorithm is the maximum over all queries in the algorithm, of the number of input bits on which the query depends. We show that a restricted view SQ algorithm for a class is a general sufficient condition for learnability in both the models of attribute noise and covered (or missing) attributes. We further show that since the algorithms in question are statistical, they can also simultaneously tolerate classification noise. Classes for which these results hold, and can therefore be learned with simultaneous attribute noise and classification noise, include k-DNF, k-term-DNF by DNF representations, conjunctions with few relevant variables, and over the uniform distribution, decision lists. These noise models are the first PAC models in which all training data, attributes and labels, may be corrupted by a random process. Previous researchers had shown that the class of k-DNF is learnable with attribute noise if the attribute noise rate is known exactly. We show that all of our attribute noise learnabil-ity results, either with or without classification noise, also hold when the exact noise rate is not known, provided that the learner instead has a polynomially good approximation of the noise rate. In addition, we show that the results also hold when there is not just one noise rate, but a distinct noise rate for each attribute. Our results for learning with random covering do not require the learner to be told even an approximation of the covering rate and in addition hold in the setting with distinct covering rates for each attribute. Finally, we give lower bounds on the number of examples required for learning in the presence of attribute noise or covering.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Javed Aslam and Scott Decatur. </author> <title> Improved noise-tolerant learning and generalized statistical queries. </title> <type> Technical Report TR-17-94, </type> <institution> Harvard University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: We combat such an example oracle by "undoing" both types of noise separately. We combine techniques of Aslam and Decatur <ref> [1] </ref> with those used in the proof of Theorem 1.
Reference: [2] <author> Shai Ben-David and Eli Dichterman. </author> <title> Learning with restricted focus of attention. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-296. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Logarithmic view SQ algorithms yield learnability with random covering for any fixed constant rate &lt; 1. A closely related model to restricted view SQ is the model of learning from examples with "restricted focus of attention" <ref> [2, 3] </ref> in which the learning algorithm submits functions to the example oracle which project the examples down to a smaller space.
Reference: [3] <author> Shai Ben-David and Eli Dichterman. </author> <title> Learnability with restricted focus of attention guarantees noise-tolerance. </title> <booktitle> In Proceedings of the Fifth International Workshop on Algorithmic Learning Theory, </booktitle> <year> 1994. </year>
Reference-contexts: Logarithmic view SQ algorithms yield learnability with random covering for any fixed constant rate &lt; 1. A closely related model to restricted view SQ is the model of learning from examples with "restricted focus of attention" <ref> [2, 3] </ref> in which the learning algorithm submits functions to the example oracle which project the examples down to a smaller space.
Reference: [4] <author> Avrim Blum and Mona Singh. </author> <title> Learning functions of k terms. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-153, </pages> <year> 1990. </year>
Reference-contexts: We demonstrate that many commonly studied classes have constant or logarithmic view SQ algorithms. Examples include: (1) k-DNF (and k-CNF); (2) k-term DNF by DNF hypotheses (by converting the Blum and Singh <ref> [4] </ref> PAC algorithm); (3) conjunctions with few relevant variables; and (4) decision lists on the uniform distribution. <p> Since k-term-DNF is a subclass of k-CNF, it is also learnable by the k-CNF algorithm, and similarly for k-clause-CNF. In order to learn k-term-DNF by a DNF representation with constant view, we make use of a result by Blum and Singh <ref> [4] </ref>. A careful conversion of their PAC algorithm yields an SQ algorithm with view k which learns k-term-DNF (k-clause-CNF) by DNF (CNF) representations. One can also consider problems over non-Boolean instance domains in which we consider covering of attributes and labels and classification noise tolerance, but not attribute noise.
Reference: [5] <author> Scott Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 262-268. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: Specifically, an SQ algorithm can be simulated in the PAC model in the presence of classification noise, malicious errors, and even hybrid models combining these two types of noise <ref> [8, 5, 6] </ref>. In order to relate statistical query learning to PAC learning with attribute noise, we define a new complexity measure on statistical query algorithms called the view.
Reference: [6] <author> Scott Decatur. </author> <title> Learning in hybrid noise environments using statistical queries. </title> <booktitle> In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 175-185, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Specifically, an SQ algorithm can be simulated in the PAC model in the presence of classification noise, malicious errors, and even hybrid models combining these two types of noise <ref> [8, 5, 6] </ref>. In order to relate statistical query learning to PAC learning with attribute noise, we define a new complexity measure on statistical query algorithms called the view.
Reference: [7] <author> Sally Goldman and Robert Sloan. </author> <title> Can PAC learning algorithms tolerate random attribute noise? Technical Report WUCS-92-25, </title> <address> Washington University, </address> <year> 1992. </year>
Reference-contexts: Examples include: (1) k-DNF (and k-CNF); (2) k-term DNF by DNF hypotheses (by converting the Blum and Singh [4] PAC algorithm); (3) conjunctions with few relevant variables; and (4) decision lists on the uniform distribution. Although Goldman and Sloan <ref> [7] </ref> show that the very simple class of Boolean conjunctions can be learned in the presence of attribute noise without knowning the noise rate, the Shackelford and Volper result for k-DNF requires that the noise rate be specified to the learner. <p> We show that the accuracy of the estimate need only be within an inverse polynomial of the true noise rate. Another negative result for attribute noise was shown by Goldman and Sloan <ref> [7] </ref> in which they consider a model of product attribute noise where each attribute x i may have a distinct noise rate i. <p> It is interesting to contrast these results with the negative result of Goldman and Sloan <ref> [7] </ref> which states that if distinct noise rates are allowed and the learner is only given a single upper bound bon all of the noise rates, then learning is impossible for any -b 2*. <p> Note: The class of conjunctions can actually be learned in the presence of attribute noise and classification noise without any knowledge of the attribute noise rate by combining our techniques with results of Goldman and Sloan <ref> [7] </ref>. When learning a function in which the number of rel-evant variables is much smaller than the total number of variables, it is desirable to have an algorithm with takes advantage of this property by using fewer example and/or less time.
Reference: [8] <author> Michael Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the 25 th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: We make use of a recently developed tool for creating noise-tolerant algorithms: the statistical query (SQ) learning model <ref> [8] </ref>. In this model, instead of asking for labelled examples, the algorithm asks for estimates of the values of statistics defined over the distribution of labelled examples. <p> Yet, this restriction has been found to be quite mild in that almost every class of concepts which is learnable in the PAC model is also learnable by statistical queries <ref> [8] </ref>. The most important use of the SQ model stems from the property that statistical queries can actually be simulated with the use of noisy example oracles. <p> Specifically, an SQ algorithm can be simulated in the PAC model in the presence of classification noise, malicious errors, and even hybrid models combining these two types of noise <ref> [8, 5, 6] </ref>. In order to relate statistical query learning to PAC learning with attribute noise, we define a new complexity measure on statistical query algorithms called the view. <p> A common example is learning Boolean conjunctions over n variables, when the number of variables in the target is at most a constant k. The SQ algorithm for this problem <ref> [8] </ref> has view k log (1=*). Under the uniform distribution on examples, decision lists can be approximated by O (log (1=*))-length decision lists, and therefore can be learned by an SQ algorithm for this class. Such an algorithm has view O (log (1=*)).
Reference: [9] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: In addition to the attribute noise model in which data is corrupted, we also investigate a model of attribute (and label) "covering" in which some of the information in a labelled example is missing (cf. Quinlan <ref> [9] </ref>). This model captures a learning environment in which a random process corrupts all aspects of the data, but errors can be detected although not corrected. Formally, we consider an example oracle which hides the value of each attribute (or label) independently with probability &lt; 1.
Reference: [10] <author> George Shackelford and Dennis Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In Proceedings of the First Workshop on Computational Learning Theory, </booktitle> <pages> pages 97-103. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: After discussing techniques for tolerating noise in the attributes of the example itself, we also address the simultaneous additional tolerance of noise in the label of the example. The formal PAC model of attribute noise was simultaneously introduced by Shackelford and Volper <ref> [10] </ref> and Sloan [11]. In this model, each attribute of the example Boolean vector is flipped randomly and independently with probability - &lt; 1=2 before it is given to the learner.
Reference: [11] <author> Robert Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the First Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: After discussing techniques for tolerating noise in the attributes of the example itself, we also address the simultaneous additional tolerance of noise in the label of the example. The formal PAC model of attribute noise was simultaneously introduced by Shackelford and Volper [10] and Sloan <ref> [11] </ref>. In this model, each attribute of the example Boolean vector is flipped randomly and independently with probability - &lt; 1=2 before it is given to the learner.
Reference: [12] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> Novem-ber </month> <year> 1984. </year>
Reference-contexts: 1 Introduction In this paper, we consider learning in the presence of faulty data in the "Probably Approximately Correct" (PAC) model introduced by Valiant <ref> [12] </ref>. In the PAC setting, a learner is given the task of determining a close approximation to an unknown f0; 1g-valued target function. The learner is told a class of functions to which the target function belongs, and accuracy and confidence parameters * and ffi.
References-found: 12

