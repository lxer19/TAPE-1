URL: http://www.cs.washington.edu/education/courses/590MV/papers/zima.ps
Refering-URL: http://www.cs.washington.edu/education/courses/590MV/
Root-URL: 
Email: E-Mail: zima@par.univie.ac.at, barbara@par.univie.ac.at  
Title: COMPILING FOR DISTRIBUTED-MEMORY SYSTEMS  
Author: Hans Zima Barbara Chapman 
Keyword: distributed-memory multiprocessor systems, numerical computation, data parallel algo rithms, data distribution, program analysis, optimization.  
Date: May 20, 1994  
Address: Vienna, Brunner Strasse 72, A-1210 VIENNA, AUSTRIA  
Affiliation: Institute for Statistics and Computer Science University of  
Abstract: Distributed-memory systems are potentially scalable to a very large number of processors and promise to be powerful tools for solving large-scale scientific and engineering problems. However, these machines are currently difficult to program, since the user has to distribute the data across the processors and explicitly formulate the communication required by the program under the selected distribution. During the past years, language extensions of standard programming languages such as Fortran were developed that allow a concise formulation of data distribution, and new compilation methods were designed and implemented that allow the programming of such machines at this relatively high level. In this paper, we describe the current state of the art in compiling procedural languages (in particular, Fortran) for distributed-memory machines, analyze the limitations of these approaches, and outline future research. fl The work described in this paper was supported by the Austrian Research Foundation (FWF) under the grant number P7576-TEC and by the Austrian Ministry for Science and Research (BMWF) 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho,R. Sethi,J.D. Ullman. </author> <title> Compilers.Principles,Techniques and Tools. </title> <publisher> Addison-Wesley, </publisher> <year> 1986 </year>
Reference: [2] <author> S. Ahuja, N. Carriero, and D. Gelernter. </author> <title> Linda and friends. </title> <journal> IEEE Computer, </journal> <volume> 19 </volume> <pages> 26-34, </pages> <month> August </month> <year> 1986. </year>
Reference: [3] <author> F. Andre, J.-L. Pazat, and H. Thomas. </author> <title> PANDORE: A system to manage data distribution. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 380-388, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Frequently, partial rather than total replication is advantageous. This has been implemented in many systems. * Dynamic Distributions The term dynamic data distribution refers to the possibility of changing the distribution associated with an array at run time. In a few systems, this has been implemented within narrow constraints <ref> [4, 3] </ref>. Some recent languages propose very general dynamic distribution features [23, 74, 14]; however these have not yet been fully implemented. * Owner Computes Paradigm The owner computes paradigm simplifies code generation considerably. <p> Nevertheless, the recognition of frequent code patterns and the consultation of a corresponding 30 database of distributions is an interesting new approach to this task. Other currently existing systems include AL, which has been implemented on the Warp systolic array processor [67], Pandore, a C-based system <ref> [3] </ref>, and Id Nouveau, a compiler for a functional language [58]. A number of systems, all based upon a set of language extensions to Fortran 77 and/or Fortran 90, are currently being implemented.
Reference: [4] <author> Marc Baber. </author> <title> Hypertasking support for dynamically redistributable and resizeable arrays on the iPSC. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> 59-66, </pages> <year> 1990. </year>
Reference-contexts: Frequently, partial rather than total replication is advantageous. This has been implemented in many systems. * Dynamic Distributions The term dynamic data distribution refers to the possibility of changing the distribution associated with an array at run time. In a few systems, this has been implemented within narrow constraints <ref> [4, 3] </ref>. Some recent languages propose very general dynamic distribution features [23, 74, 14]; however these have not yet been fully implemented. * Owner Computes Paradigm The owner computes paradigm simplifies code generation considerably.
Reference: [5] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference: [6] <author> B. Baxter and B. Greer. Apply: </author> <title> A parallel compiler on iWarp for image-processing applications. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <pages> 186-93, </pages> <year> 1991. </year>
Reference: [7] <author> S. Benkner, B. Chapman, and H. Zima. </author> <title> Vienna Fortran 90. </title> <booktitle> In Proceedings of the SHPCC Conference 1992, </booktitle> <pages> 51-59, </pages> <address> Williamsburg VA, </address> <year> 1992. </year>
Reference: [8] <author> P. Brezany, H.M. Gerndt, V. Sipkova and H. Zima. </author> <title> SUPERB support for irregular scientific computations. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> 314-321, </pages> <address> Williamsburg VA, </address> <year> 1992. </year>
Reference: [9] <author> P. Bose. </author> <title> Heuristic rule-based program transformations for enhanced vectorization. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1988. </year>
Reference: [10] <author> P. Bose. </author> <title> Interactive program improvement via EAVE: an expert adviser for vectorization. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <address> St. Malo, </address> <year> 1988. </year>
Reference: [11] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2(2), </volume> <pages> 151-69. </pages>
Reference-contexts: This presentation relies mainly upon the methods implemented in SUPERB and subsequently adopted by several other systems (cf. Section 7); some of the features were also proposed in <ref> [11] </ref>.
Reference: [12] <author> B. Chapman, H. Herbeck, and H.P. Zima. </author> <title> Automatic Support for Data Distribution. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <pages> 51-58, </pages> <year> 1991. </year>
Reference: [13] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran A Fortran language extension for distributed memory systems. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-time Environments for Distributed Memory Machines. </title> <publisher> Elsevier Press, </publisher> <year> 1992. </year>
Reference-contexts: One focus is on the provision of appropriate high-level language constructs to enable users to design programs in much the same way as they are accustomed to on a sequential machine. Several proposals have been put forth in recent months for a set of language extensions to achieve this <ref> [23, 13, 14, 74, 15, 47, 54, 65] </ref>, in particular (but not only) for Fortran, and current compiler research is aimed at implementing them.
Reference: [14] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <booktitle> Scientific Programming 1,1 33 </booktitle>
Reference-contexts: One focus is on the provision of appropriate high-level language constructs to enable users to design programs in much the same way as they are accustomed to on a sequential machine. Several proposals have been put forth in recent months for a set of language extensions to achieve this <ref> [23, 13, 14, 74, 15, 47, 54, 65] </ref>, in particular (but not only) for Fortran, and current compiler research is aimed at implementing them. <p> These two languages are introduced for the purpose of this paper only, and we will informally describe the components actually needed, based on Vienna Fortran <ref> [74, 14] </ref> in the case of DPF. The central concepts of DPF are processors and distributions: * DPF allows the explicit specification of processor arrays to define the set of (abstract) processors used to execute a program. This set will in the following be denoted by P . <p> In particular, we will not discuss COMMON and EQUIVALENCE, mainly because the unrestricted use of these features makes automatic parallelization virtually impossible. 5 However, appropriate constraints can be defined under which these primitives can be handled successfully <ref> [73, 50, 14, 54] </ref>. <p> In a few systems, this has been implemented within narrow constraints [4, 3]. Some recent languages propose very general dynamic distribution features <ref> [23, 74, 14] </ref>; however these have not yet been fully implemented. * Owner Computes Paradigm The owner computes paradigm simplifies code generation considerably. <p> This includes Adapt, being developed at the University of Southampton [49], the Yale Extensions [15], Fortran D [23, 35] (Rice University), a new set of language extensions to Cray Fortran [54], DEC's High Performance Fortran proposal [47], and Vienna Fortran <ref> [14] </ref>. The full implementation of these languages for which a common basis is being sought in the High Performance Fortran Forum will require a significant advance in the state of the art of compilation systems (see Section 8).
Reference: [15] <author> M. Chen and J. Li. </author> <title> Optimizing Fortran 90 programs for data motion on massively parallel systems. </title> <type> Technical Report YALE/DCS/TR-882, </type> <institution> Yale University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: One focus is on the provision of appropriate high-level language constructs to enable users to design programs in much the same way as they are accustomed to on a sequential machine. Several proposals have been put forth in recent months for a set of language extensions to achieve this <ref> [23, 13, 14, 74, 15, 47, 54, 65] </ref>, in particular (but not only) for Fortran, and current compiler research is aimed at implementing them. <p> A number of systems, all based upon a set of language extensions to Fortran 77 and/or Fortran 90, are currently being implemented. This includes Adapt, being developed at the University of Southampton [49], the Yale Extensions <ref> [15] </ref>, Fortran D [23, 35] (Rice University), a new set of language extensions to Cray Fortran [54], DEC's High Performance Fortran proposal [47], and Vienna Fortran [14].
Reference: [16] <author> N.P. Chrisochoides, C.E. Houstis, E.N. Houstis, P.N. Papachiou, S.K. Kortesis and J.R. Rice. </author> <title> DOMAIN DECOMPOSER: A software tool for mapping PDE computations to parallel architectures. Domain Decomposition Methods for Differential Equations, </title> <publisher> SIAM, </publisher> <pages> 341-357, </pages> <year> 1991. </year>
Reference: [17] <author> Compass, Inc. </author> <title> SUPRENUM Fortran Reference Manual. </title> <publisher> Compass Inc., </publisher> <address> Wakefield, MA, </address> <year> 1989. </year>
Reference: [18] <author> G.O. Cook. </author> <title> ALPAL A tool for development of large-scale simulation codes. </title> <type> Technical Report, </type> <institution> Lawrence Livermore National Lab. UCRL 102076, </institution> <year> 1989. </year>
Reference: [19] <author> E. DeBenedictis and P. Madams. </author> <title> nCUBE's parallel I/O with Unix compatibility. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <pages> 270-77, </pages> <year> 1991. </year>
Reference-contexts: Furthermore, compilers must facilitate the storage and retrieval of arrays in distributed files striped across disks as in modern concurrent file systems <ref> [55, 19] </ref>. Here, we discuss just a few of the relevant issues. One research topic receiving attention is the development of support for the user in the crucial task of selecting a data distribution.
Reference: [20] <author> R.J. Duffin. </author> <title> On Fourier's analysis of linear inequality systems. </title> <booktitle> Mathematical Programming Study 1, </booktitle> <pages> 71-95, </pages> <year> 1974. </year>
Reference-contexts: The corresponding set of constraints for the loop variable can be determined by applying the Fourier-Motzkin method <ref> [43, 20] </ref> to these inequalities. From these constraints, ffi A (p), and x, the overlap description OD (S, A (x)) can be computed. 4. Whenever there is insufficient static information to determine the value of a component in the overlap description, a conservative estimate has to be made.
Reference: [21] <author> Fortran 90. </author> <title> ANSI X3J3 Internal Document S.8.118, </title> <month> May </month> <year> 1991. </year>
Reference-contexts: 0.25 * (F (I,J) + U (I-1,J) + U (I+1,J) + U (I,J-1) + U (I,J+1)) ENDDO ENDDO 3.4 General Block Distributions An array A has a general block distribution if the set of elements of each processor p in the corresponding processor array is a segment, or rectilinear section <ref> [21] </ref>, of A and, further, the segments associated with different processors are either disjoint or identical. More precisely: Definition 6 General Block Distributions Let A 2 A, with I A = X n i=1 D i and D i = [l i : u i ] for all i.
Reference: [22] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference: [23] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <institution> Department of Computer Science Rice COMP TR90079, Rice University, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: One focus is on the provision of appropriate high-level language constructs to enable users to design programs in much the same way as they are accustomed to on a sequential machine. Several proposals have been put forth in recent months for a set of language extensions to achieve this <ref> [23, 13, 14, 74, 15, 47, 54, 65] </ref>, in particular (but not only) for Fortran, and current compiler research is aimed at implementing them. <p> Such distributions have been implemented in a number of systems, including Kali [48], DINO [60], and the MIMDizer [50]. Some languages propose user-defined distribution functions that allow the specification of arbitrary distributions <ref> [48, 23, 74] </ref>. In Kali, a restricted set of such functions has been implemented. Frequently, partial rather than total replication is advantageous. <p> In a few systems, this has been implemented within narrow constraints [4, 3]. Some recent languages propose very general dynamic distribution features <ref> [23, 74, 14] </ref>; however these have not yet been fully implemented. * Owner Computes Paradigm The owner computes paradigm simplifies code generation considerably. <p> A number of systems, all based upon a set of language extensions to Fortran 77 and/or Fortran 90, are currently being implemented. This includes Adapt, being developed at the University of Southampton [49], the Yale Extensions [15], Fortran D <ref> [23, 35] </ref> (Rice University), a new set of language extensions to Cray Fortran [54], DEC's High Performance Fortran proposal [47], and Vienna Fortran [14].
Reference: [24] <author> D. Gannon, B. Shei and D. Atapattu. </author> <title> Programming environments for parallel computation. </title> <year> 1988. </year>
Reference: [25] <author> H. M. Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Bonn, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Research in compiler technology has so far resulted in the development of a number of prototype systems, such as SUPERB <ref> [25, 73] </ref>, Kali [41], and the MIMDizer [50]. In contrast to the current programming paradigm, these systems enable the user to write code using global data references, as on a shared memory machine, but require him or her to specify the distribution of the program's data. <p> This description can be used to significantly improve the organization of communication; it facilitates memory allocation and the local addressing of arrays. The relevant analysis is described in detail in <ref> [25] </ref>. 15 The overlap area for an array A is specified by its overlap description, OD (A), which is determined by the maximum offsets for every dimension of the distribution segments, over all processors. <p> The communication for a data item can be moved out of a loop if no true dependence is violated. Loop distribution and vectorization [75] can then be applied to generate aggregate communication. Further optimization of communication includes fusion and elimination of redundant communication, as described in the literature <ref> [27, 25, 46] </ref>. A more general approach independent of the overlap concept is discussed in the next section (5.2). 4.3.5 Optimization of Masking After initial masking (Section 4.2.1), all processors execute the same masked statement sequence. <p> Note, too, that different incarnations of the same procedure may associate the same formal parameter with arrays that have different distribution characteristics. The code implementing the accesses to a formal array parameter has to take into account all possible cases for the corresponding argument. Gerndt <ref> [25] </ref> has formulated a sufficient condition (using the concept of a subarray) to guarantee that the formal parameter associated with a block-distributed actual argument is also distributed by block. This is used below to optimize the code generation for procedures. <p> This is used below to optimize the code generation for procedures. We describe the main features of an interprocedural optimization strategy for the handling of partitioned arrays in procedures as implemented in SUPERB. More details are given in <ref> [26, 25] </ref>. We assume that a call graph, G, for the program has been constructed. G contains a node for each procedure of the program, and an edge from node n to node n 0 if the body of n contains a call to n 0 .
Reference: [26] <author> H.M. Gerndt. </author> <title> Array Distribution in SUPERB. </title> <booktitle> In Proceedings of the 3rd International Conference on Supercomputing 1989, </booktitle> <pages> 164-174. </pages>
Reference-contexts: This is used below to optimize the code generation for procedures. We describe the main features of an interprocedural optimization strategy for the handling of partitioned arrays in procedures as implemented in SUPERB. More details are given in <ref> [26, 25] </ref>. We assume that a call graph, G, for the program has been constructed. G contains a node for each procedure of the program, and an edge from node n to node n 0 if the body of n contains a call to n 0 .
Reference: [27] <author> H.M. Gerndt, </author> <title> H.P. Zima. Optimizing Communication in SUPERB. </title> <booktitle> In Procedings of CONPAR90, </booktitle> <year> 1990. </year>
Reference-contexts: The communication for a data item can be moved out of a loop if no true dependence is violated. Loop distribution and vectorization [75] can then be applied to generate aggregate communication. Further optimization of communication includes fusion and elimination of redundant communication, as described in the literature <ref> [27, 25, 46] </ref>. A more general approach independent of the overlap concept is discussed in the next section (5.2). 4.3.5 Optimization of Masking After initial masking (Section 4.2.1), all processors execute the same masked statement sequence.
Reference: [28] <author> H.M. Gerndt. </author> <title> Work Distribution in Parallel Programs for Distributed Memory Multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: Mask Simplification: If it can be shown that a mask is always true for each instance of each process, it can be eliminated. A more general discussion of mask optimization and the associated work distribution can be found in <ref> [28] </ref>. We again illustrate these optimizations as well as the target code generation with the example program. executing processor, p. It is assumed that U (p) = U ($L1 (p) : $U 1 (p); $L2 (p) : $U 2 (p)).
Reference: [29] <author> H.M. Gerndt. </author> <title> Updating Distributed Variables in Local Computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol.2(3),171-193, </volume> <month> September </month> <year> 1990. </year>
Reference: [30] <author> H.M. Gerndt. </author> <title> Program Analysis and Transformations for Message-Passing Programs. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, </address> <year> 1992. </year>
Reference-contexts: The techniques needed to perform this potentially very useful transformation have been implemented in SUPERB and are described in <ref> [30] </ref>.
Reference: [31] <author> W.K. Giloi. </author> <title> SUPRENUM: A trendsetter in modern supercomputer development. </title> <booktitle> Parallel Computing 7, </booktitle> <pages> 283-296, </pages> <year> 1988. </year>
Reference: [32] <author> V. Guarna, D. Gannon, D. Jablonowski, A. Malony and Y. Gaur. </author> <title> Faust: An integrated environment for parallel programming. </title> <type> CSRD Report CSRD-825, </type> <institution> University of Illinois, </institution> <year> 1988. </year>
Reference: [33] <author> M. Gupta, and P. Banerjee. </author> <title> Automatic Data Partitioning on Distributed Memory Multiprocessors. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computing Conference, </booktitle> <year> 1991. </year>
Reference: [34] <author> P.J. Hatcher and M.J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press Scientific and Engineering Computation Series, </publisher> <year> 1991. </year>
Reference: [35] <author> S.Hiranandani, K.Kennedy and C.-W.Tseng. </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines. </title> <booktitle> Comm.ACM Vol.35,No.8, </booktitle> <pages> pages 66-80, </pages> <month> August </month> <year> 1992. </year> <month> 34 </month>
Reference-contexts: A number of systems, all based upon a set of language extensions to Fortran 77 and/or Fortran 90, are currently being implemented. This includes Adapt, being developed at the University of Southampton [49], the Yale Extensions [15], Fortran D <ref> [23, 35] </ref> (Rice University), a new set of language extensions to Cray Fortran [54], DEC's High Performance Fortran proposal [47], and Vienna Fortran [14].
Reference: [36] <author> E.N. Houstis and J.R. Rice. </author> <title> Parallel ELLPACK: A development and problem solving environment for high performance computing machines. </title> <booktitle> In Proceedings of IFIP Programming Environments for High-Level Scientific Problem Solving, </booktitle> <publisher> North-Holland, </publisher> <pages> 229-241, </pages> <year> 1992. </year>
Reference: [37] <author> David E. Hudak and Santosh G. Abraham. </author> <title> Compiler Techniques for Data Partitioning of Sequentially Iterated Parallel Loops. </title> <booktitle> In Proceedings of the ACM International Conference on Supercomputing, </booktitle> <month> 187-200 </month> <year> (1990) </year>
Reference: [38] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the The Fifth Distributed Memory Computing Conference, </booktitle> <pages> pages 1105-1114, </pages> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference: [39] <author> Kathleen Knobe, Joan D. Lukas, and Guy L. Steele. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing 8, </journal> <month> 102-118 </month> <year> (1990) </year>
Reference: [40] <author> C. Koelbel. </author> <title> Compiling programs for nonshared memory machines. </title> <type> Ph.D. Thesis, </type> <institution> Purdue University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: A more general approach parameterizes the program with the processor structure that will be associated with 23 the program at the time of its actual execution: the compiler must parameterize all distributions and data access functions. This approach has been implemented in Kali <ref> [48, 40] </ref>. * Distributions Cyclic and block-cyclic distributions are sometimes useful for achieving a good load balance. <p> Their semantics differ; however, they do share the common purpose of giving the programmer a means of enforcing parallel execution of loop iterations. We base our treatment of this topic on work by Saltz, Mehrotra, and Koelbel, in particular on Koelbel's Ph.D. Thesis <ref> [40] </ref>, which provides a detailed discussion.
Reference: [41] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Research in compiler technology has so far resulted in the development of a number of prototype systems, such as SUPERB [25, 73], Kali <ref> [41] </ref>, and the MIMDizer [50]. In contrast to the current programming paradigm, these systems enable the user to write code using global data references, as on a shared memory machine, but require him or her to specify the distribution of the program's data.
Reference: [42] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> In 2nd ACM SIGPLAN Symposium on Principles Practice of Parallel Programming, </booktitle> <pages> pages 177-186, </pages> <month> March </month> <year> 1990. </year>
Reference: [43] <author> H.W.Kuhn. </author> <title> Solvability and Consistency for Linear Equations and Inequalities American Mathematical Monthly 63, </title> <type> 217-232, </type> <year> 1956 </year>
Reference-contexts: The corresponding set of constraints for the loop variable can be determined by applying the Fourier-Motzkin method <ref> [43, 20] </ref> to these inequalities. From these constraints, ffi A (p), and x, the overlap description OD (S, A (x)) can be computed. 4. Whenever there is insufficient static information to determine the value of a component in the overlap description, a conservative estimate has to be made.
Reference: [44] <author> Jingke Li. </author> <title> Compiling Crystal for Distributed-Memory Machines. </title> <type> Ph.D. Thesis, </type> <institution> Yale University, </institution> <type> Technical Report DCS/RR-876, </type> <month> October </month> <year> 1991. </year>
Reference-contexts: We outline the matching algorithm in Figure 15. Some simple optimizations are performed to eliminate duplicate communications. If a series of communication primitives are invoked to exchange data, their order in the code is optimized. A detailed description can be found in [46], see also Li's Ph.D.Thesis <ref> [44] </ref>. /* Input: a reference ref and the associated communication pattern ref */ /* Step 1: Initialization */ := ref /* Step 2: Attempt perfect match */ Search the list of communication routines, beginning with the simple and proceeding to the general routines, and try to find a perfect match for
Reference: [45] <author> J. Li and M. Chen. </author> <title> Index domain alignment:minimizing cost of cross-reference between distributed arrays. </title> <booktitle> In Frontiers '90: The Third Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <month> October, </month> <year> 1990. </year>
Reference-contexts: The techniques employed will depend on the kind of program under consideration and require extensive support from performance analysis tools. Li and Chen have developed a strategy for aligning data in regular codes within the Crystal compiler <ref> [45] </ref>; several other researchers have made substantial contributions to the problem of determining a distribution of program data. These include both theoretical approaches ([57, 37]), a constraint-based approach ([33]) and an implemented system for the CM-2 ([39]).
Reference: [46] <author> J. Li and M. Chen. </author> <title> Compiling Communication-Efficient Programs for Massively Parallel Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems Vol.2(3), </journal> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The communication for a data item can be moved out of a loop if no true dependence is violated. Loop distribution and vectorization [75] can then be applied to generate aggregate communication. Further optimization of communication includes fusion and elimination of redundant communication, as described in the literature <ref> [27, 25, 46] </ref>. A more general approach independent of the overlap concept is discussed in the next section (5.2). 4.3.5 Optimization of Masking After initial masking (Section 4.2.1), all processors execute the same masked statement sequence. <p> We outline the matching algorithm in Figure 15. Some simple optimizations are performed to eliminate duplicate communications. If a series of communication primitives are invoked to exchange data, their order in the code is optimized. A detailed description can be found in <ref> [46] </ref>, see also Li's Ph.D.Thesis [44]. /* Input: a reference ref and the associated communication pattern ref */ /* Step 1: Initialization */ := ref /* Step 2: Attempt perfect match */ Search the list of communication routines, beginning with the simple and proceeding to the general routines, and try to
Reference: [47] <author> D. Loveman. </author> <title> High Performance Fortran: Proposal. In High Performance Fortran Forum, </title> <address> Houston, TX, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: One focus is on the provision of appropriate high-level language constructs to enable users to design programs in much the same way as they are accustomed to on a sequential machine. Several proposals have been put forth in recent months for a set of language extensions to achieve this <ref> [23, 13, 14, 74, 15, 47, 54, 65] </ref>, in particular (but not only) for Fortran, and current compiler research is aimed at implementing them. <p> As a consequence, languages have been defined that allow more generality for formal procedure parameters, and in particular, the explicit enforcement of a distribution. This may lead to additional communication at procedure entry and exit, but can significantly improve the efficiency of data accesses in the procedure body <ref> [47, 74] </ref>. 6.2 Explicitly Parallel Loops and Run-Time Analysis Several different kinds of explicitly parallel loop have hitherto been proposed. Their semantics differ; however, they do share the common purpose of giving the programmer a means of enforcing parallel execution of loop iterations. <p> This includes Adapt, being developed at the University of Southampton [49], the Yale Extensions [15], Fortran D [23, 35] (Rice University), a new set of language extensions to Cray Fortran [54], DEC's High Performance Fortran proposal <ref> [47] </ref>, and Vienna Fortran [14]. The full implementation of these languages for which a common basis is being sought in the High Performance Fortran Forum will require a significant advance in the state of the art of compilation systems (see Section 8).
Reference: [48] <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <editor> In A. Nicolau, D. Gelernter, T. Gross, and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 364-384. </pages> <address> Pitman/MIT-Press, </address> <year> 1991. </year>
Reference-contexts: A more general approach parameterizes the program with the processor structure that will be associated with 23 the program at the time of its actual execution: the compiler must parameterize all distributions and data access functions. This approach has been implemented in Kali <ref> [48, 40] </ref>. * Distributions Cyclic and block-cyclic distributions are sometimes useful for achieving a good load balance. <p> Such distributions have been implemented in a number of systems, including Kali <ref> [48] </ref>, DINO [60], and the MIMDizer [50]. Some languages propose user-defined distribution functions that allow the specification of arbitrary distributions [48, 23, 74]. In Kali, a restricted set of such functions has been implemented. Frequently, partial rather than total replication is advantageous. <p> Such distributions have been implemented in a number of systems, including Kali [48], DINO [60], and the MIMDizer [50]. Some languages propose user-defined distribution functions that allow the specification of arbitrary distributions <ref> [48, 23, 74] </ref>. In Kali, a restricted set of such functions has been implemented. Frequently, partial rather than total replication is advantageous. <p> Methods of relaxing this constraint, by allowing an explicit specification of where work is to be performed, have been proposed and implemented in Kali <ref> [48] </ref>; they will be discussed in Section 6.2 below. * Formal Procedure Parameters In some cases, the rule of inheritance with respect to the distributions of formal procedure parameters (section 4.4) is not appropriate.
Reference: [49] <author> J. H.Merlin. </author> <title> ADAPTing Fortran 90 Array Programs for Distributed Memory Architectures. </title> <booktitle> In H.P.Zima, editor,Parallel Computation. Proc.First International ACPC Conference, Salzburg, Austria, </booktitle> <pages> pages 184-200. </pages> <booktitle> Lecture Notes in Computer Science 591, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1991 </year>
Reference-contexts: A number of systems, all based upon a set of language extensions to Fortran 77 and/or Fortran 90, are currently being implemented. This includes Adapt, being developed at the University of Southampton <ref> [49] </ref>, the Yale Extensions [15], Fortran D [23, 35] (Rice University), a new set of language extensions to Cray Fortran [54], DEC's High Performance Fortran proposal [47], and Vienna Fortran [14].
Reference: [50] <author> MIMDizer User's Guide, </author> <note> Version 8.0. </note> <institution> Applied Parallel Research Inc., Placerville, </institution> <address> CA., </address> <year> 1992. </year>
Reference-contexts: Research in compiler technology has so far resulted in the development of a number of prototype systems, such as SUPERB [25, 73], Kali [41], and the MIMDizer <ref> [50] </ref>. In contrast to the current programming paradigm, these systems enable the user to write code using global data references, as on a shared memory machine, but require him or her to specify the distribution of the program's data. <p> In particular, we will not discuss COMMON and EQUIVALENCE, mainly because the unrestricted use of these features makes automatic parallelization virtually impossible. 5 However, appropriate constraints can be defined under which these primitives can be handled successfully <ref> [73, 50, 14, 54] </ref>. <p> Such distributions have been implemented in a number of systems, including Kali [48], DINO [60], and the MIMDizer <ref> [50] </ref>. Some languages propose user-defined distribution functions that allow the specification of arbitrary distributions [48, 23, 74]. In Kali, a restricted set of such functions has been implemented. Frequently, partial rather than total replication is advantageous.
Reference: [51] <author> E. Paalvast and H. Sips. </author> <title> A high-level language for the description of parallel algorithms. </title> <booktitle> In Proceedings of Parallel Computing 89, </booktitle> <address> Leyden, Netherlands, </address> <month> August </month> <year> 1989. </year>
Reference: [52] <author> E. Paalvast, A. van Gemund, and H. Sips. </author> <title> A method of parallel program generation with an application to Booster language. </title> <booktitle> In Proceedings of the 4th International Conference on Supercomputing, </booktitle> <address> Amsterdam, </address> <month> June </month> <year> 1990. </year> <month> 35 </month>
Reference: [53] <author> D.A. Padua and M.J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> In Communications of the ACM, </journal> <volume> 29, </volume> <pages> pp. 1184-1201. </pages>
Reference-contexts: The usefulness of pattern matching techniques paired with library routines for the target machine is exemplified by the work described below in 5.2. 5.1 Loop Transformations A number of loop transformations have been developed to parallelize code for execution on shared memory computers <ref> [53] </ref>. Many of these transformation can be equally fruitfully applied to the task of parallelizing code for distributed memory machines. Precise conditions for their valid application are known and have been described in the literature [75].
Reference: [54] <author> D. Pase. </author> <title> MPP Fortran programming model. In High Performance Fortran Forum, </title> <address> Houston, TX, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: One focus is on the provision of appropriate high-level language constructs to enable users to design programs in much the same way as they are accustomed to on a sequential machine. Several proposals have been put forth in recent months for a set of language extensions to achieve this <ref> [23, 13, 14, 74, 15, 47, 54, 65] </ref>, in particular (but not only) for Fortran, and current compiler research is aimed at implementing them. <p> In particular, we will not discuss COMMON and EQUIVALENCE, mainly because the unrestricted use of these features makes automatic parallelization virtually impossible. 5 However, appropriate constraints can be defined under which these primitives can be handled successfully <ref> [73, 50, 14, 54] </ref>. <p> This includes Adapt, being developed at the University of Southampton [49], the Yale Extensions [15], Fortran D [23, 35] (Rice University), a new set of language extensions to Cray Fortran <ref> [54] </ref>, DEC's High Performance Fortran proposal [47], and Vienna Fortran [14]. The full implementation of these languages for which a common basis is being sought in the High Performance Fortran Forum will require a significant advance in the state of the art of compilation systems (see Section 8).
Reference: [55] <author> P. Pierce. </author> <title> A concurrent file system for a highly parallel mass storage subsystem. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <year> 1989. </year>
Reference-contexts: Furthermore, compilers must facilitate the storage and retrieval of arrays in distributed files striped across disks as in modern concurrent file systems <ref> [55, 19] </ref>. Here, we discuss just a few of the relevant issues. One research topic receiving attention is the development of support for the user in the crucial task of selecting a data distribution.
Reference: [56] <author> Terence W. Pratt. </author> <title> The Pisces 2 parallel programming environment. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> 439-45, </pages> <year> 1987. </year>
Reference: [57] <author> J. Ramanujam, and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <institution> TR-90-09-03, Dept. Electrical and Comp. Engineering, Louisiana State University (Sept. </institution> <year> 1990). </year>
Reference: [58] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 69-80. </pages> <booktitle> ACM SIGPLAN, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Other currently existing systems include AL, which has been implemented on the Warp systolic array processor [67], Pandore, a C-based system [3], and Id Nouveau, a compiler for a functional language <ref> [58] </ref>. A number of systems, all based upon a set of language extensions to Fortran 77 and/or Fortran 90, are currently being implemented.
Reference: [59] <author> M. Rosing and R. Schnabel. </author> <title> An overview of Dino anew language for numerical computation on distributed memory multiprocessors. </title> <booktitle> In Proceedings of the Third SIAM Conference on Parallel Processing for Scientific Computation, </booktitle> <pages> 312-316, </pages> <year> 1987. </year>
Reference: [60] <author> M. Rosing, R. W. Schnabel, and R. P. Weaver. </author> <title> The DINO parallel programming language. </title> <type> Technical Report CU-CS-457-90, </type> <institution> University of Colorado, Boulder, CO, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Such distributions have been implemented in a number of systems, including Kali [48], DINO <ref> [60] </ref>, and the MIMDizer [50]. Some languages propose user-defined distribution functions that allow the specification of arbitrary distributions [48, 23, 74]. In Kali, a restricted set of such functions has been implemented. Frequently, partial rather than total replication is advantageous.
Reference: [61] <author> R. Ruhl and M. Annaratone. </author> <title> Parallelization of Fortran code on distributed-memory parallel processors. </title> <booktitle> In Proceedings of the ACM International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1990. </year>
Reference: [62] <author> T. Ruppelt and G. Wirtz. </author> <title> Automatic translation of high-level object-oriented specifications into parallel programs. </title> <booktitle> Parallel Computing 2, </booktitle> <year> 1988. </year>
Reference: [63] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Runtime compilation for multiprocessors (To appear: </title> <journal> Concurrency, Practice and Experience, </journal> <year> 1991). </year> <type> ICASE Report 90-59, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference: [64] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference: [65] <author> G. Steele. </author> <title> Proposals for amending High Performance Fortran. In High Performance Fortran Forum, </title> <address> Houston, TX, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: One focus is on the provision of appropriate high-level language constructs to enable users to design programs in much the same way as they are accustomed to on a sequential machine. Several proposals have been put forth in recent months for a set of language extensions to achieve this <ref> [23, 13, 14, 74, 15, 47, 54, 65] </ref>, in particular (but not only) for Fortran, and current compiler research is aimed at implementing them.
Reference: [66] <institution> CM Fortran Reference Manual, </institution> <note> Version 5.2. </note> <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference: [67] <author> P. S. Tseng. </author> <title> A systolic array programming language. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pages 1125-1130, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Nevertheless, the recognition of frequent code patterns and the consultation of a corresponding 30 database of distributions is an interesting new approach to this task. Other currently existing systems include AL, which has been implemented on the Warp systolic array processor <ref> [67] </ref>, Pandore, a C-based system [3], and Id Nouveau, a compiler for a functional language [58]. A number of systems, all based upon a set of language extensions to Fortran 77 and/or Fortran 90, are currently being implemented.
Reference: [68] <author> Y. Umetani et al. </author> <title> A numerical simulation language for vector/parallel processors. </title> <editor> In B. Ford, F. Chatelin (Eds.) </editor> <title> Problem Solving Environments for Scientific Computing, </title> <publisher> North-Holland, </publisher> <year> 1987. </year>
Reference: [69] <author> K.Y. Wang. </author> <title> A framework for intelligent parallel compilers. </title> <type> Tech. Report CSD-TR-1044, </type> <institution> Computer Science Dept., Purdue University, </institution> <year> 1990. </year>
Reference-contexts: This approach permits code to be transformed for a variety of different machines by a single compilation system <ref> [69] </ref>. Integrated Programming Environments A compiler in the conventional sense will not, of its own, suffice to support the highly complex and challenging task of producing efficient programs for parallel systems.
Reference: [70] <author> K.Y. Wang and D. Gannon. </author> <title> Applying AI techniques to program optimization for parallel computers. </title> <booktitle> Parallel Processing for Supercomputers and Artificial Intelligence, </booktitle> <volume> 12, </volume> <pages> 441-485, </pages> <publisher> McGraw-Hill, </publisher> <year> 1989. </year>
Reference: [71] <author> J. Wu, J. Saltz, H. Berryman and S. Hiranandani. </author> <title> Distributed memory compiler design for sparse problems. </title> <type> ICASE Report 91-13, </type> <month> January </month> <year> 1991. </year> <month> 36 </month>
Reference: [72] <author> F. Yamamoto and Y. Umetani. PARAGRAM: </author> <title> A high-level programming language for parallel proces-sors. </title> <booktitle> Parallel Computing, </booktitle> <year> 1989. </year>
Reference: [73] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> Superb: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Research in compiler technology has so far resulted in the development of a number of prototype systems, such as SUPERB <ref> [25, 73] </ref>, Kali [41], and the MIMDizer [50]. In contrast to the current programming paradigm, these systems enable the user to write code using global data references, as on a shared memory machine, but require him or her to specify the distribution of the program's data. <p> In particular, we will not discuss COMMON and EQUIVALENCE, mainly because the unrestricted use of these features makes automatic parallelization virtually impossible. 5 However, appropriate constraints can be defined under which these primitives can be handled successfully <ref> [73, 50, 14, 54] </ref>.
Reference: [74] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran a language specification. </title> <type> ICASE Internal Report 21, </type> <institution> ICASE, Hampton, VA, </institution> <year> 1992. </year>
Reference-contexts: One focus is on the provision of appropriate high-level language constructs to enable users to design programs in much the same way as they are accustomed to on a sequential machine. Several proposals have been put forth in recent months for a set of language extensions to achieve this <ref> [23, 13, 14, 74, 15, 47, 54, 65] </ref>, in particular (but not only) for Fortran, and current compiler research is aimed at implementing them. <p> These two languages are introduced for the purpose of this paper only, and we will informally describe the components actually needed, based on Vienna Fortran <ref> [74, 14] </ref> in the case of DPF. The central concepts of DPF are processors and distributions: * DPF allows the explicit specification of processor arrays to define the set of (abstract) processors used to execute a program. This set will in the following be denoted by P . <p> Such distributions have been implemented in a number of systems, including Kali [48], DINO [60], and the MIMDizer [50]. Some languages propose user-defined distribution functions that allow the specification of arbitrary distributions <ref> [48, 23, 74] </ref>. In Kali, a restricted set of such functions has been implemented. Frequently, partial rather than total replication is advantageous. <p> In a few systems, this has been implemented within narrow constraints [4, 3]. Some recent languages propose very general dynamic distribution features <ref> [23, 74, 14] </ref>; however these have not yet been fully implemented. * Owner Computes Paradigm The owner computes paradigm simplifies code generation considerably. <p> As a consequence, languages have been defined that allow more generality for formal procedure parameters, and in particular, the explicit enforcement of a distribution. This may lead to additional communication at procedure entry and exit, but can significantly improve the efficiency of data accesses in the procedure body <ref> [47, 74] </ref>. 6.2 Explicitly Parallel Loops and Run-Time Analysis Several different kinds of explicitly parallel loop have hitherto been proposed. Their semantics differ; however, they do share the common purpose of giving the programmer a means of enforcing parallel execution of loop iterations.
Reference: [75] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press Frontier Series, Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: We do not discuss the details of the Front End analysis and transformations in this paper: they are essentially the same as those employed for automatic vectorization and shared-memory parallelization, and as such have been discussed extensively in the literature <ref> [75] </ref>. The resulting program, Q 1 , is called the normalized program. Phase 2 (Splitting): DPF Normalized Program Q 1 7! (Host Program Q H , Node Program Q 2 ) Q 1 is split into a host program, Q H , and a node program, Q 2 . <p> Communication and masking are improved: Communication statements are moved out of loops and combined to perform aggregate communication where possible; the strip mining of loops <ref> [75] </ref> across the processors can be achieved in many cases by propagating the information in masks to the loop bounds. <p> The communication for a data item can be moved out of a loop if no true dependence is violated. Loop distribution and vectorization <ref> [75] </ref> can then be applied to generate aggregate communication. Further optimization of communication includes fusion and elimination of redundant communication, as described in the literature [27, 25, 46]. <p> For each masked statement, each processor first evaluates the mask and executes the corresponding (unmasked) statement instance if the mask yields true. The following transformations optimize the handling of masks in loops and, in many cases, lead to the strip-mining <ref> [75] </ref> of the loop across the processors by partitioning the iteration space: 1. Iteration Elimination: Iteration elimination deletes irrelevant statement instances by eliminating an entire loop iteration for a process. 2. <p> Many of these transformation can be equally fruitfully applied to the task of parallelizing code for distributed memory machines. Precise conditions for their valid application are known and have been described in the literature <ref> [75] </ref>. Their application will depend on the manner in which the program data has been distributed. Particularly useful in this context are loop interchange and loop distribution. 7 Note that more than one case may apply. 21 Loop interchange exchanges two levels of a nested loop.
Reference: [76] <author> H. Zima and B. Chapman. </author> <title> Software tools for parallel program development. </title> <booktitle> In Proceedings of the IFIP Programming Environments for High-Level Scientific Problem Solving, </booktitle> <publisher> North-Holland, </publisher> <pages> 157-73, </pages> <year> 1992. </year> <month> 37 </month>
References-found: 76

