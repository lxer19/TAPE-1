URL: http://www.cs.umd.edu/~fatma/publications/cmsc818.ps
Refering-URL: http://www.cs.umd.edu/~fatma/courses.html
Root-URL: 
Title: A Comparison of Content-Based Image/Video Retrieval Systems  
Author: Fatma Ozcan 
Date: December 15, 1997  
Abstract: Recently, content-based retrieval of images and video has become a hot research area. The reason for this is the need for effective and efficient techniques, that meet user requirements, to access large volumes of digital images and video data. In [GR 95], previous approaches to content-based retrieval is said to have taken two directions. In the first approach, image contents are modeled as a set of attributes extracted manually and managed within the framework of conventional database-management systems. The second approach depends on an integrated feature-extraction/object-recognition subsystem to overcome the limitations of attribute-based retrieval. However, it has been also stated that [GR 95] recent content-based image retrieval systems recognized the need for synergy between these two approaches. There has been numerous efforts to build content-based image retrieval systems. These include, Chabot [OS 95], SCORE [ATY 95, ATY 96], CONIVAS [AD 96], Photobook [PPS 93], CANDID [KCH 95], JACOB [ALDV 96, LA 96], VisualSEEk [SC 96] and QBIC [FSN+ 95]. In the first place, these systems differ in their querying capabilities. Images could be retrieved through the use of color, texture, sketch, shape, volume, spatial constraints, browsing, motion, text and domain concepts. Current approaches to content-based image retrieval also differ in terms of image features extracted, their level of abstraction and the degree of domain independence. In this paper, a comparative survey on these systems is conducted to figure out design tradeoffs and different approaches to content-based image retrieval. Furthermore, a taxonomy of content-based retrieval systems, as well as a generic architecture, is provided.
Abstract-found: 1
Intro-found: 1
Reference: [AD 96] <author> M. Abdel-Mottaleb, N. Dimitrova, R. Desai and J. Martino, " CONIVAS: </author> <title> CONtent-based Image and Video Access System", </title> <booktitle> ACM Multimedia'96, </booktitle> <address> Boston, MA, USA, </address> <pages> pages 427-428, </pages> <year> 1996 </year>
Reference-contexts: There is only one system, namely SCORE [ATY 95, ATY 96], as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk [SC 96], CONIVAS <ref> [AD 96] </ref>, JACOB [ALDV 96, LA 96], CANDID [KCH 95] and Photobook [PPS 93]. Finally, as an example for attribute description feature extraction systems Chabot [OS 95] system is examined. <p> However, effective content-based retrieval is believed to be based on visual image contents rather than textual descriptions. 2.2 Feature Extraction Systems This type of systems employ semi-automated or automated feature-extraction/object-recognition methods. This approach is mainly inspired by image processing researchers. The systems; CONIVAS <ref> [AD 96] </ref>, Photobook [PPS 93], CANDID [KCH 95], JACOB [ALDV 96, LA 96], VisualSEEk [SC 96] and QBIC [FSN+ 95] are examples of this approach. However, automated feature extraction and object recognition tasks are computationally intensive, difficult and expensive operations. <p> Moreover, indexing techniques employed may have false dismissals. Yet, it exploits spatial relationships between regions of an image which is also a required capability for content-based retrieval. 2.2.3 CONIVAS: CONtent-based Image and Video Access System CONIVAS (CONtent-based Image and Video Access System), <ref> [AD 96] </ref>, is a system developed for retrieving both still images and video from databases by content. The system supports retrieving of images by example via the use of edge and color information. <p> Moreover, the system also provides features to construct new images from the existing ones. Furthermore, the system provides querying by sketch capabilities. Local color features are used as opposed to global color information. In <ref> [AD 96] </ref>, it is argued that systems using global color information retrieves images having a mix of the specified colors. On the other hand, local color features preserve the spatial relationships and properties of the color regions specified in the example image in their systems. <p> In order to achieve this, a similarity measure based on a measure derived from information theory, instead of L 1 and L 2 metrics, is employed. In <ref> [AD 96] </ref>, two approaches are provided for video retrieval. In the first approach, representative frames of a video clip are searched using still image retrieving techniques to retrieve the video clip. In order to facilitate this approach, video clips are first segmented into shots. <p> In order to facilitate this approach, video clips are first segmented into shots. The boundary between the shots is defined to be places where there is a scene change or scene transition, <ref> [AD 96] </ref>. In the second case, a video clip is used as an example to retrieve the desired video clip. The similarity is calculated based on signatures derived from spatial and motion characteristics of both the example and the target video clip. In [AD 96], this method is called DC+M video <p> is a scene change or scene transition, <ref> [AD 96] </ref>. In the second case, a video clip is used as an example to retrieve the desired video clip. The similarity is calculated based on signatures derived from spatial and motion characteristics of both the example and the target video clip. In [AD 96], this method is called DC+M video retrieval method. In the prototype system, images of similar content are grouped together in a database. The user can choose one 9 of the image databases and then browse and search the activated database. <p> The user can choose one 9 of the image databases and then browse and search the activated database. The interface supports features to facilitate the formation of supported query types. The system is unique to provide video clips as examples in queries. However, in <ref> [AD 96] </ref>, no performance issues are addressed. 2.2.4 JACOB JACOB, [LA 96], is a content-based retrieval system focusing on video databases. The system automatically splits videos into shots, and extracts a few representative frames from each shot. In order to extract shots from videos shot cut detection is performed.
Reference: [ALDV 96] <author> E. Ardizzone, M. La Cascia, V. Di Gesu' and C. Valenti, </author> <title> "Content Based Indexing of Image and Video Databases by Global and Shape Features", </title> <booktitle> Int. Conf on Pattern Recognition, </booktitle> <address> ICPR, Wien, Austria, </address> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: There is only one system, namely SCORE [ATY 95, ATY 96], as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk [SC 96], CONIVAS [AD 96], JACOB <ref> [ALDV 96, LA 96] </ref>, CANDID [KCH 95] and Photobook [PPS 93]. Finally, as an example for attribute description feature extraction systems Chabot [OS 95] system is examined. <p> This approach is mainly inspired by image processing researchers. The systems; CONIVAS [AD 96], Photobook [PPS 93], CANDID [KCH 95], JACOB <ref> [ALDV 96, LA 96] </ref>, VisualSEEk [SC 96] and QBIC [FSN+ 95] are examples of this approach. However, automated feature extraction and object recognition tasks are computationally intensive, difficult and expensive operations. <p> In order to extract shots from videos shot cut detection is performed. The unique feature of JACOB is its use of a neural network approach for shot cut detection. Although this method is not able to detect shot boundaries in presence of editing effects such as fade and dissolve <ref> [ALDV 96] </ref>, it is claimed to work fine on a large variety of videos [LA 96]. Moreover, for each r-frame a descriptor based on color and texture features is computed during database population which is a completely automatic task requiring no human intervention. <p> In [LA 96], it is reported that using color and texture properties is a good starting point. Actually, a general and practical systems requires more than just color and texture features. While in [LA 96] each r-frame is characterized by only color and texture features, in a follow-on research, <ref> [ALDV 96] </ref>, r-frames are also characterized by global motion analysis. In [ALDV 96], an indexing methodology using both global features (color, texture and motion), and local features (shape) is also proposed. <p> Actually, a general and practical systems requires more than just color and texture features. While in [LA 96] each r-frame is characterized by only color and texture features, in a follow-on research, <ref> [ALDV 96] </ref>, r-frames are also characterized by global motion analysis. In [ALDV 96], an indexing methodology using both global features (color, texture and motion), and local features (shape) is also proposed. <p> The reason of this is the argument that global features may be computed and stored during database population whereas shape features tend to be more application dependent, <ref> [ALDV 96] </ref>. Color histograms are used for r-frame color-based indexing, whereas motion-based features are based on analysis of optical flow field. Moreover, both texture and geometrical features are used for shape-based indexing. The indexing methods proposed in [ALDV 96] may result in false dismissals. <p> stored during database population whereas shape features tend to be more application dependent, <ref> [ALDV 96] </ref>. Color histograms are used for r-frame color-based indexing, whereas motion-based features are based on analysis of optical flow field. Moreover, both texture and geometrical features are used for shape-based indexing. The indexing methods proposed in [ALDV 96] may result in false dismissals. JACOB is unique to employ a neural network approach in shot cut detection. Neural models involve huge computations which makes the system slow.
Reference: [ATY 95] <author> Y. A. Aslandogan, C. Thier, C. Yu, C. Liu and K. Nair, </author> <title> "Design, Implementation and Evaluation of SCORE ( a System for COntent based REtrieval of pictures)", </title> <booktitle> Proc. of the 11th International Conference on Data Engineering, </booktitle> <address> ICDE'95, </address> <month> March </month> <year> 1995, </year> <pages> pages 280-287 </pages>
Reference-contexts: They, rather, combine the properties of the two approaches. Based on this observation, a taxonomy for content-based retrieval systems is provided as in Figure 2. In the following sections, some prototype content-based retrieval systems will be discussed based on this taxonomy. There is only one system, namely SCORE <ref> [ATY 95, ATY 96] </ref>, as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk [SC 96], CONIVAS [AD 96], JACOB [ALDV 96, LA 96], CANDID [KCH 95] and Photobook [PPS 93]. <p> Queries mostly rely on text annotations. This approach is mainly taken by database researchers. The SCORE system <ref> [ATY 95, ATY 96] </ref> is one example. <p> * do not allow queries based directly on the visual properties of the images * do not provide queries for images similar to a given image * are dependent on the particular vocabulary used 2.1.1 SCORE: a System for COntent-based REtrieval of pictures SCORE (System for COntent-based REtrieval of pictures), <ref> [ATY 95, ATY 96] </ref>, is one of the example systems which rely on user-created description of images. SCORE makes use of an extended E-R model for image descriptions, and a graphical query language for formulation of queries by example. <p> Furthermore, SCORE employs a knowledge base containing a thesaurus, which is used for synonym and neighbor searching, and a set of sound and complete deduction rules used for spatial inferencing. Moreover, in <ref> [ATY 95] </ref>, it has been shown that the set of spatial relationships, containing over, under, left, right, behind, in-front-of, overlap, inside and outside, is sound and complete. Note that although there are further possible associations between entities, such as temporal, causal, logical or role relationships, SCORE does not allow them.
Reference: [ATY 96] <author> Y. A. Aslandogan, C. Thier and C. Yu, </author> <title> "A System for Effective Content Based Image Retrieval", </title> <booktitle> ACM Multimedia'96, </booktitle> <address> Boston, MA, USA, </address> <pages> pages 429-430, </pages> <year> 1996 </year>
Reference-contexts: They, rather, combine the properties of the two approaches. Based on this observation, a taxonomy for content-based retrieval systems is provided as in Figure 2. In the following sections, some prototype content-based retrieval systems will be discussed based on this taxonomy. There is only one system, namely SCORE <ref> [ATY 95, ATY 96] </ref>, as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk [SC 96], CONIVAS [AD 96], JACOB [ALDV 96, LA 96], CANDID [KCH 95] and Photobook [PPS 93]. <p> Queries mostly rely on text annotations. This approach is mainly taken by database researchers. The SCORE system <ref> [ATY 95, ATY 96] </ref> is one example. <p> * do not allow queries based directly on the visual properties of the images * do not provide queries for images similar to a given image * are dependent on the particular vocabulary used 2.1.1 SCORE: a System for COntent-based REtrieval of pictures SCORE (System for COntent-based REtrieval of pictures), <ref> [ATY 95, ATY 96] </ref>, is one of the example systems which rely on user-created description of images. SCORE makes use of an extended E-R model for image descriptions, and a graphical query language for formulation of queries by example. <p> Note that although there are further possible associations between entities, such as temporal, causal, logical or role relationships, SCORE does not allow them. In SCORE, four techniques are employed for image retrieval; normalization, object significance, conflict elimination and restricted matching across types, <ref> [ATY 96] </ref>. Normalization means that similarities are based on not only the set of matching elements, but also on those in images as well. Object significance corresponds the system's ability to let the user to define degrees of significance in an image.
Reference: [FSN+ 95] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele and P. Yanker, </author> <title> "Query by Image and Video Content: the QBIC System", </title> <note> IEEE Computer , September 1995 </note>
Reference-contexts: In the following sections, some prototype content-based retrieval systems will be discussed based on this taxonomy. There is only one system, namely SCORE [ATY 95, ATY 96], as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC <ref> [FSN+ 95] </ref>, Vi-sualSEEk [SC 96], CONIVAS [AD 96], JACOB [ALDV 96, LA 96], CANDID [KCH 95] and Photobook [PPS 93]. Finally, as an example for attribute description feature extraction systems Chabot [OS 95] system is examined. <p> This approach is mainly inspired by image processing researchers. The systems; CONIVAS [AD 96], Photobook [PPS 93], CANDID [KCH 95], JACOB [ALDV 96, LA 96], VisualSEEk [SC 96] and QBIC <ref> [FSN+ 95] </ref> are examples of this approach. However, automated feature extraction and object recognition tasks are computationally intensive, difficult and expensive operations. <p> In the following, content-based retrieval systems in this category are discussed in turn. 2.2.1 QBIC: Query By Image Content QBIC (Query By Image Content) <ref> [FSN+ 95] </ref>, is a comprehensive and operational prototype developed at IBM Almaden Research Center. The technology from this prototype is moved to a commercial stand-alone product, IBM's Ultimedia Manager, and is part of IBM's Digital Library and DB2 series of products, [FSN+ 95]. <p> Query By Image Content QBIC (Query By Image Content) <ref> [FSN+ 95] </ref>, is a comprehensive and operational prototype developed at IBM Almaden Research Center. The technology from this prototype is moved to a commercial stand-alone product, IBM's Ultimedia Manager, and is part of IBM's Digital Library and DB2 series of products, [FSN+ 95]. Based on this fact, QBIC can be termed as the most successful attempt to content-based retrieval. <p> The indexing scheme of QBIC assures sub-linear search while maintaining completeness, i.e no image satisfying the query is missed, no false dismissals but false alarms, <ref> [FSN+ 95] </ref>. For low-dimensional features such as average color, multidimensional indexing methods such a R fl trees can be used. On the other hand, for high dimensional features, such as 20-dimensional moment-based shape feature, there are two possibilities. <p> While one is to reduce the dimensionality using K-L, or principal component transform and to index the resulting low-dimensional feature using R fl trees, the other is the use of filters. The QBIC data model has two main components; still images or scenes and video shots, <ref> [FSN+ 95] </ref>. Still images contain objects and video shots consists of a set of contiguous frames. For still images, the data model distinguishes between scenes and objects. A scene is an image or a single representative frame of video whereas an object is part of a scene. <p> In addition, semiautomatic tools are also provided, including flood-fill algorithms and spline snake formulation. The second method, which is based on "snakes" concept developed in computer vision, takes a user-drawn curve and aligns it with nearby image edges, <ref> [FSN+ 95] </ref>. To extract features of video three major steps are taken. The first step is shot cut detection, the second one is choosing and creation of r-frames for each shot, and the last step is derivation of a layered representation of coherently moving structures/objects. <p> In order to create r-frames, a synthesized r-frame approach is employed instead of choosing a particular frame of the shot, <ref> [FSN+ 95] </ref>. Note that the synthesized r-frame approach is unique to QBIC. R-frames are created by seamless mosaicking of all the frames in a given shot using computed motion transformation of the dominant background. <p> In order to model the motion in video, so-called layered representation of video is generated. QBIC supports average color queries against images or objects by the use of a 3-D Munsell color coordinate feature, <ref> [FSN+ 95] </ref>. Histogram color queries are also supported which return the images with matching color distributions. QBIC employs 256-element color histograms. Moreover, QBIC uses histogram quadratic distance metric for matching images, which computes the cross similarity between colors, and thus is computationally expensive.
Reference: [GR 95] <author> V. N. Gudivada and V.V. Raghavan, </author> <title> "Content-Based Image Retrieval Systems", </title> <booktitle> IEEE Computer, </booktitle> <month> September </month> <year> 1995 </year> <month> 16 </month>
Reference-contexts: A content-based image retrieval system is required to effectively and efficiently use information that resides in huge image/video repositories. The ultimate goal of such a system is to help users retrieve relevant images based on their contents. In <ref> [GR 95] </ref>, the application areas of such content-based retrieval systems are said to enormous, diverse and includes the following; art galleries and museum management, architectural and engineering design, interior design, remote sensing and management of earth resources, geographic information systems, medical imaging, scientific database management systems, weather forecasting, retailing, fabric and <p> In order to achieve its goal, content-based retrieval systems recently, began to use ideas from areas 1 such as knowledge-based systems, cognitive science, user modeling, computer graphics, image processing, pattern recognition, database management systems and information retrieval, <ref> [GR 95] </ref>. However, content-based retrieval systems are, in general, inspired by database and image processing researchers. 2 Content-Based Image/Video Retrieval Systems: Common Char acteristics and Differences In the first place, content-based retrieval systems differ in their querying capabilities. There are two generic groups of queries; direct queries and by-example queries. <p> Whereas in queries by example, an example image is used to calculate the features automatically that will be employed in the matching process. In both types of queries, content-based retrieval systems can retrieve images, <ref> [GR 95] </ref>, through the use of their; color, texture, sketch, shape, volume, spatial constraints, browsing, motion, text and domain concepts. Note that the features specified in direct queries should be converted to their representations in the database in order to be used in the matching process. <p> As a consequence of this requirement, systems, in general, provide graphical query formulation tools which help the user to choose image features from a menu containing a list for which feature representations have been previously calculated. Furthermore, in <ref> [GR 95] </ref>, it is said that an image retrieval system providing all these query capabilities will have reasonable generality for dealing with diverse applications. <p> Content-based retrieval systems can also be developed with emphasis on domain independence. But they have to give up completely automated system for feature extraction. In these systems, a set of low-level features are extracted, and all logical ones are derived only when the images are inserted into the database, <ref> [GR 95] </ref>. Automated feature extraction techniques are costly operations in general as they involve enormous computations. Moreover, the image database size may be too large, bringing the issues of performance. A practical system is expected to answer a user query in a reasonable time. <p> In order to reduce the amount of work, some systems make use of indexes or filters in this modules. Others try to reduce the search space by employing multi-step algorithms. Yet, some others try to reduce costs by improving distance calculations. In <ref> [GR 95] </ref> two main approaches are pointed out in building content-based retrieval systems. In the first approach, image contents are modeled as a set of attributes extracted manually and managed within the framework of conventional database management systems. Queries are generally specified using these attributes.
Reference: [KCH 95] <author> P. M. Kelly, T. M. Cannon and D. R. Hush, </author> <title> "Query by image example : the CANDID approach", </title> <booktitle> Proc. of the SPIE, Storage and Retrieval for Image and Video Databases III, </booktitle> <volume> Vol. 2420, </volume> <pages> pages 238-248, </pages> <year> 1995 </year>
Reference-contexts: There is only one system, namely SCORE [ATY 95, ATY 96], as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk [SC 96], CONIVAS [AD 96], JACOB [ALDV 96, LA 96], CANDID <ref> [KCH 95] </ref> and Photobook [PPS 93]. Finally, as an example for attribute description feature extraction systems Chabot [OS 95] system is examined. <p> However, effective content-based retrieval is believed to be based on visual image contents rather than textual descriptions. 2.2 Feature Extraction Systems This type of systems employ semi-automated or automated feature-extraction/object-recognition methods. This approach is mainly inspired by image processing researchers. The systems; CONIVAS [AD 96], Photobook [PPS 93], CANDID <ref> [KCH 95] </ref>, JACOB [ALDV 96, LA 96], VisualSEEk [SC 96] and QBIC [FSN+ 95] are examples of this approach. However, automated feature extraction and object recognition tasks are computationally intensive, difficult and expensive operations. <p> Finally, the system is only a research prototype exploring 10 different methods and far from being realized as a practical implementation. 2.2.5 CANDID: Comparison Algorithm for Navigating Digital Image Databases CANDID (Comparison Algorithm for Navigating Digital Image Databases), <ref> [KCH 95] </ref>, is a content-based storage and retrieval system which uses a query by example methodology. CANDID computes global signatures for each image in the database which are derived from various image features such as localized texture, shape or color information. <p> Given the scalability problems of histogram approaches to high-dimensional data, CANDID employs probability density functions to represent the distribution of localized features, <ref> [KCH 95] </ref>. However, computation of probability density functions is much more expensive than histograms. Therefore, it is pointed that there is a trade-off between computation costs and scalability to higher dimensions. The probability density function is estimated as a Gaussian mixture. <p> Once a mixture of Gaussians has been identified, a signature over a specific N-dimensional feature space is computed. It is possible to use six different metrics for signature comparisons. The first one is the L 2 metric, but no closed-form solution said to be obtained, <ref> [KCH 95] </ref>, due to the discontinuity caused by the absolute value function. The next one is a normalized L 2 metric, which is guaranteed to be between 0 and 1. Yet, the next metric is a similarity metric which is given by the following formula, [KCH 95]; sim (I 1 ; <p> solution said to be obtained, <ref> [KCH 95] </ref>, due to the discontinuity caused by the absolute value function. The next one is a normalized L 2 metric, which is guaranteed to be between 0 and 1. Yet, the next metric is a similarity metric which is given by the following formula, [KCH 95]; sim (I 1 ; I 2 ) = [ R 2 1 where function P I i is the probability density function of the image I i . The forth one is a normalized version of this similarity metric. <p> In CANDID, visualization techniques are provided to help the user to identify which part of the resulting images contributed to the match so that the user can refine its query and get better results. CANDID has been used to retrieve multi-spectral satellite data, <ref> [KCH 95] </ref>. For multi-spectral satellite data experiments, a database containing 100 512 x 512 images were created. In the experiments, images from four different geographic locations were used. <p> From these experiments it is observed that if the images have a lot of common parts, then the resulting similarity measure will be close to each other for all images, whereas most similar ones still stay at 11 the top of the list. Furthermore, in <ref> [KCH 95] </ref> a set of experiments have been conducted with medical imagery, namely pulmonary CT data to evaluate the performance of the six different distance/similarity measures. In these experiments, a database containing 220 lung images with 512 x 512 size were used.
Reference: [LA 96] <author> M. La Cascia and E. Ardizzone, "JACOB: </author> <title> Just a content-based query system for video databases", </title> <address> ICASSP-96, </address> <month> May 7-10, </month> <year> 1996, </year> <institution> Atlanta, Georgia (USA) </institution>
Reference-contexts: There is only one system, namely SCORE [ATY 95, ATY 96], as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk [SC 96], CONIVAS [AD 96], JACOB <ref> [ALDV 96, LA 96] </ref>, CANDID [KCH 95] and Photobook [PPS 93]. Finally, as an example for attribute description feature extraction systems Chabot [OS 95] system is examined. <p> This approach is mainly inspired by image processing researchers. The systems; CONIVAS [AD 96], Photobook [PPS 93], CANDID [KCH 95], JACOB <ref> [ALDV 96, LA 96] </ref>, VisualSEEk [SC 96] and QBIC [FSN+ 95] are examples of this approach. However, automated feature extraction and object recognition tasks are computationally intensive, difficult and expensive operations. <p> The interface supports features to facilitate the formation of supported query types. The system is unique to provide video clips as examples in queries. However, in [AD 96], no performance issues are addressed. 2.2.4 JACOB JACOB, <ref> [LA 96] </ref>, is a content-based retrieval system focusing on video databases. The system automatically splits videos into shots, and extracts a few representative frames from each shot. In order to extract shots from videos shot cut detection is performed. <p> Although this method is not able to detect shot boundaries in presence of editing effects such as fade and dissolve [ALDV 96], it is claimed to work fine on a large variety of videos <ref> [LA 96] </ref>. Moreover, for each r-frame a descriptor based on color and texture features is computed during database population which is a completely automatic task requiring no human intervention. Both direct queries and queries by example are supported in JACOB. <p> Both direct queries and queries by example are supported in JACOB. However, no graphical user query tools are provided to aid in formulation of direct queries. Furthermore, the retrieval engine of <ref> [LA 96] </ref>, is based on sequential scan and a vector distance measure is used in matching. In order to determine r-frames of a shot, a simple method is used, [LA 96]. For each shot shorter than one second a specific frame, namely the middle frame, is chosen. <p> However, no graphical user query tools are provided to aid in formulation of direct queries. Furthermore, the retrieval engine of <ref> [LA 96] </ref>, is based on sequential scan and a vector distance measure is used in matching. In order to determine r-frames of a shot, a simple method is used, [LA 96]. For each shot shorter than one second a specific frame, namely the middle frame, is chosen. And for shots longer than one second, a frame is selected for each one second portion of the shot. <p> Edge density, which is the ratio between pixels whose intensity gradient is over a fixed threshold and the whole number of pixels in the image, is used as a texture feature. For each r-frame, four edge density values are computed, <ref> [LA 96] </ref>. In [LA 96], a few sample queries are reported to validate their approach. The experiments were run on their WWW demo database which contained about 1000 r-frames acquired from TV. In [LA 96], it is reported that using color and texture properties is a good starting point. <p> Edge density, which is the ratio between pixels whose intensity gradient is over a fixed threshold and the whole number of pixels in the image, is used as a texture feature. For each r-frame, four edge density values are computed, <ref> [LA 96] </ref>. In [LA 96], a few sample queries are reported to validate their approach. The experiments were run on their WWW demo database which contained about 1000 r-frames acquired from TV. In [LA 96], it is reported that using color and texture properties is a good starting point. <p> For each r-frame, four edge density values are computed, <ref> [LA 96] </ref>. In [LA 96], a few sample queries are reported to validate their approach. The experiments were run on their WWW demo database which contained about 1000 r-frames acquired from TV. In [LA 96], it is reported that using color and texture properties is a good starting point. Actually, a general and practical systems requires more than just color and texture features. While in [LA 96] each r-frame is characterized by only color and texture features, in a follow-on research, [ALDV 96], r-frames <p> In <ref> [LA 96] </ref>, it is reported that using color and texture properties is a good starting point. Actually, a general and practical systems requires more than just color and texture features. While in [LA 96] each r-frame is characterized by only color and texture features, in a follow-on research, [ALDV 96], r-frames are also characterized by global motion analysis. In [ALDV 96], an indexing methodology using both global features (color, texture and motion), and local features (shape) is also proposed.
Reference: [OS 95] <author> V. Ogle and M. Stonebraker, "Chabot: </author> <title> Retrieval from a Relational Database of Images", </title> <booktitle> IEEE Computer, </booktitle> <month> September </month> <year> 1995 </year>
Reference-contexts: On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk [SC 96], CONIVAS [AD 96], JACOB [ALDV 96, LA 96], CANDID [KCH 95] and Photobook [PPS 93]. Finally, as an example for attribute description feature extraction systems Chabot <ref> [OS 95] </ref> system is examined. <p> Thus, the systems in this category combines the properties of the other two approaches. Moreover, the systems in this category do not provide any similarity measure, rather, they rely on exact matches. The Chabot system, <ref> [OS 95] </ref>, is one example such that it is based on text annotations, but it also provides retrieval of images based on their color content. 2.3.1 Chabot Chabot is an example to systems where both meta-data and extracted image features are stored with the image and images are retrieved based on <p> The goal of the Chabot system is to integrate image analysis techniques into the retrieval system so that image requests do not depend solely on stored textual information, <ref> [OS 95] </ref>. Chabot project, moreover, aims at providing a system that can store and search diverse data types using the features of an advanced DBMS such as a high-level query language, query optimization and flexible indexing. <p> On one hand, the use of a predefined function within the framework of an object relational DBMS makes some query optimization possible. On the other hand, the correctness of concept queries depends on the user making the concept definition. Furthermore, the system may dismiss relevant images. In <ref> [OS 95] </ref>, a performance study based on precision (the percentage of retrieved materials that are relevant to the search) and recall (percentage of relevant material that is retrieved) measures is conducted. <p> The results show that a fair amount of experimentation is necessary to discover the right combination of color content and keywords needed to compose a successful query. Moreover, it is stated that the efficacy of the concepts defined depends somewhat on the familiarity with the image collection <ref> [OS 95] </ref>. In addition, they have found that precision and recall are inversely proportional. The best retrieval results are obtained when text-based search are combined with content-based criteria and a coarse granularity is used for content-analysis [OS 95]. <p> efficacy of the concepts defined depends somewhat on the familiarity with the image collection <ref> [OS 95] </ref>. In addition, they have found that precision and recall are inversely proportional. The best retrieval results are obtained when text-based search are combined with content-based criteria and a coarse granularity is used for content-analysis [OS 95]. The need for a coarse granularity arises from the use of limited color samples (20) only. When finer granularity is used more relevant images are missed. Within the light of these discussions, Chabot system is far from being a general content-based retrieval system.
Reference: [PPS 93] <author> A. Pentland, R. W. Picard and S. Sclaroff, "Photobook: </author> <title> Content-Based Manipulation of Image Databases", MIT Media Laboratory Perceptual Computing Technical report, </title> <type> No 255, </type> <month> Nov </month> <year> 1993 </year>
Reference-contexts: Note that the features extracted at database population step determines the search criteria at querying time. For example, if the system extracts solely color information, then it can only answer queries involving color features. Only one of the systems discussed in this paper, namely Photobook <ref> [PPS 93] </ref>, addresses this problem and tries to circumvent predetermined search criteria. Now that video has become commonplace in today's systems, some content-based retrieval systems also consider video data. For video, there is a common pattern of processing at database population time. <p> There is only one system, namely SCORE [ATY 95, ATY 96], as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk [SC 96], CONIVAS [AD 96], JACOB [ALDV 96, LA 96], CANDID [KCH 95] and Photobook <ref> [PPS 93] </ref>. Finally, as an example for attribute description feature extraction systems Chabot [OS 95] system is examined. <p> However, effective content-based retrieval is believed to be based on visual image contents rather than textual descriptions. 2.2 Feature Extraction Systems This type of systems employ semi-automated or automated feature-extraction/object-recognition methods. This approach is mainly inspired by image processing researchers. The systems; CONIVAS [AD 96], Photobook <ref> [PPS 93] </ref>, CANDID [KCH 95], JACOB [ALDV 96, LA 96], VisualSEEk [SC 96] and QBIC [FSN+ 95] are examples of this approach. However, automated feature extraction and object recognition tasks are computationally intensive, difficult and expensive operations. <p> Based on these facts and the high computation costs of probability density functions, it is fair to say that CANDID is not a practical system and it will not scale well. 2.2.6 Photobook Photobook system, <ref> [PPS 93] </ref>, which is a system developed at MIT Media Lab, attempts to circumvent the issue of predetermined search criteria in content-based retrieval systems by storing enough information about each image to make runtime computations possible. <p> Image search is based on 2-D matching instead of 3-D matching because it is computationally less expensive than 3-D matching and 2-D matching can be directly trained from image data, <ref> [PPS 93] </ref>. Photobook, [PPS 93], can have many different types of descriptions available to it. In particular, in [PPS 93], three types of Photobook is identified; first is Appearance Photobook in which search is based on appearance, second one is Shape Photobook which uses 2-D shape and the third one is <p> Image search is based on 2-D matching instead of 3-D matching because it is computationally less expensive than 3-D matching and 2-D matching can be directly trained from image data, <ref> [PPS 93] </ref>. Photobook, [PPS 93], can have many different types of descriptions available to it. In particular, in [PPS 93], three types of Photobook is identified; first is Appearance Photobook in which search is based on appearance, second one is Shape Photobook which uses 2-D shape and the third one is Texture Photobook where <p> Image search is based on 2-D matching instead of 3-D matching because it is computationally less expensive than 3-D matching and 2-D matching can be directly trained from image data, <ref> [PPS 93] </ref>. Photobook, [PPS 93], can have many different types of descriptions available to it. In particular, in [PPS 93], three types of Photobook is identified; first is Appearance Photobook in which search is based on appearance, second one is Shape Photobook which uses 2-D shape and the third one is Texture Photobook where search is based on textual properties. <p> Moreover, each of the descriptions used in these three Photobook systems can be made rotation and scale invariant. Appearance Photobook uses Karhunen-Loeve transform method to extract information about images, <ref> [PPS 93] </ref>. This transform uses the eigenvectors of the covariance matrix of the set of image features, that is it uses principal components of the distribution of image components. Only a few eigenvectors with the largest eigenvalues are 12 employed to code salient features. <p> Only a few eigenvectors with the largest eigenvalues are 12 employed to code salient features. This technique has been applied to databases of face images and video rep-resentative frames in <ref> [PPS 93] </ref>. The approach is view-based, and thus is not invariant to rotations and orientation. In Shape Photobook, the physical interconnectedness of the shape is modeled by employing finite element methods which produces a stiffness matrix, [PPS 93]. <p> technique has been applied to databases of face images and video rep-resentative frames in <ref> [PPS 93] </ref>. The approach is view-based, and thus is not invariant to rotations and orientation. In Shape Photobook, the physical interconnectedness of the shape is modeled by employing finite element methods which produces a stiffness matrix, [PPS 93]. The stiffness matrix plays the same role in Shape Photobook that covariance matrix does in Appearance Photobook. The eigenvectors of the stiffness matrix is calculated and used in encoding deformations relative to some base or average shape. <p> Finally, in Texture Photobook, a model based on Wold decomposition is used, in which each image is decomposed into three components; a harmonic field, a generalized evanescent field and a purely indeterministic field, <ref> [PPS 93] </ref>. These components refer to periodicity, directionality and randomness, respectively. In Photobook system, [PPS 93], the search is performed by comparing the coefficients of the compressed images with the query image via sequential scan. Although this search is claimed to be efficient, [PPS 93], since compressions are compact, there is <p> Finally, in Texture Photobook, a model based on Wold decomposition is used, in which each image is decomposed into three components; a harmonic field, a generalized evanescent field and a purely indeterministic field, <ref> [PPS 93] </ref>. These components refer to periodicity, directionality and randomness, respectively. In Photobook system, [PPS 93], the search is performed by comparing the coefficients of the compressed images with the query image via sequential scan. Although this search is claimed to be efficient, [PPS 93], since compressions are compact, there is definitely a need for indexing techniques for Photobook to be a practical system storing <p> evanescent field and a purely indeterministic field, <ref> [PPS 93] </ref>. These components refer to periodicity, directionality and randomness, respectively. In Photobook system, [PPS 93], the search is performed by comparing the coefficients of the compressed images with the query image via sequential scan. Although this search is claimed to be efficient, [PPS 93], since compressions are compact, there is definitely a need for indexing techniques for Photobook to be a practical system storing millions of images. Although the system, tries to eliminate predetermined search criteria, it introduces a classification problem.
Reference: [SC 96] <author> J. R. Smith and S-F Chang, "VisualSEEk: </author> <title> a fully automated content-based image query system", </title> <booktitle> ACM Multimedia'96, </booktitle> <address> Boston MA, USA, </address> <pages> pages 87-98, </pages> <year> 1996 </year> <month> 17 </month>
Reference-contexts: There is only one system, namely SCORE [ATY 95, ATY 96], as a representative of attribute description systems. On the other hand, feature extraction systems category contains various systems, including QBIC [FSN+ 95], Vi-sualSEEk <ref> [SC 96] </ref>, CONIVAS [AD 96], JACOB [ALDV 96, LA 96], CANDID [KCH 95] and Photobook [PPS 93]. Finally, as an example for attribute description feature extraction systems Chabot [OS 95] system is examined. <p> This approach is mainly inspired by image processing researchers. The systems; CONIVAS [AD 96], Photobook [PPS 93], CANDID [KCH 95], JACOB [ALDV 96, LA 96], VisualSEEk <ref> [SC 96] </ref> and QBIC [FSN+ 95] are examples of this approach. However, automated feature extraction and object recognition tasks are computationally intensive, difficult and expensive operations. <p> However, it still requires a domain expert to specify which features to extract. Yet, it is the most successful content-based retrieval system which has been successfully commercialized. 2.2.2 VisualSEEk In VisualSEEk project <ref> [SC 96] </ref>, the focus is on providing support for a variety of joint color/spatial queries. VisualSEEk provides a graphical query tool to formulate queries and supports querying by image regions and spatial layout. Furthermore, it has a client/server architecture, supporting WWW access, where the client software is Java-based. <p> VisualSEEk provides a graphical query tool to formulate queries and supports querying by image regions and spatial layout. Furthermore, it has a client/server architecture, supporting WWW access, where the client software is Java-based. The unique features of VisualSEEk <ref> [SC 96] </ref> are summarized as; * joint content-based (color) and spatial querying * automated region extraction * direct indexing of color features The system, [SC 96], automatically extracts and indexes salient color regions from the images. The queries are formulated by diagramming spatial arrangements of color regions. <p> Furthermore, it has a client/server architecture, supporting WWW access, where the client software is Java-based. The unique features of VisualSEEk <ref> [SC 96] </ref> are summarized as; * joint content-based (color) and spatial querying * automated region extraction * direct indexing of color features The system, [SC 96], automatically extracts and indexes salient color regions from the images. The queries are formulated by diagramming spatial arrangements of color regions. Then, the system finds images that contain the most similar arrangements of similar regions. <p> Each image is decomposed into regions which have feature properties, such as color and spatial properties such as size, location and relationships to other regions, <ref> [SC 96] </ref>. Images are compared by comparing their regions. <p> On the other hand, with use of color sets some color information maybe lost. Basically three types of queries can be specified; color set queries, single region queries and multiple region queries. The color set query compares the color content of regions or images. In <ref> [SC 96] </ref>, it is stated that this query strategy both enables efficient indexing and provides for no false dismissals although the final candidate list may contain false alarms. For single region queries, properties of regions are specified in terms of their color, size and absolute location, [SC 96]. <p> In <ref> [SC 96] </ref>, it is stated that this query strategy both enables efficient indexing and provides for no false dismissals although the final candidate list may contain false alarms. For single region queries, properties of regions are specified in terms of their color, size and absolute location, [SC 96]. In order to accelerate the matching process, indexing of region centroids with spatial quad-trees and minimum bounding rectangles with R-trees are supported. The minimum bounding rectangle of a region is the smallest vertically aligned rectangle that completely encloses the region. <p> rectangle where the spatial distance between regions is calculated by the Euclidean distance of centroids and the distance in area between two regions is given by the absolute distance function. 8 For multiple region queries, there exist three possibilities; by absolute location, by relative location and by special spatial relationships, <ref> [SC 96] </ref>. To get the final answer to a multiple region query, queries on individual regions are joined. The joining process is computed by intersecting the results of single region queries, identifying candidate images that contain matches to all query regions. <p> By pruning the search space in the first phase, the cost of the second phase is reduced. In order to support relative spatial constraints, 2-D string representation of spatial relationships is employed. In VisualSEEk, <ref> [SC 96] </ref>, region adjacency, nearness, overlap, and surround spatial relationships are implemented. The 2-D string representation of VisualSEEk provides scale invariance but not rotation invariance. In oder to circumvent this, some approximations are employed. In [SC 96], a set of experiments have been performed to see the effectiveness of VisualSEEk. <p> In VisualSEEk, <ref> [SC 96] </ref>, region adjacency, nearness, overlap, and surround spatial relationships are implemented. The 2-D string representation of VisualSEEk provides scale invariance but not rotation invariance. In oder to circumvent this, some approximations are employed. In [SC 96], a set of experiments have been performed to see the effectiveness of VisualSEEk. In the experiments, 500 synthetic images are used. It has been found that some information is lost in the feature extraction process through color set back projection. <p> In the experiments, 500 synthetic images are used. It has been found that some information is lost in the feature extraction process through color set back projection. Furthermore, color/spatial indexing techniques also caused drop in retrieval effectiveness. As a result, it has been stated that, <ref> [SC 96] </ref>, in the indexing strategy of VisualSEEk, candidate target images are required to possess regions which are all sufficiently close to the query regions. If this is not true, the system may dismiss some matching images.
References-found: 11

