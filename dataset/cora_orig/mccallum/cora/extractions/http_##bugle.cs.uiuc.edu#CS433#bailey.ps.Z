URL: http://bugle.cs.uiuc.edu/CS433/bailey.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/CS433.html
Root-URL: http://www.cs.uiuc.edu
Author: David H. Bailey 
Address: Moffett Field, CA 94035.  
Affiliation: (NAS) Systems Division at NASA Ames Research Center,  
Note: The author is with the Numerical Aerodynamic Simulation  
Date: December 1, 1992  
Pubnum: RNR Technical Report RNR-92-005  
Abstract: Misleading Performance Reporting in the Supercomputing Field Abstract In a previous humorous note, I outlined twelve ways in which performance figures for scientific supercomputers can be distorted. In this paper, the problem of potentially misleading performance reporting is discussed in detail. Included are some examples that have appeared in recent published scientific papers. This paper also includes some proposed guidelines for reporting performance, the adoption of which would raise the level of professionalism and reduce the level of confusion in the field of supercomputing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov and D. Sorenson, </author> <title> The LAPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: Further, Strassen's algorithm can be employed to accelerate a variety of linear algebra calculations [3, 9] by substituting a Strassen-based matrix multiply routine for the conventional matrix multiply routine in a LAPACK <ref> [1] </ref> implementation. If a Strassen-based flop count were adopted for computing the megaflops rate in the solution of a 16; 000 fi 16; 000 linear system, the resulting rate would have to be cut by roughly one-third from the usual reckoning.
Reference: [2] <author> D. H. Bailey, </author> <title> "Extra-High Speed Matrix Multiplication on the Cray-2", </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> vol. 9, no. 3, </volume> <month> (May </month> <year> 1988), </year> <editor> p. </editor> <volume> 603 - 607. </volume>
Reference-contexts: Along this line, perhaps those of us performing research in the area of numerical linear algebra should at some point reconsider the usage of classical formulas for flop counts in favor of flop counts based on implementations that employ Strassen's algorithm <ref> [2] </ref>. Strassen's algorithm is a scheme to multiply matrices that requires fewer floating point operations than the conventional scheme. It has been demonstrated that Strassen's algorithm is now practical and in fact produces real speedups for matrices with dimensions larger than about 128 [2]. <p> based on implementations that employ Strassen's algorithm <ref> [2] </ref>. Strassen's algorithm is a scheme to multiply matrices that requires fewer floating point operations than the conventional scheme. It has been demonstrated that Strassen's algorithm is now practical and in fact produces real speedups for matrices with dimensions larger than about 128 [2]. Further, Strassen's algorithm can be employed to accelerate a variety of linear algebra calculations [3, 9] by substituting a Strassen-based matrix multiply routine for the conventional matrix multiply routine in a LAPACK [1] implementation.
Reference: [3] <author> D. H. Bailey, K. Lee, and H. D. Simon, </author> <title> "Using Strassen's Algorithm to Accelerate the Solution of Linear Systems", </title> <journal> Journal of Supercomputing, </journal> <volume> vol. 4., no. </volume> <month> 4 (Jan. </month> <year> 1991), </year> <editor> p. </editor> <volume> 357 - 371. </volume>
Reference-contexts: It has been demonstrated that Strassen's algorithm is now practical and in fact produces real speedups for matrices with dimensions larger than about 128 [2]. Further, Strassen's algorithm can be employed to accelerate a variety of linear algebra calculations <ref> [3, 9] </ref> by substituting a Strassen-based matrix multiply routine for the conventional matrix multiply routine in a LAPACK [1] implementation. <p> In this vein, I myself must confess to citing potentially misleading performance figures in <ref> [3] </ref>. These articles include one processor Cray-2 and Y-MP performance rates for some Strassen matrix routines. Following established custom, my co-authors and I computed megaflops rates based on the classical flop count for matrix multiplication (2n 3 ).
Reference: [4] <author> D. H. Bailey and P. O. Frederickson, </author> <title> "Performance Results for Two of the NAS Parallel Benchmarks", </title> <booktitle> Proceedings of Supercomputing '91, IEEE, </booktitle> <address> Los Alamitos, CA, </address> <year> 1991, </year> <editor> p. </editor> <volume> 166 - 163. </volume>
Reference-contexts: Also, it is known that on the CM-2, "CM Busy Time" and "CM Elapsed Time" are quite different for some codes, even with no I/O and no other users sharing the partition. This can be seen in <ref> [4, 6] </ref>.
Reference: [5] <author> D. H. Bailey, </author> <title> "Twelve Ways to Fool the Masses When Giving Performance Results on Parallel Computers", </title> <booktitle> Supercomputing Review, </booktitle> <month> August </month> <year> 1991, </year> <note> p. 54 - 55. Also published in Supercomputer, </note> <month> September </month> <year> 1991, </year> <editor> p. </editor> <volume> 4 - 7. </volume>
Reference-contexts: 1. Introduction Many readers may have read my previous article "Twelve Ways to Fool the Masses When Giving Performance Reports on Parallel Computers" <ref> [5] </ref>. The attention that this article received frankly has been surprising [11]. Evidently it has struck a responsive chord among many professionals in the field who share my concerns. The following is a very brief summary of the "Twelve Ways": 1.
Reference: [6] <author> M. Garbey and D. Levine, </author> <title> "Massively Parallel Computation of Conservation Laws", </title> <booktitle> Proceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, PA 1990, p. 340 - 345. </address>
Reference-contexts: Also, it is known that on the CM-2, "CM Busy Time" and "CM Elapsed Time" are quite different for some codes, even with no I/O and no other users sharing the partition. This can be seen in <ref> [4, 6] </ref>.
Reference: [7] <author> J. Gustafson, G. R. Montry and R. E. Benner, </author> <title> "Development of Parallel Methods for a 1024-Processor Hypercube", </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> vol. 9, no. </volume> <month> 4 (July </month> <year> 1988), </year> <editor> p. </editor> <volume> 609 - 638. </volume>
Reference-contexts: Clearly speedup figures should be based instead on the timing of a well optimized, purely single processor program (i.e. a program without unnecessary multiprocessor constructs). Some authors present "scaled speedup" figures, first introduced in <ref> [7] </ref>, where the problem size is scaled up with the number of processors. Such figures may be informative, but 13 it is essential that authors who quote such figures clearly disclose the fact that they have scaled their problem size to match the processor count.
Reference: [8] <author> J. Gustafson, D. Rover, S. Elbert and M. Carter, "SLALOM: Surviving Adolescence", Supercomputing Review, </author> <month> December </month> <year> 1991, </year> <editor> p. </editor> <volume> 54 - 61. </volume>
Reference-contexts: Thus while the parallel megaflops figures may provide some useful information, it is clear that the BPSA megaflops figures are more meaningful when comparing computational performance. Another article that emphasizes this same point is <ref> [8] </ref>, where the authors of the Slalom benchmark describe Bjorstad and Boman's discovery.
Reference: [9] <author> N. J. Higham, </author> <title> "Exploiting Fast Matrix Multiplication Within the Level 3 BLAS", </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. </volume> <month> 16 </month> <year> (1990), </year> <editor> p. </editor> <volume> 352 - 368. </volume>
Reference-contexts: It has been demonstrated that Strassen's algorithm is now practical and in fact produces real speedups for matrices with dimensions larger than about 128 [2]. Further, Strassen's algorithm can be employed to accelerate a variety of linear algebra calculations <ref> [3, 9] </ref> by substituting a Strassen-based matrix multiply routine for the conventional matrix multiply routine in a LAPACK [1] implementation.
Reference: [10] <author> O. M. Lubeck, M. L. Simmons and H. J. Wasserman, </author> <title> "The Performance Realities of Massively Parallel Processors: A Case Study", Proceedings of Supercomputing '92, </title> <note> to appear. </note>
Reference-contexts: A code that has been implemented efficiently on a CM-2 should be directly translatable to a efficient vectorizable code on the Cray. An excellent demonstration of this principle can be found in <ref> [10] </ref>, where three applications that had been programmed and tuned on the CM-2 were ported back to the Cray Y-MP with high performance.
Reference: [11] <author> J. </author> <title> Markoff, "Measuring How Fast Computers Really Are", </title> <address> New York Times, Septem-ber 22, </address> <year> 1991, </year> <note> p. 14F. </note>
Reference-contexts: 1. Introduction Many readers may have read my previous article "Twelve Ways to Fool the Masses When Giving Performance Reports on Parallel Computers" [5]. The attention that this article received frankly has been surprising <ref> [11] </ref>. Evidently it has struck a responsive chord among many professionals in the field who share my concerns. The following is a very brief summary of the "Twelve Ways": 1. Quote only 32-bit performance results, not 64-bit results, and compare your 32-bit results with others' 64-bit results. 2.
Reference: [12] <author> J. N. Shadid and R. S. Tuminaro, </author> <title> "Iterative Methods for Nonsymmetric Systems on MIMD Machines", </title> <booktitle> Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992, </year> <note> to appear. 18 </note>
Reference-contexts: The first paper contains an interesting comparison of several different numerical schemes that can be used to solve a convection-diffusion problem <ref> [12] </ref>. Based on the authors' data, I have computed both parallel megaflops rates and BPSA megaflops rates for four of these schemes. These figures are shown in Table 2. The BPSA megaflops rates are based on the flop count of the multigrid algorithm.
Reference: [13] <institution> Paper A. </institution>
Reference-contexts: Further, plots are in many cases all that high-level managers (such as those with authority for computer acquisitions) have time to digest. Unfortunately, plots can also mislead an audience, especially if prepared carelessly or if presented without important qualifying information. defense application <ref> [13] </ref>. The plot compares timings of the authors' nCUBE/10 code with timings of a comparable code running on a Cray X-MP/416. The plot appears to indicate an impressive performance advantage for the nCUBE system on all problem sizes except a small region at the far left.
Reference: [14] <institution> Paper B. </institution>
Reference-contexts: Thus it appears that the authors were entirely professional in the text of their paper. But the reader is left to wonder what fraction of the audience that has seen this plot fully appreciates the details behind it. dynamics application <ref> [14] </ref>. This plot compares timings of the author's codes running on a 64K CM-2 with those of comparable codes running on a one processor Cray X-MP. The two curves shown for each computer system represent a structured and an unstructured grid version of the code, respectively.
Reference: [15] <institution> Paper C. </institution>
Reference-contexts: One example of a performance comparison of this sort is in <ref> [15] </ref>. In this paper, the performance of a code fragment is listed as 18 megaflops on a Cray-2, but the translated code is claimed to run at 741 megaflops on the CM-2.
Reference: [16] <institution> Paper D. </institution>
Reference-contexts: It is regrettable that such material appeared in a published conference proceedings. Fortunately, however, the above-mentioned code fragment and performance comparison was omitted when the paper was subsequently republished as a journal article. Another apparent example of this type of potentially misleading material can be seen in <ref> [16] </ref>. This paper compares the performance of a physics application running on several systems, including a 16K CM-2 and a Cray X-MP/14. <p> An excellent demonstration of this principle can be found in [10], where three applications that had been programmed and tuned on the CM-2 were ported back to the Cray Y-MP with high performance. Thus the reader of <ref> [16] </ref> is left to wonder why, if the CM-2 code for this application runs so well, did the author not try to adapt the CM-2 code to the X-MP? Since one of the author's CM-2 codes was written entirely in CM Fortran (i.e.
Reference: [17] <institution> Paper E. </institution>
Reference-contexts: Nonetheless, one is left to wonder about how reliable these comparisons are, and whether they will always be quoted with the appropriate disclaimer. In most cases authors clearly disclose estimates and projections, but not always. In <ref> [17] </ref>, the author gives performance results for his fluid dynamics code in a table at the end of the article. Timings are included for an 8K CM-2, a 16K CM-2 and a 64K CM-2.
Reference: [18] <institution> Paper F. </institution>
Reference-contexts: However, by noting that 10 this column of numbers is identical to the 16K numbers, shifted down by one, one has to conclude that the 64K numbers are merely linear projections from the 16K results. Some authors have taken the practice of citing projections one step further. In <ref> [18] </ref> the author states in his abstract that his code runs "at the speed of a single processor of a Cray-2 on 1/4 of a CM-2".
Reference: [19] <institution> Paper G. </institution>
Reference-contexts: It seems that many scientists using parallel computers are willing to assume as an established fact one of the most fundamental questions in the field! We have already seen one instance of citing extrapolated results. Another example is <ref> [19] </ref>, where the authors compare their defense application running on an nCUBE-2 with comparable codes running on a Cray Y-MP and a CM-2. Three tables of timings are included in this paper. Fortunately, all of the nCUBE-2 timings in the three tables are real timings.
Reference: [20] <institution> Paper H. </institution>
Reference-contexts: For example, users of the Intel iPSC and other message passing systems often base speedup figures on a single node timing of the multiple node version of the program (for example, in <ref> [20] </ref>). When running on a single node, the multiple node program needlessly synchronizes with itself and passes messages to itself. These "messages" are handled quite rapidly, since the operating system recognizes that these are local transmissions.
Reference: [21] <author> Paper I. </author> <month> 19 </month>
Reference-contexts: I suspect that in the majority of cases where the authors do not clearly state the data type, the results are indeed for 32-bit data. One example of this is <ref> [21] </ref>, where in an otherwise excellent ten-page paper, the authors never state whether their impressive performance rates are for 32-bit or 64-bit calculations, at least not in any place where a reader would normally look for such information.
References-found: 21

