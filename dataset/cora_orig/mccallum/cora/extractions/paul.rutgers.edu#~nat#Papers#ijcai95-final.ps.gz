URL: paul.rutgers.edu/~nat/Papers/ijcai95-final.ps.gz
Refering-URL: 
Root-URL: 
Email: nat@paul.rutgers.edu  fmyers,gluckg@pavlov.rutgers.edu  
Title: A Novelty Detection Approach to Classification  
Author: Nathalie Japkowicz Catherine Myers and Mark Gluck 
Address: New Brunswick, New Jersey 08903  Newark, New Jersey 07102  
Affiliation: Department of Computer Science Rutgers University  Aidekman Research Center Rutgers University  
Abstract: Novelty Detection techniques are concept-learning methods that proceed by recognizing positive instances of a concept rather than differentiating between its positive and negative instances. Novelty Detection approaches consequently require very few, if any, negative training instances. This paper presents a particular Novelty Detection approach to classification that uses a Redundancy Compression and Non-Redundancy Differentiation technique based on the (Gluck & Myers 1993) model of the hippocampus, a part of the brain critically involved in learning and memory. In particular, this approach consists of training an autoencoder to reconstruct positive input instances at the output layer and then using this au-toencoder to recognize novel instances. Classification is possible, after training, because positive instances are expected to be reconstructed accurately while negative instances are not. The purpose of this paper is to compare Hippo, the system that implements this technique, to C4.5 and feedforward neural network classification on several applications. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Gluck, M. A., and Myers, C. E. </author> <year> 1993. </year> <title> Hippocam-pal mediation of stimulus representation: </title> <journal> A computational theory. </journal> <volume> Hippocampus 3(4) </volume> <pages> 491-516. </pages>
Reference-contexts: Identifying positive and negative instances of a concept is therefore equivalent to assessing how well such instances are reconstructed by the autoencoder. These same processes have previously been proposed to model computations occurring in the hippocampus <ref> (Gluck & Myers 1993) </ref>, a part of the brain involved in learning and memory. <p> The last part is an overview of the overall functioning of the system. Redundancy Compression and Non-Redundancy Differentiation Redundancy Compression and Non-Redundancy Differentiation is a process believed to occur in the hip-pocampus <ref> (Gluck & Myers 1993) </ref>. Gluck and Myers have used connectionist models to study the function of the hippocampal region in the brain and its implications for learning and memory behaviors. <p> The system they built based on these considerations was able to accurately predict a range of classical conditioning behaviors observed in normal and hippocampal-damaged animals <ref> (Gluck & Myers 1993) </ref>. Because these compression and differentiation constraints appear to be useful during learning in the brain, Gluck and Myers suggest that they may be useful for machine learning tasks as well.
Reference: <author> Gorman, R., and Sejnowski, T. </author> <year> 1988. </year> <title> Analysis of hidden units in a layered network trained to classify sonar targets. </title> <booktitle> Neural Networks 1 </booktitle> <pages> 75-89. </pages>
Reference-contexts: Since Hippo learns a concept from positive data only, the negative data was eliminated from the 1 This explains why the results reported in section 3.3 are different from those reported in previous experiments on this data, such as <ref> (Gorman & Sejnowski 1988) </ref> 2 For use with FF Classification, the input of the threshold-determination component was taken to be the value of the output node and its output has to be reversed since positive instances are supposed to return a larger signal than negative ones. concept-learning training set of this
Reference: <author> Hinton, G. </author> <year> 1989. </year> <title> Connectionist learning procedures. </title> <booktitle> Artificial Intelligence 40 </booktitle> <pages> 185-234. </pages>
Reference-contexts: The particular Novelty Detection technique introduced in this paper uses an autoencoder <ref> (Hinton 1989) </ref>. An autoencoder is a neural network which learns to map from its inputs, through a narrow hidden layer, to output nodes which attempt to reconstruct the input. <p> Because these compression and differentiation constraints appear to be useful during learning in the brain, Gluck and Myers suggest that they may be useful for machine learning tasks as well. The Redundancy Compression and Non-Redundancy Differentiation technique uses an au-toencoder, of the type that was proposed by <ref> (Hinton 1989) </ref>. An autoencoder is an artificial neural network composed of a given number, I, of input nodes; the same number of output nodes, O; and a given number, H, of hidden nodes with H &lt; I = O.
Reference: <author> Hirsh, H., and Noordewier, M. </author> <year> 1994. </year> <title> Using background knowledge to improve learning of DNA sequences. </title> <booktitle> In Proceedings of the Tenth IEEE Conference on Artificial Intelligence for Applications, </booktitle> <pages> 351-357. </pages>
Reference: <author> Kolesar, B., and NRaD. </author> <year> 1994. </year> <title> Helicopter gearbox monitoring. In NIPS-94 Workshop on Novelty Detection and Adaptive System Monitoring (presented). </title>
Reference: <author> Kortge, C. A. </author> <year> 1990. </year> <title> Episodic memory in connectionist networks. </title> <booktitle> In The Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 764-771. </pages>
Reference-contexts: Figure 1 presents an autoencoder with I = O = 6 and H = 3. In the past, autoencoders have been used for estimating learning algorithms reliability (Pomerleau 1993) and for solving the catastrophic inference problem <ref> (Kortge 1990) </ref>. We now discuss how autoencoders can also be used for Novelty Detection. In order to be used for Novelty Detection, the au-toencoder is trained on positive instances of the concept, using backpropagation.
Reference: <author> Norton, S. W. </author> <year> 1994. </year> <title> Learning to recognize promoter sequences in E. coli by modeling uncertainty in the training data. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle>
Reference-contexts: The promoter data was obtained from the U.C. Irvine Repository of Machine Learning and was modified in response to Norton's critique of the biological flaws underlying the original formulation of the data <ref> (Norton 1994) </ref>. In addition, as is usual for this problem when run on a connectionist system, each example was converted into a 204-bit long vector where each nucleotide was represented with 4 bits.
Reference: <author> Petsche, T.; Hanson, S.; and Gluck, M. </author> <year> 1994. </year> <title> Workshop on novelty detection and adaptive system monitoring. </title> <booktitle> In Neural Information Processing Systems. </booktitle> <volume> Volume 7. </volume>
Reference-contexts: As an alternative to learning a concept using a broad range of positive and negative training examples, some concept-learning techniques that require mainly|or only|positive training examples have recently been introduced <ref> (Petsche, Hanson, & Gluck 1994) </ref>. Such techniques are grouped as Novelty Detection methods (the term stems for the fact that negative inputs are recognized as being novel compared to positive inputs which are more familiar as they belong to the class that was used for training).
Reference: <author> Pomerleau, D. A. </author> <year> 1993. </year> <title> Input reconstruction reliability estimation. </title> <booktitle> In Proceedings of the Fifth Neural Information Processing Systems Conference. </booktitle>
Reference-contexts: Figure 1 presents an autoencoder with I = O = 6 and H = 3. In the past, autoencoders have been used for estimating learning algorithms reliability <ref> (Pomerleau 1993) </ref> and for solving the catastrophic inference problem (Kortge 1990). We now discuss how autoencoders can also be used for Novelty Detection. In order to be used for Novelty Detection, the au-toencoder is trained on positive instances of the concept, using backpropagation.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Good performance often depends on constructing training sets which contain a broad range of positive and negative examples. Many classification methods have been designed under these conditions, including C4.5 <ref> (Quinlan 1993) </ref> and backpropagation applied to a feedforward neu fl This paper appears on pp. 518-523 of the Proceedings of the Fourteenth International Conference on Artificial Intelligence held on August 20-25, 1995 in Montreal (Canada). y Center for Molecular and Behavioral Neuroscience ral network (FF Classification) (Rumelhart, Hinton, & Williams 1986). <p> Figure 3 illustrates the functioning of the overall concept learner. Experiments Hippo was tested in three domains: CH46 helicopter gearbox fault detection, molecular biology promoter recognition, and sonar target classification. Its results are compared to those of two standard approaches to classification: C4.5, a decision tree learning system <ref> (Quinlan 1993) </ref> and FF Classification, another connectionist learning method (Rumelhart, Hinton, & Williams 1986). We begin by introducing the three domains considered and the methodology used to evaluate the three approaches. We then discuss the results of the experiments.
Reference: <author> Rumelhart, D. E.; Hinton, G. E.; and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., eds., </editor> <booktitle> Parallel Distributed Processing. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <pages> 318-364. </pages>
Reference-contexts: these conditions, including C4.5 (Quinlan 1993) and backpropagation applied to a feedforward neu fl This paper appears on pp. 518-523 of the Proceedings of the Fourteenth International Conference on Artificial Intelligence held on August 20-25, 1995 in Montreal (Canada). y Center for Molecular and Behavioral Neuroscience ral network (FF Classification) <ref> (Rumelhart, Hinton, & Williams 1986) </ref>. As an alternative to learning a concept using a broad range of positive and negative training examples, some concept-learning techniques that require mainly|or only|positive training examples have recently been introduced (Petsche, Hanson, & Gluck 1994). <p> This type of network learns to reproduce its inputs at the output layer, using a multilayer learning algorithm such as backpropagation <ref> (Rumelhart, Hinton, & Williams 1986) </ref>, the learning paradigm used in this work. Figure 1 presents an autoencoder with I = O = 6 and H = 3. In the past, autoencoders have been used for estimating learning algorithms reliability (Pomerleau 1993) and for solving the catastrophic inference problem (Kortge 1990). <p> Experiments Hippo was tested in three domains: CH46 helicopter gearbox fault detection, molecular biology promoter recognition, and sonar target classification. Its results are compared to those of two standard approaches to classification: C4.5, a decision tree learning system (Quinlan 1993) and FF Classification, another connectionist learning method <ref> (Rumelhart, Hinton, & Williams 1986) </ref>. We begin by introducing the three domains considered and the methodology used to evaluate the three approaches. We then discuss the results of the experiments. The Case Studies The CH46 Helicopter Gearbox data was obtained from NRaD.(Kolesar & NRaD 1994).
Reference: <author> Weiss, S. M., and Kulikowski, C. A. </author> <year> 1991. </year> <title> Computer Systems That Learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: For both the promoter and the sonar target recognition problems, the noisy method of section 2.2 was selected and applied to 5 positive and 5 negative instances. In the three domains considered, the three systems were evaluated using 5-fold crossvalidation <ref> (Weiss & Kulikowski 1991) </ref>. At every fold of every experiment, the training set used by C4.5 was divided into a training set for concept-learning and one for threshold-determination for Hippo and FF Classification.
References-found: 12

