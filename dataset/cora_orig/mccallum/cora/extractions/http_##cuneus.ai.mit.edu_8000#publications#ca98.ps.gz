URL: http://cuneus.ai.mit.edu:8000/publications/ca98.ps.gz
Refering-URL: http://cuneus.ai.mit.edu:8000/personal/publications.html
Root-URL: 
Email: tonebone@ai.mit.edu tp-temp@ai.mit.edu  
Title: MikeTalk: A Talking Facial Display Based on Morphing Visemes  
Author: Tony Ezzat Tomaso Poggio 
Affiliation: MIT Center for Biological and Computational Learning  
Date: June 1998  
Address: Philadelphia, Pennsylvania,  45 Carleton St. E25-204 Cambridge, MA 02141  
Note: Appears in Proceedings of the Computer Animation Conference,  
Abstract: We present MikeTalk, a text-to-audiovisual speech synthesizer which converts input text into an audiovisual speech stream. MikeTalk is built using visemes, which are a set of images spanning a large range of mouth shapes. The visemes are acquired from a recorded visual corpus of a human subject which is specifically designed to elicit one instantiation of each viseme. Using optical flow methods, correspondence from every viseme to every other viseme is computed automatically. By morphing along this correspondence, a smooth transition between viseme images may be generated. A complete visual utterance is constructed by concatenating viseme transitions. Finally, phoneme and timing information extracted from a text-to-speech synthesizer is exploited to determine which viseme transitions to use, and the rate at which the morphing process should occur. In this manner, we are able to synchronize the visual speech stream with the audio speech stream, and hence give the impression of a photorealis-tic talking face. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Beier and S. Neely. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In SIGGRAPH '92 Proceedings, </booktitle> <pages> pages 35-42, </pages> <address> Chicago, IL, </address> <year> 1992. </year>
Reference-contexts: Consequently, a mechanism of transitioning from each viseme image to every other viseme image is needed, and this transition must be smooth and realistic. In this work, a morphing technique was adopted to create this transition. 3.1. Background Morphing was first popularized by Beier & Neely <ref> [1] </ref> in the context of generating transitions between different faces for Michael Jackson's Black or White music video. <p> In addition, the process of specifying the feature regions usually requires hand-coding a large number of ad-hoc geometric primitives such as line segments, cornerpoints, arcs, circles, and meshes. Beier & Neely <ref> [1] </ref> in fact, make the explicit statement that the specification of the correspondence between images constitutes the most time-consuming aspect of the morph. As a result, we have resorted to optical flow methods to alleviate these problems. 3.2.
Reference: [2] <author> J. Bergen and R. Hingorani. </author> <title> Hierarchical motion-based frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center, Princeton, </institution> <address> New Jersey, </address> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: In general, determining optical flow is a highly under-constrained problem, and an additional set of assumptions about the underlying motion need to be made. In the particular case of the optical flow algorithm used in this work (Bergen & Hingorani <ref> [2] </ref>), one such assumption made is that the motion between images is small. This small motion assumption is extremely detrimental, however, because in many cases it prevents the optical flow algorithm from computing correct correspondence between viseme images that exhibit large differences in motion between each other. <p> The blending process thus allows the two warps to be effectively combined, and the "new" pixels of the second viseme to become involved in the viseme transition itself. It should be noted that both Beymer, Shashua, & Poggio [3] and Bergen & Hingorani <ref> [2] </ref> noticed the viability of morphing along optical flow vectors as a means of creating realistic transitions between two images. Since the second warp in a morph is a warp of viseme B towards viseme A, an inverse flow from B to viseme A needs to be computed.
Reference: [3] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1431, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: At the other end of the facial modeling spectrum are a group of image-based approaches where the face is modeled using images alone. One such early approach was that of Beymer, Shashua, and Poggio <ref> [3] </ref>, who utilized a morphing technique to synthesize novel, intermediate images of a face from example endpoints. In doing so, their algorithm was capable of modeling rigid facial transformations such as pose changes, as well as non-rigid transformations such as smiles. <p> A new audiovisual sentence is constructed by concatenating the appropriate triphone sequences from the database together. The approach described in this work falls into the image-based, morphing category, and is close in spirit to the work of <ref> [3] </ref> and [18]. In the following sections, we describe the various aspects of our approach in detail. 2. <p> The transformations between images occur as a warp of the first image into the second, a similar inverse warp of the second image into the first, and a final cross-dissolve or blend of the warped images. It should be noted that both Beymer, Shashua, Poggio <ref> [3] </ref> and Scott, Kagels, et al. [18] noticed the viability of using diphthong visemes, and the silence viseme. morphing as a method of transitioning between various facial pose, expression, and mouth position imagery. <p> The blending process thus allows the two warps to be effectively combined, and the "new" pixels of the second viseme to become involved in the viseme transition itself. It should be noted that both Beymer, Shashua, & Poggio <ref> [3] </ref> and Bergen & Hingorani [2] noticed the viability of morphing along optical flow vectors as a means of creating realistic transitions between two images. <p> Since the second warp in a morph is a warp of viseme B towards viseme A, an inverse flow from B to viseme A needs to be computed. In this work, the inverse flow is computed using an algorithm that was first described in Beymer, Shashua, and Poggio <ref> [3] </ref>. Figure 5c depicts the set of images generated as a result of warping along the inverse flow from viseme B to viseme A. A final morph sequence is shown in Figure 5d. The blending parameter ff is interpolated linearly between 0:0 and 1:0. 3.5.
Reference: [4] <author> A. Black and P. Taylor. </author> <title> The Festival Speech Synthesis System. </title> <institution> University of Edinburgh, </institution> <year> 1997. </year>
Reference-contexts: The transition between viseme transitions is smooth because the nuhn viseme image is the same image in both viseme transitions. 4. Synthesizing the New Audiovisual Sentence We have incorporated the Festival TTS system (Black & Taylor <ref> [4] </ref>), developed at the University of Edinburgh, into our work. A voice in the Festival system consists of a set of recorded diphones, which are stored as LPC coefficients and corresponding residuals (Hunt, Zwierzynski, et al. [11]). <p> Since there are 16 visemes in our final viseme set, a total of 256 optical flow vectors are computed. Synthesizing the New Audiovisual Sentence: Finally, we utilize a text-to-speech system (Black & Taylor <ref> [4] </ref>) to convert unconstrained input text into a string of phonemes, along with duration information for each phoneme. Using this information, we determine the appropriate sequence of viseme transitions to make, as well as the rate of the transformations.
Reference: [5] <author> C. Bregler, M. Covell, and M. Slaney. </author> <title> Video rewrite: Driving visual speech with audio. </title> <booktitle> In SIGGRAPH '97 Proceedings, </booktitle> <address> Los Angeles, CA, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: To animate the face, a morphing algorithm is developed which is capable of transitioning between the various mouth shapes in a smooth and realistic manner. Most recently, Bregler, Covell, el al. <ref> [5] </ref> also described an image-based approach to facial modeling: in their work, a set of short audiovisual sequences are extracted from a larger audiovisual corpus. Each one of these short sequences is a triphone segment, and a large database with all the acquired triphones is built.
Reference: [6] <author> S. E. Chen and L. Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In SIGGRAPH '93 Proceedings, </booktitle> <pages> pages 279-288, </pages> <address> Anaheim, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Another reason is that local image expansion involved in the underlying motion of the lips causes the optical flow vectors themselves to diverge. To remedy this, a hole-filling algorithm first proposed by Chen & Williams <ref> [6] </ref> was adopted. The algorithm pre-fills the destination images with a special reserved background color. After performing the forward warp, the hole-filling algorithm traverses the destination image in rasterized order and fills in the holes by interpolating linearly between their non-hole endpoints.
Reference: [7] <author> M. M. Cohen and D. W. Massaro. </author> <title> Modeling coarticu-lation in synthetic visual speech. </title> <editor> In N. M. Thalmann and D. Thalmann, editors, </editor> <booktitle> Models and Techniques in Computer Animation, </booktitle> <pages> pages 139-156. </pages> <publisher> Springer-Verlag, </publisher> <address> Tokyo, </address> <year> 1993. </year>
Reference-contexts: One approach is to model the face using traditional 3D modeling methods. Parke [17] was one of the earliest to adopt such an approach by creating a polygonal facial model. Recent work on TTVS systems that is based on Parke's models include the work of Cohen & Massaro <ref> [7] </ref> and LeGoff & Benoit [13]. Extending this 3D modeling approach, Terzopoulos & Waters [19] built a facial model which also includes muscle and bone structures. To improve the photore-alism of the facial model, Lee, Terzopoulos, et al. [12] resorted to Cyberware scanning. <p> In the former case, the ntn viseme assumes a rounded shape in anticipation of the upcoming nuun sound, while the latter assumes a more spread shape in anticipation of the upcoming niin sound. The reader is referred to Cohen and Massaro <ref> [7] </ref> for an in-depth discussion on the theories behind coarticulation, and to Owens and Blasek [16] for a study on consonantal perception in various vocalic contexts. At the present stage of our work, we have decided for the sake of simplicity to ignore coarticulation effects.
Reference: [8] <author> T. Ezzat. </author> <title> Example-based analysis and synthesis for images of human faces. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: Consequently, direct application of our optical flow algorithm only succeeds when the motion between any two viseme images is small. However, we have found that a flow concatenation procedure (Ezzat <ref> [8] </ref>) overcomes the problems which occur when the small motion assumption fails to apply. Since the original visual corpus is digitized at 30 fps, there are many intermediate frames that lie between the chosen viseme images. <p> Consequently, we compute a series of consecutive optical flow vectors between each intermediate image and its predecessor, and then concatenate them all into one large flow vector that defines the global transformation between the chosen visemes. Further details of the flow concatenation procedure itself may be found in Ezzat <ref> [8] </ref>. 3.3.
Reference: [9] <author> C. G. Fisher. </author> <title> Confusions among visually perceived consonants. </title> <journal> Jour. Speech and Hearing Research, </journal> <volume> 11 </volume> <pages> 796-804, </pages> <year> 1968. </year>
Reference-contexts: Corpus and Viseme Data Acquiry The basic underlying assumption of our facial synthesis approach is that the complete set of mouth shapes associated with human speech may be reasonably spanned by a finite set of visemes. The term viseme itself was coined initially by Fisher <ref> [9] </ref> as an amalgamation of the words "visual" and "phoneme". To date, there has been no precise definition for the term, but in general it has come to refer to a speech segment that is visually contrastive from another.
Reference: [10] <author> B. K. P. Horn and B. G. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: As a result, we have resorted to optical flow methods to alleviate these problems. 3.2. Optical Flow Optical flow was originally formulated by Horn & Schunck <ref> [10] </ref> in the context of measuring the motion of objects in images. This motion is captured as a two-dimensional vector field fd x ; d y g that describes how each pixel has moved between the viseme images.
Reference: [11] <author> M. J. Hunt, D. A. Zwierzynski, and R. Carr. </author> <title> Issues in high quality lpc analysis and synthesis. </title> <booktitle> In Proceedings of EUROSPEECH, </booktitle> <volume> volume 2, </volume> <pages> pages 348-351, </pages> <address> Paris, France, </address> <year> 1989. </year>
Reference-contexts: A voice in the Festival system consists of a set of recorded diphones, which are stored as LPC coefficients and corresponding residuals (Hunt, Zwierzynski, et al. <ref> [11] </ref>). It is interesting to note that the final audio speech stream is constructed by concatenating the appropriate diphones together, in a manner that is completely analogous to our method for concatenating viseme transitions.
Reference: [12] <author> Y. Lee, D. Terzopoulos, and K. Waters. </author> <title> Realistic modeling for facial animation. </title> <booktitle> In SIGGRAPH '95 Proceedings, </booktitle> <pages> pages 55-62, </pages> <address> Los Angeles, CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Extending this 3D modeling approach, Terzopoulos & Waters [19] built a facial model which also includes muscle and bone structures. To improve the photore-alism of the facial model, Lee, Terzopoulos, et al. <ref> [12] </ref> resorted to Cyberware scanning. The Cyberware scanner produces accurate 3D structure and texture maps of the scanned face, and these maps are animated by overlaying them on top of the muscle-based models.
Reference: [13] <author> B. LeGoff and C. Benoit. </author> <title> A text-to-audiovisual-speech synthesizer for french. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing (ICSLP), </booktitle> <address> Philadelphia, USA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Parke [17] was one of the earliest to adopt such an approach by creating a polygonal facial model. Recent work on TTVS systems that is based on Parke's models include the work of Cohen & Massaro [7] and LeGoff & Benoit <ref> [13] </ref>. Extending this 3D modeling approach, Terzopoulos & Waters [19] built a facial model which also includes muscle and bone structures. To improve the photore-alism of the facial model, Lee, Terzopoulos, et al. [12] resorted to Cyberware scanning.
Reference: [14] <author> A. Montgomery and P. Jackson. </author> <title> Physical characteristics of the lips underlying vowel lipreading performance. </title> <journal> Jour. Acoustical Society of America, </journal> <volume> 73(6) </volume> <pages> 2134-2144, </pages> <year> 1983. </year>
Reference-contexts: This difference, however, does not manifest itself visually, and hence the two phonemes should be placed in the same visemic category. The reader is referred to Owens and Blasek [16] for a discussion of the consonantal visemic categories, and to Montgomery and Jackson <ref> [14] </ref> for a discussion of the vocalic visemic categories. Conversely, the literature points out that the map from phonemes to visemes is also one-to-many: the same phoneme can have many different visual forms. <p> This was done in a subjective manner, by comparing the viseme images visually to assess their similarity. The authors were also guided in this process by the conclusions in Owens and Blasek [16] for the case of consonantal visemes, and in Montgomery and Jackson <ref> [14] </ref> for the case of vocalic visemes. This grouping step is, in effect, a decision to use a many-to-one mapping strategy instead of a one-to-one mapping strategy. The final reduced set of visemes are shown in Figures 3 and 4. There were 6 final visemes representing the 24 consonantal phonemes.
Reference: [15] <author> J. Olive, A. Greenwood, and J. Coleman. </author> <title> Acoustics of American English Speech: A Dynamic Approach. </title> <publisher> Springer-Verlag, </publisher> <address> New York, USA, </address> <year> 1993. </year>
Reference-contexts: At the present stage of our work, we have decided for the sake of simplicity to ignore coarticulation effects. Consequently, the recorded corpus, which is shown in to visemes, and hence one word is uttered for every phoneme. The example words uttered are obtained from Olive, Greenwood, et al. <ref> [15] </ref>, and are generally categorized into example words which instantiate consonantal, monophthong vocalic, and diphthong vocalic phonemes. the left of each example word is the phonemic transcription label being used. After the whole corpus is recorded and digitized, one lip image is extracted as an instance of that viseme.
Reference: [16] <author> E. Owens and B. Blazek. </author> <title> Visemes observed by hearing-impaired and normal-hearing adult viewers. </title> <journal> Jour. Speech and Hearing Research, </journal> <volume> 28 </volume> <pages> 381-393, </pages> <month> September </month> <year> 1985. </year>
Reference-contexts: This difference, however, does not manifest itself visually, and hence the two phonemes should be placed in the same visemic category. The reader is referred to Owens and Blasek <ref> [16] </ref> for a discussion of the consonantal visemic categories, and to Montgomery and Jackson [14] for a discussion of the vocalic visemic categories. Conversely, the literature points out that the map from phonemes to visemes is also one-to-many: the same phoneme can have many different visual forms. <p> The reader is referred to Cohen and Massaro [7] for an in-depth discussion on the theories behind coarticulation, and to Owens and Blasek <ref> [16] </ref> for a study on consonantal perception in various vocalic contexts. At the present stage of our work, we have decided for the sake of simplicity to ignore coarticulation effects. Consequently, the recorded corpus, which is shown in to visemes, and hence one word is uttered for every phoneme. <p> This was done in a subjective manner, by comparing the viseme images visually to assess their similarity. The authors were also guided in this process by the conclusions in Owens and Blasek <ref> [16] </ref> for the case of consonantal visemes, and in Montgomery and Jackson [14] for the case of vocalic visemes. This grouping step is, in effect, a decision to use a many-to-one mapping strategy instead of a one-to-one mapping strategy.
Reference: [17] <author> F. I. Parke. </author> <title> A parametric model of human faces. </title> <type> PhD thesis, </type> <institution> University of Utah, </institution> <year> 1974. </year>
Reference-contexts: The main research issue underlying the construction of a TTVS visual stream is the nature of the facial model to use. One approach is to model the face using traditional 3D modeling methods. Parke <ref> [17] </ref> was one of the earliest to adopt such an approach by creating a polygonal facial model. Recent work on TTVS systems that is based on Parke's models include the work of Cohen & Massaro [7] and LeGoff & Benoit [13].
Reference: [18] <author> K. Scott, D. Kagels, S. Watson, H. Rom, J. Wright, M. Lee, and K. Hussey. </author> <title> Synthesis of speaker facial movement to match selected speech sequences. </title> <booktitle> In Proceedings of the Fifth Australian Conference on Speech Science and Technology, </booktitle> <volume> volume 2, </volume> <pages> pages 620-625, </pages> <month> De-cember </month> <year> 1994. </year>
Reference-contexts: In doing so, their algorithm was capable of modeling rigid facial transformations such as pose changes, as well as non-rigid transformations such as smiles. In a similar vein, Scott, Kagels, et al. <ref> [18] </ref> also employed an image-based morphing method in their work. Their facial model is composed of a set of images which capture a large range of the mouth shapes occurring during speech. <p> A new audiovisual sentence is constructed by concatenating the appropriate triphone sequences from the database together. The approach described in this work falls into the image-based, morphing category, and is close in spirit to the work of [3] and <ref> [18] </ref>. In the following sections, we describe the various aspects of our approach in detail. 2. <p> It should be noted that both Beymer, Shashua, Poggio [3] and Scott, Kagels, et al. <ref> [18] </ref> noticed the viability of using diphthong visemes, and the silence viseme. morphing as a method of transitioning between various facial pose, expression, and mouth position imagery.
Reference: [19] <author> D. Terzopoulos and K. Waters. </author> <title> Analysis and synthesis of facial image sequences using physical and anatomical models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(6) </volume> <pages> 569-579, </pages> <year> 1993. </year>
Reference-contexts: Recent work on TTVS systems that is based on Parke's models include the work of Cohen & Massaro [7] and LeGoff & Benoit [13]. Extending this 3D modeling approach, Terzopoulos & Waters <ref> [19] </ref> built a facial model which also includes muscle and bone structures. To improve the photore-alism of the facial model, Lee, Terzopoulos, et al. [12] resorted to Cyberware scanning.
References-found: 19

