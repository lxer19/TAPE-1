URL: file://ftp.cs.unc.edu/pub/projects/proteus/reports/ipps92.ps.gz
Refering-URL: http://www.cs.unc.edu/Research/proteus/proteus-publications.html
Root-URL: http://www.cs.unc.edu
Title: Prototyping N-body Simulation in Proteus  
Author: Peter H. Mills Lars S. Nyland Jan F. Prins John H. Reif 
Note: In Proceedings of the Sixth International Parallel Processing Symposium, (Beverly Hills, Ca., March 23-26), pp. 476-482, IEEE, 1992. Abstract  
Address: Durham, N.C. 27706 Chapel Hill, N.C. 27599-3175 USA  
Affiliation: Department of Computer Science, Department of Computer Science, Duke University, University of North Carolina,  
Abstract: This paper explores the use of Proteus, an architecture-independent language suitable for prototyp-ing parallel and distributed programs. Proteus is a high-level imperative notation based on sets and sequences with a single construct for the parallel composition of processes communicating through shared memory. Several different parallel algorithms for N-body simulation are presented in Proteus, illustrating how Proteus provides a common foundation for expressing the various parallel programming models. This common foundation allows prototype parallel programs to be tested and evolved without the use of machine-specific languages. To transform prototypes to implementations on specific architectures, program refinement techniques are utilized. Refinement strategies are illustrated that target broad-spectrum parallel intermediate languages, and their viability is demonstrated by refining an N-body algorithm to data-parallel CVL code. 
Abstract-found: 1
Intro-found: 1
Reference: [Agh90] <author> G. Agha, </author> <title> "Concurrent object-oriented programming," </title> <journal> Comm. ACM, </journal> <volume> vol. 33, </volume> <pages> pp. 125-141, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Ongoing work in the area of the Proteus language design is concentrated in several areas. First, we are are investigating the inclusion of higher-level features for distributed programming, using the notion of concurrent objects as the basis of an approach to controlling process parallelism <ref> [Agh90] </ref>. Second, we are investigating the modeling of time-constrained com putation in the form of annotations for the relative execution rates of processes. Finally, we are currently involved in the implementation of key features of the language and refinement system to assess the suitability of the approach.
Reference: [App85] <author> A. W. Appel, </author> <title> "An efficient program for many-body simulation," </title> <journal> Siam J. Sci. Stat. Comput., </journal> <volume> vol. 6, </volume> <pages> pp. 85-103, </pages> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: The N-body problem characterizes physical phenomena that arise in many important applications in fields such as astrophysics, plasma physics, and molecular dynamics <ref> [App85, Gre90] </ref>. While numerical solutions for the N-body problem are thus critically needed, they unfortunately require large amounts of computation due to the typically large number of particles and nature of the N 2 pairwise interaction. Many algorithmic refinements have been proposed to render N-body simulation more tractable. <p> Many algorithmic refinements have been proposed to render N-body simulation more tractable. These include methods which approximate interaction of a particle with a cluster of particles that are far away by modeling the cluster as a single particle (so-called far-field interactions) <ref> [App85] </ref>, and tree-code methods which compute far-field interactions by recursively decomposing the spatial domain [BH86]. A further optimization is obtained by the Fast Multipole Method [Gre90], which uses multigrid techniques and multi-pole approximations for far clusters to yield a faster and more accurate algorithm.
Reference: [BC90] <author> G. Blelloch and S. Chatterjee, </author> <title> "VCODE: a data-parallel intermediate language," </title> <booktitle> in Proc. Frontiers 90, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: For example, we intend initially to reduce data-parallelism to the set of parallel vector operations provided by the CVL library [BCSZ90], developed by Guy Blel-loch and colleagues at Carnegie-Mellon as a machine-independent library used in the interpretation of the data-parallel intermediate code VCODE <ref> [BC90] </ref>. Likewise, we intend to reduce process parallelism to the set of procedures provided with the threads facility of Mach [BRS + 85]. 3.3. Refinement to SIMD We now apply these strategies to our N-body program to yield execution on an SIMD architecture.
Reference: [BCSZ90] <author> G. Blelloch, S. Chatterjee, J. Sipelstein, and M. Zahga, "CVL: </author> <title> A C vector library," </title> <type> Draft Technical Note, </type> <institution> Carnegie Mellon University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: For example, we intend initially to reduce data-parallelism to the set of parallel vector operations provided by the CVL library <ref> [BCSZ90] </ref>, developed by Guy Blel-loch and colleagues at Carnegie-Mellon as a machine-independent library used in the interpretation of the data-parallel intermediate code VCODE [BC90]. Likewise, we intend to reduce process parallelism to the set of procedures provided with the threads facility of Mach [BRS + 85]. 3.3.
Reference: [BG90] <author> L. Blaine and A. Goldberg, </author> <title> "Modules and types for a common prototyping language," </title> <type> Technical Report, </type> <institution> Kestrel Institute, Palo Alto, California, </institution> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: The KIDS system (Kestrel Interactive Development System) [Smi90] has been used to develop programs from specifications, and includes a number of algorithm design tactics and data refinement transformations <ref> [BG90] </ref>. Providing refinement techniques to target many specific architectures is likely to be prohibitive, hence our strategy is to refine to existing or proposed intermediate languages which permit reasonably efficient execution on a class of parallel platforms.
Reference: [BH86] <author> J. Barnes and P. Hut, </author> <title> "A hierarchical O(N log N) force-calculation algorithm," </title> <journal> Nature, </journal> <volume> vol. 324, </volume> <pages> pp. 446-449, </pages> <year> 1986. </year>
Reference-contexts: These include methods which approximate interaction of a particle with a cluster of particles that are far away by modeling the cluster as a single particle (so-called far-field interactions) [App85], and tree-code methods which compute far-field interactions by recursively decomposing the spatial domain <ref> [BH86] </ref>. A further optimization is obtained by the Fast Multipole Method [Gre90], which uses multigrid techniques and multi-pole approximations for far clusters to yield a faster and more accurate algorithm. <p> Note that, although the entire cluster array CL is declared as private, Proteus implementations need only copy on demand referenced boundary elements. The same technique of state isolation can be applied to parallelize further optimizations of the N-body simulation, specifically for Barnes-Hut tree-codes <ref> [BH86] </ref> and the Fast Multipole Method [Gre90]. Both employ a hierarchical decomposition of cluster space, such as quad-trees for 2D or oct-trees for 3D, which can be used to recursively partition areas of otherwise near-body interaction so as to treat them as far-body effects.
Reference: [BRS + 85] <author> R. Baron, R. Rashid, E. Siegel, A. Tevanian, and M. Young, "Mach-1: </author> <title> An operating environment for large-scale multiprocessor applications," </title> <journal> IEEE Software, </journal> <month> July </month> <year> 1985. </year>
Reference-contexts: Languages for these asynchronous distributed-state systems include CSP [Hoa85] and Strand [FT90]. Shared-memory multiprocessors, like the Multimax or the Sequent, are typically programmed using languages that support shared variables with access-exclusion and synchronization mechanisms like monitors, such as found in Concurrent Pascal, or threads such as found in Mach <ref> [BRS + 85] </ref>. Highly-parallel processors such as the TMC CM-2 or the MasPar MP-1 are programmed using data-parallel operations and barrier synchronization. <p> Likewise, we intend to reduce process parallelism to the set of procedures provided with the threads facility of Mach <ref> [BRS + 85] </ref>. 3.3. Refinement to SIMD We now apply these strategies to our N-body program to yield execution on an SIMD architecture. <p> To execute this prototype, the control-parallelism of the group of parallel cluster processes can be straightforwardly imple mented, if needed, in terms of threads (for example in Mach <ref> [BRS + 85] </ref>). A key point is that each cluster process can be vectorized since it uses the same techniques as the simple Proteus program for pairwise interaction.
Reference: [BS90] <author> G. Blelloch and G. Sabot, </author> <title> "Compiling collection-oriented languages into massively parallel computers," </title> <journal> Journal of Par. and Distr. Computing, </journal> <volume> vol. 8, </volume> <pages> pp. 119-134, </pages> <year> 1990. </year>
Reference-contexts: In order to target SIMD execution, we must refine the program to separate the reduction operation from the nested sequence generators, and combine the nested sequence generators into one. We do this by following techniques outlined in <ref> [BS90] </ref>, yielding a form of the program that can be translated to vector operations. In this case we performed the refinement and translation manually, but based on insights gained from this experiment we are developing tools in the Refine system to perform these steps semi-automatically. <p> Transform elwise (f ,...) operations into applications of the vector extension of f derived by converting scalar operations in f to vector operations. This corresponds to compiling Paralation "elwise" forms into vector operations, a topic ad dressed in detail in <ref> [BS90] </ref>. 3. Transform segmented reduction (using g) into vector operations, either by deriving from the scalar operations of g a sequence of segmented reductions, or by using the vector extension of g to implement the reduction using well-known techniques such as doubling.
Reference: [CGL86] <author> N. Carriero, D. Gelernter, and J. Leichter, </author> <title> "Distributed data structures in Linda," </title> <booktitle> in Proc. 13th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pp. 236-242, </pages> <publisher> ACM, </publisher> <year> 1986. </year>
Reference-contexts: Related work A variety of parallel languages are cited as being useful for programming broad classes of concurrent systems. These languages might be roughly divided into the following categories. * Languages with widely translatable logical models, such as Linda's distributed data structures <ref> [CGL86] </ref>, the synchronization-variable methods of Strand [FT90] and PCN [CT92], or the data-parallel abstraction of the Paralation model [Sab88]. * Languages which incorporate a large variety of par allel primitives, such as Ease [Zen90]. * Wide-spectrum parallel languages that rely on refinement from architecture-independent specification.
Reference: [Che86] <author> M. Chen, </author> <title> "Very-high-level parallel programming in Crystal," </title> <booktitle> in Proc. 1986 Hypercube Conference, </booktitle> <address> (Knoxville,Tn.), </address> <year> 1986. </year>
Reference-contexts: Notable wide-spectrum parallel language efforts include Crystal <ref> [Che86] </ref> and variants of the Bird-Meertens functional formalism [Ski90]. UNITY [CM88], although not a wide-spectrum notation, is, as its name suggests, a particularly elegant notation for describing a large range of parallel and distributed computations. We see Proteus as falling into the last category.
Reference: [CM88] <author> K. Chandy and J. Misra, </author> <title> Parallel Program Design, A Foundation. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Notable wide-spectrum parallel language efforts include Crystal [Che86] and variants of the Bird-Meertens functional formalism [Ski90]. UNITY <ref> [CM88] </ref>, although not a wide-spectrum notation, is, as its name suggests, a particularly elegant notation for describing a large range of parallel and distributed computations. We see Proteus as falling into the last category.
Reference: [CT92] <author> K. Chandy and S. Taylor, </author> <title> An Introduction to Parallel Programming. </title> <publisher> Jones & Bartlett, </publisher> <year> 1992. </year>
Reference-contexts: These languages might be roughly divided into the following categories. * Languages with widely translatable logical models, such as Linda's distributed data structures [CGL86], the synchronization-variable methods of Strand [FT90] and PCN <ref> [CT92] </ref>, or the data-parallel abstraction of the Paralation model [Sab88]. * Languages which incorporate a large variety of par allel primitives, such as Ease [Zen90]. * Wide-spectrum parallel languages that rely on refinement from architecture-independent specification.
Reference: [CZ89] <author> R. Cole and O. Zajicek, </author> <title> "The APRAM: Incorporating asynchrony into the PRAM model," </title> <booktitle> in Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 169-178, </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: Families of abstract computational models for these classes of synchronous and asynchronous shared-memory machines z This work was supported under DARPA / SISTO contract N00014-91-C-0114 administered through ONR. may be found in the PRAM and APRAM respectively <ref> [CZ89] </ref>. The proliferation of languages following different concurrent programming paradigms targeting different architectures, together with the emergence of heterogeneous systems and mixed-mode architectures, pose problems for the development of parallel software. The diversity of languages and paradigms increases the complexity of programming and reduces software portability, reuse, and reliability.
Reference: [Dij78] <author> E. Dijkstra, </author> <title> "Guarded commands, nondetermi-nacy and the formal derivation of programs," </title> <journal> Comm. ACM, </journal> <volume> vol. 18, </volume> <pages> pp. 453-457, </pages> <year> 1978. </year>
Reference-contexts: Figure 1 summarizes a number of control operators over sequences of statements and the familiar syntax that may be used when all of the statement values are explicit rather than generated. While the guarded command constructs behave similarly to those of Dijkstra and Hoare <ref> [Dij78, Hoa85] </ref> | for example, the rep operator repeatedly executes one command selected arbitrarily from those with true guards until all guards are false | Proteus provides greater expressive power by permitting operands to be dynamically generated by sequence construction. 2.1.
Reference: [F + 88] <author> G. Fox et al., </author> <title> Solving problems on concurrent processors. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: At the end of the simulation step the private clusters are merged: this effects exchange of boundary cells between clusters since they are effectively recopied as privates in the next par cycle. This corresponds to exchanging guard strips in other multigrid simulations <ref> [F + 88] </ref>. Note that, although the entire cluster array CL is declared as private, Proteus implementations need only copy on demand referenced boundary elements.
Reference: [FT90] <author> I. Foster and S. Taylor, Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <year> 1990. </year>
Reference-contexts: For example, distributed systems and distributed-memory multiprocessors such as the Intel iPSC and its descendants are typically programmed using the concepts of processes and message-passing. Languages for these asynchronous distributed-state systems include CSP [Hoa85] and Strand <ref> [FT90] </ref>. Shared-memory multiprocessors, like the Multimax or the Sequent, are typically programmed using languages that support shared variables with access-exclusion and synchronization mechanisms like monitors, such as found in Concurrent Pascal, or threads such as found in Mach [BRS + 85]. <p> Related work A variety of parallel languages are cited as being useful for programming broad classes of concurrent systems. These languages might be roughly divided into the following categories. * Languages with widely translatable logical models, such as Linda's distributed data structures [CGL86], the synchronization-variable methods of Strand <ref> [FT90] </ref> and PCN [CT92], or the data-parallel abstraction of the Paralation model [Sab88]. * Languages which incorporate a large variety of par allel primitives, such as Ease [Zen90]. * Wide-spectrum parallel languages that rely on refinement from architecture-independent specification.
Reference: [GG89] <author> L. Greengard and W. Gropp, </author> <title> "A parallel version of the fast multipole method," in Parallel Processing for Scientific Computing (G. </title> <editor> Ro-drigue, ed.), p. </editor> <volume> 213, </volume> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: A further optimization is obtained by the Fast Multipole Method [Gre90], which uses multigrid techniques and multi-pole approximations for far clusters to yield a faster and more accurate algorithm. To further decrease computational complexity, parallel implementations of these algorithms have also been explored <ref> [GG89] </ref>, in particular on data-parallel architectures such as the Connection Machine [ZJ89]. In following sections we consider two parallel algorithms for N-body simulation. We first present a simple N 2 interaction per step simulation, and refine this algorithm toward a highly-parallel SIMD architecture.
Reference: [Gre90] <author> L. Greengard, </author> <title> "The numerical solution of the N-body problem," </title> <booktitle> Computers in Physics, </booktitle> <pages> pp. 142-152, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: The N-body problem characterizes physical phenomena that arise in many important applications in fields such as astrophysics, plasma physics, and molecular dynamics <ref> [App85, Gre90] </ref>. While numerical solutions for the N-body problem are thus critically needed, they unfortunately require large amounts of computation due to the typically large number of particles and nature of the N 2 pairwise interaction. Many algorithmic refinements have been proposed to render N-body simulation more tractable. <p> A further optimization is obtained by the Fast Multipole Method <ref> [Gre90] </ref>, which uses multigrid techniques and multi-pole approximations for far clusters to yield a faster and more accurate algorithm. To further decrease computational complexity, parallel implementations of these algorithms have also been explored [GG89], in particular on data-parallel architectures such as the Connection Machine [ZJ89]. <p> Note that, although the entire cluster array CL is declared as private, Proteus implementations need only copy on demand referenced boundary elements. The same technique of state isolation can be applied to parallelize further optimizations of the N-body simulation, specifically for Barnes-Hut tree-codes [BH86] and the Fast Multipole Method <ref> [Gre90] </ref>. Both employ a hierarchical decomposition of cluster space, such as quad-trees for 2D or oct-trees for 3D, which can be used to recursively partition areas of otherwise near-body interaction so as to treat them as far-body effects.
Reference: [Hoa85] <author> C. Hoare, </author> <title> Communicating Sequential Processes. </title> <publisher> Addison-Wesley, </publisher> <year> 1985. </year>
Reference-contexts: For example, distributed systems and distributed-memory multiprocessors such as the Intel iPSC and its descendants are typically programmed using the concepts of processes and message-passing. Languages for these asynchronous distributed-state systems include CSP <ref> [Hoa85] </ref> and Strand [FT90]. Shared-memory multiprocessors, like the Multimax or the Sequent, are typically programmed using languages that support shared variables with access-exclusion and synchronization mechanisms like monitors, such as found in Concurrent Pascal, or threads such as found in Mach [BRS + 85]. <p> Figure 1 summarizes a number of control operators over sequences of statements and the familiar syntax that may be used when all of the statement values are explicit rather than generated. While the guarded command constructs behave similarly to those of Dijkstra and Hoare <ref> [Dij78, Hoa85] </ref> | for example, the rep operator repeatedly executes one command selected arbitrarily from those with true guards until all guards are false | Proteus provides greater expressive power by permitting operands to be dynamically generated by sequence construction. 2.1.
Reference: [MNP + 91] <author> P. H. Mills, L. S. Nyland, J. F. Prins, J. H. Reif, and R. W. Wagner, </author> <title> "Prototyping parallel and distributed programs in proteus," </title> <booktitle> in Proc. 3rd IEEE Symp. on Parallel and Distributed Processing, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: In this paper we briefly review the salient features of Proteus, a language we are developing specifically to support the prototyping of parallel and distributed programs. A more detailed description of the language and comparison with other approaches can be found in <ref> [Nyl91, MNP + 91] </ref>. Proteus provides a high-level set-theoretic notation together with a sparse but powerful set of mechanisms for controlling parallel execution. A shared-memory model is the basis for communication between processes: this memory can be partitioned into shared and private variables.
Reference: [Nyl91] <author> L. S. Nyland, </author> <title> The Design of A Prototyping Programming Language for Parallel and Sequential Algorithms. </title> <type> Ph.D. dissertation, </type> <institution> Duke University, </institution> <month> Feb. 3 </month> <year> 1991. </year>
Reference-contexts: In this paper we briefly review the salient features of Proteus, a language we are developing specifically to support the prototyping of parallel and distributed programs. A more detailed description of the language and comparison with other approaches can be found in <ref> [Nyl91, MNP + 91] </ref>. Proteus provides a high-level set-theoretic notation together with a sparse but powerful set of mechanisms for controlling parallel execution. A shared-memory model is the basis for communication between processes: this memory can be partitioned into shared and private variables.
Reference: [Ref88] <institution> Reasoning Systems, Inc., Palo Alto, California, Refine 2.0 Language Summary, </institution> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: We conclude the paper with a discussion of directions of ongoing research. 2. Basic features of Proteus Our language starts with rich data models and operators along the lines of SETL [SDDS86] and REFINE <ref> [Ref88] </ref>, which employ the high-level mathematical notions of sets, sequences, and maps. The core of our language is a conventional imperative notation to the degree that it is assignment-based and block-structured; program state is maintained in typed, lexically-scoped variables, and assignment statements or procedure calls modify this state.
Reference: [Sab88] <author> G. Sabot, </author> <title> The Paralation Model: Architecture-Independent Parallel Programming. </title> <publisher> MIT, </publisher> <year> 1988. </year>
Reference-contexts: Each private variable has its values in all processes combined using a specified merge function f , and the result updates the corresponding variable in the enclosing scope. This combining action is similar to that used to resolve conflict in message collisions <ref> [Sab88] </ref>, although Proteus applies the reduction of f only across the changed values from all processes. <p> These languages might be roughly divided into the following categories. * Languages with widely translatable logical models, such as Linda's distributed data structures [CGL86], the synchronization-variable methods of Strand [FT90] and PCN [CT92], or the data-parallel abstraction of the Paralation model <ref> [Sab88] </ref>. * Languages which incorporate a large variety of par allel primitives, such as Ease [Zen90]. * Wide-spectrum parallel languages that rely on refinement from architecture-independent specification. Notable wide-spectrum parallel language efforts include Crystal [Che86] and variants of the Bird-Meertens functional formalism [Ski90].
Reference: [SDDS86] <author> J. Schwartz, R. Dewar, E. Dubinsky, and E. Schonberg, </author> <title> Programming with Sets, An Introduction to SETL. </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: We conclude the paper with a discussion of directions of ongoing research. 2. Basic features of Proteus Our language starts with rich data models and operators along the lines of SETL <ref> [SDDS86] </ref> and REFINE [Ref88], which employ the high-level mathematical notions of sets, sequences, and maps.
Reference: [Ski90] <author> D. Skillicorn, </author> <title> "Architecture-independent parallel computation," </title> <journal> IEEE Computer, </journal> <volume> vol. 23, </volume> <pages> pp. 38-50, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Notable wide-spectrum parallel language efforts include Crystal [Che86] and variants of the Bird-Meertens functional formalism <ref> [Ski90] </ref>. UNITY [CM88], although not a wide-spectrum notation, is, as its name suggests, a particularly elegant notation for describing a large range of parallel and distributed computations. We see Proteus as falling into the last category. <p> One promising avenue for data type and algorithmic refinement relies on techniques to recognize the presense of CVL patterns such as elwise and product operations, as well as transformational strategies based on algebraic laws for functional languages <ref> [Ski90] </ref>. 4. Far-field approximation We now consider an evolution of the original Pro-teus N-body simulation program that yields a parallel algorithm suitable for targeting asynchronous collections of SIMD processors. We take advantage of the far-field approximation mentioned earlier.
Reference: [Smi90] <author> D. R. Smith, </author> <title> "KIDS a semi-automatic program development system," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 16, </volume> <pages> pp. 1024-1043, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: The KIDS system (Kestrel Interactive Development System) <ref> [Smi90] </ref> has been used to develop programs from specifications, and includes a number of algorithm design tactics and data refinement transformations [BG90].
Reference: [Zen90] <author> S. Zenith, </author> <title> "Programming with Ease: a semiotic definition of the language," </title> <institution> Research Report RR809, Yale University, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: into the following categories. * Languages with widely translatable logical models, such as Linda's distributed data structures [CGL86], the synchronization-variable methods of Strand [FT90] and PCN [CT92], or the data-parallel abstraction of the Paralation model [Sab88]. * Languages which incorporate a large variety of par allel primitives, such as Ease <ref> [Zen90] </ref>. * Wide-spectrum parallel languages that rely on refinement from architecture-independent specification. Notable wide-spectrum parallel language efforts include Crystal [Che86] and variants of the Bird-Meertens functional formalism [Ski90].
Reference: [ZJ89] <author> F. Zhao and S. L. Johnsson, </author> <title> "The Parallel Mul-tipole Method on the Connection Machine," </title> <institution> Research Report CS89-6, Massachusetts Institute of Technology, </institution> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: To further decrease computational complexity, parallel implementations of these algorithms have also been explored [GG89], in particular on data-parallel architectures such as the Connection Machine <ref> [ZJ89] </ref>. In following sections we consider two parallel algorithms for N-body simulation. We first present a simple N 2 interaction per step simulation, and refine this algorithm toward a highly-parallel SIMD architecture. Next a variant of the algorithm is considered that utilizes far-field interactions. <p> We are examining prototypes of parallel Fast Multipole algorithms in Proteus which isolate boundary cell communication while at the lowest level are vectorizable | how these can be efficiently implemented on asynchronous collections of SIMD processors, or can be refined into existing data-parallel Mul-tipole methods <ref> [ZJ89] </ref> is still being investigated. 5. Summary and future work In this paper we explored the use of Proteus, a pro-totyping language whose constructs for parallelism can serve as a foundation for expressing many concurrent programming models.
References-found: 28

