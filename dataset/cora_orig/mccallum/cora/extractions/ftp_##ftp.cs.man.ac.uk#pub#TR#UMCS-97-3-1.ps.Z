URL: ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-97-3-1.ps.Z
Refering-URL: http://www.cs.man.ac.uk/cstechrep/Abstracts/UMCS-97-3-1.html
Root-URL: http://www.cs.man.ac.uk
Title: Secondary Cache Data Prefetching for Multiprocessors  
Author: Ando Ki Alan E. Knowles 
Affiliation: Computer Science University of Manchester  
Pubnum: ISSN 1361 6161  
Abstract: Department of Computer Science University of Manchester Technical Report Series UMCS-97-3-1 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baer, J.-L., and Wang, W.-H. </author> <title> Architectural choices for multilevel cache hierarchies. </title> <booktitle> In International Conference on Parallel Processing (Aug. </booktitle> <year> 1987), </year> <pages> pp. 258-261. </pages>
Reference-contexts: The secondary cache is a write-back, direct-mapped data cache with write-allocate on write misses. The size of primary cache is smaller than that of secondary cache, and we use a mechanism to guarantee the content multi-level inclusion property <ref> [1] </ref>. The global shared memory modules are organised as low-order interleaved memory, so that multiple memory accesses can be handled concurrently. Data is distributed subpage-wise (subpage size is the secondary cache line size) in a round-robin fashion. Each memory module can service a request at a every memory cycle.
Reference: [2] <author> Chen, T.-F. </author> <title> Data Prefetching for High-Performance Processors. </title> <type> Ph.D. thesis, </type> <institution> University of Wash-ington, 1993. Department of Computer Science and Engineering. </institution>
Reference-contexts: The stride is used to determine the possible next access by adding the stride and the current operand address. Methods in the literature <ref> [6, 17, 2] </ref> can be classified as stride prefetch methods. Conventional high-performance computers usually use a two-level cache hierarchy which consists of a fast but small primary on-chip cache and a larger secondary off-chip/on-board cache.
Reference: [3] <author> Dahlgren, F., Dubois, M., and Stenstr om, P. </author> <title> Sequential hardware prefetching in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 6, </journal> <month> 7 (July </month> <year> 1995), </year> <pages> 733-746. </pages>
Reference-contexts: In the sequential prefetch method, access to a cache line causes prefetching for successive line or lines. If there is sufficient spatial locality, this approach can work well. Methods in the literature <ref> [18, 9, 3] </ref> can be classified as sequential prefetch methods. In a stride prefetch method, the instruction and operand addresses of each load are stored in a hardware lookup table, and the instruction address enables the system to identify the access stream which belongs to the same instruction.
Reference: [4] <author> Dahlgren, F., and Stenstr om, P. </author> <title> Evaluation of hardware-based stride and sequential prefetch-ing in shared-memory multiprocessors. </title> <booktitle> In International Conference on High-Performance Computer Architecture (Jan. </booktitle> <year> 1995), </year> <pages> pp. 68-77. </pages>
Reference-contexts: The hardware requirements, that include more than three hardware list tables and complex comparators, are relatively high. The results presented by Hagersten were incomplete because he studied his scheme under the unit stride without stride detection. Dahlgren and Stenstrom evaluated stride and sequential prefetching in <ref> [4] </ref>. In their study, they compared the performance of two stride detection mechanisms. One was instruction address driven and the other was operand address driven. The latter was same as that proposed by Hagersten [8]. <p> The partitions enable the separation of accesses in different streams and to detect strides from accessing in the same partition. They studied prefetching for stream buffer and uniprocessor system. The ROT method, proposed in [8] and studied in <ref> [4] </ref>, needs complex hardware that includes complex hardware list tables and comparators. In contrast, our method only needs a simple hardware table.
Reference: [5] <author> Fraser, C. W., and Hanson, D. R. </author> <title> A Retargetable C Compiler: Design and Implementation. </title> <publisher> Benjamin Cummings, </publisher> <year> 1995. </year>
Reference-contexts: The prefetch operation used in this work has following properties [15]: non-binding, non-blocking property, and non-excepting property. 2 The compiler is implemented using the Princeton University Lcc <ref> [5] </ref>. The compiler transforms a C-language source program to an assembler language output which is a simple non-pipelined RISC like "load/store" order code [10]. The simulator interprets the assembler language directly. The word size is fixed at the host machine's integer type size (typically, it is 32-bit (4-Byte)).
Reference: [6] <author> Fu, J. W. C., Patel, J. H., and Janssens, B. L. </author> <title> Stride directed prefetching in scalar processors. </title> <booktitle> In Proceedings of the 25th International Symposium on Microarchitecture (1992), </booktitle> <pages> pp. 102-110. 14 </pages>
Reference-contexts: The stride is used to determine the possible next access by adding the stride and the current operand address. Methods in the literature <ref> [6, 17, 2] </ref> can be classified as stride prefetch methods. Conventional high-performance computers usually use a two-level cache hierarchy which consists of a fast but small primary on-chip cache and a larger secondary off-chip/on-board cache. <p> Comparing the DRT and stride method is a useful exercise because the stride method will capture all possible access streams. In this work, we use 256 entries for ST because this size of ST will make 90% hit-rate on ST <ref> [6] </ref>. time is divided further into miss stall time and pending stall time (see subsection 4.2). Most benchmark programs experience reduced load stall times by using prefetching, except fmm and lu with virtual-address DRT.
Reference: [7] <author> Gharachorloo, K., Lenoski, D., Laudon, J., Gibbons, P., Gupta, A., and Hennessy, J. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In 17th ISCA (June 1990), </booktitle> <pages> pp. 15-26. </pages>
Reference-contexts: The pending stall time comes from waiting for blocks that have already been requested by previously issued prefetches but are not yet complete. We concentrate on how to reduce the load stall because write stall can be completely hidden by using write-buffer and relaxed consistency model <ref> [7] </ref>. Under the release consistency model one of relaxed consistency models, write requests can be overlapped with other memory requests and computations. * The network traffic is defined as the total number of network activities.
Reference: [8] <author> Hagersten, E. </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Sweden, </institution> <month> July </month> <year> 1993. </year> <institution> Department of Telecommunication and Computer Systems. </institution>
Reference-contexts: Only a few of them are for the secondary cache based on stride. In this section, we give a brief survey of secondary cache data prefetching using stride. The stride directed prefetching for secondary cache was studied by Hagersten <ref> [8] </ref> in right-on-time (henceforth referred as ROT) prefetching. ROT uses the address of accesses missing in the primary cache instead of every operand address. The ROT consists of three main parts: stride detection, stream detection and prefetch. The stride detection part detects new strides. <p> Dahlgren and Stenstrom evaluated stride and sequential prefetching in [4]. In their study, they compared the performance of two stride detection mechanisms. One was instruction address driven and the other was operand address driven. The latter was same as that proposed by Hagersten <ref> [8] </ref>. They pointed out that sequential prefetching does better or at least equally well as stride prefetching. However, stride prefetching has the potential to outperform sequential prefetching if network/memory bandwidth is limited, because stride prefetching can make fewer useless prefetches. <p> The stride detection scheme uses partitions of the physical address space and a history buffer. The partitions enable the separation of accesses in different streams and to detect strides from accessing in the same partition. They studied prefetching for stream buffer and uniprocessor system. The ROT method, proposed in <ref> [8] </ref> and studied in [4], needs complex hardware that includes complex hardware list tables and comparators. In contrast, our method only needs a simple hardware table.
Reference: [9] <author> Jouppi, N. P. </author> <title> Improving direct-mapped cache performance by the addition of a small full-associative cache and prefetch buffers. </title> <booktitle> In 17th ISCA (1990), </booktitle> <pages> pp. 364-373. </pages>
Reference-contexts: In the sequential prefetch method, access to a cache line causes prefetching for successive line or lines. If there is sufficient spatial locality, this approach can work well. Methods in the literature <ref> [18, 9, 3] </ref> can be classified as sequential prefetch methods. In a stride prefetch method, the instruction and operand addresses of each load are stored in a hardware lookup table, and the instruction address enables the system to identify the access stream which belongs to the same instruction.
Reference: [10] <author> Ki, A. </author> <title> MUSIC Processor: Manchester university simplified instruction code processor. </title> <type> internal report, </type> <institution> University of Manchester, 1995. Department of Computer Science. </institution>
Reference-contexts: The processor models a load-store RISC architecture called MUSIC (Manchester University Simplified Instruction Code) processor <ref> [10] </ref>. All instructions execute in one processor cycle except those load and store instructions which require memory references. The primary cache is a write-through, direct-mapped data cache with no-allocation of blocks on write misses. The secondary cache is a write-back, direct-mapped data cache with write-allocate on write misses. <p> The compiler transforms a C-language source program to an assembler language output which is a simple non-pipelined RISC like "load/store" order code <ref> [10] </ref>. The simulator interprets the assembler language directly. The word size is fixed at the host machine's integer type size (typically, it is 32-bit (4-Byte)).
Reference: [11] <author> Ki, A. </author> <title> Aspects of hardware-controlled data prefetching. </title> <type> internal report, </type> <institution> University of Manchester, 1996. Department of Computer Science. </institution>
Reference-contexts: How prefetching behaves for different architectural parameters has been presented in <ref> [11] </ref>. We have made some architectural assumptions and simplifications in our simulation. We assume that there is sufficient depth of network and memory queues, so that no retries are needed but the effects of network contention and memory conflict are reflected in the queueing delay.
Reference: [12] <author> Kroft, D. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In 8th ISCA (1981), </booktitle> <pages> pp. 81-87. </pages>
Reference-contexts: We assume a write-buffer between the primary cache and the secondary cache. Write hits on the secondary cache do not stall the processor, but write misses on the secondary cache make the processor stall, and loads cannot bypass store in the write-buffer. The caches are lockup-free <ref> [12] </ref> : i.e. they can support multiple outstanding requests. When a request misses in the cache, a cache line is reserved and the request is sent to the memory. Prefetching is considered for load only.
Reference: [13] <author> Lusk, E., Boyle, J., Bntler, R., Disz, T., Glickfeld, B., Overbeek, R., Patterson, J., and Stevens, R. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehar and Winston, Inc., </publisher> <year> 1987. </year>
Reference: [14] <author> Manchester Computing. </author> <title> An Introduction to the IBM Compute Intensive Facility, </title> <year> 1994. </year> <month> RSS101. </month>
Reference-contexts: (water-nsquared version) The problem size of each benchmark has been kept relatively small because of the slow instruction interpretation rate of the simulator even when running on a IBM Power Series 6000 Model 590 computer which uses the IBM Power 2 RISC chip running at 66MHz with 0.5GByte real memory <ref> [14] </ref>. The problem size limits the amount of parallelism available, which in turn determines the maximum number of processors that can be utilised.
Reference: [15] <author> Mowry, T. C., and Gupta, A. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing 12 (1991), </journal> <pages> 87-106. </pages>
Reference-contexts: When a request misses in the cache, a cache line is reserved and the request is sent to the memory. Prefetching is considered for load only. Prefetched data is brought into the secondary cache, and this data remains visible to the cache coherence protocol to guarantee the non-binding property <ref> [15] </ref>. We assume that cache lines for prefetches are allocated in the cache at issue time in order to find the interference produced by prefetching. Prefetches for non-allocated pages are not allowed in order to reduce not-used prefetches and to prevent from prefetching out of data scope of programs. <p> Prefetches for non-allocated pages are not allowed in order to reduce not-used prefetches and to prevent from prefetching out of data scope of programs. The prefetch operation used in this work has following properties <ref> [15] </ref>: non-binding, non-blocking property, and non-excepting property. 2 The compiler is implemented using the Princeton University Lcc [5]. The compiler transforms a C-language source program to an assembler language output which is a simple non-pipelined RISC like "load/store" order code [10]. The simulator interprets the assembler language directly.
Reference: [16] <author> Palacharla, S., and Kessler, R. E. </author> <title> Evaluating stream buffers as a secondary cache replacement. </title> <booktitle> In 21st ISCA (1994), </booktitle> <pages> pp. 24-33. </pages>
Reference-contexts: They also showed that instruction address driven stride prefetching worked as well as, or better, than that of operand address driven. Palacharla and Kessler studied a stride detection method for a stream buffer <ref> [16] </ref>. The stride detection scheme uses partitions of the physical address space and a history buffer. The partitions enable the separation of accesses in different streams and to detect strides from accessing in the same partition. They studied prefetching for stream buffer and uniprocessor system. <p> In contrast, our method only needs a simple hardware table. Our experimental results show that a simple method, like the physical address data-range-table method, can achieve a performance improvement comparable to that of conventional stride prefetching, despite its much simpler hardware mechanisms. Palacharla and Kessler's method <ref> [16] </ref> is quite similar to ours. However, they did not think about software assisted method as our virtual address DRT nor did they think about hardware only approach as our physical address DRT, and their experiments were only about uniprocessor and stream buffer.
Reference: [17] <author> Sklenar, I. </author> <title> Prefetch unit for vector operations on scalar computers. Computer Architecture News 20, </title> <address> 4 (Sept. </address> <year> 1992), </year> <pages> 31-37. </pages>
Reference-contexts: The stride is used to determine the possible next access by adding the stride and the current operand address. Methods in the literature <ref> [6, 17, 2] </ref> can be classified as stride prefetch methods. Conventional high-performance computers usually use a two-level cache hierarchy which consists of a fast but small primary on-chip cache and a larger secondary off-chip/on-board cache. <p> of experiments shows the impact of different page partitions and the number of DRT entries on the performance of physical-address DRT prefetching. 7 5.1 Comparison with stride prefetching To measure the effectiveness of a DRT method described in section 3, we use the conventional stride directed prefetching method proposed in <ref> [17] </ref> (henceforth referred to as stride method). In the stride method, we assume that the instruction addresses are somehow available outside the processor. The stride method uses a hardware lookup table called the stride table (henceforth referred to as ST) which stores the instruction and operand addresses previously referenced.
Reference: [18] <author> Smith, A. J. </author> <title> Sequential program prefetching in memory hierarchies. </title> <booktitle> IEEE Computer 11, </booktitle> <month> 2 (Dec. </month> <year> 1978), </year> <pages> 7-21. </pages>
Reference-contexts: In the sequential prefetch method, access to a cache line causes prefetching for successive line or lines. If there is sufficient spatial locality, this approach can work well. Methods in the literature <ref> [18, 9, 3] </ref> can be classified as sequential prefetch methods. In a stride prefetch method, the instruction and operand addresses of each load are stored in a hardware lookup table, and the instruction address enables the system to identify the access stream which belongs to the same instruction.
Reference: [19] <author> Woo, S. C., Ohara, M., Torrie, E., Singh, J. P., and Gupta, A. </author> <title> The SPLASH-2 Programs: Characterization and methodological considerations. </title> <booktitle> In 22nd ISCA (1995), </booktitle> <pages> pp. 24-36. 15 </pages>
Reference-contexts: We subdivide the network traffic further into load traffic and store traffic, coherency traffic which includes invalidation and write-back, synchronisation traffic, and prefetching traffic. 4.3 Benchmark programs We use parallel benchmark programs taken from SPLASH-2 <ref> [19] </ref>. They are written in the C-language with a shared memory architecture in mind and parallelism is implemented by augmenting the program with the notation of the Argonne National Laboratory parallel macros (PARMACS)[13]. Table 2 summarises benchmark programs.
References-found: 19

