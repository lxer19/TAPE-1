URL: http://ai.fri.uni-lj.si/papers/pompe96-mscthesis.ps.gz
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Title: Uros Pompe Restricting the hypothesis space, guiding the search, and handling the redundant information in
Degree: M.Sc. Thesis Thesis supervisor dr. Igor Kononenko  
Date: Ljubljana, May 1996  
Affiliation: University of Ljubljana Faculty of Computer and Information Science  
Abstract-found: 0
Intro-found: 1
Reference: <author> Ali, K. M. & Pazzani, M. J. </author> <year> (1993). </year> <title> Hydra: A noise-tolerant relational concept learning algorithm. </title> <booktitle> In Proceedings of the 13 th International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1064-1070). </pages> <address> Chambery, France. </address>
Reference-contexts: The second reason might be that propositional learners take into account additional information by probabilistically analyzing the hypothesis over the training set. The probabilistic aspects of the hypothesis were considered before within ILP. For instance, the HYDRA system <ref> (Ali & Pazzani, 1993) </ref> uses probabilistical evaluation of clauses to the improve classification accuracy. However, its winner-takes-all strategy results in returning the class of the most reliable clause that was satisfied by the test example. Therefore, it does not exploit all available information.
Reference: <author> Bratko, I. </author> <year> (1990). </year> <booktitle> Prolog programming for artificial intelligence. </booktitle> <address> Wokingham, England: </address> <publisher> Addison-Wesley, second edition. </publisher>
Reference-contexts: Its techniques form the core of various knowledge elicitation tools. These tools were successfully used to alleviate the knowledge acquisition bottleneck, generally accepted to be the main problem of building expert systems. Between the several forms of learning, ranging from "learning by being told" to "learning by discovery" <ref> (Bratko, 1990) </ref>, the majority of current ML research is concerned with inductive learning, learning from examples. The learner is given a set of examples and its task is to find a generalization, that underlines them.
Reference: <author> Breiman, L., Friedman, J., & Olshen, R. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Kononenko (1994) showed that (3.1) highly correlates with the gini index <ref> (Breiman et al., 1984) </ref> and that it implicitly uses a kind of normalization for multi valued attributes (impurity functions tend to overestimate multi valued attributes and various normalization heuristics are needed to avoid this tendency, see (Quinlan, 1986)).
Reference: <author> Cestnik, B. </author> <year> (1990). </year> <title> Estimating probabilities: A crucial task in machine learning. </title> <booktitle> In Proceedings of European Conference on Artificial Intelligence (pp. </booktitle> <pages> 147-149). </pages> <address> Stockholm. </address>
Reference-contexts: Finally, given the training set E + of labeled (classified) examples, the probability P (F i ) is computed with the Laplace's law of succession of probability of class F i . The conditional probability P (F i jC j ) is assessed using the m-estimate <ref> (Cestnik, 1990) </ref>. It is computed over the set Cov (C j ; E + ). When classifying the example T the coverage factor j is determined by the outcome of trying to cover T with C j .
Reference: <author> Cestnik, B. & Bratko, I. </author> <year> (1991). </year> <title> On estimating probabilities in tree pruning. </title> <booktitle> In Y. </booktitle>
Reference: <editor> Kodratoff (Ed.), </editor> <booktitle> Proceedings of European Working Session on Learning (pp. </booktitle> <pages> 138-150). </pages> <publisher> Porto: Springer Verlag. </publisher>
Reference: <author> Cestnik, B., Kononenko, I., & Bratko, I. </author> <year> (1987). </year> <title> Assistant 86: A knowledge elicitation tool for sophisticated users. </title> <editor> In I. Bratko & N. Lavrac (Eds.), </editor> <booktitle> Progress in Machine Learning (pp. </booktitle> <pages> 31-45). </pages> <address> Wilmslow, England: </address> <publisher> Sigma Press. </publisher>
Reference-contexts: Representatives of these systems are for instance: ID3 (Quinlan, 1986), Assistant <ref> (Cestnik et al., 1987) </ref>, Assistant-R (Kononenko et al., 1996), CN2 (Clark & Niblett, 1987), AQ (Michalski et al., 1986), just to name a few.
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1987). </year> <title> Induction in noisy domains. </title> <editor> In I. Bratko & N. Lavrac (Eds.), </editor> <booktitle> Progress in Machine Learning (pp. </booktitle> <pages> 11-30). </pages> <address> Wilmslow, England: </address> <publisher> Sigma Press. </publisher>
Reference-contexts: Representatives of these systems are for instance: ID3 (Quinlan, 1986), Assistant (Cestnik et al., 1987), Assistant-R (Kononenko et al., 1996), CN2 <ref> (Clark & Niblett, 1987) </ref>, AQ (Michalski et al., 1986), just to name a few. Recently, encouraged by the success of the 0 th order siblings and by the rapid development of logic programming theory, a new breed of learning systems emerged. As the 2 CHAPTER 1.
Reference: <author> Cohen, W. W. </author> <year> (1993a). </year> <title> Learnability of restricted logic programs. </title> <editor> In S. Muggleton (Ed.), </editor> <booktitle> Proceedings of The Third International Workshop on Inductive Logic Programming ILP'93 (pp. </booktitle> <pages> 41-71). </pages> <address> Bled, Slovenia. </address>
Reference: <author> Cohen, W. W. </author> <year> (1993b). </year> <title> Pac-learning a restricted class of recursive logic programs. </title> <editor> In S. Muggleton (Ed.), </editor> <booktitle> Proceedings of The Third International Workshop on Inductive Logic Programming ILP'93 (pp. </booktitle> <pages> 73-86). </pages> <address> Bled, Slovenia. </address>
Reference: <author> Cohen, W. W. </author> <year> (1996). </year> <title> Learning to classify english text with ilp methods. </title> <editor> In L. De Raedt (Ed.), </editor> <booktitle> Advances in Inductive Logic Programming (pp. </booktitle> <pages> 124-143). </pages> <address> Amsterdam, Nether-lands: </address> <publisher> IOS Press. </publisher>
Reference-contexts: In addition, we also measured the precision of the induced hypotheses. This is a measure denoting the proportion of correctly classifier instances. It is similar to the one in <ref> (Cohen, 1996) </ref>, but is generalized to handle multiple classes. It can be computed as: = precision = # of correctly classified instances # of covered instances (6.2) For the naive Bayes, this measure is equal to the accuracy.
Reference: <author> De Raedt, L. & Dzeroski, S. </author> <year> (1994). </year> <title> First order jk-clausal theories are pac-learnable. </title> <booktitle> In S. </booktitle>
Reference: <editor> Wrobel (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming Bad Honnef/Bonn, </booktitle> <address> Germany. </address> <note> 60 BIBLIOGRAPHY Dolsak, </note> <author> B., Bratko, I., & Jezernik, A. </author> <year> (1994). </year> <title> Finite element mesh design: An engineering domain for ILP application. </title> <editor> In S. Wrobel (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 305-320). </pages> <address> Bad Honnef/Bonn, Germany: </address> <institution> GMD. </institution>
Reference: <author> Dolsak, B. & Muggleton, S. </author> <year> (1992). </year> <title> The application of inductive logic programming to finite elements mesh design. </title> <editor> In S. Muggleton (Ed.), </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference-contexts: set have better chances than ILP learners with MESH5R data set to reveal a good hypothesis. 6.3 Relational domains MESH5R: We tested the performance of ILP-R on the problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem <ref> (Dolsak & Muggleton, 1992) </ref>. There are five objects for which the human experts have constructed the appropriate meshes. In each of five experiments the edges of one object are used for testing and the edges of the other four for objects learning and the results are averaged.
Reference: <author> Dzeroski, S. </author> <year> (1991). </year> <title> Handling noise in inductive logic programming. </title> <type> Master's thesis, </type> <institution> University of Ljubljana, Faculty of electrical engineering and computer science, Ljubl-jana, Slovenia. </institution>
Reference-contexts: Top down algorithms, on the other hand, start with the most general hypothesis, which is then specialized to become consistent with training data (FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), mFOIL <ref> (Dzeroski, 1991) </ref>, MILP (Kovacic, 1994; Kovacic, 1995), FOSSIL (Furnkranz, 1994), FORS (Karalic, 1995)). Our approach belongs to the second class. 1.2. OVERVIEW OF THE THESIS 3 1.2 Overview of the thesis In the next chapter, a weak syntactic declarative bias is introduced. Its properties are shown and proved. <p> From everything said above, it seems that any algorithm using "powerful" representation language should be heuristically guided in order to be practically applicable. FOIL (Quinlan, 1990), and similar heuristic top-down algorithms, such as mFOIL <ref> (Dzeroski, 1991) </ref>, MILP (Kovacic, 1995), SFOIL (Pompe et al., 1993), seem to be a promising step in this direction. Unfortunately, each of them suffer from the problem of how to efficiently store the partial proof of a training example. <p> These measures, however, assume that the estimated literals are independent and therefore the greedy search has poor chance of revealing a good hypothesis. To overcome this problem, various extensions of the greedy search are employed: limited backtracking (Quinlan & Cameron-Jones, 1993), beam search <ref> (Dzeroski, 1991) </ref>, and stochastic search (Kovacic et al., 1992; Pompe et al., 1993; Kovacic, 1994). Kira and Rendell (1992b) developed an algorithm, called RELIEF, which was shown to be very effective in estimating the quality of attributes in learning the propositional theories. <p> KRK: The problem of legality of the King-Rook-King chess endgame. The relational background knowledge consists of the relations between the pieces such as adj file, adj rank, less file, and less rank. For learning we used 5 random sets of 100 examples each <ref> (Dzeroski, 1991) </ref>. The whole data set (5000 examples) was used for testing.
Reference: <author> Dzeroski, S., Muggleton, S., & Russell, S. </author> <year> (1993). </year> <title> Learnability of constraint logic programs. </title> <editor> In P. B. Brazdil (Ed.), </editor> <booktitle> Proceedings of European Conference on Machine Learning Vienna, </booktitle> <address> Austria: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Dzeroski, S., Todorovski, L., & Urbancic, T. </author> <year> (1995). </year> <title> Handling real numbers in inductive logic programming: A step towards better behavioural clones. </title> <editor> In N. </editor> <publisher> Lavrac & S. </publisher>
Reference: <editor> Wrobel (Eds.), </editor> <booktitle> Proceedings of European Conference on Machine Learning Heraclion, </booktitle> <address> Crete, Greece: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Furnkranz, J. </author> <year> (1994). </year> <title> Fossil: A robust relational learner. </title> <editor> In F. Bergadano & L. De Raedt (Eds.), </editor> <booktitle> Proceedings of European Conference on Machine Learning Catania, </booktitle> <address> Italy: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Top down algorithms, on the other hand, start with the most general hypothesis, which is then specialized to become consistent with training data (FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), mFOIL (Dzeroski, 1991), MILP (Kovacic, 1994; Kovacic, 1995), FOSSIL <ref> (Furnkranz, 1994) </ref>, FORS (Karalic, 1995)). Our approach belongs to the second class. 1.2. OVERVIEW OF THE THESIS 3 1.2 Overview of the thesis In the next chapter, a weak syntactic declarative bias is introduced. Its properties are shown and proved.
Reference: <author> Karalic, A. </author> <year> (1995). </year> <title> First Order Regression. </title> <type> PhD thesis, </type> <institution> University of Ljubljana, Faculty of Electrical Engineering and Computer Science, Ljubljana, Slovenia. </institution>
Reference-contexts: Top down algorithms, on the other hand, start with the most general hypothesis, which is then specialized to become consistent with training data (FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), mFOIL (Dzeroski, 1991), MILP (Kovacic, 1994; Kovacic, 1995), FOSSIL (Furnkranz, 1994), FORS <ref> (Karalic, 1995) </ref>). Our approach belongs to the second class. 1.2. OVERVIEW OF THE THESIS 3 1.2 Overview of the thesis In the next chapter, a weak syntactic declarative bias is introduced. Its properties are shown and proved.
Reference: <author> Kearns, M. J. & Vazirani, U. V. </author> <year> (1994). </year> <title> An Introduction to Computational Learning Theory. </title> <address> Cambridge, Massachusetts: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Its performance is evaluated empirically in Chapter 6. The last chapter contains the discussion of our approach. Future work is proposed there as well. 4 CHAPTER 1. INTRODUCTION Chapter 2 Efficient proof encoding Approximately at the time when ILP emerged, the theory of learnability was also developed <ref> (Kearns & Vazirani, 1994) </ref>. Many researchers started to study the learnability properties of ILP systems. It turned out, unfortunately, that many interesting classes of logic programs are not polynomialy learnable under arbitrary sample distribution. Cohen (1993a), for instance, showed that most recursive programs are not polynomialy predictable.
Reference: <author> Kietz, J.-U. </author> <year> (1993). </year> <title> Some lower bounds for the computational complexity of inductive logic programming. </title> <booktitle> In Proceedings of Sixth European Conference on Machine Learning (pp. </booktitle> <pages> 115-123). </pages> <address> Berlin, Germany: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992a). </year> <title> The feature selection problem: traditional methods and new algorithm. </title> <address> In AAAI'92 San Jose, CA. </address>
Reference-contexts: For that purpose, given an instance, RELIEF searches for its two nearest neighbors: one from the same class (called nearest hit) and the other from a different class (called nearest miss). The original algorithm of RELIEF <ref> (Kira & Rendell, 1992a) </ref> randomly selects n training instances, where n is the user-defined parameter, and computes the quality of the attributes as follows: set all weights W [A] := 0.0; for i := 1 to n do begin randomly select an instance R; find nearest hit H and nearest miss
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992b). </year> <title> A practical approach to feature selection. </title> <editor> In D. Sleeman & P. Edwards (Eds.), </editor> <booktitle> Proceedings of International Conference on Machine Learning (pp. </booktitle> <pages> 249-256). </pages> <address> Aberdeen: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of relief. In L. </title>
Reference-contexts: Those two neighbours together with the given example are then used to compute the quality of the literal. It was shown in <ref> (Kononenko, 1994) </ref> that it is sometimes desirable to look at somewhat larger neighbourhood. Computing the quality statistics over the larger set of neighbours can improve its reliability which is important when learning in the presence of noise.
Reference: <editor> De Raedt & F. Bergadano (Eds.), </editor> <booktitle> Proceedings of European Conference on Machine Learning Catania: </booktitle> <publisher> Springer Verlag. BIBLIOGRAPHY 61 Kononenko, </publisher> <editor> I. & Bratko, I. </editor> <year> (1991). </year> <title> Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 67-80. </pages>
Reference: <author> Kononenko, I., Simec, E., & Robnik, M. </author> <year> (1996). </year> <title> Overcoming the myopia of inductive learning algorithms with relieff. </title> <journal> Journal of Applied Intelligence. </journal> <note> to appear. </note>
Reference-contexts: Representatives of these systems are for instance: ID3 (Quinlan, 1986), Assistant (Cestnik et al., 1987), Assistant-R <ref> (Kononenko et al., 1996) </ref>, CN2 (Clark & Niblett, 1987), AQ (Michalski et al., 1986), just to name a few. Recently, encouraged by the success of the 0 th order siblings and by the rapid development of logic programming theory, a new breed of learning systems emerged. <p> Tables 6.2 and 6.3 show the classification accuracy and the average information score, respectively, on the domains that are available in both, attributional and relational, versions. The tables were taken from <ref> (Kononenko et al., 1996) </ref>. The descriptions of individual systems can be found there as well. 44 CHAPTER 6. EXPERIMENTAL EVALUATION In the following, some of the tables are presented and discussed. The rest of the results, our comments are based on, can be found in Appendix A.
Reference: <author> Kovacic, M. </author> <year> (1994). </year> <title> Milp-a stochastic approach to inductive logic programming. </title> <booktitle> In S. </booktitle>
Reference: <editor> Wrobel (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 123-138). </pages> <address> Bad Honnef/Bonn, Germany. </address>
Reference: <author> Kovacic, M. </author> <year> (1995). </year> <title> Stochastic inductive logic programming. </title> <type> PhD thesis, </type> <institution> University of Ljubljana, Faculty of electrical engineering and computer science, Ljubljana, Slovenia. </institution>
Reference-contexts: From everything said above, it seems that any algorithm using "powerful" representation language should be heuristically guided in order to be practically applicable. FOIL (Quinlan, 1990), and similar heuristic top-down algorithms, such as mFOIL (Dzeroski, 1991), MILP <ref> (Kovacic, 1995) </ref>, SFOIL (Pompe et al., 1993), seem to be a promising step in this direction. Unfortunately, each of them suffer from the problem of how to efficiently store the partial proof of a training example.
Reference: <author> Kovacic, M., Lavrac, N., Grobelnik, M., Zupanic, D., & Mladenic, D. </author> <year> (1992). </year> <title> Stochastic search in inductive logic programming. </title> <booktitle> In Proceedings of the 10 th European Conference on Artificial Intelligence Vienna. </booktitle>
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of recursive bayesian classifiers. </title> <editor> In P. B. Brazdil (Ed.), </editor> <booktitle> Proceedings of European Conference on Machine Learning Vienna, </booktitle> <address> Austria: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Lavrac, N. & Dzeroski, S. </author> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <address> Chichester: </address> <publisher> Ellis Horwood. </publisher>
Reference: <author> Lloyd, J. W. </author> <year> (1987). </year> <booktitle> Foundations of Logic Programming. </booktitle> <address> Berlin, Germany: </address> <publisher> Springer Verlag, second edition. </publisher>
Reference-contexts: In the discussion, at the end of the chapter, we will argue that our approach, despite its limitations, is general enough to be of practical importance. 2.1 Hypothesis language bias It is assumed in the following that the reader is familiar with the terminology and basic assertions found in <ref> (Lloyd, 1987) </ref>. Some of the key definitions and assertions can be found in Appendix B. From now on, when not specified otherwise, our domain of interest are normal programs (B.41) and normal clauses (B.40). Further more, we will consider only flat clauses, containing no other terms but variables and constants. <p> NAIVE BAYESIAN HYPOTHESIS INTERPRETATION 4.2 Procedural classification The common way of using the induced hypothesis is to run it through the Prolog interpreter and collect the results. What this effectively means is that we interpret the hypothesis in a procedural way. More formally <ref> (Lloyd, 1987) </ref>, given the hypothesis H, which is in our case a normal program without function symbols (except for the constant), the background knowledge B and an unclassified example T , we are looking for the first computed answer fi c which we encounter during the SLDNF-resolution of the goal T
Reference: <author> Michalski, R., Mozetic, I., Hong, J., & Lavrac, N. </author> <year> (1986). </year> <title> The AQ15 inductive learning system: an overview and experiments. </title> <booktitle> In Proceedings of IMAL 1986 Orsay: </booktitle> <institution> Univer-site de Paris-Sud. </institution>
Reference-contexts: Representatives of these systems are for instance: ID3 (Quinlan, 1986), Assistant (Cestnik et al., 1987), Assistant-R (Kononenko et al., 1996), CN2 (Clark & Niblett, 1987), AQ <ref> (Michalski et al., 1986) </ref>, just to name a few. Recently, encouraged by the success of the 0 th order siblings and by the rapid development of logic programming theory, a new breed of learning systems emerged. As the 2 CHAPTER 1.
Reference: <author> Mladenic, D. </author> <year> (1993). </year> <title> Combinatorial optimization in inductive concept learning. </title> <booktitle> In Proceedings of the 10 th International Conference on Machine Learning (pp. </booktitle> <pages> 205-211). </pages> <address> Amherst: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Muggleton, S. </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <address> London, England: </address> <publisher> Academic Press. </publisher>
Reference-contexts: There are two main approaches to the search for the appropriate hypothesis. The bottom up (rlgg, inverting resolution, : : : ) starts with a set of examples, most specific hypothesis, and generalizes them to obtain the hypothesis (relative to the background knowledge) (CIGOL <ref> (Muggleton & Buntine, 1992) </ref>, ITOU (Rouveirol, 1992), GOLEM (Muggleton & Feng, 1992)). <p> The bottom up (rlgg, inverting resolution, : : : ) starts with a set of examples, most specific hypothesis, and generalizes them to obtain the hypothesis (relative to the background knowledge) (CIGOL (Muggleton & Buntine, 1992), ITOU (Rouveirol, 1992), GOLEM <ref> (Muggleton & Feng, 1992) </ref>). Top down algorithms, on the other hand, start with the most general hypothesis, which is then specialized to become consistent with training data (FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), mFOIL (Dzeroski, 1991), MILP (Kovacic, 1994; Kovacic, 1995), FOSSIL (Furnkranz, 1994), FORS (Karalic, 1995)). <p> set have better chances than ILP learners with MESH5R data set to reveal a good hypothesis. 6.3 Relational domains MESH5R: We tested the performance of ILP-R on the problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem <ref> (Dolsak & Muggleton, 1992) </ref>. There are five objects for which the human experts have constructed the appropriate meshes. In each of five experiments the edges of one object are used for testing and the edges of the other four for objects learning and the results are averaged.
Reference: <author> Muggleton, S. </author> <year> (1993). </year> <title> Inductive logic programming: Derivations, </title> <publisher> successes, shortcomings. </publisher>
Reference: <editor> In P. B. Brazdil (Ed.), </editor> <booktitle> Proceedings of European Conference on Machine Learning Vienna, </booktitle> <address> Austria: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Muggleton, S. & Buntine, W. </author> <year> (1992). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <editor> In S. Muggleton (Ed.), </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference-contexts: There are two main approaches to the search for the appropriate hypothesis. The bottom up (rlgg, inverting resolution, : : : ) starts with a set of examples, most specific hypothesis, and generalizes them to obtain the hypothesis (relative to the background knowledge) (CIGOL <ref> (Muggleton & Buntine, 1992) </ref>, ITOU (Rouveirol, 1992), GOLEM (Muggleton & Feng, 1992)). <p> The bottom up (rlgg, inverting resolution, : : : ) starts with a set of examples, most specific hypothesis, and generalizes them to obtain the hypothesis (relative to the background knowledge) (CIGOL (Muggleton & Buntine, 1992), ITOU (Rouveirol, 1992), GOLEM <ref> (Muggleton & Feng, 1992) </ref>). Top down algorithms, on the other hand, start with the most general hypothesis, which is then specialized to become consistent with training data (FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), mFOIL (Dzeroski, 1991), MILP (Kovacic, 1994; Kovacic, 1995), FOSSIL (Furnkranz, 1994), FORS (Karalic, 1995)). <p> set have better chances than ILP learners with MESH5R data set to reveal a good hypothesis. 6.3 Relational domains MESH5R: We tested the performance of ILP-R on the problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem <ref> (Dolsak & Muggleton, 1992) </ref>. There are five objects for which the human experts have constructed the appropriate meshes. In each of five experiments the edges of one object are used for testing and the edges of the other four for objects learning and the results are averaged.
Reference: <author> Muggleton, S. & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In S. Muggleton (Ed.), </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press. 62 BIBLIOGRAPHY Muggleton, </publisher> <editor> S. & Page Jr., C. D. </editor> <year> (1994). </year> <title> A learning model for universal representation. </title>
Reference-contexts: There are two main approaches to the search for the appropriate hypothesis. The bottom up (rlgg, inverting resolution, : : : ) starts with a set of examples, most specific hypothesis, and generalizes them to obtain the hypothesis (relative to the background knowledge) (CIGOL <ref> (Muggleton & Buntine, 1992) </ref>, ITOU (Rouveirol, 1992), GOLEM (Muggleton & Feng, 1992)). <p> The bottom up (rlgg, inverting resolution, : : : ) starts with a set of examples, most specific hypothesis, and generalizes them to obtain the hypothesis (relative to the background knowledge) (CIGOL (Muggleton & Buntine, 1992), ITOU (Rouveirol, 1992), GOLEM <ref> (Muggleton & Feng, 1992) </ref>). Top down algorithms, on the other hand, start with the most general hypothesis, which is then specialized to become consistent with training data (FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993), mFOIL (Dzeroski, 1991), MILP (Kovacic, 1994; Kovacic, 1995), FOSSIL (Furnkranz, 1994), FORS (Karalic, 1995)). <p> set have better chances than ILP learners with MESH5R data set to reveal a good hypothesis. 6.3 Relational domains MESH5R: We tested the performance of ILP-R on the problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem <ref> (Dolsak & Muggleton, 1992) </ref>. There are five objects for which the human experts have constructed the appropriate meshes. In each of five experiments the edges of one object are used for testing and the edges of the other four for objects learning and the results are averaged.
Reference: <editor> In S. Wrobel (Ed.), </editor> <booktitle> Proceedings of the Fourth International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 139-160). </pages> <address> Bad Honnef/Bonn, Germany. </address>
Reference: <author> Pompe, U. & Kononenko, I. </author> <year> (1995a). </year> <title> Linear space induction in first order logic with relief. </title> <editor> In R. Kruse, R. Viertl, & G. Della Riccia (Eds.), </editor> <booktitle> CISM Lecture notes. Udine, Italy: Springer Verlag. Presented at: International School for the Synthesis of Expert Knowledge, Udine, </booktitle> <year> 1994. </year>
Reference-contexts: Since we are using the extensional background knowledge, we are neglecting this problem. We implemented this proof encoding scheme in our ILP system ILP-R <ref> (Pompe & Kononenko, 1995a) </ref> (limited to 2-dependable languages). Empirically, it proved to be efficient for many artificial and real world domains by both, being able to express the target hypothesis, and to compute the proofs efficiently (time-wise). 22 CHAPTER 2. <p> The look ahead RELIEF heuristics checks also, if some of this new value can be proved in the next step of the clause construction by the use of predicates of arity 1 (attributes) from the background knowledge. The details of the algorithm can be found in <ref> (Pompe & Kononenko, 1995a) </ref>. In the following text, when not specified otherwise, it is assumed that the simple version of RELIEF (without look ahead) is used. 30 CHAPTER 3.
Reference: <author> Pompe, U. & Kononenko, I. </author> <year> (1995b). </year> <title> Naive bayesian classifier within ILP-R. In L. </title>
Reference: <editor> De Raedt (Ed.), </editor> <booktitle> Proceedings of the 5 th International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 417-436). </pages> <institution> Leuven, Belgium: Katholieke Universiteit Leuven. </institution>
Reference: <author> Pompe, U., Kovacic, M., & Kononenko, I. </author> <year> (1993). </year> <title> Sfoil: Stochastic approach to inductive logic programming. </title> <booktitle> In Proceedings of the Second Electrotechnical and Computer Science Conference ERK'93 (pp. </booktitle> <pages> 189-192). </pages> <address> Portoroz, Slovenia. </address>
Reference-contexts: From everything said above, it seems that any algorithm using "powerful" representation language should be heuristically guided in order to be practically applicable. FOIL (Quinlan, 1990), and similar heuristic top-down algorithms, such as mFOIL (Dzeroski, 1991), MILP (Kovacic, 1995), SFOIL <ref> (Pompe et al., 1993) </ref>, seem to be a promising step in this direction. Unfortunately, each of them suffer from the problem of how to efficiently store the partial proof of a training example. <p> The results reported by Dzeroski (1991) for various ILP systems are 12 % of classification accuracy for FOIL, 22 % for mFOIL and 29 % for GOLEM and the result reported by <ref> (Pompe et al., 1993) </ref> is 28 % for SFOIL. The description of the MESH5R problem is appropriate for ILP systems. For attribute learners only relations with arity 1 (i.e. attributes) can be used to describe the problem.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Representatives of these systems are for instance: ID3 <ref> (Quinlan, 1986) </ref>, Assistant (Cestnik et al., 1987), Assistant-R (Kononenko et al., 1996), CN2 (Clark & Niblett, 1987), AQ (Michalski et al., 1986), just to name a few. <p> Kononenko (1994) showed that (3.1) highly correlates with the gini index (Breiman et al., 1984) and that it implicitly uses a kind of normalization for multi valued attributes (impurity functions tend to overestimate multi valued attributes and various normalization heuristics are needed to avoid this tendency, see <ref> (Quinlan, 1986) </ref>). Due to the "nearest instance" condition, we can interpret the RELIEF's estimates as the average over local estimates in the small parts of the instance space. This enables RELIEF to take into account the dependencies between attributes which can be detected in the context of locality.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference-contexts: From everything said above, it seems that any algorithm using "powerful" representation language should be heuristically guided in order to be practically applicable. FOIL <ref> (Quinlan, 1990) </ref>, and similar heuristic top-down algorithms, such as mFOIL (Dzeroski, 1991), MILP (Kovacic, 1995), SFOIL (Pompe et al., 1993), seem to be a promising step in this direction. Unfortunately, each of them suffer from the problem of how to efficiently store the partial proof of a training example.
Reference: <author> Quinlan, J. R. </author> <year> (1991). </year> <title> Determinate literals in inductive logic programming. </title> <booktitle> In Proceedings of International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 746-750). </pages> <address> Sydney, Australia. </address>
Reference: <author> Quinlan, J. R. & Cameron-Jones, R. </author> <year> (1993). </year> <title> Foil: A midterm report. </title> <editor> In P. B. Brazdil (Ed.), </editor> <booktitle> Proceedings of European Conference on Machine Learning Vienna, </booktitle> <address> Austria: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: These measures, however, assume that the estimated literals are independent and therefore the greedy search has poor chance of revealing a good hypothesis. To overcome this problem, various extensions of the greedy search are employed: limited backtracking <ref> (Quinlan & Cameron-Jones, 1993) </ref>, beam search (Dzeroski, 1991), and stochastic search (Kovacic et al., 1992; Pompe et al., 1993; Kovacic, 1994). Kira and Rendell (1992b) developed an algorithm, called RELIEF, which was shown to be very effective in estimating the quality of attributes in learning the propositional theories.
Reference: <author> Rouveirol, C. </author> <year> (1992). </year> <title> Extensions of inversion of resolution applied to theory completion. </title>
Reference-contexts: There are two main approaches to the search for the appropriate hypothesis. The bottom up (rlgg, inverting resolution, : : : ) starts with a set of examples, most specific hypothesis, and generalizes them to obtain the hypothesis (relative to the background knowledge) (CIGOL (Muggleton & Buntine, 1992), ITOU <ref> (Rouveirol, 1992) </ref>, GOLEM (Muggleton & Feng, 1992)).
Reference: <editor> In S. Muggleton (Ed.), </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference: <author> Smyth, P., Goodman, R., & Higgins, C. </author> <year> (1990). </year> <title> A hybrid rule-based bayesian classifier. </title> <booktitle> In Proceedings of European Conference on Artificial Intelligence (pp. </booktitle> <pages> 610-615). </pages> <address> Stockholm. </address>
References-found: 53

