URL: ftp://ftp.cs.virginia.edu/pub/dissertations/9502.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/dissertations/README.html
Root-URL: http://www.cs.virginia.edu
Title: PREDICTABILITY AND CONSISTENCY IN REAL-TIME TRANSACTION PROCESSING  
Author: Young-Kuk Kim 
Degree: A Dissertation Presented to the Faculty of the  In Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy in Computer Science by  
Date: May 1995  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. Abbott and H. Garcia-Molina. </author> <title> Scheduling Real-Time Transactions: A Performance Evaluation. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 17(3) </volume> <pages> 513-560, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Most previous work assumes only soft or firm deadline transactions in a real-time database system <ref> [1, 23, 25, 28, 52] </ref>. In such systems, the transaction scheduler is usually supposed to have no idea about a transaction's computing time and resource requirement in advance. <p> We agree that, under the conventional database management system's architecture and operating system environment, it is almost impossible to estimate the worst case behavior of a transaction correctly. Some real-time transaction scheduling algorithms <ref> [1, 59] </ref> that utilize a priori knowledge about transactions are unrealistic. However, there are many potential applications of hard real-time database systems in the real world, such as flight control systems and missile guidance systems. <p> This allows the system to discard infeasible transactions (i.e., transactions which may not complete before their deadlines) even before they begin execution so that wasted computations, aborts, and restarts can be avoided <ref> [1, 39] </ref>. 1.2.2 Correctness Criteria Another limitation of current real-time transaction scheduling algorithms is that most of them rely on serializability to preserve the logical consistency of the database, but they fail to address how to maintain temporal consistency of real-time data. <p> The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance. <p> they cannot guarantee that each transaction will complete by its deadline, but try to minimize the deadline miss ratio of transactions or to maximize the total value of transactions completed by their deadlines, when transactions have different values. 25 This class of algorithms are High Priority (or Priority Abort ) <ref> [29, 1] </ref>, Priority Inheri--tance (or Wait Promote) [30, 28, 1] protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols [24, 23] with optimistic concurrency control techniques, and hybrid concurrency control algorithms [52, 63]. <p> complete by its deadline, but try to minimize the deadline miss ratio of transactions or to maximize the total value of transactions completed by their deadlines, when transactions have different values. 25 This class of algorithms are High Priority (or Priority Abort ) [29, 1], Priority Inheri--tance (or Wait Promote) <ref> [30, 28, 1] </ref> protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols [24, 23] with optimistic concurrency control techniques, and hybrid concurrency control algorithms [52, 63]. <p> Intuitively, this information can help the scheduler to determine eligibility of transactions, reducing the wasted CPU time and recovery overhead due to aborted transactions. In this class of algorithms, Least Slack priority assignment policy can be used, combined with conflict resolution policies such as Conditional Restart <ref> [1] </ref> and Conditional Priority Inheritance [30, 28] under the two phase locking concurrency control scheme. However, some of the performance results for those algorithms are contradictory and there is no consensus on which are the best algorithms. For example, it is reported in [1] that concurrency control policies that combine blocking <p> conflict resolution policies such as Conditional Restart <ref> [1] </ref> and Conditional Priority Inheritance [30, 28] under the two phase locking concurrency control scheme. However, some of the performance results for those algorithms are contradictory and there is no consensus on which are the best algorithms. For example, it is reported in [1] that concurrency control policies that combine blocking with priority inheritance (Wait Promote, Conditional Restart) generally perform better than a policy that aborts lower priority transactions in order to favor higher priority ones (High Priority), whereas the opposite to this result is reported in [30]. <p> scheduling mechanism of a real-time database management system that utilizes such information and guarantees both consistency and timing constraints. 31 Chapter 3 Real-Time Database System Model Most real-time database scheduling algorithms have been developed and evaluated under almost the same workload and operating environment model used in conventional database systems <ref> [1, 25, 28, 64] </ref>. That is, transactions are assumed to arrive in a Poisson stream at a specified mean rate. Each transaction consists of a random sequence of pages to be read, a subset of which are updated. In addition, a conventional disk-based database environment is assumed. <p> However, we decide not to further categorize soft real-time transactions but to concentrate on Class I and Class II transactions. Much work has been already done for Class III transactions <ref> [1, 25, 28, 64] </ref>. Moreover, all Class III transactions are supposed to have the same level of importance (non-critical) and do not require an individual performance guarantee. <p> Most real-time database research uses the models which include only a subset of the above classes (e.g., fClass Ig [61, 4, 66] or fClass IIIg <ref> [1, 25, 28, 64] </ref>), and never discriminate among transactions in the system. However, in practice, all kinds of transactions can coexist in one system. <p> In this case, we can assign priorities to Class III transactions using one of the following methods: Earliest Deadline First (EDF) or Least Slack First (LSF) <ref> [1] </ref>. <p> Since there are only Class III transactions in the system, this can reproduce the results of the past research on soft/firm real-time transaction scheduling. SOCC. As reported in <ref> [1] </ref>, EDF and LSF perform better than FCFS. At lower load settings, EDF perform close to LSF. As the load increases, the performance margin of EDF over FCFS narrows and LSF becomes the best policy. <p> StarBase uses no a priori information about transaction workload and discards tardy transactions at their deadline points. In order to realize many of these real-time goals, StarBase is constructed on top of RT-Mach, a real-time operating system developed by Carnegie Mellon University [76]. StarBase differs from previous RTDBS work <ref> [1, 25, 28] </ref>, in that a) it relies on a real-time operating system which provides priority-based scheduling and time-based synchronization, and b) it deals explicitly with data contention and deadline handling in addition to transaction scheduling, the traditional focus of simulation studies. 6.2.1 Database Overview The StarBase system is organized as
Reference: [2] <author> N. Audsley, A. Burns, M. Richardson, K. Tindell, and A. Wellings. </author> <title> Applying New Scheduling Theory to Static Priority Pre-emptive Scheduling. </title> <journal> Software Engineering Journal, </journal> <volume> 8(5) </volume> <pages> 284-292, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: slack available at all priority levels, soft tasks may be executed at the highest priority level. 22 Unfortunately, the need to map out the hyperperiod restricts the applicability of this optimal algorithm: Slack can only be stolen from hard deadline tasks which are strictly periodic and have no release jitter <ref> [2] </ref> or synchronization. Realistically, it is also limited to task sets with a manageably short hyperperiod. This is a significant restriction, as even modest task sets (e.g., 10 tasks) may have very long hyperperiods. <p> i (t) The next deadline on an invocation of t i . c i (t) The remaining execution time budget for the current invocation of t i . 1 When a transaction is subject to a bounded delay between its arrival and release, it is said to exhibit release jitter <ref> [2] </ref>. 138 Note that if the current invocation of t i is complete, then d i (t) = x i (t) + D i , i.e., d i (t) is the deadline following the next release. c i (t) can be found by subtracting the execution time used from the worst-case <p> By combining these two equations, S i (t) can be found by iterating over the interval [t, t + d i (t)), totaling up all the idle time. The first equation is derived using techniques given in <ref> [2] </ref>; two components determine the extent of busy period: 1. The level i or higher priority processing outstanding at time t. 2. The level i or higher priority processing released during the busy period. The second component implies a recursive definition. <p> The recurrence relation begins with w 0 i (t) = 0 and ends when w m+1 i (t) = w m or w m+1 i (t) &gt; d i (t). Proof of convergence follows from the analysis of similar recurrence relations by Audsley et al <ref> [2] </ref>. The final value of w i (t) defines the length of the busy period. Alternatively, we may view t + w i (t) as defining the start of a level i idle period.
Reference: [3] <author> N. C. Audsley, A. Burns, M. F. Richardson, and A. J. Wellings. </author> <title> Hard Real-Time Scheduling: The Deadline Monotonic Approach. </title> <booktitle> In Proceedings of the 8th IEEE Workshop on Real-Time Operating Systems and Software, </booktitle> <address> Atlanta, GA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: For this case, there are two popular approaches: (1) static or fixed-priority algorithms, including the rate-monotonic and deadline-monotonic algorithms <ref> [53, 3] </ref> and (2) dynamic priority algorithms, including the earliest deadline algorithm [53]. Both approaches are becoming increasingly well developed, although at the present time the static priority theory is much more complete. <p> In other static approaches, temporal consistency is not guaranteed, but just its violation ratio is tried to be minimized. Throughout this section, we assume that the transactions are scheduled under a fixed-priority scheduling framework, called deadline-monotonic scheduling scheme <ref> [3] </ref>, where priorities assigned to processes are inversely proportional to the length of the deadline. Thus, the process with the shortest deadline is assigned the highest priority and the longest deadline process is assigned the lowest priority. <p> Then, the pure execution time of t (denoted 1 In deadline-monotonic approach, the deadline and period of a process do not have to be equal. Such a relaxation enables sporadic processes to be directly incorporated without alteration to the process model <ref> [3] </ref>. 49 as C t ) can be written as C t = t init + (t fetch + t comp ) fl N fl S t + t close ; where N is the size of the database, S t is a random variable of the selectivity distribution of t <p> T II sih Set of all sporadic interrupt handlers M h Period of a sporadic interrupt handler in sih associated with a transaction t s in hps (i) C IH Computing cost of a sporadic interrupt handler Extended Schedulability Analysis The schedulability tests for the deadline-monotonic scheduling scheme presented in <ref> [3] </ref> does not include the possible system overheads involved in the underlying kernel mechanism, such as clock interrupt-driven scheduling, context switching, and sporadic interrupt handling. A more realistic off-line schedulability analysis method must be designed for transactions in T G , considering the system's operating environment.
Reference: [4] <author> N. C. Audsley, A. Burns, M. F. Richardson, and A. J. Wellings. </author> <title> Absolute and Relative Temporal Constraints in Hard Real-Time Databases. </title> <booktitle> In Proceedings of 1992 IEEE EuroMicro Workshop on Real Time Systems, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: For those applications, a real-time database system must provide mechanisms to minimize the execution time variance of a transaction, making the system's behavior predictable. There has been some work dealing with hard deadline transactions <ref> [59, 66, 4, 51] </ref>, but impractical assumptions have often been made (for example, all transactions are periodic and their 1 Firm deadline transactions are special cases of soft deadline transactions which have no value after their deadlines. 5 worst-case execution times are given) or different correctness and performance criteria for hard <p> However, although this notion of temporal consistency was used to judge the effectiveness of scheduling algorithms, their multiversion concurrency control algorithms do not guarantee the temporal consistency of real-time data objects. Audsley et al <ref> [4] </ref> characterize real-time data objects and derive some temporal consistency requirements, but they 27 do not show how a transaction scheduler can realize such requirements. Different from the above approaches, in our research different consistency criteria are applied for different types of data objects. <p> Most real-time database research uses the models which include only a subset of the above classes (e.g., fClass Ig <ref> [61, 4, 66] </ref> or fClass IIIg [1, 25, 28, 64]), and never discriminate among transactions in the system. However, in practice, all kinds of transactions can coexist in one system. <p> We observe that no transaction scheduling algorithms proposed so far completely satisfy all these requirements even though several papers in the real-time database field have pointed them out <ref> [66, 4, 50, 5] </ref>.
Reference: [5] <author> N. C. Audsley, A. Burns, M. F. Richardson, and A. J. Wellings. </author> <title> Data Consistency in Hard Real-Time Systems. </title> <type> Technical Report YCS203, </type> <institution> Department of Computer Science, University of York, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: We observe that no transaction scheduling algorithms proposed so far completely satisfy all these requirements even though several papers in the real-time database field have pointed them out <ref> [66, 4, 50, 5] </ref>.
Reference: [6] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: Finally, if t writes on a data item written by t rcc , instead of restarting t , the OCC-PS algorithm applies the Thomas' Write Rule <ref> [6] </ref>, i.e., guarantees serializability simply by not committing the write value of t to database. (A late write value of t can be discarded from the private workplace as soon as it is known that the write is late.) Different from the original OCC-PS algorithm, our Semantic Concurrency Control (SOCC) algorithm
Reference: [7] <author> A. Buchmann et al. </author> <title> Time-Critical Database Scheduling: A Framework for Integrating Real-Time Scheduling and Concurrency Control. </title> <booktitle> In Proceedings of the 5th International Conference on Data Engineering. IEEE, </booktitle> <month> February </month> <year> 1989. </year>
Reference-contexts: The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance.
Reference: [8] <author> A. Burns. </author> <title> Scheduling Hard Real-Time Systems: A Review. </title> <journal> Software Engineering Journal, </journal> <volume> 6(3) </volume> <pages> 116-128, </pages> <year> 1991. </year>
Reference-contexts: Both approaches are becoming increasingly well developed, although at the present time the static priority theory is much more complete. A 18 summary of the results available on fixed-priority scheduling can be found in review articles by Burns <ref> [8] </ref> and Lehoczky [47]. Given the success of fixed priority scheduling methods, it is natural to attempt to extend this theory to solve other important problems that arise in real-time systems. <p> Scheduling Sporadic Tasks Non-periodic tasks are those whose releases are not periodic in nature. Such tasks can be subdivided into two categories <ref> [8] </ref>: aperiodic and sporadic. The difference between these categories lies in the nature of their release frequencies. Aperiodic tasks are those whose release frequency is unbounded. In the extreme, this could lead to an arbitrarily large number of simultaneously active tasks. <p> However, we note that since the task is sporadic, the actual release times of the task will not be periodic, but successive releases will be separated by no less than M s time units. For the schedulability tests given in <ref> [8] </ref> to be effective for this task set, it is assumed that at some instant all tasks, both periodic and sporadic, are released simultaneously (i.e., a 20 critical instant ).
Reference: [9] <author> A. Burns and A. J. Wellings. </author> <title> Implementing Analysable Hard Real-time Sporadic Tasks in Ada 9X. </title> <type> Technical Report YCS209, </type> <institution> Department of Computer Science, University of York, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: A more realistic off-line schedulability analysis method must be designed for transactions in T G , considering the system's operating environment. In the following, we present our extended schedulability analysis based on the analysis in <ref> [9, 10] </ref>. The parameters and notations used in the analysis are summarized in Table 4.1. In our RTDBS model, transactions are supposed to be scheduled by a timer-driven 51 scheduler [32] (i.e., the scheduler is invoked by a regular timing interrupt with a period denoted by P clk ).
Reference: [10] <author> A. Burns, A. J. Wellings, and A. D. Hutcheon. </author> <title> The Impact of an Ada Runtime System's Performance Charactersitics on Scheduling Models. </title> <booktitle> In Proceedings of the 12th Ada Europe Conference, </booktitle> <volume> LNCS 688, </volume> <pages> pages 240-248. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: A more realistic off-line schedulability analysis method must be designed for transactions in T G , considering the system's operating environment. In the following, we present our extended schedulability analysis based on the analysis in <ref> [9, 10] </ref>. The parameters and notations used in the analysis are summarized in Table 4.1. In our RTDBS model, transactions are supposed to be scheduled by a timer-driven 51 scheduler [32] (i.e., the scheduler is invoked by a regular timing interrupt with a period denoted by P clk ).
Reference: [11] <author> P. Dasgupta and R. J. LeBlanc Jr. </author> <title> Clouds: A Support Architecture for Fault Tolerant, Distributed Systems. </title> <type> Technical report, </type> <institution> School of Information and Computer Science, Georgia Institute of Technology, </institution> <year> 1985. </year> <month> 146 </month>
Reference-contexts: However, such services are only available to DBMS users and not to general clients of the operating system such as mail programs and text editors. There has been considerable interest in providing transactions as an operating system service [71, 72, 16, 18], and several OSs have done exactly that <ref> [27, 38, 11] </ref>. Moreover, if a user wishes to have a cross-system transaction (i.e., one involving both a DBMS and another subsystem) then it is easily supported with an OS transaction manager but is nearly impossible with a DBMS-supplied one.
Reference: [12] <author> R. I. Davis. </author> <title> Approximate Slack Stealing Algorithms for Fixed Priority Preemptive Systems. </title> <type> Technical Report YCS217, </type> <institution> Department of Computer Science, University of York, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Unfortunately, the execution time overhead of the optimal dynamic algorithm is such that it is infeasible in practice. Approximate methods of determining slack presented by Davis <ref> [12] </ref> address the space and time complexity problems inherent in the optimal algorithms. These approximations form the basis of various approximate slack stealing algorithms which offer close to optimal performance with practical utility. <p> Note that the soft deadline task model used in the original slack stealing studies <ref> [48, 14, 12] </ref> is different from our Class III transaction model in that its performance goal is to minimize their average response times. <p> The combined utilization of Class I and II transactions is set to 50% throughout the following experiments. Varying Slack Calculation Overhead We did not perform the experiments for PASS with the various SLACK CALC PERIOD values, since the impact of this parameter has been already studied in <ref> [12] </ref>. Typically, to achieve close to optimal performance, the slack calculation period (SLACK CALC PERIOD) in the PASS algorithm needs to be the same order of magnitude as the shortest minimum inter-arrival time of any Class I or II transaction. <p> In this appendix, we will review the dynamic slack stealing algorithms developed by Davis et al <ref> [14, 12] </ref>. In Section A.1, we outline the computational model and assumptions used. Section A.2 presents analysis of the maximum amount of slack which may be stolen at each priority level. This is used as the basis for the optimal dynamic algorithm described in Section A.3. <p> A very short period minimizes the deadline miss ratio of non-guaranteed transactions at the expense of a large 144 overhead. While a very long period minimizes the overhead and increases the deadline miss ratio. This trade off has been further examined in <ref> [12] </ref>. Hyperperiod Approximate Slack Stealing Algorithm The overheads of the PASS algorithm are clearly dependent on its period and the cardi-nality of the transaction set T G .
Reference: [13] <author> R. I. Davis. </author> <title> Scheduling Slack Time in Fixed Priority Preemptive Systems. </title> <type> Technical Report YCS216, </type> <institution> Department of Computer Science, University of York, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Further, the dynamic algorithm is able to improve the response times of soft tasks by exploiting run-time information about hard task execution requirements, blocking and context switch times <ref> [13] </ref>. Unfortunately, the execution time overhead of the optimal dynamic algorithm is such that it is infeasible in practice. Approximate methods of determining slack presented by Davis [12] address the space and time complexity problems inherent in the optimal algorithms.
Reference: [14] <author> R. I. Davis, K. W. Tindell, and A. Burns. </author> <title> Scheduling Slack Time in Fixed Priority Pre-emptive Systems. </title> <booktitle> In Proceedings of the 14th Real-Time Systems Symposium, </booktitle> <pages> pages 222-231, </pages> <address> Raleigh-Durham, NC, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: This is a significant restriction, as even modest task sets (e.g., 10 tasks) may have very long hyperperiods. The limitations inherent in the optimal static algorithm are addressed by the dynamic slack stealer developed by Davis et al <ref> [14] </ref>. By virtue of computing slack at run time, the optimal dynamic algorithm is applicable to a more general class of scheduling problems, including task sets which contain hard deadline sporadics and tasks which exhibit release jitter and synchronization. <p> approach reviewed in Appendix A, which is shown to have better flexibility and performance than other approaches, such as background processing, the Sporadic Server algorithm [67], the Extended Priority Exchange algorithm [68] and the optimal static slack stealing algorithm [48]. 54 Different from the original dynamic slack stealing presented in <ref> [14] </ref>, our algorithm accounts the scheduler overheads included in our extended schedulability analysis. <p> Note that the soft deadline task model used in the original slack stealing studies <ref> [48, 14, 12] </ref> is different from our Class III transaction model in that its performance goal is to minimize their average response times. <p> In this appendix, we will review the dynamic slack stealing algorithms developed by Davis et al <ref> [14, 12] </ref>. In Section A.1, we outline the computational model and assumptions used. Section A.2 presents analysis of the maximum amount of slack which may be stolen at each priority level. This is used as the basis for the optimal dynamic algorithm described in Section A.3. <p> Inequality (A.3) is then used to determine if non-guaranteed transaction processing can proceed immediately in preference to t k . This dynamic slack stealing algorithm has been proved optimal in <ref> [14] </ref>. Note that the dynamic algorithm, described above, potentially requires the slack at each priority level to be re-computed at each time increment.
Reference: [15] <author> D. J. DeWitt et al. </author> <title> Implementation Techniques for Main Memory Database Systems. </title> <booktitle> In Proc. ACM SIGMOD Conference, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: This is especially important for real-time applications. Since memory prices are steadily dropping, and memory sizes are growing, memory residence of a real-time database becomes less of a restriction. During the 1980's, a good deal of research investigated the effects of the availability of very large main memories <ref> [15, 45, 44, 46, 58, 17] </ref>. The early work considered effects on query processing strategies, data structures, and failure recovery mechanisms when a substantial percentage of the database could fit into the DBMS buffer pool. Other researchers assume that the entire database can be made main memory resident.
Reference: [16] <author> H. Diel et al. </author> <title> Data Management Facilities of an Operating System Kernel. </title> <booktitle> In Proc. ACM SIGMOD Conference, </booktitle> <pages> pages 58-69, </pages> <address> Boston, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The major issue regarding the file system is the management of memory buffers in accessing data files. Some operating systems are moving toward providing file system services through virtual memory <ref> [77, 16] </ref>. In this architecture, when a file is opened, it is mapped into a virtual memory segment and then read by referencing a page in virtual memory, which is then loaded by the operating system. <p> However, such services are only available to DBMS users and not to general clients of the operating system such as mail programs and text editors. There has been considerable interest in providing transactions as an operating system service <ref> [71, 72, 16, 18] </ref>, and several OSs have done exactly that [27, 38, 11].
Reference: [17] <author> M. H. Eich. </author> <title> MARS: The Design of a Main Memory Database Machine. </title> <booktitle> In Proc. of the International Workshop on Database Machines, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: This is especially important for real-time applications. Since memory prices are steadily dropping, and memory sizes are growing, memory residence of a real-time database becomes less of a restriction. During the 1980's, a good deal of research investigated the effects of the availability of very large main memories <ref> [15, 45, 44, 46, 58, 17] </ref>. The early work considered effects on query processing strategies, data structures, and failure recovery mechanisms when a substantial percentage of the database could fit into the DBMS buffer pool. Other researchers assume that the entire database can be made main memory resident.
Reference: [18] <author> J. L. Eppinger. </author> <title> Virtual Memory Management for Transaction Processing Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> February </month> <year> 1989. </year> <note> Also available as technical report CMU-CS-89-115. </note>
Reference-contexts: However, such services are only available to DBMS users and not to general clients of the operating system such as mail programs and text editors. There has been considerable interest in providing transactions as an operating system service <ref> [71, 72, 16, 18] </ref>, and several OSs have done exactly that [27, 38, 11]. <p> Current attempts to implement these services in the conventional operating systems (e.g., Camelot <ref> [18] </ref>) are not adequate for real-time transaction processing systems. First, the access to the virtual memory mapped data objects is unpredictable, since a page in a data object sometimes resides in main memory buffer and sometimes not, depending on the OS paging mechanism. <p> Even the most advanced recoverable virtual memory management systems such as Camelot <ref> [18] </ref> do not support hard real-time applications well, 96 mainly due to their conventional rollback recovery mechanisms. We must develop a new abstraction for real-time data objects, giving a predictable behavior, and define a set of interface functions to access them.
Reference: [19] <author> David W. George. </author> <title> Implementation of indexing and concurrency control mechanisms in a real time database. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: StarBase can support both disk and in-memory relations but is geared towards the memory-resident relations. The T Tree was selected as the indexing method because of its greater performance for in-memory data <ref> [19] </ref>. 7.1.1 T Tree Index Structure The T Tree is a new balanced tree structure that evolved from AVL and B Trees both of which have certain merits for use in main memory.
Reference: [20] <author> Michel Gien. </author> <title> Micro-kernel Architecture Key to Modern Systems Design. Unix Review, </title> <month> November </month> <year> 1990. </year>
Reference-contexts: In contrast to the traditional mono-lithic operating system kernel, a microkernel provides system servers with generic services independent of a particular operating system, including (real-time) scheduling of one or more processors, memory management, and a simple IPC (Interprocess Communication) facility <ref> [20, 21, 76, 73] </ref>. This combination of elementary services forms a standard base which can support all other system-specific functions. <p> If the search is unsuccessful, a buffer fault occurs which is serviced by bringing the data page from secondary storage and loading into a buffer page. 30 CHORUS <ref> [20] </ref>, and then develop predictable database management functions (e.g., trans-action manager, recovery manager, etc.) based on this interface. Note that making transaction execution times predictable through an adequate architecture and OS support does not guarantee that the deadline of a transaction will be met.
Reference: [21] <author> M. Guillemont. </author> <title> Microkernel Design Yields Real Time in a Distributed Environment. </title> <journal> Computer Technology Review, </journal> <pages> pages 13-19, </pages> <month> Winter </month> <year> 1990. </year>
Reference-contexts: In contrast to the traditional mono-lithic operating system kernel, a microkernel provides system servers with generic services independent of a particular operating system, including (real-time) scheduling of one or more processors, memory management, and a simple IPC (Interprocess Communication) facility <ref> [20, 21, 76, 73] </ref>. This combination of elementary services forms a standard base which can support all other system-specific functions.
Reference: [22] <author> R. B. Hagmann. </author> <title> A Crash Recovery Scheme for a Memory-Resident Database System. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(9):839-843, </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: The research cited above suggests that algorithms specialized for memory resident databases offer considerably improved performance over conventional DBMS with very large buffer pools. Much of the work in memory resident databases concentrate on the recovery aspects of the system <ref> [46, 58, 22, 35] </ref>, since in MRDSs the recovery becomes more complex than disk-based database recovery mainly due to the volatility of main memory and the elimination of the separation of data storage and data processing location. <p> If the checkpointing method guarantees consistency of the checkpoint, then Restart reads less log information that it would have to if a fuzzy checkpointing 128 scheme <ref> [22] </ref> were used. In the presence of a secondary processor, the method of choice seems to be the non-interfering checkpoint method [65], since the overhead involved in generating a commit consistent checkpoint is taken care of by the recovery processor.
Reference: [23] <author> J. Haritsa, M. Carey, and M. Livny. </author> <title> Dynamic Real-Time Optimistic Concurrency Control. </title> <booktitle> In Proceedings of the 11th Real-Time Systems Symposium, </booktitle> <pages> pages 94-103, </pages> <address> Orlando, FL, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Most previous work assumes only soft or firm deadline transactions in a real-time database system <ref> [1, 23, 25, 28, 52] </ref>. In such systems, the transaction scheduler is usually supposed to have no idea about a transaction's computing time and resource requirement in advance. <p> The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance. <p> the total value of transactions completed by their deadlines, when transactions have different values. 25 This class of algorithms are High Priority (or Priority Abort ) [29, 1], Priority Inheri--tance (or Wait Promote) [30, 28, 1] protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols <ref> [24, 23] </ref> with optimistic concurrency control techniques, and hybrid concurrency control algorithms [52, 63]. Some soft or firm real-time transaction scheduling algorithms utilize the worst case execution times of transactions, without the knowledge of data requirements. <p> In this section, we present a semantic concurrency control and conflict resolution scheme under our real-time database model. Our scheme is based on an optimistic real-time concurrency control algorithm using Precise Serialization (OCC-PS) developed by Lee and Son [41]. According to the recent studies in <ref> [24, 23, 25] </ref>, optimistic approach appears 56 well-suited to real-time database systems. Especially, OCC-PS is shown to outperform other real-time optimistic concurrency control algorithms, and more importantly, it can be easily integrated with non-serializable transaction scheduling.
Reference: [24] <author> J. Haritsa, M. Carey, and M. Livny. </author> <title> On Being Optimistic About Real-Time Constraints. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Database Systems, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: the total value of transactions completed by their deadlines, when transactions have different values. 25 This class of algorithms are High Priority (or Priority Abort ) [29, 1], Priority Inheri--tance (or Wait Promote) [30, 28, 1] protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols <ref> [24, 23] </ref> with optimistic concurrency control techniques, and hybrid concurrency control algorithms [52, 63]. Some soft or firm real-time transaction scheduling algorithms utilize the worst case execution times of transactions, without the knowledge of data requirements. <p> Furthermore, the well-known debate regarding the performance of locking based con-currency control protocols versus optimistic ones has surfaced in the field of real-time database systems. One group of researchers has reported an optimistic concurrency control algorithm that can generally perform better than locking based protocol <ref> [24] </ref>. A different group of researchers has reported the opposite [31]. <p> In this section, we present a semantic concurrency control and conflict resolution scheme under our real-time database model. Our scheme is based on an optimistic real-time concurrency control algorithm using Precise Serialization (OCC-PS) developed by Lee and Son [41]. According to the recent studies in <ref> [24, 23, 25] </ref>, optimistic approach appears 56 well-suited to real-time database systems. Especially, OCC-PS is shown to outperform other real-time optimistic concurrency control algorithms, and more importantly, it can be easily integrated with non-serializable transaction scheduling.
Reference: [25] <author> J. R. Haritsa. </author> <title> Transaction Scheduling in Firm Real-Time Database Systems. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <month> August </month> <year> 1991. </year> <month> 147 </month>
Reference-contexts: Most previous work assumes only soft or firm deadline transactions in a real-time database system <ref> [1, 23, 25, 28, 52] </ref>. In such systems, the transaction scheduler is usually supposed to have no idea about a transaction's computing time and resource requirement in advance. <p> In such systems, the transaction scheduler is usually supposed to have no idea about a transaction's computing time and resource requirement in advance. Their justification can be stated as follows <ref> [70, 25] </ref>: Database systems for efficiently supporting hard deadline real-time applications, where all transaction deadlines have to be met, appear infeasible. This is because there is usually a large variance between the average case and worst case execution time of a transaction. <p> The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance. <p> scheduling mechanism of a real-time database management system that utilizes such information and guarantees both consistency and timing constraints. 31 Chapter 3 Real-Time Database System Model Most real-time database scheduling algorithms have been developed and evaluated under almost the same workload and operating environment model used in conventional database systems <ref> [1, 25, 28, 64] </ref>. That is, transactions are assumed to arrive in a Poisson stream at a specified mean rate. Each transaction consists of a random sequence of pages to be read, a subset of which are updated. In addition, a conventional disk-based database environment is assumed. <p> However, we decide not to further categorize soft real-time transactions but to concentrate on Class I and Class II transactions. Much work has been already done for Class III transactions <ref> [1, 25, 28, 64] </ref>. Moreover, all Class III transactions are supposed to have the same level of importance (non-critical) and do not require an individual performance guarantee. <p> Most real-time database research uses the models which include only a subset of the above classes (e.g., fClass Ig [61, 4, 66] or fClass IIIg <ref> [1, 25, 28, 64] </ref>), and never discriminate among transactions in the system. However, in practice, all kinds of transactions can coexist in one system. <p> In this section, we present a semantic concurrency control and conflict resolution scheme under our real-time database model. Our scheme is based on an optimistic real-time concurrency control algorithm using Precise Serialization (OCC-PS) developed by Lee and Son [41]. According to the recent studies in <ref> [24, 23, 25] </ref>, optimistic approach appears 56 well-suited to real-time database systems. Especially, OCC-PS is shown to outperform other real-time optimistic concurrency control algorithms, and more importantly, it can be easily integrated with non-serializable transaction scheduling. <p> However, if a conflict occurs between two Class III transactions, it should be resolved based on their priorities (e.g., WAIT-50 <ref> [25] </ref> and Feasible Sacrifice [40]). Class III transactions under the SOCC algorithm must go through the validation phase and read phase as detailed in Figure 4.3 and Figure 4.4, respectively. Note that as explained above, Class I and Class II transactions do not have to carry out these phases. <p> StarBase uses no a priori information about transaction workload and discards tardy transactions at their deadline points. In order to realize many of these real-time goals, StarBase is constructed on top of RT-Mach, a real-time operating system developed by Carnegie Mellon University [76]. StarBase differs from previous RTDBS work <ref> [1, 25, 28] </ref>, in that a) it relies on a real-time operating system which provides priority-based scheduling and time-based synchronization, and b) it deals explicitly with data contention and deadline handling in addition to transaction scheduling, the traditional focus of simulation studies. 6.2.1 Database Overview The StarBase system is organized as <p> To resolve data conflicts, StarBase uses a concurrency control implementation which draws heavily from the work of two research groups. First, Haritsa reasoned that optimistic 102 concurrency control can outperform lock-based algorithms in a firm real-time setting <ref> [25] </ref>. He then developed a real-time optimistic concurrency control method, WAIT-X (S), which he found empirically superior, over a wide range of resource availability and system workload levels, to a previously proposed real-time lock-based concurrency control method called 2PL-HP [25]. <p> concurrency control can outperform lock-based algorithms in a firm real-time setting <ref> [25] </ref>. He then developed a real-time optimistic concurrency control method, WAIT-X (S), which he found empirically superior, over a wide range of resource availability and system workload levels, to a previously proposed real-time lock-based concurrency control method called 2PL-HP [25]. Second, Lee et al devised an improvement to the conflict detection of optimistic concurrency control in general (Precise Serialization) [41], which StarBase integrates with Haritsa's WAIT-X (S).
Reference: [26] <author> J. R. Haritsa, M. Livny, and M. J. Carey. </author> <title> Earliest Deadline Scheduling for Real-Time Database Systems. </title> <booktitle> In Proceedings of the 12th Real-Time Systems Symposium, </booktitle> <pages> pages 232-242, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance.
Reference: [27] <author> R. Haskin et al. </author> <title> Recovery Management in QuickSilver. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 82-108, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: However, such services are only available to DBMS users and not to general clients of the operating system such as mail programs and text editors. There has been considerable interest in providing transactions as an operating system service [71, 72, 16, 18], and several OSs have done exactly that <ref> [27, 38, 11] </ref>. Moreover, if a user wishes to have a cross-system transaction (i.e., one involving both a DBMS and another subsystem) then it is easily supported with an OS transaction manager but is nearly impossible with a DBMS-supplied one.
Reference: [28] <author> J. Huang. </author> <title> Real-Time Transaction Processing: Design, Implementation, and Performance Evaluation. </title> <type> PhD thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Most previous work assumes only soft or firm deadline transactions in a real-time database system <ref> [1, 23, 25, 28, 52] </ref>. In such systems, the transaction scheduler is usually supposed to have no idea about a transaction's computing time and resource requirement in advance. <p> The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance. <p> complete by its deadline, but try to minimize the deadline miss ratio of transactions or to maximize the total value of transactions completed by their deadlines, when transactions have different values. 25 This class of algorithms are High Priority (or Priority Abort ) [29, 1], Priority Inheri--tance (or Wait Promote) <ref> [30, 28, 1] </ref> protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols [24, 23] with optimistic concurrency control techniques, and hybrid concurrency control algorithms [52, 63]. <p> In this class of algorithms, Least Slack priority assignment policy can be used, combined with conflict resolution policies such as Conditional Restart [1] and Conditional Priority Inheritance <ref> [30, 28] </ref> under the two phase locking concurrency control scheme. However, some of the performance results for those algorithms are contradictory and there is no consensus on which are the best algorithms. <p> scheduling mechanism of a real-time database management system that utilizes such information and guarantees both consistency and timing constraints. 31 Chapter 3 Real-Time Database System Model Most real-time database scheduling algorithms have been developed and evaluated under almost the same workload and operating environment model used in conventional database systems <ref> [1, 25, 28, 64] </ref>. That is, transactions are assumed to arrive in a Poisson stream at a specified mean rate. Each transaction consists of a random sequence of pages to be read, a subset of which are updated. In addition, a conventional disk-based database environment is assumed. <p> However, we decide not to further categorize soft real-time transactions but to concentrate on Class I and Class II transactions. Much work has been already done for Class III transactions <ref> [1, 25, 28, 64] </ref>. Moreover, all Class III transactions are supposed to have the same level of importance (non-critical) and do not require an individual performance guarantee. <p> Most real-time database research uses the models which include only a subset of the above classes (e.g., fClass Ig [61, 4, 66] or fClass IIIg <ref> [1, 25, 28, 64] </ref>), and never discriminate among transactions in the system. However, in practice, all kinds of transactions can coexist in one system. <p> StarBase uses no a priori information about transaction workload and discards tardy transactions at their deadline points. In order to realize many of these real-time goals, StarBase is constructed on top of RT-Mach, a real-time operating system developed by Carnegie Mellon University [76]. StarBase differs from previous RTDBS work <ref> [1, 25, 28] </ref>, in that a) it relies on a real-time operating system which provides priority-based scheduling and time-based synchronization, and b) it deals explicitly with data contention and deadline handling in addition to transaction scheduling, the traditional focus of simulation studies. 6.2.1 Database Overview The StarBase system is organized as
Reference: [29] <author> J. Huang, J. A. Stankovic, et al. </author> <title> Experimental Evaluation of Real-Time Transaction Processing. </title> <booktitle> In Proceedings of the 10th Real-Time Systems Symposium, </booktitle> <address> Santa Monica, CA, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance. <p> they cannot guarantee that each transaction will complete by its deadline, but try to minimize the deadline miss ratio of transactions or to maximize the total value of transactions completed by their deadlines, when transactions have different values. 25 This class of algorithms are High Priority (or Priority Abort ) <ref> [29, 1] </ref>, Priority Inheri--tance (or Wait Promote) [30, 28, 1] protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols [24, 23] with optimistic concurrency control techniques, and hybrid concurrency control algorithms [52, 63].
Reference: [30] <author> J. Huang, J. A. Stankovic, et al. </author> <title> On Using Priority Inheritance in Real-Time Databases. </title> <type> Technical Report COINS TR 90-121, </type> <institution> University of Massachusetts, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: complete by its deadline, but try to minimize the deadline miss ratio of transactions or to maximize the total value of transactions completed by their deadlines, when transactions have different values. 25 This class of algorithms are High Priority (or Priority Abort ) [29, 1], Priority Inheri--tance (or Wait Promote) <ref> [30, 28, 1] </ref> protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols [24, 23] with optimistic concurrency control techniques, and hybrid concurrency control algorithms [52, 63]. <p> In this class of algorithms, Least Slack priority assignment policy can be used, combined with conflict resolution policies such as Conditional Restart [1] and Conditional Priority Inheritance <ref> [30, 28] </ref> under the two phase locking concurrency control scheme. However, some of the performance results for those algorithms are contradictory and there is no consensus on which are the best algorithms. <p> For example, it is reported in [1] that concurrency control policies that combine blocking with priority inheritance (Wait Promote, Conditional Restart) generally perform better than a policy that aborts lower priority transactions in order to favor higher priority ones (High Priority), whereas the opposite to this result is reported in <ref> [30] </ref>. The reason for this may lie in the fact that they used different environments and operating ranges to evaluate the algorithms.
Reference: [31] <author> J. Huang, J. A. Stankovic, et al. </author> <title> Experimental Evaluation of Real-Time Optimistic Concurrency Control Schemes. </title> <type> Technical Report COINS TR 91-16, </type> <institution> University of Mas-sachusetts, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: One group of researchers has reported an optimistic concurrency control algorithm that can generally perform better than locking based protocol [24]. A different group of researchers has reported the opposite <ref> [31] </ref>.
Reference: [32] <author> D. I. Katcher, H. Arakawa, and J. K. Strosnider. </author> <title> Engineering and Analysis of Fixed Priority Schedulers. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(9), </volume> <month> September </month> <year> 1993. </year>
Reference-contexts: In the following, we present our extended schedulability analysis based on the analysis in [9, 10]. The parameters and notations used in the analysis are summarized in Table 4.1. In our RTDBS model, transactions are supposed to be scheduled by a timer-driven 51 scheduler <ref> [32] </ref> (i.e., the scheduler is invoked by a regular timing interrupt with a period denoted by P clk ).
Reference: [33] <author> Young-Kuk Kim and Sang H. Son. </author> <title> An Approach Towards Predictable Real-Time Transaction Processing. </title> <booktitle> In Proceedings of the 5th Euromicro Workshop on Real-Time Systems, </booktitle> <pages> pages 70-75, </pages> <address> Oulu, Finland, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The system then has only to provide a guarantee on the timing constraints, since as long as the corresponding transactions meet their deadlines, the temporal consistency of the data objects accessed by the transactions is automatically maintained <ref> [33, 51, 54] </ref>.
Reference: [34] <author> T. Kitayama, T. Nakajima, and H. Tokuda. RT-IPC: </author> <title> An IPC Extension for Real-Time Mach. </title> <type> Technical report, </type> <institution> Carnegie-Mellon University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The timing and priority information is then used as input to the RT-Mach scheduler. RT-Mach also has striven to implement priority-based resource scheduling through its interprocess communication (RT-IPC) <ref> [34] </ref> and thread synchronization (RT-Sync) [75] facilities. RT-Mach implements BPI itself as a combination of priority queuing and priority inheritance. <p> Transaction manager priorities are not specified explicitly by StarBase, however. Each obtains the correct priority assignment automatically upon receipt of a new transaction via RT-IPC's priority handoff mechanism <ref> [34] </ref>. Memory Manager Transactions, depending on the nature of their operations, require some dynamic allocation of memory during their execution. StarBase maintains a Small Memory Manager to allocate and manage dynamic memory.
Reference: [35] <author> V. Kumar and A. Burger. </author> <title> Performance Measurement of Some Main Memory Database Recovery Algorithms. </title> <journal> In IEEE Transactions on Knowledge and Data Engineering, </journal> <pages> pages 436-443, </pages> <year> 1991. </year>
Reference-contexts: The research cited above suggests that algorithms specialized for memory resident databases offer considerably improved performance over conventional DBMS with very large buffer pools. Much of the work in memory resident databases concentrate on the recovery aspects of the system <ref> [46, 58, 22, 35] </ref>, since in MRDSs the recovery becomes more complex than disk-based database recovery mainly due to the volatility of main memory and the elimination of the separation of data storage and data processing location.
Reference: [36] <author> Tei-Wei Kuo and Aloysius K. Mok. </author> <title> SSP: A Semantics-Based Protocol for Real-Time Data Access. </title> <booktitle> In Proceedings of the 14th Real-Time Systems Symposium, </booktitle> <pages> pages 76-86, </pages> <address> Raleigh-Durham, NC, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: In a dynamic approach, the system 41 keeps checking the temporal consistency at run time and tries to meet them dynamically, by either using multiple versions of data objects [66] or delaying some transactions in favor of more urgent transactions in terms of temporal consistency enforcement at the specific moment <ref> [36] </ref>. In this section, we present a static approach which we call Static Temporal Consistency Enforcement (STCE) scheme for our real-time database system model.
Reference: [37] <author> Averill M. Law and W. David Kelton. </author> <title> Simulation Modeling & Analysis. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1991. </year>
Reference-contexts: The value of each attribute is generated using an appropriate distribution function, for example, uniform, triangular or Pearson type V <ref> [37] </ref>. The arrival times of the Class III transactions follow an exponential random distribution over the test duration. The number of Class III transactions is varied to produce a range of total processor utilization levels (plotted on the x-axis of the graphs).
Reference: [38] <author> E. F. Lazowska et al. </author> <title> The Architecture of the Eden System. </title> <booktitle> In Proceedings of the 8th Symposium on Operating System Principles, </booktitle> <pages> pages 148-159, </pages> <address> Pacific Grove, Calif., </address> <month> December </month> <year> 1981. </year> <journal> ACM. </journal> <volume> 148 </volume>
Reference-contexts: However, such services are only available to DBMS users and not to general clients of the operating system such as mail programs and text editors. There has been considerable interest in providing transactions as an operating system service [71, 72, 16, 18], and several OSs have done exactly that <ref> [27, 38, 11] </ref>. Moreover, if a user wishes to have a cross-system transaction (i.e., one involving both a DBMS and another subsystem) then it is easily supported with an OS transaction manager but is nearly impossible with a DBMS-supplied one.
Reference: [39] <author> J. Lee and S. H. Son. </author> <title> Using Dynamic Adjustment of Serialization Order for Real-Time Database Systems. </title> <booktitle> In Proceedings of the 14th Real-Time Systems Symposium, </booktitle> <pages> pages 66-75, </pages> <address> Raleigh-Durham, NC, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: This allows the system to discard infeasible transactions (i.e., transactions which may not complete before their deadlines) even before they begin execution so that wasted computations, aborts, and restarts can be avoided <ref> [1, 39] </ref>. 1.2.2 Correctness Criteria Another limitation of current real-time transaction scheduling algorithms is that most of them rely on serializability to preserve the logical consistency of the database, but they fail to address how to maintain temporal consistency of real-time data. <p> The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance.
Reference: [40] <author> J. Lee and S. H. Son. </author> <title> Deadline-Sensitive Conflict Resolution for Real-Time Optimistic Concurrency Control. </title> <note> submitted for publication, </note> <year> 1994. </year>
Reference-contexts: However, if a conflict occurs between two Class III transactions, it should be resolved based on their priorities (e.g., WAIT-50 [25] and Feasible Sacrifice <ref> [40] </ref>). Class III transactions under the SOCC algorithm must go through the validation phase and read phase as detailed in Figure 4.3 and Figure 4.4, respectively. Note that as explained above, Class I and Class II transactions do not have to carry out these phases.
Reference: [41] <author> J. Lee and S. H. Son. </author> <title> Precise Serialization for an Optimistic Concurrency Control Algorithm. </title> <note> submitted for publication, </note> <year> 1994. </year>
Reference-contexts: In this section, we present a semantic concurrency control and conflict resolution scheme under our real-time database model. Our scheme is based on an optimistic real-time concurrency control algorithm using Precise Serialization (OCC-PS) developed by Lee and Son <ref> [41] </ref>. According to the recent studies in [24, 23, 25], optimistic approach appears 56 well-suited to real-time database systems. Especially, OCC-PS is shown to outperform other real-time optimistic concurrency control algorithms, and more importantly, it can be easily integrated with non-serializable transaction scheduling. <p> Second, Lee et al devised an improvement to the conflict detection of optimistic concurrency control in general (Precise Serialization) <ref> [41] </ref>, which StarBase integrates with Haritsa's WAIT-X (S). Detailed descriptions on their implementation for StarBase can be found in [49]. 6.2.3 Summary We outlines the architecture to support firm deadline transactions assuming no a priori knowledge of transaction workload characteristics.
Reference: [42] <author> S. J. Le*er, M. K. McKusick, M. J. Karels, and J. S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: The first requirement demands precise performance monitoring software which typical operating systems do not provide. Operating systems usually accumulate usage statistics for each process by sampling during regular clock interrupts <ref> [42] </ref>, but this information is not very precise over short intervals. Furthermore, the execution behavior of the monitored program must be independent of the sampling period. A more precise mechanism would measure durations between context switches and would account for interrupt processing time and other "system overhead". <p> Even if the system can accurately measure capacity consumption on a per-process basis, other problems arise. Usage statistics in traditional operating systems consist of system-level usage time and user-level time for each process <ref> [42] </ref>. For monolithic operating systems, this approach is sufficient, but for microkernel systems where operating system services are offered by different user-level servers, the usage statistics of an activity cannot found in the usage statistics of a single process.
Reference: [43] <author> T. J. Lehman. </author> <title> Design and Performance Evaluation of a Main Memory Database System. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: In the following subsections, we will discuss the memory-resident database recovery approach proposed in [46], which will be integrated with the slack stealing approach, and present a composite recovery scheme. Further, we will briefly discuss other possible approaches to recovery. 7.2.1 A Memory-Resident DBMS Structure In <ref> [43] </ref>, a complete architecture for a memory-resident relational DBMS is presented, with a new index structure, new recovery methods and possibly new concurrency control approaches.
Reference: [44] <author> T. J. Lehman and M. J. Carey. </author> <title> A Study of Index Structures for Main Memory Database Management Systems. </title> <booktitle> In Proc. 12th Conf. on Very Large Data Bases, </booktitle> <pages> pages 294-303, </pages> <address> Kyoto, Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: This is especially important for real-time applications. Since memory prices are steadily dropping, and memory sizes are growing, memory residence of a real-time database becomes less of a restriction. During the 1980's, a good deal of research investigated the effects of the availability of very large main memories <ref> [15, 45, 44, 46, 58, 17] </ref>. The early work considered effects on query processing strategies, data structures, and failure recovery mechanisms when a substantial percentage of the database could fit into the DBMS buffer pool. Other researchers assume that the entire database can be made main memory resident. <p> It may be noted that the graphs shown do not include the time taken for the construction of the T Trees as Tree Merge is a viable solution only if the indices already exist. A study undertaken in <ref> [44] </ref> reports that the arrays can be built and sorted in sixty percent of the time it would take to build the trees. <p> If not the performance of the other algorithms is better than the Tree Merge. As is reported in <ref> [44] </ref>, arrays can be built and sorted in 60% of the time to build trees, and also arrays can be scanned in about 60% of the time taken to scan a tree.
Reference: [45] <author> T. J. Lehman and M. J. Carey. </author> <title> Query Processing in Main Memory Database Management Systems. </title> <booktitle> In Proc. ACM SIGMOD Conference, </booktitle> <pages> pages 239-250, </pages> <address> Washington D.C., </address> <month> May </month> <year> 1986. </year>
Reference-contexts: This is especially important for real-time applications. Since memory prices are steadily dropping, and memory sizes are growing, memory residence of a real-time database becomes less of a restriction. During the 1980's, a good deal of research investigated the effects of the availability of very large main memories <ref> [15, 45, 44, 46, 58, 17] </ref>. The early work considered effects on query processing strategies, data structures, and failure recovery mechanisms when a substantial percentage of the database could fit into the DBMS buffer pool. Other researchers assume that the entire database can be made main memory resident. <p> In this chapter, we present our ideas for query processing and recovery mechanisms in the context of the memory-resident RTDBS. 7.1 Query Processing on Memory-Resident Databases A number of different indexing methods are applicable for use in database systems. In <ref> [45] </ref> an indexing structure for use in memory-resident databases, called the T Tree was introduced which was reported to perform significantly better than the existing indexing methods for memory-resident data retrieval. Memory-resident indexes are designed to reduce the overall computation time while minimizing the amount of memory needed. <p> The parameters that were variable in the study undertaken by <ref> [45] </ref> were: * The relation cardinality (jRj). 108 * The number of join column duplicate values (as a percentage of jRj). * The semijoin selectivity that is the number of values in the larger relation that participate in the join expressed as a percentage of the larger relation. <p> A study undertaken in [44] reports that the arrays can be built and sorted in sixty percent of the time it would take to build the trees. Based on the performance curves presented in <ref> [45] </ref> we conclude that if the T Tree has been built on the join columns in both the columns then Tree Merge is the best algorithm. If the tree indices is not built on the join columns of the two relations Hash join is the most efficient algorithm. <p> Chained Bucket Hashing is a static structure used both in memory and on disk. It is very fast because it is a static structure it never has to reorganize its data. This however requires that the size of the hash be appropriately estimated. Tests reported in <ref> [45] </ref> show that the optimal table size for a given number of relations has an 112 Algorithm Tree Merge (*R1, *R2) begin S1 = smallest element (struct tnode *R1); S2 = smallest element (struct tnode *R2); while S1&gt;val != NULL and S2&gt;val != NULL do /* find the nodes in the
Reference: [46] <author> T. J. Lehman and M. J. Carey. </author> <title> A Recovery Algorithm for a High-Performance Memory-Resident Database System. </title> <booktitle> In Proc. ACM SIGMOD Conference, </booktitle> <pages> pages 104-117, </pages> <address> San Francisco, CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: This is especially important for real-time applications. Since memory prices are steadily dropping, and memory sizes are growing, memory residence of a real-time database becomes less of a restriction. During the 1980's, a good deal of research investigated the effects of the availability of very large main memories <ref> [15, 45, 44, 46, 58, 17] </ref>. The early work considered effects on query processing strategies, data structures, and failure recovery mechanisms when a substantial percentage of the database could fit into the DBMS buffer pool. Other researchers assume that the entire database can be made main memory resident. <p> The research cited above suggests that algorithms specialized for memory resident databases offer considerably improved performance over conventional DBMS with very large buffer pools. Much of the work in memory resident databases concentrate on the recovery aspects of the system <ref> [46, 58, 22, 35] </ref>, since in MRDSs the recovery becomes more complex than disk-based database recovery mainly due to the volatility of main memory and the elimination of the separation of data storage and data processing location. <p> The first, a memory-resident database design proposed in <ref> [46] </ref>, uses a dedicated recovery processor to handle all recovery tasks that need disk access. The second is a slack stealing framework discussed in Section 4.2.2 and Appendix A. Our aim is to integrate these two approaches and provide a recovery mechanism that guarantees a certain level of deadline-cognizance. <p> The second is a slack stealing framework discussed in Section 4.2.2 and Appendix A. Our aim is to integrate these two approaches and provide a recovery mechanism that guarantees a certain level of deadline-cognizance. In the following subsections, we will discuss the memory-resident database recovery approach proposed in <ref> [46] </ref>, which will be integrated with the slack stealing approach, and present a composite recovery scheme. <p> Restart usually takes time of the order of hours to complete. To amortize interference of the Restart process with normal transaction processing, a "recovery on demand" approach is proposed in <ref> [46] </ref>. During its initialization phase, each transaction declares the set of relations and indices that it will need. The transaction manager checks the relation catalog to see if they are memory resident. If they are not, it initiates a set of recovery transactions to recover them, one per partition. <p> In addition, recovery operations must not be responsible for transaction deadline misses, or at least the deadline miss percentage must be minimized. In this subsection, we propose a method to integrate the recovery methods discussed in <ref> [46] </ref> with the slack stealing framework described in Section 4.2.2, to introduce a certain level of deadline cognizance into the recovery techniques. As seen in the previous section, the logging process does not result in any perceptible overhead for the main CPU. <p> If the failures are very frequent, then Restart would have to be optimized; i.e., commit consistent checkpoints are desirable. However, if failures are few and far between, then a more efficient checkpointing mechanism is desirable. In such a case the partition-level checkpointing scheme <ref> [46] </ref> would be most appropriate. 7.2.4 Temporal Constraints Typically, a real time system consists of a controlling system and a controlled system.
Reference: [47] <author> J. P. Lehoczky. </author> <title> Real-Time Resource Managenment Techniques. </title> <editor> In J. J. Marciniak, editor, </editor> <booktitle> Encyclopedia of Software Engineering, </booktitle> <pages> pages 1011-1020. </pages> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Both approaches are becoming increasingly well developed, although at the present time the static priority theory is much more complete. A 18 summary of the results available on fixed-priority scheduling can be found in review articles by Burns [8] and Lehoczky <ref> [47] </ref>. Given the success of fixed priority scheduling methods, it is natural to attempt to extend this theory to solve other important problems that arise in real-time systems.
Reference: [48] <author> J. P. Lehoczky and S. Ramos-Thuel. </author> <title> An Optimal Algorithm for Scheduling Soft-Aperiodic Tasks in Fixed-Priority Preemptive Systems. </title> <booktitle> In Proceedings of the 13th Real-Time Systems Symposium, </booktitle> <pages> pages 110-123, </pages> <address> Phoenix, AZ, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: This spare capacity, termed gain time, can however be reclaimed by the Extended Priority Exchange algorithm. Slack Stealing Algorithms The static slack stealing algorithm of Lehoczky and Ramos-Thuel <ref> [48] </ref> suffers from none of these disadvantages. It is optimal in the sense that it minimizes the response times of soft tasks among all algorithms which meet all hard periodic task deadlines. The slack stealer services soft requests by making any spare processing time available as soon as possible. <p> In doing so, it effectively steals slack from the hard deadline periodic tasks. A means of determining the maximum amount of slack which may be stolen, without jeopardizing the hard timing constraints, is therefore key to the operation of the algorithm. In <ref> [48] </ref>, Lehoczky and Ramos-Thuel describe how the slack available can be found. This is done by mapping out the processor schedule for the hard periodic tasks over their hyperperiod (the least common multiple of task periods). <p> approach to scheduling non-guaranteed transactions is based on dynamic slack stealing approach reviewed in Appendix A, which is shown to have better flexibility and performance than other approaches, such as background processing, the Sporadic Server algorithm [67], the Extended Priority Exchange algorithm [68] and the optimal static slack stealing algorithm <ref> [48] </ref>. 54 Different from the original dynamic slack stealing presented in [14], our algorithm accounts the scheduler overheads included in our extended schedulability analysis. <p> Note that the soft deadline task model used in the original slack stealing studies <ref> [48, 14, 12] </ref> is different from our Class III transaction model in that its performance goal is to minimize their average response times.
Reference: [49] <author> Matthew R. Lehr and Sang H. Son. </author> <title> Managing Contention and Timing Constraints in a Real-Time Database System. </title> <type> Technical Report CS-94-19, </type> <institution> University of Virginia, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: That is, they are viable only when the underlying subsystems provide deterministic services to transactions. In this chapter, we discuss how such a platform can be provided, making use of the current real-time microkernel technology and memory-resident databases. We also investigate how our experimental real-time database testbed, called StarBase <ref> [49] </ref>, which is currently supporting only firm deadline transactions, can be extended to provide predictable transaction processing. First, we propose a conceptual structure for achieving a deterministic system. Then, we briefly describe the current status of StarBase. <p> Second, Lee et al devised an improvement to the conflict detection of optimistic concurrency control in general (Precise Serialization) [41], which StarBase integrates with Haritsa's WAIT-X (S). Detailed descriptions on their implementation for StarBase can be found in <ref> [49] </ref>. 6.2.3 Summary We outlines the architecture to support firm deadline transactions assuming no a priori knowledge of transaction workload characteristics. Unlike previous simulation studies, Star-Base uses a real-time operating system to provide basic real-time functionality and deals with issues beyond transaction scheduling: resource contention, data contention, and enforcing deadlines.
Reference: [50] <author> K.-J. Lin. </author> <title> Consistency Issues in Real-Time Database Systems. </title> <booktitle> In Proceedings of the 22nd Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: The concept of "temporal consistency" suggests that the age of data should be taken into account in making scheduling decisions. Specifying bounds on the start time of one transaction relative to the stop time of one or more transactions is a new form of timing constraint <ref> [50] </ref>. Temporal consistency can reference either the absolute age of the data read by a transaction, or the age of each data item relative to the age of every other data item in the read set of a transaction [66]. <p> We observe that no transaction scheduling algorithms proposed so far completely satisfy all these requirements even though several papers in the real-time database field have pointed them out <ref> [66, 4, 50, 5] </ref>.
Reference: [51] <author> K.-J. Lin, F. Jahanian, A. Jhingran, and C. D. Locke. </author> <title> A Model of Hard Real-Time Transaction Systems. </title> <type> Technical Report RC No. 17515, </type> <institution> IBM T. J. Watson Research Center, </institution> <month> January </month> <year> 1992. </year> <month> 149 </month>
Reference-contexts: For those applications, a real-time database system must provide mechanisms to minimize the execution time variance of a transaction, making the system's behavior predictable. There has been some work dealing with hard deadline transactions <ref> [59, 66, 4, 51] </ref>, but impractical assumptions have often been made (for example, all transactions are periodic and their 1 Firm deadline transactions are special cases of soft deadline transactions which have no value after their deadlines. 5 worst-case execution times are given) or different correctness and performance criteria for hard <p> The system then has only to provide a guarantee on the timing constraints, since as long as the corresponding transactions meet their deadlines, the temporal consistency of the data objects accessed by the transactions is automatically maintained <ref> [33, 51, 54] </ref>.
Reference: [52] <author> Y. Lin and S. H. Son. </author> <title> Concurrency Control in Real-Time Databases by Dynamic Adjustment of Serialization Order. </title> <booktitle> In Proceedings of the 11th Real-Time Systems Symposium, </booktitle> <pages> pages 94-103, </pages> <address> Orlando, FL, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Most previous work assumes only soft or firm deadline transactions in a real-time database system <ref> [1, 23, 25, 28, 52] </ref>. In such systems, the transaction scheduler is usually supposed to have no idea about a transaction's computing time and resource requirement in advance. <p> The primary goal of those research efforts is to minimize the deadline miss ratio of transactions <ref> [1, 39, 7, 23, 26, 25, 29, 28, 52] </ref>. However, less attention has been paid to architectural and operating system aspects of the system which support the predictable behavior of a real-time transaction. Without adequate support from the underlying subsystems, none of the scheduling algorithms can guarantee predictable transaction performance. <p> different values. 25 This class of algorithms are High Priority (or Priority Abort ) [29, 1], Priority Inheri--tance (or Wait Promote) [30, 28, 1] protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols [24, 23] with optimistic concurrency control techniques, and hybrid concurrency control algorithms <ref> [52, 63] </ref>. Some soft or firm real-time transaction scheduling algorithms utilize the worst case execution times of transactions, without the knowledge of data requirements. Intuitively, this information can help the scheduler to determine eligibility of transactions, reducing the wasted CPU time and recovery overhead due to aborted transactions.
Reference: [53] <author> C. L. Liu and J. W. Layland. </author> <title> Scheduling Algorithms for Multiprogramming in a Hard Real-Time Environment. </title> <journal> Journal of the ACM, </journal> <volume> 20(1) </volume> <pages> 46-61, </pages> <year> 1973. </year>
Reference-contexts: For this case, there are two popular approaches: (1) static or fixed-priority algorithms, including the rate-monotonic and deadline-monotonic algorithms <ref> [53, 3] </ref> and (2) dynamic priority algorithms, including the earliest deadline algorithm [53]. Both approaches are becoming increasingly well developed, although at the present time the static priority theory is much more complete. <p> For this case, there are two popular approaches: (1) static or fixed-priority algorithms, including the rate-monotonic and deadline-monotonic algorithms [53, 3] and (2) dynamic priority algorithms, including the earliest deadline algorithm <ref> [53] </ref>. Both approaches are becoming increasingly well developed, although at the present time the static priority theory is much more complete. A 18 summary of the results available on fixed-priority scheduling can be found in review articles by Burns [8] and Lehoczky [47]. <p> Thus, the process with the shortest deadline is assigned the highest priority and the longest deadline process is assigned the lowest priority. This priority ordering defaults to a rate-monotonic ordering <ref> [53] </ref> when the period equals to the deadline. Class IA Transactions A Class IA transaction t x is responsible for maintaining the absolute temporal consistency of an image object x. <p> RT-Mach's services in turn are based on two major ideas (among others) which have been developed to ensure the allocation of resources to more important tasks in real-time systems. Those ideas are priority-based CPU scheduling <ref> [53] </ref> and the Basic Priority Inheritance Protocol (BPI) [60] for non-preemptible resources. With both ideas, tasks to be performed are ranked by their relative priorities (a function of their criticality and/or feasibility), and the highest priority tasks are granted access to the resource in question.
Reference: [54] <author> H. Nakazato. </author> <title> Issues on Synchronization and Scheduling Tasks in Real-Time Database Systems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1993. </year> <note> Also available as UIUCDCS-R-93-1786. </note>
Reference-contexts: The system then has only to provide a guarantee on the timing constraints, since as long as the corresponding transactions meet their deadlines, the temporal consistency of the data objects accessed by the transactions is automatically maintained <ref> [33, 51, 54] </ref>. <p> The conflicting Class III transaction must wait until the Class II transaction commits. Since the priority inversion problem due to shared data objects will not occur, no priority ceiling protocol (PCP)-based synchronization scheme <ref> [54] </ref> is necessary. Therefore, Class II transactions can also bypass the validation phase of SOCC and always commit.
Reference: [55] <author> P. E. O'Neil, K. Ramamritham, and C. Pu. </author> <title> Towards Predictable Transaction Executions in Real-Time Database Systems. </title> <type> Technical Report CS-TR-92-35, </type> <institution> University of Massachusetts at Amherst, </institution> <year> 1992. </year>
Reference-contexts: possible to get such information in advance, since, unlike the conventional real-time applications, there are more factors that 8 contribute to the unpredictability of transaction execution in database systems, such as in-teractions with indeterministic subsystems, data dependence of transaction execution, data and resource conflicts among transactions, and conventional recovery mechanisms <ref> [55] </ref>. Consequently, we cannot give this kind of predictability to all real-time transactions. Instead, we should provide a different level of guarantee to each class of real-time transactions.
Reference: [56] <author> Calton Pu. </author> <title> On-the-Fly, Incremental, Consistent Reading of Entire Databases. </title> <journal> Algo-rithmica, </journal> <volume> 1(4) </volume> <pages> 271-287, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Checkpointing In this subsection, two approaches to checkpointing in a memory-resident database that guarantee a certain degree of consistency are presented - Black/White Policies <ref> [56] </ref> and 127 an interference-free checkpoint mechanism surveyed in [65]. In the Black/White policy, the database is colored "read" or "not read yet" (by the checkpointer).
Reference: [57] <author> Krithi Ramamritham. </author> <title> Real-Time Databases. </title> <journal> International Journal of Distributed and Parallel Databases, </journal> <volume> 1(1), </volume> <year> 1992. </year>
Reference-contexts: Real-time system researchers have been attracted by the opportunity that real-time database systems can provide for building a complex real-time system with lots of time-sensitive data <ref> [57] </ref>. Although some of the necessary research has been done, many issues remain, especially seamless integration of real-time systems and the state-of-the-art database systems technology. The design and implementation of RTDBS introduces several interesting problems.
Reference: [58] <author> K. Salem and H. Garcia-Molina. </author> <title> System M: A Transaction Processing Testbed for Memory Resident Data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 161-172, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: This is especially important for real-time applications. Since memory prices are steadily dropping, and memory sizes are growing, memory residence of a real-time database becomes less of a restriction. During the 1980's, a good deal of research investigated the effects of the availability of very large main memories <ref> [15, 45, 44, 46, 58, 17] </ref>. The early work considered effects on query processing strategies, data structures, and failure recovery mechanisms when a substantial percentage of the database could fit into the DBMS buffer pool. Other researchers assume that the entire database can be made main memory resident. <p> The research cited above suggests that algorithms specialized for memory resident databases offer considerably improved performance over conventional DBMS with very large buffer pools. Much of the work in memory resident databases concentrate on the recovery aspects of the system <ref> [46, 58, 22, 35] </ref>, since in MRDSs the recovery becomes more complex than disk-based database recovery mainly due to the volatility of main memory and the elimination of the separation of data storage and data processing location.
Reference: [59] <author> L. Sha, R. Rajkumar, and J. Lehoczky. </author> <title> Concurrency Control for Distributed Real-Time Databases. </title> <journal> ACM SIGMOD Record, </journal> <volume> 17(1) </volume> <pages> 82-98, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: We agree that, under the conventional database management system's architecture and operating system environment, it is almost impossible to estimate the worst case behavior of a transaction correctly. Some real-time transaction scheduling algorithms <ref> [1, 59] </ref> that utilize a priori knowledge about transactions are unrealistic. However, there are many potential applications of hard real-time database systems in the real world, such as flight control systems and missile guidance systems. <p> For those applications, a real-time database system must provide mechanisms to minimize the execution time variance of a transaction, making the system's behavior predictable. There has been some work dealing with hard deadline transactions <ref> [59, 66, 4, 51] </ref>, but impractical assumptions have often been made (for example, all transactions are periodic and their 1 Firm deadline transactions are special cases of soft deadline transactions which have no value after their deadlines. 5 worst-case execution times are given) or different correctness and performance criteria for hard <p> Many such protocols have been developed in the context of real-time systems but they do not apply directly to real-time database systems. However, we can prevent the unbounded blocking of a transaction (e.g., priority ceiling protocol <ref> [59] </ref>), assuming that a priori knowledge about transactions is available, which is often the case in hard real-time applications, or avoid the blocking by allowing non-serializable schedule, which is acceptable for some types of real-time transactions. 13 Conventional recovery mechanisms In conventional database systems, recovery mechanisms after failure often depend on <p> This requires that many restrictions be placed on the structure and characteristics of real-time transactions. Only a few real-time transaction scheduling algorithms using this class of transaction model have been proposed so far. Sha et al proposed algorithms for scheduling a fixed set of periodic transactions with hard deadlines <ref> [59, 61] </ref>. Their model assumes that transaction priorities and resource requirements are known a priori. The rate-monotonic algorithm is used for determining transaction priority and scheduling the CPU. A priority ceiling protocol based on locking is used for concurrency control.
Reference: [60] <author> L. Sha, R. Rajkumar, and J. P. Lehoczky. </author> <title> Priority Inheritance Protocols: An Approach to Real-Time Synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(9) </volume> <pages> 1175-1185, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: RT-Mach's services in turn are based on two major ideas (among others) which have been developed to ensure the allocation of resources to more important tasks in real-time systems. Those ideas are priority-based CPU scheduling [53] and the Basic Priority Inheritance Protocol (BPI) <ref> [60] </ref> for non-preemptible resources. With both ideas, tasks to be performed are ranked by their relative priorities (a function of their criticality and/or feasibility), and the highest priority tasks are granted access to the resource in question. <p> when a message cannot be immediately received because all potential receivers are busy, RT-Mach queues the waiting thread or message in priority order and then boosts the priority of the thread inside the critical section or the priority of one of the potential receivers in accordance with the BPI protocol <ref> [60] </ref>. StarBase employs RT-Mach's priority-based CPU and BPI resource scheduling in several ways: to determine the transaction service order, to provide high-priority transactions the means to progress faster than low-priority transactions, and to provide priority-consistent access to facilities such as the Small Memory Manager and Concurrency Controller.
Reference: [61] <author> L. Sha, R. Rajkumar, S. H. Son, and C. Chang. </author> <title> A Real-Time Locking Protocol. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(7), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: This requires that many restrictions be placed on the structure and characteristics of real-time transactions. Only a few real-time transaction scheduling algorithms using this class of transaction model have been proposed so far. Sha et al proposed algorithms for scheduling a fixed set of periodic transactions with hard deadlines <ref> [59, 61] </ref>. Their model assumes that transaction priorities and resource requirements are known a priori. The rate-monotonic algorithm is used for determining transaction priority and scheduling the CPU. A priority ceiling protocol based on locking is used for concurrency control. <p> Most real-time database research uses the models which include only a subset of the above classes (e.g., fClass Ig <ref> [61, 4, 66] </ref> or fClass IIIg [1, 25, 28, 64]), and never discriminate among transactions in the system. However, in practice, all kinds of transactions can coexist in one system.
Reference: [62] <author> S. H. Son. </author> <title> Real-Time Database Systems: A New Challenge. </title> <journal> IEEE Data Engineering, </journal> <volume> 13(4) </volume> <pages> 39-43, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: In addition, a conventional disk-based database environment is assumed. The general approach is to utilize existing concurrency control protocols, especially two-phase locking, and to apply time-critical transaction scheduling methods that favor more urgent transactions <ref> [62] </ref>.
Reference: [63] <author> S. H. Son, J. Lee, and Y. Lin. </author> <title> Hybrid Protocols Using Dynamic Adjustment of Serialization Order for Real-Time Concurrency Control. </title> <journal> Journal of Real-Time Systems, </journal> <volume> 4(3) </volume> <pages> 269-276, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: different values. 25 This class of algorithms are High Priority (or Priority Abort ) [29, 1], Priority Inheri--tance (or Wait Promote) [30, 28, 1] protocols with the two-phase locking scheme, OPT-BC , OPT-SACRIFICE , OPT-WAIT , WAIT-50 protocols [24, 23] with optimistic concurrency control techniques, and hybrid concurrency control algorithms <ref> [52, 63] </ref>. Some soft or firm real-time transaction scheduling algorithms utilize the worst case execution times of transactions, without the knowledge of data requirements. Intuitively, this information can help the scheduler to determine eligibility of transactions, reducing the wasted CPU time and recovery overhead due to aborted transactions.
Reference: [64] <author> S. H. Son, S. Park, and Y. Lin. </author> <title> An Integrated Real-Time Locking Protocol. </title> <booktitle> In Proceedings of the 8th IEEE International Conference on Data Engineering, </booktitle> <pages> pages 527-534, </pages> <address> Phoenix, AZ, </address> <month> February </month> <year> 1992. </year> <month> 150 </month>
Reference-contexts: scheduling mechanism of a real-time database management system that utilizes such information and guarantees both consistency and timing constraints. 31 Chapter 3 Real-Time Database System Model Most real-time database scheduling algorithms have been developed and evaluated under almost the same workload and operating environment model used in conventional database systems <ref> [1, 25, 28, 64] </ref>. That is, transactions are assumed to arrive in a Poisson stream at a specified mean rate. Each transaction consists of a random sequence of pages to be read, a subset of which are updated. In addition, a conventional disk-based database environment is assumed. <p> However, we decide not to further categorize soft real-time transactions but to concentrate on Class I and Class II transactions. Much work has been already done for Class III transactions <ref> [1, 25, 28, 64] </ref>. Moreover, all Class III transactions are supposed to have the same level of importance (non-critical) and do not require an individual performance guarantee. <p> Most real-time database research uses the models which include only a subset of the above classes (e.g., fClass Ig [61, 4, 66] or fClass IIIg <ref> [1, 25, 28, 64] </ref>), and never discriminate among transactions in the system. However, in practice, all kinds of transactions can coexist in one system.
Reference: [65] <author> Sang H. Son. </author> <title> An Adaptive Checkpointing Scheme for Distributed Databases with Mixed Types of Transactions. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(4), </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: Checkpointing In this subsection, two approaches to checkpointing in a memory-resident database that guarantee a certain degree of consistency are presented - Black/White Policies [56] and 127 an interference-free checkpoint mechanism surveyed in <ref> [65] </ref>. In the Black/White policy, the database is colored "read" or "not read yet" (by the checkpointer). A transaction can continue processing as long as it modifies only information that has already been read by the checkpointing procedure or has not been read yet. <p> Transactions that have modified "not read yet" data and desire to write "read" data are aborted. A second approach to designing checkpointing mechanisms that do not interfere with transaction processing is presented by Son <ref> [65] </ref>. <p> If the checkpointing method guarantees consistency of the checkpoint, then Restart reads less log information that it would have to if a fuzzy checkpointing 128 scheme [22] were used. In the presence of a secondary processor, the method of choice seems to be the non-interfering checkpoint method <ref> [65] </ref>, since the overhead involved in generating a commit consistent checkpoint is taken care of by the recovery processor. A factor to be kept in mind is the frequency of failure in the system.
Reference: [66] <author> X. Song and J. Liu. </author> <title> Performance of Multiversion Concurrency Control Algorithms in Maintaining Temporal Consistency. </title> <booktitle> In Proceedings of the IEEE 14th Annual International Computer Software and Applications Conference (COMPSAC), </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: For those applications, a real-time database system must provide mechanisms to minimize the execution time variance of a transaction, making the system's behavior predictable. There has been some work dealing with hard deadline transactions <ref> [59, 66, 4, 51] </ref>, but impractical assumptions have often been made (for example, all transactions are periodic and their 1 Firm deadline transactions are special cases of soft deadline transactions which have no value after their deadlines. 5 worst-case execution times are given) or different correctness and performance criteria for hard <p> Temporal consistency can reference either the absolute age of the data read by a transaction, or the age of each data item relative to the age of every other data item in the read set of a transaction <ref> [66] </ref>. Not much work has been reported on schedulers that preserve both logical and temporal consistency of real-time database at the same time. All the real-time transaction scheduling algorithms mentioned in the previous section use "serializability" as the only database correctness criteria. Liu and Song [66] apply temporal consistency along with <p> read set of a transaction <ref> [66] </ref>. Not much work has been reported on schedulers that preserve both logical and temporal consistency of real-time database at the same time. All the real-time transaction scheduling algorithms mentioned in the previous section use "serializability" as the only database correctness criteria. Liu and Song [66] apply temporal consistency along with serializability as a criterion for correctness. This is consistent with the observation that the correct operation interleav-ings for real-time transactions are those serializable interleavings which meet their timing constraints. <p> Most real-time database research uses the models which include only a subset of the above classes (e.g., fClass Ig <ref> [61, 4, 66] </ref> or fClass IIIg [1, 25, 28, 64]), and never discriminate among transactions in the system. However, in practice, all kinds of transactions can coexist in one system. <p> In a dynamic approach, the system 41 keeps checking the temporal consistency at run time and tries to meet them dynamically, by either using multiple versions of data objects <ref> [66] </ref> or delaying some transactions in favor of more urgent transactions in terms of temporal consistency enforcement at the specific moment [36]. In this section, we present a static approach which we call Static Temporal Consistency Enforcement (STCE) scheme for our real-time database system model. <p> We observe that no transaction scheduling algorithms proposed so far completely satisfy all these requirements even though several papers in the real-time database field have pointed them out <ref> [66, 4, 50, 5] </ref>.
Reference: [67] <author> B. Sprunt. </author> <title> Aperiodic Task Scheduling for Real-Time Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Conversely, no soft tasks may be ready when the server is released, wasting its high priority capacity. The latter drawback is avoided by the Priority Exchange [68] and Deferrable/Sporadic Server <ref> [67] </ref> algorithms. These are all based on similar principles to the polling server. However, they are able to preserve capacity if no soft tasks are pending when they are 21 released. Due to this property, they are termed "bandwidth preserving algorithms". <p> Our approach to scheduling non-guaranteed transactions is based on dynamic slack stealing approach reviewed in Appendix A, which is shown to have better flexibility and performance than other approaches, such as background processing, the Sporadic Server algorithm <ref> [67] </ref>, the Extended Priority Exchange algorithm [68] and the optimal static slack stealing algorithm [48]. 54 Different from the original dynamic slack stealing presented in [14], our algorithm accounts the scheduler overheads included in our extended schedulability analysis.
Reference: [68] <author> B. Sprunt, J. Lehoczky, and L. Sha. </author> <title> Exploiting Unused Periodic Time for Aperiodic Service Using the Extended Priority Exchange Algorithm. </title> <booktitle> In Proceedings of the 9th Real-Time Systems Symposium, </booktitle> <pages> pages 251-258, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Conversely, no soft tasks may be ready when the server is released, wasting its high priority capacity. The latter drawback is avoided by the Priority Exchange <ref> [68] </ref> and Deferrable/Sporadic Server [67] algorithms. These are all based on similar principles to the polling server. However, they are able to preserve capacity if no soft tasks are pending when they are 21 released. Due to this property, they are termed "bandwidth preserving algorithms". <p> Our approach to scheduling non-guaranteed transactions is based on dynamic slack stealing approach reviewed in Appendix A, which is shown to have better flexibility and performance than other approaches, such as background processing, the Sporadic Server algorithm [67], the Extended Priority Exchange algorithm <ref> [68] </ref> and the optimal static slack stealing algorithm [48]. 54 Different from the original dynamic slack stealing presented in [14], our algorithm accounts the scheduler overheads included in our extended schedulability analysis.
Reference: [69] <author> J. Stankovic. </author> <title> Real-Time Computing Systems: The Next Generation. </title> <type> Technical Report TR-88-06, </type> <institution> University of Massachusetts, Amherst, </institution> <month> January </month> <year> 1988. </year> <note> Also available as Misconceptioins about Real-Time Computing, IEEE Computer, </note> <month> October </month> <year> 1988. </year>
Reference-contexts: The growing importance of real-time computing in a diverse number of applications such as defense systems, industrial automation, aerospace and medical applications, has resulted in an increased research effort in this area <ref> [69] </ref>. Real-time systems often require database management services to store large amounts of time-sensitive data and to manipulate the data within given timing constraints. <p> The real-time database systems model to be presented in the following chapter supports various types of real-time transactions, including both hard and soft real-time transactions. 1.2 Requirements of Real-Time Database Systems 1.2.1 Predictability and Timeliness Real-time computing is not equivalent to fast computing <ref> [69] </ref>. There are more important properties of RTDBS than speed: timeliness, i.e., the ability to produce expected results early or at the right time, and predictability , i.e., the ability to function as deterministically as necessary to satisfy system specifications, including timing constraints.
Reference: [70] <author> J. A. Stankovic and W. Zhao. </author> <title> On Real-Time Transactions. </title> <journal> ACM SIGMOD Record, </journal> <volume> 17(1) </volume> <pages> 4-18, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: In such systems, the transaction scheduler is usually supposed to have no idea about a transaction's computing time and resource requirement in advance. Their justification can be stated as follows <ref> [70, 25] </ref>: Database systems for efficiently supporting hard deadline real-time applications, where all transaction deadlines have to be met, appear infeasible. This is because there is usually a large variance between the average case and worst case execution time of a transaction.
Reference: [71] <author> M. Stonebraker. </author> <title> Virtual Memory Transaction Management. </title> <journal> ACM Operating Systems Review, </journal> <volume> 18(2) </volume> <pages> 8-16, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: However, such services are only available to DBMS users and not to general clients of the operating system such as mail programs and text editors. There has been considerable interest in providing transactions as an operating system service <ref> [71, 72, 16, 18] </ref>, and several OSs have done exactly that [27, 38, 11].
Reference: [72] <author> M. Stonebraker, D. DuBourdieux, and W. Edwards. </author> <title> Problems in Supporting Database Transactions in an Operating System Transaction Manager. </title> <journal> ACM Operating Systems Review, </journal> <volume> 19(1) </volume> <pages> 6-14, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: However, such services are only available to DBMS users and not to general clients of the operating system such as mail programs and text editors. There has been considerable interest in providing transactions as an operating system service <ref> [71, 72, 16, 18] </ref>, and several OSs have done exactly that [27, 38, 11].
Reference: [73] <author> H. Tokuda. </author> <title> RT-Thread Model for Real-Time Mach. </title> <booktitle> In IEEE Workshop on Real-Time Operating Systems and Software, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: In contrast to the traditional mono-lithic operating system kernel, a microkernel provides system servers with generic services independent of a particular operating system, including (real-time) scheduling of one or more processors, memory management, and a simple IPC (Interprocess Communication) facility <ref> [20, 21, 76, 73] </ref>. This combination of elementary services forms a standard base which can support all other system-specific functions.
Reference: [74] <author> H. Tokuda and C. Mercer. </author> <title> ARTS: A Distributed Real-Time Kernel. </title> <journal> ACM Operating Systems Review, </journal> <volume> 23(3), </volume> <month> July </month> <year> 1989. </year>
Reference-contexts: The support for recoverable update of real-time data objects, not relying on backward recovery, is required for real-time transaction processing. One possible solution is to build a deterministic service interface for real-time data objects on top of the real-time microkernel, such as ARTS <ref> [74] </ref>, Real-Time Mach [76], or 1 The buffer space is divided into pages of the same size, called buffer pages. 2 When a process attempts to read from a file, the buffer manager first searches the buffer pages for the specific data page.
Reference: [75] <author> H. Tokuda and T. Nakajima. </author> <title> Evaluation of Real-Time Synchronization in Real-Time Mach. </title> <booktitle> In Proceedings of the Second USENIX Mach Workshop, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: The timing and priority information is then used as input to the RT-Mach scheduler. RT-Mach also has striven to implement priority-based resource scheduling through its interprocess communication (RT-IPC) [34] and thread synchronization (RT-Sync) <ref> [75] </ref> facilities. RT-Mach implements BPI itself as a combination of priority queuing and priority inheritance.
Reference: [76] <author> H. Tokuda, T. Nakajima, and P. Rao. </author> <title> Real-Time Mach: Towards Predictable Real-Time Systems. </title> <booktitle> In Proceedings of the USENIX 1990 Mach Workshop, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: In contrast to the traditional mono-lithic operating system kernel, a microkernel provides system servers with generic services independent of a particular operating system, including (real-time) scheduling of one or more processors, memory management, and a simple IPC (Interprocess Communication) facility <ref> [20, 21, 76, 73] </ref>. This combination of elementary services forms a standard base which can support all other system-specific functions. <p> The support for recoverable update of real-time data objects, not relying on backward recovery, is required for real-time transaction processing. One possible solution is to build a deterministic service interface for real-time data objects on top of the real-time microkernel, such as ARTS [74], Real-Time Mach <ref> [76] </ref>, or 1 The buffer space is divided into pages of the same size, called buffer pages. 2 When a process attempts to read from a file, the buffer manager first searches the buffer pages for the specific data page. <p> StarBase uses no a priori information about transaction workload and discards tardy transactions at their deadline points. In order to realize many of these real-time goals, StarBase is constructed on top of RT-Mach, a real-time operating system developed by Carnegie Mellon University <ref> [76] </ref>. <p> RT-Mach provides several priority-based scheduling regimes, including Fixed Priority, Earliest Deadline First, Rate Monotonic, and Deadline Monotonic. RT-Mach's real-time thread model <ref> [76] </ref> distinguishes real-time threads of execution from ordinary ones, requiring the explicit specification of timing constraints and criticality on a per-thread basis. The timing and priority information is then used as input to the RT-Mach scheduler.
Reference: [77] <author> I. L. Traiger. </author> <title> Virtual Memory Management for Data Base Systems. </title> <journal> ACM Operating Systems Review, </journal> <volume> 16(4) </volume> <pages> 26-48, </pages> <month> October </month> <year> 1982. </year> <month> 151 </month>
Reference-contexts: The major issue regarding the file system is the management of memory buffers in accessing data files. Some operating systems are moving toward providing file system services through virtual memory <ref> [77, 16] </ref>. In this architecture, when a file is opened, it is mapped into a virtual memory segment and then read by referencing a page in virtual memory, which is then loaded by the operating system.
References-found: 77

