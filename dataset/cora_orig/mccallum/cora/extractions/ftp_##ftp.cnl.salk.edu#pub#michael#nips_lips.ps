URL: ftp://ftp.cnl.salk.edu/pub/michael/nips_lips.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00243.html
Root-URL: 
Email: Email: mgray, jmovellan, tsejnowski@ucsd.edu  
Title: Dynamic features for visual speech reading: A systematic comparison  
Author: Michael S. Gray ; Javier R. Movellan Terrence J. Sejnowski ; and 
Address: La Jolla, CA 92093  P. O. Box 85800 San Diego, CA 92186-5800  
Affiliation: Departments of Cognitive Science 1 and Biology 2 University of California, San Diego  Howard Hughes Medical Institute 3 Computational Neurobiology Lab The Salk Institute,  
Abstract: Humans use visual as well as auditory speech signals to recognize spoken words. A variety of systems have been investigated for performing this task. The main purpose of this research was to systematically compare the performance of a range of dynamic visual features on a speechreading task. We have found that normalization of images to eliminate variation due to translation, scale, and planar rotation yielded substantial improvements in generalization performance regardless of the visual representation used. In addition, the dynamic information in the difference between successive frames yielded better performance than optical-flow based approaches, and compression by local low-pass filtering worked surprisingly better than global principal components analysis (PCA). These results are examined and possible explanations are explored. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Bregler and Y. Konig. </author> <title> Eigenlips for robust speech recognition. </title> <booktitle> In Proceedings of IEEE ICASSP, </booktitle> <pages> pages 669-672. </pages> <address> Adelaide, Australia, </address> <year> 1991. </year>
Reference-contexts: A separate TDNN was trained on the acoustic signal. The output probabilities for the visual and acoustic signals were then combined multiplicatively. Bregler and Konig <ref> [1] </ref> also utilized a TDNN architecture. In this work, the visual information was captured by the first 10 principal components of a contour model fit to the lips. This was enough to specify the full range of lip shapes ("eigenlips"). Bregler and Konig [1] combined the acoustic and visual information in <p> Bregler and Konig <ref> [1] </ref> also utilized a TDNN architecture. In this work, the visual information was captured by the first 10 principal components of a contour model fit to the lips. This was enough to specify the full range of lip shapes ("eigenlips"). Bregler and Konig [1] combined the acoustic and visual information in the input representation, which gave improved performance in noisy environments, compared with acoustic information alone. Surprisingly, the visual signal alone carries a substantial amount of information about spoken words. <p> The input representation for the hidden Markov model consisted of low-pass filtered pixel intensity information at each time step, as well as a delta image that showed the pixel by pixel difference between subsequent time steps. The motivation for the current work was succinctly stated by Bregler and Konig <ref> [1] </ref>: "The real information in lipreading lies in the temporal change of lip positions, rather than the absolute lip shape." Although different kinds of dynamic visual information have been explored, there has been no careful comparison of different methods.
Reference: [2] <author> O.N. Garcia, A.J. Goldschen, and E.D. Petajan. </author> <title> Feature extraction for op tical automatic speech recognition or automatic lipreading. </title> <type> Technical Report GWU-IIST-9232, </type> <institution> Dept. of Electrical Engineering and Computer Science, George Washington University, </institution> <year> 1992. </year>
Reference-contexts: Bregler and Konig [1] combined the acoustic and visual information in the input representation, which gave improved performance in noisy environments, compared with acoustic information alone. Surprisingly, the visual signal alone carries a substantial amount of information about spoken words. Garcia, Goldschen, and Petajan <ref> [2] </ref> used a variety of visual features from the mouth region of a speaker's face to recognize test sentences using hidden Markov models (HMMs). Those features that were found to give the best discrimination tended to be dynamic in nature, rather than static.
Reference: [3] <author> J. Luettin, N.A. Thacker, </author> <title> and S.W. Beet. Visual speech recognition using active shape models and hidden markov models. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 817-820, </pages> <address> Atlanta, Ga, 1996. </address> <publisher> IEEE. </publisher>
Reference-contexts: The first dataset contained the raw video images described above. In the second dataset, images were normalized so that variations due to translation, scale, and planar rotation were eliminated. The images were normalized using parameters obtained from contour modeling of the lips by Luettin, Thacker, and Beet <ref> [3] </ref>. 2.3 RECOGNITION ENGINE The different visual representations described above formed the input to hidden Markov models which were separately trained for each word category. The images were modeled as mixtures of Gaussian distributions in pixel space.
Reference: [4] <author> K. Mase and A. Pentland. </author> <title> Automatic lipreading by optical-flow analysis. </title> <journal> Sys tems and Computers in Japan, </journal> <volume> 22(6) </volume> <pages> 67-76, </pages> <year> 1991. </year>
Reference-contexts: Those features that were found to give the best discrimination tended to be dynamic in nature, rather than static. Mase and Pent-land <ref> [4] </ref> also explored the dynamic information present in lip images through the use of optical flow. They found that a template matching approach on the optical flow of 4 windows around the edges of the mouth yielded results similar to humans on a digit recognition task.
Reference: [5] <author> H. McGurk and J. MacDonald. </author> <title> Hearing lips and seeing voices. </title> <journal> Nature, </journal> <volume> 264:126 130, </volume> <year> 1976. </year>
Reference-contexts: 1 INTRODUCTION Visual speech recognition is a challenging task in sensory integration. Psychophysical work by McGurk and MacDonald <ref> [5] </ref> first showed the powerful influence of visual information on speech perception that has led to increased interest in this area. A wide variety of techniques have been used to model speech-reading.
Reference: [6] <author> J.R. Movellan. </author> <title> Visual speech recognition with stochastic networks. </title> <editor> In G. Tesauro, D.S. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 851-858. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: They found that a template matching approach on the optical flow of 4 windows around the edges of the mouth yielded results similar to humans on a digit recognition task. Movellan <ref> [6] </ref> investigated the recognition of spoken digits using only visual information. The input representation for the hidden Markov model consisted of low-pass filtered pixel intensity information at each time step, as well as a delta image that showed the pixel by pixel difference between subsequent time steps. <p> The bottom panel shows the reconstructed optical flow representation learned by a 1-state HMM. This can be considered the canonical or prototypical representation for the digit "one" across our database of 12 individuals. 2 METHODS AND MODELS 2.1 TRAINING SAMPLE The training sample was the Tulips1 database (Movellan <ref> [6] </ref>): 96 digitized movies of 12 undergraduate students (9 males, 3 females) from the Cognitive Science Department at UC-San Diego. Video capturing was performed in a windowless room at the Center for Research in Language at UC-San Diego. <p> The left half of each image contained pixel intensity information, and the right half represented the delta image. + flow. The video images were made symmetric by averaging pixels from the left and right side of the image (Figure 1, left column). The low-pass + delta representation (Movellan <ref> [6] </ref>) consisted of 2 parts (Figure 1, middle column). The images were low-pass filtered, and downsampled to a resolution of 15 x 20 pixels. Delta images were formed from the pixel-by-pixel difference between subsequent time frames, and then low-pass filtered and downsampled to 15 x 20 pixels. <p> Each set of simulations took approximately 20 hours on a 300 MHz DEC Alpha processor. The best performance (of the 9 architectures) for each input representation is shown in Table 1. Results from the low-pass + delta representation closely matched, as expected, Movellan <ref> [6] </ref>. The small difference between Movellan [6] and the results reported here are likely due to differences in the low-pass filtering kernel, and different initializations of the parameters of the Gaussians during k-means clustering. The flow representations (with or without low-pass pixel intensity information) gave similar results. <p> Each set of simulations took approximately 20 hours on a 300 MHz DEC Alpha processor. The best performance (of the 9 architectures) for each input representation is shown in Table 1. Results from the low-pass + delta representation closely matched, as expected, Movellan <ref> [6] </ref>. The small difference between Movellan [6] and the results reported here are likely due to differences in the low-pass filtering kernel, and different initializations of the parameters of the Gaussians during k-means clustering. The flow representations (with or without low-pass pixel intensity information) gave similar results. <p> The low-pass + delta representation gave excellent results for both raw and normalized images. Although the low-pass + delta representation matched the performance of normal humans (89.9%), none of these models has yet to reach the level of trained lipreaders (95.5%) on this same database (Movellan <ref> [6] </ref>). The states learned by the HMM for these flow inputs give us information about the dynamic movement of the lips through time.
Reference: [7] <author> G.J. Wolff, K.V. Prasad, D.G. Stork, and M. Hennecke. </author> <title> Lipreading by neu ral networks: Visual preprocessing, learning and sensory integration. </title> <editor> In J.D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 1027-1034. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference-contexts: A wide variety of techniques have been used to model speech-reading. Yuhas, Goldstein, Sejnowski, and Jenkins [8] used feedforward networks to combine gray scale images with acoustic representations of vowels. Wolff, Prasad, Stork, and Hen-necke <ref> [7] </ref> explicitly computed information about the position of the lips, the shape of the mouth, and motion. This approach has the advantage of dramatically reducing the dimensionality of the input, but critical information may be lost.
Reference: [8] <author> B.P. Yuhas, Jr. Goldstein, M.H., T.J. Sejnowski, and R.E. Jenkins. </author> <title> Neural net work models of sensory integration for improved vowel recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10) </volume> <pages> 1658-1668, </pages> <year> 1990. </year>
Reference-contexts: Psychophysical work by McGurk and MacDonald [5] first showed the powerful influence of visual information on speech perception that has led to increased interest in this area. A wide variety of techniques have been used to model speech-reading. Yuhas, Goldstein, Sejnowski, and Jenkins <ref> [8] </ref> used feedforward networks to combine gray scale images with acoustic representations of vowels. Wolff, Prasad, Stork, and Hen-necke [7] explicitly computed information about the position of the lips, the shape of the mouth, and motion.
References-found: 8

