URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.tr535.Using_simple_page_placement_policies.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/stets/sosp97/csm_2l_sosp97/node34.html
Root-URL: 
Email: fmwm,kthanasi,ricardo,scottg@cs.rochester.edu  
Title: Using Simple Page Placement Policies to Reduce the Cost of Cache Fills in Coherent Shared-Memory Systems  
Author: Michael Marchetti, Leonidas Kontothanassis, Ricardo Bianchini, and Michael L. Scott 
Date: September 1994  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: The cost of a cache miss depends heavily on the location of the main memory that backs the missing line. For certain applications, this cost is a major factor in overall performance. We report on the utility of OS-based page placement as a mechanism to increase the frequency with which cache fills access local memory in a distributed shared memory multiprocessor. Even with the very simple policy of first-use placement, we find significant improvements over round-robin placement for many applications on both hardware and software-coherent systems. For most of our applications, dynamic placement allows 35 to 75 percent of cache fills to be performed locally, resulting in performance improvements of 20 to 40 percent. We have also investigated the performance impact of more sophisticated policies including hardware support for page placement, dynamic page migration, and page replication. We were surprised to find no performance advantage for the more sophisticated policies; in fact in most cases performance of our applications suffered. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, G. D'Souza, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B. Lim, G. Maa, D. Nussbaum, M. Parkin, and D. Yeung. </author> <title> Directory-Based Cache Coherence in Large-Scale Multiprocessors. </title> <editor> In M. Dubois and S. S. Thakkar, editors, </editor> <title> Scalable Shared Memory Multiprocessors. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: CC-NUMA machines are characterized by local per-processor caches, distributed main memory, scalable interconnection networks, and a protocol that maintains cache coherence. Examples of such machines include the Stanford DASH [13] and the MIT Alewife <ref> [1] </ref>. COMA machines are similar to CC-NUMAs, except that the local portion of main memory is organized as a cache (named attraction memory) and data is replicated to local memory as well as the cache on a miss.
Reference: [2] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: To distinguish the dirty words, we assume that the cache includes per-word dirty bits. 5 Mgrid is a simplified shared-memory version of the multigrid kernel from the NAS Parallel Benchmarks <ref> [2] </ref>. It performs a more elaborate over-relaxation using multi-grid techniques to compute an approximate solution to the Poisson equation on the unit cube. We simulated 2 iterations, with 5 relaxation steps on each grid, and grid sizes from 64 fi 64 fi 32 down to 16 fi 16 fi 8.
Reference: [3] <author> W. J. Bolosky, R. P. Fitzgerald, and M. L. Scott. </author> <title> Simple But Effective Techniques for NUMA Memory Management. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: We present results in section 4 and conclude in section 5. 2 Related Work Page migration and replication has also been used on cacheless NUMA multiprocessors in order to take advantage of the lower cost of accessing local memory instead of remote memory <ref> [3, 4, 6, 11, 12] </ref>. By using efficient block-transfer hardware to transfer page-size blocks, these "NUMA memory management" systems reduce the average cost per reference.
Reference: [4] <author> W. J. Bolosky, M. L. Scott, R. P. Fitzgerald, R. J. Fowler, and A. L. Cox. </author> <title> NUMA Policies and Their Relation to Memory Architecture. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: We present results in section 4 and conclude in section 5. 2 Related Work Page migration and replication has also been used on cacheless NUMA multiprocessors in order to take advantage of the lower cost of accessing local memory instead of remote memory <ref> [3, 4, 6, 11, 12] </ref>. By using efficient block-transfer hardware to transfer page-size blocks, these "NUMA memory management" systems reduce the average cost per reference.
Reference: [5] <author> R. Chandra, S. Devine, B. Verghese, A. Gupta, and M. Rosenblum. </author> <title> An Evaluation of OS Scheduling and Page Migration for CC-NUMA Multiprocessors. </title> <booktitle> In Fourth Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <title> Held in conjunction with ISCA '94. </title>
Reference-contexts: Our approach is applicable to both NUMA machines with non-coherent caches and CC-NUMA machines, and requires little or no additional hardware. Chandra et. al. have independently studied migration in the context of CC-NUMAs with eager hardware cache coherence <ref> [5] </ref>. They simulated several migration policies based on counting cache misses and/or TLB misses; some of the policies allowed a page to move only once, and others allowed multiple migrations to occur. One of their policies (single move on the first cache miss) is similar to our dynamic placement policy.
Reference: [6] <author> A. L. Cox and R. J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: We present results in section 4 and conclude in section 5. 2 Related Work Page migration and replication has also been used on cacheless NUMA multiprocessors in order to take advantage of the lower cost of accessing local memory instead of remote memory <ref> [3, 4, 6, 11, 12] </ref>. By using efficient block-transfer hardware to transfer page-size blocks, these "NUMA memory management" systems reduce the average cost per reference.
Reference: [7] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM | A Cache-Only Memory Architecture. </title> <journal> Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Examples of such machines include the commercially available KSR-1 [9] and the Swedish Data Diffusion Machine (DDM) <ref> [7] </ref>. COMA multiprocessors have a significant advantage over CC-NUMA multiprocessors when it comes to servicing capacity and conflict cache misses. Since the local memory of a node serves as a large secondary or tertiary cache, most such misses are satisfied locally, incurring smaller miss penalties and less interconnection traffic.
Reference: [8] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: This protocol employs an eager implementation of release consistency. Our software-coherent NUMA machine uses a scalable extension of the work of Petersen and Li [15], with additional ideas from the work of Keleher et al. <ref> [8] </ref>. It employs a lazy implementation of release consistency, in which invalidation messages are sent only at synchronization release points, and processed (locally) only at synchronization acquire points.
Reference: [9] <author> Kendall Square Research. </author> <title> KSR1 Principles of Operation. </title> <address> Waltham MA, </address> <year> 1992. </year>
Reference-contexts: COMA machines are similar to CC-NUMAs, except that the local portion of main memory is organized as a cache (named attraction memory) and data is replicated to local memory as well as the cache on a miss. Examples of such machines include the commercially available KSR-1 <ref> [9] </ref> and the Swedish Data Diffusion Machine (DDM) [7]. COMA multiprocessors have a significant advantage over CC-NUMA multiprocessors when it comes to servicing capacity and conflict cache misses.
Reference: [10] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Software Cache Coherence for Large Scale Multiprocessors. </title> <type> TR 513, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> March </month> <year> 1994. </year> <note> Submitted for publication. </note>
Reference-contexts: If the only previously-existing mapping had read-write permissions, or if the current fault was a write fault and all previously-existing mappings were read-only, then the page is added to the weak list. Full details of this protocol can be found in a technical report <ref> [10] </ref>. 3.2 Page Placement Mechanisms The changes required to add page placement to both the hardware and software coherence protocols were straightforward. The basic idea is that the first processor to touch a given page of shared memory becomes that page's home node. <p> The software and hardware coherence systems generally exhibit comparable performance both with and without migration. Our applications exhibit coarse grained sharing and therefore scale nicely under both coherence schemes. The one exception is mp3d, which requires several modifications to work well on a software coherent system <ref> [10] </ref>. These modifications were not applied to the code in these experiments. after migration. Without dynamic placement, the applications in our suite satisfy less than two percent of their misses locally, as would be expected from round-robin placement on 64 processors.
Reference: [11] <author> R. P. LaRowe Jr. and C. S. Ellis. </author> <title> Experimental Comparison of Memory Management Policies for NUMA Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: We present results in section 4 and conclude in section 5. 2 Related Work Page migration and replication has also been used on cacheless NUMA multiprocessors in order to take advantage of the lower cost of accessing local memory instead of remote memory <ref> [3, 4, 6, 11, 12] </ref>. By using efficient block-transfer hardware to transfer page-size blocks, these "NUMA memory management" systems reduce the average cost per reference.
Reference: [12] <author> R. P. LaRowe Jr., M. A. Holliday, and C. S. Ellis. </author> <title> An Analysis of Dynamic Page Placement on a NUMA Multiprocessor. </title> <booktitle> In Proceedings of the 1992 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Newport, RI, </address> <month> June </month> <year> 1992. </year> <month> 11 </month>
Reference-contexts: We present results in section 4 and conclude in section 5. 2 Related Work Page migration and replication has also been used on cacheless NUMA multiprocessors in order to take advantage of the lower cost of accessing local memory instead of remote memory <ref> [3, 4, 6, 11, 12] </ref>. By using efficient block-transfer hardware to transfer page-size blocks, these "NUMA memory management" systems reduce the average cost per reference.
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: CC-NUMA machines are characterized by local per-processor caches, distributed main memory, scalable interconnection networks, and a protocol that maintains cache coherence. Examples of such machines include the Stanford DASH <ref> [13] </ref> and the MIT Alewife [1]. COMA machines are similar to CC-NUMAs, except that the local portion of main memory is organized as a cache (named attraction memory) and data is replicated to local memory as well as the cache on a miss. <p> The actual cost of a page fault is the sum of the interrupt, page table, and TLB overheads. Table 1 summarizes the default parameters used in our simulations. The CC-NUMA machine uses the directory-based write-invalidate coherence protocol of the Stan-ford DASH machine <ref> [13] </ref>. This protocol employs an eager implementation of release consistency. Our software-coherent NUMA machine uses a scalable extension of the work of Petersen and Li [15], with additional ideas from the work of Keleher et al. [8].
Reference: [14] <author> D. B. Loveman. </author> <title> High Performance Fortran. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(1) </volume> <pages> 25-42, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Compiler technology has not yet advanced to the point where this task is feasible; the current state of the art assumes a user-specified distribution of data among processors (e.g. as in HPF <ref> [14] </ref>). Moreover, there will always be important programs for which reference patterns cannot be determined at compile time, e.g. because they depend on input data [16]. Even for those applications in which compile-time placement is feasible, it still seems possible that OS-level placement will offer a simpler, acceptable solution.
Reference: [15] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Table 1 summarizes the default parameters used in our simulations. The CC-NUMA machine uses the directory-based write-invalidate coherence protocol of the Stan-ford DASH machine [13]. This protocol employs an eager implementation of release consistency. Our software-coherent NUMA machine uses a scalable extension of the work of Petersen and Li <ref> [15] </ref>, with additional ideas from the work of Keleher et al. [8]. It employs a lazy implementation of release consistency, in which invalidation messages are sent only at synchronization release points, and processed (locally) only at synchronization acquire points.
Reference: [16] <author> J. H. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-Time Parallelization and Scheduling of Loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Moreover, there will always be important programs for which reference patterns cannot be determined at compile time, e.g. because they depend on input data <ref> [16] </ref>. Even for those applications in which compile-time placement is feasible, it still seems possible that OS-level placement will offer a simpler, acceptable solution. Our work shows that we can achieve effective page placement with no hardware support other than the standard address translation and page fault mechanisms.
Reference: [17] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We simulated 2 iterations, with 5 relaxation steps on each grid, and grid sizes from 64 fi 64 fi 32 down to 16 fi 16 fi 8. Mp3d is part of the SPLASH suite <ref> [17] </ref>. It simulates rarefied fluid flow using a Monte Carlo algorithm. We simulated 20,000 particles for 10 time steps. Appbt is from the NAS Parallel Benchmarks suite. It computes an approximate solution to the Navier-Stokes equations.
Reference: [18] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: CC-NUMAs can approach the behavior of COMA machines if data are laid out intelligently in main memory so that most misses are satisfied by a node's local memory. Past work <ref> [18] </ref> has shown that with additional hardware, or programmer and compiler intervention, data pages can be migrated to the nodes that would miss on them the most, achieving performance comparable to that of COMA machines.
Reference: [19] <author> J. E. Veenstra. </author> <title> Mint Tutorial and User Manual. </title> <type> TR 452, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Our simulator consists of two parts: a front end, Mint <ref> [19, 20] </ref>, which simulates the execution of the processors, and a back end that simulates the memory system. The front end calls the back end on every data reference (instruction fetches are assumed to always be cache hits).
Reference: [20] <author> J. E. Veenstra and R. J. Fowler. Mint: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS '94), </booktitle> <pages> pages 201-207, </pages> <address> Durham, NC, </address> <month> January - February </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: Our simulator consists of two parts: a front end, Mint <ref> [19, 20] </ref>, which simulates the execution of the processors, and a back end that simulates the memory system. The front end calls the back end on every data reference (instruction fetches are assumed to always be cache hits).
References-found: 20

