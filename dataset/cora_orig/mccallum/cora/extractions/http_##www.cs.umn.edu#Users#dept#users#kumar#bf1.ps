URL: http://www.cs.umn.edu/Users/dept/users/kumar/bf1.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Author: Vipin Kumar K. Ramesh, and V. Nageshwara Rao 
Address: Austin, Texas 78712  
Affiliation: Artificial Intelligence Laboratory Department of Computer Sciences University of Texas at Austin  
Abstract: Parallel Best-First Search of State-Space Graphs: A Summary of Results fl Abstract This paper presents many different parallel formulations of the A*/Branch-and-Bound search algorithm. The parallel formulations primarily differ in the data structures used. Some formulations are suited only for shared-memory architectures, whereas others are suited for distributed-memory architectures as well. These parallel formulations have been implemented to solve the vertex cover problem and the TSP problem on the BBN Butterfly parallel processor. Using appropriate data structures, we are able to obtain fairly linear speedups for as many as 100 processors. We also discovered problem characteristics that make certain formulations more (or less) suitable for some search problems. Since the best-first search paradigm of A*/Branch-and-Bound is very commonly used, we expect these parallel formulations to be effective for a variety of problems. Concurrent and distributed priority queues used in these parallel formulations can be used in many parallel algorithms other than parallel A*/branch-and-bound. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, John E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: The effectiveness of different parallel formulations is also strongly dependent upon the characteristics of the problem being solved. We have tested the performance of these formulations on the 15-puzzle [17], the traveling salesman problem (TSP), and the vertex cover problem (VCP) <ref> [1] </ref> on the BBN Butterfly multiprocessor. The results for the 15-puzzle and VCP are very similar; hence we only present the results for the VCP and TSP. Although both TSP and VCP are NP-hard problems, they generate search spaces that are qualitatively different from each other. <p> Since the only operations done on OPEN are deletions of smallest cost element and insertion of elements, OPEN is essentially a priority queue, and is often implemented as a heap <ref> [1] </ref>.
Reference: [2] <author> J. S. Conery and D. F. Kibler. </author> <title> Parallelism in ai programs. </title> <booktitle> In Proceedings of International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 53-56, </pages> <year> 1985. </year>
Reference-contexts: In such cases a solution can be found by searching the space of potential solutions. Clearly, if many processors are available, then they can search different parts of the space concurrently. Investigation of parallelism in different AI search procedures is an active area of research <ref> [9; 23; 6; 14; 15; 2] </ref>. For many problems, heuristic domain knowledge is available, which can be used to avoid searching some (unpromising) parts of the search space.
Reference: [3] <author> William J. Dally. </author> <title> A VLSI Architecture for Concurrent Data Structures. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1987. </year>
Reference-contexts: This ensures that if some processor has a good part of the search space, then others get a part of it. 5 This strategy can be easily implemented on distributed-memory systems with low diameter (such as Hypercube [25], Torus <ref> [3] </ref>) as well as shared memory multiprocessors such as the Butterfly. If the frequency of transfer is high, then the redundancy factor can be small; otherwise it can be very large. The choice of frequency of transfer is effectively determined by the cost of communication. <p> We have also investigated various means of artificially increasing the granularity of the problem (i.e., increase T exp ). 8 These results are not presented in this paper. A number of researchers have suggested distributed strategies similar to the random communication scheme <ref> [28; 3] </ref>, and the ring communication scheme [27; 29]. Wah and Ma [29] found the ring communication scheme to give good speedup on the vertex cover problem and hypothesized that this could be a good strategy for best-first Branch-and-Bound in general.
Reference: [4] <author> S.-R. Huang and Larry S. Davis. </author> <title> A tight upper bound for the speedup of parallel best-first branch-and-bound algorithms. </title> <type> Technical report, </type> <institution> Center for Automation Research, University of Maryland, College Park, MD, </institution> <year> 1987. </year>
Reference-contexts: Even on shared memory architectures, contention for OPEN limits the performance to T exp /T access , where T exp is the average time for one node expansion, and T access is the average time spent in accessing OPEN per node expansion <ref> [4] </ref>. Note that the access to CLOSED does not cause contention, as different processors would manipulate different nodes. We have implemented this scheme for solving the Traveling Salesman Problem (TSP) and the vertex cover problem (VCP). <p> The centralized scheme has been studied in <ref> [?; 20; 4] </ref>. Parallel A* with the centralized scheme for solving the TSP is essentially the same as Mo-han's parallel algorithm for TSP in [?]. Mohan reported a speedup of 8 on 16 processors on the Cm*.
Reference: [5] <author> Keki B. Irani and Yi Fong Shih. </author> <title> Parallel a* and ao* algorithms: An optimality criterion and performance evaluation. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 274-277, </pages> <year> 1986. </year>
Reference-contexts: We shall call it a centralized strategy because each processor gets work from the global OPEN list. As discussed in <ref> [5] </ref>, this strategy should not result in much redundant search.
Reference: [6] <author> L. N. Kanal and Vipin Kumar. </author> <title> Branch-and-bound formulations for sequential and parallel game tree searching. </title> <booktitle> Proceedings of International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 569-571, </pages> <year> 1981. </year>
Reference-contexts: In such cases a solution can be found by searching the space of potential solutions. Clearly, if many processors are available, then they can search different parts of the space concurrently. Investigation of parallelism in different AI search procedures is an active area of research <ref> [9; 23; 6; 14; 15; 2] </ref>. For many problems, heuristic domain knowledge is available, which can be used to avoid searching some (unpromising) parts of the search space.
Reference: [7] <editor> L. N. Kanal and Vipin Kumar. </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Heuristic search is an important technique that is used to solve a variety of problems in Artificial Intelligence (AI) and other areas of computer science <ref> [7; 17; 18] </ref>. Search techniques are useful when one is able to specify the space of potential solutions, but the exact solution is not known before hand. In such cases a solution can be found by searching the space of potential solutions.
Reference: [8] <author> R. E. Korf. </author> <title> Depth-first iterative-deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 97-109, </pages> <year> 1985. </year>
Reference: [9] <author> V. Kumar and L. N. Kanal. </author> <title> Parallel branch-and-bound formulations for and/or tree search. </title> <journal> IEEE Transactions Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6:768-778, </volume> <year> 1984. </year>
Reference-contexts: In such cases a solution can be found by searching the space of potential solutions. Clearly, if many processors are available, then they can search different parts of the space concurrently. Investigation of parallelism in different AI search procedures is an active area of research <ref> [9; 23; 6; 14; 15; 2] </ref>. For many problems, heuristic domain knowledge is available, which can be used to avoid searching some (unpromising) parts of the search space. <p> However, due to other factors such as communication overhead, etc., the actual the speedup may be less than P W p =W s . We have been investigating the use of parallel processing for speeding up different heuristic search algorithms <ref> [9; 23; 21; 11] </ref>. In this paper, we discuss a number of parallel formulations of the A* state-space search algorithm. As discussed in [10; 16], A* is essentially a "best-first" branch-and-bound algorithm. The parallel formulations presented in this paper are also applicable to many other best-first branch-and-bound procedures.
Reference: [10] <author> Vipin Kumar. </author> <title> Branch-and-bound search. </title> <editor> In Stuart C. Shapiro, editor, </editor> <booktitle> Encyclopaedia of Artificial Intelligence: </booktitle> <volume> Vol 2, </volume> <pages> pages 1000-1004. </pages> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1987. </year> <note> Revised version appears in the second edition 1992. </note>
Reference-contexts: We have been investigating the use of parallel processing for speeding up different heuristic search algorithms [9; 23; 21; 11]. In this paper, we discuss a number of parallel formulations of the A* state-space search algorithm. As discussed in <ref> [10; 16] </ref>, A* is essentially a "best-first" branch-and-bound algorithm. The parallel formulations presented in this paper are also applicable to many other best-first branch-and-bound procedures. The parallel formulations primarily differ in the data structures used to implement the OPEN list (priority queue) of the A* algorithm.
Reference: [11] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, December 1987. 
Reference-contexts: However, due to other factors such as communication overhead, etc., the actual the speedup may be less than P W p =W s . We have been investigating the use of parallel processing for speeding up different heuristic search algorithms <ref> [9; 23; 21; 11] </ref>. In this paper, we discuss a number of parallel formulations of the A* state-space search algorithm. As discussed in [10; 16], A* is essentially a "best-first" branch-and-bound algorithm. The parallel formulations presented in this paper are also applicable to many other best-first branch-and-bound procedures. <p> It is easy to see that IDA*[8] outperforms A* on those problems for which V i grows very rapidly. We have already presented a parallel implementation of IDA* that is able to provide virtually unlimited speedup (for large enough problems) on a variety of architectures <ref> [23; 11] </ref>. Also IDA*, unlike A* requires very little memory, hence can solve large problem instances without running out of memory. The speedup anomalies on the VCP are fully explained by the fact that a large number of nodes have the the cost equal to that of the optimal solution.
Reference: [12] <author> T. H. Lai and Sartaj Sahni. </author> <title> Anomalies in parallel branch and bound algorithms. </title> <journal> Communications of the ACM, </journal> <pages> pages 594-602, </pages> <year> 1984. </year>
Reference-contexts: Hence, the speedup depends upon when the actual solution is encountered by the search (sequential or parallel). The phenomenon of speedup anomalies in best-first branch-and-bound has been extensively studied in <ref> [12; 20] </ref>. Our recent work [24] shows that it is possible to expect superlinear speedup on the average. 3 To study the speedup behavior in absence of anomaly, we modified the A* algorithm to find all optimal solutions of the VCP. <p> Although a number of researchers have investigated the phenomenon of speedup anomalies in best-first branch-and-bound, all of them hypothesized that the phenomenon is unlike to occur in real problems <ref> [12; 20] </ref>.
Reference: [13] <author> E. L. Lawler and D. Woods. </author> <title> Branch-and-bound methods: A survey. </title> <journal> Operations Research, </journal> <volume> 14, </volume> <year> 1966. </year>
Reference-contexts: TSP can be solved using the A*/Branch-and-Bound algorithm. A number of heuristics are known for the TSP. We have used the LMSK heuristic <ref> [13] </ref> in our experiments. Although the LMSK heuristic is quite powerful, it is not as good as the assignment heuristic [26]. We chose LMSK primarily because it was easy to implement and was adequate to show the power of different data structures and parallel control strategies discussed in this paper.
Reference: [14] <author> D. B. Leifker and L. N. Kanal. </author> <title> A hybrid sss*/alpha-beta algorithm for parallel search of game trees. </title> <booktitle> In Proceedings of International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1044-1046, </pages> <year> 1985. </year>
Reference-contexts: In such cases a solution can be found by searching the space of potential solutions. Clearly, if many processors are available, then they can search different parts of the space concurrently. Investigation of parallelism in different AI search procedures is an active area of research <ref> [9; 23; 6; 14; 15; 2] </ref>. For many problems, heuristic domain knowledge is available, which can be used to avoid searching some (unpromising) parts of the search space. <p> But the termination criterion can be easily modified to ensure that termination occurs only after a best solution has been found <ref> [14; 19] </ref>. (2) Since OPEN will be accessed by all the processors very frequently, it will have to be maintained in a shared memory that is easily accessible to all the proces 1 Since there can be more than one path by which a particular node can be reached from the
Reference: [15] <author> T. A. Marsland and M. Campbell. </author> <title> Parallel search of strongly ordered game trees. </title> <journal> Computing Surveys, </journal> <volume> 14 </volume> <pages> 533-551, </pages> <year> 1982. </year>
Reference-contexts: In such cases a solution can be found by searching the space of potential solutions. Clearly, if many processors are available, then they can search different parts of the space concurrently. Investigation of parallelism in different AI search procedures is an active area of research <ref> [9; 23; 6; 14; 15; 2] </ref>. For many problems, heuristic domain knowledge is available, which can be used to avoid searching some (unpromising) parts of the search space.
Reference: [16] <author> D. S. Nau, Vipin Kumar, and L. N. Kanal. </author> <title> General branch-and-bound and its relation to a* and ao*. </title> <journal> Artificial Intelligence, </journal> <volume> 23, </volume> <year> 1984. </year>
Reference-contexts: We have been investigating the use of parallel processing for speeding up different heuristic search algorithms [9; 23; 21; 11]. In this paper, we discuss a number of parallel formulations of the A* state-space search algorithm. As discussed in <ref> [10; 16] </ref>, A* is essentially a "best-first" branch-and-bound algorithm. The parallel formulations presented in this paper are also applicable to many other best-first branch-and-bound procedures. The parallel formulations primarily differ in the data structures used to implement the OPEN list (priority queue) of the A* algorithm.
Reference: [17] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1980. </year>
Reference-contexts: 1 Introduction Heuristic search is an important technique that is used to solve a variety of problems in Artificial Intelligence (AI) and other areas of computer science <ref> [7; 17; 18] </ref>. Search techniques are useful when one is able to specify the space of potential solutions, but the exact solution is not known before hand. In such cases a solution can be found by searching the space of potential solutions. <p> Some formulations are suited only for shared-memory architectures, whereas others are suited for distributed-memory architectures as well. The effectiveness of different parallel formulations is also strongly dependent upon the characteristics of the problem being solved. We have tested the performance of these formulations on the 15-puzzle <ref> [17] </ref>, the traveling salesman problem (TSP), and the vertex cover problem (VCP) [1] on the BBN Butterfly multiprocessor. The results for the 15-puzzle and VCP are very similar; hence we only present the results for the VCP and TSP. <p> It is easy to emulate distributed memory multiprocessors on a shared-memory multiprocessor. We study the suitability of different parallel formulations for both shared-memory and distributed-memory multiprocessors. 2 The A* Algorithm We assume familiarity with the A* algorithm. See <ref> [17] </ref> for a good introduction to A*. We will also use the terminology presented in [17]. Here we provide a brief overview of the algorithm. A* is used to find a least-cost path between a start state and a (set of) goal state (s) of a given state-space graph. <p> We study the suitability of different parallel formulations for both shared-memory and distributed-memory multiprocessors. 2 The A* Algorithm We assume familiarity with the A* algorithm. See <ref> [17] </ref> for a good introduction to A*. We will also use the terminology presented in [17]. Here we provide a brief overview of the algorithm. A* is used to find a least-cost path between a start state and a (set of) goal state (s) of a given state-space graph. <p> It was proved in <ref> [17] </ref> that if the heuristic estimate h is admissible, then A* would terminate with an optimal solution (if a solution exists). <p> See <ref> [17] </ref> for details. sors. Hence distributed-memory architectures such as the Hypercube [25] are effectively ruled out. <p> In A*, if the heuristic is consistent <ref> [17] </ref>, then the cost of the nodes expanded in successive iterations never goes down (it either goes up or stays the same). Let V i be the set of nodes expanded by A* after the cost has gone up ith time but before it has gone up i+1 th time.
Reference: [18] <author> Judea Pearl. </author> <title> Heuristics-Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction Heuristic search is an important technique that is used to solve a variety of problems in Artificial Intelligence (AI) and other areas of computer science <ref> [7; 17; 18] </ref>. Search techniques are useful when one is able to specify the space of potential solutions, but the exact solution is not known before hand. In such cases a solution can be found by searching the space of potential solutions.
Reference: [19] <author> Michael J. Quinn. </author> <title> Designing Efficient Algorithms for Parallel Computers. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: But the termination criterion can be easily modified to ensure that termination occurs only after a best solution has been found <ref> [14; 19] </ref>. (2) Since OPEN will be accessed by all the processors very frequently, it will have to be maintained in a shared memory that is easily accessible to all the proces 1 Since there can be more than one path by which a particular node can be reached from the
Reference: [20] <author> Michael J. Quinn and Narsingh Deo. </author> <title> An upper bound for the speedup of parallel branch-and-bound algorithms. </title> <journal> BIT, </journal> <volume> 26,No 1, </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: Hence, the speedup depends upon when the actual solution is encountered by the search (sequential or parallel). The phenomenon of speedup anomalies in best-first branch-and-bound has been extensively studied in <ref> [12; 20] </ref>. Our recent work [24] shows that it is possible to expect superlinear speedup on the average. 3 To study the speedup behavior in absence of anomaly, we modified the A* algorithm to find all optimal solutions of the VCP. <p> Although a number of researchers have investigated the phenomenon of speedup anomalies in best-first branch-and-bound, all of them hypothesized that the phenomenon is unlike to occur in real problems <ref> [12; 20] </ref>. <p> The centralized scheme has been studied in <ref> [?; 20; 4] </ref>. Parallel A* with the centralized scheme for solving the TSP is essentially the same as Mo-han's parallel algorithm for TSP in [?]. Mohan reported a speedup of 8 on 16 processors on the Cm*.
Reference: [21] <author> V. Nageshwara Rao and V. Kumar. </author> <title> Parallel depth-first search, part I: Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):479-499, December 1987. 
Reference-contexts: However, due to other factors such as communication overhead, etc., the actual the speedup may be less than P W p =W s . We have been investigating the use of parallel processing for speeding up different heuristic search algorithms <ref> [9; 23; 21; 11] </ref>. In this paper, we discuss a number of parallel formulations of the A* state-space search algorithm. As discussed in [10; 16], A* is essentially a "best-first" branch-and-bound algorithm. The parallel formulations presented in this paper are also applicable to many other best-first branch-and-bound procedures.
Reference: [22] <author> V. Nageshwara Rao and V. Kumar. </author> <title> Concurrent insertions and deletions in a priority queue. </title> <booktitle> In Proceedings of the 1988 Parallel Processing Conference, </booktitle> <year> 1988. </year>
Reference-contexts: This shows that the centralized parallel strategy is quite effective for parallelizing the TSP instances of large granularity. On problems with smaller granularities, the contention for OPEN shows up. To reduce the contention for OPEN, we implemented it as a concurrent heap <ref> [22] </ref>. On a concurrent heap, OPEN needs to be locked only for O (1) time, which allows O (log N ) processors to access OPEN simultaneously (N is the number of nodes in OPEN).
Reference: [23] <author> V. Nageshwara Rao, V. Kumar, and K. Ramesh. </author> <title> A parallel implementation of iterative-deepening-a*. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-87), </booktitle> <pages> pages 878-882, </pages> <year> 1987. </year>
Reference-contexts: In such cases a solution can be found by searching the space of potential solutions. Clearly, if many processors are available, then they can search different parts of the space concurrently. Investigation of parallelism in different AI search procedures is an active area of research <ref> [9; 23; 6; 14; 15; 2] </ref>. For many problems, heuristic domain knowledge is available, which can be used to avoid searching some (unpromising) parts of the search space. <p> However, due to other factors such as communication overhead, etc., the actual the speedup may be less than P W p =W s . We have been investigating the use of parallel processing for speeding up different heuristic search algorithms <ref> [9; 23; 21; 11] </ref>. In this paper, we discuss a number of parallel formulations of the A* state-space search algorithm. As discussed in [10; 16], A* is essentially a "best-first" branch-and-bound algorithm. The parallel formulations presented in this paper are also applicable to many other best-first branch-and-bound procedures. <p> It is easy to see that IDA*[8] outperforms A* on those problems for which V i grows very rapidly. We have already presented a parallel implementation of IDA* that is able to provide virtually unlimited speedup (for large enough problems) on a variety of architectures <ref> [23; 11] </ref>. Also IDA*, unlike A* requires very little memory, hence can solve large problem instances without running out of memory. The speedup anomalies on the VCP are fully explained by the fact that a large number of nodes have the the cost equal to that of the optimal solution.
Reference: [24] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> On the efficicency of parallel backtracking. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(4) </volume> <pages> 427-437, </pages> <month> April </month> <year> 1993. </year> <note> Also available as Technical Report TR 90-55, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution> <note> Available via anonymous ftp from ftp.cs.umn.edu at users/kumar/suplin.ps. </note>
Reference-contexts: Hence, the speedup depends upon when the actual solution is encountered by the search (sequential or parallel). The phenomenon of speedup anomalies in best-first branch-and-bound has been extensively studied in [12; 20]. Our recent work <ref> [24] </ref> shows that it is possible to expect superlinear speedup on the average. 3 To study the speedup behavior in absence of anomaly, we modified the A* algorithm to find all optimal solutions of the VCP. <p> Initially, the search space is statically divided and given to different processors (by expanding some nodes and distributing them to the local OPEN lists of different pro 2 the computation of h (n) for a node is done using an algorithm given in [28]. 3 Although the work reported in <ref> [24] </ref> deals with average su-perlinear speedup in depth-first search, it is also applicable to best-first search.
Reference: [25] <author> Charles L. Seitz. </author> <title> The cosmic cube. </title> <journal> Communications of the ACM, </journal> <volume> 28-1:22-33, </volume> <year> 1985. </year>
Reference-contexts: See [17] for details. sors. Hence distributed-memory architectures such as the Hypercube <ref> [25] </ref> are effectively ruled out. Even on shared memory architectures, contention for OPEN limits the performance to T exp /T access , where T exp is the average time for one node expansion, and T access is the average time spent in accessing OPEN per node expansion [4]. <p> This ensures that if some processor has a good part of the search space, then others get a part of it. 5 This strategy can be easily implemented on distributed-memory systems with low diameter (such as Hypercube <ref> [25] </ref>, Torus [3]) as well as shared memory multiprocessors such as the Butterfly. If the frequency of transfer is high, then the redundancy factor can be small; otherwise it can be very large. The choice of frequency of transfer is effectively determined by the cost of communication.
Reference: [26] <author> H. S. Stone. </author> <title> High-Performance Computer Architectures. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: TSP can be solved using the A*/Branch-and-Bound algorithm. A number of heuristics are known for the TSP. We have used the LMSK heuristic [13] in our experiments. Although the LMSK heuristic is quite powerful, it is not as good as the assignment heuristic <ref> [26] </ref>. We chose LMSK primarily because it was easy to implement and was adequate to show the power of different data structures and parallel control strategies discussed in this paper. We implemented parallel A* (with the LMSK heuristic) using the centralized control strategy on BBN Butterfly.
Reference: [27] <author> Olivier Vornberger. </author> <title> Implementing branch-and-bound in a ring of processors. </title> <type> Technical Report 29, </type> <institution> University of Paderborn, </institution> <address> FRG, </address> <year> 1986. </year>
Reference-contexts: We have also investigated various means of artificially increasing the granularity of the problem (i.e., increase T exp ). 8 These results are not presented in this paper. A number of researchers have suggested distributed strategies similar to the random communication scheme [28; 3], and the ring communication scheme <ref> [27; 29] </ref>. Wah and Ma [29] found the ring communication scheme to give good speedup on the vertex cover problem and hypothesized that this could be a good strategy for best-first Branch-and-Bound in general.
Reference: [28] <author> Olivier Vornberger. </author> <title> Load balancing in a network of trans-puters. </title> <booktitle> In 2nd International Workshop on Distributed Parallel Algorithms, </booktitle> <year> 1987. </year>
Reference-contexts: Initially, the search space is statically divided and given to different processors (by expanding some nodes and distributing them to the local OPEN lists of different pro 2 the computation of h (n) for a node is done using an algorithm given in <ref> [28] </ref>. 3 Although the work reported in [24] deals with average su-perlinear speedup in depth-first search, it is also applicable to best-first search. <p> We have also investigated various means of artificially increasing the granularity of the problem (i.e., increase T exp ). 8 These results are not presented in this paper. A number of researchers have suggested distributed strategies similar to the random communication scheme <ref> [28; 3] </ref>, and the ring communication scheme [27; 29]. Wah and Ma [29] found the ring communication scheme to give good speedup on the vertex cover problem and hypothesized that this could be a good strategy for best-first Branch-and-Bound in general.
Reference: [29] <author> Benjamin W. Wah and Y. W. Eva Ma. </author> <title> Manip| a multicomputer architecture for solving combinatorial extremum-search problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-33, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: We have also investigated various means of artificially increasing the granularity of the problem (i.e., increase T exp ). 8 These results are not presented in this paper. A number of researchers have suggested distributed strategies similar to the random communication scheme [28; 3], and the ring communication scheme <ref> [27; 29] </ref>. Wah and Ma [29] found the ring communication scheme to give good speedup on the vertex cover problem and hypothesized that this could be a good strategy for best-first Branch-and-Bound in general. <p> A number of researchers have suggested distributed strategies similar to the random communication scheme [28; 3], and the ring communication scheme [27; 29]. Wah and Ma <ref> [29] </ref> found the ring communication scheme to give good speedup on the vertex cover problem and hypothesized that this could be a good strategy for best-first Branch-and-Bound in general.
References-found: 29

