URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-97-30.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+97-30
Root-URL: 
Title: On the Induction of Intelligible Ensembles  
Author: Bernhard Pfahringer 
Address: Vienna, Austria  
Affiliation: Austrian Research Institute for AI,  
Abstract: Ensembles of classifiers, e.g. decision trees, often exhibit greater predictive accuracy than single classifiers alone. Bagging and boosting are two standard ways of generating and combining multiple classifiers. Unfortunately, the increase in predictive performance is usually linked to a dramatic decrease in intelligibility: ensembles are more or less black boxes comparable to neural networks. So far attempts at pruning of ensembles have not been very successful, approximately reducing ensembles into half. This paper describes a different approach which both tries to keep ensemble-sizes small during induction already and also limits the complexity of single classifiers rigorously. Single classifiers are decision-stumps of a prespecified maximal depth. They are combined by majority voting. Ensembles are induced and pruned by a simple hill-climbing procedure. These ensembles can reasonably be transformed into equivalent decision trees. We conduct some empirical evaluation to investigate both predictive accuracies and classifier complexities.
Abstract-found: 1
Intro-found: 1
Reference: [Bailey & Elkan 93] <author> Bailey T.L., Elkan C.: </author> <title> Estimating the Accuracy of Learned Concepts, in Bajcsy R.(ed.), </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.895-901, </address> <year> 1993. </year>
Reference-contexts: Multi-class domains were transformed into N respective two-class problems in the obvious way, always trying to learn how to separate a specific class from all other classes. To estimate predictive accuracy, stratified five-fold cross-validation was repeated five times. The rationale is this. Single cross-validation can be rather unstable <ref> [Bailey & Elkan 93] </ref>, therefore one should average at least a few. Results of five-fold cross-validation do not seem to differ greatly from those of the more commonly chosen ten-fold cross-validation.
Reference: [Breiman 96a] <author> Breiman L.: </author> <title> Bagging Predictors, </title> <journal> Machine Learning, </journal> <volume> 24(2), </volume> <year> 1996. </year>
Reference-contexts: 1 Introduction Ensembles of classifiers, e.g. decision trees, often exhibit greater predictive accuracy than single classifiers alone. Boosting [Freund & Schapire 96] and bagging <ref> [Breiman 96a] </ref> are two standard ways of generating and combining multiple classifiers. Unfortunately, the increase in predictive performance is usually linked to a dramatic decrease in intelligibility: ensembles are more or less black boxes comparable to neural networks when it comes to explaining the rationale of some classificatory decision. <p> <ref> [Breiman 96a] </ref> are two standard ways of generating and combining multiple classifiers. Unfortunately, the increase in predictive performance is usually linked to a dramatic decrease in intelligibility: ensembles are more or less black boxes comparable to neural networks when it comes to explaining the rationale of some classificatory decision. Breiman [Breiman 96a] wrote that "What one loses, with the [bagging of] trees, is a simple and interpretable structure. What one gains is increased accuracy." So far few have attempted to remedy this situation. Various heuristics for pruning of ensembles are introduced in [Margineantu D.D & Dietterich T.G.].
Reference: [Buntine 91] <author> Buntine W.L.: </author> <title> Learning Classification Trees, </title> <journal> Statistics and Computing, </journal> <volume> Vol.2:63-73, </volume> <year> 1991. </year>
Reference-contexts: But the only principled way of choosing a good upper bound for ensemble sizes seems to be a (costly) wrapper-like approach based on cross-validation. A comparison to bayesian approaches as described in <ref> [Oliver 95, Buntine 91] </ref> is clearly necessary for judging the merits of the presented ideas.
Reference: [Freund & Schapire 96] <author> Freund Y., Schapire R.E.: </author> <title> Experiments with a New Boosting Algorithm, in Saitta L.(ed.), </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, pp.148-156, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Ensembles of classifiers, e.g. decision trees, often exhibit greater predictive accuracy than single classifiers alone. Boosting <ref> [Freund & Schapire 96] </ref> and bagging [Breiman 96a] are two standard ways of generating and combining multiple classifiers.
Reference: [Kohavi & Kunz 97] <author> Kohavi R., Kunz C.: </author> <title> Option Decision Trees with Majority Votes, </title> <booktitle> in Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Their net result is a two-fold reduction in ensemble size at an almost identical predictive error rate. Obviously, this does not improve intelligibility significantly, as it is only marginally easier to understand the combined behaviour of just 50 decision trees instead of 100 trees. Alternatively, Kohavi & Kunz <ref> [Kohavi & Kunz 97] </ref> have investigated option decision trees which they show to be comparable in terms of predictive accuracy to bagging, but which they claim to be easier to understand and to interpret because "option decision trees provide the human with a single structure that is easier to interpret, albeit
Reference: [Kohavi & Li 95] <author> Kohavi R., Li C.-H.: </author> <title> Oblivious Decision Trees, Graphs, and Top-Down Pruning, </title> <editor> in Mellish C.S.(ed.), </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, pp.1071-1077, </address> <year> 1995. </year>
Reference-contexts: Furthermore, as these are a very regular decision-trees due to the way it is constructed, there are possibilities for further simplifications including logical pruning and transformation into a decision graph (In <ref> [Kohavi & Li 95] </ref> a procedure is defined for transforming so-called oblivious decision trees into decision graphs, and the trees constructed from ensembles of decision stumps happen to be almost oblivious). The next section defines the algorithms used for selecting stumps during ensemble induction and for optional post-pruning of ensembles. <p> Admittedly, this is a best-case example, that was wisely chosen, but in general especially post-pruned ensembles seem to stay within reasonably bounded complexity. We have not yet automated this visualization process, but it should be straightforward to implement the mechanisms described in <ref> [Kohavi & Li 95] </ref> for this purpose. 4 Conclusions and Discussion We have devised a simple ensemble-generating procedure which enjoys a reasonable degree of intelligibility at the expense of less dramatic improvement of predictive accuracy (when compared to e.g. boosting).
Reference: [Margineantu D.D & Dietterich T.G.] <author> Margineantu D.D., Dietterich T.G.: </author> <title> Pruning Adaptive Boosting, </title> <booktitle> in Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: Breiman [Breiman 96a] wrote that "What one loses, with the [bagging of] trees, is a simple and interpretable structure. What one gains is increased accuracy." So far few have attempted to remedy this situation. Various heuristics for pruning of ensembles are introduced in <ref> [Margineantu D.D & Dietterich T.G.] </ref>. Their net result is a two-fold reduction in ensemble size at an almost identical predictive error rate. Obviously, this does not improve intelligibility significantly, as it is only marginally easier to understand the combined behaviour of just 50 decision trees instead of 100 trees.
Reference: [Merz & Murphy 96] <author> Merz, C.J., Murphy, </author> <title> P.M.: UCI Repository of machine learning databases, </title> <institution> University of California, Department of Information and Computer Science, </institution> <address> Irvine, CA, </address> <year> 1996. </year>
Reference-contexts: We compare average predictive error rates and average tree-depths for C4.5 [Quinlan 93], for ensembles of decision-stumps of max-depth 2 and for pruned ensembles on a sample of small and medium sized databases available from the UC Irvine repository <ref> [Merz & Murphy 96] </ref>. These databases are Breast (BR), Colic (CO), Credit (CR), Diabetes (DI), German (GE), Labor (LA), Sonar (SO), Vote (VO), Lymph (L1,L2), Glass (G0,..,G5), and Iris (I0,I1,I2).
Reference: [Oliver 95] <author> Oliver J.J.: </author> <title> On Pruning and Averaging Decision Trees, </title> <editor> in Prieditis A. & Russell S.(eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML95), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: But the only principled way of choosing a good upper bound for ensemble sizes seems to be a (costly) wrapper-like approach based on cross-validation. A comparison to bayesian approaches as described in <ref> [Oliver 95, Buntine 91] </ref> is clearly necessary for judging the merits of the presented ideas.
Reference: [Quinlan 93] <author> Quinlan J.R.: C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Pruning an ensemble. In most experiments the effects of pruning were moderate, i.e. only a small number of stumps was usually pruned from the respective ensemble. 3 Experiments This section reports experimental results for the algorithm described above. We compare average predictive error rates and average tree-depths for C4.5 <ref> [Quinlan 93] </ref>, for ensembles of decision-stumps of max-depth 2 and for pruned ensembles on a sample of small and medium sized databases available from the UC Irvine repository [Merz & Murphy 96].

References-found: 10

