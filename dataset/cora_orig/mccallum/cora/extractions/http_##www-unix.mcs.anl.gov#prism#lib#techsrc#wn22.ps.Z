URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn22.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: A Case Study of MPI: Portable and Efficient Libraries*  
Author: Christian Bischof Steven Huss-Lederman Xiaobai Sun Anna Tsao Thomas Turnbull 
Abstract: In this paper, we discuss the performance achieved by several implementations of the recently defined Message Passing Interface (MPI) standard. In particular, performance results for different implementations of the broadcast operation are analyzed and compared on the Delta, Paragon, SP1 and CM5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Auslander, L. & A. Tsao, </author> <title> On parallelizable eigensolvers, </title> <journal> Adv. Appl. Math. </journal> <volume> 13 (1992), </volume> <pages> 253-261. </pages>
Reference-contexts: 1 Introduction For the past several years, members of the Parallel Research on Invariant Subspace Methods (PRISM) project have been investigating scalable parallel eigensolvers for distributed memory systems <ref> [1, 3] </ref>. The ultimate objective of this research is the development of portable and efficient libraries for this fundamental numerical linear algebra kernel. In the course of our work, we, like many other library developers, have been faced with many issues relating to portable programming.
Reference: [2] <author> Beguelin, A., J. J. Dongarra, G. A. Geist, R. Manchek & V. S. Sunderam, </author> <title> A user's guide to pvm parallel virtual machine, </title> <institution> ORNL/TM-11826, Oak Ridge National Laboratory (July 1991). </institution>
Reference-contexts: Calls to these routines are then either translated into native calls on each machine or into a message passing system provided by the third party. These strategies have been used in many packages including Chameleon [8], Express [14], p4 [4], PARMACS [9] and PVM <ref> [2] </ref>. A third alternative is to take a code written for one native message passing system and use a look-alike library to allow usage of the same code on another native message passing system (cf. [8, 12]). These solutions have several drawbacks.
Reference: [3] <author> Bischof, C. H., S. Huss-Lederman, X. Sun, & A. Tsao, </author> <title> The PRISM project: infrastructure and algorithms for parallel eigensolvers, </title> <booktitle> Proceedings, Scalable Parallel Libraries Conference (Starksville, </booktitle> <address> MS, </address> <month> Oct. </month> <pages> 6-8, </pages> <year> 1993), </year> <note> IEEE, </note> <year> 1993, </year> <pages> pp. 123-131, </pages> <note> (also PRISM Working Note #12). </note>
Reference-contexts: 1 Introduction For the past several years, members of the Parallel Research on Invariant Subspace Methods (PRISM) project have been investigating scalable parallel eigensolvers for distributed memory systems <ref> [1, 3] </ref>. The ultimate objective of this research is the development of portable and efficient libraries for this fundamental numerical linear algebra kernel. In the course of our work, we, like many other library developers, have been faced with many issues relating to portable programming.
Reference: [4] <author> Butler, R. & E. Lusk, </author> <title> User's guide to the p4 parallel programming system, </title> <journal> ANL-92/17, </journal> <note> Argonne National Laboratory (October 1992). </note>
Reference-contexts: Calls to these routines are then either translated into native calls on each machine or into a message passing system provided by the third party. These strategies have been used in many packages including Chameleon [8], Express [14], p4 <ref> [4] </ref>, PARMACS [9] and PVM [2]. A third alternative is to take a code written for one native message passing system and use a look-alike library to allow usage of the same code on another native message passing system (cf. [8, 12]). These solutions have several drawbacks.
Reference: [5] <author> Franke, H., P. Hochschild, P. Pattnaik & M. Snir, </author> <title> An efficient implementation of MPI, </title> <booktitle> IFIP WG10.3 Working Conference on Program,ing Environments for Massively Parallel Distributed Systems, </booktitle> <month> April 25-29, </month> <year> 1994, </year> <note> (also IBM Research Report: RC 19493(84718) 3/25/94). </note>
Reference-contexts: This MPI implementation runs on networks of workstations, the IBM SP1, the Intel Delta and Paragon, the Meiko CS-2, the Thinking Machine CM5 and other machines. In addition, IBM has produced its own version for the SP1 <ref> [5, 6] </ref>. Cray Research, Intel and Meiko have announced their intention to support MPI on their latest platforms. Other implementations of MPI have also been produced, such as those by Ohio State University and University of Edinburgh.
Reference: [6] <author> Franke, H., P. Hochschild, P. Pattnaik & M. Snir, </author> <title> MPI-F: An efficient implementation of MPI on IBM-SP1, </title> <booktitle> 1994 International Conference on Parallel Processing, </booktitle> <month> August 15-19, </month> <year> 1994. </year>
Reference-contexts: This MPI implementation runs on networks of workstations, the IBM SP1, the Intel Delta and Paragon, the Meiko CS-2, the Thinking Machine CM5 and other machines. In addition, IBM has produced its own version for the SP1 <ref> [5, 6] </ref>. Cray Research, Intel and Meiko have announced their intention to support MPI on their latest platforms. Other implementations of MPI have also been produced, such as those by Ohio State University and University of Edinburgh.
Reference: [7] <author> Gropp, W. & E. Lusk, </author> <title> An abstract device definition to support the implementation of a high-level point-to-point message-passing interface, </title> <institution> MCS-P342-1193, Argonne National Laboratory (1993). </institution>
Reference-contexts: Since the final release of the MPI standard in April 1994, a number of MPI implementations have been introduced. The most widely available is the Argonne National Laboratory/Mississippi State (ANL/MS) portable, public domain version <ref> [7] </ref>. This MPI implementation runs on networks of workstations, the IBM SP1, the Intel Delta and Paragon, the Meiko CS-2, the Thinking Machine CM5 and other machines. In addition, IBM has produced its own version for the SP1 [5, 6].
Reference: [8] <author> Gropp, William D. & Barry Smith, </author> <title> Chameleon parallel programming tools users manual, </title> <address> ANL-93/23, Argonne National Laboratory (March 1993). </address>
Reference-contexts: Another method is to use the communication routines provided by a third party. Calls to these routines are then either translated into native calls on each machine or into a message passing system provided by the third party. These strategies have been used in many packages including Chameleon <ref> [8] </ref>, Express [14], p4 [4], PARMACS [9] and PVM [2]. A third alternative is to take a code written for one native message passing system and use a look-alike library to allow usage of the same code on another native message passing system (cf. [8, 12]). <p> A third alternative is to take a code written for one native message passing system and use a look-alike library to allow usage of the same code on another native message passing system (cf. <ref> [8, 12] </ref>). These solutions have several drawbacks. First, the library writers must either port the communications software themselves or depend on software from a third party. Second, debugging is often complicated by the fact that calls are converted into native calls on each system.
Reference: [9] <author> Hempel, R., </author> <title> The ANL/GMD macros (PARMACS) in Fortran for portable parallel parogramming using the message passing programming model users' guide and reference manual, GMD (November 1991). </title>
Reference-contexts: Calls to these routines are then either translated into native calls on each machine or into a message passing system provided by the third party. These strategies have been used in many packages including Chameleon [8], Express [14], p4 [4], PARMACS <ref> [9] </ref> and PVM [2]. A third alternative is to take a code written for one native message passing system and use a look-alike library to allow usage of the same code on another native message passing system (cf. [8, 12]). These solutions have several drawbacks.
Reference: [10] <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Optimizing communication primitives on the Intel Touchstone Delta, </title> <type> Technical Report, </type> <note> Supercomputing Research Center (to appear). </note>
Reference-contexts: If the processors are connected in a linear array in the order given in the list passed to the routine, then no contention for wires will occur during the algorithm. A more detailed description of this routine can be found in <ref> [10] </ref>. For comparison purposes, two or three versions of broadcast were run. The first version simply uses the MPI collective call. The second version implements our algorithm utilizing MPI send and receive calls to perform message passing.
Reference: [11] <author> Littlefield, R., </author> <title> Characterizing and tuning communications performance for real applications, presentation overheads, </title> <booktitle> Proceedings, First Intel Delta Applications Workshop, </booktitle> <month> CCSF-14-92 (February, </month> <year> 1992), </year> <title> Caltech Concurrent Supercomputing Facilities, </title> <address> Pasadena, California, </address> <year> 1992, </year> <pages> pp. 179-190. </pages>
Reference-contexts: Using ready mode allows some systems to avoid using the normal flow control protocols and bypass the use of system buffers. The flow control protocol on the Delta causes forced-type to be approximately 25% faster than regular-type for long messages <ref> [15, 11] </ref>. The flow control protocol on the Paragon has been improved so that this is no longer the case. However, forced-type can still be a win for longer messages since it avoids system buffering on the receiving end.
Reference: [12] <author> Meiko Limited, </author> <title> Tagged Message Passing & Global Reduction (1993). </title>
Reference-contexts: A third alternative is to take a code written for one native message passing system and use a look-alike library to allow usage of the same code on another native message passing system (cf. <ref> [8, 12] </ref>). These solutions have several drawbacks. First, the library writers must either port the communications software themselves or depend on software from a third party. Second, debugging is often complicated by the fact that calls are converted into native calls on each system.
Reference: [13] <author> Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface standard, </title> <institution> Computer Science Dept. </institution> <note> Technical Report CS-94-230 (April 1994), </note> <institution> University of Tennessee, </institution> <note> (To appear in the International Journal of Supercomputer Applications, Volume 8, Number 3/4, </note> <year> 1994). </year>
Reference-contexts: The recently completed Message Passing Interface (MPI) standard defines a common set of message passing routines that will be supported by most vendors <ref> [13] </ref>. Its design allows for implementation on a wide range of platforms, including networks of workstations and homogeneous or heterogeneous Massively Parallel Processors (MPPs).
Reference: [14] <author> Parasoft Corporation, </author> <title> Express Version 1.0: A Communication Environment for Parallel Computers, </title> <year> 1988. </year>
Reference-contexts: Calls to these routines are then either translated into native calls on each machine or into a message passing system provided by the third party. These strategies have been used in many packages including Chameleon [8], Express <ref> [14] </ref>, p4 [4], PARMACS [9] and PVM [2]. A third alternative is to take a code written for one native message passing system and use a look-alike library to allow usage of the same code on another native message passing system (cf. [8, 12]). These solutions have several drawbacks.
Reference: [15] <author> Regnier, G., </author> <title> Intel Touchstone Delta message passing performance, Intel Supercomputing Systems Division (August, 1991), </title> <type> preprint. </type>
Reference-contexts: Using ready mode allows some systems to avoid using the normal flow control protocols and bypass the use of system buffers. The flow control protocol on the Delta causes forced-type to be approximately 25% faster than regular-type for long messages <ref> [15, 11] </ref>. The flow control protocol on the Paragon has been improved so that this is no longer the case. However, forced-type can still be a win for longer messages since it avoids system buffering on the receiving end.
References-found: 15

