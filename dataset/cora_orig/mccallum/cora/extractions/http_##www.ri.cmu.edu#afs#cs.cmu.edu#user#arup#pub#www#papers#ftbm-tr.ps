URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/arup/pub/www/papers/ftbm-tr.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/arup/pub/www/resume.html
Root-URL: 
Title: Measuring Software Dependability by Robustness Benchmarking  
Author: Arup Mukherjee Daniel P. Siewiorek 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: May 1994  
Abstract: Inability to identify weaknesses or to quantify advancements in software system robustness frequently hinders the development of robust software systems. Efforts have been made to develop benchmarks of software robustness to address this problem, but they all suffer from significant shortcomings. This paper presents the various features that are desirable in a benchmark of system robustness, and evaluates some existing benchmarks according to these features. A new hierarchically structured approach to building robustness benchmarks, which overcomes many deficiencies of past efforts, is also presented. This approach has been applied to building a hierarchically structured benchmark that tests part of the Unix file and virtual memory systems. The resultant benchmark has successfully been used to identify new response class stuctures that were not detected in a similar situation by other less organized techniques. This research was supported in part by the Computer Sciences Corporation under contract number G.S.09K90BHD0001LFP and by the Office of Naval Research under contract number N00014-91-J-4139. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of CSC or ONR. 
Abstract-found: 1
Intro-found: 1
Reference: [Barton90] <author> J. H. Barton, E. W. Czeck, Z. Z. Segall, D. P. Siewiorek, </author> <title> "Fault Injection Experiments Using FIAT," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Volume 39, Number 4, </volume> <month> April </month> <year> 1990, </year> <pages> pp. 575-582. </pages>
Reference-contexts: The possible effects of a fault may be classified according to any of several taxonomies, such as those of <ref> [Barton90] </ref>, [Cristian91], or [Suh93], which are summarized in Table 1. All of these taxonomies necessitate a means of measuring the effect on processes other than those owned by the benchmark itself.
Reference: [Cristian91] <author> F. Cristian, </author> <title> "Understanding fault-tolerant distributed systems," </title> <journal> Communications of the ACM, </journal> <volume> Volume 34, Number 2, </volume> <month> February </month> <year> 1991, </year> <pages> pp. 56-78. </pages>
Reference-contexts: The possible effects of a fault may be classified according to any of several taxonomies, such as those of [Barton90], <ref> [Cristian91] </ref>, or [Suh93], which are summarized in Table 1. All of these taxonomies necessitate a means of measuring the effect on processes other than those owned by the benchmark itself.
Reference: [Curnow76] <author> H. J. Curnow, B. A. Wichmann, </author> <title> "A Synthetic Benchmark," </title> <journal> The Computer Journal, </journal> <volume> Volume 19, Number 1, </volume> <year> 1976, </year> <pages> pp. 43-49. </pages>
Reference-contexts: Later, focus shifted to attempts to measure overall system performance in scenarios designed to reflect common uses of the system. The latter approach led first to synthetic benchmarks, such as Whetstone <ref> [Curnow76] </ref> and Dhrystone [Weicker84], and then to applications oriented benchmarks such as the SPEC [SPEC90] suite, which measures 1 performance under prototypical workloads built from a collection of real applications. Similarly, the advent of reliable computing systems is spurring the development of robustness benchmarks to quantify improvements in system reliability.
Reference: [Dingman93] <author> C. Dingman, D. Siewiorek, </author> <title> "Measuring Robustness of a Fault Tolerant Aerospace System," </title> <note> unpublished. </note>
Reference-contexts: of Result Class Tested Tests correct unexpected error bad success terminated warm restart cold restart read file 252 175 77 0 0 0 0 write file 252 178 42 24 0 0 0 Table 7: The results of running the read file and write file tests on an ASCM system <ref> [Dingman93] </ref>.
Reference: [Kanawati92] <author> G. A. Kanawati, N. A. Kanawati, J. A. Abraham, "FERRARI: </author> <title> a tool for the validation of system dependability properties," </title> <booktitle> The 1992 IEEE Workshop on Fault-Tolerant Parallel and Distributed Systems, </booktitle> <address> Amherst, MA, USA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Differentiation amongst systems should reflect the number of such errors uncovered. In attempting to design a useful benchmark with the most general applicability, several issues must be considered. For example, if a benchmark simulates memory faults via fault injection into the supervisor code of an operating system (as in <ref> [Kanawati92] </ref> or [Kao93]), it is not likely to be easily portable between operating systems, perhaps not even between operating systems that are very similar from an application's point of view similar operating system interfaces are often backed by very different bodies of code.
Reference: [Kao93] <author> W.-I. Kao, R. K. Iyer, D. Tang, </author> <title> "FINE: A fault injection and monitoring environment for tracing the UNIX system behavior under faults," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Volume 19, Number 11, </volume> <month> November </month> <year> 1993, </year> <pages> pp. 1105-18. </pages>
Reference-contexts: In attempting to design a useful benchmark with the most general applicability, several issues must be considered. For example, if a benchmark simulates memory faults via fault injection into the supervisor code of an operating system (as in [Kanawati92] or <ref> [Kao93] </ref>), it is not likely to be easily portable between operating systems, perhaps not even between operating systems that are very similar from an application's point of view similar operating system interfaces are often backed by very different bodies of code.
Reference: [Miller90] <author> B. P. Miller, L. Fredriksen, B. </author> <title> So, "An Empirical Study of the Reliability of UNIX Utilities," </title> <journal> Communications of the ACM, </journal> <volume> Volume 33, Number 12, </volume> <month> December </month> <year> 1990, </year> <pages> pp. 32-43. </pages>
Reference-contexts: To date, much of the effort in building robust systems has been devoted to building robust hardware. Efforts to evaluate the robustness of software systems have become common only recently, and are exemplified by such studies as <ref> [Miller90] </ref> and [Suh93]. These studies both concentrate, like all robustness benchmarks, on studying the behavior produced when a system is subjected to unusual (rather than common-case) stimuli. Both studies perform their evaluations via a collection of isolated tests, and draw conclusions from the collected results. <p> Both studies perform their evaluations via a collection of isolated tests, and draw conclusions from the collected results. Unfortunately, it is often difficult to evaluate the relative significance of each of the individual results collected through such test suites. For example, <ref> [Miller90] </ref> examines the behavior of Unix utilities when they are supplied with randomly generated input data. <p> Thus, if several utilities crash due to a bug in an underlying shared system library, the robustness of the system being measured might be perceived to be unduly low | the robustness benchmark is affected by a lack of knowledge of the system's structure. <ref> [Miller90] </ref> and other such studies are similar to synthetic benchmarks in the performance arena | The validity of these benchmarks depends on the accuracy with which they are constructed to emulate the normal workload of the system.
Reference: [Russinovich92] <author> M. E. Russinovich, Z. Segall, </author> <title> "Open System Fault Management: Fault Tolerant MACH," </title> <institution> Research Report # CMUCDS-92-8, CMU Research Center for Dependable Systems, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213-3890. </address>
Reference-contexts: This localization can be further improved by restricting the tests to only a subset of the Unix system calls. Such restriction, together with the monitoring of system calls via the sentry mechanism described in <ref> [Russinovich92] </ref>, has been used successfully to identify some errors in the Mach 3.0 Unix server. 5.3. Modular Benchmarks Another approach is that of modular benchmarking. Modular benchmarks are separate tests of individual system modules.
Reference: [SPEC90] <institution> Standard Performance Evaluation Corporation, </institution> <note> SPEC Newsletter, Volume 2, Issue 2, Spring 1990, Waterside Assoc, Freemont, CA. </note>
Reference-contexts: Later, focus shifted to attempts to measure overall system performance in scenarios designed to reflect common uses of the system. The latter approach led first to synthetic benchmarks, such as Whetstone [Curnow76] and Dhrystone [Weicker84], and then to applications oriented benchmarks such as the SPEC <ref> [SPEC90] </ref> suite, which measures 1 performance under prototypical workloads built from a collection of real applications. Similarly, the advent of reliable computing systems is spurring the development of robustness benchmarks to quantify improvements in system reliability.
Reference: [Suh93] <author> B.-H. Suh, J. Hudak, D. Siewiorek, Z. Segall, </author> <title> "Development of a Benchmark to Measure System Robustness," Fault Tolerant Computing Systems: </title> <booktitle> Twenty-Third International Symposium, </booktitle> <address> Toulouse, Frane, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: To date, much of the effort in building robust systems has been devoted to building robust hardware. Efforts to evaluate the robustness of software systems have become common only recently, and are exemplified by such studies as [Miller90] and <ref> [Suh93] </ref>. These studies both concentrate, like all robustness benchmarks, on studying the behavior produced when a system is subjected to unusual (rather than common-case) stimuli. Both studies perform their evaluations via a collection of isolated tests, and draw conclusions from the collected results. <p> The possible effects of a fault may be classified according to any of several taxonomies, such as those of [Barton90], [Cristian91], or <ref> [Suh93] </ref>, which are summarized in Table 1. All of these taxonomies necessitate a means of measuring the effect on processes other than those owned by the benchmark itself. <p> Modular benchmarks are separate tests of individual system modules. These benchmarks are constructed by regarding the system as a collection of isolated modules, and writing one or more tests to exercise each module independently. One example of a modular benchmark is documented in <ref> [Suh93] </ref>. Another example is a set of robustness benchmarks that was recently constructed at CMU to test the robustness of the Advanced Space-borne Computer Module (ASCM)[Dingman93].
Reference: [Sullivan91] <author> M. Sullivan, R. Chillarege, </author> <title> "Software Defects and their Impact on System Availability | A Study of Field Failures in Operating Systems," Fault Tolerant Computing Systems: </title> <booktitle> Twenty-First International Symposium, </booktitle> <address> Montreal, Que., Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: A closely related issue is the amount of localization of triggering events <ref> [Sullivan91] </ref> that is reflected in the reported results | i.e. the extent to which the results pinpoint the error (s) that were detected, and their possible causes. Good localization is especially valuable to system designers trying to focus on improving the depenability of the operating system.
Reference: [Weicker84] <author> R. P. Weicker, "Dhrystone: </author> <title> A Synthetic Systems Programming Benchmark," </title> <journal> Communications of the ACM, </journal> <volume> Volume 27, Number 10, </volume> <month> October </month> <year> 1984, </year> <pages> pp. 1013-1030 </pages>
Reference-contexts: Later, focus shifted to attempts to measure overall system performance in scenarios designed to reflect common uses of the system. The latter approach led first to synthetic benchmarks, such as Whetstone [Curnow76] and Dhrystone <ref> [Weicker84] </ref>, and then to applications oriented benchmarks such as the SPEC [SPEC90] suite, which measures 1 performance under prototypical workloads built from a collection of real applications. Similarly, the advent of reliable computing systems is spurring the development of robustness benchmarks to quantify improvements in system reliability.
Reference: [Weiderman90] <author> N. Weiderman, "Hartstone: </author> <title> Synthetic Benchmark Requirements for Hard Real-Time Applications," </title> <journal> Ada Letters, </journal> <volume> Volume 10, Number 3, </volume> <month> Winter </month> <year> 1990, </year> <pages> pp. 126-36. 24 </pages>
References-found: 13

