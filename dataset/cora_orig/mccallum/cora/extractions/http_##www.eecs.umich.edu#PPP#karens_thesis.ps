URL: http://www.eecs.umich.edu/PPP/karens_thesis.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: Domain Decomposition, Irregular Applications, and Parallel Computers  
Author: by Karen Arnold Tomko Dr. Santosh G. Abraham, Co-Chair Professor Edward S. Davidson, Co-Chair Professor Gregory M. Hulbert 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science and Engineering) in The  Doctoral Committee:  Professor Trevor N. Mudge Professor Quentin F. Stout  
Date: 1995  
Affiliation: University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Harold S. Stone and John Cocke. </author> <booktitle> Computer architecture in the 1990s. IEEE Computer, </booktitle> <pages> pages 30-38, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The nodes are interconnected by a ring or ring hierarchy (KSR1, KSR2, SPP-1000), low-dimensional mesh (T3D) or a multistage network (SP1, SP2). The latency of accessing remote memory (i.e. memory in some other node) through this interconnect is two to three orders of magnitude higher than accessing local memory <ref> [1] </ref>.
Reference: [2] <author> Marsha J. Berger and Shahid H. Bokhari. </author> <title> A partitioning strategy for nonuniform problems on multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):570-580, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition <ref> [2] </ref>, recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], [5], [6], greedy algorithms [7], [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms.
Reference: [3] <author> Stephen Barnard and Horst Simon. </author> <title> A fast multilevel implementation of recursive spectral bisections for partitioning unstructured problems. </title> <type> Technical Report RNR-92-33, </type> <institution> NASA Ames Research Center, NAS Systems Division, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition <ref> [3] </ref>, [4], [5], [6], greedy algorithms [7], [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], <p> bisection (RSB) decomposition <ref> [3] </ref>, [4], [5], [6], greedy algorithms [7], [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14]. In this dissertation we perform domain decomposition with multi 3 level algorithms based on both spectral and greedy methods. However, our methods are general and can be used with other decomposition algorithms. <p> However, our methods are general and can be used with other decomposition algorithms. Domain decomposition tools have become available which implement several of the algorithms mentioned above. We have used the RSB <ref> [3] </ref>, TOP/DOMDEC [15], Chaco [16], and Metis [17] decomposition packages in conjunction with our research. Many of the algorithms support weighted input graphs and can be used to partition heterogeneous domains. <p> This approach is used by many implementations and provides partitions which are as good or better than the partitions given by traditional single level algorithms <ref> [3] </ref>, [12], [7], [13], and [14]. Like the greedy algorithm, multilevel algorithms can be implemented as a recursive bisection or as a k-way partition algorithm. A bisection algorithm recursively performs multilevel bisection in which each step splits the graph into two subgraphs. <p> Reproduced from [56] 87 elements in the A matrix for the two data sets respectively. The data sets were partitioned into subdomains using a program provided by Barnard and Simon which implements their Recursive Spectral Bisection (RSB) algorithm <ref> [3] </ref>. The A matrix was partitioned directly (as opposed to partitioning the finite element mesh) since the structure of the A matrix determines the communication pattern within the iterative solver. The A matrix is read-only; hence the communication pattern is static.
Reference: [4] <author> Horst Simon. </author> <title> Partitioning of unstructured problems for parallel processing. </title> <journal> Computing Systems in Engineering, </journal> 2(2/3):135-148, 1991. 
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], <ref> [4] </ref>, [5], [6], greedy algorithms [7], [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. <p> Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], <ref> [4] </ref>, [5], [6], greedy algorithms [7], [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14]. <p> In the subsequent paragraphs we illustrate steps 1) through 4) by calculating vertex 28 integer array A mask <ref> [4; 4] </ref> real array A [4; 4] integer i; j do i 1 to 4 if (A mask (i; j) = 1) then A (i; j) F (A (i; j); A (i 1; j); A (i + 1; j); A (i; j 1); A (i; j + 1)) else if (a <p> In the subsequent paragraphs we illustrate steps 1) through 4) by calculating vertex 28 integer array A mask <ref> [4; 4] </ref> real array A [4; 4] integer i; j do i 1 to 4 if (A mask (i; j) = 1) then A (i; j) F (A (i; j); A (i 1; j); A (i + 1; j); A (i; j 1); A (i; j + 1)) else if (a mask (i; j) = 2)
Reference: [5] <author> Bruce Hendrickson and Robert Leland. </author> <title> Multidimensional spectral load balancing. </title> <type> Technical Report SAND93-0074, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], <ref> [5] </ref>, [6], greedy algorithms [7], [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. <p> Thus all vertices below the median belong to one subdomain and all those above the median belong to the other subdomain. Bisection is then applied recursively to the graph until the desired number of subdomains are achieved. Spectral algorithm formulations are given in [3],[4], and <ref> [5] </ref>. Spectral based algorithms have a global view of the graph since all vertices are ordered 13 by the Fiedler vector and they perform very well in comparison to other domain decompo-sition methods on a wide variety of input graphs.
Reference: [6] <author> Bruce Hendrickson and Robert Leland. </author> <title> An improved spectral graph partitioning algorithm for mapping parallel computations. </title> <journal> SIAM Journal Scientific Computing, </journal> <volume> 16(2) </volume> <pages> 452-469, </pages> <year> 1995. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], [5], <ref> [6] </ref>, greedy algorithms [7], [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14].
Reference: [7] <author> George Karypis and Vipin Kumar. </author> <title> A fast and high quality multilevel scheme for partitioning irregular graphs. </title> <type> Technical Report TR 95-035, </type> <institution> Dept. Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], [5], [6], greedy algorithms <ref> [7] </ref>, [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14]. <p> Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], [5], [6], greedy algorithms <ref> [7] </ref>, [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14]. <p> decomposition [3], [4], [5], [6], greedy algorithms <ref> [7] </ref>, [8], [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14]. In this dissertation we perform domain decomposition with multi 3 level algorithms based on both spectral and greedy methods. However, our methods are general and can be used with other decomposition algorithms. Domain decomposition tools have become available which implement several of the algorithms mentioned above. <p> As each vertex is annexed its neighbors become candidates for annexation. Subdomain growth continues in this manner until the desired sum of vertex weights for the subdomain is reached <ref> [7] </ref>, [8] and [9]. The greedy algorithm can be implemented as a recursive bisection algorithm or it can be implemented as a k-way partitioning algorithm. As a bisection algorithm only one subdomain is grown and the remaining vertices are assigned to the other subdomain. <p> This approach is used by many implementations and provides partitions which are as good or better than the partitions given by traditional single level algorithms [3], [12], <ref> [7] </ref>, [13], and [14]. Like the greedy algorithm, multilevel algorithms can be implemented as a recursive bisection or as a k-way partition algorithm. A bisection algorithm recursively performs multilevel bisection in which each step splits the graph into two subgraphs. <p> Graph coarsening based on matching algorithms is described in <ref> [7] </ref>, [14] and [12]. 16 Uncoarsening and Refinement Algorithms The uncoarsening and refinement phase (sometimes simply referred to as the refinement phase) has two functions: project the partition of a coarse graph onto the next finer graph and locally refine the partition for the finer graph. <p> Many different types of algorithms have been used for refinement for single and multilevel decomposition algorithms; one of the most common is the Kernighan-Lin (KL) heuristic and its variations. Both bisection and k-way extensions of the original KL heuristic are used in multilevel algorithms [12], <ref> [7] </ref>, [14]. The basic idea behind the KL heuristic is to compute a gain associated with moving a vertex into a different subdomain. The gain is defined as the net reduction in the cost of cut edges that results if the vertex is moved. <p> Spectral and Greedy algorithms have each been used for the bisection phase of multilevel algorithms [12], <ref> [7] </ref>. We have developed a bisection algorithm based on the greedy algorithm which maintains load balance between the two sets of weights as a constraint. Our greedy algorithm is similar to the Greedy Graph Growing Algorithm (GGGP) used in Metis [7]. <p> been used for the bisection phase of multilevel algorithms [12], <ref> [7] </ref>. We have developed a bisection algorithm based on the greedy algorithm which maintains load balance between the two sets of weights as a constraint. Our greedy algorithm is similar to the Greedy Graph Growing Algorithm (GGGP) used in Metis [7]. Our dual weight algorithm is described below following the definition of some relevant terms. gain The gain of a vertex, v is the decrease in the edge-cut if v is added to V .
Reference: [8] <author> Charbel Farhat. </author> <title> A simple and efficient automatic FEM domain decomposer. </title> <journal> Computers & Structures, </journal> <volume> 28(5) </volume> <pages> 579-602, </pages> <year> 1988. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], [5], [6], greedy algorithms [7], <ref> [8] </ref>, [9], and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14]. <p> As each vertex is annexed its neighbors become candidates for annexation. Subdomain growth continues in this manner until the desired sum of vertex weights for the subdomain is reached [7], <ref> [8] </ref> and [9]. The greedy algorithm can be implemented as a recursive bisection algorithm or it can be implemented as a k-way partitioning algorithm. As a bisection algorithm only one subdomain is grown and the remaining vertices are assigned to the other subdomain.
Reference: [9] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], [5], [6], greedy algorithms [7], [8], <ref> [9] </ref>, and simulated annealing. References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14]. <p> As each vertex is annexed its neighbors become candidates for annexation. Subdomain growth continues in this manner until the desired sum of vertex weights for the subdomain is reached [7], [8] and <ref> [9] </ref>. The greedy algorithm can be implemented as a recursive bisection algorithm or it can be implemented as a k-way partitioning algorithm. As a bisection algorithm only one subdomain is grown and the remaining vertices are assigned to the other subdomain.
Reference: [10] <author> Charbel Farhat and Michel Lesoinne. </author> <title> Automatic partitioning of unstructured meshes for the parallel solution of problems in computational mechanics. </title> <journal> International Journal for Numerical Methods in Engineering, </journal> <volume> 36 </volume> <pages> 745-764, </pages> <year> 1993. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], [5], [6], greedy algorithms [7], [8], [9], and simulated annealing. References [4], <ref> [10] </ref>, [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14].
Reference: [11] <author> Robert Leland and Bruce Hendrickson. </author> <title> An empirical study of static load balancing algorithms. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 682-685, </pages> <year> 1994. </year>
Reference-contexts: Many domain decomposition algorithms have been proposed. Some examples are: binary decomposition [2], recursive coordinate bisection (RCB), recursive graph bisection (RGB), recursive spectral bisection (RSB) decomposition [3], [4], [5], [6], greedy algorithms [7], [8], [9], and simulated annealing. References [4], [10], <ref> [11] </ref> and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and [14].
Reference: [12] <author> Bruce Hendrickson and Robert Leland. </author> <title> A multilevel algorithm for partitioning graphs. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year> <note> to appear in. 102 </note>
Reference-contexts: References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], <ref> [12] </ref>, [7], [13], and [14]. In this dissertation we perform domain decomposition with multi 3 level algorithms based on both spectral and greedy methods. However, our methods are general and can be used with other decomposition algorithms. <p> This approach is used by many implementations and provides partitions which are as good or better than the partitions given by traditional single level algorithms [3], <ref> [12] </ref>, [7], [13], and [14]. Like the greedy algorithm, multilevel algorithms can be implemented as a recursive bisection or as a k-way partition algorithm. A bisection algorithm recursively performs multilevel bisection in which each step splits the graph into two subgraphs. <p> Graph coarsening based on matching algorithms is described in [7], [14] and <ref> [12] </ref>. 16 Uncoarsening and Refinement Algorithms The uncoarsening and refinement phase (sometimes simply referred to as the refinement phase) has two functions: project the partition of a coarse graph onto the next finer graph and locally refine the partition for the finer graph. <p> Many different types of algorithms have been used for refinement for single and multilevel decomposition algorithms; one of the most common is the Kernighan-Lin (KL) heuristic and its variations. Both bisection and k-way extensions of the original KL heuristic are used in multilevel algorithms <ref> [12] </ref>, [7], [14]. The basic idea behind the KL heuristic is to compute a gain associated with moving a vertex into a different subdomain. The gain is defined as the net reduction in the cost of cut edges that results if the vertex is moved. <p> Spectral and Greedy algorithms have each been used for the bisection phase of multilevel algorithms <ref> [12] </ref>, [7]. We have developed a bisection algorithm based on the greedy algorithm which maintains load balance between the two sets of weights as a constraint. Our greedy algorithm is similar to the Greedy Graph Growing Algorithm (GGGP) used in Metis [7].
Reference: [13] <author> George Karypis and Vipin Kumar. </author> <title> Analysis of multilevel graph partitioning. </title> <type> Technical Report TR 95-037, </type> <institution> Dept. Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], <ref> [13] </ref>, and [14]. In this dissertation we perform domain decomposition with multi 3 level algorithms based on both spectral and greedy methods. However, our methods are general and can be used with other decomposition algorithms. Domain decomposition tools have become available which implement several of the algorithms mentioned above. <p> This approach is used by many implementations and provides partitions which are as good or better than the partitions given by traditional single level algorithms [3], [12], [7], <ref> [13] </ref>, and [14]. Like the greedy algorithm, multilevel algorithms can be implemented as a recursive bisection or as a k-way partition algorithm. A bisection algorithm recursively performs multilevel bisection in which each step splits the graph into two subgraphs.
Reference: [14] <author> George Karypis and Vipin Kumar. </author> <title> Multilevel k-way partitioning scheme for irregular graphs. </title> <type> Technical Report TR 95-0??, </type> <institution> Dept. Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: References [4], [10], [11] and [7] provide comparisons of several algorithms. Recently, very fast multilevel variations of both spectral and greedy algorithms have become available which partition as well as their traditional single level counterparts [3], [12], [7], [13], and <ref> [14] </ref>. In this dissertation we perform domain decomposition with multi 3 level algorithms based on both spectral and greedy methods. However, our methods are general and can be used with other decomposition algorithms. Domain decomposition tools have become available which implement several of the algorithms mentioned above. <p> This approach is used by many implementations and provides partitions which are as good or better than the partitions given by traditional single level algorithms [3], [12], [7], [13], and <ref> [14] </ref>. Like the greedy algorithm, multilevel algorithms can be implemented as a recursive bisection or as a k-way partition algorithm. A bisection algorithm recursively performs multilevel bisection in which each step splits the graph into two subgraphs. <p> Graph coarsening based on matching algorithms is described in [7], <ref> [14] </ref> and [12]. 16 Uncoarsening and Refinement Algorithms The uncoarsening and refinement phase (sometimes simply referred to as the refinement phase) has two functions: project the partition of a coarse graph onto the next finer graph and locally refine the partition for the finer graph. <p> Many different types of algorithms have been used for refinement for single and multilevel decomposition algorithms; one of the most common is the Kernighan-Lin (KL) heuristic and its variations. Both bisection and k-way extensions of the original KL heuristic are used in multilevel algorithms [12], [7], <ref> [14] </ref>. The basic idea behind the KL heuristic is to compute a gain associated with moving a vertex into a different subdomain. The gain is defined as the net reduction in the cost of cut edges that results if the vertex is moved.
Reference: [15] <author> Michael Sharp and Charbel Farhat. </author> <note> TOP/DOMDEC Version 1.4 User's Manual. </note> <institution> PGSoft and The University of Colorado, Boulder, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: However, our methods are general and can be used with other decomposition algorithms. Domain decomposition tools have become available which implement several of the algorithms mentioned above. We have used the RSB [3], TOP/DOMDEC <ref> [15] </ref>, Chaco [16], and Metis [17] decomposition packages in conjunction with our research. Many of the algorithms support weighted input graphs and can be used to partition heterogeneous domains. <p> Decoupling domain decomposition from the application itself has given us the flexibility to experiment with many domain decomposition algorithms and packages. We have used Chaco [16], Metis [17], and Top/Domdec <ref> [15] </ref> to generate decompositions for FCRASH. Obviously, this implementation is not designed for production use. For the experiments described in this chapter we used the multilevel spectral algorithm as implemented in Chaco V1.0 [16] to decompose the input files.
Reference: [16] <author> Bruce Hendrickson and Robert Leland. </author> <title> The Chaco user's guide: Version 2.0. </title> <type> Technical Report SAND94-2692, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: However, our methods are general and can be used with other decomposition algorithms. Domain decomposition tools have become available which implement several of the algorithms mentioned above. We have used the RSB [3], TOP/DOMDEC [15], Chaco <ref> [16] </ref>, and Metis [17] decomposition packages in conjunction with our research. Many of the algorithms support weighted input graphs and can be used to partition heterogeneous domains. <p> Decoupling domain decomposition from the application itself has given us the flexibility to experiment with many domain decomposition algorithms and packages. We have used Chaco <ref> [16] </ref>, Metis [17], and Top/Domdec [15] to generate decompositions for FCRASH. Obviously, this implementation is not designed for production use. For the experiments described in this chapter we used the multilevel spectral algorithm as implemented in Chaco V1.0 [16] to decompose the input files. <p> We have used Chaco <ref> [16] </ref>, Metis [17], and Top/Domdec [15] to generate decompositions for FCRASH. Obviously, this implementation is not designed for production use. For the experiments described in this chapter we used the multilevel spectral algorithm as implemented in Chaco V1.0 [16] to decompose the input files. The scripts for converting input file formats and sifting through profile data were developed in Perl. <p> Recently two software packages have become available which implement multilevel domain decomposition algorithms Chaco <ref> [16] </ref> and Metis [17]. Our multilevel algorithm is similar to the bisection algorithm found in Metis since we implemented our algorithm within the Metis package. Figure 3.1 diagrams a general bisection step of a multilevel algorithm, which is outlined below.
Reference: [17] <author> George Karypis and Vipin Kumar. </author> <title> METIS unstructured graph partitioning and sparse matrix ordering system, version 2.0. </title> <type> Technical Report TR 95-0??, </type> <institution> Dept. Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: However, our methods are general and can be used with other decomposition algorithms. Domain decomposition tools have become available which implement several of the algorithms mentioned above. We have used the RSB [3], TOP/DOMDEC [15], Chaco [16], and Metis <ref> [17] </ref> decomposition packages in conjunction with our research. Many of the algorithms support weighted input graphs and can be used to partition heterogeneous domains. <p> Decoupling domain decomposition from the application itself has given us the flexibility to experiment with many domain decomposition algorithms and packages. We have used Chaco [16], Metis <ref> [17] </ref>, and Top/Domdec [15] to generate decompositions for FCRASH. Obviously, this implementation is not designed for production use. For the experiments described in this chapter we used the multilevel spectral algorithm as implemented in Chaco V1.0 [16] to decompose the input files. <p> Recently two software packages have become available which implement multilevel domain decomposition algorithms Chaco [16] and Metis <ref> [17] </ref>. Our multilevel algorithm is similar to the bisection algorithm found in Metis since we implemented our algorithm within the Metis package. Figure 3.1 diagrams a general bisection step of a multilevel algorithm, which is outlined below.
Reference: [18] <author> David M. Nicol and Joel H. Saltz. </author> <title> An analysis of scatter decomposition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(11) </volume> <pages> 1337-1345, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Like weighted domain decomposition, scatter decomposition provides a static means of load balancing heterogeneous domains. This technique has been by analyzed by Nicol and Saltz <ref> [18] </ref> and used for a Volume Rendering application by Karia [19]. The main drawback of the method is that each processor is assigned several small disjoint regions of the domain which often destroys locality and increases the communication requirements of the application.
Reference: [19] <author> Raju J. Karia. </author> <title> Load balancing of parallel volume rendering with scattered decomposition. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 252-258, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Like weighted domain decomposition, scatter decomposition provides a static means of load balancing heterogeneous domains. This technique has been by analyzed by Nicol and Saltz [18] and used for a Volume Rendering application by Karia <ref> [19] </ref>. The main drawback of the method is that each processor is assigned several small disjoint regions of the domain which often destroys locality and increases the communication requirements of the application. We, instead, use weighted decomposition algorithms which attempt to provide a connected subdomain for each processor.
Reference: [20] <author> Terry W. Clark, Reinhard v. Hanxleden, J. Andrew McCammon, and L. Ridgway Scott. </author> <title> Parallelizing molecular dynamics using spatial decomposition. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 95-102, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Adaptive decomposition techniques have been developed for use with applications which have dynamic runtime behavior. Researchers have investigated adaptive decomposition for molecular dynamics application such as GROMOS or CHARMM <ref> [20] </ref>, [21], Direct Simulation Monte Carlo applications [22], N-body Methods [23], [24] and Adaptive Finite Element methods [25]. The adaptive algorithms periodically redistribute data in order to maintain load bal 4 ance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains.
Reference: [21] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Milan Hodoscek, and Bernard Brooks. </author> <title> Par-allelizing molecular dynamics programs for distributed memory machines: An application of the CHAOS runtime support library. </title> <type> Technical Report UMAICS-TR-94-125 or CS-TR-371, </type> <institution> UMAICS or University of Maryland Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Adaptive decomposition techniques have been developed for use with applications which have dynamic runtime behavior. Researchers have investigated adaptive decomposition for molecular dynamics application such as GROMOS or CHARMM [20], <ref> [21] </ref>, Direct Simulation Monte Carlo applications [22], N-body Methods [23], [24] and Adaptive Finite Element methods [25]. The adaptive algorithms periodically redistribute data in order to maintain load bal 4 ance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains. <p> However, on a shared-address system, the size of messages is fixed to the cache line size and the scheduling of messages (the timing of inter-cache transfers) is determined by the data layout and access pattern. The PARTI [30] [31] library and its successors CHAOS <ref> [21] </ref>, LARPAX [32] and PI-LAR [33] provide run-time support (RTS) for irregular computations on message-passing multiprocessors. These RTS libraries provide primitives for inspectors which analyze the dependency structure of the application, and executors which perform required interpro-cessor communication and translation between local and global data indices.
Reference: [22] <author> Bongki Moon and Joel Saltz. </author> <title> Adaptive runtime support for Direct Simulation Monte Carlo Methods on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 176-183, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Adaptive decomposition techniques have been developed for use with applications which have dynamic runtime behavior. Researchers have investigated adaptive decomposition for molecular dynamics application such as GROMOS or CHARMM [20], [21], Direct Simulation Monte Carlo applications <ref> [22] </ref>, N-body Methods [23], [24] and Adaptive Finite Element methods [25]. The adaptive algorithms periodically redistribute data in order to maintain load bal 4 ance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains.
Reference: [23] <author> Jaswinder Pal Singh, Chris Holt, Takashi Totsuka, Anoop Gupta, and John L. Hen-nessy. </author> <title> Load balancing and data locality in adaptive hierarchical n-body methods: Barnes-hut, fast multipole, and radiosity. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1995. </year>
Reference-contexts: Adaptive decomposition techniques have been developed for use with applications which have dynamic runtime behavior. Researchers have investigated adaptive decomposition for molecular dynamics application such as GROMOS or CHARMM [20], [21], Direct Simulation Monte Carlo applications [22], N-body Methods <ref> [23] </ref>, [24] and Adaptive Finite Element methods [25]. The adaptive algorithms periodically redistribute data in order to maintain load bal 4 ance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains.
Reference: [24] <author> Michael S. Warren and John K. Salmon. </author> <title> A parallel hashed oct-tree n-body algorithm. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 12-21, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Adaptive decomposition techniques have been developed for use with applications which have dynamic runtime behavior. Researchers have investigated adaptive decomposition for molecular dynamics application such as GROMOS or CHARMM [20], [21], Direct Simulation Monte Carlo applications [22], N-body Methods [23], <ref> [24] </ref> and Adaptive Finite Element methods [25]. The adaptive algorithms periodically redistribute data in order to maintain load bal 4 ance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains.
Reference: [25] <author> Karen D. Devine, Joseph E. Flaherty, Stephen R. Wheat, and Arthur B. Maccabe. </author> <title> A massively parallel adaptive finite element method with dynamic load balancing. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 2-11, </pages> <month> November </month> <year> 1993. </year> <month> 103 </month>
Reference-contexts: Adaptive decomposition techniques have been developed for use with applications which have dynamic runtime behavior. Researchers have investigated adaptive decomposition for molecular dynamics application such as GROMOS or CHARMM [20], [21], Direct Simulation Monte Carlo applications [22], N-body Methods [23], [24] and Adaptive Finite Element methods <ref> [25] </ref>. The adaptive algorithms periodically redistribute data in order to maintain load bal 4 ance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains. In either case communication is required to redistribute data at each rebalancing.
Reference: [26] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical schedul-ing scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Alternatively our methods can be used in conjunction with dynamic schemes to improve the initial decomposition and possibly reduce the number of redistribution steps required. 1.1.3 Dynamic Scheduling and Relaxed Synchronization Several "self-scheduling" techniques have been proposed for dynamically load balancing arbitrary parallel loops. Polychronopoulos introduced guided self-scheduling in <ref> [26] </ref> and Tzen and Ni propose Trapezoid self-scheduling in [27]. There is one draw-back to these and related approaches to load balancing: no locality is maintained between iterations. Thus on distributed memory multiprocessors, large communication penalties are possible.
Reference: [27] <author> Ten H. Tzen and Lionel M. Ni. </author> <title> Trapezoid self-scheduling: A practical scheduling scheme for parallel compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 87-98, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Polychronopoulos introduced guided self-scheduling in [26] and Tzen and Ni propose Trapezoid self-scheduling in <ref> [27] </ref>. There is one draw-back to these and related approaches to load balancing: no locality is maintained between iterations. Thus on distributed memory multiprocessors, large communication penalties are possible. Our methods, while not dynamic, do attempt to minimize interprocessor communication while carrying out the load balancing.
Reference: [28] <author> Rajiv Gupta. </author> <title> Synchronization and communication costs of loop partitioning on shared-memory multiprocessor systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 23-30, </pages> <year> 1989. </year>
Reference-contexts: Another way to improve the load balance of parallel applications is to provide a relaxed synchronization mechanism which allows each processor to perform some useful work while waiting for results from other processors. Gupta <ref> [28] </ref> and Eichenberger and Abraham [29] use one such approach, "fuzzy barriers". These approaches are useful for accommodating non-deterministic load imbalance due to random delays from communication contention, operating system overhead and small workload changes within an application. However, they can not accommodate large imbalances caused by poor application partitioning.
Reference: [29] <author> Alexandre E. Eichenberger and Santosh G. Abraham. </author> <title> Impact of load imbalance on the design of software barriers. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 63-72, </pages> <year> 1995. </year>
Reference-contexts: Another way to improve the load balance of parallel applications is to provide a relaxed synchronization mechanism which allows each processor to perform some useful work while waiting for results from other processors. Gupta [28] and Eichenberger and Abraham <ref> [29] </ref> use one such approach, "fuzzy barriers". These approaches are useful for accommodating non-deterministic load imbalance due to random delays from communication contention, operating system overhead and small workload changes within an application. However, they can not accommodate large imbalances caused by poor application partitioning.
Reference: [30] <author> Ravi Mirchandaney, Joel H. Saltz, Roger M. Smith, David M. Nicol, and Kay Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <year> 1988. </year>
Reference-contexts: However, on a shared-address system, the size of messages is fixed to the cache line size and the scheduling of messages (the timing of inter-cache transfers) is determined by the data layout and access pattern. The PARTI <ref> [30] </ref> [31] library and its successors CHAOS [21], LARPAX [32] and PI-LAR [33] provide run-time support (RTS) for irregular computations on message-passing multiprocessors.
Reference: [31] <author> Raja Das, Joel Saltz, and Harry Berryman. </author> <title> A Manual for PARTI Runtime Primitives. </title> <institution> NASA Langley Research Center, Institute for Computer Applications in Science and Engineering. </institution>
Reference-contexts: However, on a shared-address system, the size of messages is fixed to the cache line size and the scheduling of messages (the timing of inter-cache transfers) is determined by the data layout and access pattern. The PARTI [30] <ref> [31] </ref> library and its successors CHAOS [21], LARPAX [32] and PI-LAR [33] provide run-time support (RTS) for irregular computations on message-passing multiprocessors.
Reference: [32] <author> Scott R. Kohn and Scott B. Baden. </author> <title> A robust parallel programming model for dynamic non-uniform scientific computations. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 509-517, </pages> <year> 1994. </year>
Reference-contexts: However, on a shared-address system, the size of messages is fixed to the cache line size and the scheduling of messages (the timing of inter-cache transfers) is determined by the data layout and access pattern. The PARTI [30] [31] library and its successors CHAOS [21], LARPAX <ref> [32] </ref> and PI-LAR [33] provide run-time support (RTS) for irregular computations on message-passing multiprocessors. These RTS libraries provide primitives for inspectors which analyze the dependency structure of the application, and executors which perform required interpro-cessor communication and translation between local and global data indices.
Reference: [33] <author> Antonio Lain and Prithviraj Banerjee. </author> <title> Exploiting spatial regularity in irregular iterative applications. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 820-827, </pages> <year> 1995. </year>
Reference-contexts: However, on a shared-address system, the size of messages is fixed to the cache line size and the scheduling of messages (the timing of inter-cache transfers) is determined by the data layout and access pattern. The PARTI [30] [31] library and its successors CHAOS [21], LARPAX [32] and PI-LAR <ref> [33] </ref> provide run-time support (RTS) for irregular computations on message-passing multiprocessors. These RTS libraries provide primitives for inspectors which analyze the dependency structure of the application, and executors which perform required interpro-cessor communication and translation between local and global data indices.
Reference: [34] <author> Ravi Ponnusamy, Joel Saltz, Raja Das, Charles Koelbel, and Alok Choudhary. </author> <title> A runtime data mapping scheme for irregular problems. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 216-219, </pages> <year> 1992. </year>
Reference-contexts: We also determine which data items must be fetched from or transferred to another processor in Chapter 4. However, we derive this information directly from boundaries of the subdomains provided by the domain decomposition algorithm. Ponnusamy, et al. <ref> [34] </ref> and Lu and Chen [35] give methods for building an iteration dependence graph at runtime which is partitioned for parallel execution. Our method also requires generation of a runtime graph; however, we use a graph which represents the communication dependencies in the data structure not dependencies between loop iterations.
Reference: [35] <author> L.-C. Lu and M. Chen. </author> <title> Parallelizing loops with indirect array references or pointers. </title> <booktitle> In Proceedings of the 4th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 201-217, </pages> <year> 1991. </year>
Reference-contexts: We also determine which data items must be fetched from or transferred to another processor in Chapter 4. However, we derive this information directly from boundaries of the subdomains provided by the domain decomposition algorithm. Ponnusamy, et al. [34] and Lu and Chen <ref> [35] </ref> give methods for building an iteration dependence graph at runtime which is partitioned for parallel execution. Our method also requires generation of a runtime graph; however, we use a graph which represents the communication dependencies in the data structure not dependencies between loop iterations.
Reference: [36] <author> Peter Brezany, Michael Gerndt, Viera Sipkova, and Hans Zima. </author> <title> SUPERB support for irregular scientific computations. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 314-321, </pages> <year> 1992. </year>
Reference-contexts: We use a domain decomposition tool to partition the graph and determine the communication dependencies between the domains. Brezany, Gerndt, Sipkova and Zima have incorporated generation of calls to the PARTI library into their SUPERB compiler <ref> [36] </ref>. Several researchers have been involved with incorporating support for irregular applications into FORTRAN D and HPF compilers.
Reference: [37] <author> Reinhard v. Hanxleden and Ken Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transformations. </title> <institution> Technical Report Rice COMP TR92-181, Rice University, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Several researchers have been involved with incorporating support for irregular applications into FORTRAN D and HPF compilers. Das, Saltz, Hanxledon, Kennedy and Koelbel have developed compiler analysis methods for irregular applications <ref> [37] </ref>, [38], [39] and Ponnusamy, Saltz, Choudhary, Das, Uysal, and Hwang present methods for optimizing communication performance [40], [41].
Reference: [38] <author> Reinhard von Hanxleden. </author> <title> Handling irregular problems with Fortran D a preliminary report. </title> <type> Technical Report CRPC-TR93339-S, </type> <institution> Rice University, </institution> <month> October </month> <year> 1993. </year> <month> 104 </month>
Reference-contexts: Several researchers have been involved with incorporating support for irregular applications into FORTRAN D and HPF compilers. Das, Saltz, Hanxledon, Kennedy and Koelbel have developed compiler analysis methods for irregular applications [37], <ref> [38] </ref>, [39] and Ponnusamy, Saltz, Choudhary, Das, Uysal, and Hwang present methods for optimizing communication performance [40], [41].
Reference: [39] <author> Raja Das, Joel Saltz, and Reinhard v. Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168, </pages> <year> 1993. </year>
Reference-contexts: Several researchers have been involved with incorporating support for irregular applications into FORTRAN D and HPF compilers. Das, Saltz, Hanxledon, Kennedy and Koelbel have developed compiler analysis methods for irregular applications [37], [38], <ref> [39] </ref> and Ponnusamy, Saltz, Choudhary, Das, Uysal, and Hwang present methods for optimizing communication performance [40], [41].
Reference: [40] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <type> Technical Report UMAICS-TR-93-109 or CS-TR-3163, </type> <institution> UMAICS or University of Maryland Department of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: Several researchers have been involved with incorporating support for irregular applications into FORTRAN D and HPF compilers. Das, Saltz, Hanxledon, Kennedy and Koelbel have developed compiler analysis methods for irregular applications [37], [38], [39] and Ponnusamy, Saltz, Choudhary, Das, Uysal, and Hwang present methods for optimizing communication performance <ref> [40] </ref>, [41]. The program analysis in the two compilers mentioned above is similar to the analysis that is 6 required to automate our irregular partitioning scheme, and can be used to enhance this method in the future.
Reference: [41] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choudhary. </author> <title> Runtime compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 361-370, </pages> <year> 1993. </year>
Reference-contexts: Several researchers have been involved with incorporating support for irregular applications into FORTRAN D and HPF compilers. Das, Saltz, Hanxledon, Kennedy and Koelbel have developed compiler analysis methods for irregular applications [37], [38], [39] and Ponnusamy, Saltz, Choudhary, Das, Uysal, and Hwang present methods for optimizing communication performance [40], <ref> [41] </ref>. The program analysis in the two compilers mentioned above is similar to the analysis that is 6 required to automate our irregular partitioning scheme, and can be used to enhance this method in the future.
Reference: [42] <author> David E. Hudak and Santosh G. Abraham. </author> <title> Compiling Parallel Loops for High Performance Computers: Partitioning, Data Assignment, and Remapping. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: The communication optimizations in the SUPERB and FORTRAN D compilers are designed for message passing machines and are not directly useful for shared address space machines. Automatic partitioning techniques which minimize coherence traffic for regular parallel loops on cache-coherent processors have been developed by Hudak and Abraham <ref> [42] </ref> and by Agarwal, Kranz and Natarajan [43]. In [44] we provided a method for partitioning regular parallel loops with strided data accesses. These techniques find optimal partitions for programs with linear array subscript expressions.
Reference: [43] <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic partitioning of parallel loops for cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 2-11, </pages> <year> 1993. </year>
Reference-contexts: Automatic partitioning techniques which minimize coherence traffic for regular parallel loops on cache-coherent processors have been developed by Hudak and Abraham [42] and by Agarwal, Kranz and Natarajan <ref> [43] </ref>. In [44] we provided a method for partitioning regular parallel loops with strided data accesses. These techniques find optimal partitions for programs with linear array subscript expressions. However, the mathematical approaches used do not extend to indirect array references which are common in irregular code.
Reference: [44] <author> Karen A. Tomko and Santosh G. Abraham. </author> <title> Iteration partitioning for resolving stride conflicts on cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 95-102, </pages> <year> 1993. </year>
Reference-contexts: Automatic partitioning techniques which minimize coherence traffic for regular parallel loops on cache-coherent processors have been developed by Hudak and Abraham [42] and by Agarwal, Kranz and Natarajan [43]. In <ref> [44] </ref> we provided a method for partitioning regular parallel loops with strided data accesses. These techniques find optimal partitions for programs with linear array subscript expressions. However, the mathematical approaches used do not extend to indirect array references which are common in irregular code.
Reference: [45] <author> S. L. Graham, P. B. Kessler, and M. K. McKusick. </author> <title> gprof: A call graph execution profiler. </title> <booktitle> SIGPLAN '82 Symposium on Compiler Construction, SIGPLAN Notices, </booktitle> <volume> 17(6) </volume> <pages> 120-126, </pages> <year> 1982. </year>
Reference-contexts: Gprof which provides the amount of time spent in each subroutine of an application is now standard on most Unix systems <ref> [45] </ref>. Profiling has also become a useful technique for application optimization. The IMPACT compiler developed by Hwu et el. uses profile information to enhance several compiler optimizations such as basic block enlargement, constant propagation, dead code removal, and common subex-pression elimination [46].
Reference: [46] <author> Pohua P. Chang, Scott A. Mahlke, and Wen mei W. Hwu. </author> <title> Using profile information to assist clasic code optimizations. </title> <journal> Software Practice and Experience, </journal> <volume> 21(12) </volume> <pages> 1301-1321, </pages> <year> 1991. </year>
Reference-contexts: Profiling has also become a useful technique for application optimization. The IMPACT compiler developed by Hwu et el. uses profile information to enhance several compiler optimizations such as basic block enlargement, constant propagation, dead code removal, and common subex-pression elimination <ref> [46] </ref>. Profile-driven optimization is sometimes viewed as too costly or as unreliable. Conte et al. investigate approaches for improving the profile gathering process to reduce the cost and inconvenience of use [47] [48]. Wall examines the reliability of profile-based optimizations in [49].
Reference: [47] <author> Thomas M. Conte, Burzin A. Patel, and J. Stan Cox. </author> <title> Using branch handling hardware to support profile-driven optimization. </title> <booktitle> In Proceeding of the 27th Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1994. </year>
Reference-contexts: Profile-driven optimization is sometimes viewed as too costly or as unreliable. Conte et al. investigate approaches for improving the profile gathering process to reduce the cost and inconvenience of use <ref> [47] </ref> [48]. Wall examines the reliability of profile-based optimizations in [49]. We use application profiling to enhance domain decomposition of parallel applications. Many tools are available for generating and in some cases viewing application profiling data on parallel computers. The GIST tool from BBN allows user-specified events to be monitored.
Reference: [48] <author> J. Stan Cox, David P. Howell, and Thomas M. Conte. </author> <title> Commercializing profile-driven optimization. </title> <booktitle> In Proceedings of the Twenty-Eighth Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 221-228, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Profile-driven optimization is sometimes viewed as too costly or as unreliable. Conte et al. investigate approaches for improving the profile gathering process to reduce the cost and inconvenience of use [47] <ref> [48] </ref>. Wall examines the reliability of profile-based optimizations in [49]. We use application profiling to enhance domain decomposition of parallel applications. Many tools are available for generating and in some cases viewing application profiling data on parallel computers. The GIST tool from BBN allows user-specified events to be monitored.
Reference: [49] <author> David W. Wall. </author> <title> Predicting program behavior using real or estimated profiles. </title> <booktitle> In Proceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 59-70, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Profile-driven optimization is sometimes viewed as too costly or as unreliable. Conte et al. investigate approaches for improving the profile gathering process to reduce the cost and inconvenience of use [47] [48]. Wall examines the reliability of profile-based optimizations in <ref> [49] </ref>. We use application profiling to enhance domain decomposition of parallel applications. Many tools are available for generating and in some cases viewing application profiling data on parallel computers. The GIST tool from BBN allows user-specified events to be monitored.
Reference: [50] <institution> CONVEX Computer Corporation. </institution> <note> CXpa Reference, second edition. </note>
Reference-contexts: The GIST tool from BBN allows user-specified events to be monitored. CXpa from CONVEX extends gprof type profiling to a parallel computing 7 environment. It provides loop iteration and routine execution counts, wall clock time, CPU time, and cache miss counts for each thread of a parallel application <ref> [50] </ref>. Similarly, PMON on the KSR1 and KSR2 provides CPU time, and cache miss data on a per thread basis. We make use of such standard parallel profile information to guide parallel program optimizations.
Reference: [51] <author> Dennis Gannon, William Jalby, and Kyle Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year> <month> 105 </month>
Reference-contexts: Specifically, we use per thread subroutine execution time to calculate graph weights which are used to improve the load balance of irregular applications. 1.1.6 Cache Management Blocking and data prefetching are common techniques for improving memory hierarchy performance on uniprocessors as well as multiprocessors. Gannon, Jalby and Gallivan <ref> [51] </ref> and Wolf and Lam [52] describe methods for improving uniprocessor cache performance using blocking for regular applications. Temam and Jalby [53] model the cache behavior of sparse codes and propose a diagonal blocking technique.
Reference: [52] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Pro--ceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <year> 1991. </year>
Reference-contexts: Gannon, Jalby and Gallivan [51] and Wolf and Lam <ref> [52] </ref> describe methods for improving uniprocessor cache performance using blocking for regular applications. Temam and Jalby [53] model the cache behavior of sparse codes and propose a diagonal blocking technique. We propose a blocking technique using domain decomposition to partition the data assigned to each processor into blocks.
Reference: [53] <author> Olivier Temam and William Jalby. </author> <title> Characterizing the behavior of sparse algorithms on caches. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 578-587, </pages> <year> 1992. </year>
Reference-contexts: Gannon, Jalby and Gallivan [51] and Wolf and Lam [52] describe methods for improving uniprocessor cache performance using blocking for regular applications. Temam and Jalby <ref> [53] </ref> model the cache behavior of sparse codes and propose a diagonal blocking technique. We propose a blocking technique using domain decomposition to partition the data assigned to each processor into blocks. Our results comply with the results in [53] in that blocking in sparse codes is beneficial only when unavoidable <p> Temam and Jalby <ref> [53] </ref> model the cache behavior of sparse codes and propose a diagonal blocking technique. We propose a blocking technique using domain decomposition to partition the data assigned to each processor into blocks. Our results comply with the results in [53] in that blocking in sparse codes is beneficial only when unavoidable capacity misses 2 are not dominant. Gornish, Granston and Veidenbaum [54] and Callahan, Kennedy and Porterfield [55] both propose compiler algorithms for inserting prefetch instructions in regular dense applications. However, neither algorithm takes indirect array accesses into account.
Reference: [54] <author> Edward H. Gornish, Elana D. Granston, and Alexander V. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <year> 1990. </year>
Reference-contexts: We propose a blocking technique using domain decomposition to partition the data assigned to each processor into blocks. Our results comply with the results in [53] in that blocking in sparse codes is beneficial only when unavoidable capacity misses 2 are not dominant. Gornish, Granston and Veidenbaum <ref> [54] </ref> and Callahan, Kennedy and Porterfield [55] both propose compiler algorithms for inserting prefetch instructions in regular dense applications. However, neither algorithm takes indirect array accesses into account. Windheiser et al. [56] evaluate using prefetch and poststore commands in a sparse matrix application.
Reference: [55] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <year> 1991. </year>
Reference-contexts: Our results comply with the results in [53] in that blocking in sparse codes is beneficial only when unavoidable capacity misses 2 are not dominant. Gornish, Granston and Veidenbaum [54] and Callahan, Kennedy and Porterfield <ref> [55] </ref> both propose compiler algorithms for inserting prefetch instructions in regular dense applications. However, neither algorithm takes indirect array accesses into account. Windheiser et al. [56] evaluate using prefetch and poststore commands in a sparse matrix application. Similarly, we insert prefetches into a sparse matrix application.
Reference: [56] <author> Daniel Windheiser, Eric Boyd, Eric Hao, Santosh G. Abraham, and Edward Davidson. </author> <title> KSR1 multiprocessor: Analysis of latency hiding techniques in a sparse solver. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 454-461, </pages> <year> 1993. </year>
Reference-contexts: Gornish, Granston and Veidenbaum [54] and Callahan, Kennedy and Porterfield [55] both propose compiler algorithms for inserting prefetch instructions in regular dense applications. However, neither algorithm takes indirect array accesses into account. Windheiser et al. <ref> [56] </ref> evaluate using prefetch and poststore commands in a sparse matrix application. Similarly, we insert prefetches into a sparse matrix application. <p> Most parallel machines have latency hiding features (e.g. asynchronous messaging, prefetching) to facilitate computation/communication overlap. Communication performance for the IBM SP2 and the Kendall Square Research KSR1 is explored in [58], [59], <ref> [56] </ref>, [60] and [61]. We use prefetch operations for latency hiding on the KSR1 as described in Chapter 4 When tuning a parallel application it is common to encounter a trade-off between load balance and communication. <p> A state diagram of the coherence protocol for the KSR1 is given in Figure 1.3. Kendall Square Research KSR1 The Kendall Square Research KSR1 was used to evaluate our methods in Chapter 4. We give a brief description of the architecture here, which is summarized from <ref> [56, 59] </ref>. The KSR1 is characterized by a hierarchical ring interconnection network and cache-only memory architecture. Each cell, consisting of a 20 megahertz processor, a 512 kilobyte subcache, and a 32 megabyte local cache, is connected to a unidirectional pipelined slotted 21 ring. <p> Read (Cycles) Each Subcache 0.25 2 (1 per clock) Local Cache (existing page) 32.0 (allocated block) 23.4 (unallocated block) 49.2 Remote Cache 32.0 each (allocated page AE:0) (1024 total) 135-175 (allocated page AE:1) (34816 total) 470-600 Table 1.1: Memory Size and Read Access Latencies of the KSR1 (Repro duced from <ref> [56] </ref>) to a shared copy of a subpage, it must send a transaction around the ring requesting that each processor with a copy of the subpage in its local cache mark the subpage as invalid. <p> Subsequently these copies must be invalidated before the vector update of p line 9. Since A is static, the communication pattern is the same for all iterations of the solver. A detailed description of the parallelization of FEM-ATS is given in <ref> [56] </ref>. In our experiments, we used two data sets of size n = 8; 841 (9k data set) and n = 20; 033 (20k data set). <p> Reproduced from <ref> [56] </ref> 87 elements in the A matrix for the two data sets respectively. The data sets were partitioned into subdomains using a program provided by Barnard and Simon which implements their Recursive Spectral Bisection (RSB) algorithm [3].
Reference: [57] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: Amdahl's Law states that the performance improvement to be gained from using p processors is limited by the fraction of time spent doing sequential work, and the speedup of the parallel fraction, Speedup = 1 f sequential + f parallel Speedup parallel <ref> [57] </ref>. Ideally, if a problem is decomposed into subproblems and executed on p processors, then it will run p times faster than the original program, a speedup of p 3 .
Reference: [58] <author> Gheith A. Abandah. </author> <title> Modeling the communication and computation performance of the IBM SP2. </title> <type> Technical Report CSE-TR-258-95, </type> <institution> University of Michigan, Department of Electrical Engineering and Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Most parallel machines have latency hiding features (e.g. asynchronous messaging, prefetching) to facilitate computation/communication overlap. Communication performance for the IBM SP2 and the Kendall Square Research KSR1 is explored in <ref> [58] </ref>, [59], [56], [60] and [61]. We use prefetch operations for latency hiding on the KSR1 as described in Chapter 4 When tuning a parallel application it is common to encounter a trade-off between load balance and communication. <p> More information on the POWER2 architecture can be found in [65],[66],[67],[68]. The High-Performance Switch is a point-to-point, packet switching network, that uses a buffered form of wormhole routing for flow control. Each SP2 frame contains a two-staged 19 (Reproduced from <ref> [58] </ref>) 16 fi 16 switch board as diagramed in Figure 1.2. The switch boards are composed of eight bidirectional 4 fi 4 crossbar switches, each crossbar switch link is bidirectional and has a bandwidth of 40MB/s in each direction. <p> The first routine, subroutine 1, is fairly well balanced under our default decomposition scheme, an unweighted multilevel spectral partitioning of elements of the FE mesh. This routine has been modeled by other members of our research group and its behavior is well understood [72] <ref> [58] </ref>. The 33 second routine shows high imbalance under the default decomposition and thus there is potentially more to be gained by weighted decomposition. Consequently, we focus more on subroutine 2. Our experiments were executed on an IBM SP2 parallel computer.
Reference: [59] <author> Eric Boyd, John-David Wellman, Santosh G. Abraham, and Edward Davidson. </author> <title> Evaluating the communication performance of MPPs using synthetic sparse matrix multiplication workloads. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 240-250, </pages> <year> 1993. </year>
Reference-contexts: Most parallel machines have latency hiding features (e.g. asynchronous messaging, prefetching) to facilitate computation/communication overlap. Communication performance for the IBM SP2 and the Kendall Square Research KSR1 is explored in [58], <ref> [59] </ref>, [56], [60] and [61]. We use prefetch operations for latency hiding on the KSR1 as described in Chapter 4 When tuning a parallel application it is common to encounter a trade-off between load balance and communication. <p> A state diagram of the coherence protocol for the KSR1 is given in Figure 1.3. Kendall Square Research KSR1 The Kendall Square Research KSR1 was used to evaluate our methods in Chapter 4. We give a brief description of the architecture here, which is summarized from <ref> [56, 59] </ref>. The KSR1 is characterized by a hierarchical ring interconnection network and cache-only memory architecture. Each cell, consisting of a 20 megahertz processor, a 512 kilobyte subcache, and a 32 megabyte local cache, is connected to a unidirectional pipelined slotted 21 ring.
Reference: [60] <author> B. Z. Kahhaleh. </author> <title> Analysis of memory latency factors and their impact on KSR1 MPP performance. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Most parallel machines have latency hiding features (e.g. asynchronous messaging, prefetching) to facilitate computation/communication overlap. Communication performance for the IBM SP2 and the Kendall Square Research KSR1 is explored in [58], [59], [56], <ref> [60] </ref> and [61]. We use prefetch operations for latency hiding on the KSR1 as described in Chapter 4 When tuning a parallel application it is common to encounter a trade-off between load balance and communication.
Reference: [61] <author> E. Rosti, E. Smirni, T. D. Wagner, A. W. Apon, and L. W. Dowdy. </author> <title> The KSR1: Experimentation and modeling of poststore. </title> <booktitle> In SIGMETRICS Conference on Performance Measurement and Modeling, </booktitle> <pages> pages 74-85, </pages> <year> 1993. </year>
Reference-contexts: Most parallel machines have latency hiding features (e.g. asynchronous messaging, prefetching) to facilitate computation/communication overlap. Communication performance for the IBM SP2 and the Kendall Square Research KSR1 is explored in [58], [59], [56], [60] and <ref> [61] </ref>. We use prefetch operations for latency hiding on the KSR1 as described in Chapter 4 When tuning a parallel application it is common to encounter a trade-off between load balance and communication.
Reference: [62] <author> M. Garey, D. Johnson, and L. Stockmeyer. </author> <title> Some simplified NP-complete graph problems. </title> <journal> Theoretical Computer Science, </journal> <volume> 1(3) </volume> <pages> 237-267, </pages> <year> 1976. </year>
Reference-contexts: Objective: minimize P k c (e k ), 8e k 2 E s.t. e k = edge (u; v) and 9i; j (i 6= j) s.t. u 2 V i , w 2 V j . Weighted graph partitioning is an NP-complete problem <ref> [62] </ref>, but there are several efficient heuristic algorithms available. Two prominent algorithms in the literature are spectral partitioning and greedy partitioning, both have been used in multilevel partitioning approaches that speed up partitioning for large graphs.
Reference: [63] <author> T. Agerwala et al. </author> <title> SP2 system architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 152-184, </pages> <year> 1995. </year>
Reference-contexts: Each node contains private memory and runs its own copy of the AIX operating system, communication between nodes is explicitly programmed using one of MPL, PVMe or MPI 18 message passing libraries. The SP2 system architecture is discussed at length in <ref> [63] </ref> and the MPL communication software is discussed in [64]. The SP2 is organized into frames, each containing 16 processing nodes and a portion of the high-performance switch as diagramed in Figures 1.1 and 1.2.
Reference: [64] <author> M. Snir, P. Hochschild, D. D. Frye, and K. J. Gildea. </author> <title> The communication software and parallel environment of the IBM SP2. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: The SP2 system architecture is discussed at length in [63] and the MPL communication software is discussed in <ref> [64] </ref>. The SP2 is organized into frames, each containing 16 processing nodes and a portion of the high-performance switch as diagramed in Figures 1.1 and 1.2. Each processing node consists of a superscalar POWER2 RISC System/6000 processor, 64 to 256 Mbytes of memory, and a communication adaptor.
Reference: [65] <author> S. W. White and S. Dhawan. POWER2: </author> <title> next generation of the RISC system/6000 family. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(5) </volume> <pages> 493-502, </pages> <year> 1994. </year>
Reference: [66] <author> T. N. Hicks, R. E. Fry, and P. E. Harvey. </author> <title> POWER2 floating-point unit: Architecture and implementation. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(5) </volume> <pages> 525-536, </pages> <year> 1994. </year> <month> 106 </month>
Reference: [67] <author> J. I. Barreh, R. T. Golla, L. B. Arimilli, and P. J. Jordan. </author> <title> POWER2 instruction cache unit. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(5) </volume> <pages> 537-544, </pages> <year> 1994. </year>
Reference: [68] <author> D. J. Shipppy and T. W. Griffith. </author> <title> POWER2 fixed-point, data cache, and storage control units. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 38(5) </volume> <pages> 503-524, </pages> <year> 1994. </year>
Reference: [69] <author> C. B. Stunkel et al. </author> <title> The SP2 high-performance switch. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: The switch boards are composed of eight bidirectional 4 fi 4 crossbar switches, each crossbar switch link is bidirectional and has a bandwidth of 40MB/s in each direction. A detailed description of the High-Performance Switch is given in <ref> [69] </ref>. 1.5.2 Shared Address Space Parallel Computers Many parallel computers use a shared address space machine model.
Reference: [70] <author> Karen A. Tomko and Santosh G. Abraham. </author> <title> Data and program restructuring of irregular applications for cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 214-225, </pages> <year> 1994. </year>
Reference-contexts: We show that cache coherence traffic is reduced by more than 30% using our data layout scheme and that more than 53% of the latency due to coherence traffic can be hidden using prefetch instructions. This work presented in this chapter is also described in <ref> [70] </ref>. 26 CHAPTER 2 Profile Driven Weighted Decomposition 2.1 Introduction Applications with irregular domains are a challenge to parallelize efficiently, and heuristic algorithms are used to partition them for parallel execution.
Reference: [71] <author> Dan Anderson, Alex Akkerman, and Dave Strenski. FCRASH, </author> <title> optimization of an explicit FEA based crash simulation code for the Cray C916/16-51. </title> <booktitle> In Cray User Group Conference, </booktitle> <address> Denver, CO., </address> <month> March, </month> <year> 1995. </year>
Reference-contexts: It is a production application consisting of hundreds of thousands of lines of code, and like most production codes it is proprietary and some details cannot be reported. A brief general discussion of the application and the code development effort can be found in <ref> [71] </ref>. We choose two very different subroutines from FCRASH to analyze. The first routine, subroutine 1, is fairly well balanced under our default decomposition scheme, an unweighted multilevel spectral partitioning of elements of the FE mesh.
Reference: [72] <author> Eric L. Boyd, Gheith A. Abandah, Hsien-Hsin Lee, and Edward S. Davidson. </author> <title> Modeling computation and communication performance of parallel scientific applications: A case study of the IBM SP2. </title> <type> Technical Report CSE-TR-236-95, </type> <institution> University of Michigan, Department of Electrical Engineering and Computer Science, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The first routine, subroutine 1, is fairly well balanced under our default decomposition scheme, an unweighted multilevel spectral partitioning of elements of the FE mesh. This routine has been modeled by other members of our research group and its behavior is well understood <ref> [72] </ref> [58]. The 33 second routine shows high imbalance under the default decomposition and thus there is potentially more to be gained by weighted decomposition. Consequently, we focus more on subroutine 2. Our experiments were executed on an IBM SP2 parallel computer.
Reference: [73] <author> Waterloo Maple Software, </author> <title> Waterloo, Ontario, Canada. Maple V Language Reference Manual. </title>
Reference-contexts: We used the 1 Sixteen processor runs were performed at MHPCC on Thin Nodes with 64 Mbytes of memory, runs on fewer processors were performed at the CPC on Thin Nodes with 256 Mbytes of memory. 34 MAPLE (version VR3) mathematics program <ref> [73] </ref>, [74] to find the linear least-squares approximation when calculating weights. 2.3.1 Experimental Results: Subroutine 1 The main loop of subroutine 1 iterates over all of the elements of the FE mesh and performs some calculation on each element.
Reference: [74] <author> Waterloo Maple Software, </author> <title> Waterloo, Ontario, Canada. Maple V Library Reference Manual. </title>
Reference-contexts: We used the 1 Sixteen processor runs were performed at MHPCC on Thin Nodes with 64 Mbytes of memory, runs on fewer processors were performed at the CPC on Thin Nodes with 256 Mbytes of memory. 34 MAPLE (version VR3) mathematics program [73], <ref> [74] </ref> to find the linear least-squares approximation when calculating weights. 2.3.1 Experimental Results: Subroutine 1 The main loop of subroutine 1 iterates over all of the elements of the FE mesh and performs some calculation on each element.
Reference: [75] <author> Lee W. Johnson and R. Dean Riess. </author> <title> Numerical Analysis, Second Edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: The goodness of the curve fits gives one indication of the quality of the weights. We can compute relative error using the l 1 and l 1 norms to measure the goodness of our curve fits <ref> [75] </ref>.
Reference: [76] <author> A. Chatterjee, J.M. Jin, and J.L. Volakis. </author> <title> Application of edge-based finite elements and ABCs to 3-D scattering. </title> <journal> IEEE Transactions on Antennas and Propagation, </journal> <volume> 41 </volume> <pages> 221-226, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: We therefore determine the block size required for each parallel region and use the minimum (most constrained) block size for the entire control loop. 4.5 Experimental Results In order to evaluate the layout technique above, we hand restructured the FEM-ATS radiation modeling application developed at the University of Michigan <ref> [76] </ref> and ran several experiments on a 64 processor Kendall Square Research KSR1. The main routine in FEM-ATS is a sparse solver which uses the biconjugate gradient method to iteratively calculate approximate solutions to a linear system. The vector operations of the main loop are outlined in Figure 4.11.
Reference: [77] <institution> Kendall Square Research Corporation, </institution> <address> 170 Tracer Lane, Waltham, MA, </address> <month> 02154-1379. </month> <title> KSR1 Principles of Operation, </title> <booktitle> 1991. </booktitle> <pages> 107 </pages>
Reference-contexts: This transformation was done for all of our experiments in order to minimize conflict cache misses in the data subcache of the KSR1. The measurements reported in our experiments were gathered using the KSR PMON library <ref> [77] </ref>. PMON provides performance data on a per processor basis for all threads in a process. PMON allows access to a hardware event monitor which counts cache miss events, prefetch events and clock cycles for timing.
References-found: 77

