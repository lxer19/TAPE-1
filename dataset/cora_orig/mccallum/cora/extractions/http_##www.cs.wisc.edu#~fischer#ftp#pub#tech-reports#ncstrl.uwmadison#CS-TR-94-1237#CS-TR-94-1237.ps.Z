URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1237/CS-TR-94-1237.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1237/
Root-URL: http://www.cs.wisc.edu
Email: flarus,richards,gviswanag@cs.wisc.edu  
Title: LCM: Memory System Support for Parallel Language Implementation  
Author: James R. Larus Brad Richards Guhan Viswanathan 
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM.  
Abstract: Higher-level parallel programming languages can be difficult to implement efficiently on parallel machines. This paper shows how a flexible, compiler-controlled memory system can help achieve good performance for language constructs that previously appeared too costly to be practical. Our compiler-controlled memory system is called Loosely Coherent Memory (LCM). It is an example of a larger class of Reconcilable Shared Memory (RSM) systems, which generalize the replication and merge policies of cache-coherent shared-memory. RSM protocols differ in the action taken by a processor in response to a request for a location and the way in which a processor reconciles multiple outstanding copies of a location. LCM memory becomes temporarily inconsistent to implement the semantics of C** parallel functions efficiently. RSM provides a compiler with control over memory-system policies, which it can use to implement a language's semantics, improve performance, or detect errors. We illustrate the first two points with LCM and our compiler for the data-parallel language C**. fl This work is supported in part by NSF PYI/NYI Awards CCR-9157366 and CCR-9357779, NSF Grants CCR-9101035 and MIP-9225097, DOE Grant DE-FG02-93ER25176, and donations from Digital Equipment Corporation, Thinking Machines Corporation, and Xerox Corporation. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the Univ. of Wisconsin Graduate School. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A Unified Formalization of Four Shared-Memory Models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <year> 1993. </year>
Reference-contexts: Compilers can circumvent coherence policies only by sending messages [19], even when language semantics or program analysis shows that much coherence traffic is unnecessary [9, 14, 22]. Relaxed consistency models trade a simple view of memory as a sequentially consistent store for increased hardware performance <ref> [1] </ref>. Most models adopt the view that memory need only be consistent at program-specified synchronization points. Relaxed consistency, instead of providing mechanisms by which a compiler or programmer can control memory semantics, takes the opposite view and leaves semantics as unspecified as possible.
Reference: [2] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: Section 7 explores other applications of the memory system. Section 8 concludes the paper. 2 Related Work Existing, scalable shared-memory machines are inflexible and provide minimal assistance for language implementation. Many use cache-coherence protocols whose policies are implemented permanently in either hardware or a combination of hardware and software <ref> [2, 11, 18, 23, 32] </ref>. These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes.
Reference: [3] <author> Zahira Ammarguellat and W.L. Harrison III. </author> <title> Automatic Recognition of Induction Variables and Recurrence Relations by Abstract Interpretation. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 283-295, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In a RSM system, a compiler that detects the reduction in the loop <ref> [3] </ref> could choose a reconciliation function for total's cache block that sums the values added to the location's initial value. When the loop completed, the locally cached accumulators are reconciled into a single value.
Reference: [4] <author> Andrew W. Appel, John R. Ellis, and Kai Li. </author> <title> Real-time Concurrent Collection on Stock Multiprocessors. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 11-20, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: It also uses a single coherence mechanism based on object version numbers. The division of labor between the C** compiler and LCM system is reminiscent of the interaction between Lisp and ML systems and virtual memory [5] for stack and heap bounds checking and concurrent garbage collection <ref> [4] </ref>. Many data-parallel languages have been designed and implemented for MIMD machines, for example, C* [27], Data Parallel C [16], VCODE [12], and parts of HPF [17]. Data-parallel languages handle data races in different ways. Functional languages, such as NESL [8], avoid conflicts entirely by omitting side effects.
Reference: [5] <author> Andrew W. Appel and Kai Li. </author> <title> Virtual Memory Primitives for User Programs. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 96-107, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: It also uses a single coherence mechanism based on object version numbers. The division of labor between the C** compiler and LCM system is reminiscent of the interaction between Lisp and ML systems and virtual memory <ref> [5] </ref> for stack and heap bounds checking and concurrent garbage collection [4]. Many data-parallel languages have been designed and implemented for MIMD machines, for example, C* [27], Data Parallel C [16], VCODE [12], and parts of HPF [17]. Data-parallel languages handle data races in different ways.
Reference: [6] <author> Monica Beltrametti, Kenneth Bobey, and John R. Zor-bas. </author> <title> The Control Mechanism for the Myrias Parallel Computer System. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The Myrias machine implemented a copy-and-reconcile operation similar to the one in LCM, but in hardware with a fixed policy and page granularity <ref> [6] </ref>. Distributed shared memory systems, in general, provide few mechanisms for an application to control a memory system [7, 13, 24]. LCM shares with Munin [7, 10] the ability to adapt shared-memory policies to an application.
Reference: [7] <author> John K. Bennett, John B. Carter, and Willy Zwanepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 168-176, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The Myrias machine implemented a copy-and-reconcile operation similar to the one in LCM, but in hardware with a fixed policy and page granularity [6]. Distributed shared memory systems, in general, provide few mechanisms for an application to control a memory system <ref> [7, 13, 24] </ref>. LCM shares with Munin [7, 10] the ability to adapt shared-memory policies to an application. Munin, however, provides a set of fixed coherence policies, each tailored for a specific sharing pattern. A programmer or compiler associates a policy with a language-level object. <p> The Myrias machine implemented a copy-and-reconcile operation similar to the one in LCM, but in hardware with a fixed policy and page granularity [6]. Distributed shared memory systems, in general, provide few mechanisms for an application to control a memory system [7, 13, 24]. LCM shares with Munin <ref> [7, 10] </ref> the ability to adapt shared-memory policies to an application. Munin, however, provides a set of fixed coherence policies, each tailored for a specific sharing pattern. A programmer or compiler associates a policy with a language-level object.
Reference: [8] <author> Guy E. Blelloch. NESL: </author> <title> A Nested Data-Parallel Language (Version 2.6). </title> <type> Technical Report CMU-CS-93-129, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Many data-parallel languages have been designed and implemented for MIMD machines, for example, C* [27], Data Parallel C [16], VCODE [12], and parts of HPF [17]. Data-parallel languages handle data races in different ways. Functional languages, such as NESL <ref> [8] </ref>, avoid conflicts entirely by omitting side effects. Other languages, such as Paralation Lisp [28], ignore the problem by leaving the semantics of conflicting side effects unspecified. C* [27] uses a SIMD execution model in which all processors execute the same statement simultaneously.
Reference: [9] <author> William J. Bolosky and Michael L. Scott. </author> <title> False Sharing and its Effect on Shared Memory Performance. </title> <booktitle> In Proceedings of the Fourth Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS), </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes. Compilers can circumvent coherence policies only by sending messages [19], even when language semantics or program analysis shows that much coherence traffic is unnecessary <ref> [9, 14, 22] </ref>. Relaxed consistency models trade a simple view of memory as a sequentially consistent store for increased hardware performance [1]. Most models adopt the view that memory need only be consistent at program-specified synchronization points.
Reference: [10] <author> John B. Carter, John K. Bennett, and Willy Zwanepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The Myrias machine implemented a copy-and-reconcile operation similar to the one in LCM, but in hardware with a fixed policy and page granularity [6]. Distributed shared memory systems, in general, provide few mechanisms for an application to control a memory system [7, 13, 24]. LCM shares with Munin <ref> [7, 10] </ref> the ability to adapt shared-memory policies to an application. Munin, however, provides a set of fixed coherence policies, each tailored for a specific sharing pattern. A programmer or compiler associates a policy with a language-level object.
Reference: [11] <author> David Chaiken, John Kubiatowicz, and Anant Agar-wal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year> <note> 10 Appears in: "ASPLOS VI," </note> <month> Oct. </month> <year> 1994. </year> <note> Reprinted by permission of ACM. </note>
Reference-contexts: Section 7 explores other applications of the memory system. Section 8 concludes the paper. 2 Related Work Existing, scalable shared-memory machines are inflexible and provide minimal assistance for language implementation. Many use cache-coherence protocols whose policies are implemented permanently in either hardware or a combination of hardware and software <ref> [2, 11, 18, 23, 32] </ref>. These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes. <p> Since it fits within this model, it provides a natural default policy for a RSM system. Requests in these shared-memory systems return a copy of a block, subject to the guarantee that only one processor holds a writable copy at a time. In many systems <ref> [11, 23] </ref>, a centralized directory controller records which processors hold copies of a location and invalidates outstanding copies upon request. Reconciliation policies in these systems are also simple. Read-only copies are identical and so can be combined by a null reconciliation function.
Reference: [12] <author> Siddhartha Chatterjee, Guy E. Blelloch, and Allan L. Fis-cher. </author> <title> Size and Access Inference for Data-Parallel Programs. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 130-144, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Many data-parallel languages have been designed and implemented for MIMD machines, for example, C* [27], Data Parallel C [16], VCODE <ref> [12] </ref>, and parts of HPF [17]. Data-parallel languages handle data races in different ways. Functional languages, such as NESL [8], avoid conflicts entirely by omitting side effects. Other languages, such as Paralation Lisp [28], ignore the problem by leaving the semantics of conflicting side effects unspecified. <p> C** atomic and simultaneous semantics are similar, but generalize to user-defined data-parallel functions and unstructured data. Considerable research has been devoted to increasing grain size and removing unnecessary synchronization when compiling data parallel languages for MIMD machines <ref> [12, 16] </ref>.
Reference: [13] <author> Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, and Willy Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The Myrias machine implemented a copy-and-reconcile operation similar to the one in LCM, but in hardware with a fixed policy and page granularity [6]. Distributed shared memory systems, in general, provide few mechanisms for an application to control a memory system <ref> [7, 13, 24] </ref>. LCM shares with Munin [7, 10] the ability to adapt shared-memory policies to an application. Munin, however, provides a set of fixed coherence policies, each tailored for a specific sharing pattern. A programmer or compiler associates a policy with a language-level object.
Reference: [14] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <month> November </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes. Compilers can circumvent coherence policies only by sending messages [19], even when language semantics or program analysis shows that much coherence traffic is unnecessary <ref> [9, 14, 22] </ref>. Relaxed consistency models trade a simple view of memory as a sequentially consistent store for increased hardware performance [1]. Most models adopt the view that memory need only be consistent at program-specified synchronization points. <p> Second, RSM is likely to be less expensive than shared-memory code since RSM can implement the reduction by sending messages rather than communicating through memory locations <ref> [14, 19] </ref>. 7.2 Semantic Violation Detection A RSM system can help implement other parallel languages. For example, Steele proposed a language semantics that forbids programs with conflicting or racing side effects [31]. Static analysis is too conservative since it may disallow programs that do not violate the semantics.
Reference: [15] <author> Michael J. Feeley and Henry M. Levy. </author> <title> Distributed Shared Memory with Versioned Objects. </title> <booktitle> In OOPSLA '93: Object-Oriented Programming Systems, Languages and Applications Conference Proceedings, </booktitle> <pages> pages 247-262, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: RSM provides a more general framework in which to develop application-specific policies by breaking coherence policies into two components, each of which is specified separately. LCM's mechanisms also apply at cache block granularity, not to entire objects or memory pages. In VDOM <ref> [15] </ref>, memory objects are immutable. Modifying an object produces a new version of the object. Like LCM, both systems allow multiple, distinct copies of memory to develop. VDOM handles coherence at an object level, as opposed to LCM's finer-grained cache block operations.
Reference: [16] <author> Phillip J. Hatcher, Michael J. Quinn, Anthony J. La-padula, Bradley K. Seevers, Ray J. Anderson, and Robert R. Jones. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Many data-parallel languages have been designed and implemented for MIMD machines, for example, C* [27], Data Parallel C <ref> [16] </ref>, VCODE [12], and parts of HPF [17]. Data-parallel languages handle data races in different ways. Functional languages, such as NESL [8], avoid conflicts entirely by omitting side effects. Other languages, such as Paralation Lisp [28], ignore the problem by leaving the semantics of conflicting side effects unspecified. <p> C** atomic and simultaneous semantics are similar, but generalize to user-defined data-parallel functions and unstructured data. Considerable research has been devoted to increasing grain size and removing unnecessary synchronization when compiling data parallel languages for MIMD machines <ref> [12, 16] </ref>.
Reference: [17] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Semantically, each function invocation executes atomically and simultaneously, so conflicting data accesses are impossible. When all invocations from an application complete, their modifications are combined into a new global state. Fortran 90 and HPF <ref> [17] </ref> whole-array operations share a similar semantics, albeit restricted to predefined functions and arrays. Loosely-Coherent Memory (LCM) is a RSM memory system that implements C** semantics with a fine-grained copy-on-write operation and a language-specific reconciliation function. <p> Many data-parallel languages have been designed and implemented for MIMD machines, for example, C* [27], Data Parallel C [16], VCODE [12], and parts of HPF <ref> [17] </ref>. Data-parallel languages handle data races in different ways. Functional languages, such as NESL [8], avoid conflicts entirely by omitting side effects. Other languages, such as Paralation Lisp [28], ignore the problem by leaving the semantics of conflicting side effects unspecified. <p> Other languages, such as Paralation Lisp [28], ignore the problem by leaving the semantics of conflicting side effects unspecified. C* [27] uses a SIMD execution model in which all processors execute the same statement simultaneously. The array-based data parallel subset of HPF <ref> [17] </ref> specifies that all inputs to a data-parallel operation are read before any are written. C** atomic and simultaneous semantics are similar, but generalize to user-defined data-parallel functions and unstructured data.
Reference: [18] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November </month> <year> 1993. </year> <note> Earlier version appeared in ASPLOS V, </note> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Section 7 explores other applications of the memory system. Section 8 concludes the paper. 2 Related Work Existing, scalable shared-memory machines are inflexible and provide minimal assistance for language implementation. Many use cache-coherence protocols whose policies are implemented permanently in either hardware or a combination of hardware and software <ref> [2, 11, 18, 23, 32] </ref>. These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes.
Reference: [19] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubia-towicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: To demonstrate this approach, we built a RSM system called Loosely-Coherent Memory (LCM) and used it to implement the large-grain data-parallel language C** [21]. The hardware base for RSM is a parallel computer with a Tempest-like interface, which provides mechanisms that permit user-level software to implement shared-memory policies <ref> [19, 26] </ref>. A Tempest memory system is possible on a wide range of parallel systems, including those without shared-memory hardware [30]. Tempest offers a program both control over communications and data placement, as is possible with message passing, and the dynamic fine-grain policies possible with shared memory. <p> These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes. Compilers can circumvent coherence policies only by sending messages <ref> [19] </ref>, even when language semantics or program analysis shows that much coherence traffic is unnecessary [9, 14, 22]. Relaxed consistency models trade a simple view of memory as a sequentially consistent store for increased hardware performance [1]. <p> Second, RSM is likely to be less expensive than shared-memory code since RSM can implement the reduction by sending messages rather than communicating through memory locations <ref> [14, 19] </ref>. 7.2 Semantic Violation Detection A RSM system can help implement other parallel languages. For example, Steele proposed a language semantics that forbids programs with conflicting or racing side effects [31]. Static analysis is too conservative since it may disallow programs that do not violate the semantics.
Reference: [20] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Several proposed systems, including Wisconsin Typhoon and Blizzard [26, 30] and Stanford FLASH 2 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM. <ref> [20] </ref>, provide low-level mechanisms to implement coherence policies in software. Typhoon and Blizzard provide the Tempest interface, which allows user-level software to implement these policies.
Reference: [21] <author> James R. Larus. </author> <title> C**: a Large-Grain, Object-Oriented, Data-Parallel Programming Language. </title> <editor> In Utpal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages And Compilers for Parallel Computing (5th International Workshop), </booktitle> <pages> pages 326-341, </pages> <address> New Haven, </address> <month> August </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference-contexts: To demonstrate this approach, we built a RSM system called Loosely-Coherent Memory (LCM) and used it to implement the large-grain data-parallel language C** <ref> [21] </ref>. The hardware base for RSM is a parallel computer with a Tempest-like interface, which provides mechanisms that permit user-level software to implement shared-memory policies [19, 26]. A Tempest memory system is possible on a wide range of parallel systems, including those without shared-memory hardware [30]. <p> For example, global reductions and stale data fit naturally into the RSM model. Finally, RSM can help detect unsynchronized data accesses (data races). We implemented a RSM system to support C**, which is a new large grain data-parallel programming language based on C++ <ref> [21] </ref>. In C**, when a program applies a parallel function to an aggregate data collection, the function is invoked separately on each element in the aggregate. Semantically, each function invocation executes atomically and simultaneously, so conflicting data accesses are impossible. <p> Update-based systems reconcile after modification to a shared location by assigning the new value to all copies. 4 C** Language We implemented the memory model described previously and used it to support programs written in C**, a new data-parallel language <ref> [21] </ref>. This section provides a brief overview of the language and a sample C** program. 4.1 Overview of C** C** is a data-parallel programming language that extends C++ with a small number of features.
Reference: [22] <author> James R. Larus. </author> <title> Compiling for Shared-Memory and Message-Passing Computers. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 2(1-4):165-180, </volume> <month> March-December </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Compiling parallel languages for parallel computers is difficult. Most of these languages assume a shared address space in which any part of a computation can reference any data. Parallel machines provide either too little or too much support for many languages <ref> [22] </ref>. On one hand, message-passing machines require a compiler to statically analyze and handle all details of data placement and access, or pay a large cost to defer these decisions to run time. <p> These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes. Compilers can circumvent coherence policies only by sending messages [19], even when language semantics or program analysis shows that much coherence traffic is unnecessary <ref> [9, 14, 22] </ref>. Relaxed consistency models trade a simple view of memory as a sequentially consistent store for increased hardware performance [1]. Most models adopt the view that memory need only be consistent at program-specified synchronization points.
Reference: [23] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Section 7 explores other applications of the memory system. Section 8 concludes the paper. 2 Related Work Existing, scalable shared-memory machines are inflexible and provide minimal assistance for language implementation. Many use cache-coherence protocols whose policies are implemented permanently in either hardware or a combination of hardware and software <ref> [2, 11, 18, 23, 32] </ref>. These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes. <p> Since it fits within this model, it provides a natural default policy for a RSM system. Requests in these shared-memory systems return a copy of a block, subject to the guarantee that only one processor holds a writable copy at a time. In many systems <ref> [11, 23] </ref>, a centralized directory controller records which processors hold copies of a location and invalidates outstanding copies upon request. Reconciliation policies in these systems are also simple. Read-only copies are identical and so can be combined by a null reconciliation function.
Reference: [24] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The Myrias machine implemented a copy-and-reconcile operation similar to the one in LCM, but in hardware with a fixed policy and page granularity [6]. Distributed shared memory systems, in general, provide few mechanisms for an application to control a memory system <ref> [7, 13, 24] </ref>. LCM shares with Munin [7, 10] the ability to adapt shared-memory policies to an application. Munin, however, provides a set of fixed coherence policies, each tailored for a specific sharing pattern. A programmer or compiler associates a policy with a language-level object.
Reference: [25] <author> Robert H.B. Netzer and Barton P. Miller. </author> <title> Improving the Accuracy of Data Race Detection. </title> <booktitle> In Third ACM SIG-PLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 133-144, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Detecting actual races would require flushing all read-only cache blocks from the caches after each reconciliation. The loss in performance is less critical here since a run-time race detection system is most likely used only while debugging, and other run-time techniques have high overheads <ref> [25] </ref>. 7.4 Reducing False Sharing False sharing occurs when several processors concurrently access (with at least one write) different locations in the same cache block. LCM-like systems can reduce the effects of false sharing.
Reference: [26] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: To demonstrate this approach, we built a RSM system called Loosely-Coherent Memory (LCM) and used it to implement the large-grain data-parallel language C** [21]. The hardware base for RSM is a parallel computer with a Tempest-like interface, which provides mechanisms that permit user-level software to implement shared-memory policies <ref> [19, 26] </ref>. A Tempest memory system is possible on a wide range of parallel systems, including those without shared-memory hardware [30]. Tempest offers a program both control over communications and data placement, as is possible with message passing, and the dynamic fine-grain policies possible with shared memory. <p> We built a LCM system on Blizzard, which is a fine-grain distributed shared memory system (not a simulator) that runs at near shared-memory hardware speeds on a Thinking Machines CM-5 [30]. We compared the performance of four C** programs running under both the unmodified Stache protocol <ref> [26] </ref> and LCM (implemented using the Tempest mechanisms provided by Blizzard). We found that the LCM memory system improved performance by up to a factor of 2 for applications that used dynamic data structures. <p> LCM allows memory to become inconsistent between synchronization points, but this "loose" consistency is both under program control and is an end in itself, not a byproduct of hardware optimization. Several proposed systems, including Wisconsin Typhoon and Blizzard <ref> [26, 30] </ref> and Stanford FLASH 2 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM. [20], provide low-level mechanisms to implement coherence policies in software. Typhoon and Blizzard provide the Tempest interface, which allows user-level software to implement these policies. <p> A processor eventually returns a (possibly modified) location to its home processor for a variety of reasons, such as a cache replacement or a program-initiated cache flush, or in response to a request from another processor. As described above, RSM assumes the same basic mechanisms as cache-coherent shared memory <ref> [26, 32] </ref> but generalizes the policies. RSM is a family of protocols that differ in the action taken by a processor in response to a request for a location and the way in which a processor reconciles multiple outstanding copies of a location. <p> Blizzard implements the Tempest interface <ref> [26] </ref>, which permits user-level control of the memory system. LCM is user-level code that runs on the CM-5. We started with the user-level Stache protocol [26], which provides cache-coherent shared memory and uses a processor's local memory as a large, fully associative cache. <p> Blizzard implements the Tempest interface <ref> [26] </ref>, which permits user-level control of the memory system. LCM is user-level code that runs on the CM-5. We started with the user-level Stache protocol [26], which provides cache-coherent shared memory and uses a processor's local memory as a large, fully associative cache. This cache is essential to ensure that a processor's locally modified (inconsistent) blocks are not lost by being flushed to their home node. <p> As a baseline, we ran the same C** programs, compiled under an option to perform explicit copying, on the unmodified Stache protocol <ref> [26] </ref>. Both LCM and Stache run under Blizzard-E [30] on a CM-5 with 32 processors. We measured two versions of LCM. The first, LCM-scc, keeps a single clean copy of each marked cache block at the block's home node. <p> The numbers above also show that a compiler should not rely exclusively on LCM. In situations, such as Stencil-stat, in which communication is simple and rarely occurs under a protocol like Stache, LCM has little to offer. One of the virtues of user-level shared memory <ref> [26] </ref> is that a compiler can make this choice (or even, use both in a program) by selecting the libraries linked with a program. 7 Other LCM Applications Reconcilable shared memory proved useful in implementing C** parallel functions. But RSM is not limited to this role.
Reference: [27] <author> John R. Rose and Guy L. Steele Jr. </author> <title> C*: An Extended C Language for Data Parallel Programming. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <pages> pages 2-16, </pages> <address> Santa Clara, California, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Many data-parallel languages have been designed and implemented for MIMD machines, for example, C* <ref> [27] </ref>, Data Parallel C [16], VCODE [12], and parts of HPF [17]. Data-parallel languages handle data races in different ways. Functional languages, such as NESL [8], avoid conflicts entirely by omitting side effects. <p> Data-parallel languages handle data races in different ways. Functional languages, such as NESL [8], avoid conflicts entirely by omitting side effects. Other languages, such as Paralation Lisp [28], ignore the problem by leaving the semantics of conflicting side effects unspecified. C* <ref> [27] </ref> uses a SIMD execution model in which all processors execute the same statement simultaneously. The array-based data parallel subset of HPF [17] specifies that all inputs to a data-parallel operation are read before any are written. <p> It supports a new form of data parallelism (large-grain data parallelism), that offers much of the semantic simplicity of SIMD data parallelism <ref> [27] </ref>, but does not require lockstep execution. In the C** data-parallel model, parallelism results from applying a parallel function across a collection of data called an aggregate. Aggregates look and behave like C++ arrays, but form the basis for parallel functions.
Reference: [28] <author> Gary W. Sabot. </author> <title> The Paralation Model: Architecture-Independent Parallel Programming. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Data-parallel languages handle data races in different ways. Functional languages, such as NESL [8], avoid conflicts entirely by omitting side effects. Other languages, such as Paralation Lisp <ref> [28] </ref>, ignore the problem by leaving the semantics of conflicting side effects unspecified. C* [27] uses a SIMD execution model in which all processors execute the same statement simultaneously.
Reference: [29] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-Time Parallelization and Scheduling of Loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: These programs require extensive (and expensive) run-time operations to run in parallel <ref> [29] </ref>. For example, consider an adaptive mesh version of stencil, which selectively subdivides some mesh points into finer detail. It is part of a program that 6 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM. computes electric potentials in a box. <p> These measurements show that LCM helps efficiently implement C**, particularly for programs whose behavior is difficult to analyze or predict statically. Few compiler techniques have addressed these programs, which are typically written by hand, with the aid of elaborate libraries <ref> [29] </ref>. The numbers above also show that a compiler should not rely exclusively on LCM. In situations, such as Stencil-stat, in which communication is simple and rarely occurs under a protocol like Stache, LCM has little to offer.
Reference: [30] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <month> October </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: The hardware base for RSM is a parallel computer with a Tempest-like interface, which provides mechanisms that permit user-level software to implement shared-memory policies [19, 26]. A Tempest memory system is possible on a wide range of parallel systems, including those without shared-memory hardware <ref> [30] </ref>. Tempest offers a program both control over communications and data placement, as is possible with message passing, and the dynamic fine-grain policies possible with shared memory. Reconcilable Shared Memory (RSM) provides a global address space and basic coherence policy whose 1 Appears in: "ASPLOS VI," Oct. 1994. <p> We built a LCM system on Blizzard, which is a fine-grain distributed shared memory system (not a simulator) that runs at near shared-memory hardware speeds on a Thinking Machines CM-5 <ref> [30] </ref>. We compared the performance of four C** programs running under both the unmodified Stache protocol [26] and LCM (implemented using the Tempest mechanisms provided by Blizzard). We found that the LCM memory system improved performance by up to a factor of 2 for applications that used dynamic data structures. <p> LCM allows memory to become inconsistent between synchronization points, but this "loose" consistency is both under program control and is an end in itself, not a byproduct of hardware optimization. Several proposed systems, including Wisconsin Typhoon and Blizzard <ref> [26, 30] </ref> and Stanford FLASH 2 Appears in: "ASPLOS VI," Oct. 1994. Reprinted by permission of ACM. [20], provide low-level mechanisms to implement coherence policies in software. Typhoon and Blizzard provide the Tempest interface, which allows user-level software to implement these policies. <p> However, values written by C**'s reduction assignments (Section 4.2) require different reconciliation functions that combine values. 5.1 LCM Implementation We implemented a LCM system on Blizzard, which is a fine-grain distributed shared memory system that runs at near shared-memory hardware speeds on a Thinking Machines CM-5 <ref> [30] </ref>. Blizzard implements the Tempest interface [26], which permits user-level control of the memory system. LCM is user-level code that runs on the CM-5. We started with the user-level Stache protocol [26], which provides cache-coherent shared memory and uses a processor's local memory as a large, fully associative cache. <p> As a baseline, we ran the same C** programs, compiled under an option to perform explicit copying, on the unmodified Stache protocol [26]. Both LCM and Stache run under Blizzard-E <ref> [30] </ref> on a CM-5 with 32 processors. We measured two versions of LCM. The first, LCM-scc, keeps a single clean copy of each marked cache block at the block's home node. Section 5.1 explains why this approach can perform poorly for applications with spatial or temporal reuse between invocations. <p> If a word in a block is modified by multiple processors, a conflict occurred. A processor can record the modified words in a cache block by setting its access control to ReadOnly <ref> [30] </ref> and trapping stores until all words are modified. 2 A read-write semantic violation occurs if a memory location is concurrently read by one processor and written by another. When reconciliation is performed, modified cache blocks are returned to their home node to be combined.
Reference: [31] <author> Guy L. Steele Jr. </author> <title> Making Asynchronous Parallelism Safe for the World. </title> <booktitle> In Conference Record of the Seventeenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: For example, Steele proposed a language semantics that forbids programs with conflicting or racing side effects <ref> [31] </ref>. Static analysis is too conservative since it may disallow programs that do not violate the semantics. Steele proposed a scheme for detecting run-time violations that required maintaining an access history for each memory location that could possibly communicate side-effects from one processor to another.
Reference: [32] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-168, </pages> <month> May </month> <year> 1993. </year> <month> 11 </month>
Reference-contexts: Section 7 explores other applications of the memory system. Section 8 concludes the paper. 2 Related Work Existing, scalable shared-memory machines are inflexible and provide minimal assistance for language implementation. Many use cache-coherence protocols whose policies are implemented permanently in either hardware or a combination of hardware and software <ref> [2, 11, 18, 23, 32] </ref>. These systems support only a single coherence protocol and consistency model (typically, sequential consistency) and provide few performance-enhancing mechanisms beyond prefetches and cache flushes. <p> A processor eventually returns a (possibly modified) location to its home processor for a variety of reasons, such as a cache replacement or a program-initiated cache flush, or in response to a request from another processor. As described above, RSM assumes the same basic mechanisms as cache-coherent shared memory <ref> [26, 32] </ref> but generalizes the policies. RSM is a family of protocols that differ in the action taken by a processor in response to a request for a location and the way in which a processor reconciles multiple outstanding copies of a location.
References-found: 32

