URL: http://www.cs.ubc.ca/spider/cebly/Papers/abstraction.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: email: dearden,cebly@cs.ubc.ca  
Title: Abstraction and Approximate Decision Theoretic Planning  
Author: Richard Dearden and Craig Boutilier 
Keyword: Planning, decision theory, abstraction, approximation, search, heuristics, execution, Markov decision processes  
Address: CANADA, V6T 1Z4  
Affiliation: Department of Computer Science University of British Columbia Vancouver, British Columbia  
Abstract: Markov decision processes (MDPs) have recently been proposed as useful conceptual models for understanding decision-theoretic planning. However, the utility of the associated computational methods remains open to question: most algorithms for computing optimal policies require explicit enumeration of the state space of the planning problem. We propose an abstraction technique for MDPs that allows approximately optimal solutions to be computed quickly. Abstractions are generated automatically, using an intensional representation of the planning problem (probabilistic strips rules) to determine the most relevant problem features and optimally solving a reduced problem based on these relevant features. The key features of our method are: abstractions can be generated quickly; the abstract solution can be applied directly to the original problem; and the loss of optimality can be bounded. We also describe methods by which the abstract solution can be viewed as a set of default reactions that can be improved incrementally, and used as a heuristic for search-based planning or other MDP methods. Finally, we discuss certain difficulties that point toward other forms of aggregation for MDPs. fl Some parts of this report appeared in preliminary form in "Using Abstractions for Decision-Theoretic Planning with Time Constraints," Proc. of Twelfth National Conf. on Artificial Intelligence (AAAI-94), Seattle, pp.1016-1022 (1994); and in "Integrating Planning and Execution in Stochastic Domains," Proc. of Tenth Conf. on Uncertainty in Artificial Intelligence (UAI-94), Seattle, pp.162-169 (1994). y Corresponding author: Craig Boutilier 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bruce W. Ballard. </author> <title> The *-minimax search procedure for trees containing chance nodes. </title> <journal> Artificial Intelligence, </journal> <volume> 21 </volume> <pages> 327-350, </pages> <year> 1983. </year>
Reference-contexts: The only difference from standard game tree search is the existence of chance nodes, at which expectations are taken, instead of adversary nodes, at which minimum values are backed up. This is the basis of, for instance, Ballard's ?-minimax search <ref> [1] </ref>. We discuss the relationship of our abstraction mechanism and decision tree search below, but we first describe the search mechanism in slightly more detail. The search algorithm constructs a partial decision tree rooted at the current state to determine the best action to perform.
Reference: [2] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1-2):81-138, </volume> <year> 1995. </year> <month> 62 </month>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) [26, 54, 45] have been proposed as a semantic and computational framework in which to formulate DTP problems <ref> [15, 2, 56, 10, 8, 13, 11] </ref>. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value. <p> Much emphasis in DTP research has been placed on the issue of speeding up computation by means of approximation. One class of methods involves restricting search or dynamic programming to local regions or envelopes of the state space <ref> [15, 2, 56] </ref>. This approach reduces the state space to locally accessible regions and allows OR methods to be used on reduced problems. While optimality is sacrificed, judicious choice of relevant states can lead to good approximations. <p> However, local search through the concrete state space (i.e., the construction of a decision tree) can be used to refine these reactions when additional computation time is available. Such a model is reminiscent of reaction-first search [18] or real-time dynamic programming <ref> [2] </ref>. A crucial difference is the existence of an abstract value function to guide this search. <p> can be used to focus computational effort on only those states that directly impact the value of the current state. 2 This view is reminiscent of the envelope method of Dean et al. [15], but is most closely related to the real-time dynamic programming model of Barto, Bradtke and Singh <ref> [2] </ref>. <p> This is the basis for the known relationships between heuristic search techniques and stochastic dynamic programming <ref> [5, 2] </ref>. The advantage of forward search over dynamic programming (at least for short horizon problems) lies in the fact that we need only compute the relevant n-stage value for states reachable from 40 the initial state s (at the appropriate stage n). <p> Values are compared with those for no pruning. <ref> [2] </ref> generalizes RTA* to deal with MDPs, essentially adapting a form of asynchronous value iteration [5] to a real-time setting. The search procedure described above can be viewed as a form of RTDP, where a search tree is used determine which backups are to be performed. <p> We have also performed experiments to investigate the value of caching previously computed best actions (similar to LRTA* [31] or LRTDP <ref> [2] </ref>), and the value of interleaving execution with search. Ta- ble 8 summarizes the results of the search for a sequence of ten actions in a small version of the coffee domain. The columns where execution is interleaved with search show the standard algorithm as described above.
Reference: [3] <author> Richard E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Prince-ton, </address> <year> 1957. </year>
Reference-contexts: In Section 2, we briefly describe the aims of decision-theoretic planning and the suitability of Markov decision processes as a foundational model for DTP. We describe MDPs and various methods for constructing optimal policies, such as value iteration and policy iteration, based on the dynamic programming principle <ref> [3] </ref>. We discuss compact representations for MDPs; in particular, we adopt a variant of the probabilistic strips operators used in [32] that captures independence of action effects in a manner similar to Bayes nets [41].
Reference: [4] <author> D. P. Bertsekas and D. A. Castanon. </author> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34 </volume> <pages> 589-598, </pages> <year> 1989. </year>
Reference-contexts: One of the most promising avenues appears to be the use of more general forms of aggregation. While our method of aggregation, exploiting intensional problem representations, is novel, the notion of aggregation of states to solve MDPs has been explored previously <ref> [4, 52] </ref>. For instance, Bertsekas and Castanon [4] propose an adaptive aggregation method that allows one to group together states in the evaluation phase of policy iteration such that the value produced for any cluster of states approximates the value for each constituent state. <p> One of the most promising avenues appears to be the use of more general forms of aggregation. While our method of aggregation, exploiting intensional problem representations, is novel, the notion of aggregation of states to solve MDPs has been explored previously [4, 52]. For instance, Bertsekas and Castanon <ref> [4] </ref> propose an adaptive aggregation method that allows one to group together states in the evaluation phase of policy iteration such that the value produced for any cluster of states approximates the value for each constituent state.
Reference: [5] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1987. </year>
Reference-contexts: There are several algorithms that can be used to determine V (see <ref> [5, 45] </ref> for details). A straightforward iterative algorithm, called successive approximation, proceeds by constructing the sequence of n-stage-to-go value functions V n . The quantity V n (s) is the expected discounted future reward received when is executed for n stages starting at state s. <p> This is the basis for the known relationships between heuristic search techniques and stochastic dynamic programming <ref> [5, 2] </ref>. The advantage of forward search over dynamic programming (at least for short horizon problems) lies in the fact that we need only compute the relevant n-stage value for states reachable from 40 the initial state s (at the appropriate stage n). <p> Define the quantities M + and M as follows: M + = maxfR (s) : s 2 Sg 1 fi minfR (s) : s 2 Sg 1 fi These quantities are quickly computable (assuming R is represented compactly) and it is easy to see <ref> [5, 45] </ref> that M V fl (s) M + for all states s. In addition, the value function e V fl must also satisfy the same relation (since the range of the abstract reward function e R can only be tighter than that of R). <p> Values are compared with those for no pruning. [2] generalizes RTA* to deal with MDPs, essentially adapting a form of asynchronous value iteration <ref> [5] </ref> to a real-time setting. The search procedure described above can be viewed as a form of RTDP, where a search tree is used determine which backups are to be performed.
Reference: [6] <author> Mark Boddy and Thomas L. Dean. </author> <title> Solving time-dependent planning problems. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 979-984, </pages> <address> Detroit, </address> <year> 1989. </year>
Reference-contexts: This type of issue is addressed in the work of Boddy and Dean <ref> [6, 7] </ref>, Horvitz [23], Russell and Wefald [47] and other work on anytime methods. It is important that we provide techniques for estimating solution time and solution quality as well as methods for improving solution quality in a way that can interact with a module assessing the time-quality tradeoffs.
Reference: [7] <author> Mark Boddy and Thomas L. Dean. </author> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 245-285, </pages> <year> 1994. </year>
Reference-contexts: This type of issue is addressed in the work of Boddy and Dean <ref> [6, 7] </ref>, Horvitz [23], Russell and Wefald [47] and other work on anytime methods. It is important that we provide techniques for estimating solution time and solution quality as well as methods for improving solution quality in a way that can interact with a module assessing the time-quality tradeoffs.
Reference: [8] <author> Craig Boutilier, Thomas Dean, and Steve Hanks. </author> <title> Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Third European Workshop on Planning, </booktitle> <address> Assisi, Italy, </address> <year> 1995. </year>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) [26, 54, 45] have been proposed as a semantic and computational framework in which to formulate DTP problems <ref> [15, 2, 56, 10, 8, 13, 11] </ref>. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value. <p> First, we present a particular structured representation of MDPs using a variant of the probabilistic strips operators used in [32] to describe actions and rewards. This representation is a syntactic variant of certain types of "two-stage" Bayesian networks or influence diagrams <ref> [16, 10, 8] </ref>. This in itself allows large problems to be specified and represented in a concise and natural fashion. The key aim of this paper is the exploitation of structured representations to quickly identify appropriate dimensions for abstraction. <p> In addition, the expected value of a certain course of action is a function of the 3 At least, as currently formulated: there is some possibility that regression methods may prove useful in approximation methods for DTP <ref> [8] </ref>. 6 transitions it induces, allowing rewards to be associated with different aspects of the problem rather than with all-or-nothing goal propositions. Finally, plans can be optimized over a fixed finite period of time, or over an infinite horizon, suitable for ongoing processes. <p> Move | 13.92 Move | 17.92 Move | 14.46 HU C; HRC; O BuyC | 13.05 BuyC | 17.06 BuyC | 13.81 HUC; HRC; O Move | 12.34 Move | 16.34 GetU | 15.66 Table 1: Optimal policy for the COFFEE domain and naturalness are described in some detail in <ref> [8] </ref>. The optimal policy and the corresponding value function V fl for this example are shown in Table 1, as computed by policy iteration using a discounting factor of 0:95.
Reference: [9] <author> Craig Boutilier and Richard Dearden. </author> <title> Approximating value trees in structured dynamic programming. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <address> Bari, Italy, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: It is also possible to apply of the approximation methods developed in this paper to such dynamic, nonuniform methods <ref> [9] </ref>. An advantage of an adaptive scheme like that described in [10, 9] is that the problem of estimating the impact of ignoring marginally relevant atoms (the bottleneck in Section 5) is obviated. <p> It is also possible to apply of the approximation methods developed in this paper to such dynamic, nonuniform methods [9]. An advantage of an adaptive scheme like that described in <ref> [10, 9] </ref> is that the problem of estimating the impact of ignoring marginally relevant atoms (the bottleneck in Section 5) is obviated.
Reference: [10] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1104-1111, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) [26, 54, 45] have been proposed as a semantic and computational framework in which to formulate DTP problems <ref> [15, 2, 56, 10, 8, 13, 11] </ref>. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value. <p> First, we present a particular structured representation of MDPs using a variant of the probabilistic strips operators used in [32] to describe actions and rewards. This representation is a syntactic variant of certain types of "two-stage" Bayesian networks or influence diagrams <ref> [16, 10, 8] </ref>. This in itself allows large problems to be specified and represented in a concise and natural fashion. The key aim of this paper is the exploitation of structured representations to quickly identify appropriate dimensions for abstraction. <p> This would provide a very direct encoding of the reward function we adopt in this example. A related action representation uses "two-stage" Bayes nets <ref> [16, 38, 10] </ref>, in which each action is modeled with a Bayesian network with two "slices" or sets of variables. The first slice represents the values of (possibly multi-valued) variables before the action is performed while the second slice represents the value after the action. <p> In addition, the locally exponential probability tables in the network fail to capture some of the regularities in transition probabilities that allow the strips model to be specified more compactly (e.g., the table for Wet could be represented more compactly <ref> [21, 10, 44] </ref>). Notice however that the independence of the effect of Move on Office and Wet is captured naturally in the network, while standard (stochastic) strips rules cannot express this independence. <p> We feel that the appropriate manner in which to deal with these considerations requires the integration of the abstraction mechanism with dynamic programming solution methods. In particular, to deal with these problems we suggest that adaptive abstraction mechanisms must be adopted. We have begun investigations of such techniques in <ref> [10] </ref>; we elaborate further in Section 6.2.1. 6 Concluding Remarks 6.1 Summary We have argued that Markov decision processes provide a useful foundation for understanding decision theoretic planning, and that computational tools for optimal policy construction may be adapted for DTP. <p> It often turns out that the irrelevance of o 2 can be detected early in the development of an optimal policy <ref> [10] </ref>. More generally, the relevance of literals can vary dramatically with the policy adopted. This means 60 that dynamic rather than fixed aggregations should be adopted: one should often allow the aggregation to vary with the current policy in policy iteration. <p> In contrast to a uniform aggregation, where clusters are of the same "size" and make the same distinctions, a nonuniform aggregation would be appropriate here. We have begun explorations of the use of intensional representations for creating dynamic, nonuniform aggregation techniques for solving MDPs. In <ref> [10] </ref> we describe an algorithm in which a decision-tree representation is used to represent value functions and policies so that regularities in these functions can be exploited and the functions themselves can be represented compactly. <p> It is also possible to apply of the approximation methods developed in this paper to such dynamic, nonuniform methods [9]. An advantage of an adaptive scheme like that described in <ref> [10, 9] </ref> is that the problem of estimating the impact of ignoring marginally relevant atoms (the bottleneck in Section 5) is obviated.
Reference: [11] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially ob-servable decision processes using compact representations. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Portland, OR, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) [26, 54, 45] have been proposed as a semantic and computational framework in which to formulate DTP problems <ref> [15, 2, 56, 10, 8, 13, 11] </ref>. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value. <p> Our initial investigations of representational and abstraction methods for MDPs, described in this paper, are therefore directed toward FOMDPs. However, we fully expect these and related methods will be adaptable to POMDPs (see <ref> [11] </ref> for investigations of this point). Primarily for reasons of presentation, we do not consider action costs in our formulation of MDPs. All utilities are associated with states (or propositions). This assumption is not especially restrictive, for our algorithms can be augmented to deal with more general reward specifications. <p> The computational difficulty of solving POMDPs optimally is well-documented, so the use of approximation becomes crucial. Some methods based on function approximation are described in [39, 33]; and preliminary investigations of the use of intensional representations to determine dynamic, nonuniform, exact aggregations are described in <ref> [11] </ref>.
Reference: [12] <author> Craig Boutilier and Martin L. Puterman. </author> <title> Process-oriented planning and averagereward optimality. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1096-1103, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value. It can also be applied to settings without obvious termination conditions, such as on-going processes <ref> [12] </ref>, which cannot easily be dealt with by current goal-based planning algorithms. While MDPs provide firm semantic foundations for much of DTP, the question of their computational utility for AI remains. <p> The articulation of explicit goals is also frequently absent in DTP problems, which often have a process-oriented flavor. For example, our agent above may be acting in a constant loop of anticipating and performing routine tasks and achieving certain requests without consideration of termination conditions <ref> [12] </ref>. Manufacturing processes are often best viewed this way as well: the aim is not to reach some final state where a certain number of units have been produced, rather one wishes to maximize throughput subject to other considerations of importance (such as safety, labor and maintenance constraints). <p> However, explicit consideration of action costs would detract from the main points of this paper. Finally, we note that our examples are primarily goal-based, again for ease of presentation. However, our algorithms can be applied directly to process- oriented problems (see, e.g., <ref> [12] </ref> for process-oriented problems that extend the types of examples we present here). 2.1 Markov Decision Processes For our purposes, a Markov decision process can be defined as a tuple hS; A; T; Ri, where S is a finite set of states or possible worlds, A is a finite set of
Reference: [13] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) [26, 54, 45] have been proposed as a semantic and computational framework in which to formulate DTP problems <ref> [15, 2, 56, 10, 8, 13, 11] </ref>. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value. <p> Indeed, while special purpose code for FOMDPs can often handle systems with hundreds of thousands of states, dealing with twenty-state systems is often problematic for POMDP algorithms <ref> [35, 13] </ref>. Our initial investigations of representational and abstraction methods for MDPs, described in this paper, are therefore directed toward FOMDPs. However, we fully expect these and related methods will be adaptable to POMDPs (see [11] for investigations of this point).
Reference: [14] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Input generalization in delayed rein-forcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 726-731, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: Aggregation methods and function approximation have also been studied to a large extent in the reinforcement learning community, albeit not usually based on intensional problem descriptions, and particular ideas in that work can also play a crucial role in determining good abstractions <ref> [37, 14, 53] </ref>. It is important to point out that many aggregation algorithms are useful for dealing with metric state spaces, such as robot navigation domains, where states can be clustered according to their distance from each other.
Reference: [15] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 574-579, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) [26, 54, 45] have been proposed as a semantic and computational framework in which to formulate DTP problems <ref> [15, 2, 56, 10, 8, 13, 11] </ref>. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value. <p> Much emphasis in DTP research has been placed on the issue of speeding up computation by means of approximation. One class of methods involves restricting search or dynamic programming to local regions or envelopes of the state space <ref> [15, 2, 56] </ref>. This approach reduces the state space to locally accessible regions and allows OR methods to be used on reduced problems. While optimality is sacrificed, judicious choice of relevant states can lead to good approximations. <p> to view abstraction: the abstraction process is used to generate default reactions and heuristics (or static-evaluation functions) to guide and prune an online search for good actions; or the abstraction process provides a fast means for building approximately optimal plans in an off-line planning system. 1 The envelope method of <ref> [15] </ref> in fact uses a heuristic function to estimate the value of falling out of the current envelope; but it is not clear how to construct such functions for a desired level of accuracy. 3 The main aims of this paper are: to present a compact and natural represen-tation of MDPs; <p> policy and value iteration, which consider the appropriate action choice and value of all states, local search can be used to focus computational effort on only those states that directly impact the value of the current state. 2 This view is reminiscent of the envelope method of Dean et al. <ref> [15] </ref>, but is most closely related to the real-time dynamic programming model of Barto, Bradtke and Singh [2]. <p> Another way of exploiting known starting states is the adoption of envelope methods <ref> [15, 56] </ref>, where (likely) reachability from the start state is used to cluster states into a set of IN states and OUT states.
Reference: [16] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: First, we present a particular structured representation of MDPs using a variant of the probabilistic strips operators used in [32] to describe actions and rewards. This representation is a syntactic variant of certain types of "two-stage" Bayesian networks or influence diagrams <ref> [16, 10, 8] </ref>. This in itself allows large problems to be specified and represented in a concise and natural fashion. The key aim of this paper is the exploitation of structured representations to quickly identify appropriate dimensions for abstraction. <p> This would provide a very direct encoding of the reward function we adopt in this example. A related action representation uses "two-stage" Bayes nets <ref> [16, 38, 10] </ref>, in which each action is modeled with a Bayesian network with two "slices" or sets of variables. The first slice represents the values of (possibly multi-valued) variables before the action is performed while the second slice represents the value after the action.
Reference: [17] <author> Denise Draper, Steve Hanks, and Daniel Weld. </author> <title> A probabilistic model of action for least-commitment planning with information gathering. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 178-186, </pages> <address> Seattle, </address> <year> 1994. </year> <month> 63 </month>
Reference-contexts: The role of decision theory in such a process is to decide which tests are "worth it"; thus a priori goal states such as "Part X should be free of faults with probability 0:995" are useful only in specific structured settings (see, e.g., <ref> [32, 17] </ref> where this view is pursued). Features such as these make Markov decision processes an ideal model for modeling DTP problems.
Reference: [18] <author> Mark Drummond, Keith Swanson, John Bresina, and Richard Levinson. </author> <title> Reaction-first search. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1408-1414, </pages> <address> Chambery, </address> <year> 1993. </year>
Reference-contexts: However, local search through the concrete state space (i.e., the construction of a decision tree) can be used to refine these reactions when additional computation time is available. Such a model is reminiscent of reaction-first search <ref> [18] </ref> or real-time dynamic programming [2]. A crucial difference is the existence of an abstract value function to guide this search.
Reference: [19] <author> Richard E. Fikes and Nils J. Nilsson. </author> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: Such representations are extremely compact in normal circumstances because actions exhibit a number of regularities that can be exploited. To represent stochastic actions compactly, we adopt a probabilistic variant of strips rules very similar to that used in buridan [32]. In the classical strips representation <ref> [19] </ref>, an action is represented using a list of effects, or a set of literals that become true when the action is executed. When an action is executed at a state, the effect is "applied" to the current state to determine the new state that results.
Reference: [20] <author> Simon French. </author> <title> Decision Theory. </title> <publisher> Halsted Press, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: In a decision-theoretic setting such as ours, state space search amounts to the construction of a decision tree, familiar from decision analysis <ref> [27, 20, 41] </ref>. The value of taking an action a at a state s is the (discounted) weighted average of the value of all possible states that may result from a. The action selected for s is that with the highest expected value.
Reference: [21] <author> Dan Geiger and David Heckerman. </author> <booktitle> Advances in probabilistic reasoning. In Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 118-126, </pages> <address> Los Angeles, </address> <year> 1991. </year>
Reference-contexts: In addition, the locally exponential probability tables in the network fail to capture some of the regularities in transition probabilities that allow the strips model to be specified more compactly (e.g., the table for Wet could be represented more compactly <ref> [21, 10, 44] </ref>). Notice however that the independence of the effect of Move on Office and Wet is captured naturally in the network, while standard (stochastic) strips rules cannot express this independence.
Reference: [22] <author> Peter Haddawy and Anhai Doan. </author> <title> Abstracting probabilistic actions. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 270-277, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: To some extent, our abstraction mechanism does aggregate actions, either by collapsing "branches" when discriminants within an action become the same, or (implicitly) combining actions themselves. However, this action simplification is driven by abstraction in the state space. Considerations unique to actions themselves may also be applied (see, e.g., <ref> [22] </ref> where hand-crafted action abstractions are analyzed), but the automatic aggregation of (components of) action descriptions remains largely unexplored. 6.2.2 Other Directions Other directions in which this work can be extended include the development of problem-specific abstraction mechanisms.
Reference: [23] <author> Eric J. Horvitz. </author> <title> Computation and action under bounded resources. </title> <type> Technical Report KSL-90-76, </type> <institution> Stanford University, Stanford, </institution> <month> December </month> <year> 1990. </year> <type> Ph.D. thesis. </type>
Reference-contexts: This type of issue is addressed in the work of Boddy and Dean [6, 7], Horvitz <ref> [23] </ref>, Russell and Wefald [47] and other work on anytime methods. It is important that we provide techniques for estimating solution time and solution quality as well as methods for improving solution quality in a way that can interact with a module assessing the time-quality tradeoffs.
Reference: [24] <author> Eric J. Horvitz and Adrian C. Klein. </author> <title> Utility-based abstraction and categorization. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 128-135, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: es 2 e S , fi fi e V e (es) V (s) fi ffi Proof We prove inductively that for all n fi fi e V n (es) V n fi fi i=0 2 14 The use of utility spans to generate abstractions is proposed by Horvitz and Klein <ref> [24] </ref>, who use the notion in single-step decision making. Our analysis can be applied to their framework to establish bounds on the degree to which an "abstract decision" can be less than optimal.
Reference: [25] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference-contexts: We can also exactly compute the value V using the following formula due to Howard <ref> [25] </ref>: V (s) = R (s) + fi t2S We can find the value of for all states by solving this set of linear equations V (s); 8s 2 S. <p> As with successive approximation, V fl is a fixed point of Equation 3 and, if used as an initial value estimate, results in immediate convergence. Policy iteration is an ingenious algorithm proposed by Howard <ref> [25] </ref> for optimal policy construction. It proceeds as follows: 1. Let 0 be any policy on S 2. <p> This continues until no local policy improvement is possible. The algorithm is guaranteed to converge <ref> [25] </ref> and in practice tends to do so in relatively few iterations [45]. The evaluation phase requires solving the set of jSj linear equations. Algorithms for solving linear equations of this kind are typically O (n 3 ) where n is the number of variables (here n = jSj).
Reference: [26] <author> Ronald A. Howard. </author> <title> Dynamic Probabilistic Systems. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) <ref> [26, 54, 45] </ref> have been proposed as a semantic and computational framework in which to formulate DTP problems [15, 2, 56, 10, 8, 13, 11]. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value.
Reference: [27] <author> Ronald A. Howard and James E. </author> <title> Matheson, </title> <editor> editors. </editor> <booktitle> Readings on the Principles and Applications of Decision Analysis. Strategic Decision Group, </booktitle> <address> Menlo Park, CA, </address> <year> 1984. </year>
Reference-contexts: We briefly sketch ways to compute the error bound associated with a particular abstraction as well as improve this bound through judicious selection of new relevant atoms. Deciding which atoms to add to the set IR is essentially a value of information calculation <ref> [27, 41] </ref>. We could imagine for example being given a time bound and wanting the best possible solution computable within that time. <p> Thus we cannot guarantee that determining the most important single atom will aid in constructing the most valuable set of two (or more) atoms. Determining a set of variables of fixed size with greatest information value generally requires exhaustive search through the space of possible sets <ref> [27] </ref>. If time restrictions require a set IR of size k, we potentially have to enumerate all size k subsets of the set of atoms P and determine their span, choosing a set with smallest span. <p> In a decision-theoretic setting such as ours, state space search amounts to the construction of a decision tree, familiar from decision analysis <ref> [27, 20, 41] </ref>. The value of taking an action a at a state s is the (discounted) weighted average of the value of all possible states that may result from a. The action selected for s is that with the highest expected value.
Reference: [28] <author> R. L. Keeney and H. Raiffa. </author> <title> Decisions with Multiple Objectives: Preferences and Value Trade-offs. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: An alternative representation, which we do not pursue but which could exploited by our algorithms below, is the association of independent, additive rewards with a number of propositions in the manner of multi-attribute utility theory <ref> [28] </ref>, and to sum the individual rewards of each proposition satisfied by s to determine R (s). This would provide a very direct encoding of the reward function we adopt in this example.
Reference: [29] <author> Craig A. Knoblock. </author> <title> Automatically generating abstractions for planning. </title> <journal> Artificial Intelligence, </journal> <volume> 68 </volume> <pages> 243-302, </pages> <year> 1994. </year>
Reference-contexts: We present an approximation method for MDPs based on the construction and solution of a smaller abstract MDP. Our method for generating abstract MDPs is based on Knoblock's <ref> [29] </ref> abstraction generation technique for classical planning: certain literals are deleted from the problem description. These literals are, roughly speaking, those whose impact on the value of a state or policy is "negligible." However, there are some critical differences in our model. <p> In Section 4, we describe how abstract MDPs and their solutions can be exploited and improved in the planning process, in both off-line and online models of plan construction. We first describe levels of abstraction, or abstraction hierarchies <ref> [29] </ref>. In classical abstraction, hierarchies are generally constructed so 4 that various refinement properties are satisfied; that is, the solution at a given abstraction level can be refined (without changing any of its components) to provide a solution at a less abstract level. <p> While the solutions to these simpler problems are not generally executable plans, they reduce the complexity of the problem by guiding the search for a solution at less abstract levels <ref> [49, 29, 30] </ref>. We describe an abstraction method, similar in spirit to those used in classical planning, for dealing with large state spaces in solving MDPs. <p> We describe an abstraction method, similar in spirit to those used in classical planning, for dealing with large state spaces in solving MDPs. In particular, we adopt a method similar to abstrips <ref> [49, 29] </ref> in which an abstract problem is one where certain details of the original problem, in this case propositional atoms, are ignored. However, in contrast to this traditional work, the solutions to our abstract problems will be directly executable. <p> Automatic construction of an abstract MDP requires first that we identify the set of relevant atoms that must be retained in the abstract problem description. The procedure that makes this identification uses a form of value of information as well as a variant of Knoblock's <ref> [29] </ref> algorithm for constructing abstractions in a classical setting. The abstract state space e S is the set of states induced by the language obtained by deleting the set of irrelevant atoms. <p> this fact allows the user to specify the degree of accuracy required of the abstraction, and to have an abstract policy calculated automatically. (The set IR may be chosen automatically as well; see Section 3.3.) The algorithm we use to generate the set of relevant atoms is based on Knoblock's <ref> [29] </ref> algorithm for determining constraints for problem-specific abstractions. Intuitively, the algorithm backchains through action descriptions to see what atoms influence immediately relevant atoms, what atoms influence those, and so on, until a fixed point is reached. The algorithm is described in above, and a set IR of immediately relevant atoms.
Reference: [30] <author> Craig A. Knoblock, Josh D. Tenenberg, and Qiang Yang. </author> <title> Characterizing abstrac-tion hierarchies for planning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 692-697, </pages> <address> Anaheim, </address> <year> 1991. </year>
Reference-contexts: While the solutions to these simpler problems are not generally executable plans, they reduce the complexity of the problem by guiding the search for a solution at less abstract levels <ref> [49, 29, 30] </ref>. We describe an abstraction method, similar in spirit to those used in classical planning, for dealing with large state spaces in solving MDPs.
Reference: [31] <author> Richard E. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 189-211, </pages> <year> 1990. </year>
Reference-contexts: Expectation pruning is quite closely related to what Korf <ref> [31] </ref> calls alpha-pruning. The difference is that while Korf relies on a property 42 of the heuristic that it is always increasing, we rely on an estimate of the actual error in the heuristic. <p> For the best results, we suggest allowing pruning to a depth one less than the search tree. 4.3 Integrating Planning and Execution The type of search described above provides an online, anytime method for planning and action selection. These types of considerations form, for instance, the basis of Korf's <ref> [31] </ref> RTA* algorithm. Real-time dynamic programming (RTDP) 45 Search depth Prune depth Percentage of Percentage of states pruned search time 2 1 18.6 88.2 3 1 13.1 70.6 3 59.5 112.0 2 9.5 75.2 4 67.1 140.2 Table 7: Effects of limiting the depth to which pruning is performed. <p> We have also performed experiments to investigate the value of caching previously computed best actions (similar to LRTA* <ref> [31] </ref> or LRTDP [2]), and the value of interleaving execution with search. Ta- ble 8 summarizes the results of the search for a sequence of ten actions in a small version of the coffee domain. The columns where execution is interleaved with search show the standard algorithm as described above.
Reference: [32] <author> Nicholas Kushmerick, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilis-tic least-commitment planning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1073-1078, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: The unrealistic assumptions embodied in much classical planning research, such as complete knowledge of the initial state and completely predictable action effects, have been challenged in, for instance, work on conditional planning [50, 43] and probabilistic planning <ref> [32] </ref>. The problem of decision-theoretic planning (DTP) involves the design of plans or policies in situations where the initial conditions and the effects of actions are not known with certainty, and in which multiple, potentially conflicting objectives must be traded against one another to determine an optimal course of action. <p> In this paper, we explore a different way of coping with the computational difficulties involved in optimal policy generation for large state spaces. First, we present a particular structured representation of MDPs using a variant of the probabilistic strips operators used in <ref> [32] </ref> to describe actions and rewards. This representation is a syntactic variant of certain types of "two-stage" Bayesian networks or influence diagrams [16, 10, 8]. This in itself allows large problems to be specified and represented in a concise and natural fashion. <p> We describe MDPs and various methods for constructing optimal policies, such as value iteration and policy iteration, based on the dynamic programming principle [3]. We discuss compact representations for MDPs; in particular, we adopt a variant of the probabilistic strips operators used in <ref> [32] </ref> that captures independence of action effects in a manner similar to Bayes nets [41]. Such a representation allows MDPs to be specified concisely and naturally by exploiting structural regularities in the domain and in the effects of actions. <p> The role of decision theory in such a process is to decide which tests are "worth it"; thus a priori goal states such as "Part X should be free of faults with probability 0:995" are useful only in specific structured settings (see, e.g., <ref> [32, 17] </ref> where this view is pursued). Features such as these make Markov decision processes an ideal model for modeling DTP problems. <p> Such representations are extremely compact in normal circumstances because actions exhibit a number of regularities that can be exploited. To represent stochastic actions compactly, we adopt a probabilistic variant of strips rules very similar to that used in buridan <ref> [32] </ref>. In the classical strips representation [19], an action is represented using a list of effects, or a set of literals that become true when the action is executed. <p> When applied to state s = f:P; :Q; R; Sg, the resulting state is a (s) = fP; :Q; R; Sg. Pednault [42] generalizes these descriptions somewhat by allowing actions to have conditional effects, or context-dependent effects that vary with the initial state. Following <ref> [32] </ref> we assume that the conditions under which an action can have different effects are described by a finite set of discriminants D = fd 1 ; d n g. This is a set of mutually exclusive and exhaustive logical formulae that partitions the state space. <p> The second domain, the builder domain, involves an agent that must join two objects together and is adapted from standard job-shop scheduling problems used to test partial-order planners like SNLP [36] and buridan <ref> [32] </ref>. It is not designed with the ability to construct good abstractions in mind. For maximum reward, the objects must be machined to the correct shape, clean, painted, and joined together. The reward for any given state is simply the sum of the individual rewards for all of these attributes.
Reference: [33] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 362-370, </pages> <address> Lake Tahoe, </address> <year> 1995. </year>
Reference-contexts: Finally, methods such as these must be extended to partially-observable settings if they are to be applied to general DTP problems. The computational difficulty of solving POMDPs optimally is well-documented, so the use of approximation becomes crucial. Some methods based on function approximation are described in <ref> [39, 33] </ref>; and preliminary investigations of the use of intensional representations to determine dynamic, nonuniform, exact aggregations are described in [11].
Reference: [34] <author> Michael L. Littman, Thomas L. Dean, and Leslie Pack Kaelbling. </author> <title> On the complex-ity of solving Markov decision problems. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 394-402, </pages> <address> Montreal, </address> <year> 1995. </year> <month> 64 </month>
Reference-contexts: Aside from the number of iterations, the time per iteration is generally O (jSj 3 ). See <ref> [34] </ref> for a survey of complexity results regarding the solution of MDPs. 27 value and the value of an optimal policy for the original MDP.
Reference: [35] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 </volume> <pages> 47-66, </pages> <year> 1991. </year>
Reference-contexts: Indeed, while special purpose code for FOMDPs can often handle systems with hundreds of thousands of states, dealing with twenty-state systems is often problematic for POMDP algorithms <ref> [35, 13] </ref>. Our initial investigations of representational and abstraction methods for MDPs, described in this paper, are therefore directed toward FOMDPs. However, we fully expect these and related methods will be adaptable to POMDPs (see [11] for investigations of this point).
Reference: [36] <author> David McAllester and David Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 634639, </pages> <address> Anaheim, </address> <year> 1991. </year>
Reference-contexts: The second domain, the builder domain, involves an agent that must join two objects together and is adapted from standard job-shop scheduling problems used to test partial-order planners like SNLP <ref> [36] </ref> and buridan [32]. It is not designed with the ability to construct good abstractions in mind. For maximum reward, the objects must be machined to the correct shape, clean, painted, and joined together.
Reference: [37] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Aggregation methods and function approximation have also been studied to a large extent in the reinforcement learning community, albeit not usually based on intensional problem descriptions, and particular ideas in that work can also play a crucial role in determining good abstractions <ref> [37, 14, 53] </ref>. It is important to point out that many aggregation algorithms are useful for dealing with metric state spaces, such as robot navigation domains, where states can be clustered according to their distance from each other.
Reference: [38] <author> Ann E. Nicholson and Leslie Pack Kaelbling. </author> <title> Toward approximate planning in very large stochastic domains. </title> <booktitle> In AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <pages> pages 190-196, </pages> <address> Stanford, </address> <year> 1994. </year>
Reference-contexts: This would provide a very direct encoding of the reward function we adopt in this example. A related action representation uses "two-stage" Bayes nets <ref> [16, 38, 10] </ref>, in which each action is modeled with a Bayesian network with two "slices" or sets of variables. The first slice represents the values of (possibly multi-valued) variables before the action is performed while the second slice represents the value after the action.
Reference: [39] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1088-1094, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: Finally, methods such as these must be extended to partially-observable settings if they are to be applied to general DTP problems. The computational difficulty of solving POMDPs optimally is well-documented, so the use of approximation becomes crucial. Some methods based on function approximation are described in <ref> [39, 33] </ref>; and preliminary investigations of the use of intensional representations to determine dynamic, nonuniform, exact aggregations are described in [11].
Reference: [40] <author> Judea Pearl. </author> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: In practice, an interruptible search using an iterative deepening technique may well be used, so that at any time the algorithm can be interrupted and the current best action performed. We cannot guarantee that deeper search will produce better results <ref> [40] </ref>; however, the deeper the tree is expanded, the more accurate the estimates of action utility will tend to be, and the more confidence we should have that the action selected approaches optimality. <p> This phenomenon is well-documented in the search literature <ref> [40] </ref>. Search in the coffee domain using a heuristic derived from the 256 state abstract MDP finds a very close to optimal policy; but even four step search is unable to improve on it since the heuristic is so good. heuristics in the coffee domain.
Reference: [41] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1988. </year>
Reference-contexts: We discuss compact representations for MDPs; in particular, we adopt a variant of the probabilistic strips operators used in [32] that captures independence of action effects in a manner similar to Bayes nets <ref> [41] </ref>. Such a representation allows MDPs to be specified concisely and naturally by exploiting structural regularities in the domain and in the effects of actions. In Section 3, we begin to address the computational difficulties associated with solving MDPs optimally. <p> We briefly sketch ways to compute the error bound associated with a particular abstraction as well as improve this bound through judicious selection of new relevant atoms. Deciding which atoms to add to the set IR is essentially a value of information calculation <ref> [27, 41] </ref>. We could imagine for example being given a time bound and wanting the best possible solution computable within that time. <p> In a decision-theoretic setting such as ours, state space search amounts to the construction of a decision tree, familiar from decision analysis <ref> [27, 20, 41] </ref>. The value of taking an action a at a state s is the (discounted) weighted average of the value of all possible states that may result from a. The action selected for s is that with the highest expected value.
Reference: [42] <author> Edwin Pednault. </author> <title> ADL: Exploring the middle ground between STRIPS and the situation calculus. </title> <booktitle> In Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 324-332, </pages> <address> Toronto, </address> <year> 1989. </year>
Reference-contexts: Thus large classes of state transitions can be represented using a single effect. As an example, consider the action a with effect fP; :Qg. When applied to state s = f:P; :Q; R; Sg, the resulting state is a (s) = fP; :Q; R; Sg. Pednault <ref> [42] </ref> generalizes these descriptions somewhat by allowing actions to have conditional effects, or context-dependent effects that vary with the initial state.
Reference: [43] <author> Mark A. Peot and David E. Smith. </author> <title> Conditional nonlinear planning. </title> <booktitle> In Proceedings of the First International Conference on AI Planning Systems, </booktitle> <pages> pages 189-197, </pages> <address> College Park, MD, </address> <year> 1992. </year>
Reference-contexts: The unrealistic assumptions embodied in much classical planning research, such as complete knowledge of the initial state and completely predictable action effects, have been challenged in, for instance, work on conditional planning <ref> [50, 43] </ref> and probabilistic planning [32].
Reference: [44] <author> David Poole. </author> <title> Exploiting the rule structure for decision making within the inde-pendent choice logic. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 454-463, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: In addition, the locally exponential probability tables in the network fail to capture some of the regularities in transition probabilities that allow the strips model to be specified more compactly (e.g., the table for Wet could be represented more compactly <ref> [21, 10, 44] </ref>). Notice however that the independence of the effect of Move on Office and Wet is captured naturally in the network, while standard (stochastic) strips rules cannot express this independence.
Reference: [45] <author> Martin L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) <ref> [26, 54, 45] </ref> have been proposed as a semantic and computational framework in which to formulate DTP problems [15, 2, 56, 10, 8, 13, 11]. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value. <p> We refer to <ref> [45] </ref> for further discussion of MDPs and different optimality criteria. <p> There are several algorithms that can be used to determine V (see <ref> [5, 45] </ref> for details). A straightforward iterative algorithm, called successive approximation, proceeds by constructing the sequence of n-stage-to-go value functions V n . The quantity V n (s) is the expected discounted future reward received when is executed for n stages starting at state s. <p> We set V 0 (s) = R (s) and inductively compute V n X Pr (tj (s); s)V n1 As n ! 1, V n ! V ; and the convergence rate and error for a fixed n can be bounded easily <ref> [45] </ref>. <p> This continues until no local policy improvement is possible. The algorithm is guaranteed to converge [25] and in practice tends to do so in relatively few iterations <ref> [45] </ref>. The evaluation phase requires solving the set of jSj linear equations. Algorithms for solving linear equations of this kind are typically O (n 3 ) where n is the number of variables (here n = jSj). <p> This algorithm tends to work extremely well in practice and can be tuned so that both policy iteration and value iteration are special cases <ref> [46, 45] </ref>. We note that all three policy construction methods produce the value function V fl as well as an optimal policy. <p> Define the quantities M + and M as follows: M + = maxfR (s) : s 2 Sg 1 fi minfR (s) : s 2 Sg 1 fi These quantities are quickly computable (assuming R is represented compactly) and it is easy to see <ref> [5, 45] </ref> that M V fl (s) M + for all states s. In addition, the value function e V fl must also satisfy the same relation (since the range of the abstract reward function e R can only be tighter than that of R).
Reference: [46] <author> Martin L. Puterman and M.C. Shin. </author> <title> Modified policy iteration algorithms for discounted Markov decision problems. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137, </pages> <year> 1978. </year>
Reference-contexts: The improvement phase uses these values in a local computation to find an action that, if executed once at state s, followed by execution of the current policy , results in improved value. The main cost per iteration in the policy iteration is clearly policy evaluation. Puterman and Shin <ref> [46] </ref> have observed that the exact value of the current policy 10 is typically not needed to check for improvement. Their modified policy iteration algorithm is exactly like policy iteration except that the evaluation phase uses some number of successive approximation steps instead of the exact solution method. <p> This algorithm tends to work extremely well in practice and can be tuned so that both policy iteration and value iteration are special cases <ref> [46, 45] </ref>. We note that all three policy construction methods produce the value function V fl as well as an optimal policy.
Reference: [47] <author> Stuart J. Russell and Eric Wefald. </author> <title> Do the Right Thing: Studies in Limited Rationality. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: Furthermore, finer-grained abstractions are guaranteed to increase the value of policies. Finally, abstractions can be generated quickly. These factors allow abstract policies of varying degrees of accuracy to be constructed in an anytime fashion (in particular, in the style of contract anytime algorithms <ref> [47, 48] </ref>). While the abstraction method of approximating optimal policies is orthogonal to the envelope approach, the model we propose actually illustrates that the two approaches complement one another quite nicely. <p> This type of issue is addressed in the work of Boddy and Dean [6, 7], Horvitz [23], Russell and Wefald <ref> [47] </ref> and other work on anytime methods. It is important that we provide techniques for estimating solution time and solution quality as well as methods for improving solution quality in a way that can interact with a module assessing the time-quality tradeoffs.
Reference: [48] <author> Stuart J. Russell and Shlomo Zilberstein. </author> <title> Composing real-time systems. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 212-217, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: Furthermore, finer-grained abstractions are guaranteed to increase the value of policies. Finally, abstractions can be generated quickly. These factors allow abstract policies of varying degrees of accuracy to be constructed in an anytime fashion (in particular, in the style of contract anytime algorithms <ref> [47, 48] </ref>). While the abstraction method of approximating optimal policies is orthogonal to the envelope approach, the model we propose actually illustrates that the two approaches complement one another quite nicely.
Reference: [49] <author> Earl D. Sacerdoti. </author> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 5 </volume> <pages> 115-135, </pages> <year> 1974. </year>
Reference-contexts: While the solutions to these simpler problems are not generally executable plans, they reduce the complexity of the problem by guiding the search for a solution at less abstract levels <ref> [49, 29, 30] </ref>. We describe an abstraction method, similar in spirit to those used in classical planning, for dealing with large state spaces in solving MDPs. <p> We describe an abstraction method, similar in spirit to those used in classical planning, for dealing with large state spaces in solving MDPs. In particular, we adopt a method similar to abstrips <ref> [49, 29] </ref> in which an abstract problem is one where certain details of the original problem, in this case propositional atoms, are ignored. However, in contrast to this traditional work, the solutions to our abstract problems will be directly executable.
Reference: [50] <author> Earl D. Sacerdoti. </author> <title> The nonlinear nature of plans. </title> <booktitle> In Proceedings of the Fourth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 206-214, </pages> <year> 1975. </year>
Reference-contexts: The unrealistic assumptions embodied in much classical planning research, such as complete knowledge of the initial state and completely predictable action effects, have been challenged in, for instance, work on conditional planning <ref> [50, 43] </ref> and probabilistic planning [32].
Reference: [51] <author> M. J. Schoppers. </author> <title> Universal plans for reactive robots in unpredictable environ-ments. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1039-1046, </pages> <address> Milan, </address> <year> 1987. </year> <month> 65 </month>
Reference-contexts: An agent adopting such a policy performs action (s) whenever it finds itself in state s. 6 In a sense, is a conditional and universal plan <ref> [51] </ref>, specifying an action to perform in every possible circumstance. An agent following policy can also be thought of as a reactive system. From a given start state s 0 , a fixed policy induces a distribution over possible system trajectories.
Reference: [52] <author> Paul L. Schweitzer, Martin L. Puterman, and Kyle W. Kindle. </author> <title> Iterative aggregation-disaggregation procedures for discounted semi-Markov reward pro-cesses. </title> <journal> Operations Research, </journal> <volume> 33 </volume> <pages> 589-605, </pages> <year> 1985. </year>
Reference-contexts: One of the most promising avenues appears to be the use of more general forms of aggregation. While our method of aggregation, exploiting intensional problem representations, is novel, the notion of aggregation of states to solve MDPs has been explored previously <ref> [4, 52] </ref>. For instance, Bertsekas and Castanon [4] propose an adaptive aggregation method that allows one to group together states in the evaluation phase of policy iteration such that the value produced for any cluster of states approximates the value for each constituent state.
Reference: [53] <author> Satinder P. Singh, Tommi Jaakkola, and Michael I. Jordan. </author> <title> Reinforcement learn-ing with soft state aggregation. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, ed-itors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, </address> <year> 1994. </year>
Reference-contexts: Aggregation methods and function approximation have also been studied to a large extent in the reinforcement learning community, albeit not usually based on intensional problem descriptions, and particular ideas in that work can also play a crucial role in determining good abstractions <ref> [37, 14, 53] </ref>. It is important to point out that many aggregation algorithms are useful for dealing with metric state spaces, such as robot navigation domains, where states can be clustered according to their distance from each other.
Reference: [54] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21:10711088, </volume> <year> 1973. </year>
Reference-contexts: For this reason, one can view a DTP problem as a problem of optimal stochastic control. Recently, Markov decision processes (MDPs) <ref> [26, 54, 45] </ref> have been proposed as a semantic and computational framework in which to formulate DTP problems [15, 2, 56, 10, 8, 13, 11]. This model allows the formulation of actions with stochastic effects and the specification of states or objectives of differing value.
Reference: [55] <author> David E. Smith and Mark A. Peot. </author> <title> Postponing threats in partial-order plan-ning. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 500-506, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: This ensures that computing span (IR) is exponential in jIRj, but linear in the 15 We note that appropriate preprocessing of actions - e.g., constructing an operator graph as described in <ref> [55] </ref> can make this much more efficient. 33 size of the induced (abstract) state space. 16 To determine the best set of immediately relevant atoms, say, of a given size is equivalent to determining the subset of atoms of that size with the greatest value of information.

References-found: 55

