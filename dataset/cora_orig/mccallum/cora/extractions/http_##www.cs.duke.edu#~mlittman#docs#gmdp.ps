URL: http://www.cs.duke.edu/~mlittman/docs/gmdp.ps
Refering-URL: 
Root-URL: 
Email: szepes@math.u-szeged.hu  mlittman@cs.brown.edu  
Title: Generalized Markov Decision Processes: Dynamic-programming and Reinforcement-learning Algorithms  
Author: Csaba Szepesvari Michael L. Littman 
Date: November 25, 1997  
Address: Szeged 6720 Aradi vrt tere 1. HUNGARY  Providence, RI 02912-1910 USA  
Affiliation: Bolyai Institute of Mathematics "Jozsef Attila" University of Szeged  Department of Computer Science Brown University  
Abstract: The problem of maximizing the expected total discounted reward in a completely observable Markovian environment, i.e., a Markov decision process (mdp), models a particular class of sequential decision problems. Algorithms have been developed for making optimal decisions in mdps given either an mdp specification or the opportunity to interact with the mdp over time. Recently, other sequential decision-making problems have been studied prompting the development of new algorithms and analyses. We describe a new generalized model that subsumes mdps as well as many of the recent variations. We prove some basic results concerning this model and develop generalizations of value iteration, policy iteration, model-based reinforcement-learning, and Q-learning that can be used to make optimal decisions in the generalized model under various assumptions. Applications of the theory to particular models are described, including risk-averse mdps, exploration-sensitive mdps, sarsa, Q-learning with spreading, two-player games, and approximate max picking via sampling. Central to the results are the contraction property of the value operator and a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew G. Barto, S. J. Bradtke, and Satinder P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138, </pages> <year> 1995. </year>
Reference-contexts: Also, if R t = R and P t = P for all t, this result implies that asynchronous dynamic programming converges to the optimal value function <ref> [2, 1] </ref>. 25 4.7 SAMPLED MAX The asynchronous dynamic-programming algorithm uses insights from the reinforcement-learning literature to solve dynamic programming problems more efficiently.
Reference: [2] <author> Andrew G. Barto, Richard S. Sutton, and Christopher J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <type> Technical Report 89-95, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1989. </year> <title> Also published in Learning and Computational Neuroscience: Foundations of Adaptive Networks, </title> <editor> Michael Gabriel and John Moore, editors. </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: A discount parameter 0 fl &lt; 1 controls the degree to which future rewards are significant compared to immediate rewards. The theory of Markov decision processes can be used as a theoretical foundation for important results concerning this decision-making problem <ref> [2] </ref>. A (finite) Markov decision process (mdp) [31] is defined by the tuple hX; A; P; Ri, where X represents a finite set of states, A a finite set of actions, P a transition function, and R a reward function. <p> Also, if R t = R and P t = P for all t, this result implies that asynchronous dynamic programming converges to the optimal value function <ref> [2, 1] </ref>. 25 4.7 SAMPLED MAX The asynchronous dynamic-programming algorithm uses insights from the reinforcement-learning literature to solve dynamic programming problems more efficiently.
Reference: [3] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: These simultaneous equations, known as the Bellman equations, can be solved using a variety of techniques ranging from successive approximation <ref> [3] </ref> to linear programming [11]. In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. <p> Section 3 describes a general theorem that can be used to prove the convergence of several reinforcement-learning algorithms in the generalized mdp framework. 2 SOLVING GENERALIZED MDPS VIA A MODEL The most basic algorithms for solving mdps are value iteration <ref> [3] </ref> and policy iteration [18]; both date back the late 1950s. <p> This section describes how these algorithms can be applied to solve generalized Markov decision processes. 2.1 VALUE ITERATION The method of value iteration, or successive approximations <ref> [3, 39] </ref>, is a way of iteratively computing arbitrarily good approximations to the optimal value function V fl . A single step of the process starts with an estimate V t1 of the optimal value function, and produces a better estimate V t = T V t1 .
Reference: [4] <author> A. Benveniste, M. Metivier, and P. Priouret. </author> <title> Adaptive Algorithms and Stochastic Approximations. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: We note that these results rely only on the non-expansion and contraction properties of the involved operators. The above convergence theorem can be extended to the case when the agent follows a given exploration "metapolicy" (e.g., by using the results from stochastic-approximation theory <ref> [4] </ref>) which ensures that every state-action pair is visited infinitely often and that there exists a limit probability distribution over the states X. For example, persistently exciting (exploring) policies satisfy these conditions.
Reference: [5] <author> D.P. Bertsekas. </author> <title> Monotone mappings with application in dynamic programming. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 15(3) </volume> <pages> 438-464, </pages> <year> 1977. </year>
Reference-contexts: His model can be viewed as the continuation of the work of Bertsekas <ref> [5] </ref> and Bertsekas and Shreve [6], who proved similar statements under different assumptions. Waldmann [58] developed a highly general model of dynamic-programming problems, with a focus on deriving approximation bounds. Heger [15, 16] extended many of the standard mdp results to cover the risk-sensitive model. <p> The problem is that, in practice, it is usually clear that V fl n = T n V , but it is much harder to show that V fl n converges to V fl <ref> [5, 48] </ref>. 32 Another possible direction for future research is to apply to modern ODE (ordinary dif-ferential equation) theory of stochastic approximations.
Reference: [6] <author> D.P. Bertsekas and S.E. Shreve. </author> <title> Stochastic Optimal Control: The Discrete Time Case. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Grouping like terms gives kV V fl k *=(1 fl). q.e.d. We next bound the distance between V and V fl in terms of *, the Bellman error magnitude (related arguments have been made before <ref> [6, 42, 62, 16] </ref> 6 ). Theorem 2 Let V be a value function, V be the value function for the myopic policy with respect to V , and V fl be the optimal value function. <p> Continue in this way until t+1 = t . The traditional proof of convergence relies on the following facts [18]: 6 The most general of these arguments is due to Bertsekas and Shreve <ref> [6] </ref> (Proposition 4.5) for extremization problems (although, the authors do not exploit this property). They also consider value iteration when the precision of computation is limited. <p> Most of the previous results on asynchronous policy iteration can be repeated since those proofs depend only on the monotonicity and the contraction properties of the involved operators [61, 40]. The work of Bertsekas and Shreve <ref> [6] </ref> is also worth mentioning here: they have considered a version of policy iteration in which both myopic policies and the evaluation of these policies are determined with a precision geometrically increasing in time. <p> He found that better learning performance can be achieved if the Q-learning rule is changed to incorporate the condition of persistent exploration. More precisely, in some domains, John's learning rule performs better than standard Q-learning when exploration is 12 Such a criterion was also analyzed by Bertsekas and Shreve <ref> [6] </ref>. 13 The necessity of this condition is clear since in this Q-learning algorithm we need to estimate the operator min y:P (x;a;y)&gt;0 from the observed transitions, and the underlying iterative method-as discussed in Section 3.2|is consistent only if the initial estimate is overestimating. <p> His model can be viewed as the continuation of the work of Bertsekas [5] and Bertsekas and Shreve <ref> [6] </ref>, who proved similar statements under different assumptions. Waldmann [58] developed a highly general model of dynamic-programming problems, with a focus on deriving approximation bounds. Heger [15, 16] extended many of the standard mdp results to cover the risk-sensitive model.
Reference: [7] <author> Justin A. Boyan. </author> <title> Modular neural networks for learning context-dependent game strategies. </title> <type> Master's thesis, </type> <institution> Department of Engineering and Computer Laboratory, University of Cambridge, </institution> <address> Cambridge, UK, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: A great deal of reinforcement-learning research has been directed to solving games of this kind <ref> [51, 52, 37, 7] </ref>, Algorithms for solving mdps and their convergence proofs do not apply directly to these problems. <p> Markov games <ref> [7] </ref> max a or min b f (x; b) P risk-sensitive mdps [15] max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) expl.-sens. mdps [20] max 2P 0 P P Markov games [24] max A min b P P Table 1: Some models and their specification as generalized
Reference: [8] <author> S.J. Bradtke. </author> <title> Incremental dynamic programming for on-line adaptive optimal control. </title> <type> Technical Report 94-62, </type> <institution> Department of Computer and Information Science, University of Massachusetts, Amherst, Massachusetts, </institution> <year> 1994. </year>
Reference-contexts: ff t , satisfies the criteria P 1 P 1 t &lt; 1 then for each (x; a) P 1 x t ; a = a t ) = 1 and t=1 ff 2 t (x = x t ; a = a t ) &lt; 1 will still hold <ref> [8] </ref>. This second condition is satisfied since ff 2 t (x = x t ; a = a t ) ff 2 t for all t 1. The following propositions give some conditions under which the first condition is still met.
Reference: [9] <author> Anne Condon. </author> <title> The complexity of stochastic games. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 203-224, </pages> <month> February </month> <year> 1992. </year> <month> 50 </month>
Reference-contexts: In the zero-sum games we consider, the rewards to player 2 (the minimizer) are simply the additive inverse of the rewards for player 1 (the maximizer). Markov decision processes are a special case of alternating Markov games in which X 2 = ;; Condon <ref> [9] </ref> proves this and the other unattributed results in this section. A common optimality criterion for alternating Markov games is discounted minimax optimality. Under this criterion, the maximizer should choose actions so as to maximize its reward in the event that the minimizer chooses the best possible counter-policy.
Reference: [10] <author> Anne Condon. </author> <title> On algorithms for simple stochastic games. </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> 13 </volume> <pages> 51-71, </pages> <year> 1993. </year>
Reference-contexts: A key difference between mdps and alternating Markov games is that the former can be solved (i.e., an optimal policy can be found) in polynomial time using linear programming; no such algorithm is known for solving alternating Markov games <ref> [10] </ref> 1 . 1.3 GENERALIZED MDPS In alternating Markov games and mdps, optimal behavior can be identified by solving the Bellman equations; any myopic policy with respect to the optimal value function is optimal. <p> For alternating Markov games this would mean that V t+1 (x) V t (x) for x 2 X 1 and V t+1 (x) V t (x) for x 2 X 2 . However, as a careful analysis of an example by Condon <ref> [10] </ref> shows, the additive structure of rewards is incompatible with this condition. To be able to work with additive rewards, we need to separate the minimumization and maximumization operators. A more complex example is Markov games, in which R is not finite; it will be described in Section 4.2.
Reference: [11] <author> Cyrus Derman. </author> <title> Finite State Markovian Decision Processes. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1970. </year>
Reference-contexts: These simultaneous equations, known as the Bellman equations, can be solved using a variety of techniques ranging from successive approximation [3] to linear programming <ref> [11] </ref>. In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions.
Reference: [12] <author> Geoffrey J. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <editor> In Ar-mand Prieditis and Stuart Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 261-268, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is interesting and important to ask how close Q 0 , the fixed point of T where T is defined by (17), is to the true optimal Q fl . By Theorem 6.2 of Gordon <ref> [12] </ref> we have that kQ 0 Q fl k 1 fl where * = inff kQ Q fl k j F Q = Q g; where (F Q)(z; a) = P x ^s (z; a; x)Q (x; a). This helps us to define the spreading coefficients s (z; a; x). <p> In order to understand this, let us reformulate the model suggested by Gordon <ref> [12] </ref>. Let us fix a subset of X, say X 0 . This is the "sample space", which should be much smaller than X. Let A : (X 0 fi A ! &lt;) ! (X fi A ! &lt;) be a "function approximator".
Reference: [13] <author> Vijaykumar Gullapalli and Andrew G. Barto. </author> <title> Convergence of indirect adaptive asynchronous value iteration algorithms. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 695-702, </pages> <address> San Ma-teo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Both model-free (direct) methods, such as Q-learning [59, 60], and model-based (indirect) methods, such as prioritized sweeping [29] and DYNA [46], have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref> [60, 53, 19, 13] </ref>. <p> In model-based reinforcement learning, R and P are estimated on-line, and the value function is updated according to the approximate dynamic-programming operator derived from these estimates; this algorithm converges to the optimal value function under a wide variety of choices of the order states are updated <ref> [13] </ref>. The method of Q-learning [59] uses experience to estimate the optimal value function without ever explicitly approximating R and P . <p> The convergence of asynchronous dynamic programming to the optimal value function (under the assumption that all states are visited infinitely often) follows from the work of Gullapalli and Barto <ref> [13] </ref> and the results in this paper. When the set of actions is extremely large, computing the value of the maximum action in Equation (11) becomes impractical.
Reference: [14] <author> Matthias Heger. </author> <title> Risk-averse reinforcement learning. </title> <type> Ph.D. thesis, </type> <note> in preparation. </note>
Reference-contexts: Heger studied the risk-sensitive criterion defined by ( g)(x; a) = min y:P (a;x;y)&gt;0 g (x; a; y) and ( f )(x) = max a f (x; a). In this section, we will prove the following theorem (also proven by Heger <ref> [14] </ref>).
Reference: [15] <author> Matthias Heger. </author> <title> Consideration of risk in reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 105-111, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this paper, we introduce a generalized Markov decision process model with applications to reinforcement learning, and list some of the important results concerning the model. 2 Generalized mdps provide a foundation for decision making in mdps and games, as well as in risk-sensitive models <ref> [15] </ref>, exploration-sensitive models [20, 36], simultaneous-action games [39], and other models. The common feature of these decision problems is that the reward function is based on the total, discounted cost|this latter property enables us to apply arguments based on the properties of contraction mappings, which makes the analysis tractable. <p> Markov games [7] max a or min b f (x; b) P risk-sensitive mdps <ref> [15] </ref> max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) expl.-sens. mdps [20] max 2P 0 P P Markov games [24] max A min b P P Table 1: Some models and their specification as generalized Markov decision processes. the norm on the appropriate spaces 3 . <p> Section 4.8 discusses this in more detail. Lemma 4 is concerned with the case when the x t states are sampled asymptotically according to a distribution function p 1 defined over X (Pr (x t = x) converges to p 1 (x)). 4.4 RISK-SENSITIVE MODELS Heger <ref> [15] </ref> described an optimality criterion for mdps in which only the worst possible value of the next state makes a contribution to the value of a state 12 . <p> For the model in which ( N f )(x) = max a f (x; a), Heger defined a Q-learning-like algorithm that converges to optimal policies without estimating R and P online <ref> [15] </ref>. In essence, the learning algorithm uses an update rule analogous to the rule in Q-learning with the additional requirement that the initial Q function be set optimistically; that is, Q 0 (x; a) Q fl (x; a) for all x and a 13 . <p> His model can be viewed as the continuation of the work of Bertsekas [5] and Bertsekas and Shreve [6], who proved similar statements under different assumptions. Waldmann [58] developed a highly general model of dynamic-programming problems, with a focus on deriving approximation bounds. Heger <ref> [15, 16] </ref> extended many of the standard mdp results to cover the risk-sensitive model. Although his work derives many of the important theorems, it does not present these theorems in a generalized way to allow them to be applied to any other models.
Reference: [16] <author> Matthias Heger. </author> <title> The loss from imperfect value functions in expectation-based and minimax-based tasks. </title> <journal> Machine Learning, </journal> 22(1/2/3):197-226, 1996. 
Reference-contexts: Grouping like terms gives kV V fl k *=(1 fl). q.e.d. We next bound the distance between V and V fl in terms of *, the Bellman error magnitude (related arguments have been made before <ref> [6, 42, 62, 16] </ref> 6 ). Theorem 2 Let V be a value function, V be the value function for the myopic policy with respect to V , and V fl be the optimal value function. <p> His model can be viewed as the continuation of the work of Bertsekas [5] and Bertsekas and Shreve [6], who proved similar statements under different assumptions. Waldmann [58] developed a highly general model of dynamic-programming problems, with a focus on deriving approximation bounds. Heger <ref> [15, 16] </ref> extended many of the standard mdp results to cover the risk-sensitive model. Although his work derives many of the important theorems, it does not present these theorems in a generalized way to allow them to be applied to any other models.
Reference: [17] <author> A. J. Hoffman and R. M. Karp. </author> <title> On nonterminating stochastic games. </title> <journal> Management Science, </journal> <volume> 12 </volume> <pages> 359-370, </pages> <year> 1966. </year>
Reference-contexts: Applied to mdps, it is equivalent to Howard's policy-iteration algorithm [18] and applied to alternating Markov games, it is equivalent to Hoffman and Karp's policy-iteration algorithm <ref> [17] </ref>. Policy iteration for mdps proceeds as follows: Choose an initial policy 0 and evaluate it. Let the next policy, 1 , be the greedy policy with respect to the value function V 0 . Continue in this way until t+1 = t .
Reference: [18] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: Section 3 describes a general theorem that can be used to prove the convergence of several reinforcement-learning algorithms in the generalized mdp framework. 2 SOLVING GENERALIZED MDPS VIA A MODEL The most basic algorithms for solving mdps are value iteration [3] and policy iteration <ref> [18] </ref>; both date back the late 1950s. <p> This requires that fl, P and R are expressed with a polynomial number of bits. 2.3 POLICY ITERATION In this section, we define a generalized version of policy iteration. Applied to mdps, it is equivalent to Howard's policy-iteration algorithm <ref> [18] </ref> and applied to alternating Markov games, it is equivalent to Hoffman and Karp's policy-iteration algorithm [17]. Policy iteration for mdps proceeds as follows: Choose an initial policy 0 and evaluate it. <p> Let the next policy, 1 , be the greedy policy with respect to the value function V 0 . Continue in this way until t+1 = t . The traditional proof of convergence relies on the following facts <ref> [18] </ref>: 6 The most general of these arguments is due to Bertsekas and Shreve [6] (Proposition 4.5) for extremization problems (although, the authors do not exploit this property). They also consider value iteration when the precision of computation is limited.
Reference: [19] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1185-1201, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Both model-free (direct) methods, such as Q-learning [59, 60], and model-based (indirect) methods, such as prioritized sweeping [29] and DYNA [46], have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref> [60, 53, 19, 13] </ref>. <p> Q-learning converges to the optimal Q function under the proper conditions <ref> [60, 53, 19] </ref>. 1.2 ALTERNATING MARKOV GAMES In alternating Markov games, two players take turns issuing actions to try to maximize their own expected discounted total reward. We now describe this model to show how closely it parallels mdps. <p> Even more, we do not have an a priori estimate of the convergence rate of t to zero, which would enable a traditional treatment. However, the idea of homogeneous perturbed processes <ref> [19] </ref> can be used to show that the effect of this perturbation can be neglected. To use Inequality (4) to show that ffi t (x) goes to zero with probability one, we use some auxiliary results proven in Appendix E. q.e.d. <p> For infinite models they have derived some very general results 18 that are too general to be useful in practice. Jaakkola, Jordan, and Singh <ref> [19] </ref> and Tsitsiklis [53] developed the connection between stochastic-approximation theory and reinforcement learning in mdps. Our work is similar in spirit to that of Jaakkola, et al. <p> Lemma 3 also provides a bound on the convergence rate of the algorithm; it is no slower than value iteration, but perhaps faster. E REDUCTION OF SOME PARALLEL ITERATIVE PROCEDURES TO SIMPLER ONES Jaakkola et al. <ref> [19] </ref> proved (Lemma 2) that if ffi t+1 (x) G t (x)ffi t (x) + F t (x)kffi t k (22) and F t fl (1 G t ) for some 0 fl &lt; 1 and lim n!1 Q n t=k G t = 0 with probability one uniformly over X <p> Before proving Lemma 12 we prove some additional statements that are required for the proof. The proof of Lemma 12 follows along the same lines as the proof of Lemma 2 of Jaakkola et al. <ref> [19] </ref>. First, we prove a simplified version of Lemma 12 then a rather technical lemma follows. (It may be considered as an extension to Jaakkola et al.'s Lemma 1 [19].) This lemma is about the convergence of homogeneous processes. <p> The proof of Lemma 12 follows along the same lines as the proof of Lemma 2 of Jaakkola et al. <ref> [19] </ref>. First, we prove a simplified version of Lemma 12 then a rather technical lemma follows. (It may be considered as an extension to Jaakkola et al.'s Lemma 1 [19].) This lemma is about the convergence of homogeneous processes. We will use this result to show that the perturbation caused by t can be neglected. Finally, the proof of Lemma 12 follows. <p> E.1 THE MAIN CONVERGENCE LEMMA Now, we prove our version of Jaakkola et al.'s <ref> [19] </ref> Lemma 2. Note that both our assumptions and our proof are slightly different from theirs.
Reference: [20] <author> George H. John. </author> <title> When the best move isn't optimal: Q-learning with exploration. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> page 1464, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: In this paper, we introduce a generalized Markov decision process model with applications to reinforcement learning, and list some of the important results concerning the model. 2 Generalized mdps provide a foundation for decision making in mdps and games, as well as in risk-sensitive models [15], exploration-sensitive models <ref> [20, 36] </ref>, simultaneous-action games [39], and other models. The common feature of these decision problems is that the reward function is based on the total, discounted cost|this latter property enables us to apply arguments based on the properties of contraction mappings, which makes the analysis tractable. <p> Markov games [7] max a or min b f (x; b) P risk-sensitive mdps [15] max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) expl.-sens. mdps <ref> [20] </ref> max 2P 0 P P Markov games [24] max A min b P P Table 1: Some models and their specification as generalized Markov decision processes. the norm on the appropriate spaces 3 . <p> In particular, an agent following the optimal policy will not visit every state and take every action infinitely often, and this is necessary to assure that an optimal policy is learned. John <ref> [20, 21] </ref> devised an approach to this problem based on the idea that any learning agent must continue to explore forever. Such an agent should still seek out actions that result in high expected discounted total reward, but not to the exclusion of taking exploratory actions.
Reference: [21] <author> George H. John. </author> <title> When the best move isn't optimal: Q-learning with exploration. </title> <type> Unpublished manuscript, </type> <note> available through URL ftp://starry.stanford.edu/pub/gjohn/papers/rein-nips.ps, </note> <year> 1995. </year>
Reference-contexts: In particular, an agent following the optimal policy will not visit every state and take every action infinitely often, and this is necessary to assure that an optimal policy is learned. John <ref> [20, 21] </ref> devised an approach to this problem based on the idea that any learning agent must continue to explore forever. Such an agent should still seek out actions that result in high expected discounted total reward, but not to the exclusion of taking exploratory actions. <p> It can be viewed as an action-sampled version of John's update rule. This rule has also been studied by John <ref> [21] </ref>, and under the name "SARSA" by Sutton [47] and Singh and Sutton [41].
Reference: [22] <author> R. E. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 189-211, </pages> <year> 1990. </year>
Reference-contexts: Like Q-learning, this learning algorithm is a general ization of Korf's <ref> [22] </ref> LRTA* algorithm for stochastic environments.
Reference: [23] <author> P.R. Kumar. </author> <title> A survey of some results in stochastic adaptive controls. </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 23 </volume> <pages> 329-380, </pages> <year> 1985. </year> <month> 51 </month>
Reference-contexts: Let M = sup x max a jR (x; a)j = kRk. If the agent received a reward of M 4 Traditionally, it was the field of adaptive control that considered such "learning" problems <ref> [23] </ref>. Adaptive-control researchers, however, usually considered linear models only, i.e., when the evolution equation of the controlled object is linear.
Reference: [24] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learn-ing. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Markov games [7] max a or min b f (x; b) P risk-sensitive mdps [15] max a f (x; a) min y:P (x;a;y)&gt;0 g (x; a; y) expl.-sens. mdps [20] max 2P 0 P P Markov games <ref> [24] </ref> max A min b P P Table 1: Some models and their specification as generalized Markov decision processes. the norm on the appropriate spaces 3 . <p> Note that both L N defined this way are non-expansions and monotonic (see Appendices B and C). The Q-learning update rule for Markov games <ref> [24] </ref> given step t experience hx t ; a t ; b t ; y t ; r t i has the form Q t+1 (x t ; (a t ; b t )) := (1 ff t (x t ; (a t ; b t )))Q t (x t ;
Reference: [25] <author> Michael L. Littman. </author> <title> Combining exploration and contol in reinforcement learning: The convergence of SARSA. </title> <type> Unpublished manuscript. </type> <note> Available through URL http://www.cs.duke.edu/~mlittman, 1996. </note>
Reference-contexts: The conclusion is that John's learning rule converges to the optimal Q function for this type of exploration-sensitive mdp. These results are discussed in a forthcoming technical note <ref> [25] </ref>. This update rule was also described by Rummery [35] in the context of variations of the TD () rule. <p> This rule has also been studied by John [21], and under the name "SARSA" by Sutton [47] and Singh and Sutton [41]. Once again, it is possible to apply Theorem 3.1 to show that Q t converges to Q fl as defined in Equation (10) <ref> [25] </ref>. (In Section 4.7 we describe a related algorithm in which N is estimated by computing randomized maximums.) 24 4.6 MODEL-BASED LEARNING METHODS The defining assumption in reinforcement learning is that the reward and transition functions, R and P , are not known in advance.
Reference: [26] <author> Michael Lederman Littman. </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <month> February </month> <year> 1996. </year> <note> Also Technical Report CS-96-09. </note>
Reference-contexts: Value iteration and also policy iteration can be used to solve alternating Markov games in pseudo-polynomial time. For further information on these topics the interested reader is referred to the PhD Thesis of Michael Littman <ref> [26] </ref>. 5 As a first step towards a general model, we will express the Bellman equations for mdps and alternating Markov games in a unified notation. For succinctness, we will use an operator-based notation in which addition and scalar multiplication are generalized in a natural way. <p> An analogous condition defines when L is a non-expansion. Many natural operators are non-expansions, such as max, min, midpoint, median, mean, and fixed weighted averages of these operations (see Appendix B). Mode and Boltzmann-weighted averages are not non-expansions (see Littman's thesis <ref> [26] </ref> for information on Boltzmann-weighted averages). Several previously described sequential decision-making models are special cases of this generalized mdp model|Table 1 gives a brief sampling. For more information about the specific models listed, see the associated references. <p> A further refinement, which relies on the contraction property of the dynamic-programming operator, puts a pseudo-polynomial bound on the number of iterations required to find an optimal policy <ref> [26] </ref>. This requires that fl, P and R are expressed with a polynomial number of bits. 2.3 POLICY ITERATION In this section, we define a generalized version of policy iteration. <p> The class of conservative non-expansions is quite broad. It is tempting to think that any operator that satisfies Condition (20) will be a non-expansion. Boltzmann averaging is an example where this is not the case <ref> [26] </ref>. It is also easy to construct summary operators that are non-expansions, but not conservative: J 38 C POLICY ITERATION AND MAXIMIZING MOD- ELS Appendix B describes a collection of important non-expansion operators based on element selection, ordering, convex combinations, and composition.
Reference: [27] <author> Sridhar Mahadevan. </author> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> 22(1/2/3):159-196, 1996. 
Reference-contexts: The results in this paper primarily concern reinforcement-learning in contractive models (fl &lt; 1), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion <ref> [38, 27] </ref>. Extending Theorem 3.1 to all-policies-proper mdps should not be too difficult. Actor-critic systems and asynchronous policy iteration would also worth the study. It would be interesting to develop a TD () algorithm [45] for generalized mdps; this has already been done for mdps [30] and exploration-sensitive mdps [35].
Reference: [28] <author> Matthew A. F. McDonald and Philip Hingston. </author> <title> Approximate discounted dynamic programming is unreliable. </title> <type> Technical report 94/7, </type> <institution> Department of Computer Science, The University of Western Australia, </institution> <address> Crawkey, WA, 6009, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: First, kV V k kV T V k+kT V V k = kV T V k+kT V T V k *+flkV V k. Grouping like terms gives kV V k *=(1 fl). 5 McDonald and Hingston <ref> [28] </ref> pointed out that optimal values can be exponentially small in the number of states for special classes of mdps. 10 Similarly, kV V fl k kV T V k+kT V V fl k = kV T V k+kT V T V fl k *+flkV V fl k.
Reference: [29] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Both model-free (direct) methods, such as Q-learning [59, 60], and model-based (indirect) methods, such as prioritized sweeping <ref> [29] </ref> and DYNA [46], have been explored and many have been shown to converge to optimal value functions under the proper conditions [60, 53, 19, 13]. <p> Although Q-learning shows that optimal value functions can be estimated without ever explicitly learning R and P , learning R and P makes more efficient use of experience at the expense of additional storage and computation <ref> [29] </ref>. The parameters of R and P can be learned from experience by keeping statistics for each state-action pair on the expected reward and the proportion of transitions to each next state.
Reference: [30] <author> Jing Peng and Ronald J. Williams. </author> <title> Incremental multi-step Q-learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 226-232, </pages> <address> San Francisco, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Extending Theorem 3.1 to all-policies-proper mdps should not be too difficult. Actor-critic systems and asynchronous policy iteration would also worth the study. It would be interesting to develop a TD () algorithm [45] for generalized mdps; this has already been done for mdps <ref> [30] </ref> and exploration-sensitive mdps [35]. Theorem 3.1 is not restricted to finite state spaces, and it might be valuable to prove the convergence of a finite reinforcement-learning algorithm for an infinite state-space model.
Reference: [31] <author> Martin L. Puterman. </author> <title> Markov Decision Processes|Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: A discount parameter 0 fl &lt; 1 controls the degree to which future rewards are significant compared to immediate rewards. The theory of Markov decision processes can be used as a theoretical foundation for important results concerning this decision-making problem [2]. A (finite) Markov decision process (mdp) <ref> [31] </ref> is defined by the tuple hX; A; P; Ri, where X represents a finite set of states, A a finite set of actions, P a transition function, and R a reward function. <p> These results are well established; proofs of the unattributed claims can be found in Puterman's mdp book <ref> [31] </ref>. The ultimate target of a decision-making algorithm is to find an optimal policy. A policy is some function that tells the agent which actions should be chosen under which circumstances. <p> The first is that, for maximizing generalized mdps, V fl (x) = max V ! (x); meaning that the optimal value function dominates or equals the value functions for all possible values of !. The second is a generalization of a result of Puterman <ref> [31] </ref> that shows that the iterates of policy iteration are bounded below by the iterates of value iteration. &gt;From these two facts, we can conclude that policy iteration converges to the optimal value function, and furthermore, that its convergence is at least as fast as the convergence of value iteration. <p> Theorem 3.1 is not restricted to finite state spaces, and it might be valuable to prove the convergence of a finite reinforcement-learning algorithm for an infinite state-space model. A proof of convergence for modified policy iteration <ref> [31] </ref> in generalized mdps should not be difficult. 18 Here is an example of their statements translated into our framework. <p> The results in this section apply to value functions defined by Bellman equations; to relate the Bellman equations to a notion of optimality, it is necessary to put forth arguments such as are given in Puterman's book <ref> [31] </ref>.
Reference: [32] <author> Carlos Ribeiro and Csaba Szepesvari. </author> <title> Spatial spreading of action values in Q-learning. </title> <booktitle> In Proceedings of ISRF-IEE International Conference: Intelligent and Cognitive Systems, Neural Networks Symposium, </booktitle> <pages> pages 32-36, </pages> <year> 1996. </year>
Reference-contexts: Theorem 3.1 therefore implies that this generalized Q-learning algorithm converges to the optimal Q function with probability one uniformly over X fi A. The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for a "spreading" learning rule <ref> [32] </ref> are given in Appendix 4.8. 11 Here, denotes the characteristic function. A common choice for learning rates is ff t (x; a) = 1=(1 + n t (x; a)), where n t (x; a) is the number of times (x; a) has been visited before t. <p> Szepesvari studied the above process when s (z; a; x) is replaced by a time dependent function which is also a function of the actual action, that is, the spreading coefficient of (z; a) at time t is given by s t (z; a; x t ; a t ) <ref> [32] </ref>.
Reference: [33] <author> C.H.C. Ribeiro. </author> <title> Attentional mechanisms as a strategy for generalisation in the Q-learning algorithm. </title> <booktitle> In Proceedings of ICANN'95, </booktitle> <volume> volume 1, </volume> <pages> pages 455-460, </pages> <year> 1995. </year>
Reference-contexts: Whether V fl is a good approximation of the true value function depends on the sampling method used and the degree to which suboptimal action choices in the underlying mdp result in near optimal values. 4.8 Q-LEARNING WITH SPREADING Ribeiro <ref> [33] </ref> argued that the use of available information in Q-learning is inefficient: in each step it is only the actual state and action whose Q-value is reestimated. The training process is local both in space and time.
Reference: [34] <author> Herbert Robbins and Sutton Monro. </author> <title> A stochastic approximation method. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference-contexts: For example, the theorem makes the convergence of Q-learning a consequence of the classical Robbins-Monro theorem <ref> [34] </ref>. In many problems we do not have full access to the operator L or the immediate rewards R 10 . <p> ; a t ; y t ), * the learning rates are decayed so that P 1 P 1 t=1 (x t = x; a t = a)ff t (x; a) 2 &lt; 1 uniformly with probability one 11 , then a standard result from the theory of stochastic approximation <ref> [34] </ref> states that T t approximates K at Q fl with probability one. That is, this method of using a decayed, exponentially weighted average correctly computes the average one-step reward.
Reference: [35] <author> G. A. Rummery. </author> <title> Problem solving with reinforcement learning. </title> <type> PhD thesis, </type> <institution> Cambridge University Engineering Department, </institution> <year> 1994. </year>
Reference-contexts: The conclusion is that John's learning rule converges to the optimal Q function for this type of exploration-sensitive mdp. These results are discussed in a forthcoming technical note [25]. This update rule was also described by Rummery <ref> [35] </ref> in the context of variations of the TD () rule. <p> Extending Theorem 3.1 to all-policies-proper mdps should not be too difficult. Actor-critic systems and asynchronous policy iteration would also worth the study. It would be interesting to develop a TD () algorithm [45] for generalized mdps; this has already been done for mdps [30] and exploration-sensitive mdps <ref> [35] </ref>. Theorem 3.1 is not restricted to finite state spaces, and it might be valuable to prove the convergence of a finite reinforcement-learning algorithm for an infinite state-space model.
Reference: [36] <author> G. A. Rummery and M. Niranjan. </author> <title> On-line Q-learning using connectionist systems. </title> <type> Technical Report CUED/F-INFENG/TR 166, </type> <institution> Cambridge University Engineering Department, </institution> <year> 1994. </year> <month> 52 </month>
Reference-contexts: In this paper, we introduce a generalized Markov decision process model with applications to reinforcement learning, and list some of the important results concerning the model. 2 Generalized mdps provide a foundation for decision making in mdps and games, as well as in risk-sensitive models [15], exploration-sensitive models <ref> [20, 36] </ref>, simultaneous-action games [39], and other models. The common feature of these decision problems is that the reward function is based on the total, discounted cost|this latter property enables us to apply arguments based on the properties of contraction mappings, which makes the analysis tractable.
Reference: [37] <author> Nicol N. Schraudolph, Peter Dayan, and Terrence J. Sejnowski. </author> <title> Temporal difference learning of position evaluation in the game of Go. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 817-824, </pages> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A great deal of reinforcement-learning research has been directed to solving games of this kind <ref> [51, 52, 37, 7] </ref>, Algorithms for solving mdps and their convergence proofs do not apply directly to these problems.
Reference: [38] <author> Anton Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 298-305, </pages> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The results in this paper primarily concern reinforcement-learning in contractive models (fl &lt; 1), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion <ref> [38, 27] </ref>. Extending Theorem 3.1 to all-policies-proper mdps should not be too difficult. Actor-critic systems and asynchronous policy iteration would also worth the study. It would be interesting to develop a TD () algorithm [45] for generalized mdps; this has already been done for mdps [30] and exploration-sensitive mdps [35].
Reference: [39] <author> L.S. Shapley. </author> <title> Stochastic games. </title> <booktitle> Proceedings of the National Academy of Sciences of the United States of America, </booktitle> <volume> 39 </volume> <pages> 1095-1100, </pages> <year> 1953. </year>
Reference-contexts: When 0 fl &lt; 1, these equations have a unique solution and can be solved by successive-approximation methods <ref> [39] </ref>. In addition, we show in Section 4.1 that the natural extension of several reinforcement-learning algorithms for solving mdps converge to optimal value functions in two-player games. <p> paper, we introduce a generalized Markov decision process model with applications to reinforcement learning, and list some of the important results concerning the model. 2 Generalized mdps provide a foundation for decision making in mdps and games, as well as in risk-sensitive models [15], exploration-sensitive models [20, 36], simultaneous-action games <ref> [39] </ref>, and other models. The common feature of these decision problems is that the reward function is based on the total, discounted cost|this latter property enables us to apply arguments based on the properties of contraction mappings, which makes the analysis tractable. <p> This section describes how these algorithms can be applied to solve generalized Markov decision processes. 2.1 VALUE ITERATION The method of value iteration, or successive approximations <ref> [3, 39] </ref>, is a way of iteratively computing arbitrarily good approximations to the optimal value function V fl . A single step of the process starts with an estimate V t1 of the optimal value function, and produces a better estimate V t = T V t1 . <p> More results on this can be found in Appendix F. 20 4.2 Q-LEARNING FOR MARKOV GAMES Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step. The basic model was developed by Shap-ley <ref> [39] </ref> and is defined by the tuple hX; A; B; P; Ri and discount factor fl.
Reference: [40] <author> Satinder P. Singh and Vijaykumar Gullapalli. </author> <title> Asynchronous modified policy iteration with single-sided updates. </title> <type> Unpublished manuscript. </type> <note> Available through URL ftp://ftp.cs.colorado.edu/users/baveja/Papers/single-sided.ps.Z, </note> <year> 1993. </year>
Reference-contexts: With a little change, the above framework is also capable of expressing asynchronous policy-iteration algorithms. Most of the previous results on asynchronous policy iteration can be repeated since those proofs depend only on the monotonicity and the contraction properties of the involved operators <ref> [61, 40] </ref>. The work of Bertsekas and Shreve [6] is also worth mentioning here: they have considered a version of policy iteration in which both myopic policies and the evaluation of these policies are determined with a precision geometrically increasing in time.
Reference: [41] <author> Satinder P. Singh and Richard S. Sutton. </author> <title> Reinforcement learning with replacing eligibility traces. </title> <journal> Machine Learning, </journal> 22(1/2/3):123-158, 1996. 
Reference-contexts: It can be viewed as an action-sampled version of John's update rule. This rule has also been studied by John [21], and under the name "SARSA" by Sutton [47] and Singh and Sutton <ref> [41] </ref>.
Reference: [42] <author> Satinder Pal Singh and Richard C. Yee. </author> <title> An upper bound on the loss from approximate optimal-value functions. </title> <booktitle> Machine Learning, </booktitle> <address> 16:227, </address> <year> 1994. </year>
Reference-contexts: Grouping like terms gives kV V fl k *=(1 fl). q.e.d. We next bound the distance between V and V fl in terms of *, the Bellman error magnitude (related arguments have been made before <ref> [6, 42, 62, 16] </ref> 6 ). Theorem 2 Let V be a value function, V be the value function for the myopic policy with respect to V , and V fl be the optimal value function.
Reference: [43] <author> S.P. Singh, T. Jaakkola, and M.I. Jordan. </author> <title> Reinforcement learning with soft state aggregation. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 361-368, </pages> <address> Cambridge, MA, 1995. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: This approach is extended by Singh et al. <ref> [43] </ref>, where the authors consider learning Q-values for "softly aggregated" states, i.e., for any given aggregated state s there is a probability distribution over the states which determines to which extent a given state from X belongs to s (this can also be viewed as fuzzy sets over the state space
Reference: [44] <author> D.R. </author> <title> Smart. Fixed point theorems. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1974. </year>
Reference-contexts: To see this, let fl be an optimal policy. Then V fl is the fixed point of T because V fl V fl . Thus, V fl = V fl when fl &lt; 1, because T has a unique fixed point by the Banach fixed-point theorem <ref> [44] </ref>. All the statements of this section and some other basic facts about generalized mdps are proved in Appendices A through D. 1.4 SOLVING GENERALIZED MDPS The previous subsections have motivated and described our generalization of Markov decision processes. <p> Proof: &gt;From Lemma 5, the T and K operators for the generalized mdp are contraction mappings with respect to the max norm. The existence and uniqueness of V fl and Q fl follow directly from the Banach fixed-point theorem <ref> [44] </ref>. We can define the optimal value function and the optimal Q function in terms of each other: V fl = Q fl ; (19) L (R + flV fl ).
Reference: [45] <author> Richard S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: k k denotes 2 The definitions of L N L P N where S : (X fi A fi Y ) ! &lt; and Q : (X fi A) ! &lt;. 6 model/example reference ( N L disc. exp. mdps [60] max a f (x; a) P exp. return of <ref> [45] </ref> P P alt. <p> Extending Theorem 3.1 to all-policies-proper mdps should not be too difficult. Actor-critic systems and asynchronous policy iteration would also worth the study. It would be interesting to develop a TD () algorithm <ref> [45] </ref> for generalized mdps; this has already been done for mdps [30] and exploration-sensitive mdps [35]. Theorem 3.1 is not restricted to finite state spaces, and it might be valuable to prove the convergence of a finite reinforcement-learning algorithm for an infinite state-space model.
Reference: [46] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> Austin, TX, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Both model-free (direct) methods, such as Q-learning [59, 60], and model-based (indirect) methods, such as prioritized sweeping [29] and DYNA <ref> [46] </ref>, have been explored and many have been shown to converge to optimal value functions under the proper conditions [60, 53, 19, 13].
Reference: [47] <author> Richard S. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <address> Cambridge, MA, 1996. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: It can be viewed as an action-sampled version of John's update rule. This rule has also been studied by John [21], and under the name "SARSA" by Sutton <ref> [47] </ref> and Singh and Sutton [41].
Reference: [48] <author> Cs. Szepesvari. </author> <title> Abstract dynamic programming under monotonicity assumptions: Non-Markovian policies, policy iteration and the optimality equation. </title> <type> Technical Report 96-103, </type> <institution> Research Group on Artificial Intelligence, JATE-MTA, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: This is in no way related to the contraction property of the value iteration operator, i.e., it holds for arbitrary monotone and continuous operators <ref> [48] </ref>. This means that value iteration can be used to find optimal value functions in finite time for generalized mdps in this subclass. <p> This result can also be proved for continuous and monotone value-iteration operators of maximizing type without assuming the contraction property <ref> [48] </ref>. Theorem 3 Let V fl (x) = max O M V ! (x) = (R + flV ! ) (x) where N and are non-expansions and monotonic and R is compact. <p> This section provides some concluding thoughts. 5.1 RELATED WORK The work presented here is closely related to several previous research efforts. Szepesvari <ref> [50, 48] </ref> described a generalized reinforcement-learning model that is both more and less general than the present model. His model enables more general value propagation than L with fl &lt; 1 but is restricted to maximization problems, i.e., when N = max. <p> The problem is that, in practice, it is usually clear that V fl n = T n V , but it is much harder to show that V fl n converges to V fl <ref> [5, 48] </ref>. 32 Another possible direction for future research is to apply to modern ODE (ordinary dif-ferential equation) theory of stochastic approximations.
Reference: [49] <author> Cs. Szepesvari. </author> <title> Synthesis of neural networks: the case of cascaded Hebbians. Technical Report TR-96-102, </title> <booktitle> Research Group on Artificial Intelligence, </booktitle> <address> JATE-MTA, </address> <month> August </month> <year> 1996. </year> <month> 53 </month>
Reference-contexts: Now, Equation (25) is replaced by y t+1 (z) = g t (z)y t (z) + fl (1 g t (z))(C 1 + * t ): Now, y t still converges to flC 1 by Lemma 3.5 of Szepesvari <ref> [49] </ref> and also 0 ^x t y t . Thus, the whole argument of Lemma 10 can be repeated for the process ^x t , and we get that k^x t k converges to zero with probability one and consequently so does kx t k. q.e.d.
Reference: [50] <author> Csaba Szepesvari. </author> <title> A general framework for reinforcement learning. </title> <booktitle> In Proceedings of ICANN'95, </booktitle> <volume> volume 2, </volume> <pages> pages 165-170, </pages> <year> 1995. </year>
Reference-contexts: This section provides some concluding thoughts. 5.1 RELATED WORK The work presented here is closely related to several previous research efforts. Szepesvari <ref> [50, 48] </ref> described a generalized reinforcement-learning model that is both more and less general than the present model. His model enables more general value propagation than L with fl &lt; 1 but is restricted to maximization problems, i.e., when N = max.
Reference: [51] <author> Gerald Tesauro. </author> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 58-67, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: A great deal of reinforcement-learning research has been directed to solving games of this kind <ref> [51, 52, 37, 7] </ref>, Algorithms for solving mdps and their convergence proofs do not apply directly to these problems.
Reference: [52] <author> Sebastian Thrun. </author> <title> Learning to play the game of chess. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 1069-1076, </pages> <address> Cambridge, MA, 1995. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: A great deal of reinforcement-learning research has been directed to solving games of this kind <ref> [51, 52, 37, 7] </ref>, Algorithms for solving mdps and their convergence proofs do not apply directly to these problems.
Reference: [53] <author> John N. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3) </volume> <pages> 185-202, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Both model-free (direct) methods, such as Q-learning [59, 60], and model-based (indirect) methods, such as prioritized sweeping [29] and DYNA [46], have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref> [60, 53, 19, 13] </ref>. <p> Q-learning converges to the optimal Q function under the proper conditions <ref> [60, 53, 19] </ref>. 1.2 ALTERNATING MARKOV GAMES In alternating Markov games, two players take turns issuing actions to try to maximize their own expected discounted total reward. We now describe this model to show how closely it parallels mdps. <p> For infinite models they have derived some very general results 18 that are too general to be useful in practice. Jaakkola, Jordan, and Singh [19] and Tsitsiklis <ref> [53] </ref> developed the connection between stochastic-approximation theory and reinforcement learning in mdps. Our work is similar in spirit to that of Jaakkola, et al.
Reference: [54] <author> John N. Tsitsiklis and Benjamin Van Roy. </author> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> March </month> <year> 1996. </year> <note> Available through URL http://web.mit.edu/bvr/www/td.ps. To appear in IEEE Transactions on Automatic Control. </note>
Reference-contexts: Because all the results in this chapter are stated in terms of norms, they apply to any update rule as long as the dynamic-programming operator under consideration is a contraction mapping. (See recent work by Tsitsiklis and van Roy <ref> [54] </ref> for the use of another important and interesting norm for reinforcement learning.) The fact that the optimal value functions are well defined does not imply that they are meaningful; that is, it may be the case that the optimal value function is not the same as the value function for
Reference: [55] <author> John N. Tsitsiklis and Benjamin Van Roy. </author> <title> Feature-based methods for large scale dynamic programming. </title> <journal> Machine Learning, </journal> 22(1/2/3):59-94, 1996. 
Reference-contexts: The most sensible way of dealing with this difficulty is to generate compact parametric representations that approximate the Q function. One form of compact representation, as described by Tsitsiklis and Van Roy <ref> [55] </ref>, is based on the use of feature extraction to map the set of states into a much smaller set of feature vectors.
Reference: [56] <author> Sergio Verdu and H. Vincent Poor. </author> <title> Abstract dynamic programming models under commutativity conditions. </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 25(4) </volume> <pages> 990-1006, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Heger [15, 16] extended many of the standard mdp results to cover the risk-sensitive model. Although his work derives many of the important theorems, it does not present these theorems in a generalized way to allow them to be applied to any other models. Verdu and Poor <ref> [56] </ref> introduced a class of abstract dynamic-programming models that is far more comprehensive than the model discussed here. <p> Now, the statement which concerns infinite-horizon models goes like this: if V fl n converges to V fl (their Condition 3 <ref> [56] </ref>) then T n V converges to V fl .
Reference: [57] <author> O. J. Vrieze and S. H. Tijs. </author> <title> Fictitious play applied to sequences of games and discounted stochastic games. </title> <journal> International Journal of Game Theory, </journal> <volume> 11(2) </volume> <pages> 71-85, </pages> <year> 1982. </year>
Reference-contexts: In general, it is necessary to solve a linear program to compute the update given above. We hypothesize that Theorem 3.1 can be combined with the results of Vrieze and Tijs <ref> [57] </ref> on solving Markov games by "fictitious play" to prove the convergence of a linear-programming-free version of Q-learning for Markov games. 21 4.3 CONVERGENCE UNDER ERGODIC SAMPLING In most of the sequential decision problems that arise in practice, the state space is huge.
Reference: [58] <author> K.-H. Waldmann. </author> <title> On bounds for dynamic programs. </title> <journal> Mathematics of Operations Research, </journal> <volume> 10(2) </volume> <pages> 220-232, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: His model can be viewed as the continuation of the work of Bertsekas [5] and Bertsekas and Shreve [6], who proved similar statements under different assumptions. Waldmann <ref> [58] </ref> developed a highly general model of dynamic-programming problems, with a focus on deriving approximation bounds. Heger [15, 16] extended many of the standard mdp results to cover the risk-sensitive model.
Reference: [59] <author> Christopher J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK, </address> <year> 1989. </year>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Both model-free (direct) methods, such as Q-learning <ref> [59, 60] </ref>, and model-based (indirect) methods, such as prioritized sweeping [29] and DYNA [46], have been explored and many have been shown to converge to optimal value functions under the proper conditions [60, 53, 19, 13]. <p> The method of Q-learning <ref> [59] </ref> uses experience to estimate the optimal value function without ever explicitly approximating R and P .
Reference: [60] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Both model-free (direct) methods, such as Q-learning <ref> [59, 60] </ref>, and model-based (indirect) methods, such as prioritized sweeping [29] and DYNA [46], have been explored and many have been shown to converge to optimal value functions under the proper conditions [60, 53, 19, 13]. <p> Both model-free (direct) methods, such as Q-learning [59, 60], and model-based (indirect) methods, such as prioritized sweeping [29] and DYNA [46], have been explored and many have been shown to converge to optimal value functions under the proper conditions <ref> [60, 53, 19, 13] </ref>. <p> Q-learning converges to the optimal Q function under the proper conditions <ref> [60, 53, 19] </ref>. 1.2 ALTERNATING MARKOV GAMES In alternating Markov games, two players take turns issuing actions to try to maximize their own expected discounted total reward. We now describe this model to show how closely it parallels mdps. <p> 2 B 1 kT f T gk kf gk, where k k denotes 2 The definitions of L N L P N where S : (X fi A fi Y ) ! &lt; and Q : (X fi A) ! &lt;. 6 model/example reference ( N L disc. exp. mdps <ref> [60] </ref> max a f (x; a) P exp. return of [45] P P alt.
Reference: [61] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Analysis of some incremental variants of policy iteration: First steps toward understanding actor-critic learning systems. </title> <type> Technical Report NU-CCS-93-11, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: With a little change, the above framework is also capable of expressing asynchronous policy-iteration algorithms. Most of the previous results on asynchronous policy iteration can be repeated since those proofs depend only on the monotonicity and the contraction properties of the involved operators <ref> [61, 40] </ref>. The work of Bertsekas and Shreve [6] is also worth mentioning here: they have considered a version of policy iteration in which both myopic policies and the evaluation of these policies are determined with a precision geometrically increasing in time.
Reference: [62] <author> Ronald J. Williams and Leemon C. Baird, III. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> Northeastern University, College of Computer Science, </institution> <address> Boston, MA, </address> <month> November </month> <year> 1993. </year> <month> 54 </month>
Reference-contexts: Grouping like terms gives kV V fl k *=(1 fl). q.e.d. We next bound the distance between V and V fl in terms of *, the Bellman error magnitude (related arguments have been made before <ref> [6, 42, 62, 16] </ref> 6 ). Theorem 2 Let V be a value function, V be the value function for the myopic policy with respect to V , and V fl be the optimal value function. <p> They also consider value iteration when the precision of computation is limited. Williams and Baird <ref> [62] </ref> have proved these bounds tight for mdps, and this should hold for generalized mdps, as well. 11 (i) V t+1 V t , and the inequality is strict for at least one state if t is not optimal, (ii) there are a finite number of policies (since X and A
References-found: 62

