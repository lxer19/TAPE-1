URL: http://www.icsi.berkeley.edu/ftp/global/pub/speech/papers/snowbird95-spam+remap.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/real/spam.html
Root-URL: http://www.icsi.berkeley.edu
Email: Email:  bourlard@icsi.berkeley.edu  
Phone: Phone: (510)-643-9153 Fax: (510)-643-7684  
Title: TRANSITION-BASED STATISTICAL TRAINING FOR ASR  
Author: Nelson Morgan Yochai Konig Su-Lin Wu and Herve Bourlard 
Note: morgan, konig, sulin,  
Address: 1947 Center Street, Berkeley, CA 94704  
Affiliation: International Computer Science Institute,  (also Faculte Polytechnique de Mons, Belgium and University of California at Berkeley)  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bengio, Y. R., De Mori, R., Flammia, G., & Kompe, R. </author> <year> (1992). </year> <title> "Global optimization of a neural-hidden Markov model hybrid," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 252-258. </pages>
Reference-contexts: Will be smooth estimates of conditional transition probabilities, 8 possible (k; `) state transition pairs in M and 8n 2 <ref> [1; n] </ref>. 2.
Reference: [2] <author> Bourlard, H., Konig, Y., & Morgan, N., </author> <title> "REMAP: recursive estimation and maximization of a posteriori probabilities Application to transition-based connectionist speech recognition," </title> <type> ICSI Technical Report TR-94-064, Intl. </type> <institution> Computer Science Institute, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: When training the ANN for iteration t + 1, will lead to new estimates of fi t+1 and P (q ` n jx n ; q k that are guaranteed to incrementally increase (2)? In <ref> [2] </ref>, we prove that a re-estimate of ANN targets that guarantee convergence to a local maximum of (2) is given by: n jx n ; q k n jX; q k which means that the new ANN target associated with x n and a specific transition q k ! q ` <p> In <ref> [2] </ref>, we further prove that alternating ANN target estimation (the "estimation" step) and ANN training (the "maximization" step) is guaranteed to incrementally in crease (2) over t; we also provide efficient forward and backward-like recurrences to compute (3). 3.2. <p> DISCUSSION AND RESULTS Of course, a wide range of discriminant approaches (e.g., MMI, GPD see <ref> [2] </ref> for a discussion of these) to speech recognition have been studied by researchers. A significant difficulty that has remained in applying these approaches to continuous speech recognition has been the requirement to run computationally intensive algorithms on all of the rival sentences.
Reference: [3] <author> Furui, S., </author> <title> "On the role of spectral transition for speech perception," </title> <journal> J. Acoust. Soc. Am., </journal> <volume> vol. 80, no. 4, </volume> <pages> pp. 1016-1025, </pages> <year> 1986. </year>
Reference-contexts: 1. INTRODUCTION It is known that in human speech recognition, the perceptually-dominant and information-rich portions of the speech signal, which may also be the parts with a better chance to withstand adverse acoustical conditions, are the (phonetic) transitions (see, e.g., <ref> [3] </ref> for some experimental evidence). A first step in this direction was to use highpass or bandpass filtering of critical band trajectories (RASTA processing) to emphasize transitions [5]. <p> SPAM: STOCHASTIC PERCEPTUAL AUDITORY-EVENT-BASED MODELS Speech can be viewed as a sequence of Auditory Events (Avents), which are elementary decisions made in response to significant changes in spectral amplitudes (as in <ref> [3] </ref>). Avents are presumed to occur about once per phone boundary. The statistical model uses these Avents as fundamental building blocks for words and utterances, separated by states corresponding to the more stationary regions.
Reference: [4] <author> Ghitza, O. and Sondhi, </author> <title> M.M., "Hidden Markov models with templates as non-stationary states: an application to speech recognition," </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 2 </volume> <pages> 101-119, </pages> <year> 1993. </year>
Reference-contexts: This is a mismatch to the underlying speech model in standard HMMs, which has been designed to represent piecewise stationary signals. In general, modeling transitions or any non-stationary properties of speech signal require major modifications of standard HMMs <ref> [4] </ref>. Therefore, it is likely that transition-based systems will require a fundamentally different kind of underlying statistical model. We have been developing a statistical model (SPAM) and a statistical training algorithm (REMAP) that may be more appropriate to this perspective. 2.
Reference: [5] <author> Hermansky, H. and Morgan, N., </author> <title> "RASTA processing of speech", </title> <journal> IEEE Transactions on Speech and Audio Processing, special issue on Robust Speech Recognition, </journal> <volume> vol.2 no. 4, </volume> <pages> pp. 578-589, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: A first step in this direction was to use highpass or bandpass filtering of critical band trajectories (RASTA processing) to emphasize transitions <ref> [5] </ref>. While this is sometimes helpful in reducing errors due to (channel) mismatches between training and testing conditions, the resulting observation sequence is a representation that has emphasized the regions of strong change and de-emphasized temporal regions without significant spectral change.
Reference: [6] <author> Morgan, N., Wu, S.-L., & Bourlard, H., </author> <title> "Digit recognition with stochastic perceptual models," </title> <note> to be published in Proc. </note> <institution> Eurospeech'95 (Madrid, Spain), </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: This set is currently initialized to correspond to left context-dependent phonetic onsets. As discussed in <ref> [6] </ref>, one can do SPAM recognition based on the following local acoustic probabilities: P (q ` n (n) ; (n); X n+c 8k = 1; 2; : : : ; K (1) in which X n+d nc = fx nc ; : : : ; x n ; : : : <p> According to our SPAM constraints, these local probabilities are used for training and decoding in particular left-to-right HMMs constituted by sequences of Avent states (with no loop allowed) separated by (looped) tied non-Avent states. Preliminary experiments on isolated telephone digits (plus yes and no) are reported in <ref> [6] </ref>. In these experiments, in which we simplify (1) to the simple Avent posterior P (q ` nd ), it is shown that the reduced SPAM has about double the error rate of our best phone-based system.
References-found: 6

