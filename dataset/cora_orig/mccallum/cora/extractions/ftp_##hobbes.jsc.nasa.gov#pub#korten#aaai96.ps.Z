URL: ftp://hobbes.jsc.nasa.gov/pub/korten/aaai96.ps.Z
Refering-URL: http://tommy.jsc.nasa.gov/~bonasso/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: korten@mickey.jsc.nasa.gov  
Title: Recognizing and interpreting gestures on a mobile robot  
Author: David Kortenkamp, Eric Huber, and R. Peter Bonasso 
Address: ER2 Houston, TX 77058  
Affiliation: Metrica, Inc. Robotics and Automation Group NASA Johnson Space Center  
Abstract: Gesture recognition is an important skill for robots that work closely with humans. Gestures help to clarify spoken commands and are a compact means of relaying geometric information. We have developed a real-time, three-dimensional gesture recognition system that resides on-board a mobile robot. Using a coarse three-dimensional model of a human to guide stereo measurements of body parts, the system is capable of recognizing six distinct gestures made by an unadorned human in an unaltered environment. An active vision approach focuses the vision system's attention on small, moving areas of space to allow for frame rate processing even when the person and/or the robot are moving. This paper describes the gesture recognition system, including the coarse model and the active vision approach. This paper also describes how the gesture recognition system is integrated with an intelligent control architecture to allow for complex gesture interpretation and complex robot action. Results from experiments with an actual mobile robot are given. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bonasso, R. P.; Kortenkamp, D.; Miller, D. P.; and Slack, M. </author> <year> 1995. </year> <title> Experiences with an architecture for intelligent, reactive agents. </title> <booktitle> In Proceedings 1995 IJCAI Workshop on Agent Theories, Architectures, and Languages. </booktitle>
Reference-contexts: For the last several years we have been working on an intelligent control architecture, known as 3 T, which can integrate reactive vision and robot processes with more deliberative reasoning techniques to produce intelligent, reactive robot behavior <ref> (Bonasso et al. 1995) </ref>. The architecture consists of three layers of control: skills, sequencing and planning. Only the first two layers (skills and sequencing) have been used in the system described in this paper. <p> If this happens the robot's last recollection of a gesture will be "Halting." In these cases, the robot tells the designating agent that they need to start over, and continues as in the first method. These RAPs do not show the details of enabling actual skills, see <ref> (Bonasso et al. 1995) </ref> for details of how this works. Conclusions Our goal is to develop technologies that allow for effective human/robot teams in dynamic environments. The ability to use the human's natural communication tendencies allows the robot to be more effective and safer when working among humans.
Reference: <author> Borenstein, J., and Koren, Y. </author> <year> 1991. </year> <title> The Vector Field Histogram for fast obstacle-avoidance for mobile robots. </title> <journal> IEEE Journal of Robotics and Automation 7(3). </journal>
Reference-contexts: Figure 5 shows the skill network for our work in gesture recognition. The skills labeled vfh are obstacle avoidance and robot motion skills base on the Vector Field Histogram method <ref> (Borenstein & Koren 1991) </ref>. They take a goal location, generated from any skill, and move the robot to that goal location. The move-to-point, the track-agent and the recognize-gesture skills allow can provide goal locations to the vfh skills.
Reference: <author> Brooks, R. A. </author> <year> 1986. </year> <title> A Robust Layered Control System for a Mobile Robot. </title> <journal> IEEE Journal of Robotics and Automation 2(1). </journal>
Reference-contexts: Behaviors generate motion vectors by assessing the visual information within the proximity space. There are behaviors for following, clinging, pulling, migrating to a boundary and resiz ing (which does not generate a motion vector, but a size for the proximity space). Patterned after the sub-sumption architecture <ref> (Brooks 1986) </ref>, these behaviors compete for control of the proximity space. In dynamic terms, the proximity space acts as an inertial mass and the behaviors as forces acting to accelerate that mass (see (Huber & Kortenkamp 1995) for a more detailed description).
Reference: <author> Cassell, J. </author> <year> 1995. </year> <title> Speech, action and gestures as context for on-going task-oriented talk. </title> <booktitle> In Working Notes of the 1995 AAAI Fall Symposium on Embodied Language and Action. </booktitle>
Reference-contexts: Introduction In order to work effectively with humans, robots will need to track and recognize human gestures. Gestures are an integral part of communication. They provide clarity in situations where speech is ambiguous or noisy <ref> (Cassell 1995) </ref>. Gestures are also a compact means of relaying geometric information. For example, in robotics, gestures can tell the robot where to go, where to look and when to stop. We have implemented a real-time, three-dimensional gesture recognition system on a mobile robot.
Reference: <author> Darrell, T. J.; Maes, P.; Blumberg, B.; and Pentland, A. </author> <year> 1994. </year> <title> A novel environment for situated vision and behavior. In Workshop on Visual Behaviors: </title> <journal> Computer Vision and Pattern Recognition. </journal>
Reference-contexts: In this paper we present a gesture recognition system that meets these requirements. Related work Gesture recognition has become a very important research area in recent years and there are several mature implementations. The ALIVE system <ref> (Darrell et al. 1994) </ref> allows people to interact with virtual agents via gestures. The ALIVE system differs from ours in that the cameras are fixed (i.e., not on a mobile platform as ours are) and that it requires a known background.
Reference: <author> Firby, R. J. </author> <year> 1994. </year> <title> Task networks for controlling continuous processes. </title> <booktitle> In Proceedings of the Second International Conference on AI Planning Systems. </booktitle>
Reference-contexts: Wanting to exploit the skills described above in as many situations as possible, we have observed that in many tasks a human pointing gesture can have a wide range of interpretations depending on the task. The middle layer of our 3 T architecture is the RAPs system <ref> (Firby 1994) </ref>. A reactive action package (RAP) specifies how and when to carry out routine procedures through conditional sequencing. As such, a RAP provides a way to interpret gestures through context limiting procedures of action.
Reference: <author> Gavrila, D. M., and Davis, L. </author> <year> 1995. </year> <title> 3-D model-based tracking of human upper body movement: A multi-view approach. </title> <booktitle> In IEEE Symposium on Computer Vision. </booktitle>
Reference-contexts: The ALIVE system differs from ours in that the cameras are fixed (i.e., not on a mobile platform as ours are) and that it requires a known background. Similar restrictions hold for a system by Gavrila and Davis <ref> (Gavrila & Davis 1995) </ref>. The Perseus system (Kahn et al. 1996) uses a variety of techniques (e.g., motion, color, edge detection) to segment a person and their body parts. Based on this segmentation the Perseus system can detect pointing vectors.
Reference: <author> Huber, E., and Kortenkamp, D. </author> <year> 1995. </year> <title> Using stereo vision to pursue moving agents with a mobile robot. </title> <booktitle> In 1995 IEEE International Conference on Robotics and Automation. </booktitle>
Reference-contexts: Patterned after the sub-sumption architecture (Brooks 1986), these behaviors compete for control of the proximity space. In dynamic terms, the proximity space acts as an inertial mass and the behaviors as forces acting to accelerate that mass (see <ref> (Huber & Kortenkamp 1995) </ref> for a more detailed description). Chaining multiple proximity spaces using a human model In order to recognize gestures, multiple proximity spaces are spawned, which attach themselves to various body parts in the image of the gesturing person. <p> We have successfully tracked people for periods of twenty to thirty minutes in previous work <ref> (Huber & Kortenkamp 1995) </ref>. For this work we added gesture recognition and allowed the person to stop the robot by giving the "halting" gesture.
Reference: <author> Kahn, R. E.; Swain, M. J.; Prokopowicz, P. N.; and Firby, R. J. </author> <year> 1996. </year> <title> Gesture recognition using the perseus architecture. </title> <journal> Computer Vision and Pattern Recognition. </journal>
Reference-contexts: The ALIVE system differs from ours in that the cameras are fixed (i.e., not on a mobile platform as ours are) and that it requires a known background. Similar restrictions hold for a system by Gavrila and Davis (Gavrila & Davis 1995). The Perseus system <ref> (Kahn et al. 1996) </ref> uses a variety of techniques (e.g., motion, color, edge detection) to segment a person and their body parts. Based on this segmentation the Perseus system can detect pointing vectors.
Reference: <author> Nishihara, H. </author> <year> 1984. </year> <title> Practical real-time imaging stereo matcher. Optical Engineering 23(5). </title>
Reference-contexts: Recognizing gestures Our gesture recognition system consists of a stereo pair of black and white cameras mounted on a pan/tilt/verge head that is, in turn, mounted on the mobile robot (see Figure 1). The basis of our stereo vision work is the PRISM-3 system developed by Keith Nishihara <ref> (Nishihara 1984) </ref>. The PRISM-3 system provides us with low-level spatial and temporal disparity measurements. We use these measurements as input to our algorithms for gesture recognition. Our gesture recognition process has two components. First, we concentrate our vision system's attention on small regions of 3-D visual space.
Reference: <author> Wilson, A., and Bobick, A. </author> <year> 1995. </year> <title> Configuration states for the representation and recognition of gesture. </title> <booktitle> In International Workshop on Automatic Face and Gesture Recognition. </booktitle>
Reference-contexts: The Perseus system differs from ours in that it requires a static background, doesn't detect gestures in three dimensions and relies on off-board computation, which can cause delays in recognition of gestures. Wilson and Bobick <ref> (Wilson & Bobick 1995) </ref> use a hidden Markov model to learn repeatable patterns of human gestures. Their system differs from ours in that it requires people to maintain strict constraints on their orientation with respect to the cameras.
References-found: 11

