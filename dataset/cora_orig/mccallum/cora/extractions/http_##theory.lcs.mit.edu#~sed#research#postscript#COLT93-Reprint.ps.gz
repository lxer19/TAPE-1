URL: http://theory.lcs.mit.edu/~sed/research/postscript/COLT93-Reprint.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sed/research/COLT93-abs.html
Root-URL: 
Email: sed@das.harvard.edu  
Title: Statistical Queries and Faulty PAC Oracles  
Author: Scott Evan Decatur 
Date: July 1993.  
Note: Research supported by an NDSEG Fellowship and by NSF grant CCR-89-02500. Appeared in the Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory,  
Address: Cambridge, MA 02138  
Affiliation: Aiken Computation Laboratory Harvard University  
Abstract: In this paper we study learning in the PAC model of Valiant [18] in which the example oracle used for learning may be faulty in one of two ways: either by misclassifying the example or by distorting the distribution of examples. We first consider models in which examples are misclassified. Kearns [12] recently showed that efficient learning in a new model using statistical queries is a sufficient condition for PAC learning with classification noise. We show that efficient learning with statistical queries is sufficient for learning in the PAC model with malicious error rate proportional to the required statistical query accuracy. One application of this result is a new lower bound for tolerable malicious error in learning monomials of k literals. This is the first such bound which is independent of the number of irrelevant attributes n. We also use the statistical query model to give sufficient conditions for using distribution specific algorithms on distributions outside their prescribed domains. A corollary of this result expands the class of distributions on which we can weakly learn monotone Boolean formulae. We also consider new models of learning in which examples are not chosen according to the distribution on which the learner will be tested. We examine three variations of distribution noise and give necessary and sufficient conditions for polynomial time learning with such noise. We show containments and separations between the various models of faulty oracles. Finally, we examine hypothesis boosting algorithms in the context of learning with distribution noise, and show that Schapire's result regarding the strength of weak learnabil-ity [17] is in some sense tight in requiring the weak learner to be nearly distribution free. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction We study the problem of learning in the PAC model of Valiant [18] in which the example oracle used for learning is faulty. One way the oracle may be faulty is by returning examples with incorrect classification labels. Classification noise of Angluin and Laird <ref> [1] </ref> and malicious error of Valiant [19] (studied further by Kearns and Li [15]) model two such learning environments. Alternatively, the oracle may be considered faulty if the examples are not chosen according to the target distribution, even if they are labeled correctly with respect to the target function. <p> The PAC example oracle EX (c; D) is replaced by a statistics oracle ST AT (c; D). The learner interacts with ST AT (c; D) by asking it queries of the form (; ff) where is a function from labeled examples to f0; 1g and ff 2 <ref> [0; 1] </ref>. The query is a request for the value P , the probability that the value of (hx; li) is 1 when hx; li is an example drawn randomly according to D and labeled according to c. <p> The tolerance of a concept class C, denoted ff C , is the maximum tolerance of any polynomial time learning algorithm for C. 2.1 Classification Noise Angluin and Laird <ref> [1] </ref> introduced the model of learn ing with random classification noise. In this variation of the PAC model, the learner has access to a noisy example oracle EX fi CN . When a labeled example is requested from this oracle, an example is chosen according to the hidden distribution D.
Reference: [2] <author> P. Bartlett. </author> <title> Learning with a slowly changing distribution. </title> <booktitle> In Proceedings of COLT '92, </booktitle> <pages> pages 243-252. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Bartlett <ref> [2] </ref> studies learning when the sequence of examples come from a sequence of distributions such that the shift between consecutive distributions in the sequence is bounded. Distribution shift differs from this model in that all of the training ex amples come from the same distribution.
Reference: [3] <author> P. Bartlett and R. Williamson. </author> <title> Investigating the distribution assumptions in the pac learning model. </title> <booktitle> In Proceedings of COLT '91, </booktitle> <pages> pages 24-32. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1991. </year>
Reference-contexts: Alternatively, the oracle may be considered faulty if the examples are not chosen according to the target distribution, even if they are labeled correctly with respect to the target function. Bartlett and Williamson <ref> [3] </ref> study such a model of distribution noise. Kearns [12] recently introduced a new model of learning by statistical queries in which efficient learning is a sufficient condition for PAC learning with classification noise. <p> so that fl = V (ff) in the event that * = 1 4 . 2 Corollary 17 Monotone Boolean formulae are weakly learnable on hU n ; fl (n)i for fl (n) = V ( 1 cn ), where c is a constant. 3 Distribution Noise Bartlett and Williamson <ref> [3] </ref> examine the variant of the PAC model in which the training examples come from one distribution while the testing examples may come from another distribution. <p> complexity s A ( 1 * ; 1 8 ; 1 Then for * &lt; 1 2 , C is PAC learnable with dynamic dis tribution error fi = ln s 3.3 Distribution Shift The final noise model we consider, learning with distribution shift, is similar to Bartlett and Williamson's <ref> [3] </ref> learning with "b-close" distributions. <p> This intuition has been proven by the demonstration of a class of functions that are weakly learnable on -reasonable distributions, but are not strongly learnable on -reasonable distributions <ref> [3] </ref>. A less ambitious boosting strategy would be to boost a weak learning algorithm for a class of distributions D to a strong learning algorithm for some specific "central" distribution D in D. <p> We define a class of functions that is based on a class of non-local functions F r <ref> [3] </ref> (i.e. the value of these functions on one point tells you nothing about the value on any other point, e.g. a random value of 1 or 0 is chosen for points not previously seen). The functions are non-local on Y and the constant 1 everywhere else.
Reference: [4] <author> S. Ben-David, A. Itai, and E. Kushilevitz. </author> <title> Learning by distances. </title> <booktitle> In Proceedings of COLT '90, </booktitle> <pages> pages 232-245. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Here it is sufficient to have a minimum query tolerance of * 2 to produce a hypothesis with error no more than *. This is accomplished by the technique (which has been studied previously <ref> [4] </ref>) of querying the distances between the target concept and candidate hypotheses.
Reference: [5] <author> G. Benedek and A. Itai. </author> <title> Learnability by fixed distributions. </title> <booktitle> In Proceedings of COLT '88, </booktitle> <pages> pages 80-90. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: We show that the above strategy will not work in general. In order to show such a statement, we must first rule out those distributions on which we can strongly learn by trivial means. Benedek and Itai <ref> [5] </ref> show that any concept class is strongly learnable (not necessarily in polynomial time) under a discrete distribution. We define a class of distributions on which any concept class can be strongly learned in polynomial time.
Reference: [6] <author> G. Benedek and A. Itai. </author> <title> Dominating distributions and learnability. </title> <booktitle> In Proceedings of COLT '92, </booktitle> <pages> pages 253-264. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Therefore, when given an arbitrary distribution D of distance fl to some distribution in D, we cannot hope in general to get better than error fl. We can avoid this situation if we consider dominating distributions as defined by Benedek and Itai <ref> [6] </ref>. They use dominating distributions to specify when learning with one distribution implies learning with another. They say D 1 dominates D 2 if for every x 2 X, D 1 (x) = 0 implies D 2 (x) = 0.
Reference: [7] <author> R. Board and L. Pitt. </author> <title> On the necessity of occam algorithms. </title> <booktitle> In 22nd STOC, </booktitle> <pages> pages 54-63, </pages> <address> Baltimore, </address> <year> 1990. </year>
Reference-contexts: We will also show all of the arrows above to be unidirectional (i.e. we have strict inclusion), with exception of the possibility that DE ! DDE. This question hinges on the question of whether PAC algorithms imply Occam algorithms <ref> [7] </ref>. It would also be of interest to characterize the relationship between the CN model and the DS and DDE models.
Reference: [8] <author> Y. Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of COLT '92, </booktitle> <pages> pages 391-398. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Freund <ref> [8] </ref> asks how much we can relax the requirement that the weak learning algorithm work for all distributions.
Reference: [9] <author> M. Furst, J. Jackson, and S. Smith. </author> <title> Improved learning of AC 0 functions. </title> <booktitle> In Proceedings of COLT '91, </booktitle> <pages> pages 317-325. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: We further show that statistical queries are useful for expanding the class of distributions learnable by distribution restricted algorithms. Due to the difficulty of finding distribution free algorithms, there has been considerable work in finding learning algorithms that work on restricted classes of distributions <ref> [9, 10, 13, 16] </ref>. Most of these algorithms can also be specified in the statistical query model [12]. We show how to take such statistical query algorithms and prove their correctness on larger classes of distributions.
Reference: [10] <author> T. Hancock. </author> <title> Learning 2-DNF formulas and k decision trees. </title> <booktitle> In 4th COLT, </booktitle> <pages> pages 199-209, </pages> <address> Santa Cruz, </address> <year> 1991. </year>
Reference-contexts: We further show that statistical queries are useful for expanding the class of distributions learnable by distribution restricted algorithms. Due to the difficulty of finding distribution free algorithms, there has been considerable work in finding learning algorithms that work on restricted classes of distributions <ref> [9, 10, 13, 16] </ref>. Most of these algorithms can also be specified in the statistical query model [12]. We show how to take such statistical query algorithms and prove their correctness on larger classes of distributions.
Reference: [11] <author> David Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference-contexts: Theorem 6 The class of monomials of size k over n variables (M k ) is PAC learnable in polynomial time with malicious error rate E poly MAL = ( * * ) ). Proof (Sketch): We use the set cover approach to learn ing monomials of k literals <ref> [11] </ref>. The SQ version of this algorithm is a straightforward conversion from covering actual examples to covering probabilities of examples. Such a conversion results in a SQ algorithm of toler ance fi ( * k log (1=*) ) [12].
Reference: [12] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In 25th STOC, </booktitle> <address> San Diego, </address> <year> 1993. </year> <note> To Appear. </note>
Reference-contexts: Alternatively, the oracle may be considered faulty if the examples are not chosen according to the target distribution, even if they are labeled correctly with respect to the target function. Bartlett and Williamson [3] study such a model of distribution noise. Kearns <ref> [12] </ref> recently introduced a new model of learning by statistical queries in which efficient learning is a sufficient condition for PAC learning with classification noise. <p> Due to the difficulty of finding distribution free algorithms, there has been considerable work in finding learning algorithms that work on restricted classes of distributions [9, 10, 13, 16]. Most of these algorithms can also be specified in the statistical query model <ref> [12] </ref>. We show how to take such statistical query algorithms and prove their correctness on larger classes of distributions. The amount of expanded usage is determined by the tolerance of the statistical query algorithm. <p> When this is done recursively, one can show that weak learning is equivalent to strong learning. We show that this result hinges on the ability of the weak learner to work on nearly every distribution. 2 Statistical Queries and PAC errors In the statistical query model <ref> [12] </ref>, learning is based on statistical properties of large samples as opposed to the specific properties of individual examples. The PAC example oracle EX (c; D) is replaced by a statistics oracle ST AT (c; D). <p> The statistics oracle will return an approximation ^ P such that j ^ P P j ff. Definition 1 <ref> [12] </ref> A class of concepts C is said to be efficiently learnable from statistical queries if there exists a learning algorithm A and polynomials p (; ; ), q (; ) and r (; ; ) such that for any c 2 C n , and D over X n and <p> Although noise tolerant algorithms were know for a handful of other classes, it remained open whether or not most classes which were PAC learnable, were learnable in the presence of noise. Kearns <ref> [12] </ref> showed that efficient learning in the SQ model is a sufficient condition for learning in the PAC model with classification noise. Since almost all PAC algorithms can be easily restated in the SQ model, this immediately added these classes to the list of those learnable with noise. Theorem 3 [12] <p> <ref> [12] </ref> showed that efficient learning in the SQ model is a sufficient condition for learning in the PAC model with classification noise. Since almost all PAC algorithms can be easily restated in the SQ model, this immediately added these classes to the list of those learnable with noise. Theorem 3 [12] Efficient SQ learning implies PAC learning with classification noise. <p> The SQ version of this algorithm is a straightforward conversion from covering actual examples to covering probabilities of examples. Such a conversion results in a SQ algorithm of toler ance fi ( * k log (1=*) ) <ref> [12] </ref>. The theorem follows from this tolerance and Theorem 5. 2 The previous known bound was ( * k log ( k log n ) [15] which has a small but definite dependence on n. <p> implies the ability to learn with error fi in another: M AL ! DS ! DDE ! DE In the full paper we will show that the DE and DDE models are each equivalent to their respective generalizations in which there is a "variable noise rate" as defined by Kearns <ref> [12] </ref>. We will also show all of the arrows above to be unidirectional (i.e. we have strict inclusion), with exception of the possibility that DE ! DDE. This question hinges on the question of whether PAC algorithms imply Occam algorithms [7].
Reference: [13] <author> M. Kearns, M. Li, L. Pitt, and L. G. Valiant. </author> <title> On the learnability of boolean formulae. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 285-295, </pages> <address> New York, New York, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: We further show that statistical queries are useful for expanding the class of distributions learnable by distribution restricted algorithms. Due to the difficulty of finding distribution free algorithms, there has been considerable work in finding learning algorithms that work on restricted classes of distributions <ref> [9, 10, 13, 16] </ref>. Most of these algorithms can also be specified in the statistical query model [12]. We show how to take such statistical query algorithms and prove their correctness on larger classes of distributions.
Reference: [14] <author> Michael Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: It is known that monotone Boolean formulae are weakly learnable on the uniform distribution, but to either allow non-monotonicity, allow arbitrary distributions, or strengthen from weak to strong learning results in intractability <ref> [14] </ref>. We can show that monotone Boolean formulae are weakly learnable on a class of distributions strictly containing the uniform distribution.
Reference: [15] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: One way the oracle may be faulty is by returning examples with incorrect classification labels. Classification noise of Angluin and Laird [1] and malicious error of Valiant [19] (studied further by Kearns and Li <ref> [15] </ref>) model two such learning environments. Alternatively, the oracle may be considered faulty if the examples are not chosen according to the target distribution, even if they are labeled correctly with respect to the target function. Bartlett and Williamson [3] study such a model of distribution noise. <p> With probability fi, a malicious adversary may select any example and label it either positive or negative. Valiant shows the class of k-DNF formulae is learnable with small amounts of malicious error. Kearns and Li <ref> [15] </ref> prove an upper bound of * 1+* on the amount of malicious error tolerable when learning any distinct concept class (most interesting classes are distinct) and give various other lower bounds on tolerable malicious error. <p> Such a conversion results in a SQ algorithm of toler ance fi ( * k log (1=*) ) [12]. The theorem follows from this tolerance and Theorem 5. 2 The previous known bound was ( * k log ( k log n ) <ref> [15] </ref> which has a small but definite dependence on n. For k = n, we get a bound of ( * p n log 1 ), whereas the previous bound yields ( * p n log n ). <p> It is based on the idea used to give a lower bound on tolerable malicious error for PAC learnable classes <ref> [15] </ref>.
Reference: [16] <author> Nathan Linial, Yishay Mansour, and Noam Nisan. </author> <title> Constant depth circuits, fourier transform, and learnability. </title> <booktitle> In Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 574-579, </pages> <institution> Research Triangle Park, North Carolina, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: We further show that statistical queries are useful for expanding the class of distributions learnable by distribution restricted algorithms. Due to the difficulty of finding distribution free algorithms, there has been considerable work in finding learning algorithms that work on restricted classes of distributions <ref> [9, 10, 13, 16] </ref>. Most of these algorithms can also be specified in the statistical query model [12]. We show how to take such statistical query algorithms and prove their correctness on larger classes of distributions.
Reference: [17] <author> R. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-226, </pages> <year> 1990. </year>
Reference-contexts: We study three variations of distribution noise and give necessary and sufficient conditions for polynomial time learning with such noise. In addition, we show how the malicious error and classification noise models relate to these new noise models. Finally, we examine weak to strong boosting a la Schapire <ref> [17] </ref>. We do so in the context of noisy distributions. In such boosting, the strategy is to focus the weak learner on portions of the instance space that are being misclassified by the current hypothesis in order to create a better hypothesis. <p> Which is a more powerful adversary: one that misclassifies with fixed probability or one that may maliciously change the distribution of correctly classified examples? 3.5 Weak and Strong Distribution Restricted Learning Schapire <ref> [17] </ref> shows that for any class of target functions C, if there exists a distribution free weak learning algorithm for C, then there exists a distribution free strong learning algorithm for C.
Reference: [18] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: 1 Introduction We study the problem of learning in the PAC model of Valiant <ref> [18] </ref> in which the example oracle used for learning is faulty. One way the oracle may be faulty is by returning examples with incorrect classification labels.
Reference: [19] <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings IJCAI 1985, </booktitle> <pages> pages 560-566. </pages> <publisher> Morgan Kauf-mann, </publisher> <month> August </month> <year> 1985. </year>
Reference-contexts: One way the oracle may be faulty is by returning examples with incorrect classification labels. Classification noise of Angluin and Laird [1] and malicious error of Valiant <ref> [19] </ref> (studied further by Kearns and Li [15]) model two such learning environments. Alternatively, the oracle may be considered faulty if the examples are not chosen according to the target distribution, even if they are labeled correctly with respect to the target function. <p> ance of the SQ algorithm and the query space Q is finite, then the sample complexity for learning with noise bounded by fi is O (ff (12fi)) 2 log ffi while if Q has finite VC dimension d, the sample complexity is O d ffi . 2.2 Malicious Errors Valiant <ref> [19] </ref> introduced the model of learning with malicious errors. In this variation of the PAC model, the learner has access to an example oracle EX fi MAL .
References-found: 19

