URL: http://now.cs.berkeley.edu/~alanm/Papers/clumps.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/
Root-URL: http://www.cs.berkeley.edu
Email: fstevel,alanm,cullerg@CS.Berkeley.EDU  
Title: Multi-Protocol Active Messages on a Cluster of SMP's  
Author: Steven S. Lumetta, Alan M. Mainwaring, and David E. Culler 
Date: August 25, 1997  
Address: Berkeley  
Affiliation: Computer Science Division University of California at  
Web: SC97)  
Note: (to appear in the Proceedings of  
Abstract: Clusters of multiprocessors, or Clumps, promise to be the supercomputers of the future, but obtaining high performance on these architectures requires an understanding of interactions between the multiple levels of interconnection. In this paper, we present the first multi-protocol implementation of a lightweight message layer|a version of Active Messages-II running on a cluster of Sun Enterprise 5000 servers connected with Myrinet. This research brings together several pieces of high-performance interconnection technology: bus backplanes for symmetric multiprocessors, low-latency networks for connections between machines, and simple, user-level primitives for communication. The paper describes the shared memory message-passing protocol and analyzes the multi-protocol implementation with both microbenchmarks and Split-C applications. Three aspects of the communication layer are critical to performance: the overhead of cache-coherence mechanisms, the method of managing concurrent access, and the cost of accessing state with the slower protocol. Through the use of an adaptive polling strategy, the multi-protocol implementation limits performance interactions between the protocols, delivering up to 160 MB/s of bandwidth with 3.6 microsecond end-to-end latency. Applications within an SMP benefit from this fast communication, running up to 75% faster than on a network of uniprocessor workstations. Applications running on the entire Clump are limited by the balance of NIC's to processors in our system, and are typically slower than on the NOW. These results illustrate several potential pitfalls for the Clumps architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Alexandrov, M. Ionescu, K. E. Schauser, C. Scheiman, "LogGP: </author> <title> Incorporating Long Messages into the LogP Model|One Step Closer Towards a Realistic Model for Parallel Computation," </title> <booktitle> 7th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: The overhead o is often separated into send overhead, o s , and receive overhead, o r . LogP assumes a small, fixed message length for communication, neglecting common hardware support for large transfers. LogGP <ref> [1] </ref> extends the LogP model with the parameter G, the time per byte for long messages. LogP parameters were measured using a mi-crobenchmark from the suite described in [10].
Reference: [2] <institution> Accelerated Strategic Computing Initiative, a program of the Department of Energy. </institution> <note> Information is available via http://www.llnl.gov/asci-alliances/. </note>
Reference-contexts: 1 Introduction Clusters of multiprocessors, or "Clumps," promise to be the supercomputers of the future <ref> [2, 31] </ref>, but obtaining high performance on these architectures requires an understanding of interactions between the multiple levels of interconnection. In this paper, we develop and measure a multi-protocol Active Message layer using the Sun Gigaplane memory system [27] and the Myricom network [4].
Reference: [3] <author> D. A. Bader, J. JaJa, </author> <title> "SIMPLE: A Methodology for Programming High Performance Algorithms on Clusters of Symmetric Multiprocessors (SMP's)," </title> <note> preliminary version, May 1997, available via http://www.umiacs.umd.edu/research/EXPAR. </note>
Reference-contexts: This research brings together several pieces of high-performance interconnection technology: bus backplanes for symmetric multiprocessors, low-latency networks for connections between machines, and simple, user-level primitives for communication. Several groups have studied the problem of programming Clumps <ref> [3, 6, 11, 13, 15, 16] </ref>. Some of these efforts focus on issues related to shared virtual memory [17, 32], but most relate to high-level message libraries such as MPI. <p> P4 provides mechanisms start multiple threads on one or more machines and to communicate between such threads using either message-passing or shared memory constructs. The programmer must explicitly select the appropriate library call. The library also provides a number of useful reduction operations. SIMPLE <ref> [3] </ref> provides functionality similar to P4, but extends the library with broadcast operations and a variety of tuned, many-processor communication methods. SIMPLE also attempts to lighten the programmer's burden by offering functions that involve all processors, all processors in an SMP, one processor in each SMP, and so forth.
Reference: [4] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, W. </author> <title> Su, </title> <journal> "Myrinet|A Gigabit-per-Second Local-Area Network," IEEE Micro, </journal> <volume> Vol. 15, </volume> <month> February </month> <year> 1995, </year> <pages> pp. 29-38. </pages>
Reference-contexts: In this paper, we develop and measure a multi-protocol Active Message layer using the Sun Gigaplane memory system [27] and the Myricom network <ref> [4] </ref>. Our system is the first implementation of a lightweight message layer to transparently handle multiple communication protocols. The uniform, user-level communication abstraction that results serves as a powerful building block for applications on Clumps. <p> Communication between SMP's utilizes multiple, independent SBUS connections to a Myrinet high-speed network with internal link bandwidths of 160 MB/s <ref> [4] </ref>. The critical components of hardware performance are the memory hierarchy and the network, as these characteristics have direct impact on the speed at which data moves from one processor to another. The cost of synchronization primitives is also pertinent when managing simultaneous access by multiple processes.
Reference: [5] <author> E. A. Brewer, B. C. Kuszmaul, </author> <title> "How to Get Good Performance from the CM-5 Data Network," </title> <booktitle> Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: After this call returns, the packet is marked as free and the data block is marked as in valid. 5 3.4 Polling Strategy Message polling operations are ubiquitous in Active Message layers. Responsiveness demands that a layer poll for incoming messages when sending a message <ref> [5] </ref>. In the case of the multi-protocol implementation, however, the interaction between the lightweight shared memory protocol and the more expensive network protocol can have significant impact on the performance of the former.
Reference: [6] <author> R. Butler, E. Lusk, </author> <title> "Monitors, Message, and Clusters: the p4 Parallel Programming System," </title> <note> available via http://www.mcs.anl.gov/home/lusk/p4/p4-paper/paper.html. </note>
Reference-contexts: This research brings together several pieces of high-performance interconnection technology: bus backplanes for symmetric multiprocessors, low-latency networks for connections between machines, and simple, user-level primitives for communication. Several groups have studied the problem of programming Clumps <ref> [3, 6, 11, 13, 15, 16] </ref>. Some of these efforts focus on issues related to shared virtual memory [17, 32], but most relate to high-level message libraries such as MPI. <p> This paper does not speak directly to that problem, although some insight can be gained from the section on applications. Such efforts often assume that a programmer is willing to rewrite most or all of an application to obtain performance. The P4 programming system <ref> [6] </ref> was probably one of the first systems to recognize Clumps as a platform. P4 provides mechanisms start multiple threads on one or more machines and to communicate between such threads using either message-passing or shared memory constructs. The programmer must explicitly select the appropriate library call.
Reference: [7] <author> B. N. Chun, A. M. Mainwaring, D. E. Culler, </author> <title> "A General-Purpose Protocol Architecture for a Low-Latency, Multi-gigabit System Area Network," Proceedings of Hot Interconnects V, </title> <address> Stan-ford, California, </address> <month> August </month> <year> 1997. </year> <month> 13 </month>
Reference-contexts: The section progresses through each of these dimensions, touching on the issues related to each and explaining the position taken in this work. 3.1 Data Layout The multi-protocol representation of an endpoint is a natural extension of the representation developed for the Myrinet AM-II implementation <ref> [7] </ref>. As shown in control block holds information such as the table of handler routines and the table of message destinations, the network queue block holds message queues for the network, and the shared memory queue block holds message queues for shared memory.
Reference: [8] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Kr--ishnamurthy, S. S. Lumetta, T. von Eicken, K. Yelick, </author> <title> "Parallel Programming in Split-C," </title> <booktitle> Proceedings of Supercomputing 1993, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993, </year> <pages> pp. 262-73. </pages>
Reference-contexts: The applications are drawn from the Split-C application suite <ref> [8] </ref> and are written in a bulk synchronous style|processors proceed through a sequence of coarse-grained phases, performing a global synchronization between each phase.
Reference: [9] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sa-hay, K. E. Schauser, E. Santos, R. Subramo-nian, T. von Eicken, </author> <title> "LogP: Towards a Realistic Model of Parallel Computation," </title> <booktitle> Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: We next illustrate the costs of message-passing relative to the cache-line transfer time with a detailed breakdown of the overhead involved in sending a local message. A summary of the issues and implications concludes the section. 4.1 Methodology The LogP model of network performance <ref> [9] </ref> attempts to characterize communication networks as a set of four parameters: L, an upper bound on the network latency (wire time) between processors; o, the processor busy-time required to inject a message into the network or to pull one out; g, the minimum time between message injections for large numbers
Reference: [10] <author> D. E. Culler, L. T. Liu, R. P. Martin, C. O. Yoshikawa, </author> <title> "Assessing Fast Network Interfaces", </title> <journal> IEEE Micro, </journal> <volume> Vol. 16, No. 1, </volume> <month> February </month> <year> 1996, </year> <pages> pp. 35-43. </pages>
Reference-contexts: LogP assumes a small, fixed message length for communication, neglecting common hardware support for large transfers. LogGP [1] extends the LogP model with the parameter G, the time per byte for long messages. LogP parameters were measured using a mi-crobenchmark from the suite described in <ref> [10] </ref>. To measure G, we constructed a second benchmark to fragment bulk data transfers of arbitrary length into 8 kB chunks and to pipeline those chunks through the Active Message layer.
Reference: [11] <author> S. J. Fink, S. B. Baden, </author> <title> "Non-Uniform Partitioning of Finite Difference Methods Running on SMP Clusters," </title> <note> submitted for publication, available via http://www-cse.ucsd.edu/users/baden/MT.html. </note>
Reference-contexts: This research brings together several pieces of high-performance interconnection technology: bus backplanes for symmetric multiprocessors, low-latency networks for connections between machines, and simple, user-level primitives for communication. Several groups have studied the problem of programming Clumps <ref> [3, 6, 11, 13, 15, 16] </ref>. Some of these efforts focus on issues related to shared virtual memory [17, 32], but most relate to high-level message libraries such as MPI. <p> SIMPLE also attempts to lighten the programmer's burden by offering functions that involve all processors, all processors in an SMP, one processor in each SMP, and so forth. A paper by Fink and Baden <ref> [11] </ref> attacks the problem of balance in bulk synchronous algorithms by re-balancing computation and communication for a regular problem within an SMP.
Reference: [12] <author> S. J. Fink, S. B. Baden, </author> <title> "Runtime Support for Multi-Tier Programming of Block-Structured Applications on SMP Clusters," </title> <note> submitted for publication, available via http://www-cse.ucsd.edu/users/baden/MT.html. </note>
Reference-contexts: Essentially, the analysis gives processors on boundaries less computation to balance the cost of communication. KeLP, by the same authors, seeks to simplify the process of application development. Recent extensions to KeLP <ref> [12] </ref> add new functionality to support applications on Clumps. With KeLP, a programmer expresses data decomposition and motion in a block-structured style. The runtime system then employs inspector-executor analysis to overlap communication with computation. No global barriers are used; interprocessor synchronization occurs only through communication dependencies.
Reference: [13] <author> I. Foster, C. Kesselman, S. Tuecke, </author> <title> "The Nexus Approach to Integrating Multithreading and Communication," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 37, </volume> <month> August </month> <year> 1996, </year> <pages> pp. 70-82. </pages>
Reference-contexts: This research brings together several pieces of high-performance interconnection technology: bus backplanes for symmetric multiprocessors, low-latency networks for connections between machines, and simple, user-level primitives for communication. Several groups have studied the problem of programming Clumps <ref> [3, 6, 11, 13, 15, 16] </ref>. Some of these efforts focus on issues related to shared virtual memory [17, 32], but most relate to high-level message libraries such as MPI. <p> Each view proves more natural and effective than the other for interesting classes of applications. In the smaller body of message-passing work, Nexus comes closest to our own. Nexus is a portable programming system <ref> [13] </ref> that focuses primarily on portability and on support for heterogeneity. It supports arbitrary sets of machines, processes (or contexts, in Nexus terminology) and threads. Nexus generally builds on top of existing communication layers, resulting in somewhat higher overheads than those obtained with Active Messages.
Reference: [14] <author> I. Foster, J. Geisler, C. Kesselman, S. Tuecke, </author> <title> "Managing Multiple Communication Methods in High-Performance Networked Computing Systems," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 40, </volume> <month> January </month> <year> 1997, </year> <pages> pp. 35-48. </pages>
Reference-contexts: A startpoint can be bound to multiple endpoints, allowing for multicast communication. Since Nexus platforms can support multiple communication protocols between a startpoint and an endpoint, Nexus has explored multi-protocol communication from a more general perspective than have we <ref> [14] </ref>. Although shared memory is mentioned in the work, numbers are provided only for more expensive underlying protocols, making a direct comparison impossible. The Nexus multi-protocol paper also notes the wide variance between polling costs for different protocols and presents data for fractional polling strategies.
Reference: [15] <author> W. W. Gropp, E. L. Lusk, </author> <title> "A Taxonomy of Programming Models for Symmetric Multiprocessors and SMP clusters," </title> <booktitle> Proceedings of Programming Models for Massively Parallel Computers 1995, </booktitle> <month> October </month> <year> 1995, </year> <pages> pp. 2-7. </pages>
Reference-contexts: This research brings together several pieces of high-performance interconnection technology: bus backplanes for symmetric multiprocessors, low-latency networks for connections between machines, and simple, user-level primitives for communication. Several groups have studied the problem of programming Clumps <ref> [3, 6, 11, 13, 15, 16] </ref>. Some of these efforts focus on issues related to shared virtual memory [17, 32], but most relate to high-level message libraries such as MPI.
Reference: [16] <author> M. Haines, D. Cronk, P. Mehrotra, </author> <title> "On the Design of Chant: A Talking Threads Package," </title> <booktitle> Proceedings of Supercomputing 1994, </booktitle> <address> Washing-ton, D.C., </address> <month> November </month> <year> 1994, </year> <pages> pp. 350-9. </pages>
Reference-contexts: This research brings together several pieces of high-performance interconnection technology: bus backplanes for symmetric multiprocessors, low-latency networks for connections between machines, and simple, user-level primitives for communication. Several groups have studied the problem of programming Clumps <ref> [3, 6, 11, 13, 15, 16] </ref>. Some of these efforts focus on issues related to shared virtual memory [17, 32], but most relate to high-level message libraries such as MPI.
Reference: [17] <author> D. Jiang, H. Shan, J. P. Singh, </author> <title> "Application Restructuring and Performance Portability on Shared Virtual Memory and Hardware-Coherent Multiprocessors," </title> <booktitle> Proceedings of Principles and Practice of Parallel Programming, </booktitle> <year> 1997, </year> <pages> pp. 217-29. </pages>
Reference-contexts: Several groups have studied the problem of programming Clumps [3, 6, 11, 13, 15, 16]. Some of these efforts focus on issues related to shared virtual memory <ref> [17, 32] </ref>, but most relate to high-level message libraries such as MPI. The software overheads associated with memory allocation and tag matching in traditional message-passing libraries often obscure the machine-level performance interactions and design issues associated with the actual communication. <p> The literature pertaining to Clumps is still fairly limited, but covers quite a wide range of topics. One approach to Clumps that has received much attention over the years is the extension of shared memory between SMP's. Recent efforts on this front include SVM <ref> [17] </ref> and MGS [32]. These studies investigate a problem complementary to our own in that both seek to optimize common techniques in one medium to allow use of those techniques in both. Each view proves more natural and effective than the other for interesting classes of applications.
Reference: [18] <author> B.-H. Lim, P. Heidelberger, P. Pattnaik, M. Snir, </author> <title> "Message Proxies for Efficient, Protected Communication on SMP Clusters," </title> <institution> IBM Almaden Research Report #RC 20522 (90972), </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: The Nexus multi-protocol paper also notes the wide variance between polling costs for different protocols and presents data for fractional polling strategies. We explored more adaptive strategies to reduce the impact of network polling to a satisfactory level. 11 An interesting study by Lim et. al. <ref> [18] </ref> investigates the use of one processor in each SMP as a message proxy for the remaining processors. The work focuses on providing multiple users with protected access to a single network resource and evaluates the proxy approach in detail.
Reference: [19] <author> L. T. Liu, D. E. Culler, </author> <title> "Evaluation of the Intel Paragon on Active Message Communication," </title> <booktitle> Proceedings of Intel Supercomputer Users Group Conference, </booktitle> <month> June </month> <year> 1995, </year> <note> also available via http://now.CS.Berkeley.EDU. </note>
Reference-contexts: The negative latency for the single-protocol shared memory case indicates overlap in time between the send and receive overheads <ref> [19] </ref>, in this instance due to the poll operation. The adaptive polling strategy limits the impact of network polling on local messages to an average of 0.2 microseconds for both send and receive overhead and an increase of roughly 30% in round trip time.
Reference: [20] <author> S. S. Lumetta, D. E. Culler, </author> <title> "Managing Concurrent Access for Shared Memory Active Messages," </title> <editor> U. </editor> <address> C. </address> <note> Berkeley Technical Report in preparation. </note>
Reference-contexts: Furthermore, the method outlined above results in superior application performance even for a dedicated system. The interested reader is referred to <ref> [20] </ref> for further detail. Although the AM-II library provides support for protected access to an endpoint using multiple receiver threads, we have assumed the use of a single thread per process in this work. The issues and costs for concurrent access by receivers are similar to those for senders.
Reference: [21] <author> A. M. Mainwaring, D. E. Culler, </author> <title> "Active Message Applications Programming Interface and Communication Subsystem Organization," </title> <editor> U. </editor> <address> C. </address> <institution> Berkeley Technical Report #CSD-96-918, </institution> <month> October </month> <year> 1996, </year> <note> also available via http://now.CS.Berkeley.EDU. </note>
Reference-contexts: To quan-tify these dimensions, we have built a multi-protocol implementation of Active Messages-II <ref> [21] </ref> that transparently directs message traffic through the appropriate medium, either shared memory or a high-speed network. The implementation operates on a cluster of four Sun Enterprise 5000 servers running the So-laris 2.5 operating system and interconnected by a Myrinet with multiple NIC's per SMP. <p> The AM-II specification <ref> [21] </ref> defines a uniform communication interface that provides the functionality required for general-purpose distributed programming yet permits implementations yielding performance close to that available at the hardware level. AM-II abstracts communication into point-to-point messages between communication endpoints.
Reference: [22] <author> R. Martin, "HPAM: </author> <title> an Active Message Layer for a Network of HP Workstations," Proceedings of Hot Interconnects II, </title> <publisher> Stanford, </publisher> <address> California, </address> <month> August </month> <year> 1994, </year> <pages> pp. 40-58. </pages>
Reference-contexts: Both use 167 MHz processors. The uniprocessor boasts lower memory latency but provides lower memory bandwidth as well. Note that write latency for NIC memory is usually hidden by the write buffer. nication available <ref> [22, 26, 28, 29, 30] </ref>. Each active message contains a reference to a handler routine. When a message is received, the communication layer passes the data to the handler referenced by the message, typically as formal parameters.
Reference: [23] <author> J. M. Mellor-Crummey, M. L. Scott, </author> <title> "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 9, No. 1, </volume> <month> February </month> <year> 1991, </year> <pages> pp. 21-65. </pages>
Reference-contexts: We have studied the performance of a range of mechanisms for managing concurrent access to the shared memory queue blocks, including the mutual exclusion techniques described in <ref> [23] </ref>. The communication regime is one of low resource contention. The time spent in the critical section of the send operation is small when compared with the total overhead of sending a message, and only intense all-to-one communication results in non-trivial contention for the shared queues.
Reference: [24] <author> S. S. Mukherjee, M. D. Hill, </author> <title> "A Case for Making Network Interfaces Less Peripheral," Proceedings of Hot Interconnects V, </title> <publisher> Stanford, </publisher> <address> California, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: With KeLP, a programmer expresses data decomposition and motion in a block-structured style. The runtime system then employs inspector-executor analysis to overlap communication with computation. No global barriers are used; interprocessor synchronization occurs only through communication dependencies. In work related less directly to Clumps, Mukherjee and Hill <ref> [24] </ref> have investigated the advantages of mak ing NIC memory cacheable. For multi-protocol communication, the importance of cacheable NIC memory is the resulting reduction in the cost of polling the network.
Reference: [25] <author> R. H. Saavedra, </author> <title> "Micro Benchmark Analysis of the KSR1," </title> <booktitle> Proceedings of Supercomputing 1993, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993, </year> <pages> pp. 202-13. </pages>
Reference-contexts: The cost of synchronization primitives is also pertinent when managing simultaneous access by multiple processes. Using microbenchmarks based upon those of Saavedra-Barrera <ref> [25] </ref>, we are able to measure these values for our system, as shown in Table 1. For comparison, the table also gives the parameters for an UltraSPARC Model 170 workstation, which uses the same processor as the Enterprise 5000.
Reference: [26] <author> K. E. Schauser, C. Scheiman, </author> <title> "Experiences with Active Messages on the Meiko CS-2," </title> <booktitle> Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Both use 167 MHz processors. The uniprocessor boasts lower memory latency but provides lower memory bandwidth as well. Note that write latency for NIC memory is usually hidden by the write buffer. nication available <ref> [22, 26, 28, 29, 30] </ref>. Each active message contains a reference to a handler routine. When a message is received, the communication layer passes the data to the handler referenced by the message, typically as formal parameters.
Reference: [27] <author> A. Singhal, D. Broniarczyk, F. Cerauskis, J. Price, L. Yuan, C. Cheng, D. Doblar, S. Fosth, N. Agarwal, K. Harvey, E. Hagersten, B. Lien-cres, "Gigaplane: </author> <title> A High Performance Bus for Large SMPs," </title> <booktitle> Proceedings of Hot Interconnects IV, </booktitle> <address> Stanford, California, </address> <month> August </month> <year> 1996, </year> <pages> pp. 41-52. 14 </pages>
Reference-contexts: In this paper, we develop and measure a multi-protocol Active Message layer using the Sun Gigaplane memory system <ref> [27] </ref> and the Myricom network [4]. Our system is the first implementation of a lightweight message layer to transparently handle multiple communication protocols. The uniform, user-level communication abstraction that results serves as a powerful building block for applications on Clumps. <p> Links in the network provide 160 MB/s of bidirectional bandwidth; bandwidth between the host and the NIC is limited by the SBUS to an observed maximum of 38 MB/s. up to 2.7 GB/s of bandwidth <ref> [27] </ref>. Communication between SMP's utilizes multiple, independent SBUS connections to a Myrinet high-speed network with internal link bandwidths of 160 MB/s [4].
Reference: [28] <author> L. Tucker, A. M. Mainwaring, </author> <title> "CMMD: Active Messages on the CM-5," </title> <journal> Parallel Computing, </journal> <volume> Vol. 20, No. 4, </volume> <month> August </month> <year> 1994, </year> <pages> pp. 481-96. </pages>
Reference-contexts: Both use 167 MHz processors. The uniprocessor boasts lower memory latency but provides lower memory bandwidth as well. Note that write latency for NIC memory is usually hidden by the write buffer. nication available <ref> [22, 26, 28, 29, 30] </ref>. Each active message contains a reference to a handler routine. When a message is received, the communication layer passes the data to the handler referenced by the message, typically as formal parameters.
Reference: [29] <author> T. von Eicken, V. Avula, A. Basu, V. </author> <title> Buch, "Low-latency Communication over ATM Networks Using Active Messages," Proceedings of Hot Interconnects II, </title> <publisher> Stanford, </publisher> <address> California, </address> <month> August </month> <year> 1994, </year> <pages> pp. 60-71. </pages>
Reference-contexts: Both use 167 MHz processors. The uniprocessor boasts lower memory latency but provides lower memory bandwidth as well. Note that write latency for NIC memory is usually hidden by the write buffer. nication available <ref> [22, 26, 28, 29, 30] </ref>. Each active message contains a reference to a handler routine. When a message is received, the communication layer passes the data to the handler referenced by the message, typically as formal parameters.
Reference: [30] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, K. E. Schauser, </author> <title> "Active Messages: a Mechanism for Integrated Communication and Computation," </title> <booktitle> in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Qld., Australia, </address> <month> May </month> <year> 1992, </year> <pages> pp. 256-66. </pages>
Reference-contexts: Both use 167 MHz processors. The uniprocessor boasts lower memory latency but provides lower memory bandwidth as well. Note that write latency for NIC memory is usually hidden by the write buffer. nication available <ref> [22, 26, 28, 29, 30] </ref>. Each active message contains a reference to a handler routine. When a message is received, the communication layer passes the data to the handler referenced by the message, typically as formal parameters.
Reference: [31] <author> P. R. Woodward, </author> <title> "Perspectives on Supercomputing: Three Decades of Change," </title> <journal> IEEE Computer, </journal> <volume> Vol. 29, </volume> <month> October </month> <year> 1996, </year> <pages> pp. 99-111. </pages>
Reference-contexts: 1 Introduction Clusters of multiprocessors, or "Clumps," promise to be the supercomputers of the future <ref> [2, 31] </ref>, but obtaining high performance on these architectures requires an understanding of interactions between the multiple levels of interconnection. In this paper, we develop and measure a multi-protocol Active Message layer using the Sun Gigaplane memory system [27] and the Myricom network [4].
Reference: [32] <author> D. Yeung, J. Kubiatowicz, A. Agarwal, </author> <month> "MGS: </month>
Reference-contexts: Several groups have studied the problem of programming Clumps [3, 6, 11, 13, 15, 16]. Some of these efforts focus on issues related to shared virtual memory <ref> [17, 32] </ref>, but most relate to high-level message libraries such as MPI. The software overheads associated with memory allocation and tag matching in traditional message-passing libraries often obscure the machine-level performance interactions and design issues associated with the actual communication. <p> The literature pertaining to Clumps is still fairly limited, but covers quite a wide range of topics. One approach to Clumps that has received much attention over the years is the extension of shared memory between SMP's. Recent efforts on this front include SVM [17] and MGS <ref> [32] </ref>. These studies investigate a problem complementary to our own in that both seek to optimize common techniques in one medium to allow use of those techniques in both. Each view proves more natural and effective than the other for interesting classes of applications.
References-found: 32

