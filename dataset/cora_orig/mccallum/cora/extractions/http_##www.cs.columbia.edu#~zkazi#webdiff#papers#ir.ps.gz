URL: http://www.cs.columbia.edu/~zkazi/webdiff/papers/ir.ps.gz
Refering-URL: http://www.cs.columbia.edu/~zkazi/webdiff/biblio.html
Root-URL: http://www.cs.columbia.edu
Title: Information Retrieval Algorithms: A Survey  
Author: Prabhakar Raghavan 
Abstract: We give an overview of some algorithmic problems arising in the representation of text/image/multimedia objects in a form amenable to automated searching, and in conducting these searches efficiently. These operations are central to information retrieval and digital library systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. W. Berry, S. T. Dumais, and G. W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37(4), </volume> <year> 1995, </year> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: Queries are also projected and processed in this low-dimensional space. This results not only in great savings in storage and query time (at the expense of some considerable preprocessing), but also, according to empirical evidence reported in the literature, to improved information retrieval <ref> [1, 7, 8] </ref>. Let A be an n fi m matrix as before; let the rank of A be r. Let the singular values of A be 1 2 : : : r (not necessarily distinct), i.e., 2 1 ; 2 r are the eigenvalues of AA T . <p> However, the literature <ref> [1, 6, 7, 8] </ref> goes beyond suggesting LSI as merely a dimension-reduction technique: in fact, the reported experience is that LSI tends to bring together documents that are semantically related.
Reference: [2] <author> M. W. Berry, T. Do, G. W. O'Brien, V. Krishna, and S. Varadhan. </author> <note> SVDPACKC (Version 1.0) User's Guide. </note> <institution> University of Tennessee, </institution> <month> April </month> <year> 1993. </year>
Reference: [3] <author> C. Buckley, A. Singhal, M. Mitra, G. Salton. </author> <title> New Retrieval Approaches Using SMART: </title> <booktitle> TREC 4. Proceedings of the Fourth Text Retrieval Conference, </booktitle> <institution> National Institute of Standards and Technology, </institution> <year> 1995. </year>
Reference-contexts: Considerable effort then goes into finding the "right" distance function in this vector space. The vector-space approach is generally attributed to Luhn, and was popularized by Salton in the Cornell SMART system <ref> [3] </ref>. The number of dimensions in the representation could be tens of thousands, which makes it challenging to perform the near-neighbor computation mentioned above.
Reference: [4] <author> C. Chekuri, M. Goldwasser, P. Raghavan and E. Up-fal. </author> <title> Automated categorization on world-wide web documents. </title> <type> Unpublished manuscript, </type> <year> 1996. </year>
Reference-contexts: How does one ensure that the clusters remain reasonably balanced? Goal 5. To devise generic procedures for clustering documents in a vector space with efficient implementations. Experience shows that automated categorization is very corpus-dependent. Chekuri et al. <ref> [4] </ref>, for instance, report experiments on automatically categorizing web documents according to the broad categories at the top-level of Yahoo!.
Reference: [5] <author> D. R. Cutting, J. O. Pedersen, D. R. Karger and J. W. Tukey. Scatter/Gather: </author> <title> A Cluster-based Approach to Browsing Large Document Collections. </title> <booktitle> Proceedings of ACM SIGIR, </booktitle> <pages> 318-329, </pages> <year> 1992. </year>
Reference-contexts: This is typically implemented using a set of rules derived from the grammatical structure of the language. In practice, these rules are far from perfect (it has been reported <ref> [5] </ref> for instance that documents from mathematics and anesthesiology were often confused because of 3 the occurrence of "number" in both fields, with different meanings of course). We have discussed above how retrieval systems may treat multiple search terms in various ways. <p> static (as above); instead, clustering partitions the documents into related clusters each of which may be described, perhaps, by the terms most frequently occurring in the cluster. (This may not always be the most intuitive representation from the standpoint of the user, but preliminary experience with the technique is encouraging <ref> [5] </ref>.) Further, the clustering can change, as new documents are added (and old ones deleted); thus in a corpus of news articles, the clustering may change as the focus of the news changes. Such dynamic clustering, again, could be hierarchical [5]. <p> the user, but preliminary experience with the technique is encouraging <ref> [5] </ref>.) Further, the clustering can change, as new documents are added (and old ones deleted); thus in a corpus of news articles, the clustering may change as the focus of the news changes. Such dynamic clustering, again, could be hierarchical [5]. We will explore this idea in greater detail in x 3 below. The final basic idea we discuss is that of relevance feedback. Consider a user who enters a query into a document retrieval system. <p> The difficulty in practice, though, is finding the right criteria for clustering | if two documents are "close" to each other from the user's standpoint, what criterion for clustering keeps them together in automated clustering? There has been considerable discussion of this in the literature <ref> [5, 23] </ref>, and the answers are not very clear. This is related to the issue of the "right" distance measure for measuring proximity between points in the vector space, but automated clustering raises additional issues. How does one ensure that the clusters remain reasonably balanced? Goal 5.
Reference: [6] <author> S. Deerwester, S. T. Dumais, T.K. Landauer, G.W. Furnas, and R.A. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the Society for Information Science, </journal> <volume> 41(6), </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: However, the literature <ref> [1, 6, 7, 8] </ref> goes beyond suggesting LSI as merely a dimension-reduction technique: in fact, the reported experience is that LSI tends to bring together documents that are semantically related.
Reference: [7] <author> S.T. Dumais, G.W. Furnas, T.K. Landauer and S. Deerwester. </author> <title> Using latent semantic analysis to improve information retrieval. </title> <booktitle> In Proceedings of CHI'88: Conference on Human Factors in Computing, </booktitle> <address> New York: </address> <publisher> ACM, </publisher> <pages> 281-285, </pages> <year> 1988. </year>
Reference-contexts: Queries are also projected and processed in this low-dimensional space. This results not only in great savings in storage and query time (at the expense of some considerable preprocessing), but also, according to empirical evidence reported in the literature, to improved information retrieval <ref> [1, 7, 8] </ref>. Let A be an n fi m matrix as before; let the rank of A be r. Let the singular values of A be 1 2 : : : r (not necessarily distinct), i.e., 2 1 ; 2 r are the eigenvalues of AA T . <p> However, the literature <ref> [1, 6, 7, 8] </ref> goes beyond suggesting LSI as merely a dimension-reduction technique: in fact, the reported experience is that LSI tends to bring together documents that are semantically related.
Reference: [8] <author> S.T. Dumais. </author> <title> Improving the retrieval of information from external sources. Behavior Research Methods, </title> <journal> Instruments and Computers, </journal> <volume> 23(2), </volume> <pages> 229-236, </pages> <year> 1991. </year>
Reference-contexts: Queries are also projected and processed in this low-dimensional space. This results not only in great savings in storage and query time (at the expense of some considerable preprocessing), but also, according to empirical evidence reported in the literature, to improved information retrieval <ref> [1, 7, 8] </ref>. Let A be an n fi m matrix as before; let the rank of A be r. Let the singular values of A be 1 2 : : : r (not necessarily distinct), i.e., 2 1 ; 2 r are the eigenvalues of AA T . <p> However, the literature <ref> [1, 6, 7, 8] </ref> goes beyond suggesting LSI as merely a dimension-reduction technique: in fact, the reported experience is that LSI tends to bring together documents that are semantically related.
Reference: [9] <author> R. Fagin and L. Stockmeyer. </author> <title> Relaxing the triangle inequality in pattern matching. </title> <type> IBM Research Report RJ 10031, </type> <month> June </month> <year> 1996. </year>
Reference-contexts: The number of dimensions in these applications may range in the several hundreds. The lack of the triangle inequality further complicates Goals 3 and 4 above. For one of the distance functions in the QBIC system, Fagin and Stockmeyer <ref> [9] </ref> have established a semi-triangle inequality: given any triangle in the space, the sum of any two sides is shown to be at least (1 *) times the third. The following goal is analogous to Goals 3 and 4 above. Goal 6.
Reference: [10] <author> R. Fagin. </author> <title> Combining fuzzy information from multiple systems. </title> <booktitle> Proceedings of the 15th ACM Symp. on Principles of Database Systems, </booktitle> <address> Montreal, </address> <year> 1996, </year> <pages> pp. 216-226. </pages>
Reference-contexts: In the more sophisticated systems in development, the user may use any combination of text/image/video search. The system must then combine the results of the search by each of the query media. Fagin <ref> [10] </ref> gives some algorithms for combining such search results according a fixed combination rule. More generally, the user may at query time give a weighting with which to combine the results of such searches.
Reference: [11] <author> C. Faloutsos and D. W. Oard. </author> <title> A Survey of Information Retrieval and Filtering Methods. </title> <institution> Dept. of Computer Science, Univ. of Maryland, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: Two classic texts on LSI are the books by van Rijs-bergen [23] and by Salton [24]. Further detail on algorithmic issues can be found in the volume edited by Frakes and Baeza-Yates [13]. The survey of information retrieval by Faloutsos and Oard <ref> [11] </ref> is fairly recent; Lesk [19] gives a broad historical perspective on the evolution of the field. We next provide an annotated list of information retrieval URLs on the web; these links are active at the time of this writing.
Reference: [12] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele and P. Yanker. </author> <title> Query by image and video content: The QBIC system. </title> <journal> IEEE Computer, </journal> <volume> 28, </volume> <pages> 23-32, </pages> <year> 1995. </year>
Reference-contexts: In an image one might, for instance, record for each primary color the average density of that color; each color might become an axis in the space. More sophisticated versions of this are used in the Query By Image Content (or QBIC) system <ref> [12] </ref>, and the related Photobook system [22]. These systems allow the 5 user to specify an image query using a variety of query interfaces, and the system returns objects that to a human user appear to be close to the query. <p> The crucial words in the notion of similarity are "appear to be" and "to a human": human perception will not in general conform to a clean mathematical notion of proximity (say, the difference between RGB color vectors averaged over an image). Indeed, experience with the QBIC system <ref> [12] </ref> has shown considerable variation between simple machine representations and human perception in the color component alone. Thus the choice of the distance measure (in the vector space representation) is especially difficult and important in such systems.
Reference: [13] <author> W. Frakes and R. Baeza-Yates, </author> <title> editors. Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: Two classic texts on LSI are the books by van Rijs-bergen [23] and by Salton [24]. Further detail on algorithmic issues can be found in the volume edited by Frakes and Baeza-Yates <ref> [13] </ref>. The survey of information retrieval by Faloutsos and Oard [11] is fairly recent; Lesk [19] gives a broad historical perspective on the evolution of the field. We next provide an annotated list of information retrieval URLs on the web; these links are active at the time of this writing.
Reference: [14] <author> P. Frankl and H. Maehara. </author> <title> The Johnson-Lindenstrauss Lemma and the Sphericity of some graphs, </title> <journal> J. Comb. Theory B 44 (1988), </journal> <pages> 355-362. </pages>
Reference-contexts: This builds on theorems on random subspace projection due to Johnson and Linden-strauss <ref> [14, 18] </ref>. One deterrent to the widespread proliferation of LSI in commercial systems has been the computational cost of computing the singular value decomposition on a large corpus (such as the web). A number of attempts have been made to circumvent this bottleneck.
Reference: [15] <author> G. Golub and C. Reinsch. </author> <title> Handbook for matrix computation II, Linear Algebra. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: The rows of V k D k above are then used to represent the documents. How good is this approximation? The following well-known theorem gives us some idea. Theorem 3.1. (Eckart and Young, see <ref> [15] </ref>.) Among all n fi m matrices C of rank at most k, A k is the one that minimizes kA Ck 2 = P Therefore, LSI preserves (to the extent possible) the relative distances (and hence, presumably, the retrieval capabilities) in the term-document matrix while projecting it to a lower-dimensional
Reference: [16] <author> J. Hafner and E. Upfal. </author> <title> Nearest neighbors in high dimensions using random sampling: theory and experiments. </title> <type> Unpublished manuscript, </type> <year> 1996. </year>
Reference-contexts: Some empirical success has been observed in experiments by Hafner and Upfal <ref> [16] </ref> with the QBIC system. A robust, distribution-independent probabilistic analysis would be a good first step. Goal 4.
Reference: [17] <author> D. </author> <title> Harman. </title> <booktitle> Overview of the Fourth Text REtrieval Conference (TREC-4). Proceedings of the Fourth Text Retrieval Conference, </booktitle> <institution> National Institute of Standards and Technology, </institution> <year> 1995. </year>
Reference-contexts: of as a binary attribute: a document in the corpus is either relevant to the query, or it is not; (2) when reporting experimental research, it is common to average the precision and recall figures over a large number of queries; (3) in benchmark suites such as the TREC series <ref> [17] </ref>, it is typical to provide a corpus and a set of queries; in addition there is a set of relevance judgements from an expert panel giving, for each query, the set of documents in the corpus deemed to be relevant to that query.
Reference: [18] <author> W. B. Johnson and J. Lindenstrauss. </author> <title> Extensions of Lipshitz mapping into Hilbert space, </title> <journal> Contemp. Math. </journal> <volume> 26 (1984), </volume> <pages> 189-206. </pages>
Reference-contexts: This builds on theorems on random subspace projection due to Johnson and Linden-strauss <ref> [14, 18] </ref>. One deterrent to the widespread proliferation of LSI in commercial systems has been the computational cost of computing the singular value decomposition on a large corpus (such as the web). A number of attempts have been made to circumvent this bottleneck.
Reference: [19] <author> M. Lesk. </author> <title> The Seven Ages of Information Retrieval. Proceedings of the Conference for the 50th anniversary of As We May Think, </title> <type> 12-14, </type> <year> 1995. </year> <note> Available at the time of this writing as http://community.bellcore.com/lesk/ages/ages.html </note>
Reference-contexts: Two classic texts on LSI are the books by van Rijs-bergen [23] and by Salton [24]. Further detail on algorithmic issues can be found in the volume edited by Frakes and Baeza-Yates [13]. The survey of information retrieval by Faloutsos and Oard [11] is fairly recent; Lesk <ref> [19] </ref> gives a broad historical perspective on the evolution of the field. We next provide an annotated list of information retrieval URLs on the web; these links are active at the time of this writing. Many of the articles/books listed below may be accessed through these URLs.
Reference: [20] <author> Y. Maarek and F. Smadja. </author> <title> Full text indexing based on lexical relations. an application: </title> <booktitle> Software libraries. In Proceedings of SIGIR'89, </booktitle> <editor> N.Belkin and C.van Rijsber-gen, Eds., </editor> <publisher> ACM Press, </publisher> <pages> 198-206, </pages> <year> 1989. </year>
Reference-contexts: The formula for such combinations is typically a mixture of statistical theory and experimental tuning. One idea <ref> [20] </ref> is to consider the lexical affinity of search terms: two search terms that are adjacent in the list of search terms, for instance, score higher in a document in which they occur close to each other (say, within five words).
Reference: [21] <author> C. H. Papadimitriou, P. Raghavan, H. Tamaki and S. Vempala. </author> <title> Latent Semantic Indexing: a probabilistic 8 analysis. </title> <type> Unpublished manuscript. </type>
Reference-contexts: Despite the wealth of empirical evidence supporting such improved semantic retrieval from LSI, Theorem 3.1 above only suggests that LSI doesn't do serious damage during dimension reduction. Can one theoretically explain the observed improved retrieval of LSI? In recent work, Papadimitriou et al. <ref> [21] </ref> show that under a probabilistic model for document generation, LSI will cluster documents by topic. <p> Intriguingly, though, the web search engine Excite reportedly uses a variant of LSI, and apparently indexes tens of millions of documents. At least in theory <ref> [21] </ref>, random projection speeds up the subsequent LSI computation; in practice, though, random projection may reduce the sparseness of the document matrix to the point where LSI code for sparse matrices cannot be used. Goal 8.
Reference: [22] <author> A. Pentland, R. W. Picard and S. Sclaroff. Photo-book: </author> <title> Tools for Content-Based Manipulation of Image Databases, Proc. Storage adn Retrieval for Image and Video Databases II, </title> <booktitle> 2, SPIE, </booktitle> <pages> 34-47. </pages>
Reference-contexts: In an image one might, for instance, record for each primary color the average density of that color; each color might become an axis in the space. More sophisticated versions of this are used in the Query By Image Content (or QBIC) system [12], and the related Photobook system <ref> [22] </ref>. These systems allow the 5 user to specify an image query using a variety of query interfaces, and the system returns objects that to a human user appear to be close to the query.
Reference: [23] <editor> C. J. van Rijsbergen. </editor> <booktitle> Information Retrieval. </booktitle> <address> Butter-worths, London 1979. </address>
Reference-contexts: The difficulty in practice, though, is finding the right criteria for clustering | if two documents are "close" to each other from the user's standpoint, what criterion for clustering keeps them together in automated clustering? There has been considerable discussion of this in the literature <ref> [5, 23] </ref>, and the answers are not very clear. This is related to the issue of the "right" distance measure for measuring proximity between points in the vector space, but automated clustering raises additional issues. How does one ensure that the clusters remain reasonably balanced? Goal 5. <p> In this section we give an overview of resources for finding out about current work on information retrieval. Two classic texts on LSI are the books by van Rijs-bergen <ref> [23] </ref> and by Salton [24]. Further detail on algorithmic issues can be found in the volume edited by Frakes and Baeza-Yates [13]. The survey of information retrieval by Faloutsos and Oard [11] is fairly recent; Lesk [19] gives a broad historical perspective on the evolution of the field.
Reference: [24] <author> G. Salton. </author> <title> Automatic Text Processing. </title> <address> Reading, MA: </address> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: In this section we give an overview of resources for finding out about current work on information retrieval. Two classic texts on LSI are the books by van Rijs-bergen [23] and by Salton <ref> [24] </ref>. Further detail on algorithmic issues can be found in the volume edited by Frakes and Baeza-Yates [13]. The survey of information retrieval by Faloutsos and Oard [11] is fairly recent; Lesk [19] gives a broad historical perspective on the evolution of the field.
Reference: [25] <author> H. Samet. </author> <title> The design and analysis of spatial data structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Goal 3. To find a near (est)-neighbor search procedure in "very high" dimensions that requires (in the worst case) fewer than n distance computations. A number of solutions have been proposed for this problem in the literature <ref> [25] </ref>, but none can get below n comparisons in the worst case; in fact, we know of no method that uses fewer than n comparisons even when the inputs are drawn from an arbitrary probability distribution.
Reference: [26] <author> G.K. Zipf. </author> <title> Human Behavior and the Principle of Least Effort: an Introduction to Human Ecology. </title> <publisher> Addison-Wesley, </publisher> <year> 1949. </year>
Reference-contexts: For an algorithm to succeed at such goals, it is essential to understand the underlying corpus. Do all documents use the same underlying vocabulary? What is the distribution of terms in the vocabulary? A commonly held thesis is that terms are distributed according to Zipf's law <ref> [26] </ref>, by which the ith most frequently occurring term has probability proportional to i ff , where ff is a positive constant; for the various indexing/searching goals mentioned here, how can we exploit Zipf's law? What fraction of the terms typically identify a document reasonably well? Are there many proper nouns?
References-found: 26

