URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR96658.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Regularization of Large-scale Ill-conditioned Least Squares Problems lack of robust regularization methods for large-scale ill-conditioned
Author: Marielba Rojas L. 
Note: The  this project.  
Date: October 18, 1996  
Abstract: Our goal is to develop a regularization method for the least squares problem as a large-scale discrete ill-posed problem arising in seismic inversion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Beylkin. </author> <title> Imaging of discontinuities in the inverse scattering problem by inversion of a causal generalized Radon transform. </title> <journal> J. Math. Phys., </journal> <volume> 26 </volume> <pages> 99-108, </pages> <year> 1985. </year>
Reference-contexts: For this choice of norm, the common approach to solving this minimization problem is known as linearized inversion. Two different techniques fall under this name. The first one, known as asymptotic linearized inversion (see <ref> [1] </ref>), finds an approximate inverse of the normal operator F fl F ( fl denoting adjoint) analytically.
Reference: [2] <author> A. Bjorck. </author> <title> Numerical Methods for Least Squares Problems. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: The least squares problem has been thoroughly studied and is treated in most numerical linear algebra text books like [16] and [49], and also in specialized sources like the classic [31] and more recently in <ref> [2] </ref>. The problem is stated as follows min k Ax b k 2 where A 2 R mfin , m n and b 2 R m .
Reference: [3] <author> A. Bjorck and L. Elden. </author> <title> Methods in numerical algebra for ill-posed problems. </title> <type> Technical Report LiTH-MAT-R-33-1979, </type> <institution> Department of Mathematics, Linkoping University, </institution> <year> 1979. </year>
Reference-contexts: The methods presented here are suitable for ill-conditioned linear systems and least squares problems, the nonlinear least squares case has been studied in [54] and [11]. For a more detailed description of the methods, we refer the reader to [26]. Early surveys of regularization methods appeared in <ref> [3] </ref> and [53].
Reference: [4] <author> A. Bjorck, E. Grimme, and Paul van Dooren. </author> <title> An implicit shift bidiag-onalization algorithm for ill-posed systems. </title> <journal> BIT, </journal> <volume> 34 </volume> <pages> 510-534, </pages> <year> 1994. </year>
Reference-contexts: III. Hybrid Methods These methods combine an iterative scheme to compute the regularized solution with a strategy to compute the regularization parameter, aiming to control the number of iterations in the iterative method. The method presented in <ref> [4] </ref> belong to this class. We will discuss that approach in section 6. 6 Regularization in the large-scale case As we can see from section 5, there are very few methods for the regularization of large-scale ill-conditioned problems. The ones discussed here are the hybrid method presented in [4] (BGvD), the <p> presented in <ref> [4] </ref> belong to this class. We will discuss that approach in section 6. 6 Regularization in the large-scale case As we can see from section 5, there are very few methods for the regularization of large-scale ill-conditioned problems. The ones discussed here are the hybrid method presented in [4] (BGvD), the method for the quadratically constrained least squares problem presented in [17] (GvM) and the Conjugate Gradient method [28] applied to the normal equations (CGLS). The first method is the only complete regularization method in the sense that it computes both the regularization parameter and the regularized solution.
Reference: [5] <author> D. Calvetti, L. Reichel, and D.C. Sorensen. </author> <title> An implicitly restarted Lanczos method for large symmetric eigenvalue problems. </title> <journal> ETNA, </journal> <volume> 2 </volume> <pages> 1-21, </pages> <year> 1994. </year>
Reference-contexts: The known methods for the large-scale TRS were given by Sorensen in [47], Rendl and Wolkowicz in [41] and Santos and Sorensen in [43]. In [47] the TRS is recasted as a parameterized eigenvalue problem. The method relies on the Implicitly Restarted Lanczos Method (IRLM) ([46], <ref> [5] </ref>) to solve a sequence of large symmetric eigenvalues problems. The parameter is updated by an interpolation scheme and there is a separate treatment of a special case known as the hard case.
Reference: [6] <author> T.F. Chan, J.A. Olkin, and D.W. Cooley. </author> <title> Solving quadratically constrained least squares using black box solvers. </title> <journal> BIT, </journal> <volume> 32 </volume> <pages> 481-495, </pages> <year> 1992. </year>
Reference-contexts: The use of methods for the TRS in the regularization context has been suggested in [41] and [47]. In [32] a new trust region strategy is presented and applied to a regularization problem as an example. That particular strategy is not suitable for large-scale problems. <ref> [6] </ref> presents a method for the TRS for the special case of ill-conditioned least squares problems. The method works under the assumption that &lt;k A y b k 2 .
Reference: [7] <author> N. Clinthorne, T. Pan, P. Chiao, W. Rogers, and J. Stamos. </author> <title> Preconditioning methods for improved convergence rates in iterative reconstruction. </title> <journal> IEEE Trans. Med. Imag., </journal> <volume> 12(1) </volume> <pages> 78-83, </pages> <year> 1993. </year>
Reference-contexts: This makes preconditioning for ill-conditioned problems a very difficult area and the object of current research. For problems where large and small parts of the spectrum can be identified, it has been possible to build efficient preconditioners as the ones reported in <ref> [7] </ref> and [36]. These problems are characterized by having a highly structured coefficient matrix (Toeplitz matrix) for which circulant preconditioners have proved to be successful.
Reference: [8] <author> J.E. Dennis and H.H. Mei. </author> <title> Two new unconstrained optimization algorithms which use function and gradient values. </title> <journal> J. Opt. Theory Appl., </journal> <volume> 28 </volume> <pages> 453-482, </pages> <year> 1979. </year>
Reference-contexts: This problem has been treated extensively in the context of the trust-region globalization strategy for optimization methods. In the small to medium scale setting, there are efficient methods to solve the problem. Such methods include Powell's dogleg method [40], Dennis and Mei's double dogleg method <ref> [8] </ref> and More and Sorensen's method [34]. The latter one being the method of choice in most applications. 24 A complete survey of methods was given by More in [33]. In that refer-ence, Steihaug's method [48] is also mentioned.
Reference: [9] <author> L. Elden. </author> <title> Algorithms for the regularization of ill-conditioned least squares problems. </title> <journal> BIT, </journal> <volume> 17 </volume> <pages> 134-145, </pages> <year> 1977. </year>
Reference-contexts: If the problem is given in general form (L 6= I) it is possible to transform it to the standard form (L = I) by means of the algorithms given in <ref> [9] </ref> and [26]. I. Methods for computing the regularized solution 1. Direct Methods (a) Tikhonov Regularization. The method solves the following problem minfk Ax b k 2 2 + 2 k x k 2 where is the regularization parameter.
Reference: [10] <author> H. W. Engl. </author> <title> Regularization methods for the stable solution of inverse problems. </title> <journal> Surveys Math. Ind., </journal> <volume> 3 </volume> <pages> 71-143, </pages> <year> 1993. </year>
Reference-contexts: The singular value distribution for a problem of this type (problem heat from [25]) can be observed in figure 2. An extensive coverage of these two classes of ill-conditioned problems was presented in [26]. A detailed description of applications can be found in [20], <ref> [10] </ref> and the references therein. A specific application from geophysics will be described in this work.
Reference: [11] <author> J. Eriksson. </author> <title> Optimization and regularization of nonlinear least squares problems. </title> <type> Doctoral thesis, </type> <institution> Ume-a University, Ume-a, Sweden, </institution> <year> 1996. </year>
Reference-contexts: In section 6 we discussed methods for the large-scale case in detail. The methods presented here are suitable for ill-conditioned linear systems and least squares problems, the nonlinear least squares case has been studied in [54] and <ref> [11] </ref>. For a more detailed description of the methods, we refer the reader to [26]. Early surveys of regularization methods appeared in [3] and [53].
Reference: [12] <author> W. Gander. </author> <title> Least squares with a quadratic constraint. </title> <journal> Numer. Math., </journal> <volume> 36 </volume> <pages> 291-307, </pages> <year> 1981. </year>
Reference-contexts: In this approach, the following problems are considered min k Ax b k 2 (10) min k x k 2 (11) where and ff are the regularization parameters. The theoretical aspects of problems (10) and (11) were discussed in <ref> [12] </ref>. Problem (10) is known as a trust region subproblem in the optimization literature. We will return to methods to solve this problem in section 7. Methods to solve (10) based on the SVD and the QR decomposition are presented in [16, Ch. 12].
Reference: [13] <author> D. Gay. </author> <title> Computing optimal locally constrained steps. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 2(2) </volume> <pages> 186-197, </pages> <year> 1981. </year>
Reference-contexts: Two important facts about the TRS are * There is always a solution, since a continuous functional is being min imized on a compact set. * There exists a characterization of the global solutions. This is a re-markeable result obtained independently by Gay <ref> [13] </ref> and Sorensen [45]. It is contained in lemma 7.1. This problem has been treated extensively in the context of the trust-region globalization strategy for optimization methods. In the small to medium scale setting, there are efficient methods to solve the problem.
Reference: [14] <author> D. A. Girard. </author> <title> A fast `Monte-Carlo Cross-Validation' procedure for large least squares problems with noisy data. </title> <journal> Numer. Math., </journal> <volume> 56 </volume> <pages> 1-23, </pages> <year> 1989. </year>
Reference-contexts: It is therefore necessary to estimate the regularization parameter k (i.e. where to stop the iteration) very accurately. Two options mentioned in [26] are the L-curve criterion and the Monte Carlo Cross-Validation procedure from <ref> [14] </ref>. For some problems arising in medical imaging (see [36] for example), all the singular vectors of interest converge first and therefore the stopping criterion suggested above can be used.
Reference: [15] <author> G. Golub, M. Heath, and G. Wahba. </author> <title> Generalized cross-validation as a method for choosing a good Ridge parameter. </title> <journal> Technometrics, </journal> <volume> 21(2) </volume> <pages> 215-223, </pages> <year> 1979. </year>
Reference-contexts: The idea behind this criterion is that we cannot expect more accuracy in the approximate solution than the one present in the data. This method is attributed to Morozov [35]. 2. Generalized Cross-Validation (GCV). This method was presented in <ref> [15] </ref> and discussed also in [56]. The idea of using cross-validation to compute the regularization parameter is the following.
Reference: [16] <author> G. Golub and Ch. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The John Hopkins University Press, </publisher> <address> Baltimore, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: The least squares problem has been thoroughly studied and is treated in most numerical linear algebra text books like <ref> [16] </ref> and [49], and also in specialized sources like the classic [31] and more recently in [2]. The problem is stated as follows min k Ax b k 2 where A 2 R mfin , m n and b 2 R m . <p> We define this number with respect to the l 2 norm as 2 (A) = n The sensitivity of the least squares solution is measured by either the condi-tion number in the zero residual case or the square of the condition number in the nonzero residual case (see <ref> [16, Ch. 5] </ref>). This implies that when A is ill-conditioned, meaning that 2 (A) is large, the least squares solution will be unstable, i.e. very sensitive to perturbations in the data A; b. <p> Replacing A in (4) by its SVD yields x = V y U T b r X u T i where y denotes the pseudoinverse as it is defined in <ref> [16] </ref> and r is the rank of A. <p> Problem (10) is known as a trust region subproblem in the optimization literature. We will return to methods to solve this problem in section 7. Methods to solve (10) based on the SVD and the QR decomposition are presented in <ref> [16, Ch. 12] </ref>. A method for problem (10) in the large-scale case was presented in [17] and will be discussed in section 6. In problem (11) the parameter ff can be interpreted as the noise level in the data. (d) Other methods. <p> solve the least squares problems is to apply the Conjugate Gradient method to the normal equations A T Ax = A T b An implementation of such method should avoid forming the matrix A T A since doing so may introduce large rounding errors as it was pointed out in <ref> [16, example 5.3.2 on p.225] </ref>. The resulting method is known as Conjugate Gradient on the Normal Equations (CGNR, CGLS). The method has been used successfully to solve the least squares problems for noisy data due to an intrinsic regularization effect of the iteration. <p> With respect to the solution of the eigenvalue problems, it is known that the Lanczos method might perform poorly when applied to matrices with clustered eigenvalues (see <ref> [16, Ch. 9] </ref>). We saw before that this kind of matrices is likely to arise in the ill-conditioned case. 7.1.2.4 Addressing the difficulties We propose to investigate the following options to try to overcome the difficulties in x7.1.2.3. 1.
Reference: [17] <author> G. H. Golub and U. von Matt. </author> <title> Quadratically constrained least squares and quadratic problems. </title> <journal> Numer. Math., </journal> <volume> 59 </volume> <pages> 561-580, </pages> <year> 1991. </year>
Reference-contexts: We will return to methods to solve this problem in section 7. Methods to solve (10) based on the SVD and the QR decomposition are presented in [16, Ch. 12]. A method for problem (10) in the large-scale case was presented in <ref> [17] </ref> and will be discussed in section 6. In problem (11) the parameter ff can be interpreted as the noise level in the data. (d) Other methods. We mention mollifier methods where the regularized solution x reg is computed as x reg = A # b. <p> The ones discussed here are the hybrid method presented in [4] (BGvD), the method for the quadratically constrained least squares problem presented in <ref> [17] </ref> (GvM) and the Conjugate Gradient method [28] applied to the normal equations (CGLS). The first method is the only complete regularization method in the sense that it computes both the regularization parameter and the regularized solution.
Reference: [18] <author> J. </author> <title> Hadamard. Lectures on Cauchy's problem in linear partial differential equations. </title> <publisher> Yale University Press, </publisher> <address> New Haven, </address> <year> 1923. </year>
Reference-contexts: For other kinds of ill-conditioned problems, these methods are not suitable and we must use other techniques like iterative refinement, extended precision iterative refinement or preconditioning for the large-scale case. In this project we focus on the regularization of discrete ill-posed problems. 4 3 Application In 1923, Hadamard <ref> [18] </ref> introduced the concept of a well-posed and ill-posed problem. Ill-posed problems are those for which there is either no solution, or the solution is not unique, or the solution does not depend continuously on the data.
Reference: [19] <author> M. Hanke. </author> <title> Accelerated Landweber iterations for the solution of ill-posed equations. </title> <journal> Numer. Math., </journal> <volume> 60 </volume> <pages> 341-373, </pages> <year> 1991. </year>
Reference-contexts: This method in its original form is not used in practice since it is not very efficient. A modification has been proposed in <ref> [19] </ref> to accelerate convergence. (b) Conjugate Gradient on the Normal Equations (CGLS). This has been the method of choice for large-scale ill-conditioned least squares problems. We will discuss the method in section 6.
Reference: [20] <author> M. Hanke and P. C. Hansen. </author> <title> Regularization methods for large-scale problems. </title> <journal> Surveys Math. Ind., </journal> <volume> 3 </volume> <pages> 253-315, </pages> <year> 1994. </year>
Reference-contexts: The singular value distribution for a problem of this type (problem heat from [25]) can be observed in figure 2. An extensive coverage of these two classes of ill-conditioned problems was presented in [26]. A detailed description of applications can be found in <ref> [20] </ref>, [10] and the references therein. A specific application from geophysics will be described in this work. <p> A modification has been proposed in [19] to accelerate convergence. (b) Conjugate Gradient on the Normal Equations (CGLS). This has been the method of choice for large-scale ill-conditioned least squares problems. We will discuss the method in section 6. Other alternatives like the --methods referenced in <ref> [20] </ref> are still too restrictive to be used for practical problems. II. Methods for computing the regularization parameter Let us recall the multi-objective purpose of regularization, i.e. minimize the residual norm while minimizing the effect of perturbations in the data. <p> Thus, in the ill-conditioned case we should precondition only the large part of the spectrum and leave the small part untouched. This fact has been observed before in <ref> [20] </ref>, [26, Ch. 5], [21] and [36]. In general, it is not possible to distinguish a priori between the large and small parts of the spectrum of a matrix. This makes preconditioning for ill-conditioned problems a very difficult area and the object of current research.
Reference: [21] <author> M. Hanke, J. Nagy, and R. Plemmons. </author> <title> Preconditioned iterative regularization for ill-posed problems. </title> <editor> In L. Reichel, A. Ruttan, and R. Varga, editors, </editor> <title> Numerical Linear Algebra, </title> <address> Berlin, </address> <year> 1992. </year> <editor> Walter de Gruyter. </editor> <booktitle> Proceedings of the Conference in Numerical Linear Algebra and Scientific Computation, </booktitle> <address> Kent (Ohio), USA. </address>
Reference-contexts: Thus, in the ill-conditioned case we should precondition only the large part of the spectrum and leave the small part untouched. This fact has been observed before in [20], [26, Ch. 5], <ref> [21] </ref> and [36]. In general, it is not possible to distinguish a priori between the large and small parts of the spectrum of a matrix. This makes preconditioning for ill-conditioned problems a very difficult area and the object of current research.
Reference: [22] <author> P. C. Hansen. </author> <title> Analysis of discrete ill-posed problems by means of the L-curve. </title> <journal> SIAM Review, </journal> <volume> 34(4) </volume> <pages> 561-580, </pages> <year> 1992. </year>
Reference-contexts: Figure 5 shows the L-curve (and its corner) for problem heat from [25]. In this example, the curve is based on the values of the regularization parameter in Tikhonov regularization. The use of this curve to estimate the regularization parameter has been studied in <ref> [22] </ref> and [27]. The idea is to interpolate the curve in order to estimate the "corner". The L-curve method performs better than the GCV method when the noise in the data is correlated and comparably well for white noise.
Reference: [23] <author> P.C. Hansen. </author> <title> Computation of the Singular Value Expansion. </title> <journal> Computing, </journal> <volume> 40 </volume> <pages> 185-199, </pages> <year> 1988. </year>
Reference-contexts: A discretization of F usually results in a matrix whose singular values are approximations of the singular values of the operator and whose singular vectors give information on the singular functions. We refer the reader to <ref> [23] </ref>.
Reference: [24] <author> P.C. Hansen. </author> <title> The Discrete Picard Condition for discrete ill-posed problems. </title> <journal> BIT, </journal> <volume> 30 </volume> <pages> 658-672, </pages> <year> 1990. </year>
Reference-contexts: The need of this condition in order for most regularization methods to compute good approximate solutions was fully justified in <ref> [24] </ref>.
Reference: [25] <author> P.C. Hansen. </author> <title> Regularization Tools: a MATLAB package for analysis and solution of discrete ill-posed problems. </title> <journal> Numer. Algo., </journal> <volume> 6 </volume> <pages> 1-35, </pages> <year> 1994. </year> <note> Available from ftp.uni-c.dk in the directory uni-c/unipch/matlab/ReguTools. 43 </note>
Reference-contexts: The singular value distribution for a problem of this type (problem heat from <ref> [25] </ref>) can be observed in figure 2. An extensive coverage of these two classes of ill-conditioned problems was presented in [26]. A detailed description of applications can be found in [20], [10] and the references therein. A specific application from geophysics will be described in this work. <p> The name comes from the fact that this curve is L-shaped. The optimal regularization parameter gives a solution that lies around the "corner" of the curve. Figure 5 shows the L-curve (and its corner) for problem heat from <ref> [25] </ref>. In this example, the curve is based on the values of the regularization parameter in Tikhonov regularization. The use of this curve to estimate the regularization parameter has been studied in [22] and [27]. The idea is to interpolate the curve in order to estimate the "corner". <p> Moreover, we expect that the IRLM will compute those eigenvalues efficiently since there is a small number of them and they are separate. This idea is yet to be tried. 7.1.2.2 Examples All the tests refer to problem heat from the Regularization Tools package <ref> [25] </ref>. The problem is an inverse heat equation and is very ill-conditioned. The dimensions are m = n = 50. The right hand side was taken as b = b + *r, where b is the exact data vector returned by routine heat from [25]. <p> heat from the Regularization Tools package <ref> [25] </ref>. The problem is an inverse heat equation and is very ill-conditioned. The dimensions are m = n = 50. The right hand side was taken as b = b + *r, where b is the exact data vector returned by routine heat from [25]. The perturbation * was either 0 or 0.001, r is a random vector with components uniformly distributed in (0,1). The experiments were carried out in MATLAB on a SUN SPARC station IPX, with IEEE standard double precision arithmetic. Machine precision is of order 10 16 .
Reference: [26] <author> P.C. Hansen. </author> <title> Rank-Deficient and Discrete Ill-Posed Problems. </title> <type> Doctoral thesis, </type> <institution> UNI*C, Technical University of Denmark, Lyngby, Denmark, </institution> <year> 1995. </year>
Reference-contexts: There are problems however, for which the ill-conditioning is an inherent feature and for which it is not possible to find a nearby problem with a well-conditioned coefficient matrix; this is the case for discrete ill-posed problems. According to <ref> [26] </ref>, we can distinguish two main classes of ill-conditioned problems based on the properties of their coefficient matrices: 1. Rank-deficient Problems. <p> The singular value distribution for a problem of this type (problem heat from [25]) can be observed in figure 2. An extensive coverage of these two classes of ill-conditioned problems was presented in <ref> [26] </ref>. A detailed description of applications can be found in [20], [10] and the references therein. A specific application from geophysics will be described in this work. <p> The methods presented here are suitable for ill-conditioned linear systems and least squares problems, the nonlinear least squares case has been studied in [54] and [11]. For a more detailed description of the methods, we refer the reader to <ref> [26] </ref>. Early surveys of regularization methods appeared in [3] and [53]. A common 12 framework for the study of numerical regularization methods is proposed in [26] and [27]. 5.1 Methods for Rank-deficient Problems We recall from section 1 that the coefficient matrix A of a rank-deficient problem has a well-determined gap <p> For a more detailed description of the methods, we refer the reader to <ref> [26] </ref>. Early surveys of regularization methods appeared in [3] and [53]. A common 12 framework for the study of numerical regularization methods is proposed in [26] and [27]. 5.1 Methods for Rank-deficient Problems We recall from section 1 that the coefficient matrix A of a rank-deficient problem has a well-determined gap in the singular value spectrum, which makes possible to determine the numerical rank of the matrix. <p> If the problem is given in general form (L 6= I) it is possible to transform it to the standard form (L = I) by means of the algorithms given in [9] and <ref> [26] </ref>. I. Methods for computing the regularized solution 1. Direct Methods (a) Tikhonov Regularization. The method solves the following problem minfk Ax b k 2 2 + 2 k x k 2 where is the regularization parameter. <p> We mention mollifier methods where the regularized solution x reg is computed as x reg = A # b. A # is a special matrix known as the resolution matrix. Other methods are dis cussed in <ref> [26] </ref>. 2. Iterative Methods. These methods are intended for large-scale problems for which factorizations are not affordable. For these problems, the coefficient matrix is usually not available explicitly. <p> It is therefore necessary to estimate the regularization parameter k (i.e. where to stop the iteration) very accurately. Two options mentioned in <ref> [26] </ref> are the L-curve criterion and the Monte Carlo Cross-Validation procedure from [14]. For some problems arising in medical imaging (see [36] for example), all the singular vectors of interest converge first and therefore the stopping criterion suggested above can be used. <p> Thus, in the ill-conditioned case we should precondition only the large part of the spectrum and leave the small part untouched. This fact has been observed before in [20], <ref> [26, Ch. 5] </ref>, [21] and [36]. In general, it is not possible to distinguish a priori between the large and small parts of the spectrum of a matrix. This makes preconditioning for ill-conditioned problems a very difficult area and the object of current research.
Reference: [27] <author> P.C. Hansen and D.P. O'Leary. </author> <title> The use of the L-curve in the regularization of discrete ill-posed problems. </title> <journal> SIAM J. Sci. Comp., </journal> <volume> 14(6) </volume> <pages> 1487-1503, </pages> <year> 1993. </year>
Reference-contexts: For a more detailed description of the methods, we refer the reader to [26]. Early surveys of regularization methods appeared in [3] and [53]. A common 12 framework for the study of numerical regularization methods is proposed in [26] and <ref> [27] </ref>. 5.1 Methods for Rank-deficient Problems We recall from section 1 that the coefficient matrix A of a rank-deficient problem has a well-determined gap in the singular value spectrum, which makes possible to determine the numerical rank of the matrix. <p> Figure 5 shows the L-curve (and its corner) for problem heat from [25]. In this example, the curve is based on the values of the regularization parameter in Tikhonov regularization. The use of this curve to estimate the regularization parameter has been studied in [22] and <ref> [27] </ref>. The idea is to interpolate the curve in order to estimate the "corner". The L-curve method performs better than the GCV method when the noise in the data is correlated and comparably well for white noise. <p> The advantage of the L-curve criterion over the GCV in the presence of correlated noise seems to come from the fact that the L-curve method uses information on both the residual norm and the solution norm, while the GCV method uses information on the residual norm only (see <ref> [27] </ref>). Both the L-curve and the GCV methods are a posteriori methods 17 since they need several approximate solutions in order to estimate the regularization parameter. III.
Reference: [28] <author> M. R. Hestenes and E. </author> <title> Stiefel. Methods of conjugate gradients for solving linear systems. </title> <institution> J. Res. Nat. Bur. Standards., B49:409-436, </institution> <year> 1952. </year>
Reference-contexts: The ones discussed here are the hybrid method presented in [4] (BGvD), the method for the quadratically constrained least squares problem presented in [17] (GvM) and the Conjugate Gradient method <ref> [28] </ref> applied to the normal equations (CGLS). The first method is the only complete regularization method in the sense that it computes both the regularization parameter and the regularized solution.
Reference: [29] <author> L. Kaufman and A. Neumaier. </author> <title> Image reconstruction through regularization by envelope guided Conjugate Gradients. </title> <type> Technical Report 4-14, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1994. </year>
Reference-contexts: Regularization can also be regarded as a multi-objective optimization problem where one tries to balance the quality of the approximation and the effect of perturbations on the solution (this approach is taken in <ref> [29] </ref>). Associated with any regularization method there is a parameter, namely the regularization parameter, which controls how much perturbation effect is allowed. Therefore, a complete regularization method has two aspects: the computation of the regularized solution and the computation of the regularization parameter.
Reference: [30] <author> J. T. King. </author> <title> Multilevel algorithms for ill-posed problems. </title> <journal> Numer. Math., </journal> <volume> 61 </volume> <pages> 311-334, </pages> <year> 1992. </year>
Reference-contexts: These problems are characterized by having a highly structured coefficient matrix (Toeplitz matrix) for which circulant preconditioners have proved to be successful. Multilevel preconditioners for more general problems have been proposed in <ref> [30] </ref>. 23 7 Proposed Approach In this section we present our approach for regularizing large-scale ill-conditioned least squares problems and propose a strategy for computing the regularized solution.
Reference: [31] <author> C. L. Lawson and R. J. Hanson. </author> <title> Solving Least Squares Problems. </title> <booktitle> Classics in Applied Mathematics. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1995. </year>
Reference-contexts: The least squares problem has been thoroughly studied and is treated in most numerical linear algebra text books like [16] and [49], and also in specialized sources like the classic <ref> [31] </ref> and more recently in [2]. The problem is stated as follows min k Ax b k 2 where A 2 R mfin , m n and b 2 R m .
Reference: [32] <author> J. M. Martnez and S. A. Santos. </author> <title> A trust region strategy for minimization on arbitrary domains. </title> <note> To appear in Mathematical Programming, </note> <year> 1995. </year>
Reference-contexts: We proposed to use the method for large-scale trust region subproblems (TRS method) presented in [43] to compute a solution for the constrained least squares problem (12). The use of methods for the TRS in the regularization context has been suggested in [41] and [47]. In <ref> [32] </ref> a new trust region strategy is presented and applied to a regularization problem as an example. That particular strategy is not suitable for large-scale problems. [6] presents a method for the TRS for the special case of ill-conditioned least squares problems.
Reference: [33] <author> J. J. </author> <title> More. Recent developments in algorithms and software for trust region methods. </title> <editor> In A. Bachem, M. Grotschel, and B. Korte, editors, </editor> <booktitle> Mathematical Programming Bonn 1982 The state of the art, </booktitle> <pages> pages 258-287. </pages> <publisher> Mathematical Programming Society, </publisher> <address> Bonn, </address> <year> 1983. </year>
Reference-contexts: Such methods include Powell's dogleg method [40], Dennis and Mei's double dogleg method [8] and More and Sorensen's method [34]. The latter one being the method of choice in most applications. 24 A complete survey of methods was given by More in <ref> [33] </ref>. In that refer-ence, Steihaug's method [48] is also mentioned. This method is suitable for the large-scale case and gives nearly optimal solutions when the Hessian of the quadratic is positive semidefinite. Methods for the general case were not available until recently.
Reference: [34] <author> J. J. More and D. C. Sorensen. </author> <title> Computing a trust region step. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 4(3) </volume> <pages> 553-572, </pages> <year> 1983. </year>
Reference-contexts: In the small to medium scale setting, there are efficient methods to solve the problem. Such methods include Powell's dogleg method [40], Dennis and Mei's double dogleg method [8] and More and Sorensen's method <ref> [34] </ref>. The latter one being the method of choice in most applications. 24 A complete survey of methods was given by More in [33]. In that refer-ence, Steihaug's method [48] is also mentioned. <p> Proof: See [45] 2 The solutions of (13) can occur on the boundary or in the interior of the trust region fxj k x k 2 g. Interior solutions exist if and only if k x k 2 &lt; and H is positive definite (see <ref> [34] </ref>). In this case, the Lagrange multiplier is zero and the solution is given by x = H 1 g. There is yet one more special situation that may arise only when g is orthogonal to S 1 and H is not positive definite.
Reference: [35] <author> V. A. Morozov. </author> <title> On the solution of functional equations by the method of regularization. </title> <journal> Sov. Math. Doklady, </journal> <volume> 167(3) </volume> <pages> 414-417, </pages> <year> 1966. </year>
Reference-contexts: The idea behind this criterion is that we cannot expect more accuracy in the approximate solution than the one present in the data. This method is attributed to Morozov <ref> [35] </ref>. 2. Generalized Cross-Validation (GCV). This method was presented in [15] and discussed also in [56]. The idea of using cross-validation to compute the regularization parameter is the following.
Reference: [36] <author> J. Nagy, V. Pauca, R. Plemmons, and T. Torgersen. </author> <title> Space-varying restoration of optical images. </title> <type> Technical report, </type> <institution> Dept. of Mathematics and Computer Science, Wake Forest University, Winsdon-Salen, North Carolina, </institution> <year> 1996. </year>
Reference-contexts: It is therefore necessary to estimate the regularization parameter k (i.e. where to stop the iteration) very accurately. Two options mentioned in [26] are the L-curve criterion and the Monte Carlo Cross-Validation procedure from [14]. For some problems arising in medical imaging (see <ref> [36] </ref> for example), all the singular vectors of interest converge first and therefore the stopping criterion suggested above can be used. The difficulty in those applications comes from the fact that the number of large singular values is large, requiring the use of efficient preconditioners to accelerate convergence. <p> Thus, in the ill-conditioned case we should precondition only the large part of the spectrum and leave the small part untouched. This fact has been observed before in [20], [26, Ch. 5], [21] and <ref> [36] </ref>. In general, it is not possible to distinguish a priori between the large and small parts of the spectrum of a matrix. This makes preconditioning for ill-conditioned problems a very difficult area and the object of current research. <p> This makes preconditioning for ill-conditioned problems a very difficult area and the object of current research. For problems where large and small parts of the spectrum can be identified, it has been possible to build efficient preconditioners as the ones reported in [7] and <ref> [36] </ref>. These problems are characterized by having a highly structured coefficient matrix (Toeplitz matrix) for which circulant preconditioners have proved to be successful.
Reference: [37] <author> J. Olsen, P. Jorgensen, and J. Simons. </author> <title> Passing the one-billion limit in full configuration interaction (fci) calculations. </title> <journal> Chem. Phys. Letters, </journal> <volume> 169(6) </volume> <pages> 463-472, </pages> <year> 1990. </year>
Reference-contexts: The harmonic Ritz values converge faster than the Ritz values to the smallest eigenvalues of a matrix. * Develop a hybrid eigenvalue method using the method proposed in <ref> [37] </ref> (Olsen's method) to compute the smallest eigenpair of the bordered matrix and the IRLM to compute an initial guess for the eigenvector.
Reference: [38] <author> C. C. Paige and M. A. Saunders. </author> <title> LSQR: An algorithm for sparse linear equations and sparse least squares. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 8(1) </volume> <pages> 43-71, </pages> <year> 1982. </year>
Reference-contexts: The method uses an Implicitly Restarted Lanczos Bidiagonalization process to compute a sequence of approximations to the left and right singular vectors of A. The implicit restart technique from [46] is adapted to the Lanc-zos Bidiagonalization process <ref> [38] </ref> and zero shifts are used to filter out small singular values. Reorthogonalization of the Lanczos vectors is carried out at every step of the Lanczos process. <p> The method uses the Lanczos Bidiagonalization process <ref> [38] </ref> performed on A and the Cholesky factorization of small matrices, to compute a series of function pairs (L k (); U k ()) that bound the secular function f () = (A T b) T [(A T A + I) y ] 2 (A T b) A zero-finding procedure is
Reference: [39] <author> B.N. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: Note that for any value of ff, the eigenvalues of B ff 1 (ff); 2 (ff); : : : ; n+1 (ff) satisfy 1 (ff) ffi 1 2 (ff) ffi 2 : : : ffi n n+1 (ff) by the Cauchy interlacing theorem (see <ref> [39] </ref>). If we now define '(x) as '(x) = 2 = 2 i.e. '(x) is a vertical translation of (x) in (13) and therefore, both functions will have the same minimizers.
Reference: [40] <author> M.J.D. Powell. </author> <title> A new algorithm for unconstrained optimization. </title> <editor> In J.B. Rosen, O.L. Mangasarian, and K. Ritter, editors, </editor> <title> Nonlinear Programming. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: It is contained in lemma 7.1. This problem has been treated extensively in the context of the trust-region globalization strategy for optimization methods. In the small to medium scale setting, there are efficient methods to solve the problem. Such methods include Powell's dogleg method <ref> [40] </ref>, Dennis and Mei's double dogleg method [8] and More and Sorensen's method [34]. The latter one being the method of choice in most applications. 24 A complete survey of methods was given by More in [33]. In that refer-ence, Steihaug's method [48] is also mentioned.
Reference: [41] <author> F. Rendl and H. Wolkowicz. </author> <title> A semidefinite framework for trust region subproblems with applications to large scale minimization. </title> <type> Technical Report CORR Report 94-32, </type> <institution> Department of Combinatorics and Optimization, University of Waterloo, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The known methods for the large-scale TRS were given by Sorensen in [47], Rendl and Wolkowicz in <ref> [41] </ref> and Santos and Sorensen in [43]. In [47] the TRS is recasted as a parameterized eigenvalue problem. The method relies on the Implicitly Restarted Lanczos Method (IRLM) ([46], [5]) to solve a sequence of large symmetric eigenvalues problems. <p> The method relies on the Implicitly Restarted Lanczos Method (IRLM) ([46], [5]) to solve a sequence of large symmetric eigenvalues problems. The parameter is updated by an interpolation scheme and there is a separate treatment of a special case known as the hard case. In <ref> [41] </ref> the same recasting as in [47] is used but the strategy for updating the parameter is a dual simplex method in the standard case and a primal simplex method in the hard case. The eigenvalue problems are solved with a block-Lanczos procedure. <p> The eigenvalue problems are solved with a block-Lanczos procedure. In [43] the recasting of [47] is used again, but the interpolation scheme is different and there is a unified treatment of all cases. A small advantage of this method over the one in <ref> [41] </ref> is reported in [43]. We proposed to use the method for large-scale trust region subproblems (TRS method) presented in [43] to compute a solution for the constrained least squares problem (12). The use of methods for the TRS in the regularization context has been suggested in [41] and [47]. <p> the one in <ref> [41] </ref> is reported in [43]. We proposed to use the method for large-scale trust region subproblems (TRS method) presented in [43] to compute a solution for the constrained least squares problem (12). The use of methods for the TRS in the regularization context has been suggested in [41] and [47]. In [32] a new trust region strategy is presented and applied to a regularization problem as an example. That particular strategy is not suitable for large-scale problems. [6] presents a method for the TRS for the special case of ill-conditioned least squares problems.
Reference: [42] <institution> The Rice Inversion Project, Department of Computational and Applied Mathematics, Rice University. Houston, Texas. </institution> <note> DSO User manual and reference. Version 3.2, </note> <month> January </month> <year> 1995. </year>
Reference: [43] <author> S. A. Santos and D. C. Sorensen. </author> <title> A new matrix-free algorithm for the large-scale trust-region subproblem. </title> <type> Technical Report 95-20, </type> <institution> Department of Computational and Applied Mathematics, Rice University, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: The known methods for the large-scale TRS were given by Sorensen in [47], Rendl and Wolkowicz in [41] and Santos and Sorensen in <ref> [43] </ref>. In [47] the TRS is recasted as a parameterized eigenvalue problem. The method relies on the Implicitly Restarted Lanczos Method (IRLM) ([46], [5]) to solve a sequence of large symmetric eigenvalues problems. <p> In [41] the same recasting as in [47] is used but the strategy for updating the parameter is a dual simplex method in the standard case and a primal simplex method in the hard case. The eigenvalue problems are solved with a block-Lanczos procedure. In <ref> [43] </ref> the recasting of [47] is used again, but the interpolation scheme is different and there is a unified treatment of all cases. A small advantage of this method over the one in [41] is reported in [43]. <p> The eigenvalue problems are solved with a block-Lanczos procedure. In <ref> [43] </ref> the recasting of [47] is used again, but the interpolation scheme is different and there is a unified treatment of all cases. A small advantage of this method over the one in [41] is reported in [43]. We proposed to use the method for large-scale trust region subproblems (TRS method) presented in [43] to compute a solution for the constrained least squares problem (12). The use of methods for the TRS in the regularization context has been suggested in [41] and [47]. <p> A small advantage of this method over the one in [41] is reported in <ref> [43] </ref>. We proposed to use the method for large-scale trust region subproblems (TRS method) presented in [43] to compute a solution for the constrained least squares problem (12). The use of methods for the TRS in the regularization context has been suggested in [41] and [47]. In [32] a new trust region strategy is presented and applied to a regularization problem as an example. <p> S 1 = fqjHq = ffi 1 qg * The pseudoinverse will be denoted by y as in section 4. Before presenting the TRS method of <ref> [43] </ref>, let us first study the properties of problem (13). The following lemma contains the characterization of the solutions of (13). <p> Proof: See [45] 2 Near hard case situations (when g is nearly orthogonal to S 1 ) are very important in the context of regularization because they arise very frequently when the ill-conditioned TRS is treated. We will return to this in x7.1.2. To describe the method in <ref> [43] </ref> we first explain how the TRS (13) is re-casted as a parameterized eigenvalue problem, for this we define the following bordered matrix B ff = ff g T ! where ff is a parameter. <p> Moreover, the definition of () implies that 1 (ff) &lt; ffi 1 , unless fl 1 6= 0 (i.e. unless g ? S 1 ). Since () and 0 () are expensive to compute, they are interpolated. A two point interpolating scheme is used in <ref> [43] </ref>. The interpolant ^ () is built in such a way that convergence is guaranteed. Figures 7 and 8 show () and 0 () respectively in the standard case for a problem of dimension three with eigenvalues 2; 0:5; 2. <p> The following lemma establishes this. 28 29 Lemma 7.3 For any ff 2 R and q 2 S 1 ; fffi 1 ; (0 q T ) T g is an eigenpair of B ff if and only if g is orthogonal to S 1 . Proof: See <ref> [43] </ref> 2 A potential hard case situation is shown in figure 9. Note that in this case g ? S 1 and therefore, ffi 1 is no longer a pole of (). <p> If ~ff = ffi 1 g T p then fffi 1 ; (1 p T ) T g is an eigenpair of B ~ff . Moreover, (1 p T ) T is orthogonal to (0 q T ) T , for every q 2 S 1 . Proof: See <ref> [43] </ref> 2 30 In practice, it may occur that the eigenvector of interest has a small first component which will produce large roundoff errors if the required normalization is carried out. <p> Depending on the shape of ^ 0 ( ^ ), that may be the case even for small values of (see x7.1.2.2). The approach taken in <ref> [43] </ref> is to "ignore" the pole at ffi 1 and use the second smallest eigenvalue of B ff as an interpolation point. <p> In this way they try to approach the optimal value of ff (the one that yields an optimal pair f fl ; x fl g for the TRS) from the right. It is shown in <ref> [43] </ref> that the use of the second smallest eigenpair is not needed eventually and that this strategy does not affect the q-superlinear convergence of the method. Another important feature of the method in relation to convergence is the safeguarding of ^ and ff. <p> TRS method of <ref> [43] </ref> Assume that initial guesses for 1 ; ff are given. 1. <p> This limit was set to 50. 35 * Hard case. The hard case is verified by the criterion in lemma 5 from <ref> [43] </ref>. A tolerance * HC is needed for this test. * KKT and no progress in ff. KKT refers to the norm of the residual of the system in the Karush-Kuhn-Tucker (first order necessary conditions) for a minimizer of problem (13). <p> To overcome the difficulties associated to the computation of the smallest eigenvalue from a cluster, it would be interesting to investigate the possibility of recasting the TRS in terms of the largest eigenvalues of H. 7.1.3 Advantages and Drawbacks The use of the TRS method from <ref> [43] </ref> to compute a regularized solution for large-scale ill-conditioned problems has the following advantages and disadvantages. Advantages * Relies on matrix-vector products with either A; A T or A T A.
Reference: [44] <author> F. Santosa and W. W. Symes. </author> <title> An Analysis of Least-Squares velocity inversion. Geophysical monograph series, number 4. </title> <booktitle> Society of exploration geophysicists, </booktitle> <address> Tulsa, </address> <year> 1989. </year>
Reference-contexts: That particular strategy is not suitable for large-scale problems. [6] presents a method for the TRS for the special case of ill-conditioned least squares problems. The method works under the assumption that &lt;k A y b k 2 . Interestingly enough, in <ref> [44] </ref> a nonlinear least squares problem is solved by means of the Gauss-Newton method with a trust region globalization strategy, using Steihaug's method to solve the trust region subproblems. In that approach the TRS was not used as a way of regularizing the solution.
Reference: [45] <author> D. C. Sorensen. </author> <title> Newton's method with a model trust region modification. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 19(2) </volume> <pages> 409-426, </pages> <year> 1982. </year>
Reference-contexts: Two important facts about the TRS are * There is always a solution, since a continuous functional is being min imized on a compact set. * There exists a characterization of the global solutions. This is a re-markeable result obtained independently by Gay [13] and Sorensen <ref> [45] </ref>. It is contained in lemma 7.1. This problem has been treated extensively in the context of the trust-region globalization strategy for optimization methods. In the small to medium scale setting, there are efficient methods to solve the problem. <p> The following lemma contains the characterization of the solutions of (13). Lemma 7.1 A feasible vector x fl is a solution of (13) if and only if x fl sat isfies (i) (H I)x fl = g, with H I positive semidefinite. (ii) 0. Proof: See <ref> [45] </ref> 2 The solutions of (13) can occur on the boundary or in the interior of the trust region fxj k x k 2 g. Interior solutions exist if and only if k x k 2 &lt; and H is positive definite (see [34]). <p> If ffi 1 0 and k p k 2 &lt; , then the solutions of (13) consist of the set fxjx = p + z; z 2 S 1 ; k x k 2 = g. Proof: See <ref> [45] </ref> 2 Near hard case situations (when g is nearly orthogonal to S 1 ) are very important in the context of regularization because they arise very frequently when the ill-conditioned TRS is treated. We will return to this in x7.1.2.
Reference: [46] <author> D. C. Sorensen. </author> <title> Implicit application of polynomial filters in a k-step Arnoldi method. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 13 </volume> <pages> 357-385, </pages> <year> 1992. </year>
Reference-contexts: The method uses an Implicitly Restarted Lanczos Bidiagonalization process to compute a sequence of approximations to the left and right singular vectors of A. The implicit restart technique from <ref> [46] </ref> is adapted to the Lanc-zos Bidiagonalization process [38] and zero shifts are used to filter out small singular values. Reorthogonalization of the Lanczos vectors is carried out at every step of the Lanczos process.
Reference: [47] <author> D. C. Sorensen. </author> <title> Minimization of a large scale quadratic function subject to an ellipsoidal constraint. </title> <type> Technical Report 94-27, </type> <institution> Department of Computational and Applied Mathematics, Rice University, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: However, the fact that this method is based on the Preconditioned Conjugate Gradient method constitutes a major obstacle in the ill-conditioned case where finding efficient preconditioners is still an issue as we pointed out in section 6. The known methods for the large-scale TRS were given by Sorensen in <ref> [47] </ref>, Rendl and Wolkowicz in [41] and Santos and Sorensen in [43]. In [47] the TRS is recasted as a parameterized eigenvalue problem. The method relies on the Implicitly Restarted Lanczos Method (IRLM) ([46], [5]) to solve a sequence of large symmetric eigenvalues problems. <p> The known methods for the large-scale TRS were given by Sorensen in <ref> [47] </ref>, Rendl and Wolkowicz in [41] and Santos and Sorensen in [43]. In [47] the TRS is recasted as a parameterized eigenvalue problem. The method relies on the Implicitly Restarted Lanczos Method (IRLM) ([46], [5]) to solve a sequence of large symmetric eigenvalues problems. <p> The parameter is updated by an interpolation scheme and there is a separate treatment of a special case known as the hard case. In [41] the same recasting as in <ref> [47] </ref> is used but the strategy for updating the parameter is a dual simplex method in the standard case and a primal simplex method in the hard case. The eigenvalue problems are solved with a block-Lanczos procedure. In [43] the recasting of [47] is used again, but the interpolation scheme is <p> In [41] the same recasting as in <ref> [47] </ref> is used but the strategy for updating the parameter is a dual simplex method in the standard case and a primal simplex method in the hard case. The eigenvalue problems are solved with a block-Lanczos procedure. In [43] the recasting of [47] is used again, but the interpolation scheme is different and there is a unified treatment of all cases. A small advantage of this method over the one in [41] is reported in [43]. <p> We proposed to use the method for large-scale trust region subproblems (TRS method) presented in [43] to compute a solution for the constrained least squares problem (12). The use of methods for the TRS in the regularization context has been suggested in [41] and <ref> [47] </ref>. In [32] a new trust region strategy is presented and applied to a regularization problem as an example. That particular strategy is not suitable for large-scale problems. [6] presents a method for the TRS for the special case of ill-conditioned least squares problems.
Reference: [48] <author> T. Steihaug. </author> <title> The conjugate gradient method and trust regions in large scale optimization. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20(3) </volume> <pages> 262-637, </pages> <year> 1983. </year> <month> 45 </month>
Reference-contexts: Such methods include Powell's dogleg method [40], Dennis and Mei's double dogleg method [8] and More and Sorensen's method [34]. The latter one being the method of choice in most applications. 24 A complete survey of methods was given by More in [33]. In that refer-ence, Steihaug's method <ref> [48] </ref> is also mentioned. This method is suitable for the large-scale case and gives nearly optimal solutions when the Hessian of the quadratic is positive semidefinite. Methods for the general case were not available until recently. It is worth noticing that Steihaug's approach could be used on problem (12).
Reference: [49] <author> G. W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The least squares problem has been thoroughly studied and is treated in most numerical linear algebra text books like [16] and <ref> [49] </ref>, and also in specialized sources like the classic [31] and more recently in [2]. The problem is stated as follows min k Ax b k 2 where A 2 R mfin , m n and b 2 R m .
Reference: [50] <author> A. Tarantola. </author> <title> Inverse Problem Theory. </title> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1987. </year>
Reference-contexts: If we denote by P that approximate inverse operator, the solution is computed as x = PF fl b We are interested in the second approach known as linearized least squares inversion (see <ref> [50] </ref>). In this approach, F is discretized and k k is chosen to be the l 2 norm in an appropriate vector space. The problem to solve is a discrete least squares problem. Both techniques are used in practice.
Reference: [51] <author> A. N. Tikhonov and V. Y. Arsenin. </author> <title> Solutions of ill-posed problems. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: not be treated in this work. 10 i bj= i (in logarithmic scale) for exact (*) and noisy (o) data, problem deriv2 (m = n = 50) from the Regularization Tools package. 11 5 Regularization Concepts and Methods Regularization is a technique that was originally devised for continuous ill-posed problems <ref> [51] </ref>. When the technique is extended to the discrete context it gives rise to numerical regularization methods. We limit our presentation to the latter case.
Reference: [52] <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem: Computational Aspects and Analysis, </title> <booktitle> volume 9 of Frontiers in Applied Mathematics. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: We also ignored errors in A (due to discretiza-tion or finite precision representation). If we take into account perturbations in both A and b, the difficulty of the problem increases considerably. The problem becomes a Total Least Squares problem (see <ref> [52] </ref>) and will not be treated in this work. 10 i bj= i (in logarithmic scale) for exact (*) and noisy (o) data, problem deriv2 (m = n = 50) from the Regularization Tools package. 11 5 Regularization Concepts and Methods Regularization is a technique that was originally devised for continuous
Reference: [53] <author> J. Varah. </author> <title> A practical examination of some numerical methods for linear discrete ill-posed problems. </title> <journal> SIAM Review, </journal> <volume> 21 </volume> <pages> 100-111, </pages> <year> 1979. </year>
Reference-contexts: The methods presented here are suitable for ill-conditioned linear systems and least squares problems, the nonlinear least squares case has been studied in [54] and [11]. For a more detailed description of the methods, we refer the reader to [26]. Early surveys of regularization methods appeared in [3] and <ref> [53] </ref>.
Reference: [54] <author> C.R. Vogel. </author> <title> A constrained least squares regularization method for nonlinear ill-posed problems. </title> <journal> SIAM J. Control Optim., </journal> <volume> 28(1) </volume> <pages> 34-49, </pages> <year> 1990. </year>
Reference-contexts: In section 6 we discussed methods for the large-scale case in detail. The methods presented here are suitable for ill-conditioned linear systems and least squares problems, the nonlinear least squares case has been studied in <ref> [54] </ref> and [11]. For a more detailed description of the methods, we refer the reader to [26]. Early surveys of regularization methods appeared in [3] and [53].
Reference: [55] <author> C.R. Vogel. </author> <title> Non-convergence of the L-curve regularization parameter selection method. Inverse Problems, </title> <note> 1996. to appear. </note>
Reference-contexts: The use of the L-curve criterion seems to be a good option for computing the regularization parameter, as long as the DPC holds. It was showed in <ref> [55] </ref> that if that condition is not satisfied the L-curve criterion may fail to compute a good estimate for the regularization parameter. 40 8 Final Remarks In this work we have shown that large-scale ill-conditioned least squares problems arise in important applications and that there is a lack of robust numerical
Reference: [56] <author> G. Wahba. </author> <title> Spline Models for Observational Data. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990. </year>
Reference-contexts: The idea behind this criterion is that we cannot expect more accuracy in the approximate solution than the one present in the data. This method is attributed to Morozov [35]. 2. Generalized Cross-Validation (GCV). This method was presented in [15] and discussed also in <ref> [56] </ref>. The idea of using cross-validation to compute the regularization parameter is the following.
References-found: 56

