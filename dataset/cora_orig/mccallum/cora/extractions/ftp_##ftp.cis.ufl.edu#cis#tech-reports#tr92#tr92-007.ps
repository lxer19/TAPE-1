URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr92/tr92-007.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr92-abstracts.html
Root-URL: http://www.cis.ufl.edu
Email: ted@cis.ufl.edu  
Title: Vlist: A Vectorized List  
Author: Theodore Johnson 
Keyword: SIMD, vector processor, data structure, list, memory management.  
Date: April 3, 1992  
Address: TR #92-007  Gainesville, Fl 32611-2024  
Affiliation: University of Florida CIS  Dept. of CIS, University of Florida  
Abstract: Many current supercomputers use vector or SIMD processors to exploit parallelism inexpensively and effectively. Many fast and efficient kernels have been written for vector and matrix computations, but little work has been done to vectorize data structures. As a result, maintaining the data structures can be the most time consuming part of a computation. In this paper, we introduce a vectorized list, the Vlist. The Vlist maintains same-length component lists to allow vectorized search operations. We show how inserts and deletes can be performed on the Vlist in constant time, and in a manner that preserves the same-length property of the component lists. Finally, we discuss applications to memory management, sparse matrix algorithms and object-oriented databases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: The prefix sum is simple enough to be implemented in hardware. If no such operation is available, the O (log n) parallel prefix sum algorithm <ref> [1] </ref> can be used.
Reference: [2] <author> A.W. Appel and A. Bendiksen. </author> <title> Vectorized garbage collection. </title> <journal> Journal of Supercomputing, </journal> <volume> 3 </volume> <pages> 151-160, </pages> <year> 1990. </year>
Reference-contexts: Kanada has applied this technique to hashing, sorting, tree rewriting, and other types of symbolic processing [8, 9, 7]. Appel and Bendiksen <ref> [2] </ref> uses a similar technique for vectorizing a garbage collector. The FOL method requires little modification of the existing data structure, so that it can be easily applied.
Reference: [3] <author> T. A. Davis and P. C. Yew. </author> <title> A nondeterministic parallel algorithm for general unsymmetric sparse LU factorization. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11(3) </volume> <pages> 383-402, </pages> <year> 1990. </year>
Reference-contexts: Little work has been performed on vectorizing data structures, so that symbolic processing is typically executed using only the scalar processor. Many algorithms, such those for as asymmetric sparse matrix factorization <ref> [3, 4, 6] </ref> require a significant amount of symbolic processing and use many data structures. <p> Much work has been performed on the LU-factorization of sparse matrices. Sparse matrix factorization algorithms work on the compressed matrix. As the computation proceeds, the matrix rows can shrink, as their entries are zeroed, or can grow due to fill-in (non-zeros created by the factorization). In Davis' D2 algorithm <ref> [3] </ref> rows are stored in linked blocks, and hence could benefit from a vectorized buffer allocator. Other algorithms, known as multi-frontal methods [4], build dense submatrices to factor. Allocation of the space for these submatrices can benefit from a vectorized heap memory allocator. <p> Allocation of the space for these submatrices can benefit from a vectorized heap memory allocator. Asymmetric sparse matrix factorization algorithms which calculate the pivoting sequence while they factor the matrix typically make extensive use of linked list data structures to tie together partial results <ref> [3, 4, 6] </ref>.
Reference: [4] <author> T.A. Davis and I.S. Duff. </author> <title> Unsymmetric-pattern multifrontal methods for parallel sparse LU factorization. </title> <type> Technical report, </type> <institution> University of Florida, Dept. of CIS TR-91-23, </institution> <year> 1991. </year> <note> Available at anonymous ftp site cis.ufl.edu:cis/tech-reports. </note>
Reference-contexts: Little work has been performed on vectorizing data structures, so that symbolic processing is typically executed using only the scalar processor. Many algorithms, such those for as asymmetric sparse matrix factorization <ref> [3, 4, 6] </ref> require a significant amount of symbolic processing and use many data structures. <p> In Davis' D2 algorithm [3] rows are stored in linked blocks, and hence could benefit from a vectorized buffer allocator. Other algorithms, known as multi-frontal methods <ref> [4] </ref>, build dense submatrices to factor. Allocation of the space for these submatrices can benefit from a vectorized heap memory allocator. <p> Allocation of the space for these submatrices can benefit from a vectorized heap memory allocator. Asymmetric sparse matrix factorization algorithms which calculate the pivoting sequence while they factor the matrix typically make extensive use of linked list data structures to tie together partial results <ref> [3, 4, 6] </ref>.
Reference: [5] <author> J.J. Dongarra, J. Du Croz, I.S. Duff, and S. Hammarling. </author> <title> A set of three basic linear algrbra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Researchers have written efficient vectorized kernels for vector and matrix operations (such as BLAS <ref> [5] </ref>), which allow the vector processing supercomputers to achieve Gigaflop processing rates. Little work has been performed on vectorizing data structures, so that symbolic processing is typically executed using only the scalar processor.
Reference: [6] <author> I.S. Duff, A.M. Erisman, and J.K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1986. </year>
Reference-contexts: Little work has been performed on vectorizing data structures, so that symbolic processing is typically executed using only the scalar processor. Many algorithms, such those for as asymmetric sparse matrix factorization <ref> [3, 4, 6] </ref> require a significant amount of symbolic processing and use many data structures. <p> Allocation of the space for these submatrices can benefit from a vectorized heap memory allocator. Asymmetric sparse matrix factorization algorithms which calculate the pivoting sequence while they factor the matrix typically make extensive use of linked list data structures to tie together partial results <ref> [3, 4, 6] </ref>.
Reference: [7] <author> Y. Kanada. </author> <title> A method of vector processing for shared symbolic data. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 722-731, </pages> <year> 1991. </year>
Reference-contexts: If no concurrency control is performed, then only the final write will be observable after the vector insert operation. Kanada has investigated the use of what he names the filtering-overwritten-label method, or FOL <ref> [7] </ref>, to overcome the overwriting problem. The key idea is that before each write operation, each processing unit in the vector processor writes its tag to a memory location that is associated with the memory location that is to be modified. <p> If the unit doesn't read its own tag, it remains inactive and allows the winning unit to execute. the operation is iterated until all units of the vector processor complete their operations. Kanada has applied this technique to hashing, sorting, tree rewriting, and other types of symbolic processing <ref> [8, 9, 7] </ref>. Appel and Bendiksen [2] uses a similar technique for vectorizing a garbage collector. The FOL method requires little modification of the existing data structure, so that it can be easily applied.
Reference: [8] <author> Y. Kanada, K. Kojima, and M. Sugaya. </author> <title> Vectorization techniques for Prolog. </title> <booktitle> In ACM International Conference on Supercomputing, </booktitle> <pages> pages 539-549, </pages> <year> 1988. </year>
Reference-contexts: If the unit doesn't read its own tag, it remains inactive and allows the winning unit to execute. the operation is iterated until all units of the vector processor complete their operations. Kanada has applied this technique to hashing, sorting, tree rewriting, and other types of symbolic processing <ref> [8, 9, 7] </ref>. Appel and Bendiksen [2] uses a similar technique for vectorizing a garbage collector. The FOL method requires little modification of the existing data structure, so that it can be easily applied.
Reference: [9] <author> Y. Kanada and M. Sugaya. </author> <title> Vectorization techniques for prolog without explosion. </title> <booktitle> In Int. Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 151-156, </pages> <year> 1989. </year>
Reference-contexts: If the unit doesn't read its own tag, it remains inactive and allows the winning unit to execute. the operation is iterated until all units of the vector processor complete their operations. Kanada has applied this technique to hashing, sorting, tree rewriting, and other types of symbolic processing <ref> [8, 9, 7] </ref>. Appel and Bendiksen [2] uses a similar technique for vectorizing a garbage collector. The FOL method requires little modification of the existing data structure, so that it can be easily applied.
Reference: [10] <author> D. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 1. </volume> <publisher> Addison Wesley, </publisher> <year> 1968. </year>
Reference-contexts: A data structure that is designed to support vector operations will be more efficient than one constructed using the FOL method. 2 2 The Vlist The data structure that we vectorize is the linked list, which is a set of records each linked to its immediate successor <ref> [10] </ref>. The usual scalar linked list isn't a good structure for vector operations, since the list must be followed one record at a time. In order to support vectorized operations on the linked list, we need to modify the structure of the list. <p> By comparison, a single buffer stack will require n operations to allocate n buffers, and n independent buffer stacks will become unbalanced if the request size varies. Variable-sized (or heap) memory allocation is often implemented by some variant of a free list algorithm <ref> [10] </ref>, whether first fit, best fit, or worst fit. Compared to other heap memory managers, free list algorithms are slow, but have low fragmentation. We can use the Vlist to implement a fast and space efficient free list memory manager.
Reference: [11] <author> M.J. Quinn. </author> <title> Designing Efficient Algorithms for Parallel Computers. </title> <publisher> McGraw-Hill, 19787. </publisher> <pages> 11 </pages>
Reference-contexts: fast memory managers, and discuss its application to asymmetric sparse matrix algorithms and object-oriented databases. 1.1 Previous Work Many search structures, such as balanced trees or hash tables, have logarithmic or even constant time complexity, so that using many processors to perform a single operation yields only a small speedup <ref> [11] </ref>. A more promising approach is to use the vector processor to perform many operations simultaneously. A problem that needs to be overcome is that several of the parallel operations might attempt to write into the same memory location.
Reference: [12] <author> S.Y.W Su, Y.H. Chen, and H. Lam. </author> <title> Multiple wavefromt algorithms for patern-based processing of object-oriented databases. </title> <booktitle> In Proc. of the First Int'l Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 46-55. </pages> <publisher> ACM and IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Objects of different classes that are related are linked by associations. Physically, objects in a class store the name of all other objects that they are associated with. OSAM*'s query language allows operations on sets of objects that have a specified association. Su, Chen and Lam <ref> [12] </ref> have examined parallel algorithms for determining which the objects that have a specified association. Their algorithms involve searching the list of objects in a class for the specified local associations, then passing this information to the processors that hold the associated objects.
Reference: [13] <author> S.Y.W. Su, V. Krishnamurthy, and H. Lam. </author> <title> An object-oriented semantic association model (OSAM*). </title> <editor> In S. Kumara, A.L. Soyster, and R.L. Kashyap, editors, </editor> <booktitle> Artificial Intelligence: Manufacturing Theory and Practice, </booktitle> <pages> pages 463-494. </pages> <institution> Institute of Industrial Engineers, Industrial Engineering, and Management Press, </institution> <year> 1989. </year> <pages> 12 13 </pages>
Reference-contexts: Su et al. have parallelized the OSAM* OODB <ref> [13] </ref> by managing objects of different classes on different processors. Objects of different classes that are related are linked by associations. Physically, objects in a class store the name of all other objects that they are associated with.
References-found: 13

