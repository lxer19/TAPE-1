URL: http://www.cs.iastate.edu/tech-reports/TR93-25.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: 
Title: Efficient Learning of Regular Languages Using Teacher-Supplied Positive Samples and Learner-Generated Queries TR93-25  
Author: Rajesh Parekh and Vasant Honavar 
Address: 226 Atanasoff Ames, IA 50011  
Affiliation: Iowa State University of Science and Technology Department of Computer Science  
Date: October 19, 1993  
Abstract-found: 0
Intro-found: 1
Reference: [Biermann & Feldman, 1972] <author> Biermann, A.W. and Feldman, J.A. </author> <title> A Survey of Results in Grammatical Inference. </title> <editor> In Watanabe S. (editor), </editor> <booktitle> Frontiers of Pattern Recognition. </booktitle> <publisher> Academic Press pp. </publisher> <pages> 31-54. </pages>
Reference-contexts: We present an algorithm for inference of regular grammars which makes extensive use of finite state automata. 1.3 The Grammar Inference Problem Grammatical Inference is the process of learning a rule-based grammar from a finite set of labeled examples. The grammar inference problem <ref> [Biermann & Feldman, 1972] </ref> may be described as follows : A finite set of symbol strings S + generated by an unknown grammar G and possibly a finite set of strings S generated by the complement grammar G are known, and a grammar G fl equivalent to the unknown grammar G
Reference: [Fu, 1982] <author> Fu. </author> <title> K.S., Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, N.J. </address> <year> 1982 </year>
Reference-contexts: Since this approach borrows from formal languages and grammars it is called the syntactic approach to pattern recognition. Learning to classify patterns thus reduces to learning the grammars which characterize pattern classes ( <ref> [Fu, 1982] </ref>, [Honavar, 1993]). The method of learning a grammar from a set of examples is called grammar inference. <p> Regular grammars are the simplest among phrase structure grammars. Although it is well known that the descriptive power of regular grammars is fairly limited, for several practical applications, other grammars (e.g. context-free grammars) can be closely approximated by regular grammars <ref> [Fu, 1982] </ref>. Finite State Automata can be used to recognize regular grammars. <p> More formally, given S + = fff j ff 2 G g and possibly S = ffi j fi 62 G g infer a grammar G fl such that L (G fl ) = L (G). For a discussion on grammar inference in machine learning see <ref> [Fu, 1982] </ref> and [Parekh & Honavar, 1993c]. Several solutions to the grammar inference problem have been proposed. These include the uv k w algorithm and the k-tails method due to Solomonoff and Biermann & Feldman respectively [Fu, 1982]. <p> For a discussion on grammar inference in machine learning see <ref> [Fu, 1982] </ref> and [Parekh & Honavar, 1993c]. Several solutions to the grammar inference problem have been proposed. These include the uv k w algorithm and the k-tails method due to Solomonoff and Biermann & Feldman respectively [Fu, 1982]. Owing to space constraints we present only a brief synopsis of these methods. The k-tails method involves defining equivalence classes on the canonical grammar generated by the complete positive sample set S + . <p> The grammar inference algorithm proposed here, can potentially be used in several pattern recognition applications. These include, among others, chromosome identification, speech synthesis, and DNA analysis [Miclet & Quinqueton, 1986], [Gonzales & Thomason, 1978], and <ref> [Fu, 1982] </ref>. The performance of an implementation of the grammar inference algorithm proposed here is being evaluated in several different application domains. The aver age case analysis of the bi-directional search, detailed experiments, and comparisons with performances of other grammar inference methods is in progress [Parekh & Honavar, 1993b].
Reference: [Giles et al, 1991] <author> Giles, C., Chen, D., Miller, H., Sun, G., and Lee, Y. </author> <title> Second-order recurrent neural networks for grammatical inference. </title> <booktitle> In Proceedings of the International Joint conference on Neural Networks 1991, </booktitle> <volume> vol. 2, </volume> <pages> pp. 273-281, </pages> <month> July </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: In order to uniquely identify the grammar it is essential to use information provided by the negative samples. 2 Giles et al. have done extensive research in grammatical inference using recurrent neu-ral networks <ref> [Giles et al, 1991] </ref>. They have proposed an incremental, real-time, recurrent learning method that computes the complete gradient and updates the weights of the network after each example string is presented. A dynamic clustering algorithm is then used to extract the production rules that the grammar has learned.
Reference: [Gold, 1967] <author> Gold, M. </author> <title> Language Identification in the limit. </title> <journal> Inf. Control Vol. </journal> <volume> 10, </volume> <year> 1967. </year> <pages> pp. 447-474. </pages>
Reference-contexts: The recursive rules thus obtained characterize the grammar being sought. Both these methods are heuristic in nature and rely only on the information provided by the positive samples belonging to S + . The importance of using information from negative samples in learning is well known <ref> [Gold, 1967] </ref>. There is a potentially infinite number of grammars which are consistent with a given set of positive samples.
Reference: [Gonzales & Thomason, 1978] <author> Gonzales, R.C., and Thomason, M.G. </author> <title> Syntactic Pattern Recognition An Introduction. </title> <publisher> Addison Wesley Publishing company, Inc. </publisher> <year> 1978. </year>
Reference-contexts: The grammar inference algorithm proposed here, can potentially be used in several pattern recognition applications. These include, among others, chromosome identification, speech synthesis, and DNA analysis [Miclet & Quinqueton, 1986], <ref> [Gonzales & Thomason, 1978] </ref>, and [Fu, 1982]. The performance of an implementation of the grammar inference algorithm proposed here is being evaluated in several different application domains.
Reference: [Harrison, 1965] <author> Harrison, M. </author> <title> Introduction to switching and automata theory. </title> <publisher> McGraw-Hill, </publisher> <address> New York 1965. </address>
Reference-contexts: Transitions from one state to another in the same cell form self loops. 2.2.4 Equivalence of Finite State Automata The test for equivalence of finite state automata <ref> [Harrison, 1965] </ref>, [Rabin & Scott, 1959] is summarized below. Let U = (S; M; ; s 0 ; F ) and V = (T; N; ; t 0 ; G) be two finite state automata. <p> This string y belongs to the difference machine M i M j and forms the query to be posed to the teacher. The difference machine <ref> [Harrison, 1965] </ref> is M i M j = fW; ffi w ; ; w 0 ; Cg where, W = S fi T, w 0 = (s 0 ; t 0 ) , ffi w ( (s,t) ; ) = (ffi s (s ; ); ffi t (t ; )) for
Reference: [Honavar, 1993] <author> Honavar, V.G., </author> <title> Toward Learning Systems that use Multiple Strategies and Multiple Representations. In Symbol Processors and Connectionist Network Models in Artificial Intelligence and Cognitive Modeling. Honavar, </title> <editor> V. & Uhr, L. </editor> <address> (editors) New York: </address> <note> Academic Press. To appear. </note>
Reference-contexts: Since this approach borrows from formal languages and grammars it is called the syntactic approach to pattern recognition. Learning to classify patterns thus reduces to learning the grammars which characterize pattern classes ( [Fu, 1982], <ref> [Honavar, 1993] </ref>). The method of learning a grammar from a set of examples is called grammar inference.
Reference: [Lewis & Papadimitriou, 1981] <author> Lewis, H.R., and Papadimitriou C.H. </author> <title> Elements of the Theory of Computation. </title> <publisher> The MIT press 1981. </publisher>
Reference-contexts: This means that ffi (p; a) is subset (possibly empty) of Q instead of a single state in Q. The deterministic and non-deterministic finite state automata are equivalent in expressive power and there are polynomial time algorithms which convert a non-deterministic finite state automaton to a finite state machine <ref> [Lewis & Papadimitriou, 1981] </ref>. We present an algorithm for inference of regular grammars which makes extensive use of finite state automata. 1.3 The Grammar Inference Problem Grammatical Inference is the process of learning a rule-based grammar from a finite set of labeled examples.
Reference: [Miclet & Quinqueton, 1986] <author> Miclet, L. and Quinqueton J. </author> <title> Learning from Examples in Sequences and Grammatical Inference. </title> <note> In Ferrate, </note> <author> G., Pavlidis, T., Sanfeliu, A., and Bunke H. </author> <title> (editors) Syntactic and Structural Pattern Recognition. </title> <booktitle> NATO ASI Series Vol. F45, </booktitle> <pages> pp. 153-171. </pages>
Reference-contexts: Quite a few issues need to be explored thoroughly before a definitive statement can be made on the superiority of our algorithm. The grammar inference algorithm proposed here, can potentially be used in several pattern recognition applications. These include, among others, chromosome identification, speech synthesis, and DNA analysis <ref> [Miclet & Quinqueton, 1986] </ref>, [Gonzales & Thomason, 1978], and [Fu, 1982]. The performance of an implementation of the grammar inference algorithm proposed here is being evaluated in several different application domains.
Reference: [Pao & Carr, 1978] <author> Pao, T.W.L., and Carr, J.W.III. </author> <title> A solution of the Syntactic Induction-Inference Problem for Regular Languages. </title> <journal> Computer Languages, </journal> <volume> Vol. 3 1978, </volume> <pages> pp. 53-64. </pages>
Reference-contexts: Using this information the learner can perform the inference by searching through a space of candidate solutions. The use of a teacher-student model for inference of regular grammars was originally proposed by Pao <ref> [Pao & Carr, 1978] </ref>. Their method for mapping candidate grammars to a regular structure (lattice) is used in our algorithm. However, their formulation contained a few errors [Parekh, 1993]. In addition to correcting these errors, our algorithm employs a bi-directional search to probe the space of candidate grammars. <p> Using this information the learner constructs a canonical automaton M S + which accepts only the strings which belong to S + <ref> [Pao & Carr, 1978] </ref>. The strings in S + are ordered by increasing length. Let s 0 be the start state of M S + and s F be the final state.
Reference: [Parekh & Honavar, 1993a] <author> Parekh R.G., and Honavar V.G. </author> <title> Efficient Learning of Regular Languages using teacher supplied positive samples and learner generated queries. </title> <booktitle> In Proceedings of the Fifth UNB Conference on AI, </booktitle> <address> Fredrickton, Canada. </address> <month> August 11-14, </month> <year> 1993. </year>
Reference: [Parekh & Honavar, 1993b] <author> Parekh R.G., and Honavar V.G. </author> <title> Efficient Learning of Regular Languages using teacher supplied positive samples and learner generated queries II (Experiments). </title> <note> In preparation. </note>
Reference-contexts: The performance of an implementation of the grammar inference algorithm proposed here is being evaluated in several different application domains. The aver age case analysis of the bi-directional search, detailed experiments, and comparisons with performances of other grammar inference methods is in progress <ref> [Parekh & Honavar, 1993b] </ref>. A few promising directions for future research are enlisted hereunder: 1. Lambda Pruning: A simple modification to the algorithm could potentially yield considerable savings in terms of the number of candidate grammars that are examined.
Reference: [Parekh & Honavar, 1993c] <author> Parekh R.G., and Honavar V.G. </author> <title> Grammar Inference for Machine Learning. </title> <note> In preparation. </note>
Reference-contexts: More formally, given S + = fff j ff 2 G g and possibly S = ffi j fi 62 G g infer a grammar G fl such that L (G fl ) = L (G). For a discussion on grammar inference in machine learning see [Fu, 1982] and <ref> [Parekh & Honavar, 1993c] </ref>. Several solutions to the grammar inference problem have been proposed. These include the uv k w algorithm and the k-tails method due to Solomonoff and Biermann & Feldman respectively [Fu, 1982]. Owing to space constraints we present only a brief synopsis of these methods.
Reference: [Parekh, 1993] <author> Parekh R.G. </author> <title> Efficient Learning of Regular Languages using teacher supplied positive samples and learner generated queries. </title> <institution> unpublished Master of Science Project Report, Iowa State University, Ames, Iowa. </institution> <month> September 7, </month> <year> 1993. </year>
Reference-contexts: The use of a teacher-student model for inference of regular grammars was originally proposed by Pao [Pao & Carr, 1978]. Their method for mapping candidate grammars to a regular structure (lattice) is used in our algorithm. However, their formulation contained a few errors <ref> [Parekh, 1993] </ref>. In addition to correcting these errors, our algorithm employs a bi-directional search to probe the space of candidate grammars. This results in considerable speed improvement over Pao's method. Section 2 describes the framework of the algorithm.
Reference: [Pohl, 1971] <author> Pohl, I. </author> <title> Bi-directional Search. </title> <editor> In Meltzer B., and Michie, D. (Eds.) </editor> <booktitle> Machine Intelligence 6. </booktitle> <address> New York, </address> <publisher> American Elsevier, </publisher> <year> 1971. </year> <pages> pp. 127-140. </pages>
Reference-contexts: Ultimately, the only partition remaining in the list ! L corresponds to the inferred grammar G fl . 3 The Grammar Inference Algorithm A bi-directional search <ref> [Pohl, 1971] </ref> comprising of two independent searches (which may proceed in parallel) is used to navigate through the lattice (hypothesis space) for the solution. The two searches start from opposite ends of the lattice. One proceeds top-down while the other one is bottom-up.
Reference: [Rabin & Scott, 1959] <author> Rabin, M.O., and Scott, D. </author> <title> Finite Automata and their Decision Problems. </title> <journal> IBM Journal of Research and Development, </journal> <volume> Vol. 3 1959, </volume> <pages> pp 114-125. </pages>
Reference-contexts: Transitions from one state to another in the same cell form self loops. 2.2.4 Equivalence of Finite State Automata The test for equivalence of finite state automata [Harrison, 1965], <ref> [Rabin & Scott, 1959] </ref> is summarized below. Let U = (S; M; ; s 0 ; F ) and V = (T; N; ; t 0 ; G) be two finite state automata.
Reference: [Schapire, 1991] <author> Schapire, R.E., </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year> <month> 15 </month>
Reference-contexts: A dynamic clustering algorithm is then used to extract the production rules that the grammar has learned. Rivest and Schapire have suggested a powerful new technique for grammar inference based on homing sequences <ref> [Schapire, 1991] </ref>. The output produced in executing the homing sequence completely determines the state reached by the automaton at the end of the homing sequence. Every finite state machine has a homing sequence and Schapire's technique infers the homing sequence as part of the overall inference procedure.
References-found: 17

