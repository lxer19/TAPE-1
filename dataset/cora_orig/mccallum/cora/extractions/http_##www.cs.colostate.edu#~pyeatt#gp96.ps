URL: http://www.cs.colostate.edu/~pyeatt/gp96.ps
Refering-URL: http://www.cs.colostate.edu/~pyeatt/
Root-URL: 
Email: gruau@cwi.nl whitley@cs.colostate.edu pyeatt@cs.colostate.edu  
Phone: 413  
Title: A Comparison between Cellular Encoding and Direct Encoding for Genetic Neural Networks  
Author: Frederic Gruau Darrell Whitley Larry Pyeatt 
Address: Kruislaan  1098 SJ Amsterdam  The Netherlands Fort Collins, CO 80523 USA Fort Collins, CO 80523 USA  
Affiliation: CWI,  Computer Science Dept. Computer Science Dept.  Colorado State University Colorado State University  
Abstract: This paper compares the efficiency of two encoding schemes for Artificial Neural Networks optimized by evolutionary algorithms. Direct Encoding encodes the weights for an a priori fixed neural network architecture. Cellular Encoding encodes both weights and the architecture of the neural network. In previous studies, Direct Encoding and Cellular Encoding have been used to create neural networks for balancing 1 and 2 poles attached to a cart on a fixed track. The poles are balanced by a controller that pushes the cart to the left or the right. In some cases velocity information about the pole and cart is provided as an input; in other cases the network must learn to balance a single pole without velocity information. A careful study of the behavior of these systems suggests that it is possible to balance a single pole with velocity information as an input and without learning to compute the velocity. A new fitness function is introduced that forces the neural network to compute the velocity. By using this new fitness function and tuning the syntactic constraints used with cellular encoding, we achieve a tenfold speedup over our previous study and solve a more difficult problem: balancing two poles when no information about the velocity is provided as input.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Collins, R. and Jefferson, D. </author> <title> (1991) Selection in Massively Parallel Genetic Algorithms. </title> <booktitle> Proc. 4th International Conf. on Genetic Algorithms, </booktitle> <publisher> Morgan-Kaufmann, </publisher> <pages> pp 249-256. </pages>
Reference-contexts: A more complete description of this algorithm is given by Gruau [4]. The parallel genetic program combines the advantages of the massively parallel model <ref> [1] </ref> and the island model [6]. Individuals are distributed on islands where each island is a grid forming a 2-D torus. Islands are also arranged as a grid and individuals can migrate only to the four neighbor islands. Not all the sites of a 2-D grid are occupied.
Reference: [2] <author> Gruau, F. </author> <title> (1994) Neural Network Synthesis using Cellular Encoding and the Genetic Algorithm PhD Thesis, </title> <institution> Ecole Normale Superieure de Lyon. </institution> <note> anonymous ftp: lip.ens-lyon.fr (140.77.1.11) pub/Rapports/PhD PhD94-01-E.ps.Z (English) PhD94-01-F.ps.Z (French) </note>
Reference-contexts: The results of the experiments are reported in Table 1, and will be compared to those obtained with cellular encoding. 3 Cellular Encoding With Real Weights Cellular encoding is a language for local graph transformations that controls the division of cells which grow into an artificial neural network [3] <ref> [2] </ref>. Each cell has an input site and an output site and can be linked to other cells with directed and ordered links. A cell also possesses a list of internal registers that represent local memory.
Reference: [3] <author> Gruau, F. and Whitley, D. </author> <title> (1993) Adding learning to the the cellular developmental process: a comparative study Evolutionary Computation 1(3) </title> <type> 213-233. </type>
Reference-contexts: The results of the experiments are reported in Table 1, and will be compared to those obtained with cellular encoding. 3 Cellular Encoding With Real Weights Cellular encoding is a language for local graph transformations that controls the division of cells which grow into an artificial neural network <ref> [3] </ref> [2]. Each cell has an input site and an output site and can be linked to other cells with directed and ordered links. A cell also possesses a list of internal registers that represent local memory. <p> In the parallel division denoted PAR both child cells inherit both the input and output links from the parent cell. Hence, each link is duplicated. The child cells are not connected. Examples of these types of cell division are given by Gruau and Whitley <ref> [3] </ref>. In general, a particular cell division is specified by indicating for each child cell which link is inherited from the mother cell. In this paper, we used another division called CPO illustrated in figure 1.
Reference: [4] <author> Gruau, F. </author> <title> (1994 Automatic Definition of Modular Neural Networks, Adaptive Behavior 3(2) 151-184 </title>
Reference-contexts: For tree, list and set nodes the mutation rate is 0.05, while for the integer node it is 0.5. 4 Comparative Experiments for Cellular Encoding and Direct Encoding We used a parallel Genetic Programming Algorithm with 32 subpopulations. A more complete description of this algorithm is given by Gruau <ref> [4] </ref>. The parallel genetic program combines the advantages of the massively parallel model [1] and the island model [6]. Individuals are distributed on islands where each island is a grid forming a 2-D torus.
Reference: [5] <author> Koza, J. </author> <title> (1992) Genetic Programming: On the Programming of Computers by Means of Natural Selection, </title> <publisher> Cambrige, MA the MIT Press. </publisher>
Reference-contexts: This simple mechanism ensures the closure of the crossover operator with respect to the syntactic constraints. Crossover between two trees is the classic crossover used in Genetic Programming as advocated by Koza <ref> [5] </ref>, where two subtrees are exchanged. Crossover between two integers is disabled. Crossover between two lists, or two sets is implemented like crossover between bit strings, since the underlying arrangement of all these data structures is a string. Mutation.
Reference: [6] <author> Muehlenbein, H., Schomish, M. and Born, J. </author> <title> (1991) The parallel genetic algorithm as function optimizer, </title> <booktitle> Parallel Computing 17 </booktitle> <pages> 619-632. </pages>
Reference-contexts: A more complete description of this algorithm is given by Gruau [4]. The parallel genetic program combines the advantages of the massively parallel model [1] and the island model <ref> [6] </ref>. Individuals are distributed on islands where each island is a grid forming a 2-D torus. Islands are also arranged as a grid and individuals can migrate only to the four neighbor islands. Not all the sites of a 2-D grid are occupied.
Reference: [7] <author> Whitley, D., Dominic, S., Das, R. and Anderson, C. </author> <title> (1993) Genetic reinforcement learning for neu-rocontrol problems, </title> <booktitle> Maching Learning 13 </booktitle> <pages> 259-284. </pages>
Reference: [8] <author> Whitley, D., Gruau, F., and Pyeatt, L. </author> <title> (1995) Cellular Encoding Applied to Neurocontrol Proc. </title> <booktitle> 6th International Conf. on Genetic Algorithms, </booktitle> <publisher> Morgan-Kaufmann, </publisher> <pages> pp 460-467. </pages>
Reference-contexts: Also, the number of neurons used was different than those re ported by Wieland. Relatively small populations were used (100 strings) with a high mutation rate (30%). The goal of the paper is to continue the comparison between direct encoding and cellular encoding started in <ref> [8] </ref>. The use of the new evaluation function and new syntactic constraints has allowed us to solve the simpler version of the pole balancing problem more than ten times faster with cellular encoding (CE). This speedup is both in terms number of evaluations and wall clock time. <p> This speedup is both in terms number of evaluations and wall clock time. We also solve a more difficult version of the pole balancing problem where two poles are balanced and no information about velocity is supplied. We had not previous attempted this problem (e.g. <ref> [8] </ref>) and this problem was not addressed by Wieland. In general our results indicate that cellular encoding is able to optimize both the architecture and the weights at the same time. In contrast, direct encodings assumes an a priori architecture which is suboptimal in the general case. <p> See the figures 3 and 4 for examples of neural networks. The instruction MULT x sets that coefficient to x. It is worth mentioning that in <ref> [8] </ref>, we did not used a multiplicative coefficient, and imposed weights to scale between 1 and +1. As a result, the genetic program could not directly produce ANNs with no hidden units. <p> Our initial experiments used a fitness based on the number of time steps the pole could stay balanced <ref> [8] </ref>. Our early experiments indicated that learning to control the system for a single random start state was easy to achieve, but that the resulting generalization was poor. <p> Our assumption was that the recurrent connections makes it possible to remember the previous position of the cart and the pole, and to compute the speed. More recently, we discovered that this hypothesis does not appear to be true. Using the same setting as the one described in <ref> [8] </ref> we evolved nonrecurrent networks to see whether the recurrent links were important. The genetic program could generate an ANN without hidden units (no hidden units) that could balance the pole for the 11 initial positions and for over 100,000 time steps. <p> The intervals for the cart variable are the same as the ones used in the learning set: the position varies between 2:16 meters, the velocity varies between 1:35 meters per second. The interval for the big pole is the same as the one used in <ref> [8] </ref>. The angle varies between 3:6 degrees, and the pole velocity between 8:6 degrees per second. We then used 625 test cases using settings at 0.05, 0.25, 0.5, 0.75 and 0.95 intervals within these reduced ranges. The small pole was always set to zero, because it moves so quickly.
Reference: [9] <author> Wieland, A. </author> <title> (1990) Evolving Controls for Unstable Systems. Connectionist Models: </title> <booktitle> Proc. 1990 Summer School, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp 91-102. </pages>
Reference-contexts: On the other hand, there are well known problems when attempting to train neural networks using evolutionary algorithms that emphasize recombination. Wieland <ref> [9] </ref> introduced several interesting test problems for neurocontrol that involve balancing one or more poles fixed to a cart moving on a finite track. Possible actions on the system include applying a force to the cart to push it to the left or right. <p> Second, neural networks with fixed architectures were trained using a genetic algorithm to find the weights. Following the approach originally used by Wieland, each weight was encoded using 8 bits with values distributed among the values f1; 253 255 ; 251 255 ; 1g with no representation for zero <ref> [9] </ref>. Several networks of varying sizes were tried and the best network for solving each problem was selected. The fixed architecture networks included a bias input, which was not used by Wieland. Also, the number of neurons used was different than those re ported by Wieland. <p> For the case where the velocity is not known solutions with one or two hidden units were produced. 2 Reproducing Wieland's Results This section describes the experiments done using a direct encoding of the weights, following <ref> [9] </ref>. The balancing of one pole is a four input problem. These are the pole angle and angular velocity, as well as the cart position and velocity.
References-found: 9

