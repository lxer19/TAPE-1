URL: http://www.cs.wisc.edu/~jgoldst/submit_to_VLDB98.ps
Refering-URL: 
Root-URL: 
Email: email: beyer,jgoldst,raghu,uri@cs.wisc.edu  
Title: When Is "Nearest Neighbor" Meaningful?  
Author: Kevin Beyer Jonathan Goldstein Raghu Ramakrishnan Uri Shaft Kevin Beyer 
Date: 226  
Note: The contact author is  
Address: 1210 W. Dayton St., Madison, WI 53706  
Affiliation: CS Dept., University of Wisconsin-Madison  
Pubnum: Paper no.  
Abstract: We explore the effect of dimensionality on the "nearest neighbor" problem. We show that under a broad set of conditions (much broader than independent and identically distributed dimensions), as dimensionality increases, the distance to the nearest data point approaches the distance to the farthest data point. To provide a practical perspective, we present empirical results on both real and synthetic data sets that demonstrate that this effect can occur for as few as 10-15 dimensions. These results should not be interpreted to mean that high-dimensional indexing is never useful; we illustrate this point by identifying some high-dimensional workloads for which this effect does not occur. However, our results do emphasize that the methodology used almost universally in the database literature to evaluate high-dimensional indexing techniques is flawed, and should be modified. In particular, most such techniques proposed in the literature are not evaluated versus simple linear scan, and are evaluated over workloads for which nearest neighbor is not meaningful. Often, even the reported experiments, when analyzed carefully, show that linear scan would outperform the techniques being proposed on the workloads studied in high (10-15) dimensionality! Our results also suggest that users must be able to assess how meaningful the answer to a high dimensional nearest neighbor query is. Since the "nearest neighbor" is in general at the same distance as the "farthest neighbor", before we attach significance to a nearest neighbor answer, we should have a measure of how many data points are significantly farther than the answer points. We propose an intuitive variant of the problem that provides such a measure. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, C. Faloutsos, and A. Swami. </author> <title> Efficient similarity search in sequence databases. </title> <booktitle> In Proc. 4th Inter. Conf. on FODO, </booktitle> <pages> pages 69-84, </pages> <year> 1993. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. [13, 24, 25, 19, 25, 21, 23, 16, 3]), sequences (e.g. <ref> [2, 1] </ref>), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant NAGW-3921, ORD contract 144-ET33, and NSF grant 144-GN62. 1 vectors. <p> Let the m dimensional data space S m be the boundary of an m dimensional unit hyper-cube. (i.e., S m = <ref> [0; 1] </ref> m (0; 1) m ). In addition, let the distribution of data points be uniform over S m . In other words, every point in S m has equal probability of being sampled as a data point.
Reference: [2] <author> S. F. Altschul, W. Gish, W. Miller, E. Myers, and D. J. Lipman. </author> <title> Basic local alignment search tool. </title> <journal> Journal of Molecular Biology, </journal> <volume> 215 </volume> <pages> 403-410, </pages> <year> 1990. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. [13, 24, 25, 19, 25, 21, 23, 16, 3]), sequences (e.g. <ref> [2, 1] </ref>), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant NAGW-3921, ORD contract 144-ET33, and NSF grant 144-GN62. 1 vectors.
Reference: [3] <author> Y. H. Ang, Zhao Li, and S. H. Ong. </author> <title> Image retrieval based on multidimensional feature properties. </title> <booktitle> In SPIE vol. </booktitle> <volume> 2420, </volume> <pages> pages 47-57, </pages> <year> 1995. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. <ref> [13, 24, 25, 19, 25, 21, 23, 16, 3] </ref>), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant
Reference: [4] <author> S. Arya. </author> <title> Nearest Neighbor Searching and Applications. </title> <type> PhD thesis, </type> <institution> Univ. of Maryland at College Park, </institution> <year> 1995. </year>
Reference-contexts: if the nearest neighbor is likely to be well-separated from most of the other data points, the answer size estimate can be used to choose between a linear scan or some indexing structure. 8 Related Work 8.1 Computational Geometry The nearest neighbor problem has been studied in computational geometry (e.g., <ref> [4, 5, 6, 8, 11] </ref>). However, the usual approach is to take the number of dimensions as a constant and find algorithms that behave well when the number of points is large enough.
Reference: [5] <author> S. Arya, D. M. Mount, and O. Narayan. </author> <title> Accounting for boundary effects in nearest neighbors searching. </title> <booktitle> In Proc. 11th ACM Symposium on Computational Geometry, </booktitle> <pages> pages 336-344, </pages> <year> 1995. </year>
Reference-contexts: if the nearest neighbor is likely to be well-separated from most of the other data points, the answer size estimate can be used to choose between a linear scan or some indexing structure. 8 Related Work 8.1 Computational Geometry The nearest neighbor problem has been studied in computational geometry (e.g., <ref> [4, 5, 6, 8, 11] </ref>). However, the usual approach is to take the number of dimensions as a constant and find algorithms that behave well when the number of points is large enough. <p> In [6] they recommend not to use the algorithm in more than 12 dimensions. It is impractical to use the algorithm in [8] when the number of points is much lower than exponential in the number of dimensions. 8.2 Boundary Effects and Index Structure Utility In <ref> [10, 5] </ref> it was observed that in some high dimensional cases, the estimate of NN query cost (using some index structure) can be very poor if "boundary effects" are not taken into account.
Reference: [6] <author> S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Wu. </author> <title> An optimal algorithm for nearest neighbor searching. </title> <booktitle> In Proc. 5th ACM SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 573-582, </pages> <year> 1994. </year>
Reference-contexts: Since the size of the answer set varies in our reformulation, we also provide a mechanism for estimating the size of the answer set. While this reformulation resembles the reformulation suggested in <ref> [6] </ref>, their reformulation is used to obtain an approximate nearest neighbor algorithm; in contrast, our goal is to ensure that there is a meaningful answer set for a given query. <p> We show that in many situations, for any fixed " &gt; 0, as dimensionality rises, the probability that a query is unstable converges to 1. Note that the points that fall in the enlarged query region are the valid answers to the approximate nearest neighbors problem (described in <ref> [6] </ref>). 3 3 NN in High-Dimensional Spaces This section contains our formulation of the problem, our formal analysis of the effect of dimensionality on the meaning of the result, and some formal implications of the result that enhance understanding of our primary result. 3.1 Notational Conventions We use the following notation <p> if the nearest neighbor is likely to be well-separated from most of the other data points, the answer size estimate can be used to choose between a linear scan or some indexing structure. 8 Related Work 8.1 Computational Geometry The nearest neighbor problem has been studied in computational geometry (e.g., <ref> [4, 5, 6, 8, 11] </ref>). However, the usual approach is to take the number of dimensions as a constant and find algorithms that behave well when the number of points is large enough. <p> However, the usual approach is to take the number of dimensions as a constant and find algorithms that behave well when the number of points is large enough. They observe that the problem is hard and define the approximate nearest neighbor problem as a weaker problem. In <ref> [6] </ref> there is an algorithm that retrieves an approximate nearest neighbor in O (log n) time for any data set. In [8] there is an algorithm that retrieves the true nearest neighbor in constant expected time under the IID dimensions assumption. <p> In [8] there is an algorithm that retrieves the true nearest neighbor in constant expected time under the IID dimensions assumption. However, the constants for those algorithms are exponential in dimensionality. In <ref> [6] </ref> they recommend not to use the algorithm in more than 12 dimensions.
Reference: [7] <author> A. Belussi and C. Faloutsos. </author> <title> Estimating the selectivity of spatial queries using the `correlation' fractal dimension. </title> <booktitle> In Proc. VLDB, </booktitle> <pages> pages 299-310, </pages> <year> 1995. </year>
Reference-contexts: There has been recent work on identifying these situations (e.g. <ref> [15, 7, 14] </ref>) and determining the useful dimensions (e.g. [18], which uses principal component analysis to identify meaningful dimensions). Of course, these techniques are only useful if NN in the underlying dimensionality is meaningful. 5 Experimental Studies of NN Theorem 1 only holds in the limit as dimensionality increases. <p> Moreover, in situations where nearest neighbors queries are unstable its hardly useful to process them or even estimate the cost of processing them. In addition, boundary effects are not explicitly related to distance distributions. 8.3 Fractal Dimensions In <ref> [15, 7, 14] </ref> it was suggested that real data sets usually have fractal properties (self-similarity, in particular) and that fractal dimensionality is a good tool in determining the performance of queries over the data set. 18 The following example illustrates that the fractal dimensionality of the data space from which we <p> However, are there real data sets for which the (estimated) fractal dimensionality is low, yet there is no separation between nearest and farthest neighbors? This is an intriguing question which we intend to explore in future work. (We used the technique described in <ref> [7] </ref> on the two real data sets described in Section 6. However, the fractal dimensionality of those data sets could not be estimated (when we divided the space once in each dimension, most of the data points occupied different cells).
Reference: [8] <author> J. L. Bentley, B. W. Weide, and A. C. Yao. </author> <title> Optimal expected-time algorithms for closest point problem. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 6(4) </volume> <pages> 563-580, </pages> <year> 1980. </year>
Reference-contexts: if the nearest neighbor is likely to be well-separated from most of the other data points, the answer size estimate can be used to choose between a linear scan or some indexing structure. 8 Related Work 8.1 Computational Geometry The nearest neighbor problem has been studied in computational geometry (e.g., <ref> [4, 5, 6, 8, 11] </ref>). However, the usual approach is to take the number of dimensions as a constant and find algorithms that behave well when the number of points is large enough. <p> They observe that the problem is hard and define the approximate nearest neighbor problem as a weaker problem. In [6] there is an algorithm that retrieves an approximate nearest neighbor in O (log n) time for any data set. In <ref> [8] </ref> there is an algorithm that retrieves the true nearest neighbor in constant expected time under the IID dimensions assumption. However, the constants for those algorithms are exponential in dimensionality. In [6] they recommend not to use the algorithm in more than 12 dimensions. <p> However, the constants for those algorithms are exponential in dimensionality. In [6] they recommend not to use the algorithm in more than 12 dimensions. It is impractical to use the algorithm in <ref> [8] </ref> when the number of points is much lower than exponential in the number of dimensions. 8.2 Boundary Effects and Index Structure Utility In [10, 5] it was observed that in some high dimensional cases, the estimate of NN query cost (using some index structure) can be very poor if "boundary
Reference: [9] <author> S. Berchtold, C. Bohm, B. Braunmuller, D. A. Keim, and H.-P. Kriegel. </author> <title> Fast parallel similarity search in multimedia databases. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. on Management of Data, </booktitle> <pages> pages 1-12, </pages> <year> 1997. </year>
Reference-contexts: First, our results indicate that while there exist situations in which high dimensional nearest neighbor queries are meaningful, they are very specific in nature and are quite different from the "independent dimensions" basis that most studies in the literature (e.g., <ref> [27, 17, 12, 9, 10] </ref>) use to evaluate techniques in a controlled manner. In the future, these NN technique evaluations should focus on those situations in which the results are meaningful. <p> Fetching a large number of data pages through a multi-dimensional index usually results in unordered retrieval.) For instance, the performance study of the parallel solution to the k-nearest neighbors problem presented in <ref> [9] </ref> indicates that their solution scales more poorly than a parallel scan of the data, and never beats a parallel scan in any of the presented data. [27] provides us with information on the performance of both the SS tree and the R* tree in finding the 20 nearest neighbors.
Reference: [10] <author> S. Berchtold, C. Bohm, D. A. Keim, and H.-P. Kriegel. </author> <title> A cost model for nearest neighbor search in high-dimensional data space. </title> <booktitle> In Proc. 16th ACM SIGACT-SIGMOD-SIGART Symposium on PODS, </booktitle> <pages> pages 78-86, </pages> <year> 1997. </year>
Reference-contexts: First, our results indicate that while there exist situations in which high dimensional nearest neighbor queries are meaningful, they are very specific in nature and are quite different from the "independent dimensions" basis that most studies in the literature (e.g., <ref> [27, 17, 12, 9, 10] </ref>) use to evaluate techniques in a controlled manner. In the future, these NN technique evaluations should focus on those situations in which the results are meaningful. <p> In [6] they recommend not to use the algorithm in more than 12 dimensions. It is impractical to use the algorithm in [8] when the number of points is much lower than exponential in the number of dimensions. 8.2 Boundary Effects and Index Structure Utility In <ref> [10, 5] </ref> it was observed that in some high dimensional cases, the estimate of NN query cost (using some index structure) can be very poor if "boundary effects" are not taken into account.
Reference: [11] <author> M. Bern. </author> <title> Approximate closest point queries in high dimensions. </title> <journal> Information Processing Letters, </journal> <volume> 45 </volume> <pages> 95-99, </pages> <year> 1993. </year>
Reference-contexts: if the nearest neighbor is likely to be well-separated from most of the other data points, the answer size estimate can be used to choose between a linear scan or some indexing structure. 8 Related Work 8.1 Computational Geometry The nearest neighbor problem has been studied in computational geometry (e.g., <ref> [4, 5, 6, 8, 11] </ref>). However, the usual approach is to take the number of dimensions as a constant and find algorithms that behave well when the number of points is large enough.
Reference: [12] <author> T. Bozkaya and M. Ozsoyoglu. </author> <title> Distance-based indexing for high-dimensional metric spaces. </title> <booktitle> In Proc. 16th ACM SIGACT-SIGMOD-SIGART Symposium on PODS, </booktitle> <pages> pages 357-368, </pages> <year> 1997. </year>
Reference-contexts: First, our results indicate that while there exist situations in which high dimensional nearest neighbor queries are meaningful, they are very specific in nature and are quite different from the "independent dimensions" basis that most studies in the literature (e.g., <ref> [27, 17, 12, 9, 10] </ref>) use to evaluate techniques in a controlled manner. In the future, these NN technique evaluations should focus on those situations in which the results are meaningful. <p> In [17], linear scan vastly outperforms the SR tree in all cases in this paper for the 16 dimensional synthetic dataset. For a 16 dimensional real dataset, the SR tree performs similarly to linear scan in a few experiments, but is usually beaten by linear scan. In <ref> [12] </ref>, performance numbers are presented for NN queries where bounds are imposed on the radius used to find the NN.
Reference: [13] <author> C. Faloutsos et al. </author> <title> Efficient and effective querying ny image content. </title> <journal> Journal of Intelligent Information Systems, </journal> <volume> 3(3) </volume> <pages> 231-262, </pages> <year> 1994. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. <ref> [13, 24, 25, 19, 25, 21, 23, 16, 3] </ref>), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant <p> Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. [13, 24, 25, 19, 25, 21, 23, 16, 3]), sequences (e.g. [2, 1]), video (e.g. <ref> [13] </ref>), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant NAGW-3921, ORD contract 144-ET33, and NSF grant 144-GN62. 1 vectors. <p> Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. [13, 24, 25, 19, 25, 21, 23, 16, 3]), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. <ref> [13, 26, 23, 20] </ref>)|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant NAGW-3921, ORD contract 144-ET33, and NSF grant 144-GN62. 1 vectors.
Reference: [14] <author> C. Faloutsos and V. </author> <title> Gaede. Analysis of n-dimensional quadtrees using the Housdorff fractal dimension. </title> <booktitle> In Proc. ACM SIGMOD Int. Conf. of the Management of Data, </booktitle> <year> 1996. </year> <month> 20 </month>
Reference-contexts: There has been recent work on identifying these situations (e.g. <ref> [15, 7, 14] </ref>) and determining the useful dimensions (e.g. [18], which uses principal component analysis to identify meaningful dimensions). Of course, these techniques are only useful if NN in the underlying dimensionality is meaningful. 5 Experimental Studies of NN Theorem 1 only holds in the limit as dimensionality increases. <p> Moreover, in situations where nearest neighbors queries are unstable its hardly useful to process them or even estimate the cost of processing them. In addition, boundary effects are not explicitly related to distance distributions. 8.3 Fractal Dimensions In <ref> [15, 7, 14] </ref> it was suggested that real data sets usually have fractal properties (self-similarity, in particular) and that fractal dimensionality is a good tool in determining the performance of queries over the data set. 18 The following example illustrates that the fractal dimensionality of the data space from which we
Reference: [15] <author> C. Faloutsos and I. Kamel. </author> <title> Beyond uniformity and independence: Analysis of R-trees using the concept of fractal dimension. </title> <booktitle> In Proc. 13th ACM SIGACT-SIGMOD-SIGART Symposium on PODS, </booktitle> <pages> pages 4-13, </pages> <year> 1994. </year>
Reference-contexts: There has been recent work on identifying these situations (e.g. <ref> [15, 7, 14] </ref>) and determining the useful dimensions (e.g. [18], which uses principal component analysis to identify meaningful dimensions). Of course, these techniques are only useful if NN in the underlying dimensionality is meaningful. 5 Experimental Studies of NN Theorem 1 only holds in the limit as dimensionality increases. <p> Moreover, in situations where nearest neighbors queries are unstable its hardly useful to process them or even estimate the cost of processing them. In addition, boundary effects are not explicitly related to distance distributions. 8.3 Fractal Dimensions In <ref> [15, 7, 14] </ref> it was suggested that real data sets usually have fractal properties (self-similarity, in particular) and that fractal dimensionality is a good tool in determining the performance of queries over the data set. 18 The following example illustrates that the fractal dimensionality of the data space from which we
Reference: [16] <author> U. M. Fayyad and P. Smyth. </author> <title> Automated analysis and exploration of image databases: Results, progress and challenges. </title> <journal> Journal of intelligent information systems, </journal> <volume> 4(1) </volume> <pages> 7-25, </pages> <year> 1995. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. <ref> [13, 24, 25, 19, 25, 21, 23, 16, 3] </ref>), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant
Reference: [17] <author> N. Katayama and S. Satoh. </author> <title> The SR-tree: An index structure for high-dimensional nearest neighbor queries. </title> <booktitle> In Proc. 16th ACM SIGACT-SIGMOD-SIGART Symposium on PODS, </booktitle> <pages> pages 369-380, </pages> <year> 1997. </year>
Reference-contexts: First, our results indicate that while there exist situations in which high dimensional nearest neighbor queries are meaningful, they are very specific in nature and are quite different from the "independent dimensions" basis that most studies in the literature (e.g., <ref> [27, 17, 12, 9, 10] </ref>) use to evaluate techniques in a controlled manner. In the future, these NN technique evaluations should focus on those situations in which the results are meaningful. <p> Conservatively assuming that linear scans cost 15% of a random examination of the data pages, linear scan outperforms both the SS tree and the R* tree at 10 dimensions in all cases. In <ref> [17] </ref>, linear scan vastly outperforms the SR tree in all cases in this paper for the 16 dimensional synthetic dataset. For a 16 dimensional real dataset, the SR tree performs similarly to linear scan in a few experiments, but is usually beaten by linear scan.
Reference: [18] <author> K.-I. Lin, H. V. Jagadish, and C. Faloutsos. </author> <title> The TV-Tree: An index structure for high-dimensional data. </title> <journal> VLDB Journal, </journal> <volume> 3(4) </volume> <pages> 517-542, </pages> <year> 1994. </year>
Reference-contexts: There has been recent work on identifying these situations (e.g. [15, 7, 14]) and determining the useful dimensions (e.g. <ref> [18] </ref>, which uses principal component analysis to identify meaningful dimensions). Of course, these techniques are only useful if NN in the underlying dimensionality is meaningful. 5 Experimental Studies of NN Theorem 1 only holds in the limit as dimensionality increases.
Reference: [19] <author> B. S. Manjunath and W. Y. Ma. </author> <title> Texture features for browsing and retrieval of image data. </title> <journal> In IEEE Trans. on Pattern Analysis and Machine Learning, </journal> <volume> volume 18(8), </volume> <pages> pages 837-842, </pages> <year> 1996. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. <ref> [13, 24, 25, 19, 25, 21, 23, 16, 3] </ref>), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant
Reference: [20] <author> R. Mehrotra and J. E. Gary. </author> <title> Feature-based retrieval of similar shapes. </title> <booktitle> In 9th Data Engineering Conference, </booktitle> <pages> pages 108-115, </pages> <year> 1992. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. [13, 24, 25, 19, 25, 21, 23, 16, 3]), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. <ref> [13, 26, 23, 20] </ref>)|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant NAGW-3921, ORD contract 144-ET33, and NSF grant 144-GN62. 1 vectors.
Reference: [21] <author> H. Murase and S. K. Nayar. </author> <title> Visual learning and recognition of 3D objects from appearance. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 14(1) </volume> <pages> 5-24, </pages> <year> 1995. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. <ref> [13, 24, 25, 19, 25, 21, 23, 16, 3] </ref>), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant
Reference: [22] <author> S. A. Nene and S. K. Nayar. </author> <title> A simple algorithm for nearest neighbor search in high dimensions. </title> <journal> In IEEE Trans. on Pattern Analysis and Machine Learning, </journal> <volume> volume 18(8), </volume> <pages> pages 989-1003, </pages> <year> 1996. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see <ref> [22] </ref>) complex data|such as images (e.g. [13, 24, 25, 19, 25, 21, 23, 16, 3]), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential
Reference: [23] <author> A. Pentland, R. W. Picard, and S. Scalroff. Photobook: </author> <title> Tools for content based manipulation of image databases. </title> <booktitle> In SPIE Volume 2185, </booktitle> <pages> pages 34-47, </pages> <year> 1994. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. <ref> [13, 24, 25, 19, 25, 21, 23, 16, 3] </ref>), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant <p> Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. [13, 24, 25, 19, 25, 21, 23, 16, 3]), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. <ref> [13, 26, 23, 20] </ref>)|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant NAGW-3921, ORD contract 144-ET33, and NSF grant 144-GN62. 1 vectors.
Reference: [24] <author> M. J. Swain and D. H. Ballard. </author> <title> Color indexing. </title> <journal> Inter. Journal of Computer Vision, </journal> <volume> 7(1) </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. <ref> [13, 24, 25, 19, 25, 21, 23, 16, 3] </ref>), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant
Reference: [25] <author> D. L. Swets and J. Weng. </author> <title> Using discriminant eigenfeatures for image retrieval. </title> <journal> In IEEE Trans. on Pattern Analysis and Machine Learning, </journal> <volume> volume 18(8), </volume> <pages> pages 831-836, </pages> <year> 1996. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. <ref> [13, 24, 25, 19, 25, 21, 23, 16, 3] </ref>), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. [13, 26, 23, 20])|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant
Reference: [26] <author> G. Taubin and D. B. Cooper. </author> <title> Recognition and positioning of rigid objects using algebraic moment invariants. </title> <booktitle> In SPIE Vol. </booktitle> <volume> 1570, </volume> <pages> pages 318-327, </pages> <year> 1991. </year>
Reference-contexts: Particular interest has centered on solving this problem in high dimensional spaces, which arise from techniques that approximate (e.g., see [22]) complex data|such as images (e.g. [13, 24, 25, 19, 25, 21, 23, 16, 3]), sequences (e.g. [2, 1]), video (e.g. [13]), and shapes (e.g. <ref> [13, 26, 23, 20] </ref>)|with long "feature" fl This work was partially supported by a "David and Lucile Packard Foundation Fellowship in Science and Engineering", a "Presidential Young Investigator" award, NASA research grant NAGW-3921, ORD contract 144-ET33, and NSF grant 144-GN62. 1 vectors.
Reference: [27] <author> D. A. White and R. Jain. </author> <title> Similarity indexing with the SS-Tree. </title> <booktitle> In ICDE, </booktitle> <pages> pages 516-523, </pages> <year> 1996. </year>
Reference-contexts: First, our results indicate that while there exist situations in which high dimensional nearest neighbor queries are meaningful, they are very specific in nature and are quite different from the "independent dimensions" basis that most studies in the literature (e.g., <ref> [27, 17, 12, 9, 10] </ref>) use to evaluate techniques in a controlled manner. In the future, these NN technique evaluations should focus on those situations in which the results are meaningful. <p> multi-dimensional index usually results in unordered retrieval.) For instance, the performance study of the parallel solution to the k-nearest neighbors problem presented in [9] indicates that their solution scales more poorly than a parallel scan of the data, and never beats a parallel scan in any of the presented data. <ref> [27] </ref> provides us with information on the performance of both the SS tree and the R* tree in finding the 20 nearest neighbors.
References-found: 27

