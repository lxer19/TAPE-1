URL: http://www.isi.edu/~mhall/sc91.ps
Refering-URL: http://www.isi.edu/~mhall/mypapers.html
Root-URL: http://www.isi.edu
Title: Interprocedural Transformations for Parallel Code Generation  
Author: Mary W. Hall Ken Kennedy Kathryn S. M c Kinley 
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science, Rice University,  
Abstract: We present a new approach that enables compiler optimization of procedure calls and loop nests containing procedure calls. We introduce two inter-procedural transformations that move loops across procedure boundaries, exposing them to traditional optimizations on loop nests. These transformations are incorporated into a code generation algorithm for a shared-memory multiprocessor. The code generator relies on a machine model to estimate the expected benefits of loop parallelization and parallelism-enhancing transformations. Several transformation strategies are explored and one that minimizes total execution time is selected. Efficient support of this strategy is provided by an existing interprocedural compilation system. We demonstrate the potential of these techniques by applying this code generation strategy to two scientific applications programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: However, if further optimization is desired, updating the dependence information is straightforward. 4.3 Loop Fusion Loop fusion places the bodies of two adjacent loops with the same number of iterations into a single loop <ref> [1] </ref>. When several procedure calls appear contiguously or loops and calls are adjacent, it may be possible to extract the outer loop from the called procedure (s), exposing loops for fusion and further optimization.
Reference: [2] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Ger-many, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: By first choosing a safe partition with the finest possible granularity and then grouping partitions, larger partitions may be formed. Any one of these groupings may expose the optimal parallelization of the loop. Unfortunately, there exists an exponential number of possible groupings <ref> [2] </ref>. To limit the search space, statement order is fixed based on a topological sort of all the dependences for L. Ambiguities are resolved in favor of placing parallel partitions adjacent to each other.
Reference: [3] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Section 6 describes an experiment where this approach was applied to the Perfect Benchmark programs spec77 and ocean. 2 Technical Background 2.1 Dependence Analysis Dependence analysis and testing have been widely researched, and in this paper a working knowledge of these is assumed <ref> [3, 7, 9, 17, 18, 27, 37] </ref>. In particular, the reader should be familiar with dependence graphs, where dependence edges are characterized with such information as dependence type and hybrid direction/distance vectors [25]. <p> The safety of loop interchange may be determined by inspecting the distance/direction vector to ensure that no existing dependence is reversed after interchange <ref> [3, 37] </ref>. Our algorithm considers loop interchange only when a perfect nest can be created via loop extraction, embedding, fusion, and distribution. If a loop contains more than one call, it may be possible to fuse the outer enclosing loops of calls to create a perfect nest. <p> min (hcost (fl 1 ; : : : ; l n1 ; l f ; l n g; k; body (l f )), ffuse, interchange, make l n kgi, ht; T i) endif return ht; T i then form an acyclic graph that can always be ordered using topological sort <ref> [3, 28] </ref>. By first choosing a safe partition with the finest possible granularity and then grouping partitions, larger partitions may be formed. Any one of these groupings may expose the optimal parallelization of the loop. Unfortunately, there exists an exponential number of possible groupings [2].
Reference: [4] <author> R. Allen and S. Johnson. </author> <title> Compiling C for vectorization, parallelization, and inline expansion. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution <ref> [4, 13, 23] </ref> and interprocedural analysis of array side effects [5, 9, 12, 20, 29, 30, 35]. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [5] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: In this paper, sections are extended to enable the code generator to determine the safety of inter-procedural transformations. We introduce an annotation to a section, called a slice. Slices resemble data access descriptors, but they are not as detailed <ref> [5] </ref>. A slice identifies the section of an array accessed and the order of that access in terms of a particular loop's index expression. Symbolic slices are stored only for the outermost loop of a procedure. They are also marked as exact or inexact. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution [4, 13, 23] and interprocedural analysis of array side effects <ref> [5, 9, 12, 20, 29, 30, 35] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. Section analysis used here loses precision because it only represents a few array substructures, and it merges sections for all references to a variable into a single section. <p> Sections inspired a similar but more detailed array summary analysis, data access descriptors, which stores access orders and expresses some additional shapes <ref> [5, 21, 22] </ref>. In fact, the slice annotation to sections could be obviated by using some of the techniques in Huelsbergen et. al. for determining exact array descriptors for use in dependence testing. However, slices are appealing due to our existing implementation and their simplicity.
Reference: [6] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Ocean has 1902 non-comment lines and is a 2-D fluid dynamics ocean simulation that also uses Fast Fourier Transforms. To locate opportunities for transformations, we browsed the dependences in the program using the ParaScope Editor <ref> [6, 25, 26] </ref>. Using other ParaScope tools, we determined which procedures in the program contained procedure calls. We examined the proce dures containing calls, looking for interesting call struc tures. We located adjacent calls, loops adjacent to calls, and loops containing calls which could be optimized.
Reference: [7] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Section 6 describes an experiment where this approach was applied to the Perfect Benchmark programs spec77 and ocean. 2 Technical Background 2.1 Dependence Analysis Dependence analysis and testing have been widely researched, and in this paper a working knowledge of these is assumed <ref> [3, 7, 9, 17, 18, 27, 37] </ref>. In particular, the reader should be familiar with dependence graphs, where dependence edges are characterized with such information as dependence type and hybrid direction/distance vectors [25].
Reference: [8] <author> P. Briggs, K. Cooper, M. W. Hall, and L. Torczon. </author> <title> Goal-directed interprocedural optimization. </title> <type> Technical Report TR90-147, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> De-cember </month> <year> 1990. </year>
Reference-contexts: For example, when the dimension size of a formal array parameter is also passed as a parameter, translating references of the formal to the actual can introduce multiplications of unknown symbolic values into subscript expressions. This situation occurs when inlining is used on the spec Benchmark program ma-trix300 <ref> [8] </ref>. In this paper, a hybrid approach is developed that overcomes some of these limitations. <p> Our approach to interprocedural optimization is fundamentally different from previous research in that the application of interprocedural transformations is restricted to cases where it is determined to be profitable. This strategy, called goal-directed interprocedural optimization, avoids the costs of interprocedural optimization when it is not necessary <ref> [8] </ref>. Interprocedural transformations are applied as dictated by a code generation algorithm that explores possible transformations, selecting a choice that minimizes total execution time. Estimates of execution time are provided by a machine model which takes into account the overhead of par-allelization.
Reference: [9] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Section 6 describes an experiment where this approach was applied to the Perfect Benchmark programs spec77 and ocean. 2 Technical Background 2.1 Dependence Analysis Dependence analysis and testing have been widely researched, and in this paper a working knowledge of these is assumed <ref> [3, 7, 9, 17, 18, 27, 37] </ref>. In particular, the reader should be familiar with dependence graphs, where dependence edges are characterized with such information as dependence type and hybrid direction/distance vectors [25]. <p> The array formal is replaced by a new array with the same shape as the actual. The references to the variable are translated by linearizing the formal's subscript expressions and then converting to the dimensions of the new array <ref> [9] </ref>. Finally, the subscript expressions for each dimension of the actual are added to those for the translated reference. This method is also the one that is used in inlining. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution [4, 13, 23] and interprocedural analysis of array side effects <ref> [5, 9, 12, 20, 29, 30, 35] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. Section analysis used here loses precision because it only represents a few array substructures, and it merges sections for all references to a variable into a single section.
Reference: [10] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(4) </volume> <pages> 334-358, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In this paper, the intraprocedural optimizations considered are loop fusion, loop interchange and loop distribution. However, many other transformations that require loop nests will also benefit from embedding and extraction. Some examples are loop skewing [36] and memory hierarchy optimizations such as unroll and jam <ref> [10] </ref>. As a motivating example, consider the Fortran code in Example 1 (a). The J loop in subroutine S may safely be made parallel, but the outer I loop in subroutine P may not be.
Reference: [11] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: They are also marked as exact or inexact. Figure 1 (c) illustrates the slice annotations for the program in Example 1. 3 Support for Interprocedural Optimization In this section, we present the compilation system of the ParaScope Programming Environment <ref> [11, 14] </ref>. This system was designed for the efficient support of interprocedural analysis and optimization. The tools in ParaScope cooperate to enable the compilation system to perform interprocedural analysis without direct examination of source code. This information is then used in code generation to make decisions about inter-procedural optimizations.
Reference: [12] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: A sophisticated form of interprocedural analysis, called regular section analysis, makes it possible to parallelize loops with calls by determining whether the side effects to arrays as a result of each call are limited to nonintersecting subar rays on different loop iterations <ref> [12, 20] </ref>. Interprocedural transformation is the process of moving code across procedure boundaries, either as an optimization or to enable other optimizations. The most common form of interprocedural transformation is procedure inlining. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution [4, 13, 23] and interprocedural analysis of array side effects <ref> [5, 9, 12, 20, 29, 30, 35] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. Section analysis used here loses precision because it only represents a few array substructures, and it merges sections for all references to a variable into a single section.
Reference: [13] <author> K. Cooper, M. W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In general, summary analysis for loop parallelization is less precise than the analysis of inlined code. On the other hand, inlining can yield an explosion in code size which may disastrously increase compile time and seriously inhibit separate compilation <ref> [13] </ref>. Furthermore, inlining may cause a loss of precision in dependence analysis due to the complexity of subscripts that result from array parameter reshapes. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution <ref> [4, 13, 23] </ref> and interprocedural analysis of array side effects [5, 9, 12, 20, 29, 30, 35]. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [14] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: They are also marked as exact or inexact. Figure 1 (c) illustrates the slice annotations for the program in Example 1. 3 Support for Interprocedural Optimization In this section, we present the compilation system of the ParaScope Programming Environment <ref> [11, 14] </ref>. This system was designed for the efficient support of interprocedural analysis and optimization. The tools in ParaScope cooperate to enable the compilation system to perform interprocedural analysis without direct examination of source code. This information is then used in code generation to make decisions about inter-procedural optimizations. <p> In this way, the information needed from each module of source code is available at all times and need not be derived on every compilation. Interprocedural optimization is orchestrated by the program compiler, a tool that manages and provides information about the whole program <ref> [14, 19] </ref>. The program compiler begins by building the augmented call graph described in Section 2.2. The program compiler then traverses the augmented call graph, performing interprocedural analysis, and subsequently, code generation. <p> To avoid significant code growth, multiple callers should share the same version of the optimized procedure whenever possible. This technique of generating multiple copies of a procedure and tailoring the copies to their calling environments is called procedure cloning <ref> [14] </ref>. Dependence Updates Because our code generator only applies loop extraction and loop embedding after safety and profitability are ensured, an update of local dependence information is not necessary.
Reference: [15] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: This information is then used in code generation to make decisions about inter-procedural optimizations. The code generator only examines the dependence graph for the procedure currently being compiled, not the graph for the entire program. In addition, ParaScope employs recompilation analysis after program changes to minimize program reanalysis <ref> [15] </ref>. 3.1 The ParaScope Compilation System Interprocedural analysis in the ParaScope compilation system consists of two principal phases. The first takes Page 3 Augmented Call Graph RSD Analysis RSDs Dependence Analysis Marked k Loops Dependence Graphs w/RSDs & Slices Code Generation place prior to compilation. <p> Recompilation analysis tests that interprocedural facts used to optimize a procedure have not been invalidated by editing changes <ref> [15] </ref>. To extend recompilation analysis for interprocedural transformations, a few additions are needed. When an interprocedural transformation is performed, a description of the interprocedural transformations annotates the nodes and edges in the augmented call graph.
Reference: [16] <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck. </author> <title> Supercomputer performance evaluation and the Perfect benchmarks. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: In practice, experimentation will be needed to differentiate these strategies. 6 Experimental Validation This section presents significant performance improvements due to interprocedural transformation on two scientific programs, spec77 and ocean, taken from the Perfect Benchmarks <ref> [16] </ref>. Spec77 contains 3278 non-comment lines and is a fluid dynamics weather simulation that uses Fast Fourier Transforms and rapid elliptic problem solvers. Ocean has 1902 non-comment lines and is a 2-D fluid dynamics ocean simulation that also uses Fast Fourier Transforms.
Reference: [17] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Section 6 describes an experiment where this approach was applied to the Perfect Benchmark programs spec77 and ocean. 2 Technical Background 2.1 Dependence Analysis Dependence analysis and testing have been widely researched, and in this paper a working knowledge of these is assumed <ref> [3, 7, 9, 17, 18, 27, 37] </ref>. In particular, the reader should be familiar with dependence graphs, where dependence edges are characterized with such information as dependence type and hybrid direction/distance vectors [25].
Reference: [18] <author> G. Goff, K. Kennedy, and C.-W. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Section 6 describes an experiment where this approach was applied to the Perfect Benchmark programs spec77 and ocean. 2 Technical Background 2.1 Dependence Analysis Dependence analysis and testing have been widely researched, and in this paper a working knowledge of these is assumed <ref> [3, 7, 9, 17, 18, 27, 37] </ref>. In particular, the reader should be familiar with dependence graphs, where dependence edges are characterized with such information as dependence type and hybrid direction/distance vectors [25].
Reference: [19] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: In this way, the information needed from each module of source code is available at all times and need not be derived on every compilation. Interprocedural optimization is orchestrated by the program compiler, a tool that manages and provides information about the whole program <ref> [14, 19] </ref>. The program compiler begins by building the augmented call graph described in Section 2.2. The program compiler then traverses the augmented call graph, performing interprocedural analysis, and subsequently, code generation. <p> This means that dependence analysis is only applied to procedures where it is no longer valid, allowing separate compilation to be preserved. The recompilation process after interprocedural transformations have been applied is described in more detail elsewhere <ref> [19] </ref>.
Reference: [20] <author> P. Havlak and K. Kennedy. </author> <title> Experience with interprocedural analysis of array side effects. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: A sophisticated form of interprocedural analysis, called regular section analysis, makes it possible to parallelize loops with calls by determining whether the side effects to arrays as a result of each call are limited to nonintersecting subar rays on different loop iterations <ref> [12, 20] </ref>. Interprocedural transformation is the process of moving code across procedure boundaries, either as an optimization or to enable other optimizations. The most common form of interprocedural transformation is procedure inlining. <p> The most common form of interprocedural transformation is procedure inlining. Inlining substitutes the body of a called procedure for the procedure call and optimizes it as a part of the calling procedure. Even though regular section analysis and inlining are frequently successful, each of these methods has its limitations <ref> [20, 23] </ref>. Compilation time and space considerations require that regular section analysis summarize array side effects. In general, summary analysis for loop parallelization is less precise than the analysis of inlined code. <p> This restriction on the shapes assists in making the implementation efficient <ref> [20] </ref>. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution [4, 13, 23] and interprocedural analysis of array side effects <ref> [5, 9, 12, 20, 29, 30, 35] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. Section analysis used here loses precision because it only represents a few array substructures, and it merges sections for all references to a variable into a single section. <p> However, these properties make it efficient enough to be widely used by code generation. In addition, experiments with regular section analysis on the linpack library demonstrated a 33 percent reduction in parallelism-inhibiting dependences, allowing 31 loops containing calls to be parallelized <ref> [20] </ref>. Comparing these numbers against published results of more precise techniques, there was no benefit to be gained by the increased precision of the other techniques [29, 30, 35].
Reference: [21] <author> L. Huelsbergen, D. Hahn, and J. Larus. </author> <title> Exact dependence analysis using data access descriptors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Sections inspired a similar but more detailed array summary analysis, data access descriptors, which stores access orders and expresses some additional shapes <ref> [5, 21, 22] </ref>. In fact, the slice annotation to sections could be obviated by using some of the techniques in Huelsbergen et. al. for determining exact array descriptors for use in dependence testing. However, slices are appealing due to our existing implementation and their simplicity.
Reference: [22] <author> L. Huelsbergen, D. Hahn, and J. Larus. </author> <title> Exact dependence analysis using data access descriptors. </title> <type> Technical Report 945, </type> <institution> Dept. of Computer Science, University of Wisconsin-Madison, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Sections inspired a similar but more detailed array summary analysis, data access descriptors, which stores access orders and expresses some additional shapes <ref> [5, 21, 22] </ref>. In fact, the slice annotation to sections could be obviated by using some of the techniques in Huelsbergen et. al. for determining exact array descriptors for use in dependence testing. However, slices are appealing due to our existing implementation and their simplicity.
Reference: [23] <author> C. A. Huson. </author> <title> An inline subroutine expander for Parafrase. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: The most common form of interprocedural transformation is procedure inlining. Inlining substitutes the body of a called procedure for the procedure call and optimizes it as a part of the calling procedure. Even though regular section analysis and inlining are frequently successful, each of these methods has its limitations <ref> [20, 23] </ref>. Compilation time and space considerations require that regular section analysis summarize array side effects. In general, summary analysis for loop parallelization is less precise than the analysis of inlined code. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution <ref> [4, 13, 23] </ref> and interprocedural analysis of array side effects [5, 9, 12, 20, 29, 30, 35]. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [24] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Loop distribution with Page 11 arbitrary control flow. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: For example, loop distribution may create loop nests of adjacent calls and loops which BestCost can optimize. Ordered Partitions. Loop distribution is safe if the partition of statements into new loops preserves all of the original dependences <ref> [24, 32] </ref>. Dependences are preserved if any statements involved in a cycle of dependences, a recurrence, are placed in the same loop (partition).
Reference: [25] <author> K. Kennedy, K. S. M c Kinley, and C.-W. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In particular, the reader should be familiar with dependence graphs, where dependence edges are characterized with such information as dependence type and hybrid direction/distance vectors <ref> [25] </ref>. The dependence graph specifies a conservative approximation of the partial order of memory accesses necessary to preserve the semantics of a program. <p> Ocean has 1902 non-comment lines and is a 2-D fluid dynamics ocean simulation that also uses Fast Fourier Transforms. To locate opportunities for transformations, we browsed the dependences in the program using the ParaScope Editor <ref> [6, 25, 26] </ref>. Using other ParaScope tools, we determined which procedures in the program contained procedure calls. We examined the proce dures containing calls, looking for interesting call struc tures. We located adjacent calls, loops adjacent to calls, and loops containing calls which could be optimized.
Reference: [26] <author> K. Kennedy, K. S. M c Kinley, and C.-W. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Ocean has 1902 non-comment lines and is a 2-D fluid dynamics ocean simulation that also uses Fast Fourier Transforms. To locate opportunities for transformations, we browsed the dependences in the program using the ParaScope Editor <ref> [6, 25, 26] </ref>. Using other ParaScope tools, we determined which procedures in the program contained procedure calls. We examined the proce dures containing calls, looking for interesting call struc tures. We located adjacent calls, loops adjacent to calls, and loops containing calls which could be optimized.
Reference: [27] <author> D. Kuck. </author> <title> The Structure of Computers and Computations, Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: Section 6 describes an experiment where this approach was applied to the Perfect Benchmark programs spec77 and ocean. 2 Technical Background 2.1 Dependence Analysis Dependence analysis and testing have been widely researched, and in this paper a working knowledge of these is assumed <ref> [3, 7, 9, 17, 18, 27, 37] </ref>. In particular, the reader should be familiar with dependence graphs, where dependence edges are characterized with such information as dependence type and hybrid direction/distance vectors [25].
Reference: [28] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: min (hcost (fl 1 ; : : : ; l n1 ; l f ; l n g; k; body (l f )), ffuse, interchange, make l n kgi, ht; T i) endif return ht; T i then form an acyclic graph that can always be ordered using topological sort <ref> [3, 28] </ref>. By first choosing a safe partition with the finest possible granularity and then grouping partitions, larger partitions may be formed. Any one of these groupings may expose the optimal parallelization of the loop. Unfortunately, there exists an exponential number of possible groupings [2].
Reference: [29] <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution [4, 13, 23] and interprocedural analysis of array side effects <ref> [5, 9, 12, 20, 29, 30, 35] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. Section analysis used here loses precision because it only represents a few array substructures, and it merges sections for all references to a variable into a single section. <p> Comparing these numbers against published results of more precise techniques, there was no benefit to be gained by the increased precision of the other techniques <ref> [29, 30, 35] </ref>. Sections inspired a similar but more detailed array summary analysis, data access descriptors, which stores access orders and expresses some additional shapes [5, 21, 22].
Reference: [30] <author> Z. Li and P. Yew. </author> <title> Interprocedural analysis and program restructuring for parallel programs. </title> <type> Technical Report 720, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution [4, 13, 23] and interprocedural analysis of array side effects <ref> [5, 9, 12, 20, 29, 30, 35] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. Section analysis used here loses precision because it only represents a few array substructures, and it merges sections for all references to a variable into a single section. <p> Comparing these numbers against published results of more precise techniques, there was no benefit to be gained by the increased precision of the other techniques <ref> [29, 30, 35] </ref>. Sections inspired a similar but more detailed array summary analysis, data access descriptors, which stores access orders and expresses some additional shapes [5, 21, 22].
Reference: [31] <author> R. McNaughton and H. Yamada. </author> <title> Regular expressions and state graphs for automata. </title> <journal> IRE Transactions on Electronic Computers, </journal> <volume> 9(1) </volume> <pages> 39-47, </pages> <year> 1960. </year>
Reference-contexts: Grouping partitions via dynamic programming. A dynamic programming solution is used to compute the best grouping for the finest granularity ordered partitions. This algorithm is similar to techniques for calculating the shortest path between two points in a graph <ref> [31] </ref>. The algorithm is O (N fl M 3 ). N is the number of perfectly nested loops. M is the maximum number of partitions and is less than or equal to the number of statements in the loop. Both N and M are typically small numbers.
Reference: [32] <author> Y. Muraoka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1971. </year> <note> Report No. 71-424. </note>
Reference-contexts: For example, loop distribution may create loop nests of adjacent calls and loops which BestCost can optimize. Ordered Partitions. Loop distribution is safe if the partition of statements into new loops preserves all of the original dependences <ref> [24, 32] </ref>. Dependences are preserved if any statements involved in a cycle of dependences, a recurrence, are placed in the same loop (partition).
Reference: [33] <author> C. Polychronopoulos. </author> <title> On Program Restructuring, Scheduling, and Communication for Parallel Processor Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illi-nois at Urbana-Champaign, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: This model includes the cost of arithmetic and conditional statements as well as operations such as parallel loops, sequential loops, and procedure call overhead. Both Polychronopoulos and Sarkar have used similar machine models in their research <ref> [33, 34] </ref>. 5.1 Machine Model and Performance Estimation A cost model is needed to compare the costs of various execution options. First, a method for estimating the cost of executing a sequential loop is presented.
Reference: [34] <author> V. Sarkar. </author> <title> Partition and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: This model includes the cost of arithmetic and conditional statements as well as operations such as parallel loops, sequential loops, and procedure call overhead. Both Polychronopoulos and Sarkar have used similar machine models in their research <ref> [33, 34] </ref>. 5.1 Machine Model and Performance Estimation A cost model is needed to compare the costs of various execution options. First, a method for estimating the cost of executing a sequential loop is presented.
Reference: [35] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of CALL statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 7 Related Work While the idea of interprocedural optimization is not new, previous work on interprocedural optimization for parallelization has limited its consideration to inline substitution [4, 13, 23] and interprocedural analysis of array side effects <ref> [5, 9, 12, 20, 29, 30, 35] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. Section analysis used here loses precision because it only represents a few array substructures, and it merges sections for all references to a variable into a single section. <p> Comparing these numbers against published results of more precise techniques, there was no benefit to be gained by the increased precision of the other techniques <ref> [29, 30, 35] </ref>. Sections inspired a similar but more detailed array summary analysis, data access descriptors, which stores access orders and expresses some additional shapes [5, 21, 22].
Reference: [36] <author> M. J. Wolfe. </author> <title> Loop skewing: The wavefront method revisited. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 279-293, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: These transformations expose such loops to intraprocedural optimizations. In this paper, the intraprocedural optimizations considered are loop fusion, loop interchange and loop distribution. However, many other transformations that require loop nests will also benefit from embedding and extraction. Some examples are loop skewing <ref> [36] </ref> and memory hierarchy optimizations such as unroll and jam [10]. As a motivating example, consider the Fortran code in Example 1 (a). The J loop in subroutine S may safely be made parallel, but the outer I loop in subroutine P may not be.
Reference: [37] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year> <pages> Page 12 </pages>
Reference-contexts: Section 6 describes an experiment where this approach was applied to the Perfect Benchmark programs spec77 and ocean. 2 Technical Background 2.1 Dependence Analysis Dependence analysis and testing have been widely researched, and in this paper a working knowledge of these is assumed <ref> [3, 7, 9, 17, 18, 27, 37] </ref>. In particular, the reader should be familiar with dependence graphs, where dependence edges are characterized with such information as dependence type and hybrid direction/distance vectors [25]. <p> The safety of loop interchange may be determined by inspecting the distance/direction vector to ensure that no existing dependence is reversed after interchange <ref> [3, 37] </ref>. Our algorithm considers loop interchange only when a perfect nest can be created via loop extraction, embedding, fusion, and distribution. If a loop contains more than one call, it may be possible to fuse the outer enclosing loops of calls to create a perfect nest.
References-found: 37

