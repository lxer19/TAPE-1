URL: http://www.eecis.udel.edu:80/~jchu/um94.ps
Refering-URL: http://www.eecis.udel.edu:80/~jchu/
Root-URL: http://www.cis.udel.edu
Email: Email: elzer@cis.udel.edu  Email: jchu@cis.udel.edu  Email: carberry@cis.udel.edu  
Title: Recognizing and Utilizing User Preferences in Collaborative Consultation Dialogues  
Author: Stephanie Elzer Jennifer Chu-Carroll Sandra Carberry 
Address: 19716, USA  19716, USA  Pennsylvania  
Affiliation: Department of Computer Science University of Delaware Newark, DE  Department of Computer Science University of Delaware Newark, DE  Department of Computer Science University of Delaware Visitor: IRCS University of  
Abstract: A natural language collaborative consultation system must take user preferences into account. This paper presents two strategies: one for dynamically recognizing user preferences during the course of a collaborative planning dialogue and another for exploiting the model of user preferences to detect suboptimal solutions and suggest better alternatives. By modeling and utilizing user preferences, our system is better able to fulfill its role as a collaborative agent. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bratman, M. </author> <year> 1990. </year> <title> What is intention? In Cohen, </title> <editor> P.; Morgan, J.; and Pollack, M., eds., </editor> <title> Intentions in Communication. </title> <publisher> MIT Press. </publisher> <address> chapter 2, 15--31. </address>
Reference-contexts: The system presumably has more extensive and accurate domain knowledge than the user, while the user has knowledge about his particular circumstances, intentions and preferences that are either restrictions on or potential influencers <ref> (Bratman 1990) </ref> of the domain plan that is constructed. If the domain involves preferential choice, the system needs to be able to evaluate potential solutions with respect to the user's preferences in order to determine when one set of actions is more desirable than another. <p> In addition, users are often not conscious of their preferences at the outset of planning and only bring these preferences into play as they must evaluate alternative actions and choose among them. Thus, we contend that preferences should not be treated as goals, but as potential influences <ref> (Bratman 1990) </ref> held with respect to actions. Overview We are currently modeling attribute-value preferences, where an attribute-value preference is a preference that an object in an action have a particular attribute value. <p> Van Beek's (van Beek 1987) system is capable of analyzing the user's plan and suggesting better alternatives, based on (Joshi, Webber, & Weischedel 1984). Our model improves upon van Beek's in that it 1) dynamically recognizes user preferences, 2) treats user preferences as potential influencers <ref> (Bratman 1990) </ref> of a plan rather than as goals, and 3) takes into account both the strength of the preference and the closeness of the match in evaluating user proposals. Conclusion This paper has addressed the recognition and exploitation of user preferences.
Reference: <author> Carberry, S. </author> <year> 1988. </year> <title> Modeling the user's plans and goals. </title> <note> Computational Linguistics 14(3):23--37. </note>
Reference: <author> Cawsey, A.; Galliers, J.; Logan, B.; Reece, S.; and Sparck Jones, K. </author> <year> 1993. </year> <title> Revising beliefs and intentions: A unified framework for agent interaction. </title> <booktitle> In The Ninth Biennial Conference of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour, </booktitle> <address> 130--139. </address>
Reference-contexts: Smith. (17) CS889 meets from 10:30 to 11:45am. These utterances initiate a negotiation with the user concerning the desirability of the user's proposed plan. Whereas Cawsey et al.'s system <ref> (Cawsey et al. 1993) </ref> initiates negotiation dialogues with the user based on discrepancies between the system's and the user's domain-related beliefs 6 The evidence for the preferences in Figure 4 was combined based on the reliability of the endorsements.
Reference: <author> Chin, D. </author> <year> 1989. </year> <title> KNOME: Modeling What the User Knows in UC. </title> <editor> In Kobsa, A., and Wahlster, W., eds., </editor> <title> User Models in Dialog Systems. </title> <publisher> Springer-Verlag. 74--107. </publisher>
Reference: <author> Chu-Carroll, J., and Carberry, S. </author> <year> 1994. </year> <title> A plan-based model for response generation in collaborative task-oriented dialogues. </title> <note> In Proceedings of AAAI. To Appear. </note>
Reference: <author> Cohen, P. </author> <year> 1985. </year> <title> Heuristic Reasoning about Uncertainty: An Artificial Intelligence Approach. </title> <publisher> Pitman Publishing Company. </publisher>
Reference-contexts: For example, a consultant will have more confidence in beliefs about an agent's preferences if they result from the agent's explicit statement to that effect than if they are deduced from a pattern of rejections by the agent. Our system captures this by using endorsements <ref> (Cohen 1985) </ref> to reflect the reliability of evidence. <p> Reliability Rating Endorsements Very-Strong Rej-Soln, Vol-Back Strong Vol, Q-A Moderate PH-Ded-Str Weak PH-Ded-Weak, Stereo Very-Weak PH-Ded-Init stronger. In addition, a preference can be attributed to the user on the basis of a user stereotype. Endorsements Endorsements are explicit records of factors that affect one's certainty in a hypothesis <ref> (Cohen 1985) </ref>. Cohen introduced the endorsement-based approach to reasoning under uncertainty and endorsements have since been used in belief models (Galliers 1992). We use endorsements to represent the evidence that the system has accumulated to support its belief about a user preference.
Reference: <author> Galliers, J. </author> <year> 1992. </year> <title> Autonomous belief revision and communication. </title> <editor> In Gardensfors., ed., </editor> <title> Belief Revision. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Endorsements Endorsements are explicit records of factors that affect one's certainty in a hypothesis (Cohen 1985). Cohen introduced the endorsement-based approach to reasoning under uncertainty and endorsements have since been used in belief models <ref> (Galliers 1992) </ref>. We use endorsements to represent the evidence that the system has accumulated to support its belief about a user preference. In our system, this evidence corresponds to the method used in detecting the preference. This allows us to clearly evaluate the quality of the evidence for each preference.
Reference: <author> Joshi, A.; Webber, B.; and Weischedel, R. </author> <year> 1984. </year> <title> Living up to expectations: Computing expert responses. </title> <booktitle> In Proceedings of AAAI, </booktitle> <address> 169--175. </address>
Reference-contexts: Van Beek's (van Beek 1987) system is capable of analyzing the user's plan and suggesting better alternatives, based on <ref> (Joshi, Webber, & Weischedel 1984) </ref>.
Reference: <author> Kass, R., and Finin, T. </author> <year> 1988. </year> <title> Modelling the User in Natural Language Systems. </title> <note> Computational Linguistics 14(3):5--22. </note>
Reference-contexts: Kass and Finin classify these preferences under the category of attitudes, and observe that people ``exhibit preferences and bias toward particular options or solutions. A natural language system may often need to recognize the bias and preferences a user has in order to communicate effectively.'' <ref> (Kass & Finin 1988) </ref> This observation is particularly relevant to collaborative consultation dialogues because of the different types of knowledge that the system and user bring to the dialogue.
Reference: <author> Kass, R. </author> <year> 1991. </year> <title> Building a User Model Implicitly from a Cooperative Advisory Dialog. User Modeling and User-Adapted Interaction 1(3):203--258. </title>
Reference-contexts: Related Work Previous work in user modeling has focused on such issues as inferring and utilizing user goals (Carberry 1988; McK-eown, Wish, & Matthews 1985; McKeown 1988), tailoring explanations to the user's level of expertise (Paris 1988; Chin 1989), and inferring user knowledge <ref> (Kass 1991) </ref>, as well as other relevant sources of information. However, none of the above take into account user preferences.
Reference: <author> Lambert, L., and Carberry, S. </author> <year> 1991. </year> <title> A tripartite plan-based model of dialogue. </title> <booktitle> In Proceedings of the ACL, </booktitle> <address> 47--54. </address>
Reference-contexts: When a user has proposed an action, either explicitly (e.g., Can I take CS360?) or implicitly (e.g., by asking a question such as Who is teaching CS360?, from which our plan recognition component <ref> (Lambert & Carberry 1991) </ref> would infer a Take-Course action), our response system (Chu-Carroll & Carberry 1994) will evaluate the specific action proposed by the user and determine whether a substantially better alternative exists.
Reference: <author> McKeown, K.; Wish, M.; and Matthews, K. </author> <year> 1985. </year> <title> Tailoring explanations for the user. </title> <booktitle> In Proceedings of the IJCAI. </booktitle>
Reference: <author> McKeown, K. </author> <year> 1988. </year> <title> Generating Goal-Oriented Explanations. </title> <journal> International Journal of Expert Systems 1(4):377-- 395. </journal>
Reference: <author> Morik, K. </author> <year> 1989. </year> <title> User Models and Conversational Settings: Modeling the User's Wants. </title> <editor> In Kobsa, A., and Wahlster, W., eds., </editor> <title> User Models in Dialog Systems. </title> <publisher> Springer-Verlag. 364--385. </publisher>
Reference: <author> Murray, W. </author> <year> 1991. </year> <title> An endorsement-based approach to student modelling for planner-controlled tutors. </title> <booktitle> In Proceedings of IJCAI, </booktitle> <address> 1100--1106. </address>
Reference-contexts: The endorsements are arranged into classes which reflect the reliability of the evidence represented by each endorsement. These classes are similar to Murray's endorsement reliability classes <ref> (Murray 1991) </ref>. Because our endorsements correspond to the methods by which the preferences are detected, each conversational circumstance is represented by an endorsement. The ranking of these endorsements also corresponds to their preference strength ranges. This correlation exists because of the collaborative nature of consultation dialogues.
Reference: <author> Paris, C. </author> <year> 1988. </year> <title> Tailoring Object Descriptions to a User's Level of Expertise. </title> <note> Computational Linguistics 14:64--78. </note>
Reference: <author> Reed, S. </author> <year> 1982. </year> <title> Cognition: Theory and Applications. </title> <publisher> Brooks/Cole Publishing Company. </publisher> <address> chapter 14, 337--365. </address>
Reference-contexts: Our recognition strategy is the first to recognize preferences during the course of a natural dialogue, utilizing characteristics of both the utterance itself and the dialogue in developing a model of user preferences. Our generation strategy reflects an additive model of human decision-making <ref> (Reed 1982) </ref> and takes into account both the strength of a preference and the closeness of a potential match. The examples in this paper have been run on our implemented system and are taken from a university domain of degrees, courses and requirements. <p> The instantiation with the highest rating is considered the best instantiation for the action under consideration. Thus the selection strategy employed by our ranking advisor corresponds to an additive model of human decision-making <ref> (Reed 1982) </ref>. Example of Ranking Two Objects Suppose the previous dialogue is continued as follows: (14) U: What is CS883? Our plan recognition component would infer Take-Course (U,CS883) as a domain action that the user is considering.
Reference: <author> Rich, E. </author> <year> 1979. </year> <title> User Modelling via Stereotypes. </title> <booktitle> Cognitive Science 3(4):329--354. </booktitle> <editor> van Beek, P. </editor> <year> 1987. </year> <title> A model for generating better explanations. </title> <booktitle> In Proceedings of the ACL, </booktitle> <address> 215--220. </address>
References-found: 18

