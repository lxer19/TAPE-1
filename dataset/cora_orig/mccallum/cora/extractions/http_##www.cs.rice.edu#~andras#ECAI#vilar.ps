URL: http://www.cs.rice.edu/~andras/ECAI/vilar.ps
Refering-URL: http://www.cs.rice.edu/~andras/ECAI/vilar.html
Root-URL: 
Email: fjvilar,evidalg@iti.upv.es jcamen@inf.uji.es  
Title: Learning Extended Finite State Models for Language Translation 1  
Author: J. M. Vilar E. Vidal and J. C. Amengual 
Address: 46020 Valencia, SPAIN. 12071 Castellon, SPAIN.  
Affiliation: Dpto. Sistemas Informaticos Computacion Unidad Predepartamental de Informatica Universidad Politecnica de Valencia, Universidad Jaume I,  
Abstract: The use of Subsequential Transducers (a kind of Finite-State Models) in Automatic Translation applications is considered. A methodology that improves the performance of the learning algorithm by means of an automatic reordering of the output sentences is presented. This technique yields a greater degree of synchrony between the input and output samples. The proposed approach leads to a reduction in the number of samples necessary to learn the transducer and a reduction in the size of the model so obtained. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.C. Amengual, E. Vidal. </author> <title> "Canonizacion del Lenguaje me-diante Tecnicas de Correccion de Errores" (in Spanish). </title> <type> Technical Report, </type> <institution> DSIC-II/17/95. Depto. de Sistemas In-formaticos y Computacion. Universidad Politecnica de Va-lencia. Spain. </institution> <month> September, </month> <year> 1995. </year>
Reference-contexts: This problem can be solved to some extent by means of Error-Correcting Parsing <ref> [1, 2, 3] </ref>. Under this approach, the input sentence, x, is considered as a corrupted version of some sentence ^x 2 L (L being the domain of the SST). The corruption process is modelled by means of an Error Model E, that comprises insertions, substitutions and deletions. <p> The overall distortion (word error rate) was 5%, some examples of distorted sentences can be seen in Figure 2 on p. 96. Testing was carried out through stochastic error-correcting parsing with the corresponding probabilities estimated from a distorted version of the training data <ref> [1, 2, 3] </ref>. The results, shown in Table 1, confirm that the rate of learning is significantly higher using the proposed reordering scheme and the obtained models are smaller.
Reference: [2] <author> J.C. Amengual, E. Vidal and J.M. </author> <title> Bened . "Simplifying Language through Error-Correcting Decoding". </title> <journal> Proceedings of the ICSLP96. </journal> <note> To be published. </note> <year> 1996. </year>
Reference-contexts: This problem can be solved to some extent by means of Error-Correcting Parsing <ref> [1, 2, 3] </ref>. Under this approach, the input sentence, x, is considered as a corrupted version of some sentence ^x 2 L (L being the domain of the SST). The corruption process is modelled by means of an Error Model E, that comprises insertions, substitutions and deletions. <p> The overall distortion (word error rate) was 5%, some examples of distorted sentences can be seen in Figure 2 on p. 96. Testing was carried out through stochastic error-correcting parsing with the corresponding probabilities estimated from a distorted version of the training data <ref> [1, 2, 3] </ref>. The results, shown in Table 1, confirm that the rate of learning is significantly higher using the proposed reordering scheme and the obtained models are smaller.
Reference: [3] <author> L. Baahl and F. Jelinek. </author> <title> "Decoding for Channels with Insertions, Deletions and Substitutions with Applications to Speech Recognition". </title> <journal> IEEE Transactions on Information Theory. Vol.IT-21, No.4, </journal> <volume> pp.404-411. </volume> <month> July, </month> <year> 1975. </year>
Reference-contexts: This problem can be solved to some extent by means of Error-Correcting Parsing <ref> [1, 2, 3] </ref>. Under this approach, the input sentence, x, is considered as a corrupted version of some sentence ^x 2 L (L being the domain of the SST). The corruption process is modelled by means of an Error Model E, that comprises insertions, substitutions and deletions. <p> The overall distortion (word error rate) was 5%, some examples of distorted sentences can be seen in Figure 2 on p. 96. Testing was carried out through stochastic error-correcting parsing with the corresponding probabilities estimated from a distorted version of the training data <ref> [1, 2, 3] </ref>. The results, shown in Table 1, confirm that the rate of learning is significantly higher using the proposed reordering scheme and the obtained models are smaller.
Reference: [4] <author> J. Berstel. </author> <title> Transductions and Context-Free Languages. </title> <publisher> Teubner, Stuttgart. </publisher> <year> 1979. </year>
Reference-contexts: Thanks to their conceptual and structural simplicity, these systems are significantly more robust than others based on the more conventional approach of loosely coupling an existing LT package to the output of a speech recognition front-end. In this paper we focus on Subsequential Transducers (SSTs) <ref> [4] </ref>. Output symbols or substrings are generated by a SST only after having seen enough input symbols to guarantee a correct output. The amount of symbols to wait for may be variable and context-dependent and it may also be necessary to produce output after the whole input has been seen. <p> allows for larger "asynchrony" between the input and the output 1 Work partially supported by the Spanish CICYT under grant TIC95-0984-C02-01 2 Supported by a grant of the Spanish Ministerio de Educacion y Ciencia sentences than with other simpler FS models such as Sequential Transducers and Mealy or Moore machines <ref> [4] </ref>. It should be noted that many translation tasks that may appear much more difficult, are inherently of this subsequential nature. <p> Every time an input symbol is accepted, the corresponding string is output and a new state is reached. After the whole input is processed, additional output may be produced from the last state reached in the analysis of the input <ref> [4] </ref>. Given a set of training pairs of sentences from a translation task, the Onward Subsequential Transducer Inference Algorithm (OSTIA) learns a SST that generalizes the training set [10, 11]. The algorithm builds a straightforward prefix c fl 1996 J. M. Vilar, E. Vidal and J. C.
Reference: [5] <author> P.F. Brown et al.. </author> <title> "A Statistical Approach to Machine Translation". </title> <journal> Computational Linguistics, </journal> <volume> Vol. 16, </volume> <pages> No.2, </pages> <address> pp.79-85, </address> <year> 1990. </year>
Reference: [6] <author> P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, R.L.Mercer. </author> <title> "The Mathematics of Statistical Machine Translation: Parameter Estimation". </title> <journal> Computational Linguistics, Vol.19, No.2, </journal> <volume> pp.263-311, </volume> <year> 1993. </year>
Reference-contexts: Nonetheless, rough, partial alignments can be easily obtained by pairing input/output words of each training sentence with the help of a (probabilistic) dictionary. Techniques to automatically build bilingual dictionaries from training parallel text have been proposed recently. In this work we use the so called "IBM Model-1" <ref> [6] </ref>. This simple stochastic translation model can be optimally trained from paired sentences and produces, as a byproduct, a stochastic dictionary. Those pairs of words having high likelihood of being translation of each-other are used to obtain the required partial alignments.
Reference: [7] <author> A. Castellanos, E. Vidal, I. Galiano. </author> <title> "Application of OS-TIA To Machine Translation Tasks". </title> <booktitle> 2nd International Colloquium on Grammatical Inference, proc., </booktitle> <address> Alicante, Spain, </address> <month> Sept., </month> <year> 1994. </year>
Reference-contexts: While this kind of models is often considered too simplistic to properly approach such a complex problem, results show that they can perform surprisingly well in Limited Domain (LD) tasks; that is, tasks with small or medium sized vocabulary and restricted semantic scope <ref> [7, 9, 12] </ref>. One of the reasons for this success lies on the fact that although natural languages are complex, the mappings defined by their translations can be comparatively much simpler, specially when these languages are close as is the case with many European languages. <p> Using this algorithm, a number of experiments have been carried out so far with LD LT, including speech-input applications <ref> [7, 9, 12, 14] </ref>. These works aimed at solving different problems arising in this kind of application. <p> SSTs base their translation ability on "delaying" the production of output words until enough of the input sentence has been seen to guarantee a correct output. This is illustrated in the following example of Spanish/English translation (from Feldman's task <ref> [8, 7] </ref>): se a~nade un triangulo grande y claro . a large light triangle is added . <p> parsing of S 0 yields new estimates of P L () and P E (j) and this parsing and estimation process can be iterated until convergence. 7 Experiments Spanish-English translation experiments were carried out with an extension of the so-called Miniature Language Acquisition Task recently proposed by Feldman et al. <ref> [7, 8] </ref>. A set of 16000 pairs was used for training, and a separate set of 10000 Span-ish sentences was used for testing. SSTs were learned from the training set using both the direct approach and the above described reordering scheme.
Reference: [8] <author> J.A. Feldman, G. Lakoff, A. Stolcke, S.H. Weber. </author> <title> "Miniature Language Acquisition: A touchstone for cognitive science". </title> <type> Technical Report, </type> <institution> TR-90-009. ICSI, Berkeley, Cali-fornia. </institution> <month> April, </month> <year> 1990. </year>
Reference-contexts: SSTs base their translation ability on "delaying" the production of output words until enough of the input sentence has been seen to guarantee a correct output. This is illustrated in the following example of Spanish/English translation (from Feldman's task <ref> [8, 7] </ref>): se a~nade un triangulo grande y claro . a large light triangle is added . <p> parsing of S 0 yields new estimates of P L () and P E (j) and this parsing and estimation process can be iterated until convergence. 7 Experiments Spanish-English translation experiments were carried out with an extension of the so-called Miniature Language Acquisition Task recently proposed by Feldman et al. <ref> [7, 8] </ref>. A set of 16000 pairs was used for training, and a separate set of 10000 Span-ish sentences was used for testing. SSTs were learned from the training set using both the direct approach and the above described reordering scheme.
Reference: [9] <author> V.M. Jim enez, A. Castellanos, E. Vidal, J. </author> <title> Oncina "Some Results with a Trainable Speech Translation and Understanding System". </title> <booktitle> Proc. of ICASSP95, </booktitle> <pages> pp. 113-116. </pages> <year> 1995. </year>
Reference-contexts: While this kind of models is often considered too simplistic to properly approach such a complex problem, results show that they can perform surprisingly well in Limited Domain (LD) tasks; that is, tasks with small or medium sized vocabulary and restricted semantic scope <ref> [7, 9, 12] </ref>. One of the reasons for this success lies on the fact that although natural languages are complex, the mappings defined by their translations can be comparatively much simpler, specially when these languages are close as is the case with many European languages. <p> Among the many attractive features of FS models for LT, an important one is the ease with which these models can be tightly integrated with standard acoustic-phonetic models of the input language, readily yielding quite effective speech-input LT systems <ref> [9] </ref>. Thanks to their conceptual and structural simplicity, these systems are significantly more robust than others based on the more conventional approach of loosely coupling an existing LT package to the output of a speech recognition front-end. In this paper we focus on Subsequential Transducers (SSTs) [4]. <p> Using this algorithm, a number of experiments have been carried out so far with LD LT, including speech-input applications <ref> [7, 9, 12, 14] </ref>. These works aimed at solving different problems arising in this kind of application. <p> Additionally, if models for the input and/or output languages are available, an extended version of OSTIA can be used which produces SSTs that only accept input sentences and only produce output sentences compatible with these models <ref> [9, 12] </ref>. This becomes of paramount importance when noisy and distorted input like speech is expected. SSTs base their translation ability on "delaying" the production of output words until enough of the input sentence has been seen to guarantee a correct output.
Reference: [10] <author> J. Oncina. "Aprendizaje de Lenguages Regulares y Funciones Subsecuenciales". </author> <type> Ph.D. </type> <institution> diss., Universidad Politecnica de Va-lencia, </institution> <year> 1991. </year>
Reference-contexts: In other words, we do not need to wait for a whole discourse to end before starting the translation. A distinctive advantage of SSTs is that they can be learned in a completely automatic manner from a sufficiently large corpus of training data by using a recently proposed algorithm <ref> [10, 11] </ref>. Using this algorithm, a number of experiments have been carried out so far with LD LT, including speech-input applications [7, 9, 12, 14]. These works aimed at solving different problems arising in this kind of application. <p> Given a set of training pairs of sentences from a translation task, the Onward Subsequential Transducer Inference Algorithm (OSTIA) learns a SST that generalizes the training set <ref> [10, 11] </ref>. The algorithm builds a straightforward prefix c fl 1996 J. M. Vilar, E. Vidal and J. C. Amengual Proceedings of the ECAI 96 Workshop Extended Finite State Models of Language Edited by A. <p> Finally a state merging process is carried out. The algorithm guarantees identification of the target transduction in the limit; that is, if the unknown target translation exhibits a subsequential structure, convergence to it is guaranteed whenever the set of training samples is representative <ref> [10, 11] </ref>. Additionally, if models for the input and/or output languages are available, an extended version of OSTIA can be used which produces SSTs that only accept input sentences and only produce output sentences compatible with these models [9, 12].
Reference: [11] <author> J. Oncina, P. Garc ia, E. Vidal. </author> <title> "Learning Subsequential Transducers for Pattern Recognition Interpretation Tasks". </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol.15, No.5, </journal> <volume> pp.448-458. </volume> <month> May, </month> <year> 1993. </year>
Reference-contexts: In other words, we do not need to wait for a whole discourse to end before starting the translation. A distinctive advantage of SSTs is that they can be learned in a completely automatic manner from a sufficiently large corpus of training data by using a recently proposed algorithm <ref> [10, 11] </ref>. Using this algorithm, a number of experiments have been carried out so far with LD LT, including speech-input applications [7, 9, 12, 14]. These works aimed at solving different problems arising in this kind of application. <p> Given a set of training pairs of sentences from a translation task, the Onward Subsequential Transducer Inference Algorithm (OSTIA) learns a SST that generalizes the training set <ref> [10, 11] </ref>. The algorithm builds a straightforward prefix c fl 1996 J. M. Vilar, E. Vidal and J. C. Amengual Proceedings of the ECAI 96 Workshop Extended Finite State Models of Language Edited by A. <p> Finally a state merging process is carried out. The algorithm guarantees identification of the target transduction in the limit; that is, if the unknown target translation exhibits a subsequential structure, convergence to it is guaranteed whenever the set of training samples is representative <ref> [10, 11] </ref>. Additionally, if models for the input and/or output languages are available, an extended version of OSTIA can be used which produces SSTs that only accept input sentences and only produce output sentences compatible with these models [9, 12]. <p> In the limit, as the training set becomes completely representative of a source subsequential transduction, the learning algorithm is guaranteed to yield a canonical (minimum-size) subsequential transducer <ref> [11, 12] </ref>. Thus the progress of learning generally entails a reduction of both the model size and the error rate. 8 Discussion A new technique that helps in mitigating one of the difficulties in learning (finite-state) translation models has been presented.
Reference: [12] <author> J. Oncina, A. Castellanos, E. Vidal, V. Jim enez. </author> <title> "Corpus-Based Machine Translation through Subsequential Transducers". </title> <booktitle> Third Int. Conf. on the Cognitive Science of Natural Language Processing, proc., </booktitle> <address> Dublin, </address> <year> 1994 </year>
Reference-contexts: While this kind of models is often considered too simplistic to properly approach such a complex problem, results show that they can perform surprisingly well in Limited Domain (LD) tasks; that is, tasks with small or medium sized vocabulary and restricted semantic scope <ref> [7, 9, 12] </ref>. One of the reasons for this success lies on the fact that although natural languages are complex, the mappings defined by their translations can be comparatively much simpler, specially when these languages are close as is the case with many European languages. <p> Using this algorithm, a number of experiments have been carried out so far with LD LT, including speech-input applications <ref> [7, 9, 12, 14] </ref>. These works aimed at solving different problems arising in this kind of application. <p> These works aimed at solving different problems arising in this kind of application. In particular, the need of Input/Output Language Models to cope with the distortions and noise involved by speech-input operation was first considered in <ref> [12] </ref> and specific techniques to keep the required amount of training data at reasonably small levels were first studied in [14]. <p> Additionally, if models for the input and/or output languages are available, an extended version of OSTIA can be used which produces SSTs that only accept input sentences and only produce output sentences compatible with these models <ref> [9, 12] </ref>. This becomes of paramount importance when noisy and distorted input like speech is expected. SSTs base their translation ability on "delaying" the production of output words until enough of the input sentence has been seen to guarantee a correct output. <p> In the limit, as the training set becomes completely representative of a source subsequential transduction, the learning algorithm is guaranteed to yield a canonical (minimum-size) subsequential transducer <ref> [11, 12] </ref>. Thus the progress of learning generally entails a reduction of both the model size and the error rate. 8 Discussion A new technique that helps in mitigating one of the difficulties in learning (finite-state) translation models has been presented.
Reference: [13] <author> E. Vidal, F. Casacuberta, P. Garc ia. </author> <title> "Grammatical Inference and Automatic Speech Recognition". In Speech Recognition and Coding. New Advances and Trends, </title> <editor> J. Rubio and J.M. Lopez, Eds. </editor> <publisher> Springer Verlag, </publisher> <year> 1994. </year>

References-found: 13

