URL: http://http.icsi.berkeley.edu/~luby/PAPERS/optspeedup.ps
Refering-URL: http://http.icsi.berkeley.edu/~luby/optsim.html
Root-URL: http://http.icsi.berkeley.edu
Email: email: dagum@camis.stanford.edu  email: karp@icsi.berkeley.edu  email: luby@icsi.berkeley.edu  email: smross@euler.berkeley.edu  
Title: An Optimal Algorithm for Monte Carlo Estimation  
Author: Paul Dagum Richard Karp Michael Luby Sheldon Ross 
Note: Research supported in part by National Science Foundation operating grant IRI-93-11950.  Research supported in part by National Science Foundation operating grant CCR-9304722 and NCR-9416101, United States-Israel Binational Science Foundation grant No. 92-00226, and ESPRIT BR Grant EC-US 030.  Research supported in part by operating grant DMS-9401834.  
Address: Stanford, CA 94305-5479.  Berkeley, CA 94704.  Berkeley, Berkeley, CA.  Berkeley, Berkeley, CA.  
Affiliation: Section on Medical Informatics, Stanford University School of Medicine,  International Computer Science Institute,  International Computer Science Institute, Berkeley, CA, and Computer Science Division, University of California at  Department of Industrial Engineering and Operations Research, University of California at  
Abstract: A typical approach to estimate an unknown quantity is to design an experiment that produces a random variable Z distributed in [0; 1] with E[Z] = , run this experiment independently a number of times and use the average of the outcomes as the estimate. In this paper, we consider the case when no a priori information about Z is known except that is distributed in [0; 1]. We describe an approximation algorithm AA which, given * and ffi, when running independent experiments with respect to any Z, produces an estimate that is within a factor 1 + * of with probability at least 1 ffi. We prove that the expected number of experiments run by AA (which depends on Z) is optimal to within a constant factor for every Z. An announcement of these results appears in P. Dagum, D. Karp, M. Luby, S. Ross, "An optimal algorithm for Monte-Carlo Estimation (extended abstract)", Proceedings of the Thirtysixth IEEE Symposium on Foundations of Computer Science, 1995, pp. 142-149 [3]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Broder, </author> <title> "How hard is it to marry at random? (On the approximation of the permanent)", </title> <booktitle> Proceedings of the Eighteenth IEEE Symposium on Theory of Computing, </booktitle> <year> 1986, </year> <pages> pp. 50-58. </pages> <booktitle> Erratum in Proceedings of the Twentieth IEEE Symposium on Theory of Computing, </booktitle> <year> 1988, </year> <pages> pp. 551. </pages>
Reference-contexts: One of the simplest design problems is the problem of deciding when to stop sampling. For example, suppose Z 1 ; Z 2 ; : : : are independently and identically distributed according to Z in the interval <ref> [0; 1] </ref> with mean Z . From Bernstein's inequality, we know that if N is fixed proportional to ln (1=ffi)=* 2 and S = Z 1 + + Z N , then with probability at least 1 ffi, S=N approximates Z with absolute error *. <p> For example, many researchers have devoted substantial effort to the important and difficult problem of approximating the permanent of 0 1 valued matrices <ref> [1, 4, 5, 9, 10, 13, 14] </ref>. <p> We describe a more powerful algorithm, the AA algorithm, that on inputs *, ffi, and independently and identically distributed outcomes Z 1 ; Z 2 ; ::: generated from any random variable Z distributed in <ref> [0; 1] </ref>, outputs an (*; ffi)-approximation of Z after an expected 4 number of experiments proportional to ae Z = 2 Z . Unlike the simple, stopping-rule based algorithm, we prove that for all Z, AA runs the optimal number of experiments to within a constant factor. <p> This algorithm is used in the first step of the approximation algorithm AA that we describe in Subsection 2.2. 5 2.1 Stopping Rule Algorithm Let Z be a random variable distributed in the interval <ref> [0; 1] </ref> with mean Z . Let Z 1 ; Z 2 ; : : : be independently and identically distributed according to Z. Stopping Rule Algorithm Input Parameters: (*; ffi) with 0 &lt; * &lt; 1, ffi &gt; 0. Let 1 = 1 + (1 + *). <p> Let 1 = 1 + (1 + *). Initialize N 0, S 0 While S &lt; 1 do: N N + 1; S S + Z N Output: ~ Z 1 =N Stopping Rule Theorem: Let Z be a random variable distributed in <ref> [0; 1] </ref> with Z = E [Z] &gt; 0. Let ~ Z be the estimate produced and let N Z be the number of experiments that the Stopping Rule Algorithm runs with respect to Z on input * and ffi. <p> Let Z be a random variable distributed in the interval <ref> [0; 1] </ref> with mean Z and variance oe 2 1 ; Z 0 2 ; : : : denote two sets of random variables independently and identically distributed according to Z. Approximation Algorithm AA Input Parameters: (*; ffi), with 0 &lt; * 1 and 0 &lt; ffi 1. <p> Step 3: Set N = 2 ^ae Z =^ 2 Z and initialize S 0. For i = 1; : : : ; N do: S S + Z i . Output: ~ Z AA Theorem: Let Z be any random variable distributed in <ref> [0; 1] </ref>, let Z = E [Z] &gt; 0 be the mean of Z, oe 2 Z be the variance of Z, and ae Z = maxfoe 2 Z ; * Z g. <p> Let BB be any algorithm that on input (*; ffi) works as follows with respect to Z. Let Z 1 ; Z 2 ; : : : denote independently and identically distributed according to Z with values in the interval <ref> [0; 1] </ref>. BB runs an experiment, and on the N th run BB receives the value Z N . <p> Lower Bound Theorem: Let BB be any algorithm that works as described above on input (*; ffi). Let Z be a random variable distributed in <ref> [0; 1] </ref>, let Z be the mean of Z, oe 2 Z be the variance of Z, and ae Z = maxfoe 2 Z ; * Z g. <p> Nfi 2 Since e di 0+ 0 is a constant, E [e di 0+ 0 ] = E [e di 0+ completing the proof of Equation (5). 2 We use Lemma 4.6 to generalize the Zero-One Estimator Theorem [17] from f0; 1g-valued random variables to random variables in the interval <ref> [0; 1] </ref>. Generalized Zero-One Estimator Theorem: Let Z 1 ; Z 2 ; : : : ; Z N denote random variables independent and identically distributed according to Z. <p> The first part of the proof also follows directly from Lemma 4.6. Recall that = (e 2) :72 and 1 = 1 + (1 + *) = 1 + 4 ln (2=ffi)(1 + *)=* 2 . Stopping Rule Theorem: Let Z be a random variable distributed in <ref> [0; 1] </ref> with Z = E [Z] &gt; 0. Let ~ Z be the estimate produced and let N Z be the number of experiments that the Stopping Rule Algorithm runs with respect to Z on input * and ffi. <p> E [1=~ 2 Z ] = O (1= 2 can be easily proved based on the ideas used in the proof of Part (2) of the Stopping Rule Theorem. 2 6 Proof of the AA Theorem AA Theorem: Let Z be any random variable distributed in <ref> [0; 1] </ref>, let Z be the mean of Z, oe 2 Z be the variance of Z, and ae Z = maxfoe 2 Z ; * Z g. <p> Let Z be a random variable distributed in <ref> [0; 1] </ref>, let Z be the mean of Z, oe 2 Z be the variance of Z, and ae Z = maxfoe 2 Z ; * Z g. <p> Define i + k = k + 1 and i k . For stopping time T , we get from Wald's first identity E Z [i + and T ] = E Z 0 [T ]E Z 0 <ref> [ 1 ] </ref>: Next, let denote the space of all inputs on which the test rejects H Z , and let c denotes its complement. Thus, by definition, we require that Pr Z [] = ff and Pr Z [ c ] = 1 ff.
Reference: [2] <author> R. Canetti, G. Even, O. Goldreich, </author> <title> "Lower bounds for sampling algorithms for estimating the average ", Technical Report # 789, </title> <institution> Department of Computer Science, Technion, </institution> <month> Nov. </month> <year> 1993. </year> <month> 20 </month>
Reference-contexts: such as approximating probabilistic inference in Bayesian networks [6], approximating the volume of convex bodies [7], solving the Ising model of statistical mechanics [11], solving for network reliability in planar multiterminal networks [15, 16], approximating the number of solutions to a DNF formula [17] or, more generally, to a GF <ref> [2] </ref> formula [18], and approximating the number of Eulerian orientations of a graph [19]. Define = (e 2) :72 and let oe 2 Z denote the variance of Z. <p> then BB runs an expected number of experiments proportional to at least ae Z = 2 Z . (Canetti, Evan and Goldreich prove the related lower bound (ln (1=ffi)=* 2 ) on the number of experiments required to approximate Z with absolute error * with probability at least 1 ffi <ref> [2] </ref>.) Thus we show that for any random variable Z, AA runs an expected number of experiments that is within a constant factor of the minimum expected number.
Reference: [3] <author> P. Dagum, D. Karp, M. Luby, S. Ross, </author> <title> "An optimal algorithm for Monte-Carlo estimation (extended abstract)", </title> <booktitle> Proceedings of the Thirtysixth IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1995, </year> <pages> pp. 142-149 </pages>
Reference: [4] <author> P. Dagum, M. Luby, M. Mihail, U. Vazirani, </author> <title> "Polytopes, permanents and graphs with large factors", </title> <booktitle> Proceedings of the Twentyninth IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1988, </year> <pages> pp. 412-421 </pages>
Reference-contexts: For example, many researchers have devoted substantial effort to the important and difficult problem of approximating the permanent of 0 1 valued matrices <ref> [1, 4, 5, 9, 10, 13, 14] </ref>.
Reference: [5] <author> P. Dagum, M. Luby, </author> <title> "Approximating the Permanent of Graphs with Large Factors," </title> <journal> Theoretical Computer Science, Part A, </journal> <volume> Vol. 102, </volume> <year> 1992, </year> <pages> pp. 283-305. </pages>
Reference-contexts: For example, many researchers have devoted substantial effort to the important and difficult problem of approximating the permanent of 0 1 valued matrices <ref> [1, 4, 5, 9, 10, 13, 14] </ref>.
Reference: [6] <author> P. Dagum, M. Luby, </author> <title> "An optimal approximation algorithm for Bayesian inference" Artificial Intelligence Journal, </title> <note> 1997, In press. </note>
Reference-contexts: Researchers have also used (*; ffi)-approximations to tackle many other difficult problems, such as approximating probabilistic inference in Bayesian networks <ref> [6] </ref>, approximating the volume of convex bodies [7], solving the Ising model of statistical mechanics [11], solving for network reliability in planar multiterminal networks [15, 16], approximating the number of solutions to a DNF formula [17] or, more generally, to a GF [2] formula [18], and approximating the number of Eulerian
Reference: [7] <author> M. Dyer, A. Frieze, R. Kannan, </author> <title> "A random polynomial time algorithm for approximating the volume of convex bodies", </title> <booktitle> Proceedings of the Twentyfirst IEEE Symposium on Theory of Computing, </booktitle> <year> 1989, </year> <pages> pp. 375-381. </pages>
Reference-contexts: Researchers have also used (*; ffi)-approximations to tackle many other difficult problems, such as approximating probabilistic inference in Bayesian networks [6], approximating the volume of convex bodies <ref> [7] </ref>, solving the Ising model of statistical mechanics [11], solving for network reliability in planar multiterminal networks [15, 16], approximating the number of solutions to a DNF formula [17] or, more generally, to a GF [2] formula [18], and approximating the number of Eulerian orientations of a graph [19]. <p> Yet, for many problem instances of size n, the number of experiments run by AA is significantly smaller than this bound. Other examples exist where the bounds are also extremely loose for many typical problem instances <ref> [7, 10, 11] </ref>.
Reference: [8] <author> M. Dyer, A. Frieze, R. Kannan, A. Kapoor, L. Kerkovic, U. Vazirani, </author> <title> "A Mildly Exponential Time Algorithm for Approximating the Number of Solutions to a Multidimensional Knapsack Problem", Combinatorics, </title> <journal> Probability and Computing, 1993, </journal> <volume> Vol. 2, </volume> <pages> pp. 271-284. </pages>
Reference-contexts: In other related work, Dyer et al describe a stopping rule based algorithm that provides an upper bound estimate on Z <ref> [8] </ref>. With probability 1 ffi, the estimate is at most (1 + *) Z , but the estimate can be arbitrarily smaller than in the challenging case when is small. We first describe an approximation algorithm based on a simple stopping rule.
Reference: [9] <author> M. Jerrum, </author> <title> "An analysis of a Monte Carlo algorithm for estimating the permanent", </title> <booktitle> Proceedings of the Third Conference on Integer Programming and Combinatorial Optimization, </booktitle> <year> 1993, </year> <pages> pp. 171-182. </pages>
Reference-contexts: For example, many researchers have devoted substantial effort to the important and difficult problem of approximating the permanent of 0 1 valued matrices <ref> [1, 4, 5, 9, 10, 13, 14] </ref>.
Reference: [10] <author> M. Jerrum, A. Sinclair, </author> <title> "Conductance and the rapid mixing property for Markov Chains: the approximation of the permanent resolved", </title> <booktitle> Proceedings of the Twentieth IEEE Symposium on Theory of Computing, </booktitle> <year> 1988, </year> <pages> pp. 235-243. </pages>
Reference-contexts: For example, many researchers have devoted substantial effort to the important and difficult problem of approximating the permanent of 0 1 valued matrices <ref> [1, 4, 5, 9, 10, 13, 14] </ref>. <p> Yet, for many problem instances of size n, the number of experiments run by AA is significantly smaller than this bound. Other examples exist where the bounds are also extremely loose for many typical problem instances <ref> [7, 10, 11] </ref>.
Reference: [11] <author> M. Jerrum, A. Sinclair, </author> <title> "Polynomial-time approximation algorithms for the Ising model", </title> <journal> SIAM Journal on Computing, </journal> <volume> 22, </volume> <year> 1993. </year> <month> 21 </month>
Reference-contexts: Researchers have also used (*; ffi)-approximations to tackle many other difficult problems, such as approximating probabilistic inference in Bayesian networks [6], approximating the volume of convex bodies [7], solving the Ising model of statistical mechanics <ref> [11] </ref>, solving for network reliability in planar multiterminal networks [15, 16], approximating the number of solutions to a DNF formula [17] or, more generally, to a GF [2] formula [18], and approximating the number of Eulerian orientations of a graph [19]. <p> Yet, for many problem instances of size n, the number of experiments run by AA is significantly smaller than this bound. Other examples exist where the bounds are also extremely loose for many typical problem instances <ref> [7, 10, 11] </ref>.
Reference: [12] <author> M. Jerrum, L. Valiant, V. Vazirani, </author> <title> "Random generation of combinatorial structures from a uniform distribution", </title> <booktitle> Theoretical Computer Science, 1986, </booktitle> <volume> Volume 43, </volume> <pages> pp. 169-188. </pages>
Reference-contexts: Define = (e 2) :72 and let oe 2 Z denote the variance of Z. Define ae Z = maxfoe 2 Z ; * Z g: We first prove a slight generalization of the Zero-One Estimator Theorem <ref> [12, 15, 16, 17] </ref>. The new theorem, the Generalized Zero-One Estimator Theorem, proves that if N = ae Z = 2 then S=N is an (*; ffi)-approximation of Z .
Reference: [13] <author> M. Jerrum, U. Vazirani, </author> <title> "A mildly exponential approximation algorithm for the permanent", </title> <booktitle> Proceedings of the Thirtythird IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1992, </year> <pages> pp. 320-326. </pages>
Reference-contexts: For example, many researchers have devoted substantial effort to the important and difficult problem of approximating the permanent of 0 1 valued matrices <ref> [1, 4, 5, 9, 10, 13, 14] </ref>. <p> Thus, AA provides substantial computational savings in applications that employ a poor upper bound on ae Z = 2 Z . For example, the best known a priori bound on for the problem of approximating the permanent of size n is superpolynomial in n <ref> [13] </ref>. Yet, for many problem instances of size n, the number of experiments run by AA is significantly smaller than this bound. Other examples exist where the bounds are also extremely loose for many typical problem instances [7, 10, 11].
Reference: [14] <author> N. Karmarkar, R. Karp, R. Lipton, L. Lovasz and M. Luby, </author> <title> "A Monte-Carlo Algorithm for Estimating the Permanent", </title> <journal> SIAM J. on Computing, 1993, </journal> <volume> Vol. 22, No. 2, </volume> <pages> pp. 284-293. </pages>
Reference-contexts: For example, many researchers have devoted substantial effort to the important and difficult problem of approximating the permanent of 0 1 valued matrices <ref> [1, 4, 5, 9, 10, 13, 14] </ref>.
Reference: [15] <author> R. Karp and M. Luby, </author> <title> "Monte Carlo algorithms for planar multiterminal network reliability problems", </title> <journal> Journal of Complexity, 1985, </journal> <volume> Volume 2, </volume> <pages> pp. 45-64. </pages>
Reference-contexts: Researchers have also used (*; ffi)-approximations to tackle many other difficult problems, such as approximating probabilistic inference in Bayesian networks [6], approximating the volume of convex bodies [7], solving the Ising model of statistical mechanics [11], solving for network reliability in planar multiterminal networks <ref> [15, 16] </ref>, approximating the number of solutions to a DNF formula [17] or, more generally, to a GF [2] formula [18], and approximating the number of Eulerian orientations of a graph [19]. Define = (e 2) :72 and let oe 2 Z denote the variance of Z. <p> Define = (e 2) :72 and let oe 2 Z denote the variance of Z. Define ae Z = maxfoe 2 Z ; * Z g: We first prove a slight generalization of the Zero-One Estimator Theorem <ref> [12, 15, 16, 17] </ref>. The new theorem, the Generalized Zero-One Estimator Theorem, proves that if N = ae Z = 2 then S=N is an (*; ffi)-approximation of Z .
Reference: [16] <author> R. Karp and M. Luby, </author> <title> "Monte Carlo algorithms for enumeration and reliability problems", </title> <booktitle> Proceedings of the Twentyfourth IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1983, </year> <pages> pp. 56-64. </pages>
Reference-contexts: Researchers have also used (*; ffi)-approximations to tackle many other difficult problems, such as approximating probabilistic inference in Bayesian networks [6], approximating the volume of convex bodies [7], solving the Ising model of statistical mechanics [11], solving for network reliability in planar multiterminal networks <ref> [15, 16] </ref>, approximating the number of solutions to a DNF formula [17] or, more generally, to a GF [2] formula [18], and approximating the number of Eulerian orientations of a graph [19]. Define = (e 2) :72 and let oe 2 Z denote the variance of Z. <p> Define = (e 2) :72 and let oe 2 Z denote the variance of Z. Define ae Z = maxfoe 2 Z ; * Z g: We first prove a slight generalization of the Zero-One Estimator Theorem <ref> [12, 15, 16, 17] </ref>. The new theorem, the Generalized Zero-One Estimator Theorem, proves that if N = ae Z = 2 then S=N is an (*; ffi)-approximation of Z .
Reference: [17] <author> R. Karp and M. Luby and N. </author> <title> Madras, "Monte Carlo approximation algorithms for enumeration problems", </title> <journal> Journal of Algorithms, 1989, </journal> <volume> Volume 10, </volume> <pages> pp. 429-448. </pages>
Reference-contexts: ffi)-approximations to tackle many other difficult problems, such as approximating probabilistic inference in Bayesian networks [6], approximating the volume of convex bodies [7], solving the Ising model of statistical mechanics [11], solving for network reliability in planar multiterminal networks [15, 16], approximating the number of solutions to a DNF formula <ref> [17] </ref> or, more generally, to a GF [2] formula [18], and approximating the number of Eulerian orientations of a graph [19]. Define = (e 2) :72 and let oe 2 Z denote the variance of Z. <p> Define = (e 2) :72 and let oe 2 Z denote the variance of Z. Define ae Z = maxfoe 2 Z ; * Z g: We first prove a slight generalization of the Zero-One Estimator Theorem <ref> [12, 15, 16, 17] </ref>. The new theorem, the Generalized Zero-One Estimator Theorem, proves that if N = ae Z = 2 then S=N is an (*; ffi)-approximation of Z . <p> This approach is known as sequential analysis and originated with the work of Wald on statistical decision theory [22]. Related research has applied sequential analysis to specific Monte Carlo approximation problems such as estimating the number of points in a union of sets <ref> [17] </ref> and estimating the number of self-avoiding walks [20]. In other related work, Dyer et al describe a stopping rule based algorithm that provides an upper bound estimate on Z [8]. <p> [e di 0+ 0 ] e di 0+ ff 0 fi 0 Nfi 2 Since e di 0+ 0 is a constant, E [e di 0+ 0 ] = E [e di 0+ completing the proof of Equation (5). 2 We use Lemma 4.6 to generalize the Zero-One Estimator Theorem <ref> [17] </ref> from f0; 1g-valued random variables to random variables in the interval [0; 1]. Generalized Zero-One Estimator Theorem: Let Z 1 ; Z 2 ; : : : ; Z N denote random variables independent and identically distributed according to Z.
Reference: [18] <author> M. Karpinski, M. Luby, </author> <title> "Approximating the Number of Solutions to a GF[2] Formula," </title> <booktitle> Proceedings of the Second ACM/SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1991, </year> <journal> and Journal of Algorithms, </journal> <volume> Vol. 14, No. 2, </volume> <month> March </month> <year> 1993, </year> <pages> pp. 280-287. </pages>
Reference-contexts: approximating probabilistic inference in Bayesian networks [6], approximating the volume of convex bodies [7], solving the Ising model of statistical mechanics [11], solving for network reliability in planar multiterminal networks [15, 16], approximating the number of solutions to a DNF formula [17] or, more generally, to a GF [2] formula <ref> [18] </ref>, and approximating the number of Eulerian orientations of a graph [19]. Define = (e 2) :72 and let oe 2 Z denote the variance of Z.
Reference: [19] <author> M. Mihail, P. Winkler, </author> <title> "On the number of Eulerian orientations of a graph", </title> <booktitle> Proceedings of the Third ACM/SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1992, </year> <pages> pp. 138-145. </pages>
Reference-contexts: convex bodies [7], solving the Ising model of statistical mechanics [11], solving for network reliability in planar multiterminal networks [15, 16], approximating the number of solutions to a DNF formula [17] or, more generally, to a GF [2] formula [18], and approximating the number of Eulerian orientations of a graph <ref> [19] </ref>. Define = (e 2) :72 and let oe 2 Z denote the variance of Z. Define ae Z = maxfoe 2 Z ; * Z g: We first prove a slight generalization of the Zero-One Estimator Theorem [12, 15, 16, 17].
Reference: [20] <author> D. Randall, A. Sinclair, </author> <title> "Testable algorithms for self-avoiding walks", </title> <booktitle> Proceedings of the Fifth ACM/SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1994, </year> <pages> pp. 593-602. </pages>
Reference-contexts: Related research has applied sequential analysis to specific Monte Carlo approximation problems such as estimating the number of points in a union of sets [17] and estimating the number of self-avoiding walks <ref> [20] </ref>. In other related work, Dyer et al describe a stopping rule based algorithm that provides an upper bound estimate on Z [8].
Reference: [21] <author> D. Siegmund, </author> <title> Sequential Analysis, 1985, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Theorem 7.1 states the result of the sequential probability ratio test. We prove the result for completeness, although similar proofs exist <ref> [21] </ref>. 16 Theorem 7.1 If T is the stopping time of any test of H Z against H Z 0 with error prob- abilities ff and fi, and E Z [T ]; E Z 0 [T ] &lt; 1, then E Z [T ] ! Z ff ln ff fi E
Reference: [22] <author> A. Wald, </author> <title> Sequential Analysis, 1947, </title> <publisher> Wiley, </publisher> <address> New York. </address> <month> 22 </month>
Reference-contexts: To avoid the problem encountered with the Generalized Zero-One Estimator Theorem, we use the outcomes of previous experiments to decide when to stop iterating. This approach is known as sequential analysis and originated with the work of Wald on statistical decision theory <ref> [22] </ref>. Related research has applied sequential analysis to specific Monte Carlo approximation problems such as estimating the number of points in a union of sets [17] and estimating the number of self-avoiding walks [20]. <p> follows that this is at most ffi=2. 13 The proof that Pr [N Z &gt; 1 =( Z (1 *))] ffi=2 is similar. 2 Proof of Part (2): The random variable N Z is the stopping time such that 1 S N Z &lt; 1 + 1: Using Wald's Equation <ref> [22] </ref> and E [N Z ] &lt; 1, it follows that E [S N Z ] = E [N Z ] Z and thus, 1 = Z E [N Z ] &lt; ( 1 + 1)= Z : Similar to the proof of the first part of the Stopping Rule Theorem,
References-found: 22

