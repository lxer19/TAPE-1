URL: http://www.cs.wisc.edu/~richards/thesis.ps
Refering-URL: http://www.cs.wisc.edu/~richards/richards.html
Root-URL: 
Title: Memory Systems for Parallel Programming  
Author: by Bradley Eric Richards 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1996  
Address: Wisconsin Madison  
Affiliation: University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A Unified Formalization of Four Shared-Memory Models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <year> 1993. </year>
Reference-contexts: An overview of the C** language is given in Section 3.4. Several LCM implementations are described (Section 3.5), and all are formally verified (Section 3.6). Section 3.7 presents performance results and analysis. 3.2 Related Work Relaxed consistency models <ref> [1, 26, 30] </ref> take advantage of the fact that global memory need not always appear consistent. Performance gains can be had by allowing incoherence to develop, but ensuring memory coherence at user-specified synchronization points. <p> But, the additional performance comes at the expense of 33 the memory required for the remote clean and accumulator copies. These memory overheads are detailed in Section 3.7. processors modify a pair of consecutive memory locations (A <ref> [1] </ref> and A [2]). After modifying A [1], processor P2 flushes the modified block back to the home, forcing the subsequent reference to A [2] to retrieve the block. It is flushed again after writing A [2]. <p> But, the additional performance comes at the expense of 33 the memory required for the remote clean and accumulator copies. These memory overheads are detailed in Section 3.7. processors modify a pair of consecutive memory locations (A <ref> [1] </ref> and A [2]). After modifying A [1], processor P2 flushes the modified block back to the home, forcing the subsequent reference to A [2] to retrieve the block. It is flushed again after writing A [2]. On processor P1, with the LCM-MCC policy, the flush is an entirely local process. When A [1] is flushed, a local <p> After modifying A <ref> [1] </ref>, processor P2 flushes the modified block back to the home, forcing the subsequent reference to A [2] to retrieve the block. It is flushed again after writing A [2]. On processor P1, with the LCM-MCC policy, the flush is an entirely local process. When A [1] is flushed, a local accumulator copy is created and initialized with the block containing A [1], and the local clean copy is used to refresh the cached data. The second flush, after modifying A [2], is also a local process. <p> It is flushed again after writing A [2]. On processor P1, with the LCM-MCC policy, the flush is an entirely local process. When A <ref> [1] </ref> is flushed, a local accumulator copy is created and initialized with the block containing A [1], and the local clean copy is used to refresh the cached data. The second flush, after modifying A [2], is also a local process.
Reference: [2] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk L. Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Kenneth Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proc. of the 22th Annual Int'l Symp. on Computer Architecture (ISCA'95), </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: There can also be large overheads associated with transferring page-sized regions of memory. At the other extreme, Cache-Coherent Nonuniform Memory Access (CCNUMA) systems <ref> [2, 20, 43, 55] </ref> implement access control at the granularity of a cache block, reducing the contention over each block of memory. Both access-control mechanisms and the coherence protocols that they invoke can be implemented in hardware or software. <p> But, the additional performance comes at the expense of 33 the memory required for the remote clean and accumulator copies. These memory overheads are detailed in Section 3.7. processors modify a pair of consecutive memory locations (A [1] and A <ref> [2] </ref>). After modifying A [1], processor P2 flushes the modified block back to the home, forcing the subsequent reference to A [2] to retrieve the block. It is flushed again after writing A [2]. On processor P1, with the LCM-MCC policy, the flush is an entirely local process. <p> These memory overheads are detailed in Section 3.7. processors modify a pair of consecutive memory locations (A [1] and A <ref> [2] </ref>). After modifying A [1], processor P2 flushes the modified block back to the home, forcing the subsequent reference to A [2] to retrieve the block. It is flushed again after writing A [2]. On processor P1, with the LCM-MCC policy, the flush is an entirely local process. <p> are detailed in Section 3.7. processors modify a pair of consecutive memory locations (A [1] and A <ref> [2] </ref>). After modifying A [1], processor P2 flushes the modified block back to the home, forcing the subsequent reference to A [2] to retrieve the block. It is flushed again after writing A [2]. On processor P1, with the LCM-MCC policy, the flush is an entirely local process. When A [1] is flushed, a local accumulator copy is created and initialized with the block containing A [1], and the local clean copy is used to refresh the cached data. <p> When A [1] is flushed, a local accumulator copy is created and initialized with the block containing A [1], and the local clean copy is used to refresh the cached data. The second flush, after modifying A <ref> [2] </ref>, is also a local process. Data is only transferred home at the start of the merge phase in LCM-MCC. 34 3.5.3 LCM-Update Iterative applications like stencil often access the same locations across iterations.
Reference: [3] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proc. of the 15th Annual Int'l Symp. on Computer Architecture (ISCA'88), </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: Shared-memory DSM machines require a coherence protocol to manage the replication of data and to ensure that a parallel program sees a consistent view of memory <ref> [3, 19, 32, 42, 64] </ref>. In general, coherence protocols allow at most a single 2 processor to modify a shared location, either invalidating outstanding copies or updating copies with the new value. <p> Actions may send messages to other processors, await their replies, 11 update protocol-specific information, and change access permissions. The exact states, transitions, and actions depend on the coherence algorithm. Many coherence schemes have been proposed <ref> [3, 11, 19, 32, 42] </ref>, but none works well for all applications and sharing patterns. Conceptually, a simple invalidation protocol like Stache requires only the three block states shown in Figure 2.1.
Reference: [4] <author> Randy Allen, Donn Baumgartner, Ken Kennedy, and Allan Porterfield. </author> <title> PTOOL: A Semi-Automatic Parallel Programming Assistant. </title> <type> Technical Report TR86-31, </type> <institution> Rice University, Department of Computer Science, </institution> <month> January </month> <year> 1986. </year>
Reference-contexts: Races exist between these pairs of blocks since the program imposes no orderings between them. There is no race between b6 and b10, since they are ordered by synchronization in the program (the join at the end 69 Three basic approaches have been used to detect races. Static techniques <ref> [4, 9, 16, 27, 62] </ref> examine the text of a program and use static analysis to approximate the shared-memory locations accessed by each code block. This information, combined with the concurrency information in the POEG, detects apparent races by finding pairs of unordered blocks making conflicting accesses to common locations.
Reference: [5] <author> Todd R. Allen and David A. Padua. </author> <title> Debugging Fortran on a Shared Memory Machine. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 721-727, </pages> <address> University Park PA, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users [33, 46]. Post-mortem <ref> [5, 23, 49] </ref> and on-the-fly methods [25, 33, 46, 52, 59, 61] improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods.
Reference: [6] <author> Cristiana Amza, Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, Weimin Yu, and Willy Zwanepoel. </author> <title> Tread-Marks: Shared Memory Computing on Networks of Workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: For example, LCM requires the predictable behavior of the incoherent memory for the correct implementation of C** semantics. RSM shares with Munin [13] and TreadMarks <ref> [6, 35] </ref> the ability to adapt standard distributed shared memory policies to better suit an application. Both Munin and TreadMarks provide a set of fixed coherence mechanisms, each tailored for a specific sharing pattern. The user or compiler associates a coherence mechanism with each object. <p> The fixed protocol policy in hardware-implemented protocols results in an all 50 or-nothing decision between update and invalidation protocols, neither of which may be a perfect match for all program phases. Page-based DSM systems like Munin [13] and TreadMarks <ref> [6] </ref> are an improvement, as they allow protocol policies to be selected for individual program objects, but their large coherence granularity hinders performance for programs with fine-grained sharing.
Reference: [7] <author> Andrew W. Appel, John R. Ellis, and Kai Li. </author> <title> Real-time Concurrent Collection on Stock Multiprocessors. </title> <booktitle> In Proceedings of the SIGPLAN '88 Confer 115 ence on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 11-20, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: This collaborative effort is similar to sequential language implementations, such as Lisp and ML, that exploit memory systems to perform heap bounds checking and concurrent garbage collection <ref> [7] </ref>. In both systems, operations proceed under the assumption they will not fail. A processor's memory system catches unexpected 4 situations, which are handled out of the program's normal line of code. <p> Being hardware based, the copying and reconciliation policies were necessarily fixed. The division of labor between the C** compiler and LCM is reminiscent of the techniques for stack and heap bounds checking [8] and concurrent garbage collection <ref> [7] </ref> in Lisp. With normal stop-and-copy garbage collectors, all pointers into the old heap can be replaced with pointers into the newly compacted heap. Concurrent collectors allow the computation to proceed, and attempt to copy 21 objects from the old heap to the new without interruption.
Reference: [8] <author> Andrew W. Appel and Kai Li. </author> <title> Virtual Memory Primitives for User Programs. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 96-107, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: But the Myrias scheme was implemented in hardware, and copied data at the page granularity. Being hardware based, the copying and reconciliation policies were necessarily fixed. The division of labor between the C** compiler and LCM is reminiscent of the techniques for stack and heap bounds checking <ref> [8] </ref> and concurrent garbage collection [7] in Lisp. With normal stop-and-copy garbage collectors, all pointers into the old heap can be replaced with pointers into the newly compacted heap.
Reference: [9] <author> William F. Appelbe and Charles E. McDowell. </author> <title> Anomaly Reporting: A Tool for Debugging and Developing Parallel Numerical Algorithms. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing Systems, </booktitle> <pages> pages 386-391, </pages> <year> 1985. </year>
Reference-contexts: Races exist between these pairs of blocks since the program imposes no orderings between them. There is no race between b6 and b10, since they are ordered by synchronization in the program (the join at the end 69 Three basic approaches have been used to detect races. Static techniques <ref> [4, 9, 16, 27, 62] </ref> examine the text of a program and use static analysis to approximate the shared-memory locations accessed by each code block. This information, combined with the concurrency information in the POEG, detects apparent races by finding pairs of unordered blocks making conflicting accesses to common locations.
Reference: [10] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. Orca: </author> <title> A Language For Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Reading or writing an invalid location 9 or writing a valid but read-only location must cause an access fault and invoke the coherence protocol. The second mechanism, communication, enables a system to transfer control information and data among processors. Access control can be performed at various granularities. Page-based systems <ref> [10, 12, 14, 17, 34, 44] </ref> typically use operating-system page-protection schemes to implement access control, and therefore enforce coherence at the page granularity. This approach can be used to implement shared memory on loosely-coupled systems where no hardware support for shared memory exists.
Reference: [11] <author> Sandra Johnson Baylor, Kevin P. McAuliffe, and Bharat Deep Rathi. </author> <title> An Evaluation of Cache Coherence Protocols for MIN-Based Multiprocessors. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 230-241, </pages> <address> Tokyo, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Actions may send messages to other processors, await their replies, 11 update protocol-specific information, and change access permissions. The exact states, transitions, and actions depend on the coherence algorithm. Many coherence schemes have been proposed <ref> [3, 11, 19, 32, 42] </ref>, but none works well for all applications and sharing patterns. Conceptually, a simple invalidation protocol like Stache requires only the three block states shown in Figure 2.1.
Reference: [12] <author> Monica Beltrametti, Kenneth Bobey, and John R. Zorbas. </author> <title> The Control Mechanism for the Myrias Parallel Computer System. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Reading or writing an invalid location 9 or writing a valid but read-only location must cause an access fault and invoke the coherence protocol. The second mechanism, communication, enables a system to transfer control information and data among processors. Access control can be performed at various granularities. Page-based systems <ref> [10, 12, 14, 17, 34, 44] </ref> typically use operating-system page-protection schemes to implement access control, and therefore enforce coherence at the page granularity. This approach can be used to implement shared memory on loosely-coupled systems where no hardware support for shared memory exists. <p> VDOM handles coherence at the language-object level, as opposed to RSM's finer-grained cache block level. It also uses a single, inflexible coherence mechanism based on object version numbers. Like LCM, the Myrias machine <ref> [12] </ref> copied data on-the-fly to prevent interactions between parallel tasks. But the Myrias scheme was implemented in hardware, and copied data at the page granularity. Being hardware based, the copying and reconciliation policies were necessarily fixed.
Reference: [13] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 168-176, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: But, applications have very different patterns of communication, and no single, general-purpose protocol has proven well suited to all programs. This has prompted interest in systems that enable users to select from a set of coherence protocols <ref> [13, 18] </ref> and, more recently, in systems in which a protocol is implemented in flexible software instead of being forever encoded in hardware [36, 54]. <p> There is a similar lack of coherence in RSM between reconciliations, but the incoherence conforms to a semantics and can therefore 20 be reasoned about. For example, LCM requires the predictable behavior of the incoherent memory for the correct implementation of C** semantics. RSM shares with Munin <ref> [13] </ref> and TreadMarks [6, 35] the ability to adapt standard distributed shared memory policies to better suit an application. Both Munin and TreadMarks provide a set of fixed coherence mechanisms, each tailored for a specific sharing pattern. The user or compiler associates a coherence mechanism with each object. <p> The fixed protocol policy in hardware-implemented protocols results in an all 50 or-nothing decision between update and invalidation protocols, neither of which may be a perfect match for all program phases. Page-based DSM systems like Munin <ref> [13] </ref> and TreadMarks [6] are an improvement, as they allow protocol policies to be selected for individual program objects, but their large coherence granularity hinders performance for programs with fine-grained sharing.
Reference: [14] <author> B. N. Bershad and M. J. Zekauskas. Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Reading or writing an invalid location 9 or writing a valid but read-only location must cause an access fault and invoke the coherence protocol. The second mechanism, communication, enables a system to transfer control information and data among processors. Access control can be performed at various granularities. Page-based systems <ref> [10, 12, 14, 17, 34, 44] </ref> typically use operating-system page-protection schemes to implement access control, and therefore enforce coherence at the page granularity. This approach can be used to implement shared memory on loosely-coupled systems where no hardware support for shared memory exists.
Reference: [15] <author> James Boyle, Ralph Butler, Terrence Disz, Barnett Glickfieldand Ewing Lusk, Ross Overbeek, James Patterson, and Rick Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston Inc., </publisher> <year> 1987. </year>
Reference-contexts: This is independent of the means used to express the parallelism (i.e. fork calls, DOALL loops, Parmacs CREATE <ref> [15] </ref>). For the race-detection protocols, accesses by a pair of processors are concurrent if, during execution, they are not separated by a synchronization event. Synchronization can involve all processors (a barrier), or a subset (locks or joins).
Reference: [16] <author> David Callahan and Jaspal Subhlok. </author> <title> Static Analysis of Low-Level Synchronization. </title> <booktitle> Proceedings of the ACM SIGPLAN/SIGOPS Workshop on Parallel 116 and Distributed Debugging, published in ACM SIGPLAN Notices, </booktitle> <volume> 24(1) </volume> <pages> 100-111, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Races exist between these pairs of blocks since the program imposes no orderings between them. There is no race between b6 and b10, since they are ordered by synchronization in the program (the join at the end 69 Three basic approaches have been used to detect races. Static techniques <ref> [4, 9, 16, 27, 62] </ref> examine the text of a program and use static analysis to approximate the shared-memory locations accessed by each code block. This information, combined with the concurrency information in the POEG, detects apparent races by finding pairs of unordered blocks making conflicting accesses to common locations.
Reference: [17] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc. of the 13th ACM Symp. on Operating Systems Principles (SOSP'91), </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Reading or writing an invalid location 9 or writing a valid but read-only location must cause an access fault and invoke the coherence protocol. The second mechanism, communication, enables a system to transfer control information and data among processors. Access control can be performed at various granularities. Page-based systems <ref> [10, 12, 14, 17, 34, 44] </ref> typically use operating-system page-protection schemes to implement access control, and therefore enforce coherence at the page granularity. This approach can be used to implement shared memory on loosely-coupled systems where no hardware support for shared memory exists.
Reference: [18] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: But, applications have very different patterns of communication, and no single, general-purpose protocol has proven well suited to all programs. This has prompted interest in systems that enable users to select from a set of coherence protocols <ref> [13, 18] </ref> and, more recently, in systems in which a protocol is implemented in flexible software instead of being forever encoded in hardware [36, 54].
Reference: [19] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Shared-memory DSM machines require a coherence protocol to manage the replication of data and to ensure that a parallel program sees a consistent view of memory <ref> [3, 19, 32, 42, 64] </ref>. In general, coherence protocols allow at most a single 2 processor to modify a shared location, either invalidating outstanding copies or updating copies with the new value. <p> Actions may send messages to other processors, await their replies, 11 update protocol-specific information, and change access permissions. The exact states, transitions, and actions depend on the coherence algorithm. Many coherence schemes have been proposed <ref> [3, 11, 19, 32, 42] </ref>, but none works well for all applications and sharing patterns. Conceptually, a simple invalidation protocol like Stache requires only the three block states shown in Figure 2.1. <p> Since it fits within this model, it provides a natural default policy for a RSM system. Requests in these shared-memory systems return a copy of a block, subject to the guarantee that only one processor holds a writable copy at a time. In many systems <ref> [19, 42] </ref>, a centralized directory controller records which processors hold copies of a location and invalidates outstanding copies upon request. Reconciliation policies in these systems are also simple. Read-only copies are identical and so can be combined by a null reconciliation function.
Reference: [20] <author> Rohit Chandra, Kourosh Gharachorloo, Vijayaraghavan Soundararajan, and Anoop Gupta. </author> <title> Performance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols. </title> <type> Technical Report CSL-TR-93-597, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: There can also be large overheads associated with transferring page-sized regions of memory. At the other extreme, Cache-Coherent Nonuniform Memory Access (CCNUMA) systems <ref> [2, 20, 43, 55] </ref> implement access control at the granularity of a cache block, reducing the contention over each block of memory. Both access-control mechanisms and the coherence protocols that they invoke can be implemented in hardware or software.
Reference: [21] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared-Memory Programs? In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 61-75, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: With broadcast updates, the overall computation speeds up by a factor of 1.6. The program phase solving the system of equations | the only portion with LCM support | improves by a factor of 2.8. 4.2.2 LCP LCP, written by Satish Chandra and Steve Dirkse <ref> [21] </ref>, solves the linear complementarity problem in parallel. Given a matrix M and a vector q, LCP finds a solution vector x such that M x + q 0. Like Chem, it divides the global solution vector among processors, and new values are a function of the entire vector.
Reference: [22] <author> Satish Chandra, Brad Richards, and James R. Larus. Teapot: </author> <title> Language Support for Writing Memory Coherence Protocols. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: The processor may have returned its copy and subsequently requested a readable copy. If messages can pass each other in the network, the read request must be retained and processed after the first copy has been returned. 2.2.1 Writing Protocols with Teapot Teapot <ref> [22] </ref> is an environment for designing, verifying, and implementing cache coherence protocols. It simplifies the task in two significant ways. <p> As is shown in Section 5.6.3, these spurious races can be effectively removed in a post-processing phase. 5.5 Verification The formal verification tool associated with Teapot <ref> [22, 24] </ref> was used to ensure both that the race-detection protocols maintained consistent data, and that they successfully caught data races. The verification process proceeded in two steps. First, the basic protocol was designed and tested without any race-detection functionality.
Reference: [23] <author> Jong-Deok Choi, Barton P. Miller, and Robert Netzer. </author> <title> Techniques for Debugging Parallel Programs with Flowback Analysis. </title> <type> Technical Report 786, </type> <institution> University of Wisconsin, Madison, Computer Sciences Department, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users [33, 46]. Post-mortem <ref> [5, 23, 49] </ref> and on-the-fly methods [25, 33, 46, 52, 59, 61] improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods.
Reference: [24] <author> David L. Dill, Andreas J. Drexler, Alan J. Hu, and C. Han Yang. </author> <title> Protocol Verification as a Hardware Design Aid. </title> <booktitle> In 1992 IEEE International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <pages> pages 522-525, </pages> <year> 1992. </year> <month> 117 </month>
Reference-contexts: Second, it can help ensure that protocols work correctly by generating input for a formal verification tool called Mur <ref> [24] </ref>. Experience with Teapot has shown it to reduce protocol implementation and debugging time by an order of magnitude. Language Support The Teapot protocol specification language provides a construct, Suspend, that allows handlers to be written as though they can wait on asynchronous events. <p> Teapot eases the debugging burden tremendously by exhaustively testing protocols for errors. As shown in Figure 2.5, the Teapot compiler can turn protocol specifications into executable C code, or generate input to Mur <ref> [24] </ref>, a formal verification 17 tool. The verification process exhaustively explores a state space that is the cross product of all protocol, network, and data-structure states on each simulated processor. <p> As is shown in Section 5.6.3, these spurious races can be effectively removed in a post-processing phase. 5.5 Verification The formal verification tool associated with Teapot <ref> [22, 24] </ref> was used to ensure both that the race-detection protocols maintained consistent data, and that they successfully caught data races. The verification process proceeded in two steps. First, the basic protocol was designed and tested without any race-detection functionality.
Reference: [25] <author> Anne Dinning and Edith Schonberg. </author> <title> An Empirical Comparison of Monitoring Algorithms for Access Anomaly Detection. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 1-10, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: On-the-fly checking can be used to detect races during execution, but the monitored programs have been shown to be up to a factor of six slower than the unmonitored <ref> [25] </ref>. Since cache coherence protocols manage accesses to shared data, they are aware of an application's memory references. This access information can be used, either directly or with augmentation, to detect actual data races during execution. <p> Overhead in execution time for these protocols are shown to range from zero to less than a factor of three over a set of benchmarks | a significant improvement over slow-downs of three to six for Dinning and Schonberg <ref> [25] </ref>, and five to 30 for Perkovic and Keleher [53]. Hood, Kennedy, and Mellor-Crummey [33] have lower overhead than the protocol-based techniques, at approximately 40%, but require compiler involvement. <p> Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users [33, 46]. Post-mortem [5, 23, 49] and on-the-fly methods <ref> [25, 33, 46, 52, 59, 61] </ref> improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods. <p> As described, the on-the-fly approach cannot handle programs with pairwise synchronization, since the orderings it introduces are not encoded in the block labels. (The orderings imposed by pairwise synchronizations cannot be known at compile time, and so cannot be included.) Dinning and Schonberg <ref> [25] </ref> associate a coordination list with each block that records information about immediate ancestors introduced by pairwise synchronization. A block b1 is concurrent with b2 if their labels reveal them to be unordered and none of the labels in b1 's coordination list are ordered with b2. <p> However, the lack of source-level information also means that system-level schemes cannot take advantage of optimizations requiring knowledge of the source, such as removing accesses that are statically known to be ordered from race-detection consideration. 72 5.3.1 Traditional Approaches Dinning and Schonberg <ref> [25] </ref> have implemented a general scheme for detecting apparent data races. They support both fork/join and pairwise synchronization, and obtain concurrency information from the POEG. The race-detection protocols only support barrier synchronization. <p> In comparison, Perkovic and Keleher [53], the only other system-level race-detection implementation, report slow-downs ranging from five to 30. The on-the-fly method of Dinning and Schonberg <ref> [25] </ref> has slow-downs ranging from three to six, though they handle a larger class of synchronization.
Reference: [26] <author> Sandhya Dwarkadas, Pete Keleher, Alan L. Cox, and Willy Z waenepoel. </author> <title> Evaluation of Release Consistent Software Distributed Shared Memory on Emerging Network Technology. </title> <booktitle> In Proc. of the 20th Annual Int'l Symp. on Computer Architecture (ISCA'93), </booktitle> <pages> pages 144-155, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: An overview of the C** language is given in Section 3.4. Several LCM implementations are described (Section 3.5), and all are formally verified (Section 3.6). Section 3.7 presents performance results and analysis. 3.2 Related Work Relaxed consistency models <ref> [1, 26, 30] </ref> take advantage of the fact that global memory need not always appear consistent. Performance gains can be had by allowing incoherence to develop, but ensuring memory coherence at user-specified synchronization points.
Reference: [27] <author> Perry A. Emrath and David A. Padua. </author> <title> Automatic Detection of Nondeter-minacy in Parallel Programs. </title> <booktitle> Proceedings of the ACM SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, published in ACM SIG-PLAN Notices, </booktitle> <volume> 24(1) </volume> <pages> 89-99, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Races exist between these pairs of blocks since the program imposes no orderings between them. There is no race between b6 and b10, since they are ordered by synchronization in the program (the join at the end 69 Three basic approaches have been used to detect races. Static techniques <ref> [4, 9, 16, 27, 62] </ref> examine the text of a program and use static analysis to approximate the shared-memory locations accessed by each code block. This information, combined with the concurrency information in the POEG, detects apparent races by finding pairs of unordered blocks making conflicting accesses to common locations.
Reference: [28] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 380-389, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Experiments have shown that the performance penalties for implementing coherence actions in software, instead of hardware, are relatively small (especially if common operations are accelerated by hardware [36, 55]), and that tailoring protocols to the needs of applications can result in tremendous performance increases <ref> [28] </ref>. Parallel computers of the future will likely be DSM systems with software-implemented coherence protocols. This hardware provides opportunities for a large variety of more complex and application-specific protocols and allows for protocols that do not just ensure consistent memory, but also provide new functionality and semantics. <p> Tempest [54] allows programmers to use both update and invalidation protocols as required, and significant performance improvements have been obtained by tailoring update protocols to specific applications <ref> [28] </ref>. LCM provides both update and invalidation protocol policies at a fine coherence granularity, and does so without requiring new, application-specific update protocols for each program.
Reference: [29] <author> Michael J. Feeley and Henry M. Levy. </author> <title> Distributed Shared Memory with Versioned Objects. </title> <booktitle> In OOPSLA '92: Seventh Annual Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 247-262, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The user or compiler associates a coherence mechanism with each object. RSM allows dynamic schemes that can be applied at the cache block granularity as opposed to the language-object level. In VDOM <ref> [29] </ref>, memory objects are immutable, and an attempt to modify an object produces a new version of the object. It is related to RSM in that both systems allow multiple copies of memory items to develop.
Reference: [30] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Philip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory. </title> <booktitle> In Proc. of the 17th Annual Int'l Symp. on Computer Architecture (ISCA'90), </booktitle> <pages> pages 15-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: An overview of the C** language is given in Section 3.4. Several LCM implementations are described (Section 3.5), and all are formally verified (Section 3.6). Section 3.7 presents performance results and analysis. 3.2 Related Work Relaxed consistency models <ref> [1, 26, 30] </ref> take advantage of the fact that global memory need not always appear consistent. Performance gains can be had by allowing incoherence to develop, but ensuring memory coherence at user-specified synchronization points.
Reference: [31] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The following sections describe in more detail the contributions this thesis makes towards efficiently implementing a new parallel programming language and detecting data races through the use of custom coherence protocols. 1.1 Loosely Coherent Memory Recently, there has been considerable interest in higher-level parallel languages, such as HPF <ref> [31] </ref>, in which a compiler handles the details of mapping from an abstract parallel model to a particular machine. The success of this, and other, parallel languages depends on efficient implementations.
Reference: [32] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <month> November </month> <year> 1993. </year> <booktitle> Earlier version appeared in Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V). </booktitle> <pages> 118 </pages>
Reference-contexts: Shared-memory DSM machines require a coherence protocol to manage the replication of data and to ensure that a parallel program sees a consistent view of memory <ref> [3, 19, 32, 42, 64] </ref>. In general, coherence protocols allow at most a single 2 processor to modify a shared location, either invalidating outstanding copies or updating copies with the new value. <p> Actions may send messages to other processors, await their replies, 11 update protocol-specific information, and change access permissions. The exact states, transitions, and actions depend on the coherence algorithm. Many coherence schemes have been proposed <ref> [3, 11, 19, 32, 42] </ref>, but none works well for all applications and sharing patterns. Conceptually, a simple invalidation protocol like Stache requires only the three block states shown in Figure 2.1.
Reference: [33] <author> Robert Hood, Ken Kennedy, and John Mellor-Crummey. </author> <title> Parallel Program Debugging with On-the-fly Anomaly Detection. </title> <type> Technical Report TR90-111, </type> <institution> Rice University, Department of Computer Science, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Hood, Kennedy, and Mellor-Crummey <ref> [33] </ref> have lower overhead than the protocol-based techniques, at approximately 40%, but require compiler involvement. Efficient detection of data races is possible on DSM systems because a mech 65 anism is already in place to invoke the coherence protocol in response to shared-memory accesses. <p> This information, combined with the concurrency information in the POEG, detects apparent races by finding pairs of unordered blocks making conflicting accesses to common locations. Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users <ref> [33, 46] </ref>. Post-mortem [5, 23, 49] and on-the-fly methods [25, 33, 46, 52, 59, 61] improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods. <p> Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users [33, 46]. Post-mortem [5, 23, 49] and on-the-fly methods <ref> [25, 33, 46, 52, 59, 61] </ref> improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods. <p> Mellor-Crummey [46] describes a method for encoding the POEG, offset-span labeling, that has improved space and time bounds for programs that do not use pairwise synchronization, but gives no performance results. Hood, Kennedy, and Mellor-Crummey <ref> [33] </ref>, present a technique for detecting apparent data races in Fortran programs that use barriers and structured synchronization based on ordered sequences. They keep only one entry in the access histories, and use static analysis to reduce the number of monitored shared variables. <p> The protocol can be used to monitor memory accesses by updating an access history for referenced locations on each invocation. Like others <ref> [33, 46, 47] </ref>, I choose to keep records of only the most recent read and write to a variable for reasons of efficiency. <p> Since 92 this ignores data dependences that could potentially order the events, the detected races are apparent and not necessarily feasible. As with all approaches that bound access histories <ref> [33, 46, 47] </ref>, the protocols detect a subset of the apparent races present in an execution. Races can be missed if there are multiple races involving the same location, but at least one race involving a location is guaranteed to be caught and can be used to debug parallel programs. <p> In comparison, Perkovic and Keleher [53], the only other system-level race-detection implementation, report slow-downs ranging from five to 30. The on-the-fly method of Dinning and Schonberg [25] has slow-downs ranging from three to six, though they handle a larger class of synchronization. Hood, Kennedy, and Mellor-Crummey <ref> [33] </ref> have lower overheads than the protocol-based techniques, at approximately 40%, but require compiler involvement. 109 Chapter 6 Conclusions Distributed Shared-Memory (DSM ) computers, which partition physical memory among a collection of workstation-like computing nodes, are emerging as the way to implement parallel computers, as they promise scalability and high performance.
Reference: [34] <author> P. Keleher, S. Dwarkadas, A. L. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proc. of the Winter 1994 USENIX Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Reading or writing an invalid location 9 or writing a valid but read-only location must cause an access fault and invoke the coherence protocol. The second mechanism, communication, enables a system to transfer control information and data among processors. Access control can be performed at various granularities. Page-based systems <ref> [10, 12, 14, 17, 34, 44] </ref> typically use operating-system page-protection schemes to implement access control, and therefore enforce coherence at the page granularity. This approach can be used to implement shared memory on loosely-coupled systems where no hardware support for shared memory exists.
Reference: [35] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. </author> <title> Tread-Marks: Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <type> Technical Report 93-214, </type> <institution> Department of Computer Science, Rice University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: For example, LCM requires the predictable behavior of the incoherent memory for the correct implementation of C** semantics. RSM shares with Munin [13] and TreadMarks <ref> [6, 35] </ref> the ability to adapt standard distributed shared memory policies to better suit an application. Both Munin and TreadMarks provide a set of fixed coherence mechanisms, each tailored for a specific sharing pattern. The user or compiler associates a coherence mechanism with each object.
Reference: [36] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proc. of the 21th Annual Int'l Symp. on Computer Architecture (ISCA'94), </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This has prompted interest in systems that enable users to select from a set of coherence protocols [13, 18] and, more recently, in systems in which a protocol is implemented in flexible software instead of being forever encoded in hardware <ref> [36, 54] </ref>. Experiments have shown that the performance penalties for implementing coherence actions in software, instead of hardware, are relatively small (especially if common operations are accelerated by hardware [36, 55]), and that tailoring protocols to the needs of applications can result in tremendous performance increases [28]. <p> Experiments have shown that the performance penalties for implementing coherence actions in software, instead of hardware, are relatively small (especially if common operations are accelerated by hardware <ref> [36, 55] </ref>), and that tailoring protocols to the needs of applications can result in tremendous performance increases [28]. Parallel computers of the future will likely be DSM systems with software-implemented coherence protocols. <p> These lines have recently begun to blur. Fine-grained access control has been successfully implemented in software [58], and a number of CCNUMA machines have moved their coherence protocols to software as well <ref> [36, 45, 54] </ref>.
Reference: [37] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7), </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: If variables are associated with locks, access histories could be extended to contain information about locks held by a processor during an access. Races are caused by processors referencing locations without first acquiring the necessary lock. Also, the more general vector time-stamp <ref> [37] </ref> technique could be used to detect orderings imposed by pairwise synchronization, but at the cost of an increase in the amount of data transmitted with each block. 114
Reference: [38] <author> J. R. Larus, B. Richards, and G. Viswanathan. </author> <title> LCM: Memory System Support for Parallel Language Implementation. </title> <booktitle> In Proc. of the Sixth Int'l Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <month> October </month> <year> 1994. </year>
Reference: [39] <author> James R. Larus. </author> <title> C**: a Large-Grain, Object-Oriented, Data-Parallel Programming Language. </title> <editor> In Utpal Banerjee, David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages And Compilers for Parallel Computing (5th International Workshop), </booktitle> <pages> pages 326-341. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: A processor's memory system catches unexpected 4 situations, which are handled out of the program's normal line of code. As a proof of concept, I show how LCM can be used to efficiently implement a large-grain data-parallel language called C** <ref> [39] </ref>, whose semantics are otherwise difficult to implement. In C**, a parallel function is applied simultaneously and instantaneously to each element in an aggregate. <p> practice verifying on small configurations has been sufficient to eliminate (detectable) bugs from protocols running on real systems. 18 Chapter 3 Loosely Coherent Memory and C** 3.1 Introduction This chapter describes Loosely Coherent Memory (LCM ), a custom protocol that implements the semantics of a new parallel programming language C** <ref> [39] </ref> up to three times faster than other approaches. Semantically, parallel tasks in C** execute simultaneously and instantaneously , so conflicting data accesses are impossible. Programmers need not consider potential interactions between tasks since no interaction is allowed.
Reference: [40] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory System Support for Parallel Language Implementation. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 208-218, </pages> <month> October </month> <year> 1994. </year>
Reference: [41] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> Parallel Programming in C**: A Large-Grain Data-Parallel Programming Language. </title> <editor> In Gregory V. Wilson and Paul Lu, editors, </editor> <title> Parallel Programming Using C++, </title> <booktitle> chapter 8, </booktitle> <pages> pages 297-342. </pages> <address> MITP, </address> <year> 1996. </year> <month> 119 </month>
Reference-contexts: f LOCK c; insert (p, c); UNLOCK m; g With LCM: FOR p in (my polygons ) f find overlapping cells FOR c in (cells ) f mark_modification (c); insert (p, c); g reconcile_copies (); 4.3.2 Overlay Overlay, written as a programming exercise for a book on parallel programming languages <ref> [41] </ref>, computes the geometric intersection of a pair of rectangular polygon "maps". Each map covers the same geographical area, and is composed of a set of non-overlapping polygons. Figure 4.7 shows two such maps, and their intersection.
Reference: [42] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proc. of the 17th Annual Int'l Symp. on Computer Architecture (ISCA'90), </booktitle> <pages> pages 148-159, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Shared-memory DSM machines require a coherence protocol to manage the replication of data and to ensure that a parallel program sees a consistent view of memory <ref> [3, 19, 32, 42, 64] </ref>. In general, coherence protocols allow at most a single 2 processor to modify a shared location, either invalidating outstanding copies or updating copies with the new value. <p> Actions may send messages to other processors, await their replies, 11 update protocol-specific information, and change access permissions. The exact states, transitions, and actions depend on the coherence algorithm. Many coherence schemes have been proposed <ref> [3, 11, 19, 32, 42] </ref>, but none works well for all applications and sharing patterns. Conceptually, a simple invalidation protocol like Stache requires only the three block states shown in Figure 2.1. <p> Since it fits within this model, it provides a natural default policy for a RSM system. Requests in these shared-memory systems return a copy of a block, subject to the guarantee that only one processor holds a writable copy at a time. In many systems <ref> [19, 42] </ref>, a centralized directory controller records which processors hold copies of a location and invalidates outstanding copies upon request. Reconciliation policies in these systems are also simple. Read-only copies are identical and so can be combined by a null reconciliation function.
Reference: [43] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: There can also be large overheads associated with transferring page-sized regions of memory. At the other extreme, Cache-Coherent Nonuniform Memory Access (CCNUMA) systems <ref> [2, 20, 43, 55] </ref> implement access control at the granularity of a cache block, reducing the contention over each block of memory. Both access-control mechanisms and the coherence protocols that they invoke can be implemented in hardware or software.
Reference: [44] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Reading or writing an invalid location 9 or writing a valid but read-only location must cause an access fault and invoke the coherence protocol. The second mechanism, communication, enables a system to transfer control information and data among processors. Access control can be performed at various granularities. Page-based systems <ref> [10, 12, 14, 17, 34, 44] </ref> typically use operating-system page-protection schemes to implement access control, and therefore enforce coherence at the page granularity. This approach can be used to implement shared memory on loosely-coupled systems where no hardware support for shared memory exists.
Reference: [45] <author> Tom Lovett and Russell Clapp. STiNG: </author> <title> A CC-NUMA Computer System for the Commercial Marketplace. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 308-317, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: These lines have recently begun to blur. Fine-grained access control has been successfully implemented in software [58], and a number of CCNUMA machines have moved their coherence protocols to software as well <ref> [36, 45, 54] </ref>.
Reference: [46] <author> John M. Mellor-Crummey. </author> <title> On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 24-33, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: This information, combined with the concurrency information in the POEG, detects apparent races by finding pairs of unordered blocks making conflicting accesses to common locations. Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users <ref> [33, 46] </ref>. Post-mortem [5, 23, 49] and on-the-fly methods [25, 33, 46, 52, 59, 61] improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods. <p> Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users [33, 46]. Post-mortem [5, 23, 49] and on-the-fly methods <ref> [25, 33, 46, 52, 59, 61] </ref> improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods. <p> The race-detection protocols only support barrier synchronization. Over a set of four benchmarks, Dinning and Schonberg report program slow-downs of from three to six using access histories limited to only one or two entries | roughly twice as slow as the protocol-based approach. Mellor-Crummey <ref> [46] </ref> describes a method for encoding the POEG, offset-span labeling, that has improved space and time bounds for programs that do not use pairwise synchronization, but gives no performance results. <p> The protocol can be used to monitor memory accesses by updating an access history for referenced locations on each invocation. Like others <ref> [33, 46, 47] </ref>, I choose to keep records of only the most recent read and write to a variable for reasons of efficiency. <p> Since 92 this ignores data dependences that could potentially order the events, the detected races are apparent and not necessarily feasible. As with all approaches that bound access histories <ref> [33, 46, 47] </ref>, the protocols detect a subset of the apparent races present in an execution. Races can be missed if there are multiple races involving the same location, but at least one race involving a location is guaranteed to be caught and can be used to debug parallel programs.
Reference: [47] <author> Sang Lyul Min and Jong-Deok Choi. </author> <title> An Efficient Cache-based Access Anomaly Detection Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 235-244, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: My protocols detect races as they occur, and have perfect knowledge of at least one of the conflicting references. The work in this thesis is most closely related to a hardware-based cache coherence protocol for CCNUMA machines that Min and Choi <ref> [47] </ref> designed but never implemented. Like the race-detection protocols, they limit access histories 2 They apparently have improved numbers in a version of the paper accepted to OSDI, but not yet available. 74 to a single entry and do not support pairwise synchronization. <p> The protocol can be used to monitor memory accesses by updating an access history for referenced locations on each invocation. Like others <ref> [33, 46, 47] </ref>, I choose to keep records of only the most recent read and write to a variable for reasons of efficiency. <p> Thus, we cannot make a block readable, for example, until each race-detection region on the block has been read. Doing otherwise risks missing races since accesses can be missed. 5.4.2 Detecting Concurrency As with other system-level race-detection schemes <ref> [47, 53] </ref>, coherence protocols are only aware of the processor-level concurrency present in an executing application. This is independent of the means used to express the parallelism (i.e. fork calls, DOALL loops, Parmacs CREATE [15]). <p> Since 92 this ignores data dependences that could potentially order the events, the detected races are apparent and not necessarily feasible. As with all approaches that bound access histories <ref> [33, 46, 47] </ref>, the protocols detect a subset of the apparent races present in an execution. Races can be missed if there are multiple races involving the same location, but at least one race involving a location is guaranteed to be caught and can be used to debug parallel programs.
Reference: [48] <author> Robert H. B. Netzer. </author> <title> Race Condition Detection for Debugging Shared-Memory Parallel Programs. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: The programmer must then decide which reported races are spurious and which are truly problematic. Off-line analysis of trace information collected during an execution can be used to detect both races that actually occurred, and those that could have occurred given the synchronization contained in the program <ref> [48] </ref>, but they require that large traces be generated and stored. On-the-fly checking can be used to detect races during execution, but the monitored programs have been shown to be up to a factor of six slower than the unmonitored [25]. <p> Since this lack of synchronization can lead to programs that behave unpredictably, it is important to be able to detect and report these conditions. The race detection literature has used a variety of terms to describe race conditions, but this thesis follows Netzer and Miller <ref> [48, 51] </ref>. 5.2.1 Types of Race Conditions Netzer and Miller [50, 51] recognized two fundamentally different types of races. <p> Two variations of each type of race exist: feasible and apparent. A feasible race is one that occurs in some realizable program execution. Unfortunately, as Netzer proves <ref> [48] </ref>, finding feasible races of either type is NP-hard for all types of synchronization. Apparent races occur when a race is detected in an execution permitted by the synchronization of the program (ignoring constraints caused by data dependences).
Reference: [49] <author> Robert H. B. Netzer and Barton P. Miller. </author> <title> Detecting Data Races in Parallel Program Executions. </title> <type> Technical Report TR90-894, </type> <institution> University of Wisconsin, Madison, Department of Computer Science, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users [33, 46]. Post-mortem <ref> [5, 23, 49] </ref> and on-the-fly methods [25, 33, 46, 52, 59, 61] improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods.
Reference: [50] <author> Robert H. B. Netzer and Barton P. Miller. </author> <title> Improving the Accuracy of Data Race Detection. </title> <booktitle> Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPOPP, published in ACM SIGPLAN NOTICES, </booktitle> <volume> 26(7) </volume> <pages> 133-144, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The race detection literature has used a variety of terms to describe race conditions, but this thesis follows Netzer and Miller [48, 51]. 5.2.1 Types of Race Conditions Netzer and Miller <ref> [50, 51] </ref> recognized two fundamentally different types of races. <p> required for complete traces of long-running programs, leading many to prefer the on-the-fly approach. 1 Netzer and Miller, however, describe a post-mortem analysis technique that can improve the accuracy of the detected races by ruling out some apparent races that are artifacts of earlier races or prohibited by program dependences. <ref> [50] </ref>. 70 5.3 Related Work Traditional on-the-fly approaches maintain a history of the blocks that have accessed each shared variable. Code is added to the application program so that at each reference to shared memory a block compares its label against all labels in the access history. <p> Our goal is to report only the covering race from each set, when it can be found without unreasonable overhead. Covering races bear a similarity to Netzer and Miller's first races <ref> [50] </ref>, as they capture the notion of an initial race involving a given memory location. The key difference is that covering races are reported each time a processor acquires a block. <p> The races detected depend upon the ordering of accesses to a given location, and can therefore change from run to run. This is not problematic, as the protocols are still guaranteed to find at least one of the set of races involving a location | including a first race <ref> [50] </ref> for each location. A larger concern is the selection of race-detection granularity. As Section 5.4.1 shows, race-detection accuracy is compromised when the granularity at which races are detected is larger than that at which data is shared in an application.
Reference: [51] <author> Robert H. B. Netzer and Barton P. Miller. </author> <title> What are Race Conditions? Some Issues and Formalizations. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1 </volume> <pages> 74-88, </pages> <month> March </month> <year> 1992. </year> <note> 120 [52] *I. </note> <author> Nudler and L. Rudolph. </author> <title> Tools for the Efficient Development of Efficient Parallel Programs. </title> <booktitle> In Proceedings of the First Israeli Conference on Computer Systems Engineering, </booktitle> <month> May </month> <year> 1986. </year>
Reference-contexts: Since this lack of synchronization can lead to programs that behave unpredictably, it is important to be able to detect and report these conditions. The race detection literature has used a variety of terms to describe race conditions, but this thesis follows Netzer and Miller <ref> [48, 51] </ref>. 5.2.1 Types of Race Conditions Netzer and Miller [50, 51] recognized two fundamentally different types of races. <p> The race detection literature has used a variety of terms to describe race conditions, but this thesis follows Netzer and Miller [48, 51]. 5.2.1 Types of Race Conditions Netzer and Miller <ref> [50, 51] </ref> recognized two fundamentally different types of races.
Reference: [53] <author> Dejan Perkovic and Pete Keleher. </author> <title> Data Race Detection in Release-Consistent DSM. </title> <booktitle> In Operating System Design and Implementation, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Overhead in execution time for these protocols are shown to range from zero to less than a factor of three over a set of benchmarks | a significant improvement over slow-downs of three to six for Dinning and Schonberg [25], and five to 30 for Perkovic and Keleher <ref> [53] </ref>. Hood, Kennedy, and Mellor-Crummey [33] have lower overhead than the protocol-based techniques, at approximately 40%, but require compiler involvement. Efficient detection of data races is possible on DSM systems because a mech 65 anism is already in place to invoke the coherence protocol in response to shared-memory accesses. <p> They keep only one entry in the access histories, and use static analysis to reduce the number of monitored shared variables. Their slow-downs are roughly 40%, but the technique requires compiler support. 5.3.2 System-Level Approaches Perkovic and Keleher <ref> [53] </ref> have implemented system-level race detection in CVM, a page-based release-consistent DSM. Systems that implement release consistency must maintain ordering information that enables them to make a constant-time determination of whether two accesses are concurrent. <p> Thus, we cannot make a block readable, for example, until each race-detection region on the block has been read. Doing otherwise risks missing races since accesses can be missed. 5.4.2 Detecting Concurrency As with other system-level race-detection schemes <ref> [47, 53] </ref>, coherence protocols are only aware of the processor-level concurrency present in an executing application. This is independent of the means used to express the parallelism (i.e. fork calls, DOALL loops, Parmacs CREATE [15]). <p> Overheads for these protocols range from zero to less than a factor of three, 108 though there is reason to believe that this performance will improve on systems with greater bandwidth. In comparison, Perkovic and Keleher <ref> [53] </ref>, the only other system-level race-detection implementation, report slow-downs ranging from five to 30. The on-the-fly method of Dinning and Schonberg [25] has slow-downs ranging from three to six, though they handle a larger class of synchronization.
Reference: [54] <author> Steven K. Reinhardt. </author> <title> Tempest Interface Specification (Revision 1.2.1). </title> <type> Technical Report 1267, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: This has prompted interest in systems that enable users to select from a set of coherence protocols [13, 18] and, more recently, in systems in which a protocol is implemented in flexible software instead of being forever encoded in hardware <ref> [36, 54] </ref>. Experiments have shown that the performance penalties for implementing coherence actions in software, instead of hardware, are relatively small (especially if common operations are accelerated by hardware [36, 55]), and that tailoring protocols to the needs of applications can result in tremendous performance increases [28]. <p> These lines have recently begun to blur. Fine-grained access control has been successfully implemented in software [58], and a number of CCNUMA machines have moved their coherence protocols to software as well <ref> [36, 45, 54] </ref>. <p> The flush copies directive removes modified copies from a processor's cache before another invocation starts. The reconcile copies directive causes the memory system to reconcile modified locations and update global state to a consistent value. 29 3.5.1 LCM Implementation An LCM memory system was implemented using the Tempest interface <ref> [54] </ref>. It is based on an invalidation protocol similar to Stache [55], and provides cache-coherent shared memory as its default. Deviations from globally consistent memory come as a result of the LCM directive mark modification, which creates local, writable copies of memory blocks. <p> Page-based DSM systems like Munin [13] and TreadMarks [6] are an improvement, as they allow protocol policies to be selected for individual program objects, but their large coherence granularity hinders performance for programs with fine-grained sharing. Tempest <ref> [54] </ref> allows programmers to use both update and invalidation protocols as required, and significant performance improvements have been obtained by tailoring update protocols to specific applications [28].
Reference: [55] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proc. of the 21th Annual Int'l Symp. on Computer Architecture (ISCA'94), </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Experiments have shown that the performance penalties for implementing coherence actions in software, instead of hardware, are relatively small (especially if common operations are accelerated by hardware <ref> [36, 55] </ref>), and that tailoring protocols to the needs of applications can result in tremendous performance increases [28]. Parallel computers of the future will likely be DSM systems with software-implemented coherence protocols. <p> There can also be large overheads associated with transferring page-sized regions of memory. At the other extreme, Cache-Coherent Nonuniform Memory Access (CCNUMA) systems <ref> [2, 20, 43, 55] </ref> implement access control at the granularity of a cache block, reducing the contention over each block of memory. Both access-control mechanisms and the coherence protocols that they invoke can be implemented in hardware or software. <p> The accessing processor sends a request to the home of the referenced block, which performs bookkeeping duties and returns the data. Once a processor obtains the data, it caches a copy, which can be subsequently accessed until it is invalidated. Many protocols, for example the Stache protocol <ref> [55] </ref>, enforce coherence by permitting only a single writer (or multiple readers) to a block. When a home node receives a request for a writable copy of a block, it invalidates the outstanding read-only copies before returning the writable block. <p> Both conventional cache-coherent shared memory and the LCM protocol fit within the RSM model. RSM assumes the same basic mechanisms as cache-coherent shared memory <ref> [55, 64] </ref> but generalizes the coherence policies. RSM systems differ in the action taken by a processor in response to a request for a location and the way in which a processor reconciles multiple outstanding copies of a location. <p> The reconcile copies directive causes the memory system to reconcile modified locations and update global state to a consistent value. 29 3.5.1 LCM Implementation An LCM memory system was implemented using the Tempest interface [54]. It is based on an invalidation protocol similar to Stache <ref> [55] </ref>, and provides cache-coherent shared memory as its default. Deviations from globally consistent memory come as a result of the LCM directive mark modification, which creates local, writable copies of memory blocks. In general, these writable copies are produced by locally upgrading read-only blocks. <p> The lack of structure also gives little potential for reuse of cached data within an iteration. 3.7.2 Experimental Setup All experiments were performed on a 32-processor CM-5 using the Blizzard-E [58] implementation of Tempest <ref> [55] </ref>. Five runs of each application were made on a given memory system, and the run exhibiting the smallest total execution time was selected. The LCM implementations tested were hand-coded versions written prior to the creation of the Teapot tool. <p> The version used here is the original n 2 application, and uses both locks and barriers during the computation. 98 5.6.2 Experimental Setup All experiments were performed on a 32-processor CM-5 using the Blizzard-S [58] implementation of Tempest <ref> [55] </ref>. The baseline against which the race-detection protocols were compared was the Teapot-generated version of the Stache protocol. All protocols maintained coherence at the 32-byte block granularity, and race-detection granularities varied from 4 to 32 bytes. Schemes that used both bits and bytes to represent access history information were tested.
Reference: [56] <author> Steven K. Reinhardt, Robert W. Pfile, and David A. Wood. </author> <title> Decoupled Hardware Support for Distributed Shared Memory. </title> <booktitle> In Proc. of the 23th Annual Int'l Symp. on Computer Architecture (ISCA'96), </booktitle> <month> May </month> <year> 1996. </year>
Reference: [57] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-Time Paralleliza-tion and Scheduling of Loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference: [58] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 297-307, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Traditionally, page-based DSMs have taken an all-software approach while CCNUMA designs have implemented both mechanisms and protocols in hardware. These lines have recently begun to blur. Fine-grained access control has been successfully implemented in software <ref> [58] </ref>, and a number of CCNUMA machines have moved their coherence protocols to software as well [36, 45, 54]. <p> The lack of structure also gives little potential for reuse of cached data within an iteration. 3.7.2 Experimental Setup All experiments were performed on a 32-processor CM-5 using the Blizzard-E <ref> [58] </ref> implementation of Tempest [55]. Five runs of each application were made on a given memory system, and the run exhibiting the smallest total execution time was selected. The LCM implementations tested were hand-coded versions written prior to the creation of the Teapot tool. <p> Writable Invalid Have exclusive copy Table 5.3: Access permissions and protocol states gardless of the protocol state. Such a mechanism could be used to implement the race-detection protocols without the need for keeping blocks invalid. Currently, there is a Tempest implementation on the CM-5, Blizzard-S <ref> [58] </ref>, that does maintain data on invalid blocks. Blizzard-S separates the notion of protocol access control from the access control maintained by hardware or the operating-system. At the protocol level, blocks are tagged as invalid, read-only, or read-write. <p> Blocks can be cached in a writable state (in hardware) despite being tagged invalid by the protocol. Data values can therefore be maintained on blocks tagged invalid. Table 5.3 shows the updated permissions for the previous example. Blizzard-S <ref> [58] </ref> implements fine-grained access control by using a binary-rewriting tool to insert software lookups before loads and stores to shared memory. The lookups examine a table of block tags to determine whether the pending access is allowed. If not, the coherence protocol is directly invoked. <p> The lookups examine a table of block tags to determine whether the pending access is allowed. If not, the coherence protocol is directly invoked. This is in contrast to Blizzard-E <ref> [58] </ref>, which leverages fine-grained access control off of operating-system page protections and block-level ECC codes. 78 Process 1: A (1) = 1 Process 2: A (1) = 2 Process 3: A (2) = 3 Access History Details The techniques in the previous section ensure the coherence protocol is invoked on all <p> Water is one of the Splash [60] benchmarks, and simulates a body of water molecules. The version used here is the original n 2 application, and uses both locks and barriers during the computation. 98 5.6.2 Experimental Setup All experiments were performed on a 32-processor CM-5 using the Blizzard-S <ref> [58] </ref> implementation of Tempest [55]. The baseline against which the race-detection protocols were compared was the Teapot-generated version of the Stache protocol. All protocols maintained coherence at the 32-byte block granularity, and race-detection granularities varied from 4 to 32 bytes.
Reference: [59] <author> Edith Schonberg. </author> <title> On-the-Fly Detection of Access Anomalies. </title> <booktitle> Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, published in ACM SIGPLAN Notices, </booktitle> <volume> 24(7) </volume> <pages> 285-297, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users [33, 46]. Post-mortem [5, 23, 49] and on-the-fly methods <ref> [25, 33, 46, 52, 59, 61] </ref> improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods.
Reference: [60] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: As will be seen in the following sections, these reductions are a powerful technique for improving the performance of applications with fine-grained sharing. 4.3.1 Water Water is one of the Splash <ref> [60] </ref> benchmarks, and simulates interactions between molecules in a body of water. Processors are assigned a fraction of the simulated molecules, and are responsible for computing interactions between these and all others in the system. <p> LCP is a parallel implementation of the linear complementarity problem, written by Satish Chandra. Only barrier synchronization is used during the computation, though a single lock is used to combine normalization information at the end. Water is one of the Splash <ref> [60] </ref> benchmarks, and simulates a body of water molecules. The version used here is the original n 2 application, and uses both locks and barriers during the computation. 98 5.6.2 Experimental Setup All experiments were performed on a 32-processor CM-5 using the Blizzard-S [58] implementation of Tempest [55].
Reference: [61] <author> Guy L. Steele Jr. </author> <title> Making Asynchronous Parallelism Safe for the World. </title> <booktitle> In Conference Record of the Seventeenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <month> January </month> <year> 1990. </year> <month> 121 </month>
Reference-contexts: Static methods are necessarily conservative, since information on accessed memory locations is not precise, and the resulting spurious race reports can overwhelm users [33, 46]. Post-mortem [5, 23, 49] and on-the-fly methods <ref> [25, 33, 46, 52, 59, 61] </ref> improve race-detection accuracy by instrumenting programs and collecting information from actual executions. This information is either analyzed off-line, in the case of post-mortem techniques, or during execution, in on-the-fly methods.
Reference: [62] <author> Richard N. Taylor and Leon J. Osterweil. </author> <title> Anomaly Detection in Concurrent Software by Static Data Flow Analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(3):265-278, </volume> <month> May </month> <year> 1980. </year>
Reference-contexts: Races exist between these pairs of blocks since the program imposes no orderings between them. There is no race between b6 and b10, since they are ordered by synchronization in the program (the join at the end 69 Three basic approaches have been used to detect races. Static techniques <ref> [4, 9, 16, 27, 62] </ref> examine the text of a program and use static analysis to approximate the shared-memory locations accessed by each code block. This information, combined with the concurrency information in the POEG, detects apparent races by finding pairs of unordered blocks making conflicting accesses to common locations.
Reference: [63] <author> Charles P. Thacker and Lawrence C. Stewart. Firefly: </author> <title> a Multiprocessor Workstation. </title> <booktitle> In Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS II), </booktitle> <pages> pages 164-172, </pages> <month> October </month> <year> 1987. </year>
Reference: [64] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proc. of the 20th Annual Int'l Symp. on Computer Architecture (ISCA'93), </booktitle> <pages> pages 156-168, </pages> <month> May </month> <year> 1993. </year> <note> Also appeared in CMG Transactions, Spring 1994. </note>
Reference-contexts: Shared-memory DSM machines require a coherence protocol to manage the replication of data and to ensure that a parallel program sees a consistent view of memory <ref> [3, 19, 32, 42, 64] </ref>. In general, coherence protocols allow at most a single 2 processor to modify a shared location, either invalidating outstanding copies or updating copies with the new value. <p> Both conventional cache-coherent shared memory and the LCM protocol fit within the RSM model. RSM assumes the same basic mechanisms as cache-coherent shared memory <ref> [55, 64] </ref> but generalizes the coherence policies. RSM systems differ in the action taken by a processor in response to a request for a location and the way in which a processor reconciles multiple outstanding copies of a location.
Reference: [65] <author> David A. Wood, Garth G. Gibson, and Randy H. Katz. </author> <title> Verifying a Multiprocessor Cache Controller Using Random Case Generation. </title> <journal> IEEE Design and Test of Computers, </journal> <volume> 7(4) </volume> <pages> 13-25, </pages> <month> August </month> <year> 1990. </year> <month> 122 </month>
References-found: 64

