URL: http://www.eecs.umich.edu/PPP/MICRO95b.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: alexe,davidson@eecs.umich.edu  
Title: Register Allocation for Predicated Code  
Author: Alexandre E. Eichenberger and Edward S. Davidson 
Keyword: Register Allocation, Predicated Execution, Interference, Hyperblocks, Software Pipelining.  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory EECS Department, University of Michigan  
Abstract: Current compilers for VLIW and superscalar machines increase the instruction level parallelism of an application by merging several basic blocks into an enlarged predicated block, resulting in higher performance code but increased register requirements. We present a framework that computes precisely the interferences among virtual registers in the presence of predicated operations. Graph-coloring based register allocators can directly use the resulting interference graph. For interval-graph based register allocators, and others, we propose a technique that reduces the register requirements by allowing non-interfering virtual registers that overlap in time to share a common virtual register. Preliminary measurements on a benchmark of loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels indicate the effectiveness of this technique. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Y. Hsu. </author> <title> Highly Concurrent Scalar Processing. </title> <type> PhD thesis, </type> <institution> U. of Illinois at Urbana-Champaign, </institution> <year> 1986. </year>
Reference-contexts: A well established approach uses predication to merge several basic blocks into a single enlarged predicated block. This approach relies on predicated operations <ref> [1] </ref>, a class of operations that complete normally when a logical expression, referred to as the predicate of an operation, evaluates fl Appeared in MICRO-28, pp 180-191, November 1995. to true. Otherwise, the predicated operation is transformed into a no-op and has no side effect. <p> MinReg Stage Scheduler [13]: This stage sched-uler minimizes the register requirements (without bundling) of a modulo schedule by shifting the operations by multiples of II cycles. This scheduler finds a schedule with the lowest achievable register requirements for the given machine, loop, and Modulo Reservation Table (MRT) <ref> [1] </ref>. In this paper, we use the MRT produced by the Iterative Modulo scheduler as input to this scheduler. The machine model used in these experiments corresponds to the Cydra 5 machine [26]. This choice was motivated by the availability of quality code for this 188 machine.
Reference: [2] <author> S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, and R. A. Bringmann. </author> <title> Effective compiler support for predicated execution using hyperblocks. </title> <booktitle> MICRO, </booktitle> <pages> pages 45-54, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Otherwise, the predicated operation is transformed into a no-op and has no side effect. For scalar code, the hyperblock approach <ref> [2] </ref> used in the IMPACT compiler combines frequently executed basic blocks from multiple execution paths into an enlarged predicated block. This technique enables a range of compiler optimizations and facilitates the task of the scheduler, resulting in greatly improved schedules. <p> However, information about predicate values has recently been introduced in research compilers. In the IMPACT compiler, for example, information about predicates is stored in a Predicate Hierarchy Graph (PHG) <ref> [2] </ref> used to determine useful relations among predicate values. This information is used to refine dataflow analysis, optimization, scheduling, and allocation in presence of hyperblocks. Additionally, this information is used to conditionally reserve functional units when modulo scheduling under the Reverse-IF-Conversion scheme [14]. <p> Therefore, all optimizations modifying pred 182 icates would have to maintain the consistency of the dataflow graph at each step. Another approach is to reconstruct a dataflow graph from the predicated blocks each time dataflow analysis is needed <ref> [2] </ref>. This approach eliminates the complexity of maintaining a consistent dataflow graph and requires no changes to the classical dataflow analysis, traditional optimizations and register allocation. Additionally, the complexity of this approach is low, resulting in low compile time and good maintainability. However, this approach may result in conservative results.
Reference: [3] <author> B. R. Rau, C. D. Glaeser, and R. L. </author> <title> Picard. Efficient code generation for horizontal architectures: Compiler techniques and architecture support. </title> <publisher> ISCA, </publisher> <pages> pages 131-139, </pages> <year> 1982. </year>
Reference: [4] <author> B. R. Rau. </author> <title> Iterative Modulo Scheduling: An algorithm for software pipelining loops. </title> <booktitle> MICRO, </booktitle> <pages> pages 63-74, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: In this paper, we used modulo scheduling to software pipeline the loops of the benchmark suite. Modulo scheduling [1][3] has been shown to result in high performance code while requiring only modest compilation time <ref> [4] </ref> by restricting the scheduling space: it uses the same schedule for each iteration of a loop and it initiates successive iterations at a constant rate, i.e. one Initiation Interval (II clock cycles) apart. <p> We present the register requirements, i.e. the maximum number of live values at any single cycle of the loop schedule, associated with two distinct schedulers: Iterative Modulo Scheduler <ref> [4] </ref>: This modulo scheduler has been designed to deal efficiently with realistic machine models while producing schedules with near optimal steady state throughput. <p> The machine model used in these experiments corresponds to the Cydra 5 machine [26]. This choice was motivated by the availability of quality code for this 188 machine. In particular, the machine configuration is the one used in <ref> [4] </ref>, with 7 functional units (2 memory port units, 2 address generation units, 1 FP adder unit, 1 FP multiplier unit, and 1 branch unit). We first investigate the register requirements of the two schedulers and the register requirement of our best bundling heuristic 1 .
Reference: [5] <author> J. R. Allen, Ken Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <month> Jan. </month> <year> 1983. </year>
Reference: [6] <author> J. C. H. Park M. S. Schlansker. </author> <title> On predicated execution. </title> <type> Technical Report HPL-91-58, </type> <institution> HP Laboratories, </institution> <month> May </month> <year> 1991. </year>
Reference: [7] <author> W. Mangione-Smith, S. G. Abraham, and E. S. Davidson. </author> <title> Register requirements of pipelined processors. </title> <booktitle> ICS, </booktitle> <pages> pages 260-271, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: However, predicated blocks with high levels of parallelism result in higher register requirements, for two main reasons. First, the register requirements increase as more values are needed to support more concurrent operations. This effect is inherent to parallelism and is exacerbated by wider machines and higher latency pipelines <ref> [7] </ref>. Second, we show in this paper that the register requirements increase for predicated code as current compilers do not allocate registers as well in predicated code as in unpredicated code.
Reference: [8] <author> E. Su, A. Lain, S. Ramaswamy, D. J. Palermo, E. W. Hodges IV, and P. Banerjee. </author> <title> Advanced compilation techniques in the PARADIGM compiler for distributed-memory multicomputers. </title> <booktitle> ICS, </booktitle> <pages> pages 424-433, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: This framework is applicable either before or after scheduling. The current implementation relies on a symbolic package to generate accurate results, using an approach similar to the one taken in the PARADIGM compiler <ref> [8] </ref>. For register allocators based on the classical graph coloring method, originally proposed by Chaitin [9][10], register allocation for predicted codes can be simply achieved by using a refined interference graph instead of the conventional one. However, several register allocators depart from the graph coloring method: e.g.
Reference: [9] <author> G. J. Chaitin. </author> <title> Register allocation and spilling via graph coloring. </title> <booktitle> Symp. on Compiler Construction, </booktitle> <pages> pages 98-105, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: The second contribution of this paper is a set of heuristics that reduces the register requirements by allowing non-interfering virtual registers that overlap in time to share a common virtual register. We refer to this process as the bundling of compatible virtual registers. Bundling is similar to Chaitin-style coalescing <ref> [9] </ref> in that two or more virtual registers are combined; however, bundling differs in that it combines virtual registers with distinct values. <p> Figure 4c illustrates the def components for producer of x. 2.3 Interference In this section, we compute the interference between two predicated live ranges based on Chaitin's definition: two live ranges interfere if one of them is live at a definition point of the other <ref> [9] </ref>. This definition implies that overlapping def components do not interfere, provided no operations use these values. In the context of predicated blocks, this definition implies that simultaneous writes to a unique physical register should be tolerated by the hardware of the machine.
Reference: [10] <author> Preston Briggs, Keith D. Cooper, Ken Kennedy, and L. Torczon. </author> <title> Coloring heuristics for register allocation. </title> <booktitle> PLDI, </booktitle> <volume> 24(7) </volume> <pages> 275-284, </pages> <month> June </month> <year> 1989. </year>
Reference: [11] <author> L.J. Hendren, G. R. Gao, E. R. Altman, and C. Muk-erji. </author> <title> A register allocation framework based on hierarchical cyclic interval graphs. </title> <booktitle> Intl. Conf. on Compiler Construction, </booktitle> <publisher> Springer, </publisher> <pages> pages 176-191, </pages> <year> 1992. </year>
Reference-contexts: For register allocators based on the classical graph coloring method, originally proposed by Chaitin [9][10], register allocation for predicted codes can be simply achieved by using a refined interference graph instead of the conventional one. However, several register allocators depart from the graph coloring method: e.g. Hendren et al <ref> [11] </ref> investigate a framework based on cyclic interval graphs, introducing the notion of time in the register allocator paradigm. This additional notion of time is particularly useful for the live ranges of a loop, where live ranges may cross the boundary of an iteration.
Reference: [12] <author> B. R. Rau, M. Lee, P. P. Tirumalai, and M. S. Schlansker. </author> <title> Register allocation for software pipelined loops. </title> <booktitle> PLDI, </booktitle> <pages> pages 283-299, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: This additional notion of time is particularly useful for the live ranges of a loop, where live ranges may cross the boundary of an iteration. Another approach, investigated by Rau et al <ref> [12] </ref>, proposes a general framework for the allocation of registers in software pipelined loops for various code generation and hardware support schemes. <p> However, several register allocators depart from the graph coloring method [11]<ref> [12] </ref> as graph coloring methods do not provide a notion of time that is particularly useful for the live ranges of a loop, which may cross the boundary of an iteration. Also, nontraditional constraints such as the one presented in [12] to support various code generation and hardware support schemes are difficult to express within the graph coloring framework.
Reference: [13] <author> A. E. Eichenberger, E. S. Davidson, and S. G. Abra-ham. </author> <title> Minimum register requirements for a modulo schedule. </title> <booktitle> MICRO, </booktitle> <pages> pages 75-84, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: We investigate the performance of several bundling heuristics in a benchmark suite of modulo scheduled loops extracted from the Perfect Club, SPEC-89, and the Livermore Kernels. To minimize the interaction between bundling and scheduling heuristics, we used the scheduling technique described in <ref> [13] </ref> which minimizes the register requirements for a modulo schedule. Preliminary results indicate that our best bundling heuristic decreases the average register requirements from 39.3 to 36.5 registers and increases the percent age of loops with no more than 64 registers from 85% to 95%. <p> As a result of this bias, the loops of the benchmark suite require an average of 39.3 registers versus 27.9 in the input set using the scheduling technique described in <ref> [13] </ref> which minimizes the register requirements for a modulo schedule. It contains loops with larger register requirements; consequently, decreasing the register requirements of the loops in the benchmark suite is crucial. Ops. Reg. Sched. Pred. <p> In its current form, it does not attempt to minimize the register requirements of its schedules; however, the register requirements of its schedules may be reduced significantly by simple heuristics [24]. MinReg Stage Scheduler <ref> [13] </ref>: This stage sched-uler minimizes the register requirements (without bundling) of a modulo schedule by shifting the operations by multiples of II cycles. This scheduler finds a schedule with the lowest achievable register requirements for the given machine, loop, and Modulo Reservation Table (MRT) [1].
Reference: [14] <author> N. J. Warter, G. E. Haab, and J. W. Bockhaus. </author> <title> Enhanced Modulo Scheduling for loops with conditional branches. </title> <booktitle> MICRO, </booktitle> <pages> pages 170-179, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: This information is used to refine dataflow analysis, optimization, scheduling, and allocation in presence of hyperblocks. Additionally, this information is used to conditionally reserve functional units when modulo scheduling under the Reverse-IF-Conversion scheme <ref> [14] </ref>. A complementary approach is taken with the Gated Single Assignment (GSA) form [15], where precise predicate information is embedded in the dataflow graph. This approach is used in the Polaris paralleliz-ing compiler to refine data and memory dependence analysis and to aid loop parallelization [16].
Reference: [15] <author> R. A. Ballance, A. B. Maccabe, and K. J. Ottenstein. </author> <title> The program dependence web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> PLDI, </booktitle> <pages> pages 257-271, </pages> <year> 1990. </year>
Reference-contexts: This information is used to refine dataflow analysis, optimization, scheduling, and allocation in presence of hyperblocks. Additionally, this information is used to conditionally reserve functional units when modulo scheduling under the Reverse-IF-Conversion scheme [14]. A complementary approach is taken with the Gated Single Assignment (GSA) form <ref> [15] </ref>, where precise predicate information is embedded in the dataflow graph. This approach is used in the Polaris paralleliz-ing compiler to refine data and memory dependence analysis and to aid loop parallelization [16].
Reference: [16] <author> P. Tu and D Padua. </author> <title> Gated SSA-based demand-driven symbolic analysis for parallelizing compilers. </title> <booktitle> ICS, </booktitle> <pages> pages 414-423, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: A complementary approach is taken with the Gated Single Assignment (GSA) form [15], where precise predicate information is embedded in the dataflow graph. This approach is used in the Polaris paralleliz-ing compiler to refine data and memory dependence analysis and to aid loop parallelization <ref> [16] </ref>. In this paper, we illustrate the impact of predicated code on the register requirements and outline our general framework in Section 2. The bundling heuristics after scheduling and before scheduling are respectively introduced in Sections 3 and 4. <p> This approach presents none of the cited drawbacks of the previous approaches; moreover, precise information about predicate expressions of live ranges may also be extremely useful to other optimizations, such as memory disambiguation <ref> [16] </ref>. The drawback of this approach is its reliance on a more expensive analysis, partly due to the use of a symbolic package to detect predicate expression disjointness and partly due to the computation of a more complicated interference relation.
Reference: [17] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: Then, local analysis for each basic block determines the cycle-by-cycle live range of each virtual register that is live-in, live-out, and/or referenced in the basic block <ref> [17, pp 534-525] </ref>. This second step is highly machine dependent; we assume in our examples and experiments that a virtual register is reserved in the cycle where its earliest-define operation is scheduled and becomes free in the cycle following its last-use operation. <p> As a result, the register requirements of this predicated block increase from 2 to 3. This increase in register requirements is mainly due to two reasons. First, without knowledge about predicates, the dataflow analysis must make conservative assumptions about the side effects of the predicated operations <ref> [17] </ref>. Second, the solutions of the dataflow analysis rely heavily on the connection topology among basic blocks in the flow graph, which is altered by the IF-conversion process.
Reference: [18] <author> R. C. Johnson. </author> <type> Personal communication. </type> <month> June </month> <year> 1995. </year>
Reference-contexts: However, ongoing research on efficient algorithms for precise or approximate analysis of predicated code <ref> [18] </ref> will significantly improve the execution time of this approach. We now present the mechanisms required by our framework in more detail. 2.1 Predicate Extraction The predicate extraction mechanism allows the compiler to find how predicates are defined and used.
Reference: [19] <author> V. Kathail, M. S. Schlansker, and B. R. Rau. </author> <title> HPL PlayDoh architecture specification: Version 1.0. </title> <type> Technical Report HPL-93-80, </type> <institution> HP Laboratories, </institution> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: This extraction mechanism is very sensitive to the instruction set architecture, since it relies on the precise semantics of the operations that define the predicate values. In this work, we adopt the predication scheme of the HPL Playdoh architecture <ref> [19] </ref>.
Reference: [20] <author> M. S. Schlansker, V. Kathail, and S. Anik. </author> <title> Height reduction of control recurrences for ILP processors. </title> <booktitle> MICRO, </booktitle> <pages> pages 40-51, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: The supported predicate types are: unconditional, conditional, or, and, and their complements. Operations defining the predicates guarding if-then-else constructs typically use the unconditional type and its complement. More elaborate predicate types are required to evaluate several comparisons in parallel, as advocated in <ref> [20] </ref>. For example, the or type may be used to implement a wired-or function of several comparisons executed in parallel.
Reference: [21] <author> J. R. Goodman and W.-C. Hsu. </author> <title> Code scheduling and register allocation in large basic blocs. </title> <booktitle> ICS, </booktitle> <pages> pages 442-452, </pages> <year> 1988. </year>
Reference-contexts: Instead, we must rely on the earliest and latest feasible scheduling time of an operation <ref> [21] </ref> [22]. In our current implementation, the pre-scheduling bundling heuristics rely on a weaker form of interference where two virtual registers are considered to interfere if any pair of their producer operations do not execute under mutually disjoint predicates.
Reference: [22] <author> R. A. Huff. </author> <title> Lifetime-sensitive modulo scheduling. </title> <booktitle> PLDI, </booktitle> <pages> pages 258-267, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Instead, we must rely on the earliest and latest feasible scheduling time of an operation [21] <ref> [22] </ref>. In our current implementation, the pre-scheduling bundling heuristics rely on a weaker form of interference where two virtual registers are considered to interfere if any pair of their producer operations do not execute under mutually disjoint predicates. <p> Figure 7 presents the fraction of the loops in the benchmark suite that can be scheduled for a machine with any given number of registers without spilling and without increasing II. The "Schedule Independent Lower Bound" curve corresponds to <ref> [22] </ref> and is representative of the register requirements of a machine without any resource conflicts, but does not support bundling, i.e. it ignores the possible decrease in register requirements due to the bundling of compatible virtual registers.
Reference: [23] <author> S. S. Pinter. </author> <title> Register allocation with instruction scheduling: a new approach. </title> <booktitle> PLDI, </booktitle> <pages> pages 248-257, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In this scheme, additional scheduling edges would supply some further temporal constraints on the subsequent scheduling. Some edges are guaranteed not to actually constrain the scheduler, as shown by Pinter <ref> [23] </ref>. Others, however, may constrain the scheduler.
Reference: [24] <author> A. E. Eichenberger and E. S. Davidson. </author> <title> Stage scheduling: A technique to reduce the register requirements of a modulo schedule. </title> <booktitle> MICRO, </booktitle> <pages> pages 180-191, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: For this reason, we did not investigate this approach. 5 Measurements We investigated the register requirements of the integer and floating point register file for a benchmark of loops obtained from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels (references, see <ref> [24] </ref>). Our input loops consist exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler [25]. <p> In its current form, it does not attempt to minimize the register requirements of its schedules; however, the register requirements of its schedules may be reduced significantly by simple heuristics <ref> [24] </ref>. MinReg Stage Scheduler [13]: This stage sched-uler minimizes the register requirements (without bundling) of a modulo schedule by shifting the operations by multiples of II cycles. This scheduler finds a schedule with the lowest achievable register requirements for the given machine, loop, and Modulo Reservation Table (MRT) [1].
Reference: [25] <author> J. C. Dehnert and R. A. Towle. </author> <title> Compiling for the Cydra 5. </title> <booktitle> In The J. of Supercomputing, </booktitle> <volume> volume 7, </volume> <pages> pages 181-227, </pages> <year> 1993. </year>
Reference-contexts: Our input loops consist exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler <ref> [25] </ref>. The input to our scheduling algorithms consists of the Fortran77 compiler intermediate representation after load-store elimination, recurrence back-substitution, and IF-conversion.
Reference: [26] <editor> G. R. Beck et al. </editor> <booktitle> The Cydra 5 mini-supercomputer: Architecture and implementation. In The J. of Supercomputing, </booktitle> <volume> volume 7, </volume> <pages> pages 143-180, </pages> <year> 1993. </year>
Reference-contexts: In this paper, we use the MRT produced by the Iterative Modulo scheduler as input to this scheduler. The machine model used in these experiments corresponds to the Cydra 5 machine <ref> [26] </ref>. This choice was motivated by the availability of quality code for this 188 machine. In particular, the machine configuration is the one used in [4], with 7 functional units (2 memory port units, 2 address generation units, 1 FP adder unit, 1 FP multiplier unit, and 1 branch unit).
Reference: [27] <author> A. E. Eichenberger, E. S. Davidson, and S. G. Abra-ham. </author> <title> Optimum modulo schedules for minimum register requirements. </title> <booktitle> ICS, </booktitle> <pages> pages 31-40, </pages> <month> July </month> <year> 1995. </year> <month> 191 </month>
Reference-contexts: This large gap is partly explained by the strong simplification (no resource conflict) of the lower bound, a gap that is particularly apparent for large loops and negligible for small loops <ref> [27] </ref>. The second and third curves illustrate the decrease in register requirements due to the bundling of compatible virtual registers when applied to the schedules produced by the MinReg Stage Scheduler.
References-found: 27

