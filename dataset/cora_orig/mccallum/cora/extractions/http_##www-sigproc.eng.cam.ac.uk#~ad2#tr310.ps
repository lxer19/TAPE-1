URL: http://www-sigproc.eng.cam.ac.uk/~ad2/tr310.ps
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: On Sequential Simulation-Based Methods for Bayesian Filtering  
Author: Arnaud Doucet 
Keyword: Bayesian estimation, optimal filtering, nonlinear non-Gaussian state space models, hidden Markov models, sequential Monte Carlo methods.  
Address: Cambridge CB2 1PZ Cambridge  
Affiliation: Signal Processing Group, Department of Engineering University of  
Pubnum: Technical report CUED/F-INFENG/TR.310  
Email: Email: ad2@eng.cam.ac.uk  
Date: (1998)  
Abstract: In this report, we present an overview of sequential simulation-based methods for Bayesian filtering of nonlinear and non-Gaussian dynamic models. It includes in a general framework numerous methods proposed independently in various areas of science and proposes some original developments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akashi and H. Kumamoto, </author> <title> "State Estimation for Systems under Measurements Noise with Markov Dependent Statistical Property an Algorithm based on Random Sampling", </title> <booktitle> in Proc. 6 th Conf. IFAC, </booktitle> <year> 1975. </year>
Reference-contexts: Other interesting work in automatic control was done during the 60's and 70's based on sequential Monte Carlo integration methods, see <ref> [1, 2, 4, 25, 26, 47, 61] </ref>. Most likely because of the primitive computers available at the time, these last algorithms were overlooked and forgotten 2 . In the late 80's, the great increase of computational power allowed the rebirth of numerical integration methods for Bayesian filtering [33]. <p> Moreover the trajectories n (i) o which have high importance weights ew (i) k are statistically selected many times. In (75), numer ous trajectories x (i 1 ) (i 2 ) 0:k are in fact equal for i 1 6= i 2 2 <ref> [1; : : : ; N ] </ref>. There is a loss of "diversity". Recently, Berzuini et al. [12] have however established a central limit theorem for the estimate of I (f k ) which is obtained when the SIR procedure is applied at each iteration. <p> It is possible to use a MC filter based on Rao-Blackwellisation. Indeed, conditional upon x 1 0:n is a linear Gaussian state space model and the integrations required by the Rao-Blackwellisation method can be realized using the Kalman filter. Akashi and Kumamoto <ref> [1, 4, 58] </ref> introduced this algorithm under the name of RSA (Random Sampling Algorithm) in the particular case where x 1 k is a homogeneous scalar finite state-space Markov chain 4 . In this case, they adopted the optimal importance function p k fi y 0:k ; x 1 .
Reference: [2] <author> H. Akashi and H. Kumamoto, </author> <title> "Construction of Discrete-time Nonlinear Filter by Monte Carlo Methods with Variance-reducing Techniques", </title> <journal> Sys. Cont., </journal> <volume> vol. 19, no. 4, </volume> <year> 1975, </year> <pages> pp. </pages> <note> 211-221 (in Japanese). </note>
Reference-contexts: Other interesting work in automatic control was done during the 60's and 70's based on sequential Monte Carlo integration methods, see <ref> [1, 2, 4, 25, 26, 47, 61] </ref>. Most likely because of the primitive computers available at the time, these last algorithms were overlooked and forgotten 2 . In the late 80's, the great increase of computational power allowed the rebirth of numerical integration methods for Bayesian filtering [33]. <p> Rao-Blackwellisation for Sequential Importance Sampling We propose here to improve SIS using variance reduction methods designed to make the most of the model studied. Numerous methods have been developed so as to reduce the variance of MC estimates including antithetic sampling [25, 26] and control variates <ref> [2, 26] </ref>. We apply here the Rao-Blackwellisation method [4]. We show how it is possible to apply this method successfully to an important class of HMM and obtain hybrid filters where a part of the calculations is realized analytically and the other part using MC methods. <p> Example 10. Finite State-Space HMM Let us consider the following model p x 1 fi k1 p x 2 fi k ; x 2 k ; x 2 4 Akashi and Kumamoto made the connections with the work of Handschin and Mayne in <ref> [2] </ref>. 5 In this framework, the extension to a time-varying channel h k modeled by a linear Gaussian state space model is straightforward.
Reference: [3] <author> H. Akashi, H. Kumamoto and K. Nose, </author> <title> "Application of Monte Carlo Method to Optimal Control for Linear Systems under Measurement Noise with Markov Dependent Statistical Property", </title> <journal> Int. J. Cont., </journal> <volume> vol. 22, no. 6, </volume> <year> 1975, </year> <pages> pp. 821-836. </pages>
Reference: [4] <author> H. Akashi and H. Kumamoto, </author> <title> "Random Sampling Approach to State Estimation in Switching Environments", </title> <journal> Automatica, </journal> <volume> vol. 13, </volume> <year> 1977, </year> <pages> pp. 429-434. </pages>
Reference-contexts: Other interesting work in automatic control was done during the 60's and 70's based on sequential Monte Carlo integration methods, see <ref> [1, 2, 4, 25, 26, 47, 61] </ref>. Most likely because of the primitive computers available at the time, these last algorithms were overlooked and forgotten 2 . In the late 80's, the great increase of computational power allowed the rebirth of numerical integration methods for Bayesian filtering [33]. <p> First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case <ref> [4] </ref>. More recently, this importance function has been used in [14, 15, 16, 30, 37, 38, 40]. <p> Numerous methods have been developed so as to reduce the variance of MC estimates including antithetic sampling [25, 26] and control variates [2, 26]. We apply here the Rao-Blackwellisation method <ref> [4] </ref>. We show how it is possible to apply this method successfully to an important class of HMM and obtain hybrid filters where a part of the calculations is realized analytically and the other part using MC methods. <p> It is possible to use a MC filter based on Rao-Blackwellisation. Indeed, conditional upon x 1 0:n is a linear Gaussian state space model and the integrations required by the Rao-Blackwellisation method can be realized using the Kalman filter. Akashi and Kumamoto <ref> [1, 4, 58] </ref> introduced this algorithm under the name of RSA (Random Sampling Algorithm) in the particular case where x 1 k is a homogeneous scalar finite state-space Markov chain 4 . In this case, they adopted the optimal importance function p k fi y 0:k ; x 1 . <p> In this case, they adopted the optimal importance function p k fi y 0:k ; x 1 . Indeed, it is possible to sample from this discrete distribution and to evaluate the importance weight p 0:k1 using the Kalman filter <ref> [4] </ref>. Similar developments have been proposed by Svetnik et al. [53]. The algorithm for blind deconvolution recently proposed by Liu et al. [38] is also a particular case of this method where x 2 k = h is a time-invariant channel of Gaussian prior distribution 5 .
Reference: [5] <author> D.L. Aspach and H.W. Sorenson, </author> <title> "Nonlinear Bayesian Estimation using Gaussian Sum Approximation", </title> <journal> IEEE Trans. Auto. Cont., </journal> <volume> vol. 17, no. 4, </volume> <pages> pp. 439-448, </pages> <year> 1972. </year>
Reference-contexts: From the mid 60's, a huge number of papers and books have been devoted to obtaining approximations of these distributions, see [32] for example. The most popular algorithms, the extended Kalman filter and the Gaussian sum filter, rely on analytical approximations <ref> [5, 6] </ref> but early well-known work relying on deterministic numerical integration methods was also performed by Bucy and co-workers, see [13] for example.
Reference: [6] <author> B.D.O. Anderson and J.B. Moore, </author> <title> Optimal Filtering, </title> <address> Englewood Cliffs, </address> <year> 1979. </year>
Reference-contexts: From the mid 60's, a huge number of papers and books have been devoted to obtaining approximations of these distributions, see [32] for example. The most popular algorithms, the extended Kalman filter and the Gaussian sum filter, rely on analytical approximations <ref> [5, 6] </ref> but early well-known work relying on deterministic numerical integration methods was also performed by Bucy and co-workers, see [13] for example. <p> There is no general method to build suboptimal importance functions and it is necessary to build these on a case by case basis, dependent on the model studied. To this end, it is possible to base these developments on previous work on standard suboptimal filtering methods <ref> [6, 60] </ref>. Importance distribution obtained by local linearisation. <p> Performing an approximation up to the first order of the observation equation <ref> [6] </ref>, we get y k = g (x k ) + w k @g (x k ) fi fi x k =f (x k1 ) We have now defined a new model with a similar evolution equation to (47) but with a linear Gaussian observation equation (49), obtained by linearising g <p> It is possible to use a "Rao-Blackwellised" MC filter. Indeed, condi tional upon x 1 0:n , x 2 0:n is a finite state-space Markov chain of known parameters and thus the integrations require by the Rao-Blackwellisation method can be done analytically <ref> [6] </ref>. 5. <p> For this model, the optimal filter is the Kalman filter <ref> [6] </ref>. On Sequential Simulation-Based Methods for Bayesian Filtering 21 Optimal importance function.
Reference: [7] <author> M.L. Andrade, L. Gimeno and M.J. Mendes, </author> <title> "On the Optimal and Suboptimal Nonlinear Filtering Problem for Discrete Time Systems", </title> <journal> IEEE Trans. Auto. Cont., </journal> <volume> vol. 17, </volume> <year> 1978, </year> <pages> pp. 439-448. </pages>
Reference-contexts: The SIS algorithms have similar performances to the bootstrap filter for a smaller computational cost. The most interesting algorithm is based on the optimal importance function which limits seriously the number of resampling steps. 6.2. Nonlinear series. We consider here the following nonlinear reference model <ref> [7, 23, 35, 56] </ref>: = 2 x k1 2 + 8 cos (1:2k) + v k = 2 + w k On Sequential Simulation-Based Methods for Bayesian Filtering 22 where x 0 ~ N (0; 5), v k and w k are mutually independent white Gaussian noises, v k ~ N
Reference: [8] <author> M. Askar and H. Derin, </author> <title> "A Recursive Algorithm for the Bayes Solution of the Smoothing Problem", </title> <journal> IEEE Trans. Auto. Cont., </journal> <volume> vol. 26, no. 2, </volume> <year> 1981, </year> <pages> pp. 558-561. </pages>
Reference-contexts: This problem is even more severe for the bootstrap filter where one resamples at each time instant. It is necessary to develop an alternative algorithm. We propose an original algorithm to solve this problem. This algorithm is based on the following formula <ref> [8, 33] </ref>: p ( x k j y 0:n ) = p ( x k j y 0:k ) p ( x k+1 j y 0:n ) p ( x k+1 j x k ) dx k+1 (98) We seek here an approximation of the fixed-interval smoothing distribution with the following
Reference: [9] <author> D. Avitzour, </author> <title> "A Stochastic Simulation Bayesian Approach to Multitarget Tracking", </title> <booktitle> IEE Proc. on Radar, Sonar and Navigation, </booktitle> <volume> vol. 142, no. 2, </volume> <year> 1995, </year> <pages> pp. 41-44. </pages>
Reference-contexts: Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith <ref> [9, 10, 12, 23, 31] </ref>, simultaneously developed by Kitagawa [29, 34, 35, 36], applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of
Reference: [10] <author> E.R. Beadle and P.M. Djuric, </author> <title> "A Fast Weighted Bayesian Bootstrap Filter for Nonlinear Model State Estimation", </title> <journal> IEEE Trans. Aeros. Elec. Sys., </journal> <volume> vol. 33, no. 1, </volume> <year> 1997, </year> <pages> pp. 338-343. </pages>
Reference-contexts: A straightforward application of the SIR procedure has a complexity in O (N ln N ) [23]. This complexity is very important and, so as to reduce it, Beadle et al. <ref> [10] </ref> have recently proposed several ad hoc methods. <p> Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith <ref> [9, 10, 12, 23, 31] </ref>, simultaneously developed by Kitagawa [29, 34, 35, 36], applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of
Reference: [11] <author> P.R. Benyon, </author> <title> "Monte Carlo and Other Methods for Nonlinear Non-Gaussian Estimation", </title> <journal> Math. Comp. Simul., </journal> <volume> no. 32, </volume> <year> 1990, </year> <pages> pp. 215-220. </pages>
Reference: [12] <author> C. Berzuini, N. Best, W. Gilks and C. Larizza, </author> <title> "Dynamic Conditional Independence Models and Markov Chain Monte Carlo Methods", </title> <journal> forthcoming J. Am. Stat. Assoc., </journal> <year> 1997. </year> <title> On Sequential Simulation-Based Methods for Bayesian Filtering 24 </title>
Reference-contexts: Moreover, this solution, although simple, is computationally very expensive. Other MC methods based on MCMC methods have been proposed to simulate approximately from p (i) evaluate p (i) , see <ref> [12, 18, 41] </ref>. These iterative algorithms appear to be of limited interest in an on-line framework and there is a lack of theoretical convergence results. In fact, the general framework of SIS allows us to consider other importance functions built so as to approximate analytically the optimal importance function. <p> In (75), numer ous trajectories x (i 1 ) (i 2 ) 0:k are in fact equal for i 1 6= i 2 2 [1; : : : ; N ]. There is a loss of "diversity". Recently, Berzuini et al. <ref> [12] </ref> have however established a central limit theorem for the estimate of I (f k ) which is obtained when the SIR procedure is applied at each iteration. Despite its drawbacks, the SIR algorithm is the basis of numerous works. <p> Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith <ref> [9, 10, 12, 23, 31] </ref>, simultaneously developed by Kitagawa [29, 34, 35, 36], applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of
Reference: [13] <author> R.S Bucy and K.D. Senne, </author> <title> "Digital Synthesis of Nonlinear Filters", </title> <journal> Automatica, </journal> <volume> vol. 7, </volume> <year> 1971, </year> <pages> pp. 287-298. </pages>
Reference-contexts: The most popular algorithms, the extended Kalman filter and the Gaussian sum filter, rely on analytical approximations [5, 6] but early well-known work relying on deterministic numerical integration methods was also performed by Bucy and co-workers, see <ref> [13] </ref> for example. Other interesting work in automatic control was done during the 60's and 70's based on sequential Monte Carlo integration methods, see [1, 2, 4, 25, 26, 47, 61].
Reference: [14] <author> R. Chen and J.S. Liu, </author> <title> "Predictive Updating Methods with Application to Bayesian Classification", </title> <journal> J. Roy. Stat. Soc. B, </journal> <volume> vol. 58, no. 2, </volume> <year> 1996, </year> <pages> pp. 397-415. </pages>
Reference-contexts: First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case [4]. More recently, this importance function has been used in <ref> [14, 15, 16, 30, 37, 38, 40] </ref>.
Reference: [15] <author> P. Del Moral and G. Salut, </author> <title> "Filtrage non-lineaire : resolution a la Monte Carlo", </title> <journal> C.R.A.S., </journal> <volume> vol. 320, </volume> <year> 1995, </year> <pages> pp. </pages> <note> 1147-1152 (in French). </note>
Reference-contexts: First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case [4]. More recently, this importance function has been used in <ref> [14, 15, 16, 30, 37, 38, 40] </ref>. <p> It leads in most cases to unbounded importance weights. 3. Resampling As it has been previously illustrated, the degeneracy of the algorithm based on SIS can not be avoided. In <ref> [15] </ref>, a forgetting factor on the weights associated with the optimal importance function is introduced and, under stability and regularity assumptions on the Markov model, an interesting time-uniform convergence result is obtained as N ! +1.
Reference: [16] <author> P. Del Moral, J.C. Noyer, G. Rigal and G. Salut, </author> <title> "Resolution particulaire en traite-ment non-lineaire du signal: application Radar/Sonar", </title> <journal> Trait. Signal, </journal> <volume> vol. 12, no. 4, </volume> <year> 1995, </year> <pages> pp. </pages> <note> 287-301 (in French). </note>
Reference-contexts: First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case [4]. More recently, this importance function has been used in <ref> [14, 15, 16, 30, 37, 38, 40] </ref>. <p> Practically, N &lt; +1 and this regularization slows down but does not avoid degeneracy of the algorithm <ref> [16] </ref>. It is necessary to introduce another procedures. The basic idea of resampling methods consists of eliminating the trajectories which have weak normalised importance weights and to multiply trajectories with strong importance weights. We adopt as a measure of degeneracy of the algorithm the effective sample size.
Reference: [17] <author> A. Doucet, E. Barat and P. Duvaut, </author> <title> "A Monte Carlo approach to Recursive Bayesian State Estimation", </title> <booktitle> in Proc. IEEE Work. </booktitle> <address> HOS, June 1995, Spain. </address>
Reference: [18] <author> A. Doucet, E. Barat and P. Duvaut, </author> <title> "Implantation du paradigme bayesien pour l'estimation recursive d'etat", </title> <booktitle> in Proc. 15 eme colloque GRETSI, </booktitle> <address> Juan-les-Pins, </address> <year> 1995, </year> <pages> pp. </pages> <note> 73-76 (in French). </note>
Reference-contexts: Moreover, this solution, although simple, is computationally very expensive. Other MC methods based on MCMC methods have been proposed to simulate approximately from p (i) evaluate p (i) , see <ref> [12, 18, 41] </ref>. These iterative algorithms appear to be of limited interest in an on-line framework and there is a lack of theoretical convergence results. In fact, the general framework of SIS allows us to consider other importance functions built so as to approximate analytically the optimal importance function. <p> To limit the loss of "diversity", many ad hoc procedures have been proposed. In [23], the trajectories are artificially perturbed after the resampling step. Another simple solution consists of building a semi-parametric ap proximation of b P (dx) = P N before resampling <ref> [18, 24] </ref> but the choice of a "good" kernel K () is difficult. Higuchi [27, 28] proposes various heuristic procedures taken from the genetic algorithms literature to introduce such a diversity among samples.
Reference: [19] <author> A. Doucet, </author> <title> Monte Carlo Methods for Bayesian Estimation of Hidden Markov Models. Application to Radiation Signals, </title> <type> Ph.D. Thesis, </type> <institution> Univ. Paris-Sud, Orsay, </institution> <note> 1997 (in French with chapters 4 and 5 in English). </note>
Reference-contexts: The closest work to this report is the work of Liu and Chen [41], developed independently, which underlines similarly the central role of sequential importance sampling (SIS) in sequential simulation-based methods for Bayesian filtering. 1 This technical report is a translation of chapter 3 of <ref> [19] </ref> in abbreviated form. 2 To the best of my knowledge, these important works are cited neither in any standard article and book on optimal estimation nor in any current work on the subject. 1 On Sequential Simulation-Based Methods for Bayesian Filtering 2 This report is organized as follows. <p> Proposition 4. p (i) is the importance function which minimizes the vari ance of the importance weight w fl (i) k conditional upon x (i) The proof is straightforward <ref> [19] </ref>. First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case [4]. <p> y 0:n ) w fl 0:n ; x 2 fl and var ( x 1 0:n j y 0:n ) d g D N (f n ) var ( x 1 0:n ;x 2 0:n j y 0:n ) d D N (f n ) (85) The proof is straightforward <ref> [19] </ref>.
Reference: [20] <author> A. Doucet and P. Duvaut, </author> <title> "A New Computational Method for Optimal Estimation of Nonlinear Non-Gaussian Dynamic Models", </title> <booktitle> in Proc. </booktitle> <address> ICSPC'98, </address> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: This overview includes in the general framework of SIS numerous approaches that have been previously proposed independently in the literature for nearly 30 years. Several original extensions have also been presented. In this re-emerging area, there are numerous ways of improvement including among many others new variance reduction methods <ref> [20, 21] </ref> or efficient hybrid IS/MCMC methods. 8. Acknowledgments I acknowledge Dr. T. Higuchi, Pr. G. Kitagawa and Dr. H. Tanizaki who sent me their preprints on this subject. I also acknowledge C. Andrieu, T. Clapp and Dr. S. Godsill for various comments that helped me to improve this report.
Reference: [21] <author> A. Doucet and C. Andrieu, </author> <title> "A Killing and Splitting Scheme for Sequential Importance Sampling Applied to Bayesian Filtering", </title> <note> in preparation. </note>
Reference-contexts: This overview includes in the general framework of SIS numerous approaches that have been previously proposed independently in the literature for nearly 30 years. Several original extensions have also been presented. In this re-emerging area, there are numerous ways of improvement including among many others new variance reduction methods <ref> [20, 21] </ref> or efficient hybrid IS/MCMC methods. 8. Acknowledgments I acknowledge Dr. T. Higuchi, Pr. G. Kitagawa and Dr. H. Tanizaki who sent me their preprints on this subject. I also acknowledge C. Andrieu, T. Clapp and Dr. S. Godsill for various comments that helped me to improve this report.
Reference: [22] <author> J. Geweke, </author> <title> "Bayesian Inference in Econometrics Models using Monte Carlo Integration", </title> <journal> Econometrica, </journal> <volume> vol. 57, </volume> <year> 1989, </year> <pages> pp. 1317-1339. </pages>
Reference-contexts: to) and the normalised importance weights are equal to ew (i) w n j=1 w n The "true" importance weights w fl (i) n have been replaced by the following estimate: bw fl (i) n (22) This method is well-known in the statistical literature as Bayesian IS, see for example <ref> [22, 51] </ref>. We recall here some classical results on this MC method. Assumption 1 - x 0:n ; i = 1; :::; N is a set of i.i.d. vectors distributed according to ( x 0:n j y 0:n ). <p> The support = cludes the support p = I (f n ) exists and is finite. Assumption 2 - E p ( :jy 0:n ) [w (x 0:n )] &lt; +1 and E p ( :jy 0:n ) [f 2 A sufficient condition to verify assumption 2 is <ref> [22] </ref>: var p ( :jy 0:n ) [f n (x 0:n )] &lt; +1 and w (x 0:n ) &lt; C n &lt; +1 for any x 0:n 2 (23) On Sequential Simulation-Based Methods for Bayesian Filtering 5 Proposition 1. <p> This result is important as it means that we can interpret the IS method as a simulation method to sample from P ( dx 0:n j y 0:n ) rather than as an integration method, see <ref> [22] </ref> for a similar interpretation. Using the delta method, we also obtain the following proposition. Proposition 2. (Geweke 1989 [22]) Under assumptions 1 and 2, p N!+1 f n (25) where 2 2 We show in the following subsection how it is possible to obtain easily a recursive MC filter using <p> it means that we can interpret the IS method as a simulation method to sample from P ( dx 0:n j y 0:n ) rather than as an integration method, see <ref> [22] </ref> for a similar interpretation. Using the delta method, we also obtain the following proposition. Proposition 2. (Geweke 1989 [22]) Under assumptions 1 and 2, p N!+1 f n (25) where 2 2 We show in the following subsection how it is possible to obtain easily a recursive MC filter using Bayesian IS. 2.3. Monte Carlo filter using sequential importance sampling.
Reference: [23] <author> N.J. Gordon, D.J. Salmond and A.F.M. Smith, </author> <title> "Novel Approach to Nonlinear/Non-Gaussian Bayesian State Estimation", </title> <journal> IEE-Proceedings-F, </journal> <volume> vol. 140, no. 2, </volume> <year> 1993, </year> <pages> pp. 107-113. </pages>
Reference-contexts: Implementation of the resampling procedure. If [ N eff &lt; N thres , it is necessary to implement the algorithm to sample N random variates according to a discrete distribution with N elements. A straightforward application of the SIR procedure has a complexity in O (N ln N ) <ref> [23] </ref>. This complexity is very important and, so as to reduce it, Beadle et al. [10] have recently proposed several ad hoc methods. <p> Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith <ref> [9, 10, 12, 23, 31] </ref>, simultaneously developed by Kitagawa [29, 34, 35, 36], applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of <p> To limit the loss of "diversity", many ad hoc procedures have been proposed. In <ref> [23] </ref>, the trajectories are artificially perturbed after the resampling step. Another simple solution consists of building a semi-parametric ap proximation of b P (dx) = P N before resampling [18, 24] but the choice of a "good" kernel K () is difficult. <p> The SIS algorithms have similar performances to the bootstrap filter for a smaller computational cost. The most interesting algorithm is based on the optimal importance function which limits seriously the number of resampling steps. 6.2. Nonlinear series. We consider here the following nonlinear reference model <ref> [7, 23, 35, 56] </ref>: = 2 x k1 2 + 8 cos (1:2k) + v k = 2 + w k On Sequential Simulation-Based Methods for Bayesian Filtering 22 where x 0 ~ N (0; 5), v k and w k are mutually independent white Gaussian noises, v k ~ N
Reference: [24] <author> N. Gordon, </author> <title> "A Hybrid Bootstrap Filter for Target Tracking in Clutter", </title> <journal> IEEE Trans. Aero. Elec. Sys., </journal> <volume> vol. 33, no. 1, </volume> <year> 1997, </year> <pages> pp. 353-358. </pages>
Reference-contexts: To limit the loss of "diversity", many ad hoc procedures have been proposed. In [23], the trajectories are artificially perturbed after the resampling step. Another simple solution consists of building a semi-parametric ap proximation of b P (dx) = P N before resampling <ref> [18, 24] </ref> but the choice of a "good" kernel K () is difficult. Higuchi [27, 28] proposes various heuristic procedures taken from the genetic algorithms literature to introduce such a diversity among samples.
Reference: [25] <author> J.E. Handschin and D.Q. Mayne, </author> <title> "Monte Carlo Techniques to Estimate the Conditional Expectation in Multi-stage Non-linear Filtering", </title> <journal> Int. J. Cont., </journal> <volume> vol. 9, no. 5, </volume> <year> 1969, </year> <pages> pp. 547-559. </pages>
Reference-contexts: Other interesting work in automatic control was done during the 60's and 70's based on sequential Monte Carlo integration methods, see <ref> [1, 2, 4, 25, 26, 47, 61] </ref>. Most likely because of the primitive computers available at the time, these last algorithms were overlooked and forgotten 2 . In the late 80's, the great increase of computational power allowed the rebirth of numerical integration methods for Bayesian filtering [33]. <p> A particular case of this algorithm was introduced in 1969 by Hand-schin et Mayne <ref> [25, 26] </ref> ! The numerical complexity of this algorithm is O (N ). This is important as we take N 1 in practice but it has the great advantage of being parallelizable. <p> We now present two simpler methods. Prior importance function. A simple choice consists of selecting as importance function the prior distribution of the hidden Markov model. This is the choice made by Handschin et Mayne <ref> [25, 26] </ref> in their seminal work. This distribution has been recently adopted by Tanizaki et al. [56, 57]. <p> Rao-Blackwellisation for Sequential Importance Sampling We propose here to improve SIS using variance reduction methods designed to make the most of the model studied. Numerous methods have been developed so as to reduce the variance of MC estimates including antithetic sampling <ref> [25, 26] </ref> and control variates [2, 26]. We apply here the Rao-Blackwellisation method [4].
Reference: [26] <author> J.E. Handschin, </author> <title> "Monte Carlo Techniques for Prediction and Filtering of Non-Linear Stochastic Processes", </title> <journal> Automatica, </journal> <volume> vol. 6, </volume> <year> 1970, </year> <pages> pp. 555-563. </pages>
Reference-contexts: Other interesting work in automatic control was done during the 60's and 70's based on sequential Monte Carlo integration methods, see <ref> [1, 2, 4, 25, 26, 47, 61] </ref>. Most likely because of the primitive computers available at the time, these last algorithms were overlooked and forgotten 2 . In the late 80's, the great increase of computational power allowed the rebirth of numerical integration methods for Bayesian filtering [33]. <p> A particular case of this algorithm was introduced in 1969 by Hand-schin et Mayne <ref> [25, 26] </ref> ! The numerical complexity of this algorithm is O (N ). This is important as we take N 1 in practice but it has the great advantage of being parallelizable. <p> We now present two simpler methods. Prior importance function. A simple choice consists of selecting as importance function the prior distribution of the hidden Markov model. This is the choice made by Handschin et Mayne <ref> [25, 26] </ref> in their seminal work. This distribution has been recently adopted by Tanizaki et al. [56, 57]. <p> Rao-Blackwellisation for Sequential Importance Sampling We propose here to improve SIS using variance reduction methods designed to make the most of the model studied. Numerous methods have been developed so as to reduce the variance of MC estimates including antithetic sampling <ref> [25, 26] </ref> and control variates [2, 26]. We apply here the Rao-Blackwellisation method [4]. <p> Rao-Blackwellisation for Sequential Importance Sampling We propose here to improve SIS using variance reduction methods designed to make the most of the model studied. Numerous methods have been developed so as to reduce the variance of MC estimates including antithetic sampling [25, 26] and control variates <ref> [2, 26] </ref>. We apply here the Rao-Blackwellisation method [4]. We show how it is possible to apply this method successfully to an important class of HMM and obtain hybrid filters where a part of the calculations is realized analytically and the other part using MC methods.
Reference: [27] <author> T. Higuchi, </author> <title> "Kitagawa Monte Carlo Filter from the Perspective of Genetic Algorithm", </title> <institution> Research Memorandum, The Institute of Statistical Mathematics, </institution> <address> Tokyo, Japan, </address> <year> 1995. </year>
Reference-contexts: In [23], the trajectories are artificially perturbed after the resampling step. Another simple solution consists of building a semi-parametric ap proximation of b P (dx) = P N before resampling [18, 24] but the choice of a "good" kernel K () is difficult. Higuchi <ref> [27, 28] </ref> proposes various heuristic procedures taken from the genetic algorithms literature to introduce such a diversity among samples. One can notice that, in fact, the SIR procedure has a similar mathematical structure to the selection step of genetic algorithms.
Reference: [28] <author> T. Higuchi, </author> <title> "Kitagawa Monte Carlo Filter using the Genetic Algorithm Operators", </title> <institution> Research Memorandum, The Institute of Statistical Mathematics, </institution> <address> Tokyo, Japan, </address> <year> 1995. </year>
Reference-contexts: In [23], the trajectories are artificially perturbed after the resampling step. Another simple solution consists of building a semi-parametric ap proximation of b P (dx) = P N before resampling [18, 24] but the choice of a "good" kernel K () is difficult. Higuchi <ref> [27, 28] </ref> proposes various heuristic procedures taken from the genetic algorithms literature to introduce such a diversity among samples. One can notice that, in fact, the SIR procedure has a similar mathematical structure to the selection step of genetic algorithms.
Reference: [29] <author> T. Higuchi, </author> <title> "Bayesian Model for Seasonal Small Count Time Series and Monte Carlo Filter Approach", </title> <type> technical report, </type> <month> January </month> <year> 1997. </year> <title> On Sequential Simulation-Based Methods for Bayesian Filtering 25 </title>
Reference-contexts: Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith [9, 10, 12, 23, 31], simultaneously developed by Kitagawa <ref> [29, 34, 35, 36] </ref>, applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of Bayesian networks.
Reference: [30] <author> M. Irwin, N. Cox and A. Kong, </author> <title> "Sequential Imputation for Multilocus Linkage Analysis", </title> <journal> Proc. Nat. Acad. Sci. USA, </journal> <volume> vol. 91, </volume> <year> 1994, </year> <pages> pp. 11684-11688. </pages>
Reference-contexts: First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case [4]. More recently, this importance function has been used in <ref> [14, 15, 16, 30, 37, 38, 40] </ref>.
Reference: [31] <author> M. Isard and A. Blake, </author> <title> "Contour Tracking by Stochastic Propagation of the Conditional Density", </title> <booktitle> in Proc. Europ. Conf. Comp. Vision, </booktitle> <address> Cambridge, </address> <year> 1996, </year> <pages> pp. 343-356. </pages>
Reference-contexts: Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith <ref> [9, 10, 12, 23, 31] </ref>, simultaneously developed by Kitagawa [29, 34, 35, 36], applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of
Reference: [32] <author> A.H. Jazwinski, </author> <title> Stochastic Processes and Filtering Theory, </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: From the mid 60's, a huge number of papers and books have been devoted to obtaining approximations of these distributions, see <ref> [32] </ref> for example. The most popular algorithms, the extended Kalman filter and the Gaussian sum filter, rely on analytical approximations [5, 6] but early well-known work relying on deterministic numerical integration methods was also performed by Bucy and co-workers, see [13] for example.
Reference: [33] <author> G. Kitagawa, </author> <title> "Non-Gaussian State-Space Modeling of Nonstationary Time Series", </title> <journal> J. Am. Stat. Assoc., </journal> <volume> vol. 82, no. 400, </volume> <year> 1987, </year> <pages> pp. 1032-1063. </pages>
Reference-contexts: Most likely because of the primitive computers available at the time, these last algorithms were overlooked and forgotten 2 . In the late 80's, the great increase of computational power allowed the rebirth of numerical integration methods for Bayesian filtering <ref> [33] </ref>. Current research has now focused on MC (Monte Carlo) integration methods which have the great advantage of not being subject to any linearity or Gaussianity hypotheses on the model. <p> 0:k ) = (x k ) (67) w k = w k1 p y k j x k p x k fi (i) (i) This is the choice adopted by Tanizaki et al. [54, 55] who presents this method as a stochastic alternative to the numerical integration method of Kitagawa <ref> [33] </ref>. The results obtained are rather poor as neither the dynamic of the model nor the observations are taken into account. It leads in most cases to unbounded importance weights. 3. Resampling As it has been previously illustrated, the degeneracy of the algorithm based on SIS can not be avoided. <p> This problem is even more severe for the bootstrap filter where one resamples at each time instant. It is necessary to develop an alternative algorithm. We propose an original algorithm to solve this problem. This algorithm is based on the following formula <ref> [8, 33] </ref>: p ( x k j y 0:n ) = p ( x k j y 0:k ) p ( x k+1 j y 0:n ) p ( x k+1 j x k ) dx k+1 (98) We seek here an approximation of the fixed-interval smoothing distribution with the following <p> However this complexity is a little lower than the one of the previous developed algorithms of Kitagawa [35, 36] and Tanizaki [56, 57] as it does not require any new simulation step. 5.4. Likelihood. In some applications, in particular for model choice <ref> [33, 36] </ref>, we may wish to estimate the likelihood of the data p (y 0:n ). A simple estimate of the likelihood is given, using to (15) and (18), by bp (y 0:n ) = N j=1 n (104) In practice, the introduction of resampling steps makes this approach impossible.
Reference: [34] <author> G. Kitagawa, </author> <title> "A Monte Carlo Filtering and Smoothing Method for Non-Gaussian Nonlinear State Space Models", </title> <booktitle> in Proc. 2 nd US-Japan Joint Seminar on Statistical Time Series Analysis, Honolulu, Hawaii, </booktitle> <pages> pp. 110-131, </pages> <year> 1993. </year>
Reference-contexts: Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith [9, 10, 12, 23, 31], simultaneously developed by Kitagawa <ref> [29, 34, 35, 36] </ref>, applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of Bayesian networks.
Reference: [35] <author> G. Kitagawa, </author> <title> "Monte Carlo Filter and Smoother for Non-Gaussian Nonlinear State Space Models", </title> <journal> J. Comp. Graph. Stat., </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 1-25, </pages> <year> 1996. </year>
Reference-contexts: Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith [9, 10, 12, 23, 31], simultaneously developed by Kitagawa <ref> [29, 34, 35, 36] </ref>, applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of Bayesian networks. <p> The memory requirement is O (nN ). Its complexity is O nN 2 , which is quite important as N 1. However this complexity is a little lower than the one of the previous developed algorithms of Kitagawa <ref> [35, 36] </ref> and Tanizaki [56, 57] as it does not require any new simulation step. 5.4. Likelihood. In some applications, in particular for model choice [33, 36], we may wish to estimate the likelihood of the data p (y 0:n ). <p> The SIS algorithms have similar performances to the bootstrap filter for a smaller computational cost. The most interesting algorithm is based on the optimal importance function which limits seriously the number of resampling steps. 6.2. Nonlinear series. We consider here the following nonlinear reference model <ref> [7, 23, 35, 56] </ref>: = 2 x k1 2 + 8 cos (1:2k) + v k = 2 + w k On Sequential Simulation-Based Methods for Bayesian Filtering 22 where x 0 ~ N (0; 5), v k and w k are mutually independent white Gaussian noises, v k ~ N
Reference: [36] <author> G. Kitagawa and W. Gersch, </author> <title> Smoothness Priors Analysis of Time Series, </title> <booktitle> Lecture Notes in Statistics, </booktitle> <volume> vol. 116, </volume> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: Despite its drawbacks, the SIR algorithm is the basis of numerous works. The pop ular bootstrap filter of Gordon, Salmond et Smith [9, 10, 12, 23, 31], simultaneously developed by Kitagawa <ref> [29, 34, 35, 36] </ref>, applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also [52] for a similar method developed in the closely related field of Bayesian networks. <p> The memory requirement is O (nN ). Its complexity is O nN 2 , which is quite important as N 1. However this complexity is a little lower than the one of the previous developed algorithms of Kitagawa <ref> [35, 36] </ref> and Tanizaki [56, 57] as it does not require any new simulation step. 5.4. Likelihood. In some applications, in particular for model choice [33, 36], we may wish to estimate the likelihood of the data p (y 0:n ). <p> However this complexity is a little lower than the one of the previous developed algorithms of Kitagawa [35, 36] and Tanizaki [56, 57] as it does not require any new simulation step. 5.4. Likelihood. In some applications, in particular for model choice <ref> [33, 36] </ref>, we may wish to estimate the likelihood of the data p (y 0:n ). A simple estimate of the likelihood is given, using to (15) and (18), by bp (y 0:n ) = N j=1 n (104) In practice, the introduction of resampling steps makes this approach impossible.
Reference: [37] <author> A. Kong, J.S. Liu and W.H. Wong, </author> <title> "Sequential Imputations and Bayesian Missing Data Problems", </title> <journal> J. Am. Stat. Assoc., </journal> <volume> vol. 89, no. 425, </volume> <year> 1994, </year> <pages> pp. 278-288. </pages>
Reference-contexts: Proposition 3. The unconditional variance of the importance weights, i.e. with the observations y 0:k being interpreted as random variables, increases over time. The proof of this proposition is a straightforward extension of a Kong-Liu-Wong <ref> [37, p. 285] </ref> theorem to the case of an importance function of the form (29). Thus, it is impossible to avoid a degeneracy phenomenon. <p> First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case [4]. More recently, this importance function has been used in <ref> [14, 15, 16, 30, 37, 38, 40] </ref>. <p> The basic idea of resampling methods consists of eliminating the trajectories which have weak normalised importance weights and to multiply trajectories with strong importance weights. We adopt as a measure of degeneracy of the algorithm the effective sample size. This criterion, introduced by Liu <ref> [37, 39] </ref>, is defined using the variances of the estimates of I (f k ) respectively obtained using (imaginary) i.i.d. samples according to ( x 0:k j y 0:k ) and an importance sampling method based on i.i.d. samples distributed according to p ( x 0:k j y 0:k ).
Reference: [38] <author> J.S. Liu and R. Chen, </author> <title> "Blind Deconvolution via Sequential Imputation", </title> <journal> J. Am. Stat. Assoc., </journal> <volume> vol. 90, no. 430, </volume> <year> 1995, </year> <pages> pp. 567-576. </pages>
Reference-contexts: First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case [4]. More recently, this importance function has been used in <ref> [14, 15, 16, 30, 37, 38, 40] </ref>. <p> Indeed, it is possible to sample from this discrete distribution and to evaluate the importance weight p 0:k1 using the Kalman filter [4]. Similar developments have been proposed by Svetnik et al. [53]. The algorithm for blind deconvolution recently proposed by Liu et al. <ref> [38] </ref> is also a particular case of this method where x 2 k = h is a time-invariant channel of Gaussian prior distribution 5 .
Reference: [39] <author> J.S. Liu, </author> <title> "Metropolized Independent Sampling with Comparison to Rejection Sampling and Importance Sampling", </title> <journal> Stat. Comp., </journal> <volume> vol. 6, </volume> <year> 1996, </year> <pages> pp. 113-119. </pages>
Reference-contexts: The basic idea of resampling methods consists of eliminating the trajectories which have weak normalised importance weights and to multiply trajectories with strong importance weights. We adopt as a measure of degeneracy of the algorithm the effective sample size. This criterion, introduced by Liu <ref> [37, 39] </ref>, is defined using the variances of the estimates of I (f k ) respectively obtained using (imaginary) i.i.d. samples according to ( x 0:k j y 0:k ) and an importance sampling method based on i.i.d. samples distributed according to p ( x 0:k j y 0:k ).
Reference: [40] <author> J.S. Liu, </author> <title> "Nonparametric Hierarchical Bayes via Sequential Imputations", </title> <journal> Ann. Stat., </journal> <year> 1996. </year>
Reference-contexts: First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. [61] then by Akashi et al. for a particular case [4]. More recently, this importance function has been used in <ref> [14, 15, 16, 30, 37, 38, 40] </ref>.
Reference: [41] <author> J.S. Liu and R. Chen, </author> <title> "Monte Carlo Methods for Dynamic Systems", </title> <type> technical report, </type> <institution> Department of Statistics, Stanford University, </institution> <year> 1997. </year>
Reference-contexts: The main objective of this report is to include in a unified framework many old and recent algorithms developed independently in various fields of applied science. Some original developments are also presented. The closest work to this report is the work of Liu and Chen <ref> [41] </ref>, developed independently, which underlines similarly the central role of sequential importance sampling (SIS) in sequential simulation-based methods for Bayesian filtering. 1 This technical report is a translation of chapter 3 of [19] in abbreviated form. 2 To the best of my knowledge, these important works are cited neither in any <p> Moreover, this solution, although simple, is computationally very expensive. Other MC methods based on MCMC methods have been proposed to simulate approximately from p (i) evaluate p (i) , see <ref> [12, 18, 41] </ref>. These iterative algorithms appear to be of limited interest in an on-line framework and there is a lack of theoretical convergence results. In fact, the general framework of SIS allows us to consider other importance functions built so as to approximate analytically the optimal importance function. <p> If [ N eff &lt; N thres the SIR algorithm is applied and we obtain the following approximation of the joint distribution: b P ( dx 0:k j y 0:k ) = N i=1 0:k Remark 5. In <ref> [41] </ref>, other more interesting resampling schemes are presented which re duce the MC variation of the SIR. 3.2. Implementation of the resampling procedure.
Reference: [42] <author> R.S. Mariano and H. Tanizaki, </author> <title> "Simulation-Based Inference in Nonlinear State-Space Models: Application to Testing the Permanent Income Hypothesis", in Simulation-Based Inference in Econometrics: Methods and Applications (R.S. Mariano, </title> <editor> M. Weeks and T. Schuermann, Eds.), </editor> <publisher> Cambridge University Press, </publisher> <year> 1996. </year>
Reference: [43] <author> P. Muller, </author> <title> "Monte Carlo Integration in General Dynamic Models", </title> <journal> Contemporary Math., </journal> <volume> vol. 115, </volume> <year> 1991, </year> <pages> pp. 145-163. </pages>
Reference: [44] <author> P. Muller, </author> <title> "Posterior Integration in Dynamic Models", </title> <journal> Comp. Science Stat., </journal> <volume> vol. 24, </volume> <year> 1992, </year> <pages> pp. 318-324. </pages>
Reference: [45] <author> B.D. Ripley, </author> <title> Stochastic Simulation, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: fact, it is possible to implement exactly the SIR procedure in O (N ) operations by noticing that it is possible to sample in O (N ) operations N i.i.d. variables distributed according to U [0;1] and ordered, i.e. u 1 u 2 u N , using a classical algorithm <ref> [45, pp. 96] </ref>. Algorithm ([45, pp. 96]) * For i = 1; :::; N , sample eu i ~ U [0;1] .
Reference: [46] <author> D.B. Rubin, </author> <title> "Using the SIR Algorithm to Simulate Posterior Distributions", in Bayesian Statistics 3 (Eds J.M. </title> <editor> Bernardo, M.H. DeGroot, D.V. Lindley et A.F.M. Smith), </editor> <publisher> Oxford University Press, </publisher> <pages> pp. 395-402, </pages> <year> 1988. </year>
Reference-contexts: The most popular resampling scheme is the SIR algorithm (Sampling Importance Resampling) in troduced by Rubin <ref> [46, 50] </ref>. This scheme is based on two steps: a first step is an IS step, the second step is a sampling step based on the obtained discrete distribution. 3.1. SIS/Resampling Monte Carlo filter.
Reference: [47] <author> E.I. Shapiro, </author> <title> "The Random Distribution Method and its Applications to the Solution of the Problem of Nonlinear Filtering in Discrete Time", </title> <journal> Radio Eng. Elec. Phys., </journal> <volume> vol. 26, no. 6, </volume> <year> 1981, </year> <pages> pp. 48-54. </pages>
Reference-contexts: Other interesting work in automatic control was done during the 60's and 70's based on sequential Monte Carlo integration methods, see <ref> [1, 2, 4, 25, 26, 47, 61] </ref>. Most likely because of the primitive computers available at the time, these last algorithms were overlooked and forgotten 2 . In the late 80's, the great increase of computational power allowed the rebirth of numerical integration methods for Bayesian filtering [33].
Reference: [48] <editor> N. Shephard and M.K. Pitt, </editor> <title> "Likelihood Analysis of Non-Gaussian Measurement Time Series", </title> <journal> Biometrika, </journal> <volume> forthcoming, </volume> <year> 1997. </year> <title> On Sequential Simulation-Based Methods for Bayesian Filtering 26 </title>
Reference-contexts: This last method is close to the one developed independently by Shephard and Pitt <ref> [48] </ref> in a different framework. They propose a MCMC algorithm for off-line estimation of non-Gaussian measurements time series based on the Metropolis-Hastings algorithms. The proposal distribution of this algorithm is build in the case where l (x k ) is concave using a similar method 3 .
Reference: [49] <editor> N. Shephard and M.K. Pitt, </editor> <title> "Filtering via Simulation: Auxiliary Particle Filters", </title> <type> technical report, </type> <institution> Department of Statistics, Imperial College, </institution> <address> London, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: On Sequential Simulation-Based Methods for Bayesian Filtering 14 * u N = [eu N ] . 1=i We deduce straightforwardly the algorithm to sample N i.i.d. samples according to the discrete distribution in O (N ) operations. Remark 6. This algorithm is also presented in <ref> [49] </ref> which attributed the idea of using this algorithm to Carpenter, Clifford and Fearnhead. 3.3. Limitations of the resampling scheme. The resampling procedure decreases algorithmically the degeneracy problem but introduces practical and theoretical problems. <p> One can notice that, in fact, the SIR procedure has a similar mathematical structure to the selection step of genetic algorithms. Interesting extensions of the SIR algorithm have been recently developed by Shephard and Pitt <ref> [49] </ref>. 4. Rao-Blackwellisation for Sequential Importance Sampling We propose here to improve SIS using variance reduction methods designed to make the most of the model studied. Numerous methods have been developed so as to reduce the variance of MC estimates including antithetic sampling [25, 26] and control variates [2, 26].
Reference: [50] <author> A.F.M. Smith and A.E. Gelfand, </author> <title> "Bayesian Statistics without Tears: a Sampling-Resampling Perspective", </title> <journal> Am. Stat., </journal> <volume> vol. 46, no. 2, </volume> <year> 1992, </year> <pages> pp. 84-88. </pages>
Reference-contexts: The most popular resampling scheme is the SIR algorithm (Sampling Importance Resampling) in troduced by Rubin <ref> [46, 50] </ref>. This scheme is based on two steps: a first step is an IS step, the second step is a sampling step based on the obtained discrete distribution. 3.1. SIS/Resampling Monte Carlo filter.
Reference: [51] <author> L. Stewart, </author> <title> "Bayesian Analysis using Monte Carlo Integration a Powerful Methodology for Handling some Difficult Problems", </title> <journal> The Stat., </journal> <volume> vol. 32, </volume> <year> 1983, </year> <pages> pp. 195-200. </pages>
Reference-contexts: to) and the normalised importance weights are equal to ew (i) w n j=1 w n The "true" importance weights w fl (i) n have been replaced by the following estimate: bw fl (i) n (22) This method is well-known in the statistical literature as Bayesian IS, see for example <ref> [22, 51] </ref>. We recall here some classical results on this MC method. Assumption 1 - x 0:n ; i = 1; :::; N is a set of i.i.d. vectors distributed according to ( x 0:n j y 0:n ).
Reference: [52] <author> L. Stewart and P. McCarty, </author> <title> "The Use of Bayesian Belief Networks to Fuse Continuous and Discrete Information for Target Recognition, Tracking and Situation Assessment", </title> <booktitle> in Proc. SPIE, </booktitle> <volume> vol. 1699, </volume> <year> 1992, </year> <pages> pp. 177-185. </pages>
Reference-contexts: The pop ular bootstrap filter of Gordon, Salmond et Smith [9, 10, 12, 23, 31], simultaneously developed by Kitagawa [29, 34, 35, 36], applies at each iteration a resampling step using x k j x k1 ; y k = p x k j x k1 , see also <ref> [52] </ref> for a similar method developed in the closely related field of Bayesian networks. To limit the loss of "diversity", many ad hoc procedures have been proposed. In [23], the trajectories are artificially perturbed after the resampling step.
Reference: [53] <author> V.B. Svetnik, </author> <title> "Applying the Monte Carlo Method for Optimum Estimation in Systems with Random Disturbances", </title> <journal> Auto. Remo. Cont., </journal> <volume> vol. 47, no. 6, </volume> <year> 1986, </year> <pages> pp. 818-825. </pages>
Reference-contexts: Indeed, it is possible to sample from this discrete distribution and to evaluate the importance weight p 0:k1 using the Kalman filter [4]. Similar developments have been proposed by Svetnik et al. <ref> [53] </ref>. The algorithm for blind deconvolution recently proposed by Liu et al. [38] is also a particular case of this method where x 2 k = h is a time-invariant channel of Gaussian prior distribution 5 .
Reference: [54] <author> H. Tanizaki, </author> <title> Nonlinear Filters: Estimation and Applications, </title> <booktitle> Lecture Notes in Economics and Mathematical Systems, </booktitle> <volume> no. 400, </volume> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: In this case, we have ( x k j x 0:k1 ; y 0:k ) = (x k ) (67) w k = w k1 p y k j x k p x k fi (i) (i) This is the choice adopted by Tanizaki et al. <ref> [54, 55] </ref> who presents this method as a stochastic alternative to the numerical integration method of Kitagawa [33]. The results obtained are rather poor as neither the dynamic of the model nor the observations are taken into account. It leads in most cases to unbounded importance weights. 3.
Reference: [55] <author> H. Tanizaki and R.S. Mariano, </author> <title> "Prediction, Filtering and Smoothing in Non-linear and Non-normal Cases using Monte Carlo Integration", </title> <journal> J. App. Econometrics, </journal> <volume> vol. 9, </volume> <year> 1994, </year> <pages> pp. 163-179. </pages>
Reference-contexts: In this case, we have ( x k j x 0:k1 ; y 0:k ) = (x k ) (67) w k = w k1 p y k j x k p x k fi (i) (i) This is the choice adopted by Tanizaki et al. <ref> [54, 55] </ref> who presents this method as a stochastic alternative to the numerical integration method of Kitagawa [33]. The results obtained are rather poor as neither the dynamic of the model nor the observations are taken into account. It leads in most cases to unbounded importance weights. 3.
Reference: [56] <author> H. Tanizaki and R.S. Mariano, </author> <title> "Nonlinear and Nonnormal State-Space Modeling with Monte-Carlo Stochastic Simulations", </title> <journal> forthcoming J. </journal> <volume> Econometrics, </volume> <year> 1997. </year>
Reference-contexts: Prior importance function. A simple choice consists of selecting as importance function the prior distribution of the hidden Markov model. This is the choice made by Handschin et Mayne [25, 26] in their seminal work. This distribution has been recently adopted by Tanizaki et al. <ref> [56, 57] </ref>. <p> The memory requirement is O (nN ). Its complexity is O nN 2 , which is quite important as N 1. However this complexity is a little lower than the one of the previous developed algorithms of Kitagawa [35, 36] and Tanizaki <ref> [56, 57] </ref> as it does not require any new simulation step. 5.4. Likelihood. In some applications, in particular for model choice [33, 36], we may wish to estimate the likelihood of the data p (y 0:n ). <p> The SIS algorithms have similar performances to the bootstrap filter for a smaller computational cost. The most interesting algorithm is based on the optimal importance function which limits seriously the number of resampling steps. 6.2. Nonlinear series. We consider here the following nonlinear reference model <ref> [7, 23, 35, 56] </ref>: = 2 x k1 2 + 8 cos (1:2k) + v k = 2 + w k On Sequential Simulation-Based Methods for Bayesian Filtering 22 where x 0 ~ N (0; 5), v k and w k are mutually independent white Gaussian noises, v k ~ N
Reference: [57] <author> H. Tanizaki, </author> <title> "Nonlinear and Nonnormal Filters using Monte-Carlo Methods", </title> <journal> forthcoming Comp. Stat. Data Ana., </journal> <year> 1997. </year>
Reference-contexts: Prior importance function. A simple choice consists of selecting as importance function the prior distribution of the hidden Markov model. This is the choice made by Handschin et Mayne [25, 26] in their seminal work. This distribution has been recently adopted by Tanizaki et al. <ref> [56, 57] </ref>. <p> The memory requirement is O (nN ). Its complexity is O nN 2 , which is quite important as N 1. However this complexity is a little lower than the one of the previous developed algorithms of Kitagawa [35, 36] and Tanizaki <ref> [56, 57] </ref> as it does not require any new simulation step. 5.4. Likelihood. In some applications, in particular for model choice [33, 36], we may wish to estimate the likelihood of the data p (y 0:n ).
Reference: [58] <author> J.K. Tugnait, </author> <title> "Detection and Estimation for Abruptly Changing Systems", </title> <journal> Auto-matica, </journal> <volume> vol. 18, no. 5, </volume> <year> 1982, </year> <pages> pp. 607-615. </pages>
Reference-contexts: It is possible to use a MC filter based on Rao-Blackwellisation. Indeed, conditional upon x 1 0:n is a linear Gaussian state space model and the integrations required by the Rao-Blackwellisation method can be realized using the Kalman filter. Akashi and Kumamoto <ref> [1, 4, 58] </ref> introduced this algorithm under the name of RSA (Random Sampling Algorithm) in the particular case where x 1 k is a homogeneous scalar finite state-space Markov chain 4 . In this case, they adopted the optimal importance function p k fi y 0:k ; x 1 .
Reference: [59] <author> M. West, </author> <title> "Mixtures Models, Monte Carlo, Bayesian Updating and Dynamic Models", </title> <journal> Comp. Science Stat., </journal> <volume> vol. 24, </volume> <year> 1993, </year> <pages> pp. 325-333. </pages>
Reference: [60] <author> M. West and J.F. Harrison, </author> <title> Bayesian Forecasting and Dynamic Models, </title> <publisher> Springer Verlag Series in Statistics, </publisher> <address> 2 nd edition, </address> <year> 1997. </year>
Reference-contexts: There is no general method to build suboptimal importance functions and it is necessary to build these on a case by case basis, dependent on the model studied. To this end, it is possible to base these developments on previous work on standard suboptimal filtering methods <ref> [6, 60] </ref>. Importance distribution obtained by local linearisation. <p> These models have numerous applications and allow consideration of Poisson or binomial observations, see for example <ref> [60] </ref>.
Reference: [61] <author> V.S. Zaritskii, V.B. Svetnik and L.I. Shimelevich, </author> <title> "Monte Carlo Technique in Problems of Optimal Data Processing", </title> <journal> Auto. Remo. Cont., </journal> <volume> vol. 12, </volume> <year> 1975, </year> <pages> pp. 95-103. </pages>
Reference-contexts: Other interesting work in automatic control was done during the 60's and 70's based on sequential Monte Carlo integration methods, see <ref> [1, 2, 4, 25, 26, 47, 61] </ref>. Most likely because of the primitive computers available at the time, these last algorithms were overlooked and forgotten 2 . In the late 80's, the great increase of computational power allowed the rebirth of numerical integration methods for Bayesian filtering [33]. <p> First we present how to implement the optimal importance function p (i) Optimal importance function. The optimal importance function p (i) has been introduced by Zaritskii et al. <ref> [61] </ref> then by Akashi et al. for a particular case [4]. More recently, this importance function has been used in [14, 15, 16, 30, 37, 38, 40].
References-found: 61

