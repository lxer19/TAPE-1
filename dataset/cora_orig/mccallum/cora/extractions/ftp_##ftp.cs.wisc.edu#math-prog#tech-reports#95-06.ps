URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-06.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: Mathematical Programming in Machine Learning  
Author: O. L. Mangasarian 
Abstract: Mathematical Programming Technical Report 95-06 April 1995-Revised July 1995 Abstract We describe in this work a number of central problems of machine learning and show how they can be modeled and solved as mathematical programs of various complexity.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: Although the linear programming formulation (3) is very effective for practical problems [21] and can be used in the construction of neural networks [3] as well multi-surface discriminators <ref> [1, 4] </ref>, it does not minimize the number of misclassified points by the plane (1), which may be an important consideration in certain applications. In order to minimize the number of misclassified points, we need to maximize the number of satisfied components of the inequalities (2). <p> However, various greedy sequential constructions of the planes determining the various polyhedral regions <ref> [16, 19, 1] </ref> have been quite successful in obtaining very effective algorithms for training neural networks. These algorithms are much faster than the classical online backpropagation (BP) gradient algorithm [23, 11, 20], where the training is done on one point at a time. <p> It is interesting to note that the same solution for the XOR example is given by the greedy multisurface method tree (MSMT) <ref> [1] </ref>. MSMT attempts to separate as many points of A and B as possible by a first plane obtained by solving (3), and then repeats the process for each of the ensuing halfspaces, until adequate separation is obtained.
Reference: [2] <author> K. P. Bennett and E. J. Bredensteiner. </author> <title> A parametric optimization method for machine learning. </title> <note> Department of Mathematical Sciences Report No. 217, </note> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1994. </year>
Reference-contexts: When the traditional distance of a misclassified point to a separating plane is used as an error, a single linear program [6, 15, 16, 4] usually solves the problem. Recently <ref> [10, 18, 2, 7] </ref> a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane. This problem, even though shown to be NP-complete [7], can be effectively solved by a parametric [2] or a hybrid method [7]. <p> Recently [10, 18, 2, 7] a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane. This problem, even though shown to be NP-complete [7], can be effectively solved by a parametric <ref> [2] </ref> or a hybrid method [7]. In Section 3 we describe a central problem of machine learning, that of improving generalization [27]. We give a very simple model which justifies the often accepted rule-of-thumb of machine learning and approximation theory, that overfitting leads to poor generalization. <p> Although this is an NP-complete problem [7, Proposition 2], effective methods for its solution have been proposed [18, 7] and successfully tested on real world problems <ref> [2, 7] </ref>. We outline two of these approaches briefly now. We first describe a hybrid approach, recently proposed and tested successfully on ten publicly available databases [7]. The idea of this approach is to combine the two criteria described above as follows. <p> This is so because a slight perturbation of the plane does not change the number of misclassified points. To overcome this difficulty a parametric reformulation was proposed in [18] and implemented in <ref> [2] </ref>. <p> Efficient estimation of successive values of can be achieved by a secant method applied to f (). The method seems to work quite well as evidenced by computational results given in <ref> [2, 7] </ref>. 4 3 Improving Generalization In this section we shall consider a fundamental problem of machine learning: How to train a system on a given training set so as to improve generalization on a new unseen testing set [13, 24, 28].
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: Although the linear programming formulation (3) is very effective for practical problems [21] and can be used in the construction of neural networks <ref> [3] </ref> as well multi-surface discriminators [1, 4], it does not minimize the number of misclassified points by the plane (1), which may be an important consideration in certain applications.
Reference: [4] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: When the traditional distance of a misclassified point to a separating plane is used as an error, a single linear program <ref> [6, 15, 16, 4] </ref> usually solves the problem. Recently [10, 18, 2, 7] a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane. <p> Thus, one resorts in the general case to optimizing some error criterion in the satisfaction of (2). The simplest such criterion is to use linear programming in order to construct a plane (1) that maximizes a weighted sum of the distances of correctly classified points to the plane <ref> [16, 4] </ref> as follows: max Here the rows of the matrices A 2 R mfin and B 2 R kfin represent the m points in A and the k points in B respectively, while e is a vector of ones of appropriate dimension. <p> The objective function of (3) represents the sum of distances, positive and truncated to one for correctly classified points and nonpositive for incorrectly classified points, to the plane x T w = , multiplied by kwk 2 . Although, apparently different from the robust linear program of <ref> [4, Proposition 2.4] </ref>, it is equivalent to it if we set the weights ffi 1 = ffi 2 = 1 in the latter and make a simple change of variables. <p> The linear program (3) also maintains the non-nullity properties of w <ref> [4, Theorems 2.5 & 2.6] </ref>, which can be summarized as follows here. <p> Although the linear programming formulation (3) is very effective for practical problems [21] and can be used in the construction of neural networks [3] as well multi-surface discriminators <ref> [1, 4] </ref>, it does not minimize the number of misclassified points by the plane (1), which may be an important consideration in certain applications. In order to minimize the number of misclassified points, we need to maximize the number of satisfied components of the inequalities (2). <p> MSMT attempts to separate as many points of A and B as possible by a first plane obtained by solving (3), and then repeats the process for each of the ensuing halfspaces, until adequate separation is obtained. For this example, the first plane obtained <ref> [4] </ref> is (w 1 ; 1 ) = ((2 2); 1), which separates f (1; 0)g from f (0; 0); (0; 1); (1; 1)g.
Reference: [5] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization & Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: Note that f () is a nondecreasing function of , and the largest value for which f () = 0, constitutes a maximum to the NP-complete classification maximization problem (7). The parametric approach consists of starting at some large &gt; , solving (13) by a Frank-Wolfe algorithm <ref> [8, 5] </ref>, for decreasing values of until is reached. Efficient estimation of successive values of can be achieved by a secant method applied to f (). <p> Note that the nonconvex problem (37) has a bilinear objective and two sets of bilinear constraints. Although no computation has been done with this model of a neural network, it is felt that the Frank-Wolfe approach utilized to solve efficiently numerous NP-complete problems in <ref> [5] </ref> could also be effective here as well. Briefly the approach would consist of fixing t i ; i = 1; : : : ; h and solving (37) by the bilinear approach of [5] which involves successive linear programs and line searches. <p> network, it is felt that the Frank-Wolfe approach utilized to solve efficiently numerous NP-complete problems in <ref> [5] </ref> could also be effective here as well. Briefly the approach would consist of fixing t i ; i = 1; : : : ; h and solving (37) by the bilinear approach of [5] which involves successive linear programs and line searches. Then (t i ; y i ; z i ) i = 1; : : : ; h and t are updated by solving a single linear program.
Reference: [6] <author> A. Charnes. </author> <title> Some fundamental theorems of perceptron theory and their geometry. </title> <editor> In J. T. Lou and R. H. Wilcox, editors, </editor> <booktitle> Computer and Information Sciences, </booktitle> <pages> pages 67-74, </pages> <address> Washington, D.C., 1964. </address> <publisher> Spartan Books. </publisher>
Reference-contexts: When the traditional distance of a misclassified point to a separating plane is used as an error, a single linear program <ref> [6, 15, 16, 4] </ref> usually solves the problem. Recently [10, 18, 2, 7] a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane.
Reference: [7] <author> Chunhui Chen and O. L. Mangasarian. </author> <title> Hybrid misclassification minimization. </title> <type> Technical Report 95-05, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wiscon-sin, </institution> <month> February </month> <year> 1995. </year> <note> Advances in Computational Mathematics, submitted. Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-05.ps.Z. </note>
Reference-contexts: When the traditional distance of a misclassified point to a separating plane is used as an error, a single linear program [6, 15, 16, 4] usually solves the problem. Recently <ref> [10, 18, 2, 7] </ref> a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane. This problem, even though shown to be NP-complete [7], can be effectively solved by a parametric [2] or a hybrid method [7]. <p> Recently [10, 18, 2, 7] a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane. This problem, even though shown to be NP-complete <ref> [7] </ref>, can be effectively solved by a parametric [2] or a hybrid method [7]. In Section 3 we describe a central problem of machine learning, that of improving generalization [27]. <p> This problem, even though shown to be NP-complete <ref> [7] </ref>, can be effectively solved by a parametric [2] or a hybrid method [7]. In Section 3 we describe a central problem of machine learning, that of improving generalization [27]. We give a very simple model which justifies the often accepted rule-of-thumb of machine learning and approximation theory, that overfitting leads to poor generalization. <p> This corresponds to solving the following problem: max e T (Aw e) fl + e T (Bw + e) fl (7) We refer to this problem as the classification maximization problem, that is the problem of maximizing the number of correctly classified points. Although this is an NP-complete problem <ref> [7, Proposition 2] </ref>, effective methods for its solution have been proposed [18, 7] and successfully tested on real world problems [2, 7]. We outline two of these approaches briefly now. We first describe a hybrid approach, recently proposed and tested successfully on ten publicly available databases [7]. <p> Although this is an NP-complete problem [7, Proposition 2], effective methods for its solution have been proposed <ref> [18, 7] </ref> and successfully tested on real world problems [2, 7]. We outline two of these approaches briefly now. We first describe a hybrid approach, recently proposed and tested successfully on ten publicly available databases [7]. <p> Although this is an NP-complete problem [7, Proposition 2], effective methods for its solution have been proposed [18, 7] and successfully tested on real world problems <ref> [2, 7] </ref>. We outline two of these approaches briefly now. We first describe a hybrid approach, recently proposed and tested successfully on ten publicly available databases [7]. The idea of this approach is to combine the two criteria described above as follows. <p> We outline two of these approaches briefly now. We first describe a hybrid approach, recently proposed and tested successfully on ten publicly available databases <ref> [7] </ref>. The idea of this approach is to combine the two criteria described above as follows. <p> The algorithm stops, when successive line searches in fail to decrease the number of misclassified points. Needless to say, there is no guarantee that this approach will give a global solution to the NP-complete problem (7). However it seems to have the best generalization <ref> [7] </ref> as determined by tenfold cross-validation [26] on the ten data sets employed. We describe now another approach for solving the classification maximization problem (7), by reducing it to an LPEC, a linear program with equilibrium constraints [18]. <p> Efficient estimation of successive values of can be achieved by a secant method applied to f (). The method seems to work quite well as evidenced by computational results given in <ref> [2, 7] </ref>. 4 3 Improving Generalization In this section we shall consider a fundamental problem of machine learning: How to train a system on a given training set so as to improve generalization on a new unseen testing set [13, 24, 28].
Reference: [8] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: Note that f () is a nondecreasing function of , and the largest value for which f () = 0, constitutes a maximum to the NP-complete classification maximization problem (7). The parametric approach consists of starting at some large &gt; , solving (13) by a Frank-Wolfe algorithm <ref> [8, 5] </ref>, for decreasing values of until is reached. Efficient estimation of successive values of can be achieved by a secant method applied to f ().
Reference: [9] <author> G. M. Georgiou. </author> <title> Comments on hidden nodes in neural nets. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 38:1410, </volume> <year> 1991. </year>
Reference-contexts: R n by h separating planes x T w i = i ; i = 1; : : :; h; (27) corresponds to a neural network with h hidden units. (See Figures 1 and 2.) The h separating planes (27) divide R n into at most p polyhedral regions, where <ref> [9] </ref> p := i=0 h We shall assume that A and B are contained in the interiors of two mutually exclusive subsets of these regions. (See Figure 1.) Each of these polyhedral regions can be mapped uniquely into a vertex of the unit cube in R h ; fzjz 2 R
Reference: [10] <author> David Heath. </author> <title> A geometric Framework for Machine Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Johns Hopkins University-Baltimore, Maryland, </institution> <year> 1992. </year>
Reference-contexts: When the traditional distance of a misclassified point to a separating plane is used as an error, a single linear program [6, 15, 16, 4] usually solves the problem. Recently <ref> [10, 18, 2, 7] </ref> a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane. This problem, even though shown to be NP-complete [7], can be effectively solved by a parametric [2] or a hybrid method [7].
Reference: [11] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, California, </address> <year> 1991. </year> <month> 12 </month>
Reference-contexts: defined by r (t ) := Cx (t ) c: (26) Computational results carried out in [27] have corroborated the improved generalization results of Theorem 3.1 above, as well as for more complex models such as neural networks, where a threshold tolerance in measuring the error in the backpropagation algorithm <ref> [23, 11, 20] </ref> is allowed. 6 B B A A x T w 2 = 2 110 010 001 100 and x T w 3 = 3 . <p> However, various greedy sequential constructions of the planes determining the various polyhedral regions [16, 19, 1] have been quite successful in obtaining very effective algorithms for training neural networks. These algorithms are much faster than the classical online backpropagation (BP) gradient algorithm <ref> [23, 11, 20] </ref>, where the training is done on one point at a time. Often online BP is erroneously referred to as a descent algorithm, which it is not.
Reference: [12] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: However, any two disjoint point sets in R n can be discriminated between by some polyhedral partition that corresponds to a neural network with one hidden layer with a sufficient number of hidden units <ref> [12, 19] </ref>. 7 y (x) w 1 w 2 w h 1 2 h threshold units (LTUs), input x 2 R n , and output y (x) 2 f0; 1g: The output of hidden unit i is (x T w i i ) fl ; i = 1; : : :
Reference: [13] <author> Y. le Cun, J. S. Denker, and S. A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <pages> pages 598-605, </pages> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: to work quite well as evidenced by computational results given in [2, 7]. 4 3 Improving Generalization In this section we shall consider a fundamental problem of machine learning: How to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [13, 24, 28] </ref>. We shall concentrate on some very recent results [27] obtained for a simple linear model and which make critical use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model only, seem to extend to much more complex systems, including neural networks [27].
Reference: [14] <author> Z.-Q. Luo, J.-S. Pang, D. Ralph, and S.-Q. Wu. </author> <title> Exact penalization and stationarity conditions of mathematical programs with equilibrium constraints. </title> <type> Technical Report 275, </type> <institution> Communications Research Laboratory, McMaster University, Hamilton, </institution> <address> Ontario, Hamilton, Ontario L8S 4K1, Canada, </address> <year> 1993. </year> <note> Mathematical Programming, to appear. </note>
Reference-contexts: In Section 4 we use an equivalence between the step function and the complementarity problem to show that the problem of training a neural network can be represented as mathematical program with equilibrium constraints (MPEC) which has been studied recently in the literature <ref> [14] </ref>. A word about our notation now.
Reference: [15] <author> O. L. Mangasarian. </author> <title> Linear and nonlinear separation of patterns by linear programming. </title> <journal> Operations Research, </journal> <volume> 13 </volume> <pages> 444-452, </pages> <year> 1965. </year>
Reference-contexts: When the traditional distance of a misclassified point to a separating plane is used as an error, a single linear program <ref> [6, 15, 16, 4] </ref> usually solves the problem. Recently [10, 18, 2, 7] a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane.
Reference: [16] <author> O. L. Mangasarian. </author> <title> Multi-surface method of pattern separation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:801-807, </volume> <year> 1968. </year>
Reference-contexts: When the traditional distance of a misclassified point to a separating plane is used as an error, a single linear program <ref> [6, 15, 16, 4] </ref> usually solves the problem. Recently [10, 18, 2, 7] a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane. <p> Thus, one resorts in the general case to optimizing some error criterion in the satisfaction of (2). The simplest such criterion is to use linear programming in order to construct a plane (1) that maximizes a weighted sum of the distances of correctly classified points to the plane <ref> [16, 4] </ref> as follows: max Here the rows of the matrices A 2 R mfin and B 2 R kfin represent the m points in A and the k points in B respectively, while e is a vector of ones of appropriate dimension. <p> However, various greedy sequential constructions of the planes determining the various polyhedral regions <ref> [16, 19, 1] </ref> have been quite successful in obtaining very effective algorithms for training neural networks. These algorithms are much faster than the classical online backpropagation (BP) gradient algorithm [23, 11, 20], where the training is done on one point at a time.
Reference: [17] <author> O. L. Mangasarian. </author> <title> Mathematical programming in neural networks. </title> <journal> ORSA Journal on Computing, </journal> <volume> 5(4) </volume> <pages> 349-360, </pages> <year> 1993. </year>
Reference-contexts: to a neural network with h hidden linear threshold units (with thresholds i , incoming arc weights w i ; i = 1; : : :; h) and output linear threshold unit (with threshold t and incoming arc weights t i ; i = 1; : : : ; h) <ref> [17] </ref>. This condition is necessary and sufficient for the polyhedral partition of R n in order for it to correspond to a neural network with one layer of hidden units. For more details, see [17]. "Training" a neural network consists of determining (w i ; i ) 2 R n+1 ; <p> t and incoming arc weights t i ; i = 1; : : : ; h) <ref> [17] </ref>. This condition is necessary and sufficient for the polyhedral partition of R n in order for it to correspond to a neural network with one layer of hidden units. For more details, see [17]. "Training" a neural network consists of determining (w i ; i ) 2 R n+1 ; i = 1; : : : ; h; (t; t ) 2 R h+1 ; such that the following nonlinear inequalities are satisfied as best as possible: h X (Aw i e i )
Reference: [18] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: When the traditional distance of a misclassified point to a separating plane is used as an error, a single linear program [6, 15, 16, 4] usually solves the problem. Recently <ref> [10, 18, 2, 7] </ref> a more complex, and for certain applications more realistic, error measure has been considered, namely the number of misclassified points by a separating plane. This problem, even though shown to be NP-complete [7], can be effectively solved by a parametric [2] or a hybrid method [7]. <p> Although this is an NP-complete problem [7, Proposition 2], effective methods for its solution have been proposed <ref> [18, 7] </ref> and successfully tested on real world problems [2, 7]. We outline two of these approaches briefly now. We first describe a hybrid approach, recently proposed and tested successfully on ten publicly available databases [7]. <p> However it seems to have the best generalization [7] as determined by tenfold cross-validation [26] on the ten data sets employed. We describe now another approach for solving the classification maximization problem (7), by reducing it to an LPEC, a linear program with equilibrium constraints <ref> [18] </ref>. We begin with a variation of a lemma of [18, Lemma 2.1] which ensures that the backward implication of the lemma holds also for zero components of a. <p> We describe now another approach for solving the classification maximization problem (7), by reducing it to an LPEC, a linear program with equilibrium constraints [18]. We begin with a variation of a lemma of <ref> [18, Lemma 2.1] </ref> which ensures that the backward implication of the lemma holds also for zero components of a. <p> We note that the use of * is unnecessary in <ref> [18] </ref>, because of the following equivalence: r = (a) fl ; u = (a) + () (r; u) 2 arg min fe T rj0 r ? u a 0; 0 u ? r + e 0g; (11) and because the term e T r is minimized in [18], but is being <p> is unnecessary in <ref> [18] </ref>, because of the following equivalence: r = (a) fl ; u = (a) + () (r; u) 2 arg min fe T rj0 r ? u a 0; 0 u ? r + e 0g; (11) and because the term e T r is minimized in [18], but is being maximized here (see (12) for example). <p> To overcome the nonlinear effect of the ?-condition, an implicitly exact penalty function formulation has been proposed as well as a parametric approach <ref> [18] </ref>. The parametric approach is preferable, because (12) has infinitely many stationary points as was pointed out in [18]. <p> To overcome the nonlinear effect of the ?-condition, an implicitly exact penalty function formulation has been proposed as well as a parametric approach <ref> [18] </ref>. The parametric approach is preferable, because (12) has infinitely many stationary points as was pointed out in [18]. The reason for this anomaly is that any (w; ) determining a plane x T w = that does not contain any points from either the sets A or B , is a stationary solution for problem (12). <p> This is so because a slight perturbation of the plane does not change the number of misclassified points. To overcome this difficulty a parametric reformulation was proposed in <ref> [18] </ref> and implemented in [2].
Reference: [19] <author> O. L. Mangasarian, R. Setiono, and W. H. Wolberg. </author> <title> Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Numerical Optimization, </booktitle> <pages> pages 22-31, </pages> <address> Philadelphia, Pennsylvania, </address> <year> 1990. </year> <title> SIAM. </title> <booktitle> Proceedings of the Workshop on Large-Scale Numerical Optimization, </booktitle> <institution> Cornell University, </institution> <address> Ithaca, New York, </address> <month> October 19-20, </month> <year> 1989. </year>
Reference-contexts: However, various greedy sequential constructions of the planes determining the various polyhedral regions <ref> [16, 19, 1] </ref> have been quite successful in obtaining very effective algorithms for training neural networks. These algorithms are much faster than the classical online backpropagation (BP) gradient algorithm [23, 11, 20], where the training is done on one point at a time. <p> However, any two disjoint point sets in R n can be discriminated between by some polyhedral partition that corresponds to a neural network with one hidden layer with a sufficient number of hidden units <ref> [12, 19] </ref>. 7 y (x) w 1 w 2 w h 1 2 h threshold units (LTUs), input x 2 R n , and output y (x) 2 f0; 1g: The output of hidden unit i is (x T w i i ) fl ; i = 1; : : :
Reference: [20] <author> O. L. Mangasarian and M. V. Solodov. </author> <title> Serial and parallel backpropagation convergence via nonmonotone perturbed minimization. </title> <journal> Optimization Methods and Software, </journal> <volume> 4(2) </volume> <pages> 103-116, </pages> <year> 1994. </year>
Reference-contexts: defined by r (t ) := Cx (t ) c: (26) Computational results carried out in [27] have corroborated the improved generalization results of Theorem 3.1 above, as well as for more complex models such as neural networks, where a threshold tolerance in measuring the error in the backpropagation algorithm <ref> [23, 11, 20] </ref> is allowed. 6 B B A A x T w 2 = 2 110 010 001 100 and x T w 3 = 3 . <p> However, various greedy sequential constructions of the planes determining the various polyhedral regions [16, 19, 1] have been quite successful in obtaining very effective algorithms for training neural networks. These algorithms are much faster than the classical online backpropagation (BP) gradient algorithm <ref> [23, 11, 20] </ref>, where the training is done on one point at a time. Often online BP is erroneously referred to as a descent algorithm, which it is not. <p> step function () fl the sigmoid function () is used in (33), where () := 1 1+e ff ; ff &gt; 0, we obtain an error function similar to the error function that backpropagation attempts to find a stationary point for, and for which a convergence proof is given in <ref> [20] </ref>, and stability analysis in [25].
Reference: [21] <author> O. L. Mangasarian, W. Nick Street, and W. H. Wolberg. </author> <title> Breast cancer diagnosis and prognosis via linear programming. </title> <type> Technical Report 94-10, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <year> 1994. </year> <note> Operations Research 43(4) 1995, to appear. Available from ftp://ftp.cs.wisc.edu/math-prog/tech-reports/94-10.ps.Z. </note>
Reference-contexts: Although the linear programming formulation (3) is very effective for practical problems <ref> [21] </ref> and can be used in the construction of neural networks [3] as well multi-surface discriminators [1, 4], it does not minimize the number of misclassified points by the plane (1), which may be an important consideration in certain applications.
Reference: [22] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1969. </year>
Reference-contexts: We note that the classical exclusive-or (XOR) example <ref> [22] </ref> for which A = 0 1 ; B = 0 0 , gives a maximum value of four for (33) with the following solution: (w 1 ; 1 ) = ((2 2); 1); (w 2 ; 2 ) = ((2 2); 1) (34) This corresponds to correctly separating the two
Reference: [23] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362, </pages> <address> Cambridge, Massachusetts, 1986. </address> <publisher> MIT Press. </publisher>
Reference-contexts: defined by r (t ) := Cx (t ) c: (26) Computational results carried out in [27] have corroborated the improved generalization results of Theorem 3.1 above, as well as for more complex models such as neural networks, where a threshold tolerance in measuring the error in the backpropagation algorithm <ref> [23, 11, 20] </ref> is allowed. 6 B B A A x T w 2 = 2 110 010 001 100 and x T w 3 = 3 . <p> However, various greedy sequential constructions of the planes determining the various polyhedral regions [16, 19, 1] have been quite successful in obtaining very effective algorithms for training neural networks. These algorithms are much faster than the classical online backpropagation (BP) gradient algorithm <ref> [23, 11, 20] </ref>, where the training is done on one point at a time. Often online BP is erroneously referred to as a descent algorithm, which it is not.
Reference: [24] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: to work quite well as evidenced by computational results given in [2, 7]. 4 3 Improving Generalization In this section we shall consider a fundamental problem of machine learning: How to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [13, 24, 28] </ref>. We shall concentrate on some very recent results [27] obtained for a simple linear model and which make critical use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model only, seem to extend to much more complex systems, including neural networks [27].
Reference: [25] <author> M. V. Solodov and S. K. Zavriev. </author> <title> Stability properties of the gradient projection method with applications to the backpropagation algorithm. </title> <institution> Computer Sciences Department, </institution> <note> Mathematical Programming Technical Report 94-05, </note> <institution> University of Wisconsin, Madison, Wisconsin, </institution> <month> June </month> <year> 1994. </year> <note> SIAM Journal on Optimization, submitted. 13 </note>
Reference-contexts: sigmoid function () is used in (33), where () := 1 1+e ff ; ff &gt; 0, we obtain an error function similar to the error function that backpropagation attempts to find a stationary point for, and for which a convergence proof is given in [20], and stability analysis in <ref> [25] </ref>.
Reference: [26] <author> M. Stone. </author> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 36 </volume> <pages> 111-147, </pages> <year> 1974. </year>
Reference-contexts: Needless to say, there is no guarantee that this approach will give a global solution to the NP-complete problem (7). However it seems to have the best generalization [7] as determined by tenfold cross-validation <ref> [26] </ref> on the ten data sets employed. We describe now another approach for solving the classification maximization problem (7), by reducing it to an LPEC, a linear program with equilibrium constraints [18].
Reference: [27] <author> W. Nick Street and O. L. Mangasarian. </author> <title> Improved generalization via tolerant training. </title> <type> Technical report, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: This problem, even though shown to be NP-complete [7], can be effectively solved by a parametric [2] or a hybrid method [7]. In Section 3 we describe a central problem of machine learning, that of improving generalization <ref> [27] </ref>. We give a very simple model which justifies the often accepted rule-of-thumb of machine learning and approximation theory, that overfitting leads to poor generalization. In fact we go the opposite direction, and show that inexact fitting can lead to improved generalization. <p> We shall concentrate on some very recent results <ref> [27] </ref> obtained for a simple linear model and which make critical use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model only, seem to extend to much more complex systems, including neural networks [27]. <p> We shall concentrate on some very recent results <ref> [27] </ref> obtained for a simple linear model and which make critical use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model only, seem to extend to much more complex systems, including neural networks [27]. The model that we shall consider here consists of the training set fA; ag where A is a given m fi n real matrix and a is a given m fi 1 real vector. <p> 2 * kxk 2 The key question to ask here, is this: Under what conditions does a solution x (t ) of (18), for some t &gt; 0, give a smaller error on a testing set? We are able to give an answer to this question and corroborate it computationally <ref> [27] </ref>, by considering a general testing set (C; c) 2 R kfin fi R k 5 as well as a simpler testing set, where only the right side of (14) is perturbed. <p> Theorem 3.1 <ref> [27] </ref> Improved generalization on Ax = a+t with positive training tolerance The testing set error function f (t ) of (21) has a strict local maximum at 0 and a global minimum on [0; ^t], where ^t is defined by (22), at some t &gt; 0, whenever (*x (0) + A <p> For the more general testing model given by Cx = c of (15), we have the following result for improved generalization. Theorem 3.2 <ref> [27] </ref> Improved generalization on Cx = c with positive training tolerance Let x (t ) be defined by the tolerant training of Ax = a by the quadratic program (18) with tolerance t 0. <p> maximum over t 0 whenever kr (0)k 2 2 &gt; r (t ) T r (0) for some t 2 (0; ~t] (25) for some sufficiently small ~t , where r (t ) is defined by r (t ) := Cx (t ) c: (26) Computational results carried out in <ref> [27] </ref> have corroborated the improved generalization results of Theorem 3.1 above, as well as for more complex models such as neural networks, where a threshold tolerance in measuring the error in the backpropagation algorithm [23, 11, 20] is allowed. 6 B B A A x T w 2 = 2 110
Reference: [28] <author> D. H. Wolpert, </author> <title> editor. The Mathematics of Generalization, </title> <address> Reading, MA, 1995. </address> <publisher> Addison-Wesley. </publisher> <pages> 14 </pages>
Reference-contexts: to work quite well as evidenced by computational results given in [2, 7]. 4 3 Improving Generalization In this section we shall consider a fundamental problem of machine learning: How to train a system on a given training set so as to improve generalization on a new unseen testing set <ref> [13, 24, 28] </ref>. We shall concentrate on some very recent results [27] obtained for a simple linear model and which make critical use of mathematical programming ideas. These ideas, although rigorously established for a simple linear model only, seem to extend to much more complex systems, including neural networks [27].
References-found: 28

