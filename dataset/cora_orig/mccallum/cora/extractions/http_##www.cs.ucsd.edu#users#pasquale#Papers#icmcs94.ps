URL: http://www.cs.ucsd.edu/users/pasquale/Papers/icmcs94.ps
Refering-URL: http://www.cs.ucsd.edu/users/pasquale/Pub.html
Root-URL: http://www.cs.ucsd.edu
Email: kfall@cs.ucsd.edu pasquale@cs.ucsd.edu  
Title: Improving Continuous-Media Playback Performance with In-Kernel Data Paths  
Author: Kevin Fall Joseph Pasquale 
Address: San Diego, CA 92093-0114  
Affiliation: Computer Systems Laboratory Department of Computer Science and Engineering University of California, San Diego  
Abstract: Continuous media playback suffers when a station's operating system offers insufficient I/O throughput. Conventional I/O system structures support a memory-oriented read and write interface requiring the execution of user-level processes to facilitate playback, and can incur throughput degradation due to unnecessary data copies. Our splice mechanism supports a peer-to-peer model of I/O where a requesting application can associate a data source with its corresponding data sink, allowing for system optimizations in the data path implementation. In an experiment designed to simulate remote video playback, we present measurements indicating that use of our techniques resulted in a 55% gain in throughput as compared with conventional systems. 
Abstract-found: 1
Intro-found: 1
Reference: [BALL90] <author> Brian Bershad, Thomas Ander-son, Edward Lazowska, and Henry Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Thus, context switch operations for data transfer are eliminated. Context switches consume CPU resources, degrade cache performance by reducing locality of reference [MB91], and effect the performance of virtual memory by requiring TLB invalidations <ref> [BALL90] </ref>. Additional motivations for addressing the issue of context switching performance are mentioned in [PCMI91], [CJRS89], and [CHKM88]. 1 A similar technique is used in the 4.3BSD Reno NFS Implementation [Mac91].
Reference: [Bow91] <author> Pat Bowlds. </author> <title> Micro Channel Architecture. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: Eliminating a memory-based interface provides not only the advantages listed above, but also the possibility of connecting data source and sink devices together directly at the system's I/O bus and managing the transfer by the kernel in a general way. Some current architectures (e.g. IBM's Microchannel <ref> [Bow91] </ref>) support this notion as "peer-to-peer DMA" or "stream-mode transfers." These architectures provide I/O data transfer between interface adapters directly on the system bus. During such a transfer, I/O data is not brought into the system's main memory unit.
Reference: [CHKM88] <author> Luis-Felipe Cabrera, Edward Hunter, Michael Karels, and David Mosher. </author> <title> User-process communication performance in networks of computers. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 14(1) </volume> <pages> 38-53, </pages> <month> Jan-uary </month> <year> 1988. </year>
Reference-contexts: Context switches consume CPU resources, degrade cache performance by reducing locality of reference [MB91], and effect the performance of virtual memory by requiring TLB invalidations [BALL90]. Additional motivations for addressing the issue of context switching performance are mentioned in [PCMI91], [CJRS89], and <ref> [CHKM88] </ref>. 1 A similar technique is used in the 4.3BSD Reno NFS Implementation [Mac91]. <p> In ordinary operation, UDP performs a checksum to detect corrupt data and discards datagrams whose checksums fail. The checksum operation is the dominant processing overhead associated with UDP/IP <ref> [CHKM88] </ref>. For uncompressed video, corrupt data could be delivered to the frame buffer without catas trophic result. In addition to removing checksumming for loss-tolerant data such as uncompressed audio or video, checksums may be disabled in certain restricted operating environments (e.g. when hardware checksumming is performed [KP93]).
Reference: [CJRS89] <author> D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communications, </journal> <pages> pages 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Thus, context switch operations for data transfer are eliminated. Context switches consume CPU resources, degrade cache performance by reducing locality of reference [MB91], and effect the performance of virtual memory by requiring TLB invalidations [BALL90]. Additional motivations for addressing the issue of context switching performance are mentioned in [PCMI91], <ref> [CJRS89] </ref>, and [CHKM88]. 1 A similar technique is used in the 4.3BSD Reno NFS Implementation [Mac91].
Reference: [Cla85] <author> David Clark. </author> <title> The structuring of systems using upcalls. </title> <booktitle> Proc. 10th SOSP, </booktitle> <pages> pages 171-180, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Implementing a prototype NtoF splice within the Berkeley network code framework is surprisingly straightforward. The Berkeley network framework essentially uses an upcall 3 scheme until data is delivered 2 The splice descriptor provides enough information for splice to operate asynchronously, although this feature has not been exercised. 3 Clark <ref> [Cla85] </ref> describes the notion of upcalls in detail, but to a socket buffer.
Reference: [DAPP92] <author> Peter Druschel, Mark Abbott, Michael Pagels, and Larry Peterson. </author> <title> Analysis of I/O subsystem design for multimedia workstations. </title> <booktitle> Proc. 3rd. Intl. Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <month> Novem-ber </month> <year> 1992. </year>
Reference-contexts: They introduce new ioctl calls capable of passing data between specific devices without passing directly through a user-level process. A study of I/O design by Druschel et. al. <ref> [DAPP92] </ref> suggests the use of low-level data streaming inhibits innovation in application development by disallowing applications direct access to I/O data.
Reference: [DP93] <author> Peter Druschel and Larry Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> Proc. 14th SOSP, </booktitle> <pages> pages 189-202, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: There are at least three systems which address the problem of minimizing physical data movement across domains: DASH IPC [TA91], Fbufs <ref> [DP93] </ref>, and Container Shipping [PAM94]. Each of these systems uses virtual memory remapping techniques to transfer data between protection domains without physical data copies. The DASH IPC system allows processes to exclusively own regions of a restricted area of a shared address space.
Reference: [FGB91] <author> Alessandro Forin, David Golub, and Brian Bershad. </author> <title> An I/O system for mach 3.0. </title> <booktitle> Proc. USENIX Mach Symposium, </booktitle> <pages> pages 163-176, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The splice interface removes details of any intermediate buffering from the caller. The presence or absence of intermediate buffering is detectable by the caller only in terms of performance. 7 Related Work A new I/O system for Mach 3.0 described by Forin et. al. <ref> [FGB91] </ref> suggests the problem of data copying is best addressed by the use of user-level device drivers capable of accessing device registers directly by mapping when possible. Remote device access is achieved by an IPC-based device interface relying on RPC.
Reference: [FP93] <author> Kevin Fall and Joseph Pasquale. </author> <title> Exploiting in-kernel data paths to improve I/O throughput and CPU availability. </title> <booktitle> Proc. USENIX Winter Conference, </booktitle> <pages> pages 327-333, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Such a mechanism should reduce data copies and minimize process switching, yet allow applications to control data transformations and meter the rate of data transfers. The next section describes a mechanism that addresses these concerns. 3 The Splice Mechanism The splice mechanism <ref> [FP93] </ref> is a system function used to establish a kernel-managed data path directly between I/O devices. In an MIO system exporting the traditional read and write interfaces, I/O data flowing between devices must pass through the address space of a user process. <p> A data source and sink are specified by UNIX file descriptors. In the present prototype, these file descriptors may refer to regular files, sockets or device-special files. This section describes a network to-framebuffer (NtoF) prototype implementation of splice; the implementation for regular files has been described in <ref> [FP93] </ref>. The NtoF splice is representative of the task required at a multimedia receiving station. described below and used in the subsequent performance evaluation section. A Berkeley-based network structure [LJFK86] was used to construct the prototype. <p> For any active splice, a dynamically allocated kernel-resident splice descriptor maintains references to socket or file structures (depending on the purpose of the splice; see <ref> [FP93] </ref>), plus any additional information required by the implementation. 2 For Internet domain sockets, a new field in the inpcb structure indicates the presence of an active splice for the associated socket by pointing to a valid splice descriptor.
Reference: [KP93] <author> Jonathan Kay and Joseph Pasquale. </author> <title> Measurement, analysis, and improvement of UDP/IP throughput for the DECStation 5000. </title> <booktitle> Proc. USENIX Winter Conference, </booktitle> <pages> pages 249-258, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The mechanism explicitly addresses well-known overheads associated with data copying, and also reduces the number of context-switch operations required to perform I/O operations. When combined with a scheme capable of improving network protocol processing performance (as is suggested in <ref> [KP93] </ref> for example), a greater than doubling above baseline performance may be achieved. The mechanism represents an augmentation of conventional operating system I/O interfaces, and is not a replacement for the read and write system functions. The functionality and implementation of read and write remain unaffected. <p> The faster sender simulates a more powerful computational resource at the data source, as might be typical of a video file server application. Theoretical FDDI bandwidth is 100 Mbit/s, although measured performance at the network interface (a DEC DEFZA FDDI adapter, which provides no send-side DMA) is 56-64 Mbit/s <ref> [KP93] </ref>. For the sender employing splice, 12,000 bytes of 8-bit image data are sent per frame, divided into three FDDI packets using the UDP transport protocol [Pos80] to send a sequence of 50 frames (150 FDDI packets). <p> For uncompressed video, corrupt data could be delivered to the frame buffer without catas trophic result. In addition to removing checksumming for loss-tolerant data such as uncompressed audio or video, checksums may be disabled in certain restricted operating environments (e.g. when hardware checksumming is performed <ref> [KP93] </ref>). To evaluate the relationship between splice and checksumming, we performed the experiment with checksums enabled and again with checksums disabled. 6.3 Measured Results We now describe measured results based on the use of splice and the effect of checksum computation.
Reference: [LJFK86] <author> Sam Le*er, William Joy, Robert Fabry, and Michael Karels. </author> <title> Networking implementation notes, 4.3bsd edition. 4.3BSD System Manager's Manual, </title> <month> June </month> <year> 1986. </year>
Reference-contexts: This section describes a network to-framebuffer (NtoF) prototype implementation of splice; the implementation for regular files has been described in [FP93]. The NtoF splice is representative of the task required at a multimedia receiving station. described below and used in the subsequent performance evaluation section. A Berkeley-based network structure <ref> [LJFK86] </ref> was used to construct the prototype. The splice call provides suffient information for the operating system to manage the flow of data between the I/O objects specified by the calling process. <p> Image data is stored in a statically allocated kernel memory region in the sender, and is passed directly to the network protocol subsystem to be sent. For the user-level sender, the same image data is statically allocated in a user process, and sent employing the UNIX socket layer <ref> [LJFK86] </ref>. A data copy is performed between the user and kernel address spaces.
Reference: [Mac91] <author> Rick Macklem. </author> <title> Lessons learned tuning the 4.3bsd reno implementation of the nfs protocol. </title> <booktitle> Proc. USENIX Winter Conference, </booktitle> <pages> pages 53-64, </pages> <year> 1991. </year>
Reference-contexts: Additional motivations for addressing the issue of context switching performance are mentioned in [PCMI91], [CJRS89], and [CHKM88]. 1 A similar technique is used in the 4.3BSD Reno NFS Implementation <ref> [Mac91] </ref>. For applications making no direct manipulation of I/O data (or for those allowing the operating system to make such manipulations), splice relegates the issues of managing the data flow (e.g. buffering and flow control) to the operating system.
Reference: [MB91] <author> Jeffrey Mogul and Anita Borg. </author> <title> The effect of context switches on cache performance. </title> <booktitle> Proc. ASPLOS-IV, </booktitle> <pages> pages 75-84, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Thus, context switch operations for data transfer are eliminated. Context switches consume CPU resources, degrade cache performance by reducing locality of reference <ref> [MB91] </ref>, and effect the performance of virtual memory by requiring TLB invalidations [BALL90]. Additional motivations for addressing the issue of context switching performance are mentioned in [PCMI91], [CJRS89], and [CHKM88]. 1 A similar technique is used in the 4.3BSD Reno NFS Implementation [Mac91].
Reference: [MJ93] <author> Steve Mccanne and Van Jacobson. </author> <title> The bsd packet filter: A new architecture for user-level packet capture. </title> <booktitle> Proc. USENIX Winter Conference, </booktitle> <month> Jan </month> <year> 1993. </year>
Reference-contexts: In such a system, a user would compile a module with a compiler specially modified to produce "safe" code. The system can verify the "safeness" of code and incorporate the module into its address space. Interpreted languages may also be used, as reported in [MRA87] and <ref> [MJ93] </ref>. These systems place a small interpreter in the operating system. Code is written in the specialized language. Applications requiring completely generalized direct modification of intermediate data should use the traditional read and write interfaces, manipulating data as needed.
Reference: [MRA87] <author> J. Mogul, R. Rashid, and M. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> Proc. 11th SOSP, </booktitle> <pages> pages 39-51, </pages> <month> Nov </month> <year> 1987. </year>
Reference-contexts: In such a system, a user would compile a module with a compiler specially modified to produce "safe" code. The system can verify the "safeness" of code and incorporate the module into its address space. Interpreted languages may also be used, as reported in <ref> [MRA87] </ref> and [MJ93]. These systems place a small interpreter in the operating system. Code is written in the specialized language. Applications requiring completely generalized direct modification of intermediate data should use the traditional read and write interfaces, manipulating data as needed.
Reference: [PAM94] <author> Joseph Pasquale, Eric Anderson, and P.Keith Muller. </author> <title> Container shipping: operating system support for i/o intensive applications. </title> <journal> IEEE Computer Magazine, </journal> <volume> 27(3) </volume> <pages> 84-93, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: There are at least three systems which address the problem of minimizing physical data movement across domains: DASH IPC [TA91], Fbufs [DP93], and Container Shipping <ref> [PAM94] </ref>. Each of these systems uses virtual memory remapping techniques to transfer data between protection domains without physical data copies. The DASH IPC system allows processes to exclusively own regions of a restricted area of a shared address space.
Reference: [PCMI91] <author> Michael Pasieka, Paul Crumley, Ann Marks, and Ann Infortuna. </author> <title> Distributed multimedia: How can the necessary data rates be supported? Proc. </title> <booktitle> USENIX Summer Conference, </booktitle> <pages> pages 169-182, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Thus, context switch operations for data transfer are eliminated. Context switches consume CPU resources, degrade cache performance by reducing locality of reference [MB91], and effect the performance of virtual memory by requiring TLB invalidations [BALL90]. Additional motivations for addressing the issue of context switching performance are mentioned in <ref> [PCMI91] </ref>, [CJRS89], and [CHKM88]. 1 A similar technique is used in the 4.3BSD Reno NFS Implementation [Mac91]. <p> Remote device access is achieved by an IPC-based device interface relying on RPC. In the CTMS system described by Pasieka et. al. <ref> [PCMI91] </ref>, the problem of excess data copies is addressed by a modification to the UNIX I/O model of inter-device transfers. They introduce new ioctl calls capable of passing data between specific devices without passing directly through a user-level process.
Reference: [Pos80] <author> Jon Postel. </author> <title> User Datagram Protocol. </title> <type> RFC 768, </type> <institution> Network Information Center, </institution> <month> May </month> <year> 1980. </year>
Reference-contexts: For the sender employing splice, 12,000 bytes of 8-bit image data are sent per frame, divided into three FDDI packets using the UDP transport protocol <ref> [Pos80] </ref> to send a sequence of 50 frames (150 FDDI packets). Image data is stored in a statically allocated kernel memory region in the sender, and is passed directly to the network protocol subsystem to be sent.
Reference: [Pos81] <author> Jon Postel. </author> <title> Internet Protocol. </title> <type> RFC 791, </type> <institution> Network Information Center, </institution> <month> September </month> <year> 1981. </year>
Reference-contexts: Mbufs represent the network buffer abstraction present in most Berkeley-derived network implementations. Received packets are placed in mbufs or a linked list of mbufs called an mbuf chain, and may be coalesced as required by protocol processing. Using the IP protocol <ref> [Pos81] </ref>, packet sizes are limited to 64KBytes which is adequate to hold most compressed frames (the largest JPEG-compressed images we have encountered is about 20KBytes, with typical values of 7-8KBytes). does not discuss the Berkeley networking implementation which does not perform an upcall all the way to user space. 4 Scatter/gather
Reference: [Rit84] <author> Dennis Ritchie. </author> <title> A stream input-output system. </title> <journal> AT&T Bell Laboratories Technical Journal, </journal> <volume> 63(8) </volume> <pages> 1897-1910, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Interpreters Loadable kernel modules (LKMs) are now supported by many operating systems. LKMs are relocatable object files loaded into kernel address space by priv-iledged processes. Code from LKMs are then executed by the kernel. Faulty modules may cause failure of the entire system. Streams modules, as described first in <ref> [Rit84] </ref>, offer a set of predefined operations implemented within the operating system that a user process may choose to be executed on its behalf. Standard data transformations (e.g. compression or other commonly-used coding algorithms) would be appropriate candidates for a streams-style implementation. <p> Many applications not requiring direct manipulation of I/O data can benefit from streaming. Furthermore, for applications requiring well-known data manipulations, kernel-resident processing modules (e.g. like Ritchie's Streams <ref> [Rit84] </ref>) or outboard dedicated processors are more easily exploited within the kernel operating environment than in user process. There are at least three systems which address the problem of minimizing physical data movement across domains: DASH IPC [TA91], Fbufs [DP93], and Container Shipping [PAM94].
Reference: [TA91] <author> Shin-Yuan Tzou and David Anderson. </author> <title> The performance of message-passing using restricted virtual memory remapping. </title> <journal> Software Practice and Experience, </journal> <volume> 21(3) </volume> <pages> 251-267, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: There are at least three systems which address the problem of minimizing physical data movement across domains: DASH IPC <ref> [TA91] </ref>, Fbufs [DP93], and Container Shipping [PAM94]. Each of these systems uses virtual memory remapping techniques to transfer data between protection domains without physical data copies. The DASH IPC system allows processes to exclusively own regions of a restricted area of a shared address space.
Reference: [WLAG93] <author> Robert Wahbe, Steve Lucco, Thomas An-derson, and Susan Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> Proc. 14 SOSP, </booktitle> <pages> pages 203-216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Standard data transformations (e.g. compression or other commonly-used coding algorithms) would be appropriate candidates for a streams-style implementation. Software-Based Fault Isolation <ref> [WLAG93] </ref> prevents untrusted code modules from accessing other regions of memory by restricting the set of machine-level instructions a module may execute. The scheme provides enhanced fault tolerance in exchange for some execution overhead required to examine a module's address references.
References-found: 22

