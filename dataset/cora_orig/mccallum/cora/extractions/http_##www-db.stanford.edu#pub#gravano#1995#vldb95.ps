URL: http://www-db.stanford.edu/pub/gravano/1995/vldb95.ps
Refering-URL: http://karna.cs.umd.edu:3264/people/godfrey/cites.html
Root-URL: 
Email: fgravano,hectorg@cs.stanford.edu  
Title: Generalizing GlOSS to Vector-Space Databases and Broker Hierarchies  
Author: Luis Gravano Hector Garca-Molina 
Address: Stanford, CA 94305-2140  
Affiliation: Computer Science Department Stanford University  
Abstract: As large numbers of text databases have become available on the Internet, it is harder to locate the right sources for given queries. In this paper we present gGlOSS, a generalized Glossary-Of-Servers Server, that keeps statistics on the available databases to estimate which databases are the potentially most useful for a given query. gGlOSS extends our previous work [1], which focused on databases using the boolean model of document retrieval, to cover databases using the more sophisticated vector-space retrieval model. We evaluate our new techniques using real-user queries and 53 databases. Finally, we further generalize our approach by showing how to build a hierarchy of gGlOSS brokers. The top level of the hierarchy is so small it could be widely replicated, even at end-user workstations. fl This research was sponsored by the Advanced Research Projects Agency (ARPA) of the Department of Defense under Grant No. MDA972-92-J-1029 with the Corporation for National Research Initiatives (CNRI). The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies or endorsement, either expressed or implied, of ARPA, the U.S. Government or CNRI. This work was supported by an equipment grant from IBM Corporation. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 21st VLDB Conference Zurich, Switzerland, 1995 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Luis Gravano, Hector Garca-Molina, and Anthony Tomasic. </author> <title> The effectiveness of GlOSS for the text-database discovery problem. </title> <booktitle> In Proceedings of the 1994 ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: This broker keeps only partial information on the contents of each database, so it scales with the growing number of available databases. However, this information covers the full-text content of the documents, so that the useful sources are identified. In <ref> [1] </ref> and [4] we described GlOSS (for Glossary-Of-Servers Server) 1 , a centralized broker that keeps meta-information about databases supporting the boolean model of document retrieval. GlOSS maintains statistics on the databases, and uses these statistics to estimate the actual contents of the databases. <p> Furthermore, this information is orders of magnitude smaller than that required by a full-text index of the databases, for example. Adapting the boolean-database estimates of <ref> [1] </ref>, we can estimate that the size of the gGlOSS information about a vector-space database is only around 2% of the size of a full-text vector-space index of the database. <p> These assumptions are artificial: very rarely would a set of databases and queries conform to them. However, we use them because these type of assumptions proved themselves useful in the boolean-GlOSS case for choosing the "right" databases for a query <ref> [1, 4] </ref>. 4.1 High-Correlation Scenario To derive Max (l), the first database rank with which gGlOSS tries to match the Ideal (l) database rank of Section 3, gGlOSS assumes that if two words appear together in a user query, then these words will appear in the database documents with the highest
Reference: [2] <author> Michael F. Schwartz, Alan Emtage, Brewster Kahle, and B. Clifford Neuman. </author> <title> A comparison of Internet resource discovery approaches. </title> <journal> Computer Systems, </journal> <volume> 5(4), </volume> <year> 1992. </year>
Reference-contexts: Consequently, users need a way to narrow their searches to a few useful text databases. This problem is a specific instance of the more general resource-discovery problem <ref> [2, 3] </ref>. Many tools have recently appeared on the Internet to help users select the (text) databases that might be more useful for their queries (see Section 2). However, many of these tools essentially keep a global index of the available documents. <p> Section 5 introduces the methodology for the experimental results of Section 6. Section 7 discusses alternative definitions of the ideal database rank. Finally, in Section 8 we show how to build the higher-level hGlOSS servers. 2 Related Work One approach to solving the text-database discovery problem (see <ref> [2, 3] </ref> for surveys) is to let users "browse" the databases. Well-known examples include the Pros-pero file system [7], Gopher [2], and the World-Wide Web [8]. A different approach is to let users query a database of "meta-information" about the available databases. <p> Finally, in Section 8 we show how to build the higher-level hGlOSS servers. 2 Related Work One approach to solving the text-database discovery problem (see [2, 3] for surveys) is to let users "browse" the databases. Well-known examples include the Pros-pero file system [7], Gopher <ref> [2] </ref>, and the World-Wide Web [8]. A different approach is to let users query a database of "meta-information" about the available databases.
Reference: [3] <author> Katia Obraczka, Peter B. Danzig, and Shih-Hao Li. </author> <title> Internet resource discovery services. </title> <booktitle> IEEE Computer, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Consequently, users need a way to narrow their searches to a few useful text databases. This problem is a specific instance of the more general resource-discovery problem <ref> [2, 3] </ref>. Many tools have recently appeared on the Internet to help users select the (text) databases that might be more useful for their queries (see Section 2). However, many of these tools essentially keep a global index of the available documents. <p> Section 5 introduces the methodology for the experimental results of Section 6. Section 7 discusses alternative definitions of the ideal database rank. Finally, in Section 8 we show how to build the higher-level hGlOSS servers. 2 Related Work One approach to solving the text-database discovery problem (see <ref> [2, 3] </ref> for surveys) is to let users "browse" the databases. Well-known examples include the Pros-pero file system [7], Gopher [2], and the World-Wide Web [8]. A different approach is to let users query a database of "meta-information" about the available databases.
Reference: [4] <author> Luis Gravano, Hector Garca-Molina, and Anthony Tomasic. </author> <title> Precision and recall of GlOSS estimators for database discovery. </title> <booktitle> In Proceedings of the 3rd International Conference on Parallel and Distributed Information Systems (PDIS'94), </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: This broker keeps only partial information on the contents of each database, so it scales with the growing number of available databases. However, this information covers the full-text content of the documents, so that the useful sources are identified. In [1] and <ref> [4] </ref> we described GlOSS (for Glossary-Of-Servers Server) 1 , a centralized broker that keeps meta-information about databases supporting the boolean model of document retrieval. GlOSS maintains statistics on the databases, and uses these statistics to estimate the actual contents of the databases. <p> These assumptions are artificial: very rarely would a set of databases and queries conform to them. However, we use them because these type of assumptions proved themselves useful in the boolean-GlOSS case for choosing the "right" databases for a query <ref> [1, 4] </ref>. 4.1 High-Correlation Scenario To derive Max (l), the first database rank with which gGlOSS tries to match the Ideal (l) database rank of Section 3, gGlOSS assumes that if two words appear together in a user query, then these words will appear in the database documents with the highest
Reference: [5] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to modern information retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: The users then access the databases themselves, following the order that GlOSS suggested. Although the boolean model of document retrieval is widely used, it is a rather primitive one. One of the most popular alternative models is the vector-space retrieval model <ref> [5, 6] </ref>. This model represents both the documents in a database and the queries themselves as weight vectors. Given a query, the documents are ranked according to how "similar" their corresponding vectors are to the given query vector. <p> Determining the ideal database rank for a query is a hard problem. The definition of this section is based solely on the answers (i.e., the document ranks and their scores) that each database produces when presented with the query in question. This definition does not use the relevance <ref> [5] </ref> of the documents to the end user who submitted the query. Using relevance would be appropriate for evaluating the search engines at each database; instead, we are evaluating how well gGlOSS can predict the answers that the databases return. <p> are the documents in db having the word computer in them, together with the associated weights: computer : (d 1 ; 0:8); (d 2 ; 0:7); (d 3 ; 0:9); (d 8 ; 0:9) That is, document d 1 contains the word computer with weight 0.8 (for some weight-computation algorithm <ref> [5] </ref>), document d 2 , with weight 0.7, and so on. <p> In our definitions below, we assume that a query q is expressed as a weight vector Q = (q 1 ; : : : ; q j ; : : :; q t ) <ref> [5] </ref>, where q j is the weight of word t j in query q. For example, this weight can simply be the number of times that word t j appears in the query.
Reference: [6] <author> Gerard Salton. </author> <title> Automatic text processing: the transformation, analysis, and retrieval of information by computer. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The users then access the databases themselves, following the order that GlOSS suggested. Although the boolean model of document retrieval is widely used, it is a rather primitive one. One of the most popular alternative models is the vector-space retrieval model <ref> [5, 6] </ref>. This model represents both the documents in a database and the queries themselves as weight vectors. Given a query, the documents are ranked according to how "similar" their corresponding vectors are to the given query vector. <p> Then, this word will tend to have a low associated weight in db 1 (e.g., if db 1 uses the tfidf formula for computing weights <ref> [6] </ref>). The word databases, on the other hand, might have a high associated weight in a database db 2 that is totally unrelated to computer science and contains very few document with that word. <p> Typically, the weight of a word t j in a document d is a function of the number of times that t j appears in d and the number of documents in the database that contain t j <ref> [6] </ref>. Although the information that gGlOSS stores about each database is incomplete, it will prove useful to generate database ranks that resemble the ideal database rank of Section 3, as we will see in Section 6.2. <p> The definitions of the gGlOSS ranks below reflect this fact. Also, note that the vectors with which gGlOSS represents each database can be viewed as cluster centroids <ref> [6] </ref>, where each database is considered as a single document cluster 5 . Because the information that gGlOSS keeps about each database is incomplete, it has to make assumptions regarding the distribution of query keywords and weights across the documents of each database. <p> To keep our experiments simple, we chose the same weighting algorithms for the queries and the documents across all of the databases. We indexed the documents using the SMART ntc formula, which generates document weight vectors using the cosine-normalized tfidf product <ref> [6] </ref>. We indexed the queries using the SMART nnn formula, which generates query weight vectors using the word frequencies in the queries. The similarity coefficient between a document vector and a query vector is computed by taking the inner product of the two vectors.
Reference: [7] <author> B. Clifford Neuman. </author> <title> The Prospero File System: A global file system based on the Virtual System model. </title> <journal> Computer Systems, </journal> <volume> 5(4), </volume> <year> 1992. </year>
Reference-contexts: Finally, in Section 8 we show how to build the higher-level hGlOSS servers. 2 Related Work One approach to solving the text-database discovery problem (see [2, 3] for surveys) is to let users "browse" the databases. Well-known examples include the Pros-pero file system <ref> [7] </ref>, Gopher [2], and the World-Wide Web [8]. A different approach is to let users query a database of "meta-information" about the available databases.
Reference: [8] <author> Tim Berners-Lee, Robert Cailliau, Jean-F. Groff, and Bernd Pollermann. </author> <title> World-Wide Web: The Information Universe. </title> <journal> Electronic Networking: Research, Applications and Policy, </journal> <volume> 1(2), </volume> <year> 1992. </year>
Reference-contexts: Well-known examples include the Pros-pero file system [7], Gopher [2], and the World-Wide Web <ref> [8] </ref>. A different approach is to let users query a database of "meta-information" about the available databases.
Reference: [9] <author> Brewster Kahle and Art Medlar. </author> <title> An information system for corporate users: Wide Area Information Servers. </title> <type> Technical Report TMC199, </type> <institution> Thinking Machines Corporation, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Well-known examples include the Pros-pero file system [7], Gopher [2], and the World-Wide Web [8]. A different approach is to let users query a database of "meta-information" about the available databases. For example, WAIS <ref> [9] </ref> provides a "directory of servers." Many search facilities have been created for the World-Wide Web, like the Lycos service. 2 To scale with the growing number of available databases, some of these systems index only document titles or, more generally, just a small fraction of each document (e.g., the World-Wide
Reference: [10] <author> James P. Callan, Zhihong Lu, and W. Bruce Croft. </author> <title> Searching distributed collections with inference networks. </title> <booktitle> In Proceedings of the 18 th Annual SIGIR Conference, </booktitle> <year> 1995. </year>
Reference-contexts: Other systems keep succinct, sometimes human-generated, summaries of the contents of each database (e.g., the ALIWEB system 4 ). Recently, <ref> [10] </ref> has applied inference networks (from information retrieval) to the text-database discovery problem. Their approach summarizes databases using document-frequency information for each term (the same type of information that GlOSS keeps about the databases), together with the "inverse collection frequency" of the different terms. <p> The problem of how to modify the locally computed similarities to compensate for collection-dependent factors in their computation has received attention recently in the context of the collection-fusion problem. The collection-fusion problem <ref> [18, 10, 19] </ref> studies how to merge document rankings for a query from different sources into a single document ranking. (See [10] for a way to use GlOSS-like information to scale the similarities computed at each source.) In general, determining what scaling factor to use to define the Local (l) ideal <p> The collection-fusion problem [18, 10, 19] studies how to merge document rankings for a query from different sources into a single document ranking. (See <ref> [10] </ref> for a way to use GlOSS-like information to scale the similarities computed at each source.) In general, determining what scaling factor to use to define the Local (l) ideal database rank is an interesting problem that we will explore in the near future.
Reference: [11] <author> Mark A. Sheldon, Andrzej Duda, Ron Weiss, James W. O'Toole, and David K. Gifford. </author> <title> A content routing system for distributed information servers. </title> <booktitle> In Proceedings of the 4 th International Conference on Extending Database Technology, </booktitle> <year> 1994. </year>
Reference-contexts: An inference network then uses this information to rank the databases for a given query. The "content-based routing" system of <ref> [11, 12] </ref> keeps a "content label" for each collection of objects, with attributes describing the contents of the collection.
Reference: [12] <author> Andrzej Duda and Mark A. Sheldon. </author> <title> Content routing in a network of WAIS servers. </title> <booktitle> In 14th IEEE International Conference on Distributed Computing Systems, </booktitle> <year> 1994. </year>
Reference-contexts: An inference network then uses this information to rank the databases for a given query. The "content-based routing" system of <ref> [11, 12] </ref> keeps a "content label" for each collection of objects, with attributes describing the contents of the collection.
Reference: [13] <author> C. Mic Bowman, Peter B. Danzig, Darren R. Hardy, Udi Manber, and Michael F. Schwartz. Harvest: </author> <title> A scalable, customizable discovery and access system. </title> <type> Technical Report CU-CS-732-94, </type> <institution> Department of Computer Science, University of Colorado-Boulder, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: An inference network then uses this information to rank the databases for a given query. The "content-based routing" system of [11, 12] keeps a "content label" for each collection of objects, with attributes describing the contents of the collection. The Harvest system <ref> [13] </ref> provides a flexible architecture for accessing information on the Internet. "Gatherers" collect information about the data sources, and pass it to "brokers." The "Harvest Server Registry" is a special broker that keeps information about all other brokers, among other things.
Reference: [14] <author> Anthony Tomasic, Luis Gravano, Calvin Lue, Peter Schwarz, and Laura Haas. </author> <title> Data structures for efficient broker implementation. </title> <type> Technical report, </type> <institution> IBM Almaden Research Center, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Database db 5 does not appear in rank I, because Goodness (l; q; db 5 ) = 7 Our definition of the R n metric in this section is partially based on the normalized cumulative recall metric of <ref> [14] </ref>.
Reference: [15] <author> Tak W. Yan and Hector Garca-Molina. </author> <title> SIFT-a tool for wide-area information dissemination. </title> <booktitle> In Proceedings of the USENIX 1995 Technical Conference, </booktitle> <pages> pages 177-86, </pages> <year> 1995. </year>
Reference-contexts: The queries that we used where profiles that real users submitted to the SIFT Netnews server developed at Stanford <ref> [15] </ref> 8 . Users send profiles in the form of boolean or vector-space queries to the SIFT server, which in turn filters Netnews articles every day and sends the articles matching the profiles to the corresponding users.
Reference: [16] <author> Luis Gravano and Hector Garca-Molina. </author> <title> Generalizing GlOSS to vector-space databases and broker hierarchies. </title> <type> Technical Report STAN-CS-TN-95-21, </type> <institution> Stanford University, </institution> <month> May </month> <year> 1995. </year> <note> Available as ftp://db.stanford.edu/pub/gravano/- 1995/stan.cs.tn.95.21.ps. </note>
Reference-contexts: In this section we explore alternative ideal database ranks for a query. (Even other possibilities are discussed in <ref> [16] </ref>.) We can organize the different database ranks for a 0.55 0.65 0.75 0.85 0.95 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 l 3 3 3 2 2 2 4 4 4 l, for ideal rank Ideal (l). 0.2 0.4 0.6 0.8 1 P 3 M ax (l) 3
Reference: [17] <author> Luis Gravano, Hector Garca-Molina, and Anthony Tomasic. </author> <title> Precision and recall of GlOSS estimators for database discovery. </title> <type> Technical Report STAN-CS-TN-94-010, </type> <institution> Stanford University, </institution> <month> July </month> <year> 1994. </year> <note> Available as ftp://db.stanford.edu/pub/gravano/- 1994/stan.cs.tn.94.010.ps. </note>
Reference-contexts: 0.4 0.6 0.8 1 P 3 M ax (l) 3 3 3 2 2 2 2 2 4 4 4 l, for ideal rank Ideal (l). query into two classes, according to whether the ranks depend on the number of relevant documents for the query in each database or not <ref> [17] </ref>. The first two alternative ranks belong to the first class. The first rank, Rel All, simply orders the databases based on the number of relevant documents they contain for the given query.
Reference: [18] <author> Ellen M. Voorhees, Narendra K. Gupta, and Ben Johnson-Laird. </author> <title> The collection fusion problem. </title> <booktitle> In Proceedings of the 3 rd Text Retrieval Conference (TREC-3), </booktitle> <year> 1995. </year>
Reference-contexts: The problem of how to modify the locally computed similarities to compensate for collection-dependent factors in their computation has received attention recently in the context of the collection-fusion problem. The collection-fusion problem <ref> [18, 10, 19] </ref> studies how to merge document rankings for a query from different sources into a single document ranking. (See [10] for a way to use GlOSS-like information to scale the similarities computed at each source.) In general, determining what scaling factor to use to define the Local (l) ideal
Reference: [19] <author> Alistair Moffat and Justin Zobel. </author> <title> Information retrieval systems for large document collections. </title> <booktitle> In Proceedings of the 3 rd Text Retrieval Conference (TREC-3), </booktitle> <year> 1995. </year>
Reference-contexts: The problem of how to modify the locally computed similarities to compensate for collection-dependent factors in their computation has received attention recently in the context of the collection-fusion problem. The collection-fusion problem <ref> [18, 10, 19] </ref> studies how to merge document rankings for a query from different sources into a single document ranking. (See [10] for a way to use GlOSS-like information to scale the similarities computed at each source.) In general, determining what scaling factor to use to define the Local (l) ideal
References-found: 19

