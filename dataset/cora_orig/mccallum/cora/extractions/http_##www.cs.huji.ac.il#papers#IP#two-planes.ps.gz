URL: http://www.cs.huji.ac.il/papers/IP/two-planes.ps.gz
Refering-URL: http://www.cs.huji.ac.il/papers/IP/index.html
Root-URL: 
Email: Email: fmichalb, rousso, pelegg@cs.huji.ac.il  
Title: Robust Recovery of Ego-Motion  
Author: Michal Irani Benny Rousso Shmuel Peleg 
Note: This research was supported by grants from the  
Address: 91904 Jerusalem, ISRAEL  
Affiliation: Institute of Computer Science The Hebrew University of Jerusalem  Austrian Friends of the Hebrew University and the Israel Academy of Sciences.  
Abstract: A direct method is introduced for computing the camera motion (the ego-motion) in a static scene. The method is based on detecting two planar surfaces in the scene and computing their 2D motion in the image plane. Theoretically, the 3D camera motion can sometimes be computed from the 2D image motion of a single planar surface. In practice, however, such computation is complicated, as it requires solving a set of high-order nonlinear equations, and is ill-conditioned. When using the 2D motion of two planar surfaces, however, the 3D camera motion can be computed by simple solutions of linear equations. The presented method uses image intensities, and the inherent problems of computing the optical flow and of feature matching are avoided. This result is a step forward in our effort to robustly compute motion and 3D structure from image sequences, using image intensities and not assuming any prior detection or correspondence. There are no severe restrictions on the ego-motion or on the structure of the environment. 
Abstract-found: 1
Intro-found: 1
Reference: [AD92] <author> Y. Aloimonos and Z. Duric. </author> <title> Active egomotion estimation: A qualitative approach. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 497-510, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions.
Reference: [Adi85] <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model, e.g. introducing a regularization term [HS81], assuming a limited model of the world <ref> [Adi85] </ref>, restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92], or from the correspondence of <p> e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions. <p> This method does not restrict the camera motion or the environment structure. It only requires the existence of two planar surfaces in the scene (but the scene needs not be piecewise linear, as in <ref> [Adi85] </ref>). Most indoor scenes have the required two planar surfaces, and in outdoor scenes the ground can serve as one planar surface, and only a single additional planar surface is required. <p> When the field of view is not very large and the rotation is relatively small <ref> [Adi85] </ref>, a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by [BAHH92]: " v = f c ( T X Z + y Z x 2 Y f c Z <p> rearranging the terms, we get: 1 = A B X A Z which can be rewritten using Equation (1) as: 1 = ff + fi x + fl y (3) where: ff = 1 A ; fi = B f c A : In a similar manipulation to that in <ref> [Adi85] </ref>, substituting (3) in (2) yields: " v = a + b x + c y + g x 2 + h xy # 4 where: 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; a = f c ffT X f c Y c = <p> Deriving Err (t) (p; q) with respect to the motion parameters and setting to zero yields six linear equations in the six unknowns: a, b, c, d, e, f [BBH + 91]. 14 3. A Moving planar surface (a pseudo 2D projective transformation): 8 pa-rameters <ref> [Adi85] </ref> (see Section 2.1), p (x; y; t) = a + bx + cy + gx 2 + hxy, q (x; y; t) = d + ex + f y + gxy + hy 2 .
Reference: [BA87] <author> J.R. Bergen and E.H. Adelson. </author> <title> Hierarchical, computationally efficient motion estimation algorithm. </title> <journal> J. Opt. Soc. Am. A., </journal> <volume> 4:35, </volume> <year> 1987. </year>
Reference-contexts: Our experience shows that the computed parameters g and h are not as reliable as the other six parameters (a; b; c; d; e; f ), as g and h are second order terms. 3.3 The Motion Computation Framework A multiresolution gradient-based iterative framework <ref> [BA87, BBH + 91, BBHP90] </ref> is used to compute the 2D motion parameters. The basic components of this framework are: * Construction of a Gaussian pyramid [Ros84], where the images are represented in multiple resolutions. * Starting at the lowest resolution level: 1.
Reference: [BAHH92] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 237-252, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year> <month> 17 </month>
Reference-contexts: When the field of view is not very large and the rotation is relatively small [Adi85], a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by <ref> [BAHH92] </ref>: " v = f c ( T X Z + y Z x 2 Y f c Z X ) x Z + y T Z f c + y 2 X # Assuming (X; Y; Z) lies on a planar surface in the scene, represented by the equation: 3
Reference: [BBH + 91] <author> J.R. Bergen, P.J. Burt, K. Hanna, R. Hingorani, P. Jeanne, and S. Peleg. </author> <title> Dynamic multiple-motion computation. In Y.A. </title> <editor> Feldman and A. Bruck-stein, editors, </editor> <booktitle> Artificial Intelligence and Computer Vision: Proceedings of the Israeli Conference, </booktitle> <pages> pages 147-156. </pages> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: Deriving Err (t) (p; q) with respect to the motion parameters and setting to zero yields six linear equations in the six unknowns: a, b, c, d, e, f <ref> [BBH + 91] </ref>. 14 3. <p> Our experience shows that the computed parameters g and h are not as reliable as the other six parameters (a; b; c; d; e; f ), as g and h are second order terms. 3.3 The Motion Computation Framework A multiresolution gradient-based iterative framework <ref> [BA87, BBH + 91, BBHP90] </ref> is used to compute the 2D motion parameters. The basic components of this framework are: * Construction of a Gaussian pyramid [Ros84], where the images are represented in multiple resolutions. * Starting at the lowest resolution level: 1.
Reference: [BBHP90] <author> J.R. Bergen, P.J. Burt, R. Hingorani, and S. Peleg. </author> <title> Computing two motions from three frames. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 27-32, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Our experience shows that the computed parameters g and h are not as reliable as the other six parameters (a; b; c; d; e; f ), as g and h are second order terms. 3.3 The Motion Computation Framework A multiresolution gradient-based iterative framework <ref> [BA87, BBH + 91, BBHP90] </ref> is used to compute the 2D motion parameters. The basic components of this framework are: * Construction of a Gaussian pyramid [Ros84], where the images are represented in multiple resolutions. * Starting at the lowest resolution level: 1.
Reference: [BHK91] <author> P.J. Burt, R. Hingorani, and R.J. Kolczynski. </author> <title> Mechanisms for isolating component patterns in the sequential analysis of multiple motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 187-193, </pages> <address> Princeton, New Jersey, </address> <month> Octo-ber </month> <year> 1991. </year>
Reference-contexts: White regions are those excluded from the detected region. 3.1 Detecting Multiple Moving Planar Objects 2D motion estimation is difficult when the region of support of an object in the image is not known, which is the common case. It was shown in <ref> [BHK91] </ref> that the motion parameters of a single translating object in the image plane can be recovered accurately by applying the 2D motion computation framework described in Section 3.3 to the entire image, using a 2D translation motion model (see Section 3.2). <p> This object is called the dominant object, and its 2D motion the dominant 2D motion. A thorough analysis of hierarchical 2D translation estimation is found in <ref> [BHK91] </ref>. In [IRP92] this method was extended to compute higher order 2D motions (2D affine, 2D projective) of a single planar object among differently moving objects. A segmentation step was added to the process, which segments out the region corresponding to the computed dominant 2D motion.
Reference: [DF90] <author> R. Deriche and O. Faugeras. </author> <title> Tracking line segments. </title> <booktitle> In Proc. 1st European Conference on Computer Vision, </booktitle> <pages> pages 259-268, </pages> <address> Antibes, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: overcome this difficulty, additional constraints are usually added to the motion model or to the environment model, e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence <ref> [DF90] </ref>. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92], or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. <p> These methods assume motion constancy in the temporal regions, i.e. motion should remain uniform in the analyzed sequence. In feature based methods, features are tracked over a larger time interval using recursive temporal filtering <ref> [DF90] </ref>. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested [Han91, HW88, Taa92]. Horn and Weldon [HW88] deal only with restricted cases (pure translation,pure rotation, or general motion with known depth).
Reference: [FJ90] <author> D.J. Fleet and A.D. Jepson. </author> <title> Computation of component image velocity from local phase information. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(1) </volume> <pages> 77-104, </pages> <year> 1990. </year>
Reference-contexts: Feature matching is also very sensitive to occlusions. Increasing the temporal region to more than two frames improves the accuracy of the computed motion. Methods for estimating local image velocities with large temporal regions have been introduced using a combined spatio-temporal analysis <ref> [FJ90, Hee88, SM90] </ref>. These methods assume motion constancy in the temporal regions, i.e. motion should remain uniform in the analyzed sequence. In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains.
Reference: [GU91] <author> R. Guissin and S. Ullman. </author> <title> Direct computation of the focus of expansion from velocity field measurements. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 146-155, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions.
Reference: [HA91] <author> L. Huang and Y. Aloimonos. </author> <title> Relative depth from motion using normal flow: An active and purposive solution. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 196-204, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions.
Reference: [Han91] <author> K. Hanna. </author> <title> Direct multi-resolution estimation of ego-motion and structure from motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 156-162, </pages> <address> Prince-ton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [Han91, HW88, Taa92] </ref>. Horn and Weldon [HW88] deal only with restricted cases (pure translation,pure rotation, or general motion with known depth). Hanna [Han91] suggests an iterative method for the general case, but relies strongly on a reasonably accurate initial guess. <p> The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested [Han91, HW88, Taa92]. Horn and Weldon [HW88] deal only with restricted cases (pure translation,pure rotation, or general motion with known depth). Hanna <ref> [Han91] </ref> suggests an iterative method for the general case, but relies strongly on a reasonably accurate initial guess. Taalebinezhaad [Taa92] has a complete mathematical formulation, but relies on computations at a single image point, which is very unreliable.
Reference: [Hee88] <author> D.J. Heeger. </author> <title> Optical flow using spatiotemporal filters. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 279-302, </pages> <year> 1988. </year>
Reference-contexts: Feature matching is also very sensitive to occlusions. Increasing the temporal region to more than two frames improves the accuracy of the computed motion. Methods for estimating local image velocities with large temporal regions have been introduced using a combined spatio-temporal analysis <ref> [FJ90, Hee88, SM90] </ref>. These methods assume motion constancy in the temporal regions, i.e. motion should remain uniform in the analyzed sequence. In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains.
Reference: [Hor90] <author> B.K.P. Horn. </author> <title> Relative orientation. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4(1) </volume> <pages> 58-78, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92], or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames <ref> [OFT87, Hor90] </ref>. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions. Increasing the temporal region to more than two frames improves the accuracy of the computed motion.
Reference: [HS81] <author> B.K.P. Horn and B.G. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model, e.g. introducing a regularization term <ref> [HS81] </ref>, assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, GU91, JH91, Sun91, NL91, <p> Equations (16) and (17) yield the well-known constraint <ref> [HS81] </ref>: pI x + qI y + I t = 0: (18) The desired 2D motion (p; q) minimizes the error function at Frame t in the region of analysis R: Err (t) (p; q) = (x;y)2R The error minimization is performed over the parameters of one of the following 2D <p> In order to minimize Err (t) (p; q), its derivatives with respect to a and d are set to zero. This yields two linear equations in the two unknowns, a and d. Those are the two well-known optical flow equations <ref> [HS81] </ref>, where every small window is assumed to have a single translation in the image plane. In this translation model, the entire object is assumed to have a single translation in the image plane. 2.
Reference: [HW88] <author> B.K.P. Horn and E.J. Weldon. </author> <title> Direct methods for recovering motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(1) </volume> <pages> 51-76, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model, e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions <ref> [HW88] </ref>, or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92], or from the correspondence of distinguished features (points, lines, contours) previously extracted <p> In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [Han91, HW88, Taa92] </ref>. Horn and Weldon [HW88] deal only with restricted cases (pure translation,pure rotation, or general motion with known depth). Hanna [Han91] suggests an iterative method for the general case, but relies strongly on a reasonably accurate initial guess. <p> In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested [Han91, HW88, Taa92]. Horn and Weldon <ref> [HW88] </ref> deal only with restricted cases (pure translation,pure rotation, or general motion with known depth). Hanna [Han91] suggests an iterative method for the general case, but relies strongly on a reasonably accurate initial guess.
Reference: [IRP92] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Detecting and tracking multiple moving objects using temporal integration. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 282-287, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In this work a direct method for computing the ego-motion from image intensities is introduced. It is a coarse-to-fine technique in the sense that at first only robust 2D data is extracted, and only later is it fused to obtain the relevant 3D information. We use existing methods <ref> [IRP92] </ref> to extract two planar surfaces in the scene with their 2D projective transformation. Theoretically it is enough to locate one planar surface in a static scene and use its 2D projective motion parameters in order to compute the 3D motion parameters of the camera. <p> The translation magnitude cannot be determined, but when setting T X to be the correct size (4:5cm), the resulting translation is then: (T X ; T Y ; T Z ) = (4:5cm; 0:47cm; 0cm). 3 Detecting Planes and Their 2D Motions We use existing methods <ref> [IRP92] </ref> in order to extract two planar surfaces in the scene with their 2D projective transformation. Those methods dealt with analysis of dynamic scenes, in which there were assumed to be multiple moving planar objects with 2D parametric motions. <p> This section describes very briefly the technique for detecting differently moving planar objects and their 2D motion parameters. For more details refer to <ref> [IRP92] </ref>. 12 a) b) c) d) sequence with several moving objects. a-b) The first and last frames in the sequence. The background and the car move differently. c) The segmented dominant object (the background) after 5 frames, using a 2D affine motion model. <p> This object is called the dominant object, and its 2D motion the dominant 2D motion. A thorough analysis of hierarchical 2D translation estimation is found in [BHK91]. In <ref> [IRP92] </ref> this method was extended to compute higher order 2D motions (2D affine, 2D projective) of a single planar object among differently moving objects. A segmentation step was added to the process, which segments out the region corresponding to the computed dominant 2D motion. <p> More details are found in <ref> [IRP92] </ref>. Temporal integration over longer sequences was used in order to improve the segmentation and the accuracy of the 2D motion parameters [IRP92]. Figure 5 shows the detection of the first and second dominant planar objects in an image sequence. <p> More details are found in <ref> [IRP92] </ref>. Temporal integration over longer sequences was used in order to improve the segmentation and the accuracy of the 2D motion parameters [IRP92]. Figure 5 shows the detection of the first and second dominant planar objects in an image sequence. In this sequence, taken by an infrared camera, the background moves due to camera motion, while the car has another motion.
Reference: [JH91] <author> A.D. </author> <title> Jepson and D.J. Heeger. A fast subspace algorithm for recovering rigid motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 124-131, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year> <month> 18 </month>
Reference-contexts: e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions.
Reference: [LR83] <author> D.T. Lawton and J.H. Rieger. </author> <title> The use of difference fields in processing sensor motion. </title> <booktitle> In DARPA IUWorkshop, </booktitle> <pages> pages 78-83, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions.
Reference: [NL91] <author> S. Negahdaripour and S. Lee. </author> <title> Motion recovery from image sequences using first-order optical flow information. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 132-139, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions.
Reference: [OFT87] <author> F. Lustman O.D. Faugeras and G. Toscani. </author> <title> Motion and structure from motion from point and line matching. </title> <booktitle> In Proc. 1st International Conference on Computer Vision, </booktitle> <pages> pages 25-34, </pages> <address> London, </address> <year> 1987. </year>
Reference-contexts: of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92], or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames <ref> [OFT87, Hor90] </ref>. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions. Increasing the temporal region to more than two frames improves the accuracy of the computed motion.
Reference: [Ros84] <editor> A. Rosenfeld, editor. </editor> <title> Multiresolution Image Processing and Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: The basic components of this framework are: * Construction of a Gaussian pyramid <ref> [Ros84] </ref>, where the images are represented in multiple resolutions. * Starting at the lowest resolution level: 1. Motion parameters are estimated by solving the set of linear equations to minimize Err (t) (p; q) (Equation (19)) according to the appropriate 2D motion model (Section 3.2).
Reference: [SM90] <author> M. Shizawa and K. Mase. </author> <title> Simultaneous multiple optical flow estimation. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <pages> pages 274-278, </pages> <address> Atlantic City, New Jersey, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Feature matching is also very sensitive to occlusions. Increasing the temporal region to more than two frames improves the accuracy of the computed motion. Methods for estimating local image velocities with large temporal regions have been introduced using a combined spatio-temporal analysis <ref> [FJ90, Hee88, SM90] </ref>. These methods assume motion constancy in the temporal regions, i.e. motion should remain uniform in the analyzed sequence. In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains.
Reference: [Sun91] <author> V. Sundareswaran. </author> <title> Egomotion from global flow field data. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 140-145, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions.
Reference: [Taa92] <author> M.A. Taalebinezhaad. </author> <title> Direct recovery of motion and shape in the general case by fixation. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 847-853, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [Han91, HW88, Taa92] </ref>. Horn and Weldon [HW88] deal only with restricted cases (pure translation,pure rotation, or general motion with known depth). Hanna [Han91] suggests an iterative method for the general case, but relies strongly on a reasonably accurate initial guess. <p> Horn and Weldon [HW88] deal only with restricted cases (pure translation,pure rotation, or general motion with known depth). Hanna [Han91] suggests an iterative method for the general case, but relies strongly on a reasonably accurate initial guess. Taalebinezhaad <ref> [Taa92] </ref> has a complete mathematical formulation, but relies on computations at a single image point, which is very unreliable. In this work a direct method for computing the ego-motion from image intensities is introduced.
Reference: [TS91] <author> M. Tistarelli and G. </author> <title> Sandini. Direct estimation of time-to-impact from optical flow. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 226-233, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year> <month> 19 </month>
Reference-contexts: e.g. introducing a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, GU91, JH91, Sun91, NL91, HA91, TS91, AD92] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from the two frames [OFT87, Hor90]. Both approaches usually suffer from numerical instabilities in case of noisy data. Feature matching is also very sensitive to occlusions.
References-found: 26

