URL: ftp://ftp.eecs.umich.edu/people/wellman/icml98.ps
Refering-URL: http://ai.eecs.umich.edu/people/wellman/Publications.html
Root-URL: http://www.eecs.umich.edu
Email: fjunling, wellmang@umich.edu  
Title: Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm  
Author: Junling Hu and Michael P. Wellman 
Web: http://ai.eecs.umich.edu/people/fjunling,wellmang  
Affiliation: Artificial Intelligence Laboratory University of Michigan  
Date: July 1998  
Address: Madison, WI, USA,  Ann Arbor, MI 48109-2110, USA  
Note: In Proceedings of the Fifteenth International Conference on Machine Learning (ICML-98), pages 242-250,  
Abstract: In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under specified conditions. This algorithm is useful for finding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to find optimal strategies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Tucker Balch. </author> <title> Learning roles: Behavioral diversity in robot teams. </title> <note> In Sen [11]. </note>
Reference-contexts: As a learning method that does not need a model of its environment and can be used online, reinforcement learning is well-suited for multiagent systems, where agents know little about other agents, and the environment changes during learning. Applications of reinforcement learning in multiagent systems include soccer <ref> [1] </ref>, pursuit games [14, 3] and coordination games [2]. In most of these systems, single-agent reinforcement learning methods are applied without much modification. Such approach treats other agents in the system as a part of the environment, ignoring the difference between responsive agents and passive environment.
Reference: [2] <author> Caroline Claus and Craig Boutilier. </author> <title> The dynamics of reinforcement learning in cooperative multia-gent systems. </title> <note> In Sen [11]. To appear in AAAI-98. </note>
Reference-contexts: Applications of reinforcement learning in multiagent systems include soccer [1], pursuit games [14, 3] and coordination games <ref> [2] </ref>. In most of these systems, single-agent reinforcement learning methods are applied without much modification. Such approach treats other agents in the system as a part of the environment, ignoring the difference between responsive agents and passive environment.
Reference: [3] <author> Edwin De Jong. </author> <title> Non-random exploration bonuses for online reinforcement learning. </title> <note> In Sen [11]. </note>
Reference-contexts: Applications of reinforcement learning in multiagent systems include soccer [1], pursuit games <ref> [14, 3] </ref> and coordination games [2]. In most of these systems, single-agent reinforcement learning methods are applied without much modification. Such approach treats other agents in the system as a part of the environment, ignoring the difference between responsive agents and passive environment.
Reference: [4] <author> Jerzy Filar and Koos Vrieze. </author> <title> Competitive Markov Decision Process. </title> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: In this paper, we propose that a multiagent reinforcement learning method should explicitly take other agents into account. We also propose that a new framework is needed for multiagent reinforcement learning. The framework we adopt is stochastic games (also called Markov games) <ref> [4, 15] </ref>, which are the generalization of the Markov decision processes to the case of two or more controllers. Stochastic games are defined as non-cooperative games, where agents pursue their self-interests and choose their actions independently. Littman [6] has introduced 2-player zero-sum stochastic games for multiagent reinforcement learning. <p> A Nash equilibrium is more plausible and self-enforcing than any other solution concept in such systems. If the payoff structure and state transition probabilities are known to all the agents, we can solve for an Nash equilibrium strategy using a nonlinear programming method proposed by Filar and Vrieze <ref> [4] </ref>. In this paper, we are interested in situations where agents have incomplete information of other agents' payoff functions and the state transition probabilities. We show that an multiagent Q-learning algorithm can be designed, and it converges to the Nash equilibrium Q values under certain restrictions of the game. <p> The following theorem shows that there always exist a Nash equilibrium in stationary strategies for any stochastic game. Theorem 2 (Filar and Vrieze <ref> [4] </ref>, Theorem 4.6.4) Every general-sum discounted stochastic game possesses at least one equilibrium point in stationary strategies. 4.2 Stochastic games and bimatrix games We can view each stage of a stochastic game as a bi-matrix game, as in Figure 2. <p> In order to prove that the convergence point of our Q-learning algorithm is actually the Nash equilibrium point, we need the following theorem proved by Filar and Vrieze <ref> [4] </ref>. Theorem 3 (Filar and Vrieze [4]) The following as sertions are equivalent: 1. <p> In order to prove that the convergence point of our Q-learning algorithm is actually the Nash equilibrium point, we need the following theorem proved by Filar and Vrieze <ref> [4] </ref>. Theorem 3 (Filar and Vrieze [4]) The following as sertions are equivalent: 1.
Reference: [5] <author> Leslie Kaelbling, Michael L. Littman, and An-drew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Reinforcement learning has gained attention and extensive study in recent years <ref> [5, 12] </ref>. As a learning method that does not need a model of its environment and can be used online, reinforcement learning is well-suited for multiagent systems, where agents know little about other agents, and the environment changes during learning.
Reference: [6] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163. </pages> <address> New Brunswick, </address> <year> 1994. </year>
Reference-contexts: The framework we adopt is stochastic games (also called Markov games) [4, 15], which are the generalization of the Markov decision processes to the case of two or more controllers. Stochastic games are defined as non-cooperative games, where agents pursue their self-interests and choose their actions independently. Littman <ref> [6] </ref> has introduced 2-player zero-sum stochastic games for multiagent reinforcement learning. In zero-sum games, one agent's gain is always the other agent's loss, thus agents have strictly opposite interests. In this paper, we adopt the framework of general-sum stochastic games, in which agents need no longer have opposite interests. <p> This is the minimax-Q learning algorithm in u p d a t e 2 r t 2 1 2 ( , , ) a t+1 s t a t 1 s t+1 Q s a a t t t 1 Littman <ref> [6] </ref>. For general-sum games, we cannot use mini-max algorithm because the two agent's payoffs are not the opposite of each other.
Reference: [7] <author> O. L. Mangasarian and H. Stone. </author> <title> Two-person nonzero-sum games and quadratic programming. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 9 </volume> <pages> 348-355, </pages> <year> 1964. </year>
Reference-contexts: Theorem 1 (Nash, 1951) There exists a mixed strategy Nash equilibrium for any finite bimatrix game. A mixed strategy Nash equilibrium for any bimatrix game can be found by Mangasarian-Stone algorithm <ref> [7] </ref>, which is a quadratic programming algorithm. 3 Markov Decision Process and reinforcement learning For comparison purpose, we state the framework of Markov decision process here. Later we can see how the stochastic game framework is related to Markov decision process.
Reference: [8] <author> John F. Nash. </author> <title> Non-cooperative games. </title> <journal> Annals of Mathematics, </journal> <volume> 54 </volume> <pages> 286-295, </pages> <year> 1951. </year>
Reference-contexts: General-sum games include zero-sum games as special cases. In general-sum games, the notions of "optimality" loses its meaning since each agent's payoff depends on other agents' choices. The solution concept Nash equilibrium <ref> [8] </ref> is adopted. In a Nash equilibrium, each agent's choice is the best response to the other agents' choices. Thus, no agent can gain by unilateral deviation. we are interested in the Nash equilibrium solution because we want to design learning agent for noncooperative multiagent systems.
Reference: [9] <author> Martin J. Osborne and Ariel Rubinstein. </author> <title> A Course in Game Theory. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The columns of M k correspond to actions of player 2, a 2 2 A 2 . A 1 and A 2 are the sets of discrete actions of players 1 and 2 respectively. Next, we state some solution concepts for bimatrix games. The main concept is Nash equilibrium <ref> [9] </ref>. In a Nash equilibrium, each agent's action is the best response to other agents' choices.
Reference: [10] <author> Martin L. Puterman. </author> <title> Markov Decision Processes : Discrete Stochastic Dynamic Programming. </title> <publisher> New York : John Wiley & Sons, </publisher> <year> 1994. </year>
Reference-contexts: If the agent knows the reward function and the state transition function, it can solve for fl by some iterative searching methods <ref> [10] </ref>. The learning problem arises when the agent does not know the reward function or the state transition probabilities. Now the agent needs to interact with the environment to find out its optimal policy.
Reference: [11] <editor> Sandip Sen, editor. </editor> <booktitle> Collected papers from the AAAI-97 workshop on multiagent learning. </booktitle> <publisher> AAAI Press, </publisher> <year> 1997. </year>
Reference: [12] <author> Richard S. Sutton and Andrew G. Barto. </author> <title> Reinforcement Learning: An Introduction. </title> <publisher> MIT Press/Bradford Books, </publisher> <month> March </month> <year> 1998. </year>
Reference-contexts: 1 Introduction Reinforcement learning has gained attention and extensive study in recent years <ref> [5, 12] </ref>. As a learning method that does not need a model of its environment and can be used online, reinforcement learning is well-suited for multiagent systems, where agents know little about other agents, and the environment changes during learning.
Reference: [13] <author> Csaba Szepesvari and Michael L. Littman. </author> <title> A unified analysis of value-function-based reinforcement-learning algorithms. </title> <note> submitted for review, </note> <month> December </month> <year> 1997. </year>
Reference-contexts: equilibrium strategy. 1 (s)Q 1 (s) 2 (s) 1 (s)Q 1 (s)^ 2 (s) 8^ 2 (s) 2 1 (s)Q 2 (s) 2 (s) ^ 1 (s)Q 2 (s) 2 (s) 8^ 1 (s) 2 Our convergence proof is based on the following two Lemmas proved by Szepesvari and Littman <ref> [13] </ref>. Lemma 1 (Conditional Average Lemma) Under Assumptions 1-2, the process Q t+1 = (1 ff t )Q t + ff t w t converges to E (w t jh t ; ff t ), where h t is the history at time t. <p> This is required because Nash equilibrium operator is usually not a contraction operator. However, we can probably relax the restriction by proving that a Nash equilibrium operator is a non-expansion operator. Then by the theorem in Szepesvari and Littman <ref> [13] </ref>, the convergence is guaranteed. 6 Future work There are several issues we have not addressed in this paper. The first is the equilibrium selection problem. When there exist multiple Nash equilibria, learning one Nash equilibrium strategy does not guarantee the other agent will choose the same Nash equilibrium.
Reference: [14] <author> Ming Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 330-337, </pages> <address> Amherst, MA, June 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Applications of reinforcement learning in multiagent systems include soccer [1], pursuit games <ref> [14, 3] </ref> and coordination games [2]. In most of these systems, single-agent reinforcement learning methods are applied without much modification. Such approach treats other agents in the system as a part of the environment, ignoring the difference between responsive agents and passive environment.
Reference: [15] <author> Frank Thusijsman. </author> <title> Optimality and Equilibria in Stochastic Games. </title> <publisher> Amsterdam, the Netherlands : Centrum voor Wiskunde en Informatica, </publisher> <year> 1992. </year>
Reference-contexts: In this paper, we propose that a multiagent reinforcement learning method should explicitly take other agents into account. We also propose that a new framework is needed for multiagent reinforcement learning. The framework we adopt is stochastic games (also called Markov games) <ref> [4, 15] </ref>, which are the generalization of the Markov decision processes to the case of two or more controllers. Stochastic games are defined as non-cooperative games, where agents pursue their self-interests and choose their actions independently. Littman [6] has introduced 2-player zero-sum stochastic games for multiagent reinforcement learning.
Reference: [16] <author> Christopher J.C.H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Such approach is called model-based reinforcement learning. The agent can also directly learn about its optimal policy without knowing the reward function or the state transition function. Such approach is called model-free reinforcement learning. One of the model-free reinforcement learning methods is Q learning <ref> [16] </ref>. <p> The learning rate ff t needs to decay over time in order for the learning algorithm to converge. Watkins and Dayan <ref> [16] </ref> proved that sequence (6) converges to the optimal Q fl (s; a). 4 The stochastic game framework Markov decision process (MDP) is a single agent decision problem. A natural extension of MDP to mul-tiagent systems is stochastic games, which essentially are n-agent Markov decision processes.
References-found: 16

