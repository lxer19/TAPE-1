URL: ftp://wol.ra.phy.cam.ac.uk/pub/www/mackay/nips.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  
Title: Ensemble Learning and Evidence Maximization  
Author: David J.C. MacKay 
Date: 1995  
Note: May 1995|Submitted to NIPS  
Address: Madingley Road, Cambridge CB3 0HE United Kingdom  
Affiliation: Cavendish Laboratory  
Abstract: Ensemble learning by variational free energy minimization is a tool introduced to neural networks by Hinton and van Camp in which learning is described in terms of the optimization of an ensemble of parameter vectors. The optimized ensemble is an approximation to the posterior probability distribution of the parameters. This tool has now been applied to a variety of statistical inference problems. In this paper I study a linear regression model with both parameters and hyper-parameters. I demonstrate that the evidence approximation for the optimization of regularization constants can be derived in detail from a free energy minimization view point.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. </author> <title> (1995) The Helmholtz machine. Neural Computation. </title> <note> to appear. </note>
Reference-contexts: Hinton and Zemel (1994) view the input-to-hidden `recognition' part of the autoencoder as defining an approximating distribution Q for this distribution P . A single objective function F can then be defined for simultaneous optimization of the generative model and the recognition model. The Helmholtz machine <ref> (Dayan et al. 1995) </ref> is a further generalization of these ideas. In a broader statistical context, Neal and Hinton (1993) have shown that it is possible to view the Expectation-Maximization (EM) algorithm in terms of a free energy minimization.
Reference: <author> Feynman, R. P. </author> <title> (1972) Statistical Mechanics. </title> <editor> W. A. </editor> <publisher> Benjamin, Inc. </publisher>
Reference-contexts: F () can be viewed as the sum of log P (DjH) and the Kullback 1 Variational free energy minimization is a well-established tool in statistical physics <ref> (Feynman 1972) </ref>; `mean field theory' is an important special case. The free energy can also be described in terms of description lengths. 1 Leibler divergence between Q (w; ) and P (wjD; H).
Reference: <author> Gull, S. F. </author> <title> (1988) Bayesian inductive inference and maximum entropy. </title> <booktitle> In Maximum Entropy and Bayesian Methods in Science and Engineering, </booktitle> <volume> vol. 1: </volume> <booktitle> Foundations, </booktitle> <editor> ed. by G. Erickson and C. </editor> <volume> Smith, </volume> <pages> pp. 53-74, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference: <author> Gull, S. F. </author> <title> (1989) Developments in maximum entropy data analysis. In Maximum Entropy and Bayesian Methods, Cambridge 1988 , ed. by J. </title> <booktitle> Skilling, </booktitle> <pages> pp. 53-71, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: The variables ff and fi are known as hyperparameters. Problems for which models can be written in the form (2) include linear interpolation with a fixed basis set (Gull 1988; MacKay 1992a), non-linear regression with a neural network (MacKay 1992b), and image deconvolution <ref> (Gull 1989) </ref>.
Reference: <author> Hinton, G. E., and van Camp, D., </author> <title> (1993) Keeping neural networks simple by minimizing the description length of the weights. </title> <booktitle> In: Proceedings of COLT-93. </booktitle>
Reference: <author> Hinton, G. E., and Zemel, R. S. </author> <year> (1994) </year> <month> Autoencoders, </month> <title> minimum description length and Helmholtz free energy. </title> <booktitle> In Advances in Neural Information Processing Systems 6 , ed. </booktitle> <editor> by J. D. Cowan, G. Tesauro, and J. Alspector, </editor> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> MacKay, D. J. C. </author> <title> (1992a) Bayesian interpolation. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 415-447. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1992b) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: In the Bayesian interpretation, these optimized parameters are viewed as defining the mode of a posterior probability distribution P (wjD; H) (given data D and model assumptions H), which can be approximated, with a Gaussian distribution ~ P for example <ref> (MacKay 1992b) </ref>, in order to obtain predictive distributions and optimize model control parameters. <p> The variables ff and fi are known as hyperparameters. Problems for which models can be written in the form (2) include linear interpolation with a fixed basis set (Gull 1988; MacKay 1992a), non-linear regression with a neural network <ref> (MacKay 1992b) </ref>, and image deconvolution (Gull 1989).
Reference: <author> MacKay, D. J. C., </author> <year> (1994) </year> <month> Hyperparameters: </month> <title> Optimize, or integrate out? Submitted to Neural Computation. </title>
Reference-contexts: This contrasts with the result p 2=fl from the evidence framework. 2. This free energy approximation for Q w (w) fails to produce the small order correction terms identified in <ref> (MacKay 1994) </ref>, which arise because of the uncertainty in ff. It will be interesting to investigate whether a more complex approximating distribution Q might capture these terms. 2.9 Discussion This result gives insight into the properties of both the evidence framework and ensemble learning.
Reference: <author> MacKay, D. J. C. </author> <title> (1995a) Free energy minimization algorithm for decoding and cryptanalysis. </title> <journal> Electronics Letters 31 (6): </journal> <pages> 446-447. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1995b) </year> <month> Hyperparameters: </month> <title> Optimize, or integrate out? In Maximum Entropy and Bayesian Methods, </title> <editor> Santa Barbara 1993 , ed. by G. Heidbreder, </editor> <publisher> Dordrecht. Kluwer. </publisher>
Reference: <author> Neal, R. M., and Hinton, G. E. </author> <title> (1993) A new view of the EM algorithm that justifies incremental and other variants. </title> <journal> Biometrika. </journal> <note> submitted. </note>
Reference: <author> Wolpert, D. H. </author> <title> (1993) On the use of evidence in neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 5 , ed. </booktitle> <editor> by C. L. Giles, S. J. Hanson, and J. D. </editor> <booktitle> Cowan, </booktitle> <pages> pp. 539-546, </pages> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 8 </pages>
References-found: 13

