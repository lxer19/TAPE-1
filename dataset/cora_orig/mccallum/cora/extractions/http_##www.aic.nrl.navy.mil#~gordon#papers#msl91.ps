URL: http://www.aic.nrl.navy.mil/~gordon/papers/msl91.ps
Refering-URL: http://www.aic.nrl.navy.mil/~gordon/pubs.html
Root-URL: 
Title: Adaptive Strategy Selection for Concept Learning  
Author: William M. Spears and Diana F. Gordon 
Keyword: Key words: concept learning, genetic algorithms  
Address: Washington, D.C. 20375  
Affiliation: Naval Research Laboratory  
Abstract: In this paper, we explore the use of genetic algorithms (GAs) to construct a system called GABIL that continually learns and refines concept classification rules from its interac - tion with the environment. The performance of this system is compared with that of two other concept learners (NEWGEM and C4.5) on a suite of target concepts. From this comparison, we identify strategies responsible for the success of these concept learners. We then implement a subset of these strategies within GABIL to produce a multistrategy concept learner. Finally, this multistrategy concept learner is further enhanced by allowing the GAs to adaptively select the appropriate strategies. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Back, T., F. Hoffmeister, and H. Schwefel, </author> <title> A Survey of Evolution Strategies, </title> <booktitle> Proc. 4th Int'l Conference on Genetic Algorithms and their Applications, </booktitle> <year> 1991. </year>
Reference-contexts: For this rule set the dropping condition strategy is permissible, while the adding alterna - tive strategy is not. The GA is allowed to search the space of strategies and the space of hypotheses in parallel (see also <ref> (Back et. al., 1991) </ref> for related work in GAs). The resulting adaptive system, which we call ``adaptive GABIL'', was run on the nDmC and BC target concepts. The results are presented in Table 5.
Reference: <author> Booker, L., </author> <title> Triggered Rule Discovery in Classifier Systems, </title> <booktitle> Proc. 3rd Int'l Conference on Genetic Algorithms and their Applications, </booktitle> <year> 1989. </year>
Reference-contexts: Consider the following example of a rule set with 2 disjuncts: F1 F2 F1 F2 This rule set is equivalent to: ____________________________________ 2 Previous GA concept learners have used the Michigan approach. See (Wilson, 1987) and <ref> (Booker, 1989) </ref> for details. if [(F1 = small) & (F2 = sphere)] v [(F1 = medium v large) & (F2 = cube)] then it is a widget. 2.4 Crossover and mutation Our goal was to achieve a representation that requires minimal changes to the fundamental genetic operators (crossover and mutation).
Reference: <author> Braudaway, W. and C. Tong, </author> <title> Automated Synthesis of Constrained Generators, </title> <booktitle> Proc. of the Sixth International Workshop on Machine Learning, </booktitle> <year> 1989. </year> <title> De Jong, K, Using Genetic Algorithms to Search Program Spaces, </title> <booktitle> Proc. 2nd Int'l Conference on Genetic Algorithms and their Applications, </booktitle> <year> 1987. </year>
Reference-contexts: Concept learning preferences have traditionally been implemented using a generate-and-test approach (i.e., generate hypotheses then select those that meet the preference criteria). Our approach consists of strategies that examine the criteria (test) and then fire the operator (generate). This test-and-generate approach, which is similar to that of test incorporation <ref> (Braudaway and Tong, 1989) </ref>, can be considerably more efficient than generate-and-test. Although the level of granularity of generali - zation operators is finer than that of strategies such as induction and analogy, the issues at any level of granularity are remarkably similar.
Reference: <author> De Jong, K. and W. Spears, </author> <title> Learning Concept Classification Rules Using Genetic Algorithms, </title> <booktitle> Proc. International Joint Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: It does not do superbly on any particular concept, but it also does not have a distinct region of the space of concepts on which it ____________________________________ 4 An explanation of the difficulty of systems based on ID3 on target concepts of this type is in <ref> (De Jong and Spears, 1991) </ref>. _________________________________ Prediction Accuracy _________________________________ _________________________________ TC GEM C4.5 GABIL _________________________________ 1D1C 99.78 98.50 95.24 _________________________________ 1D3C 97.43 98.48 95.73 _________________________________ 2D2C 96.81 94.27 92.67 _________________________________ 3D1C 97.98 78.77 90.40 _________________________________ 3D3C 95.30 95.38 92.80 _________________________________ 4D2C 93.81 90.53 87.40 _________________________________ Average 96.63 91.23 92.11 _________________________________ _________________________________
Reference: <author> Goldberg, D., </author> <title> Genetic Algorithms in Search, Optimization & Machine Learning, </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1989. </year>
Reference-contexts: Both components are critical to the successful applica tion of the GAs to the problem of interest. 1 ____________________________________ 1 Excellent introductions to GAs can be found in (Holland, 1975) and <ref> (Goldberg, 1989) </ref>. 2.1 Representing the search space The traditional internal representation of GAs involves using fixed-length (generally binary) strings to represent points in the space to be searched.
Reference: <author> Gordon, D., </author> <title> Active Bias Adjustment for Incremental, Supervised Concept Learning. </title> <type> Ph.D. thesis, </type> <institution> University of Maryland, College Park, MD., </institution> <year> 1990. </year>
Reference-contexts: Therefore, it is difficult to choose optimal combinations of strategies prior to the concept learning task. In response, we have modified GABIL to adaptively shift between strategies when appropriate. Adaptive strategy selection, in our context, is similar to dynamic preference (bias) adjustment for concept learning (see <ref> (Gordon, 1990) </ref> for related literature). GABIL is the first system to perform dynamic preference adjustments for concept learning using genetic algorithms, which are naturally suited to adaptive tasks. <p> This operator drops a feature from a disjunct. For example, if the disjunct is [(F1 = small v medium) & (F2 = sphere)] then dropping condition might create the new disjunct [(F2 = sphere)]. The criterion for this strategy, which is based on a criterion from <ref> (Gordon, 1990) </ref>, examines the bits of each feature in each disjunct. If most (i.e., more than half) of the bits of a feature in a disjunct are 1s, then the remaining 0 bits are changed to 1s.
Reference: <author> Holland, J., </author> <title> Adaptation in Natural and Artificial Systems, </title> <publisher> The University of Michi-gan Press, </publisher> <year> 1975. </year>
Reference-contexts: Both components are critical to the successful applica tion of the GAs to the problem of interest. 1 ____________________________________ 1 Excellent introductions to GAs can be found in <ref> (Holland, 1975) </ref> and (Goldberg, 1989). 2.1 Representing the search space The traditional internal representation of GAs involves using fixed-length (generally binary) strings to represent points in the space to be searched.
Reference: <author> Holland, J., </author> <title> Escaping Brittleness: The Possibilities of General-Purpose Learning Algorithms Applied to Parallel Rule-Based Systems. </title> <editor> In R. Michalski, J. Carbonell, </editor> <booktitle> and T. </booktitle>
Reference: <editor> Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol. </booktitle> <volume> 2). </volume> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Los Altos, CA., </address> <year> 1986. </year>
Reference: <author> Michalski, R., </author> <title> A Theory and Methodology of Inductive Learning. </title> <editor> In R. Michalski, J. Car-bonell, and T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol. </booktitle> <volume> 1). </volume> <publisher> Tioga Publishing Co., </publisher> <address> Palo Alto, CA., </address> <year> 1983. </year>
Reference-contexts: The choice of generalization operators and the frequency and conditions under which they fire will have a global effect on the type of concept learning that is done. Therefore, we can consider these strategies to be learning preferences (see <ref> (Michalski, 1983) </ref>), which are also called inductive biases. Concept learning preferences have traditionally been implemented using a generate-and-test approach (i.e., generate hypotheses then select those that meet the preference criteria). Our approach consists of strategies that examine the criteria (test) and then fire the operator (generate). <p> The result is a modified DNF that allows internal disjunction. (See <ref> (Michalski, 1983) </ref> for a discussion of internal disjunction.) With these restrictions we can now construct a fixed-length internal representation for classification rules. Each fixed-length rule will have N feature tests, one for each feature. <p> For the BC target concept, the population is 100. In addition to GABIL, two well-known concept learners had their performance evaluated on the nDmC and BC target concepts: NEWGEM (Mozetic, 1985), which is based on the AQ algorithm described in <ref> (Michalski, 1983) </ref> and is also called AQ14, and C4.5 (Quinlan, unpublished). The C4.5 system is based on the ID algorithm described in (Quinlan, 1986). All systems are run in batch-incremental mode. NEWGEM, like AQ, generates classification rules from instances using a beam search. <p> This strategy, which we call the dropping condition strategy, drops a feature (i.e., condition) from a disjunct if it seems to be nearly irrelevant within that disjunct. The operator of this dropping condition strategy is based on the generalization operator dropping condition described in <ref> (Michalski, 1983) </ref>. This operator drops a feature from a disjunct. For example, if the disjunct is [(F1 = small v medium) & (F2 = sphere)] then dropping condition might create the new disjunct [(F2 = sphere)]. <p> The purpose of this second strategy is to increase the generality of the inductive hypotheses. This strategy, which we call the adding alternative strategy, uses an operator that is based on the adding alternative operator of <ref> (Michalski, 1983) </ref>. This operator generalizes by adding a disjunct (i.e., alternative) to the current classification rule. The most useful form of this operator, according to (Michal-ski, 1983) is when an internal disjunct is added.
Reference: <author> Michalski, R., I. Mozetic, J. Hong, and Lav-rac, N., </author> <title> The AQ15 inductive learning system: An overview and experiments. </title> <institution> University of Illinois Technical Report Number UIUCDCS-R-86-1260, </institution> <year> 1986. </year>
Reference-contexts: For each concept, the 256 examples were randomly shuffled and then presented sequentially in batch-incremental mode. This procedure was repeated 10 times (trials) for each concept and learning algorithm pair. For Domain 2, we used a well-known natural database designed for diagnosing breast cancer <ref> (Michalski et. al., 1986) </ref>. This database has descriptions of cases for 286 patients, and each case (instance) is described in terms of 9 features. There is a small amount of noise in the database.
Reference: <author> Mozetic, I., NEWGEM: </author> <title> Program for learning from examples, program documentation and user's guide. </title> <institution> University of Illinois Report Number UIUCDCS-F-85-949, </institution> <year> 1985. </year>
Reference-contexts: For the BC target concept, the population is 100. In addition to GABIL, two well-known concept learners had their performance evaluated on the nDmC and BC target concepts: NEWGEM <ref> (Mozetic, 1985) </ref>, which is based on the AQ algorithm described in (Michalski, 1983) and is also called AQ14, and C4.5 (Quinlan, unpublished). The C4.5 system is based on the ID algorithm described in (Quinlan, 1986). All systems are run in batch-incremental mode.
Reference: <author> Quinlan, J., </author> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> Volume 1, Number 1, </volume> <year> 1986. </year>
Reference-contexts: The C4.5 system is based on the ID algorithm described in <ref> (Quinlan, 1986) </ref>. All systems are run in batch-incremental mode. NEWGEM, like AQ, generates classification rules from instances using a beam search.
Reference: <author> Quinlan, J., </author> <title> Documentation and User's Guide for C4.5. </title> <type> (unpublished), </type> <year> 1989. </year>
Reference: <author> Rendell, L., </author> <title> Genetic Plans and the Probabilistic Learning System: Synthesis and Results. </title> <booktitle> Proc. 1st Int'l Conference on Genetic Algorithms and their Applications. </booktitle> <year> 1985. </year>
Reference-contexts: There are two general approaches one might take to resolve this issue. The first involves changing the fundamental GA operators (crossover and mutation) to work effectively with complex non-string objects <ref> (Rendell, 1985) </ref>. This must be done carefully in order to preserve the properties that make the GAs effective adaptive search procedures (see (DeJong, 1987) for a more detailed discussion). Alternatively, one can attempt to construct a string representation that minimizes any changes to the GAs.
Reference: <author> Smith, S., </author> <title> Flexible Learning of Problem Solving Heuristics Through Adaptive Search, </title> <booktitle> Proc. 8th IJCAI, </booktitle> <year> 1983. </year>
Reference-contexts: There are currently two basic strategies: the Michigan approach exemplified by Holland's classifier system (Holland, 1986), and the Pittsburgh approach exemplified by Smith's LS-1 system <ref> (Smith, 1983) </ref>. Systems using the Michigan approach maintain a population of individual rules that compete with each other for space and priority in the population.
Reference: <author> Wilson, S., </author> <title> Quasi-Darwinian Learning in a Classifier System, </title> <booktitle> Proc. of the 4th Int'l Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufman Publishing, 1987 Valiant, </publisher> <editor> L., </editor> <title> A Theory of the Learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27, </volume> <year> 1984. </year>
Reference-contexts: Consider the following example of a rule set with 2 disjuncts: F1 F2 F1 F2 This rule set is equivalent to: ____________________________________ 2 Previous GA concept learners have used the Michigan approach. See <ref> (Wilson, 1987) </ref> and (Booker, 1989) for details. if [(F1 = small) & (F2 = sphere)] v [(F1 = medium v large) & (F2 = cube)] then it is a widget. 2.4 Crossover and mutation Our goal was to achieve a representation that requires minimal changes to the fundamental genetic operators (crossover
References-found: 17

