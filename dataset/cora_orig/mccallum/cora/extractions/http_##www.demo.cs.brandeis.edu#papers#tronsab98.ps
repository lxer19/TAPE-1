URL: http://www.demo.cs.brandeis.edu/papers/tronsab98.ps
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Abstract: We show an artificial world where animals (humans) and animats (software agents) interact in a coevolutionary arms race. The two species each use adaptation schemes of their own. Learning through interaction with humans has been out of reach for evolutionary learning techniques because too many iterations are necessary. Our work demonstrates that the Internet is a new environment where this may be possible through an appropriate setup that creates mutualism, a relationship where human and animat species benefit from their interactions with each other.
Abstract-found: 1
Intro-found: 1
Reference: <author> Axelrod, R. M. </author> <year> (1984). </year> <title> The Evolution of Cooperation. </title> <address> New York, </address> <publisher> Basic Books. </publisher>
Reference: <author> Beasley, D., Bull, D. R. and Martin, R. R. </author> <year> (1993). </year> <title> A sequential niche technique for multimodal function optimization. </title> <booktitle> Evolutionary Computation 1(2). </booktitle> <pages> 101-125. </pages>
Reference: <editor> Gilbert, L. E. and Raven, P. H. (eds.) </editor> <booktitle> (1975) Coevolution of animals and Plants. </booktitle> <institution> University of Texas Press. </institution>
Reference-contexts: In the future, more and more such environments will contain software agents that interact with human users and adapt according to the behavior displayed in those interactions (Lieberman, 1997). Such a bi-adaptive relationship could be considered a form of mutualism <ref> (Gilbert and Raven, 1975) </ref>, as both humans and agents participating have their own goals and adaptation strategies. We have built a coevolutionary environment with a real-time computer game, implemented in Java, that matches artificial agents (animats or robots) against human (animal) Internet users.
Reference: <author> Cliff, D. and Miller, G. F. </author> <year> (1995). </year> <title> Tracking the Red Queen: Measurements of adaptive progress in co-evolutionary simulations. </title>
Reference-contexts: Smoothing obtained by convolution with , normalized. a = 1024. e a 2 e a 2 learning from the data in fig. 7 in order to visualize the raw improvement of the animats. The technique of sampling suggested by <ref> (Cliff and Miller, 1995) </ref> cannot be used in this case; it is not possible to play for example player H0001 from De-cember 10th against robot R230001 because the human of that day is not available. Instead, we apply an averaging technique.
Reference: <editor> In F. Moran, A. Moreno, J. J. Merelo and P. Cachon (eds.) </editor> <booktitle> Advances in Artificial Life: Proceedings of the Third European Conference on Artificial Life (ECAL95). Lecture Notes in Artificial Intelligence 929, </booktitle> <publisher> Springer-Verlag, pp.200-218. </publisher>
Reference: <author> Darwen, P. J. </author> <year> (1996). </year> <title> Co-evolutionary Learning by Automatic Mod-ularisation with Speciation. </title> <institution> University of New South Wales, </institution> <year> 1996. </year>
Reference: <author> Floreano, D. and Mondada, F. </author> <year> (1994). </year> <title> Automatic Creation of an Autonomous Agent: Genetic Evolution of a Neural Network Driven Robot. </title> <editor> In D. Cliff, P. Husbands, J.-A. Meyer, </editor> <publisher> and S. </publisher>
Reference-contexts: Robots that are reliable enough can run repeated trials of the same experiment over a long time in order to learn using evolutionary computation techniques. Floreano and Mondada <ref> (Floreano and Mondada, 1994, 1996) </ref> run their robots for several days in order to evolve controllers for basic tasks.
Reference: <editor> Wilson (Eds.), </editor> <booktitle> From Animals to Animats III, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Floreano, D. and Mondada, F. </author> <year> (1996). </year> <title> Evolution of Homing Navigation in a Real Mobile Robot. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics--Part B: Cybernetics, </journal> <volume> 26(3), </volume> <pages> 396-407. </pages>
Reference: <author> Funes, </author> <title> Pablo (1996). The Tron Game: An experiment in Artificial Life and Evolutionary Techniques. </title> <note> Unpublished. </note>
Reference-contexts: Every sensor returns a maximum value of 1 for an immediate obstacle (i.e. a wall in an adjacent pixel), a lower number for an obstacle further away, and 0 when there are no walls in sight. In earlier exploratory experiments <ref> (Funes, 1996) </ref>, we used a Genetic Algorithm (GA) to learn the weights of a perceptron network that played Tron.
Reference: <author> Goldberg, David E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Hillis, W. D. </author> <year> (1992). </year> <title> Co-evolving parasites improve simulated evolution as an optimization procedure. </title> <editor> In Langton, C. et al. (Eds.) </editor> <booktitle> Artificial Life II, </booktitle> <publisher> Addison-Wesley. </publisher> <pages> pp. 313-324. </pages>
Reference-contexts: In evolutionary computation, the term coevolution has been used to describe any iterated adaptation involving arms races, either between learning species or between a learner and its learning environment. Examples of coevolutionary learning include the pioneering work by Hillis on sorting networks <ref> (Hillis, 1992) </ref>, Backgammon learning (Tesauro, 1992, Pollack et al., 1996, Pollack and Blair, 1997), predator/prey games (Reynolds, 1994, Miller and Cliff, 1994, Miller and Cliff, 1996) and spacial distribution problems (Juill and Pol-lack, 1996, Juill and Pollack, 1996b). We use coevolutionary programming techniques to maintain our robot populations.
Reference: <author> Holland, John H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Michigan Press. </publisher>
Reference-contexts: Evolutionary learning calls for three basic ingredients: (a) a representation that is capable of encoding each candidate solution, (b) reproductive operators that can be applied to such a representation, and (c) a fitness function that is the standard measure against which to test each candidate solution during the iterative process <ref> (Holland, 1975, Goldberg, 1989) </ref>. In natural as in artificial evolution, a population moves toward fitness optimality while maintaining variation over all the dimensions of the genetic space, including those dimensions that are not being selected.
Reference: <author> Juill, H. and Pollack, J. </author> <year> (1996). </year> <title> Dynamics of Co-evolutionary Learning. </title> <booktitle> In Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher> <pages> pp 526-534. </pages>
Reference-contexts: Examples of coevolutionary learning include the pioneering work by Hillis on sorting networks (Hillis, 1992), Backgammon learning (Tesauro, 1992, Pollack et al., 1996, Pollack and Blair, 1997), predator/prey games (Reynolds, 1994, Miller and Cliff, 1994, Miller and Cliff, 1996) and spacial distribution problems <ref> (Juill and Pol-lack, 1996, Juill and Pollack, 1996b) </ref>. We use coevolutionary programming techniques to maintain our robot populations. Two types of coevolution are involved: robot vs. robot in our background server and robot vs. human in our foreground server.
Reference: <author> Juill, H. and Pollack, J. </author> <year> (1996b). </year> <title> Co-evolving Intertwined Spirals. </title> <booktitle> in Proceedings of the Fifth Annual Conference on Evolutionary Programming, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Supporters of the Genetic Programming (GP) paradigm (Koza, 1992) suggest that this may be the case for GP as well (Rosca, 1996). <ref> (Juill and Pol-lack, 1996b) </ref> argues that the dynamics of coevolutionary fitness help to get perspicacious solutions to a problem of recognizing 194 points arranged in a spiral pattern.
Reference: <author> Koza, John R. </author> <year> (1992). </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Neural networks are thought to have generalization capabilities (Wolpert, 1990, Darwen, 1996), successfully inducing, for example, a good backgammon player from a set of suggested moves (Tesauro, 1989). Supporters of the Genetic Programming (GP) paradigm <ref> (Koza, 1992) </ref> suggest that this may be the case for GP as well (Rosca, 1996). (Juill and Pol-lack, 1996b) argues that the dynamics of coevolutionary fitness help to get perspicacious solutions to a problem of recognizing 194 points arranged in a spiral pattern. <p> While learning backgammon (Tesauro, 1995, Pol-lack et al., 1996) is a success for coevolution, the same approach has failed in most other cases. Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. <ref> (Koza, 1992) </ref> and others (Rosca, 1996) evolved players for the game of Pacman. There has been important research in pursuer-evader games (Reynolds, 1994, Miller and Cliff, 1994, 1996) as well as contests in simulated physics environments (Sims, 1994). <p> This form of collusion is a frequent suboptimal equilibrium that prohibits artificial learning through self-play. 2.1 System Overview In our present work, we are using Genetic Programming (GP) <ref> (Koza, 1992) </ref> as a means for coding artificial Tron players. The set of terminals is -_A,_B,..., _H (the eight sensors) and (random constants between 0 and 1)-. The functions are -+, -, * (arithmetic operations),% (safe division), IFLTE (if a b then-else), RIGHT (turn right) and LEFT (turn left)-.
Reference: <author> Lieberman, H. </author> <year> (1997). </year> <title> Autonomous Interface Agents, </title> <booktitle> ACM Conference on Human-Computer Interface [CHI-97], </booktitle> <address> Atlanta. </address>
Reference-contexts: In the future, more and more such environments will contain software agents that interact with human users and adapt according to the behavior displayed in those interactions <ref> (Lieberman, 1997) </ref>. Such a bi-adaptive relationship could be considered a form of mutualism (Gilbert and Raven, 1975), as both humans and agents participating have their own goals and adaptation strategies.
Reference: <author> Mataric, M and Cliff, D. </author> <year> (1996). </year> <title> Challenges In Evolving Controllers for Physical Robots. In Evolutionary Robotics, </title> <journal> special issue of Robotics and Autonomous Systems, </journal> <volume> Vol. 19, No. 1. </volume> <pages> 67-83. </pages>
Reference: <author> Miller, G. F. and Cliff, D. </author> <year> (1994). </year> <title> Protean Behavior in Dynamic Games: Arguments for the Co-Evolution of Pursuit-Evasion Tactics. </title> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior (SAB94). </booktitle> <editor> D Cliff, P. Husbands, J.-A Meyer and S W Wilson, eds. </editor> <publisher> MIT Press Bradford Books, pp.411--420. </publisher>
Reference-contexts: Examples of coevolutionary learning include the pioneering work by Hillis on sorting networks (Hillis, 1992), Backgammon learning (Tesauro, 1992, Pollack et al., 1996, Pollack and Blair, 1997), predator/prey games <ref> (Reynolds, 1994, Miller and Cliff, 1994, Miller and Cliff, 1996) </ref> and spacial distribution problems (Juill and Pol-lack, 1996, Juill and Pollack, 1996b). We use coevolutionary programming techniques to maintain our robot populations. <p> Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. (Koza, 1992) and others (Rosca, 1996) evolved players for the game of Pacman. There has been important research in pursuer-evader games <ref> (Reynolds, 1994, Miller and Cliff, 1994, 1996) </ref> as well as contests in simulated physics environments (Sims, 1994).
Reference: <author> Miller, G. F. and Cliff, D. </author> <year> (1996). </year> <title> Co-evolution of Pursuit and Evasion II: Simulation Methods and Results. </title> <booktitle> In Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher> <pages> pp 506-515. </pages>
Reference: <author> Pollack, J. B., and Blair, </author> <title> A.D. </title> <booktitle> (1997). Why did TD-Gammon work? Advances in Neural Information Processing Systems 9. </booktitle> <pages> 10-16. </pages>
Reference-contexts: It became evident that while this simple architecture is capable of coding players that could perform interestingly when facing human opponents, such good weights were difficult to find in evolutionary or coevolutionary scenarios. Collusion <ref> (Pollack and Blair, 1997) </ref> was likely to appear in most evolutionary runs in the form of live and let live strategies such as that shown in Tron players make tight spirals in order to stay as far from the opponent as possible.
Reference: <author> Pollack, J. B., Blair, A. and Land, M.(1996). </author> <title> Coevolution of A Backgammon Player. </title> <booktitle> Proceedings Artificial Life V, </booktitle> <editor> C. Langton, (Ed), </editor> <publisher> MIT Press. </publisher>
Reference-contexts: Examples of coevolutionary learning include the pioneering work by Hillis on sorting networks (Hillis, 1992), Backgammon learning (Tesauro, 1992, Pollack et al., 1996, Pollack and Blair, 1997), predator/prey games (Reynolds, 1994, Miller and Cliff, 1994, Miller and Cliff, 1996) and spacial distribution problems <ref> (Juill and Pol-lack, 1996, Juill and Pollack, 1996b) </ref>. We use coevolutionary programming techniques to maintain our robot populations. Two types of coevolution are involved: robot vs. robot in our background server and robot vs. human in our foreground server.
Reference: <author> Reynolds, C.W. </author> <year> (1994). </year> <title> Competition, Coevolution and the Game of Tag, </title> <booktitle> Proceedings of Artificial Life IV. </booktitle> <editor> R. Brooks and P. Maes, eds. </editor> <publisher> MIT Press. </publisher>
Reference-contexts: Examples of coevolutionary learning include the pioneering work by Hillis on sorting networks (Hillis, 1992), Backgammon learning (Tesauro, 1992, Pollack et al., 1996, Pollack and Blair, 1997), predator/prey games <ref> (Reynolds, 1994, Miller and Cliff, 1994, Miller and Cliff, 1996) </ref> and spacial distribution problems (Juill and Pol-lack, 1996, Juill and Pollack, 1996b). We use coevolutionary programming techniques to maintain our robot populations. <p> Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. (Koza, 1992) and others (Rosca, 1996) evolved players for the game of Pacman. There has been important research in pursuer-evader games <ref> (Reynolds, 1994, Miller and Cliff, 1994, 1996) </ref> as well as contests in simulated physics environments (Sims, 1994).
Reference: <author> Rosca, J. P. </author> <year> (1996). </year> <title> Generality versus Size in Genetic Programming. </title> <booktitle> Proceedings of the Genetic Programming 1996 Conference (GP-96). </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: Neural networks are thought to have generalization capabilities (Wolpert, 1990, Darwen, 1996), successfully inducing, for example, a good backgammon player from a set of suggested moves (Tesauro, 1989). Supporters of the Genetic Programming (GP) paradigm (Koza, 1992) suggest that this may be the case for GP as well <ref> (Rosca, 1996) </ref>. (Juill and Pol-lack, 1996b) argues that the dynamics of coevolutionary fitness help to get perspicacious solutions to a problem of recognizing 194 points arranged in a spiral pattern. <p> While learning backgammon (Tesauro, 1995, Pol-lack et al., 1996) is a success for coevolution, the same approach has failed in most other cases. Real-time, interactive games (e.g. video games) have distinctive features that differentiate them from the better known board games. (Koza, 1992) and others <ref> (Rosca, 1996) </ref> evolved players for the game of Pacman. There has been important research in pursuer-evader games (Reynolds, 1994, Miller and Cliff, 1994, 1996) as well as contests in simulated physics environments (Sims, 1994).
Reference: <author> Rosin, C. D. </author> <year> (1997). </year> <title> Coevolutionary Search Among Adversaries. </title> <type> Ph.D. thesis, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference-contexts: The new training set T is initialized to the empty set and then new members are added one at a time, choosing the highest according to the following shared fitness function: (3) This selection function is adapted from <ref> (Rosin, 1997) </ref> and acts to decrease the relevance of a case that has already been covered, that is, when there is already a player in the training set that beats it.
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some Studies in Machine Learning Using the Game of Checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 3, </volume> <pages> 210-229. </pages>
Reference-contexts: The GP function they obtain indeed defines two roughly spiral surfaces that continue outside the boundary of the original test points. 1.7 Learning game playing Game playing is one of the traditional domains of AI research. Ever since Samuels early experiments with checkers <ref> (Samuel, 1959) </ref>, we have hoped that the computer would be able to make good use of experience, improving its skills by learning from its mistakes and successes.
Reference: <author> Sims, K. </author> <year> (1994). </year> <title> Evolving 3D Morphology and Behavior by Competition. </title> <booktitle> Artificial Life IV Proceedings, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: There has been important research in pursuer-evader games (Reynolds, 1994, Miller and Cliff, 1994, 1996) as well as contests in simulated physics environments <ref> (Sims, 1994) </ref>.
Reference: <author> Tesauro, G. </author> <year> (1989). </year> <title> Neurogammon Wins Computer Olympiad. </title> <booktitle> Neural Computation I, </booktitle> <pages> 321-323. </pages>
Reference-contexts: Neural networks are thought to have generalization capabilities (Wolpert, 1990, Darwen, 1996), successfully inducing, for example, a good backgammon player from a set of suggested moves <ref> (Tesauro, 1989) </ref>. <p> In his work with the game of backgammon, Tesauro began collecting samples from human games to provide a fitness measure for training neural networks <ref> (Tesauro, 1989) </ref>. Later, he abandoned this methodology and used introspective self-play (Tesauro, 1995). Of the earlier approach, he argued that building human expertise into an evaluation function [...] has been found to be an extraordinarily difficult undertaking (p. 59).
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <booktitle> In Machine Learning. </booktitle> <volume> 8 </volume> <pages> 257-277. </pages>
Reference-contexts: In evolutionary computation, the term coevolution has been used to describe any iterated adaptation involving arms races, either between learning species or between a learner and its learning environment. Examples of coevolutionary learning include the pioneering work by Hillis on sorting networks (Hillis, 1992), Backgammon learning <ref> (Tesauro, 1992, Pollack et al., 1996, Pollack and Blair, 1997) </ref>, predator/prey games (Reynolds, 1994, Miller and Cliff, 1994, Miller and Cliff, 1996) and spacial distribution problems (Juill and Pol-lack, 1996, Juill and Pollack, 1996b). We use coevolutionary programming techniques to maintain our robot populations.
Reference: <author> Tesauro, G. </author> <title> (1995) Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38(3): </volume> <pages> 58-68. </pages>
Reference-contexts: In his work with the game of backgammon, Tesauro began collecting samples from human games to provide a fitness measure for training neural networks (Tesauro, 1989). Later, he abandoned this methodology and used introspective self-play <ref> (Tesauro, 1995) </ref>. Of the earlier approach, he argued that building human expertise into an evaluation function [...] has been found to be an extraordinarily difficult undertaking (p. 59). Learning to play a game by self-play involves a problem of transfer as well. <p> The fitness landscape (even in the coevolutionary case, where the landscape is redefined in every generation) might be an insufficient sample of the larger problem defined by the whole game and the way humans approach it. While learning backgammon <ref> (Tesauro, 1995, Pol-lack et al., 1996) </ref> is a success for coevolution, the same approach has failed in most other cases.
Reference: <author> Wolpert, D. H. </author> <year> (1990). </year> <title> A Mathematical Theory of Generalization. </title> <booktitle> Complex Systems 4: </booktitle> <pages> 151-249. </pages>
Reference-contexts: The fact that an algorithm performs well in a certain group of test cases does not usually mean that it will generalize to a wider range of situations. Neural networks are thought to have generalization capabilities <ref> (Wolpert, 1990, Darwen, 1996) </ref>, successfully inducing, for example, a good backgammon player from a set of suggested moves (Tesauro, 1989).
References-found: 31

