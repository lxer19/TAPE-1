URL: http://www.cse.psu.edu/~ugrain/vlsi-cad/MGAP/asic95.ps
Refering-URL: http://www.cse.psu.edu/~ugrain/publications.html
Root-URL: 
Email: info.pub.permission@ieee.org.  
Note: Copyright c fl1994 IEEE. All rights reserved. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE. For information on obtaining permission, send a blank email message to  By choosing to view this document, you agree to all provisions of the copyright laws protecting it.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M.J.B. Duff, </author> <title> "CLIP 4: A Large Scale Integrated Circuit Array Parallel Processor," </title> <booktitle> IEEE International Joint Conference on Pattern Recognition, </booktitle> <year> 1976, </year> <pages> pp 728-733. </pages>
Reference-contexts: Fortunately, these algorithms also have a considerable amount of inherent data-parallelism. Many architectures have been designed, primarily to achieve high performance for such problems; for examples, see references <ref> [1, 2, 3, 4] </ref> for some of the pioneering designs. These machines are all SIMD to avoid the burden of control logic in each of the processors, enabling higher levels of integration and lower processor latency. They are also fine-grain, meaning that the processors are relatively small.
Reference: [2] <author> Kenneth E. Batcher, </author> <title> "Design of a Massively Parallel Processor," </title> <journal> IEEE Transactions on Computers," </journal> <month> Sept. </month> <year> 1980, </year> <pages> pp. 836-840. </pages>
Reference-contexts: Fortunately, these algorithms also have a considerable amount of inherent data-parallelism. Many architectures have been designed, primarily to achieve high performance for such problems; for examples, see references <ref> [1, 2, 3, 4] </ref> for some of the pioneering designs. These machines are all SIMD to avoid the burden of control logic in each of the processors, enabling higher levels of integration and lower processor latency. They are also fine-grain, meaning that the processors are relatively small.
Reference: [3] <author> W.F. Wong and K.T. Lua, </author> <title> "A Preliminary Evaluation of a Massively Parallel Processor: </title> <journal> GAPP," Micropro-cessing and Microprogramming, </journal> <volume> Vol 29, No. 1, </volume> <month> July </month> <year> 1990, </year> <pages> pp 53-62. </pages>
Reference-contexts: Fortunately, these algorithms also have a considerable amount of inherent data-parallelism. Many architectures have been designed, primarily to achieve high performance for such problems; for examples, see references <ref> [1, 2, 3, 4] </ref> for some of the pioneering designs. These machines are all SIMD to avoid the burden of control logic in each of the processors, enabling higher levels of integration and lower processor latency. They are also fine-grain, meaning that the processors are relatively small.
Reference: [4] <author> Brewster A. Kahle and W. Daniel Hillis, </author> <title> "The Connection Machine Model CM-1 Architecture," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 19, No. 4, July/Aug. </volume> <year> 1989, </year> <pages> pp 707-713. </pages>
Reference-contexts: Fortunately, these algorithms also have a considerable amount of inherent data-parallelism. Many architectures have been designed, primarily to achieve high performance for such problems; for examples, see references <ref> [1, 2, 3, 4] </ref> for some of the pioneering designs. These machines are all SIMD to avoid the burden of control logic in each of the processors, enabling higher levels of integration and lower processor latency. They are also fine-grain, meaning that the processors are relatively small.
Reference: [5] <author> Massimo Maresca and Hungwen Li, </author> <title> "Connection Autonomy in SIMD Computers: A VLSI Implementation," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> July </month> <year> 1989, </year> <pages> pp 302-320. </pages>
Reference-contexts: In addition, the processors have an independent, reconfigurable interconnect. Each processor has a dedicated register which serves interprocessor communication purposes. A processor can be independently linked to either of its cardinal neighbor's dedicated communication register. This enhancement to the SIMD model is often called communication autonomy <ref> [5] </ref>. With this feature, the MGAP-2 can cluster groups of processors into larger, more powerful processors, each capable of performing complex arithmetic in a bit-serial, bit-parallel, or a digit parallel fashion. Therefore, the MGAP-2 array does not restrict programmers with a predefined notion of operand precision or operand radix.
Reference: [6] <author> Robert M. Owens, Mary Jane Irwin, Thomas P. Kelliher, Mohan Vishwanath, and Raminder Bajwa, </author> <title> "Implementing a Family of High Performance, Mi-crograined Architectures," Proceedings of Application Specific Array Processors, </title> <booktitle> 1992, </booktitle> <pages> pp. 191-205 </pages>
Reference-contexts: Therefore, the MGAP-2 array does not restrict programmers with a predefined notion of operand precision or operand radix. For example, research with the MGAP-1 <ref> [6, 7] </ref> has shown that signed-digit arithmetic can be efficiently implemented on this type of architecture. The entire array is composed of 32 - 208 pin CMOS PGAs, and each chip contains 1536 processors. <p> Figure 4 shows the actual layout of a pair of processors. In 0.8 m CMOS technology, they consume an area of 0.4 mm 2 . Although the basic architecture of the MGAP-2 processor is the same as that of the MGAP-1 <ref> [6] </ref>, we redesigned the layout and doubled the local RAM capacity. In order to be certain that our new design was functional, a test chip was built in 2.0 m CMOS technology (see figure 5). The test chip contains four processors and the address decoding logic for the RAMs.
Reference: [7] <author> Chetana Nagendra, Robert M. Owens, and Mary Jane Irwin, </author> <title> "Digit Systolic Arithmetic on Fine-Grain Array Processors," </title>
Reference-contexts: Therefore, the MGAP-2 array does not restrict programmers with a predefined notion of operand precision or operand radix. For example, research with the MGAP-1 <ref> [6, 7] </ref> has shown that signed-digit arithmetic can be efficiently implemented on this type of architecture. The entire array is composed of 32 - 208 pin CMOS PGAs, and each chip contains 1536 processors.
Reference: [8] <author> Meta-Software. </author> <title> HSPICE User's Manual H9001. </title> <address> Camp-bell, CA, 1990 3 Fig. </address> <month> 5. </month> <title> Four processor test chip 4 </title>
Reference-contexts: In addi 2 Fig. 3. Block diagram for a pair of MGAP-2 processors Fig. 4. Layout of Two MGAP-2 Processors. The area is 0.4 mm 2 tion to building the test chip, all of the chip's components have been simulated correctly in Hspice <ref> [8] </ref>. Each of the three array operations have a latency under 10 ns. Memory reads require 2 ns, writes require 3 ns, and the function multiplexers have a worst case delay of 1.3 ns.
References-found: 8

