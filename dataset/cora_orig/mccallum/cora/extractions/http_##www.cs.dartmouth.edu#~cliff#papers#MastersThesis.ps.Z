URL: http://www.cs.dartmouth.edu/~cliff/papers/MastersThesis.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~cliff/papers/
Root-URL: http://www.cs.dartmouth.edu
Title: Using Cycles and Scaling in Parallel Algorithms  
Author: by Clifford Stein David Shmoys Arthur C. Smith 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science in Electrical Engineering and Computer Science at the  Signature of Author  Certified by  Associate Professor of Mathematics Thesis Supervisor Accepted by  Chairman, Department Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: (1987)  August 1989  1989  August 18, 1989  
Affiliation: B.S.E., Computer Science Princeton University  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Aggarwal and J. Park. </author> <title> Parallel searching in multidimensional monotone arrays. </title> <journal> Journal of Algorithms, </journal> <note> 1989. Submitted. Portions of this paper appear in Proceedings of the 29th Annual IEEE Symposium on Foundations of Computer Science, </note> <year> 1988. </year>
Reference-contexts: Examples of such decompositions that have proven fruitful include ear decompositions [35, 36], and Euler tours [44, 8, 34, 17]; examples of useful general techniques include divide and conquer [32, 28] and dynamic programming <ref> [1] </ref>. The novel use of these decomposition and algorithmic techniques has led to efficient parallel algorithms for a variety of problems, and in some cases to improved sequential algorithms as well.
Reference: [2] <author> R. K. Ahuja, A. V. Goldberg, J. B. Orlin, and R. E. Tarjan. </author> <title> Finding Minimum-Cost Flows by Double Scaling. </title> <journal> Mathematical Programming, </journal> <volume> 53 </volume> <pages> 243-266, </pages> <year> 1992. </year>
Reference-contexts: Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow <ref> [15, 23, 2, 39, 19] </ref>, and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21].
Reference: [3] <author> R. K. Ahuja, K. Melhorn, J. B. Orlin, and R.E. Tarjan. </author> <title> Faster algorithms for the shortest paths problem. </title> <type> Technical Report CS-TR-154-88, </type> <institution> Department of Computer Science, Princeton University, </institution> <year> 1989. </year>
Reference-contexts: Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths <ref> [3, 18] </ref>, maximum flow [18, 4, 5], minimum-cost flow [15, 23, 2, 39, 19], and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21].
Reference: [4] <author> R. K. Ahuja and J. B. Orlin. </author> <title> A fast and simple algorithm for the maximum flow problem. </title> <journal> Operations Research, </journal> <volume> 37 </volume> <pages> 748-759, </pages> <year> 1989. </year>
Reference-contexts: Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow <ref> [18, 4, 5] </ref>, minimum-cost flow [15, 23, 2, 39, 19], and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21]. <p> Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow [15, 23, 2, 39, 19], and matching <ref> [19, 4] </ref>. In parallel computation scaling has received somewhat less attention [19, 21].
Reference: [5] <author> R. K. Ahuja, J. B. Orlin, and R.E. Tarjan. </author> <title> Improved time bounds for the maximum flow problem. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18 </volume> <pages> 939-954, </pages> <year> 1989. </year>
Reference-contexts: Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow <ref> [18, 4, 5] </ref>, minimum-cost flow [15, 23, 2, 39, 19], and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21].
Reference: [6] <author> N. Alon and M. Tarsi. </author> <title> Covering multigraphs by simple circuits. </title> <journal> SIAM Journal of Algebraic and Discrete Methods, </journal> <volume> 6 </volume> <pages> 345-350, </pages> <year> 1985. </year>
Reference-contexts: We also generalize this algorithm to multigraphs using O (log 2 n log C) time on (m + n)= log n processors. Additionally, our techniques yield a useful sequential algorithm. The sequential algorithm that finds the smallest cycle cover is that of Alon and Tarsi <ref> [6] </ref>; their algorithm guarantees a constant factor approximation. However, their algorithm requires O (m + n 2 ) time. <p> Note that for non-sparse graphs (m &gt; n log n), our techniques yield a cover whose size is within a constant factor of optimal. Further, for all classes of graphs, our algorithm is faster than any of the previous algorithms for finding a cycle cover <ref> [6, 25, 26] </ref>. In Chapter 4, we demonstrate how scaling can be used to substantially reduce the amount of work needed to find a minimum perfect matching in a bipartite graph. <p> They also conjecture that finding the minimum cycle cover is N P -complete. Alon and Tarsi <ref> [6] </ref> have developed an algorithm that finds a smaller cover, one of size at most min n 3 m; m + 7 3 , and runs in O (m + n 2 ) time.
Reference: [7] <author> R.J. Anderson and G.L. Miller. </author> <title> Deterministic parallel list ranking. </title> <booktitle> In Agaean Workshop on Computing, </booktitle> <pages> pages 81-90, </pages> <year> 1988. </year> <note> Published as Lecture Notes in Computer Science 319, Springer-Verlag. </note>
Reference-contexts: Step 3 can be implemented in O (log n) time using (n + m)= log n processors by the list-ranking algorithm of Cole and Vishkin [11] or that of Anderson and Miller <ref> [7] </ref>. A naive implementation of step 2 consists of summing at most m n + 1 vectors of length m. This approach takes O (log n) time but requires about m 2 processors.
Reference: [8] <author> B. Awerbuch, A. Israeli, and Y. Shiloach. </author> <title> Finding euler circuits in logarithmic parallel time. </title> <booktitle> In Proceedings of the 16th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 249-257, </pages> <year> 1984. </year>
Reference-contexts: Thus, in order to design fast and efficient parallel algorithms for graph theoretic problems, we must consider new, or at least different, types of graph decompositions and algorithmic techniques. Examples of such decompositions that have proven fruitful include ear decompositions [35, 36], and Euler tours <ref> [44, 8, 34, 17] </ref>; examples of useful general techniques include divide and conquer [32, 28] and dynamic programming [1]. The novel use of these decomposition and algorithmic techniques has led to efficient parallel algorithms for a variety of problems, and in some cases to improved sequential algorithms as well. <p> Euler showed that a graph is Eulerian if and only if the graph is connected and every node in the graph has even degree. Moreover, there is a linear-time sequential algorithm which, given an Eulerian graph, finds an Eulerian cycle. Awerbach, Israeli, and Shiloach <ref> [8] </ref> have given a parallel algorithm to solve this problem in O (log n) time using n + m processors, for graphs with n nodes and m edges. As this technique has proven useful for Eulerian graphs, there has been work on approximating Eulerian tours in non-Eulerian graphs.
Reference: [9] <author> C. Berge. </author> <title> Graphs and hypergraphs. </title> <publisher> North Holland Mathematical library, </publisher> <year> 1979. </year>
Reference: [10] <author> R. Cole and U. Vishkin. </author> <title> Approximate and exact parallel scheduling with applications to list, tree, and graph problems. </title> <booktitle> In Proceedings of the 27th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 478-491, </pages> <year> 1986. </year>
Reference-contexts: We note that in practice, we would change Step 7 to include a cycle C only if it contained some edge that was not already in C. This could be checked in O (log n) time on (m + n)= log n processors using pointer jumping <ref> [10] </ref>.
Reference: [11] <author> R. Cole and U. Vishkin. </author> <title> Optimal parallel algorithms for expression tree evaluation and list ranking. </title> <booktitle> In Agaean Workshop on Computing, </booktitle> <pages> pages 91-100, </pages> <year> 1988. </year> <note> Published as Lecture Notes in Computer Science 319, Springer-Verlag. 42 BIBLIOGRAPHY 43 </note>
Reference-contexts: Step 3 can be implemented in O (log n) time using (n + m)= log n processors by the list-ranking algorithm of Cole and Vishkin <ref> [11] </ref> or that of Anderson and Miller [7]. A naive implementation of step 2 consists of summing at most m n + 1 vectors of length m. This approach takes O (log n) time but requires about m 2 processors. <p> We refer to the original nodes of G as old nodes and the new nodes obtained by splitting edges as new nodes. We then use parallel tree evaluation <ref> [37, 44, 11] </ref>, to compute, 20 CHAPTER 2. FINDING A MAXIMAL SET OF EDGE DISJOINT CYCLES Input: Undirected graph G with spanning forest ^ F .
Reference: [12] <author> D. Coppersmith and S. Winograd. </author> <title> Matrix multiplication via arithmetic progression. </title> <journal> J. Symbolic Computation, </journal> <volume> 9 </volume> <pages> 251-280, </pages> <year> 1990. </year>
Reference-contexts: Currently M (n) = O (n 2:376 ) <ref> [12] </ref>, and trivially, M (n) = (n 2 ). Subsequently, a faster algorithm was discovered by Mulmuley, Vazirani, and Vazirani [38], that finds an assignment in O (log 2 n) time using nmCM (n) processors, where C is the largest edge cost in the input graph.
Reference: [13] <author> H. </author> <title> Cross. Analysis of flow in networks of conduits of conductors. </title> <type> Bulletin 286, </type> <institution> University of Illinois Engineering Experimental Station, Urbana, Ill., </institution> <year> 1936. </year>
Reference-contexts: A cycle cover is a set of cycles such that every edge in the graph appears in at least one cycle. In applications such as the analysis of irrigation systems by the Hardy Cross method <ref> [13] </ref> and the analysis of electrical 13 circuits, it is important to find a small cycle cover. Finding a minimum cover|one using the fewest possible edges|is conjectured to be NP-complete [25].
Reference: [14] <author> W.H. Cunningham and A.B. Marsh. </author> <title> A primal algorithm for optimum matching. </title> <journal> Mathematical Programming Study, </journal> <volume> 8, </volume> <year> 1978. </year>
Reference-contexts: In contrast with previous algorithms, this algorithm only works for bipartite graphs. This is because the problem of finding tight dual variables in general graphs appears to be no easier than actually finding a matching, even sequentially <ref> [14] </ref>. However, finding dual variables is the only part of the algorithm that does not generalize to general graphs.
Reference: [15] <author> J. Edmonds and R.M. Karp. </author> <title> Theoretical improvements in algorithmic efficiency for network flow problems. </title> <journal> Journal of the ACM, </journal> <volume> 19 </volume> <pages> 248-264, </pages> <year> 1972. </year>
Reference-contexts: See [18] for details. 14 CHAPTER 1. INTRODUCTION a matching in a graph with large edge costs to the problem of finding a sequence of matchings in a sequence of graphs, each of which has small edge costs. Scaling was first introduced by Edmonds and Karp <ref> [15] </ref> and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow [15, 23, 2, 39, 19], and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21]. <p> Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow <ref> [15, 23, 2, 39, 19] </ref>, and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21].
Reference: [16] <author> M.L. Fredman and R.E. Tarjan. </author> <title> Fibonacci heaps and their uses in improved network optimization algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 34 </volume> <pages> 596-615, </pages> <year> 1987. </year>
Reference-contexts: The first algorithm for the assignment problem is Kuhn's Hungarian algorithm [33]. Implemented with Fibonacci heaps <ref> [16] </ref>, this algorithm runs in O (nm+n 2 log n) time, which remains the best known strongly polynomial algorithm for the assignment problem.
Reference: [17] <author> H. Gabow. </author> <title> Using euler partitions to edge-color bipartite multigraphs. </title> <journal> International Journal of Computing and Information Science, </journal> <volume> 5 </volume> <pages> 345-355, </pages> <year> 1976. </year>
Reference-contexts: Thus, in order to design fast and efficient parallel algorithms for graph theoretic problems, we must consider new, or at least different, types of graph decompositions and algorithmic techniques. Examples of such decompositions that have proven fruitful include ear decompositions [35, 36], and Euler tours <ref> [44, 8, 34, 17] </ref>; examples of useful general techniques include divide and conquer [32, 28] and dynamic programming [1]. The novel use of these decomposition and algorithmic techniques has led to efficient parallel algorithms for a variety of problems, and in some cases to improved sequential algorithms as well.
Reference: [18] <author> H. Gabow. </author> <title> Scaling algorithms for network problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 31 </volume> <pages> 148-168, </pages> <year> 1985. </year>
Reference-contexts: RNC algorithms are NC algorithms that allow each processor to generate an O (log n) bit random number at each step in the computation. 2 The similarity assumption is the assumption that log n = O (log C) where C is the largest edge cost. See <ref> [18] </ref> for details. 14 CHAPTER 1. INTRODUCTION a matching in a graph with large edge costs to the problem of finding a sequence of matchings in a sequence of graphs, each of which has small edge costs. <p> Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths <ref> [3, 18] </ref>, maximum flow [18, 4, 5], minimum-cost flow [15, 23, 2, 39, 19], and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21]. <p> Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow <ref> [18, 4, 5] </ref>, minimum-cost flow [15, 23, 2, 39, 19], and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21]. <p> In parallel computation scaling has received somewhat less attention [19, 21]. Our algorithm combines ideas involving scaling and dual variables from the sequential algorithms of Gabow <ref> [18] </ref> and Gabow and Tarjan [19], with the parallel matching algorithms of Mulmuley, Vazirani, and Vazirani [38] and Karp, Upfal, and Wigderson [30]. <p> We conclude with three observations. First, assuming similarity, i.e. M = O (n k ) for some constant k <ref> [18] </ref>, this algorithm runs in O (log 2 n) time on (m + n)= log n processors. Second, observe that when we removed a cycle we always assigned it a multiplicity of =2. However, we could in general assign it a higher multiplicity, 24 CHAPTER 2.
Reference: [19] <author> H. N. Gabow and R. E. Tarjan. </author> <title> Faster scaling algorithms for network problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 18 </volume> <pages> 1013-1036, </pages> <year> 1989. </year>
Reference-contexts: Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow <ref> [15, 23, 2, 39, 19] </ref>, and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21]. <p> Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow [15, 23, 2, 39, 19], and matching <ref> [19, 4] </ref>. In parallel computation scaling has received somewhat less attention [19, 21]. <p> Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow [15, 23, 2, 39, 19], and matching [19, 4]. In parallel computation scaling has received somewhat less attention <ref> [19, 21] </ref>. Our algorithm combines ideas involving scaling and dual variables from the sequential algorithms of Gabow [18] and Gabow and Tarjan [19], with the parallel matching algorithms of Mulmuley, Vazirani, and Vazirani [38] and Karp, Upfal, and Wigderson [30]. <p> In parallel computation scaling has received somewhat less attention [19, 21]. Our algorithm combines ideas involving scaling and dual variables from the sequential algorithms of Gabow [18] and Gabow and Tarjan <ref> [19] </ref>, with the parallel matching algorithms of Mulmuley, Vazirani, and Vazirani [38] and Karp, Upfal, and Wigderson [30]. Chapter 2 Finding a Maximal Set of Edge Disjoint Cycles In this chapter, we introduce a new graph decomposition technique, that of decomposing a graph into a maximal set of edge-disjoint cycles. <p> Implemented with Fibonacci heaps [16], this algorithm runs in O (nm+n 2 log n) time, which remains the best known strongly polynomial algorithm for the assignment problem. Using ideas from this algorithm, the cardinality matching algorithm of Hopcroft and Karp [24], and scaling, Gabow and Tarjan <ref> [19] </ref> have developed an algorithm that runs in O ( p nm log (nC)) time. There are no known N C algorithms for the assignment problem; however, there are RN C algorithms under the assumption that the input is given in unary.
Reference: [20] <author> Z. Galil and V. Pan. </author> <title> Improved processor bounds for algebraic and combinatorial problems in RN C. </title> <booktitle> In Proceedings of the 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 490-495, </pages> <year> 1985. </year> <note> To appear in Journal of the ACM. </note>
Reference-contexts: The first RN C algorithm under this assumption was given by Karp, Upfal, and Wigderson [30]. An implementation of this algorithm by Galil and Pan <ref> [20] </ref> uses (n + C 0 )M (n) processors and O (log nlog 2 (nC 0 )) time where C 0 is an upper bound on the maximum cost of any matching, and M (n) is the minimum number of processors needed to multiply two n fi n matrices. <p> Proof: Immediate from Theorem 4.3.2 and the algorithms in [30], <ref> [20] </ref>, and [38]. Observe that our algorithm performs less work in the case that C = (n 1+* ) for some * &gt; 0.
Reference: [21] <author> A. V. Goldberg, S. A. Plotkin, and P. M. Vaidya. </author> <title> Sublinear-time parallel algorithms for matching and related problems. </title> <booktitle> In Proceedings of the 29th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 174-185, </pages> <year> 1988. </year>
Reference-contexts: Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow [15, 23, 2, 39, 19], and matching [19, 4]. In parallel computation scaling has received somewhat less attention <ref> [19, 21] </ref>. Our algorithm combines ideas involving scaling and dual variables from the sequential algorithms of Gabow [18] and Gabow and Tarjan [19], with the parallel matching algorithms of Mulmuley, Vazirani, and Vazirani [38] and Karp, Upfal, and Wigderson [30].
Reference: [22] <author> A. V. Goldberg and R. E. Tarjan. </author> <title> Finding minimum-cost circulations by canceling negative cycles. </title> <journal> Journal of the ACM, </journal> <volume> 36 </volume> <pages> 873-886, 1898. </pages>
Reference-contexts: A maximal set of weighted cycles corresponds directly to a set of capacitated cycles such that, after flow is pushed around these cycles, the graph of edges that still have positive capacity is acyclic. Goldberg and Tarjan <ref> [22] </ref> solve the minimum-cost circulation problem by repeatedly finding a maximal set of weighted cycles; they show how to solve the latter problem sequentially in O (m log n) time.
Reference: [23] <author> A. V. Goldberg and R. E. Tarjan. </author> <title> Solving minimum-cost flow problems by successive approximation. </title> <journal> Mathematics of Operations Research, </journal> <volume> 15(3) </volume> <pages> 430-466, </pages> <year> 1990. </year>
Reference-contexts: Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow <ref> [15, 23, 2, 39, 19] </ref>, and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21].
Reference: [24] <author> J. E. Hopcroft and R. M. Karp. </author> <title> An n 5=2 algorithm for maximum matching in bipartite graphs. </title> <journal> SIAM Journal on Computing, </journal> <volume> 2 </volume> <pages> 225-231, </pages> <year> 1973. </year>
Reference-contexts: Implemented with Fibonacci heaps [16], this algorithm runs in O (nm+n 2 log n) time, which remains the best known strongly polynomial algorithm for the assignment problem. Using ideas from this algorithm, the cardinality matching algorithm of Hopcroft and Karp <ref> [24] </ref>, and scaling, Gabow and Tarjan [19] have developed an algorithm that runs in O ( p nm log (nC)) time. There are no known N C algorithms for the assignment problem; however, there are RN C algorithms under the assumption that the input is given in unary.
Reference: [25] <author> A. Itai, R. J. Lipton, C.H. Papadimitriou, and M. Rodeh. </author> <title> Covering graphs by simple circuits. </title> <journal> SIAM Journal on Computing, </journal> <volume> 10(4) </volume> <pages> 746-750, </pages> <year> 1981. </year>
Reference-contexts: In applications such as the analysis of irrigation systems by the Hardy Cross method [13] and the analysis of electrical 13 circuits, it is important to find a small cycle cover. Finding a minimum cover|one using the fewest possible edges|is conjectured to be NP-complete <ref> [25] </ref>. We give the first efficient parallel approximation algorithm for this problem, as we can find an O (1 + n log n m+n ) approximation to the minimum cycle cover in O (log 2 n) time on (m + n)= log n processors. <p> Note that for non-sparse graphs (m &gt; n log n), our techniques yield a cover whose size is within a constant factor of optimal. Further, for all classes of graphs, our algorithm is faster than any of the previous algorithms for finding a cycle cover <ref> [6, 25, 26] </ref>. In Chapter 4, we demonstrate how scaling can be used to substantially reduce the amount of work needed to find a minimum perfect matching in a bipartite graph. <p> The first algorithm for this problem, by Itai and Rodeh [26], finds a cover of size O (m + n log n) in O (n 3 ) time. Subsequently, Itai, Lipton, Papadimitriou and Rodeh <ref> [25] </ref> showed that every graph has a cover of size min f3m 6; m + 6n 7g and that this cover can be found in O (n 2 ) time. <p> This result relies on a proof by Seymour [41] that every biconnected graph has a nowhere zero flow modulo 6. Alon and Tarsi also note that a certain graph called the Peterson graph <ref> [26, 25] </ref> has 15 edges and no cycle cover of size less than 21. This graph can be generalized to show that there exists an infinite family of graphs of m edges that have a minimum cycle cover of size at least 7 5 m. <p> It is known how to find such a graph B in linear-time sequentially <ref> [25] </ref>, but this requires using depth-first search. We present a parallel algorithm that does not use depth-first search and finds a graph B in O (log n) time using (m + n)= log n processors.
Reference: [26] <author> A. Itai and M. Rodeh. </author> <title> Covering a graph by circuits. </title> <booktitle> In Proceedings of the 1978 ICALP Conference, Udine, </booktitle> <year> 1978. </year>
Reference-contexts: Note that for non-sparse graphs (m &gt; n log n), our techniques yield a cover whose size is within a constant factor of optimal. Further, for all classes of graphs, our algorithm is faster than any of the previous algorithms for finding a cycle cover <ref> [6, 25, 26] </ref>. In Chapter 4, we demonstrate how scaling can be used to substantially reduce the amount of work needed to find a minimum perfect matching in a bipartite graph. <p> APPROXIMATING THE MINIMUM CYCLE COVER goals in mind. The first goal is to find a cover of small size, and the second is to get an algorithm that runs quickly. The first algorithm for this problem, by Itai and Rodeh <ref> [26] </ref>, finds a cover of size O (m + n log n) in O (n 3 ) time. <p> This result relies on a proof by Seymour [41] that every biconnected graph has a nowhere zero flow modulo 6. Alon and Tarsi also note that a certain graph called the Peterson graph <ref> [26, 25] </ref> has 15 edges and no cycle cover of size less than 21. This graph can be generalized to show that there exists an infinite family of graphs of m edges that have a minimum cycle cover of size at least 7 5 m.
Reference: [27] <editor> F. Jaeger. </editor> <booktitle> On nowhere-zero flow in multigraphs. In Proceedings of the Fifth British Combinatorial Conference, </booktitle> <pages> pages 373-378, </pages> <year> 1975. </year> <note> 44 BIBLIOGRAPHY </note>
Reference-contexts: Subsequently, Itai, Lipton, Papadimitriou and Rodeh [25] showed that every graph has a cover of size min f3m 6; m + 6n 7g and that this cover can be found in O (n 2 ) time. This result relies on a result of Jaeger <ref> [27] </ref> that shows that every biconnected graph has a nowhere zero flow modulo 8, and results of Tarjan [43] and Shiloach [42] that find edge-disjoint branchings. They also conjecture that finding the minimum cycle cover is N P -complete.
Reference: [28] <author> D. Johnson. </author> <title> Parallel algorithms for minimum cuts and maximum flows in planar networks. </title> <journal> Journal of the ACM, </journal> <pages> pages 950-967, </pages> <year> 1987. </year>
Reference-contexts: Examples of such decompositions that have proven fruitful include ear decompositions [35, 36], and Euler tours [44, 8, 34, 17]; examples of useful general techniques include divide and conquer <ref> [32, 28] </ref> and dynamic programming [1]. The novel use of these decomposition and algorithmic techniques has led to efficient parallel algorithms for a variety of problems, and in some cases to improved sequential algorithms as well.
Reference: [29] <author> H. Jung. </author> <title> An optimal parallel algorithm for computing connected components in a graph. </title> <type> Preprint, </type> <institution> Humboldt University, Berlin, German Democratic Republic, </institution> <year> 1989. </year>
Reference-contexts: Now, we focus on the time it takes to implement algorithm Maximal Cycles. Step 1 can be implemented in O (log n) time using (m + n)= log n processors by the spanning tree algorithm of Jung <ref> [29] </ref>. Step 3 can be implemented in O (log n) time using (n + m)= log n processors by the list-ranking algorithm of Cole and Vishkin [11] or that of Anderson and Miller [7]. <p> It is easy to see that each step, except for steps 4 and 5, takes constant time on n + m processors and hence takes O (log n) time on (n + m)= log n processors. By looking at the spanning tree algorithm of <ref> [29] </ref>, it is easy to see that given an acyclic set of edges that have to be in the spanning tree, the problem only becomes easier, thus steps 4 and 5 can be done in the same time as algorithm Maximal Cycles. We conclude with three observations.
Reference: [30] <author> R. Karp, E. Upfal, and A. Wigderson. </author> <title> Constructing a perfect matching is in random N C. </title> <journal> Combinatorica, </journal> <volume> 6 </volume> <pages> 35-48, </pages> <year> 1986. </year>
Reference-contexts: In Chapter 4, we demonstrate how scaling can be used to substantially reduce the amount of work needed to find a minimum perfect matching in a bipartite graph. Karp, Upfal, and Wigderson <ref> [30] </ref>, and Mulmuley, Vazirani, and Vazirani [38], have recently developed randomized N C (RN C) 1 parallel algorithms for the minimum perfect matching problem, assuming that the input is given in unary. <p> In parallel computation scaling has received somewhat less attention [19, 21]. Our algorithm combines ideas involving scaling and dual variables from the sequential algorithms of Gabow [18] and Gabow and Tarjan [19], with the parallel matching algorithms of Mulmuley, Vazirani, and Vazirani [38] and Karp, Upfal, and Wigderson <ref> [30] </ref>. Chapter 2 Finding a Maximal Set of Edge Disjoint Cycles In this chapter, we introduce a new graph decomposition technique, that of decomposing a graph into a maximal set of edge-disjoint cycles. <p> Even if this sequence of computations could be efficiently parallelized, the best known N C algorithm for computing one maximum flow in a graph with polynomial bounded capacities uses many processors and randomness <ref> [30] </ref>. Thus, we focus on a different strategy that is based on using the algorithm Maximal Cycles as a subroutine. First observe that the output of this algorithm is a set of cycles C such that m n + 1 jE (C)j m. <p> There are no known N C algorithms for the assignment problem; however, there are RN C algorithms under the assumption that the input is given in unary. The first RN C algorithm under this assumption was given by Karp, Upfal, and Wigderson <ref> [30] </ref>. <p> All other steps in the algorithm can be implemented in constant time on O (m + n) processors. Combining these observations with the fact that there are only log C iterations of the main loop, the theorem follows. Corollary 4.3.3 * Algorithm Assignment, combined with the matching algorithm of <ref> [30] </ref>, yields a randomized parallel algorithm for computing an MPM using n 2 M (n) proces sors and O (log 3 n log C) time. * Algorithm Assignment, combined with the matching algorithm of [38], yields a randomized parallel algorithm for computing an MPM using n 2 mM (n) pro cessors <p> Proof: Immediate from Theorem 4.3.2 and the algorithms in <ref> [30] </ref>, [20], and [38]. Observe that our algorithm performs less work in the case that C = (n 1+* ) for some * &gt; 0.
Reference: [31] <author> R. M. Karp and V. Ramachandran. </author> <title> A survey of parallel algorithms for shared-memory machines. </title> <booktitle> In Handbook of Theoretical Computer Science A, </booktitle> <pages> pages 869-942. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: We give a parallel algorithm that solves this problem for n-node, m-edge undirected graphs in O (log n) time using (m + n)= log n processors of a concurrent-read concurrent 11 12 CHAPTER 1. INTRODUCTION write parallel random access machine (CRCW PRAM) <ref> [31] </ref>. Since the problem requires (n + m) operations in the worst case, our algorithm is optimal in its use of parallelism. This technique is related to the Euler tour technique. In 1736, Euler posed the first graph theoretic problem, known as the Konigsberg Bridge Problem.
Reference: [32] <author> P. Klein. </author> <title> Efficient parallel algorithms for planar, chordal, and interval graphs. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Examples of such decompositions that have proven fruitful include ear decompositions [35, 36], and Euler tours [44, 8, 34, 17]; examples of useful general techniques include divide and conquer <ref> [32, 28] </ref> and dynamic programming [1]. The novel use of these decomposition and algorithmic techniques has led to efficient parallel algorithms for a variety of problems, and in some cases to improved sequential algorithms as well.
Reference: [33] <author> H.W. Kuhn. </author> <title> The hungarian method for the assignment problem. </title> <journal> In Naval Research Logistics Quarterly, </journal> <volume> volume 2, </volume> <pages> pages 83-97, </pages> <year> 1955. </year>
Reference-contexts: The first algorithm for the assignment problem is Kuhn's Hungarian algorithm <ref> [33] </ref>. Implemented with Fibonacci heaps [16], this algorithm runs in O (nm+n 2 log n) time, which remains the best known strongly polynomial algorithm for the assignment problem.
Reference: [34] <author> G. F. Lev, N. Pippenger, and L. G. Valiant. </author> <title> A fast parallel algorithm for routing in permutation networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30:93-100, </volume> <year> 1981. </year>
Reference-contexts: Thus, in order to design fast and efficient parallel algorithms for graph theoretic problems, we must consider new, or at least different, types of graph decompositions and algorithmic techniques. Examples of such decompositions that have proven fruitful include ear decompositions [35, 36], and Euler tours <ref> [44, 8, 34, 17] </ref>; examples of useful general techniques include divide and conquer [32, 28] and dynamic programming [1]. The novel use of these decomposition and algorithmic techniques has led to efficient parallel algorithms for a variety of problems, and in some cases to improved sequential algorithms as well.
Reference: [35] <author> L. Lovasz. </author> <booktitle> Computing ears and branchings in parallel. In Proceedings of the 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 464-467, </pages> <year> 1985. </year>
Reference-contexts: Thus, in order to design fast and efficient parallel algorithms for graph theoretic problems, we must consider new, or at least different, types of graph decompositions and algorithmic techniques. Examples of such decompositions that have proven fruitful include ear decompositions <ref> [35, 36] </ref>, and Euler tours [44, 8, 34, 17]; examples of useful general techniques include divide and conquer [32, 28] and dynamic programming [1].
Reference: [36] <author> Y. Maon, B. Schieber, and U. Vishkin. </author> <title> Parallel ear decomposition search (EDS) and st-numbering in graphs. </title> <booktitle> In VLSI algorithms and architectures, Lecture notes in computer science 227, </booktitle> <pages> pages 34-45. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Thus, in order to design fast and efficient parallel algorithms for graph theoretic problems, we must consider new, or at least different, types of graph decompositions and algorithmic techniques. Examples of such decompositions that have proven fruitful include ear decompositions <ref> [35, 36] </ref>, and Euler tours [44, 8, 34, 17]; examples of useful general techniques include divide and conquer [32, 28] and dynamic programming [1].
Reference: [37] <author> G. Miller and J. Reif. </author> <title> Parallel tree contraction and its application. </title> <booktitle> In Proceedings of the 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 478-489. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1985. </year>
Reference-contexts: We refer to the original nodes of G as old nodes and the new nodes obtained by splitting edges as new nodes. We then use parallel tree evaluation <ref> [37, 44, 11] </ref>, to compute, 20 CHAPTER 2. FINDING A MAXIMAL SET OF EDGE DISJOINT CYCLES Input: Undirected graph G with spanning forest ^ F .
Reference: [38] <author> K. Mulmuley, U.V. Vazirani, and V.V. Vazirani. </author> <title> Matching is as easy as matrix inversion. </title> <journal> Combinatorica, </journal> <volume> 7(1) </volume> <pages> 105-113, </pages> <year> 1987. </year>
Reference-contexts: In Chapter 4, we demonstrate how scaling can be used to substantially reduce the amount of work needed to find a minimum perfect matching in a bipartite graph. Karp, Upfal, and Wigderson [30], and Mulmuley, Vazirani, and Vazirani <ref> [38] </ref>, have recently developed randomized N C (RN C) 1 parallel algorithms for the minimum perfect matching problem, assuming that the input is given in unary. For both of these algorithms, the number of processors needed is proportional to the magnitude of the largest edge cost. <p> In parallel computation scaling has received somewhat less attention [19, 21]. Our algorithm combines ideas involving scaling and dual variables from the sequential algorithms of Gabow [18] and Gabow and Tarjan [19], with the parallel matching algorithms of Mulmuley, Vazirani, and Vazirani <ref> [38] </ref> and Karp, Upfal, and Wigderson [30]. Chapter 2 Finding a Maximal Set of Edge Disjoint Cycles In this chapter, we introduce a new graph decomposition technique, that of decomposing a graph into a maximal set of edge-disjoint cycles. <p> Currently M (n) = O (n 2:376 ) [12], and trivially, M (n) = (n 2 ). Subsequently, a faster algorithm was discovered by Mulmuley, Vazirani, and Vazirani <ref> [38] </ref>, that finds an assignment in O (log 2 n) time using nmCM (n) processors, where C is the largest edge cost in the input graph. <p> Corollary 4.3.3 * Algorithm Assignment, combined with the matching algorithm of [30], yields a randomized parallel algorithm for computing an MPM using n 2 M (n) proces sors and O (log 3 n log C) time. * Algorithm Assignment, combined with the matching algorithm of <ref> [38] </ref>, yields a randomized parallel algorithm for computing an MPM using n 2 mM (n) pro cessors in O (log 2 n log C) time. Proof: Immediate from Theorem 4.3.2 and the algorithms in [30], [20], and [38]. <p> n log C) time. * Algorithm Assignment, combined with the matching algorithm of <ref> [38] </ref>, yields a randomized parallel algorithm for computing an MPM using n 2 mM (n) pro cessors in O (log 2 n log C) time. Proof: Immediate from Theorem 4.3.2 and the algorithms in [30], [20], and [38]. Observe that our algorithm performs less work in the case that C = (n 1+* ) for some * &gt; 0. <p> At present, the most efficient parallel algorithm for this problem uses a reduction to weighted non-bipartite matching, which takes O (log 2 n) time on nmM (n) processors, and uses randomization <ref> [38] </ref>. We have also given an algorithm for the assignment problem that performs less work than the previously known RN C algorithms. It has the appealing feature of having the number of processors be independent of the size of the edge costs.
Reference: [39] <author> H. Rock. </author> <title> Scaling techniques for minimal cost network flows. </title> <editor> In U. Pape, editor, </editor> <booktitle> Discrete Structures and Algorithms, </booktitle> <pages> pages 181-191. </pages> <editor> Carl Hansen, </editor> <address> Munich, </address> <year> 1980. </year>
Reference-contexts: Scaling was first introduced by Edmonds and Karp [15] and has recently been a part of efficient sequential algorithms for shortest paths [3, 18], maximum flow [18, 4, 5], minimum-cost flow <ref> [15, 23, 2, 39, 19] </ref>, and matching [19, 4]. In parallel computation scaling has received somewhat less attention [19, 21].
Reference: [40] <author> B. Schieber and U. Vishkin. </author> <title> On finding lowest common ancestors: simplification and parallelization. </title> <booktitle> In Agaean Workshop on Computing, </booktitle> <pages> pages 111-123, </pages> <year> 1988. </year> <note> Published as Lecture Notes in Computer Science 319, Springer-Verlag. </note>
Reference-contexts: Proof: The bound follows from the previous results and the results of <ref> [40] </ref> that show how to compute lca and level in the stated time bounds. 3.3. FINDING A COVER OF SIZE O (M + N LOG N ) 31 Our parallel algorithm translates into an efficient sequential algorithm.
Reference: [41] <author> P.D. Seymour. </author> <title> Nowhere-zero 6 flows. </title> <journal> Journal of Combinatorial Theory B, </journal> <volume> 30 </volume> <pages> 130-135, </pages> <year> 1981. </year>
Reference-contexts: Alon and Tarsi [6] have developed an algorithm that finds a smaller cover, one of size at most min n 3 m; m + 7 3 , and runs in O (m + n 2 ) time. This result relies on a proof by Seymour <ref> [41] </ref> that every biconnected graph has a nowhere zero flow modulo 6. Alon and Tarsi also note that a certain graph called the Peterson graph [26, 25] has 15 edges and no cycle cover of size less than 21.
Reference: [42] <author> Y. Shiloach. </author> <title> Edge-disjoint branching in directed multigraphs. </title> <journal> Information Processing Letters, </journal> <volume> 8 </volume> <pages> 24-27, </pages> <year> 1979. </year> <note> BIBLIOGRAPHY 45 </note>
Reference-contexts: This result relies on a result of Jaeger [27] that shows that every biconnected graph has a nowhere zero flow modulo 8, and results of Tarjan [43] and Shiloach <ref> [42] </ref> that find edge-disjoint branchings. They also conjecture that finding the minimum cycle cover is N P -complete.
Reference: [43] <author> R. E. Tarjan. </author> <title> A good algorithm for edge-disjoint branchings. </title> <journal> Information Processing Letters, </journal> <volume> 3 </volume> <pages> 51-53, </pages> <year> 1975. </year>
Reference-contexts: This result relies on a result of Jaeger [27] that shows that every biconnected graph has a nowhere zero flow modulo 8, and results of Tarjan <ref> [43] </ref> and Shiloach [42] that find edge-disjoint branchings. They also conjecture that finding the minimum cycle cover is N P -complete.
Reference: [44] <author> R. E. Tarjan and U. Vishkin. </author> <title> Finding biconnected components and computing tree functions in logarithmic parallel time. </title> <booktitle> In Proceedings of the 16th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 12-20, </pages> <year> 1984. </year>
Reference-contexts: Thus, in order to design fast and efficient parallel algorithms for graph theoretic problems, we must consider new, or at least different, types of graph decompositions and algorithmic techniques. Examples of such decompositions that have proven fruitful include ear decompositions [35, 36], and Euler tours <ref> [44, 8, 34, 17] </ref>; examples of useful general techniques include divide and conquer [32, 28] and dynamic programming [1]. The novel use of these decomposition and algorithmic techniques has led to efficient parallel algorithms for a variety of problems, and in some cases to improved sequential algorithms as well. <p> We refer to the original nodes of G as old nodes and the new nodes obtained by splitting edges as new nodes. We then use parallel tree evaluation <ref> [37, 44, 11] </ref>, to compute, 20 CHAPTER 2. FINDING A MAXIMAL SET OF EDGE DISJOINT CYCLES Input: Undirected graph G with spanning forest ^ F .
References-found: 44

