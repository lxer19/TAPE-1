URL: http://www.cs.pitt.edu/~fiechter/papers/thesis.ps
Refering-URL: http://www.cs.pitt.edu/~fiechter/papers/
Root-URL: http://www.cs.pitt.edu
Title: DESIGN AND ANALYSIS OF EFFICIENT REINFORCEMENT LEARNING ALGORITHMS  
Author: by Claude-Nicolas Fiechter 
Degree: Submitted to the Graduate Faculty of Arts and Sciences in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Date: 1993  1997  
Address: 1989 M.S., University of Pittsburgh,  Pittsburgh  
Affiliation: Ing. Inf. Dipl., Ecole Polytechnique Federale de Lausanne,  University of  
Abstract-found: 0
Intro-found: 1
Reference: <institution> Bibliography </institution>
Reference: [1] <author> Charles W. Anderson. </author> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9 </volume> <pages> 31-36, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Successful applications have been developed in a variety of domains, ranging from robotics <ref> [1, 88, 63] </ref>, to industrial process control [23], to computer game playing [85, 97, 98]. <p> It has been used by several authors to illustrate and validate different learning and control techniques <ref> [20, 69, 8, 21, 1, 22] </ref>. wheeled cart carrying an inverted pendulum. The cart can move freely along a one-dimensional track and the pendulum is free to move within the vertical plan. <p> This is the approach taken in control theory where a linear feedback controller would typically be used [21]. It is also the approach taken by Anderson <ref> [1] </ref>, who implemented the controller as a two layer "neural" network that receives the values of the state variables as input. As noted earlier, however, most reinforcement learning algorithms assume that the state space is finite. <p> : : : ; x t1 ; k t1 ; R t1 ; x t , with x 0 ; : : : ; x t 2 X, k 0 ; : : : ; k t1 2 K and R 0 ; : : : ; R t1 2 <ref> [1; 1] </ref>, efficiently computes the action k t 2 K selected by for step t. Given a learning algorithm A, for every i 0 we let A i denote the policy used by A on the i th trial of the learning process. <p> For each action in the environment a decision list specifying the expected reward was generated by selecting k-terms independently at random among all possible k-terms over the n input attributes. The rewards associated with theses k-terms were uniformly drawn from the interval <ref> [0; 1] </ref> and sorted so as to create a decision list with decreasing rewards. The last element in each list was the constant function 1, associated with a null reward.
Reference: [2] <author> D. Angluin and L. G. Valiant. </author> <title> Fast probabilistic algorithms for hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Science, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <year> 1979. </year>
Reference-contexts: 2 ln 2 In the special case where the X i 's are independent identically distributed Bernoulli trials, that is, for all i, X i = &lt; 1 with probability p 0 with probability 1 p ; then the Hoeffding inequality reduces to the additive form of the Chernoff bound <ref> [2] </ref>. In that case we can also use the following multiplicative form of the Chernoff bound.
Reference: [3] <author> Dana Angluin. </author> <title> A note on the number of queries needed to identify regular languages. </title> <journal> Information and Control, </journal> <volume> 51 </volume> <pages> 76-87, </pages> <year> 1981. </year>
Reference-contexts: Such a query corresponds exactly to an experiment in which the DFA is first reset to its initial state, a sequence of actions is performed, and the resulting output is observed. Angluin <ref> [3] </ref> showed that membership queries alone are not enough to efficiently identify DFAs. She describes a class of "combination-lock" DFAs and proves that no al 24 gorithm can identify these DFAs in less than exponential time.
Reference: [4] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: Whenever the observed outcome of a sequence of actions differs from the outcome predicted by the current conjectured DFA, the learning agent has a counterexample and can revise its hypothesis. Here again, Angluin [5] showed that equivalence queries alone are not enough to efficiently identify DFAs. However, in <ref> [4] </ref>, she proved that a combination of membership and equivalence queries makes polynomial time identification of DFAs feasible. Since it uses membership queries, Angluin's algorithm implicitly assumes that a reset operation is available that returns the DFA to its initial state.
Reference: [5] <author> Dana Angluin. </author> <title> Equivalence queries and approximate fingerprint. </title> <booktitle> In Proceedings of the 1989 Workshop on Computational Learning Theory, </booktitle> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The theoretical study of DFA inference goes back to Gold's seminal paper on formal language identification [33]. Gold showed that DFAs are identifiable in the limit, from an infinite sequence of input-output examples, by a simple enumeration technique. Angluin <ref> [5] </ref>, however, showed that this could not be done efficiently. She proved that no algorithm can identify DFAs exactly from examples alone in time polynomial in the number of states of the target DFA. <p> Whenever the observed outcome of a sequence of actions differs from the outcome predicted by the current conjectured DFA, the learning agent has a counterexample and can revise its hypothesis. Here again, Angluin <ref> [5] </ref> showed that equivalence queries alone are not enough to efficiently identify DFAs. However, in [4], she proved that a combination of membership and equivalence queries makes polynomial time identification of DFAs feasible.
Reference: [6] <author> Arunabha Bagchi. </author> <title> Optimal Control of Stochastic Systems. </title> <publisher> Prentice Hall, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Such linear systems are widely used to model real-world control applications and have been extensively studied in the optimal and adaptive control literature <ref> [13, 6, 73] </ref>. The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. <p> Using the Cayley-Hamilton Theorem it can be shown <ref> [6] </ref> that if A is an n fi n matrix then G n has maximal rank among all G k , and thus that the system is reachable if and only if G n has full rank (i.e., has n linearly independent rows).
Reference: [7] <author> P. L. Bartlett, P. M. Long, and R. C. Williamson. </author> <title> Fat-shattering and the learnability of real-valued functions. </title> <booktitle> In Proceedings of the Seventh Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 299-310. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: The results presented in Chapter 5 and Chapter 6 shed some light on the subject, but much remains to be done. For supervised concept learning, there is now an almost complete characterization of the representation classes that can be PAC learned in terms of their Vapnik-Chervonenkis (or fat-shattering) dimension <ref> [16, 7] </ref>. It would be extremely interesting to obtain a similar characterization in the case of reinforcement learning. 116 The results in Chapter 5 tend to suggest that for the associative reinforcement learning problem the situation is essentially equivalent to that of supervised learning.
Reference: [8] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 13 </volume> <pages> 834-846, </pages> <year> 1983. </year>
Reference-contexts: It has been used by several authors to illustrate and validate different learning and control techniques <ref> [20, 69, 8, 21, 1, 22] </ref>. wheeled cart carrying an inverted pendulum. The cart can move freely along a one-dimensional track and the pendulum is free to move within the vertical plan. <p> On the other hand, we would expect the learning to take less time with the coarse quantization, since there are less states to consider. Barto et al. <ref> [8] </ref>, following Michie and Chambers [69], assume that the quantization is given from the start and that the controller directly receives a unary encoding of the region corresponding to the current state of the system.
Reference: [9] <author> A. G. Barto, R. S. Sutton, and C. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In M. Gabriel and J. W. Moore, editors, </editor> <title> Learning and Computational Neuroscience. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: In this dissertation we focus on a different kind of machine learning problem, known as reinforcement learning <ref> [9, 51] </ref>. Reinforcement learning considers the problem of learning 1 2 a task or behavior by interacting with one's environment. <p> We describe this model below. A detailed justification of its use a framework for the study of reinforcement learning can be found in <ref> [9] </ref> and [96]. 4 PSfrag replacements Agent Environment input x t reward R t action a t state s t 1.1.1 Sequential decision tasks Humans and animals constantly have to make sequences of actions to achieve some goals or to bring about circumstances favorable to their survival. <p> Following <ref> [9] </ref>, the methods proposed can be generally divided into two groups, model-based methods and direct methods. In the model-based approach, a model of the decision tasks is first constructed, in the form of estimated transition and immediate rewards probabilities. <p> Similar techniques were used by Samuel [85] in its famous checker playing program, and by Witten [110]. Sutton [94] and Dayan [24] proved the convergence in the limit of the TD procedure in a number of cases. Barto, Sutton and Watkins <ref> [9] </ref> show how a TD procedure can be used in sequential task learning to estimate the value of a policy, that is, the expected total reward that the agent will receive if it follows that policy. <p> Similar navigation tasks have been considered by many authors (e.g., <ref> [9, 94, 108, 59] </ref>). 13 Consider the mobile robot of Figure 1.2 that has just been put in its "grid-world" environment. The robot has no a priori knowledge of the environment and does not know the consequences of its actions, where the obstacles are, and where the goal location is.
Reference: [10] <author> K. Basye, T. Dean, and J. S. Vitter. </author> <title> Coping with uncertainty in map learning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 663-668. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: Their algorithm runs in time polynomial in 1=ffi, 1=(1=2 p) and in the number of states of the DFA. Another kind of uncertainty in the observed automaton was studied by Basye et al. <ref> [10] </ref>. They consider the case in which the result of an action is subject to random disturbances. They assume that with some probability p &lt; 1=2 the state resulting from an action is different from that specified by the target DFA. <p> In reinforcement learning, in contrast, the system to learn is considered inherently stochastic and the policy learned has to take the randomness into account. In particular, what makes the problem easier in [25] and <ref> [10] </ref> is that there is always a "correct" result that is observed more than half of the time.
Reference: [11] <author> R. Bellman and S. Dreyfus. </author> <title> Applied Dynamic Programming. </title> <publisher> Princeton Univ. Press, </publisher> <address> Princeton, NJ, </address> <year> 1962. </year>
Reference-contexts: One approach is to express the problem as a linear program and then solve it using general linear programming algorithms (see for instance [75]) or specialized policy iteration techniques [50]. Alternatively, the problem can be solved by successive approximations, using dynamic programming techniques <ref> [11] </ref>. 7 In most real situations however the agent does not have a perfect knowledge of its environment and does not know exactly the consequences of its actions. <p> When there is a perfect model of the system at hand, that is, when the transition probabilities p k (i; j) and the expected immediate rewards r k (i; j) are exactly known, there are well-known methods to determine the optimal policy fl . In particular, dynamic programming <ref> [11] </ref> can be applied to compute the optimal value function v fl and the optimal policy fl in a simple iterative manner. The algorithm, known as value iteration, is shown below.
Reference: [12] <author> D. A. Berry and B. Fristedt. </author> <title> Bandits Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <address> London, UK, </address> <year> 1985. </year>
Reference-contexts: We consider the mistake at each trial, called the instantaneous mistake, as well as the cumulative mistake on all the trials from the very beginning of the learning process. This last measure is equivalent to the notion of regret used in Bandits problems <ref> [12] </ref>. Using these measures of performance we can define criteria for efficient on-line reinforcement learning which are both intuitive and theoretically sound.
Reference: [13] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Stochastic Control. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year> <pages> 119 120 </pages>
Reference-contexts: It can however can be modeled using an extension of the basic framework, known as a partially observable Markov decision process <ref> [13] </ref>. <p> Such linear systems are widely used to model real-world control applications and have been extensively studied in the optimal and adaptive control literature <ref> [13, 6, 73] </ref>. The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. <p> remain the same. 6.4 Optimal Policy A fundamental result in modern control theory is that, provided the controllability and observability condition mentioned above are satisfied, the linear system (6.1) with quadratic cost functional (6.2) can be optimally controlled by a simple linear feedback, as illustrated in Figure 6.2 (see, e.g., <ref> [13] </ref>). The current state x t is fed back as input of the system through the linear feedback gain matrix L fl , forming a closed-loop system. <p> The cost of the optimal policy fl can also be expressed directly as a function of the solution K of the Riccati equation. Specifically, v fl fl E w w T Kw : Furthermore it can be proven <ref> [13] </ref> that the closed-loop system formed by applying the optimal policy is stable.
Reference: [14] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: When fl = 0 the agent is "greedy" and concerned only with the immediate payoff of the actions, whereas when fl approaches one the long-term future is given more and more weight. It can be shown (see e.g., <ref> [14] </ref>) that in the limit, when fl tends to one, the infinite-horizon discounted model becomes equivalent to the average-reward model, where the agent tries to maximize the long-run average reward per step, lim 1 (R 0 + R 1 + R 2 + : : : + R T ) :
Reference: [15] <editor> Sergio Bittanti, Alan J. Laub, and Jan C. Willems, editors. </editor> <title> The Riccati Equation. Communications and Control Engineering Series. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The matrix Riccati equation has been extensively studied in the literature and several methods exist to compute its solutions <ref> [15] </ref>. The cost of the optimal policy fl can also be expressed directly as a function of the solution K of the Riccati equation.
Reference: [16] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association on Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: The results presented in Chapter 5 and Chapter 6 shed some light on the subject, but much remains to be done. For supervised concept learning, there is now an almost complete characterization of the representation classes that can be PAC learned in terms of their Vapnik-Chervonenkis (or fat-shattering) dimension <ref> [16, 7] </ref>. It would be extremely interesting to obtain a similar characterization in the case of reinforcement learning. 116 The results in Chapter 5 tend to suggest that for the associative reinforcement learning problem the situation is essentially equivalent to that of supervised learning.
Reference: [17] <author> V. Borkar and P. Varaiya. </author> <title> Adaptive control of Markov chains, I: Finite parameter set. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 24 </volume> <pages> 953-957, </pages> <year> 1979. </year>
Reference-contexts: Most methods proposed in the control theory and operations research literature are of this type. They are known there as adaptive control of Markov processes. Examples are provided by Silver [92], Martin [62], Satia and Lave [86], Borkar and Varaiya <ref> [17] </ref>, and Sato, Abe and Takeda [87]. These methods are usually based on a Bayesian formulation of the Markov decision process and differ on the details of the formulation, on the prior distributions assumed and on how the policy is updated based on the estimated probabilities.
Reference: [18] <author> C. Boutilier, R. Dearden, and M. Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on AI, </booktitle> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: In other words, if the one-step transition in the environment can be characterized by a linear system then so can two steps, and, by extension, k steps, for any k. This is not the case for many other structures. For instance, Boutilier et al. <ref> [18] </ref> suggest using a form of Bayesian network to represent structure in the transition probabilities in terms of state attributes that are probabilistically independent. The problem there is a that after a few steps every attribute depends on every other attribute, even if they are originally independent.
Reference: [19] <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: Many practical applications of reinforcement learning are based on this approach and use a TD algorithm or Q-learning where the value function is represented by a function approximator (often a simple linear combination or a neural network) that maps the state representation into a value <ref> [104, 98, 19] </ref>. Boyan and Moore [19] show that this will not work in all cases, and that even on simple environments that are completely known, value iteration combined with function approximation can diverge and fail to produce an optimal policy. <p> Boyan and Moore <ref> [19] </ref> show that this will not work in all cases, and that even on simple environments that are completely known, value iteration combined with function approximation can diverge and fail to produce an optimal policy.
Reference: [20] <author> R. H. Cannon. </author> <title> Dynamics of Physical Systems. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1967. </year>
Reference-contexts: It has been used by several authors to illustrate and validate different learning and control techniques <ref> [20, 69, 8, 21, 1, 22] </ref>. wheeled cart carrying an inverted pendulum. The cart can move freely along a one-dimensional track and the pendulum is free to move within the vertical plan. <p> The objective of the controller is to keep the pendulum balanced and the cart within certain bounds by applying a lateral force to the cart. The cart and pendulum form an inherently unstable fourth-order dynamic system (see e.g., <ref> [20] </ref>). <p> , the equations of motion are: = m p L cos 2 (m c + m p )L y = m p cos 2 (m c + m p ) were g denotes the acceleration due to gravity, and u is the lateral force applied to the cart (see, e.g., <ref> [20] </ref>).
Reference: [21] <author> K. C. Cheok and N. K. Loh. </author> <title> A ball-balancing demonstration of optimal and disturbance-accomodating control. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 7 </volume> <pages> 54-57, </pages> <month> Febru-ary </month> <year> 1987. </year>
Reference-contexts: It has been used by several authors to illustrate and validate different learning and control techniques <ref> [20, 69, 8, 21, 1, 22] </ref>. wheeled cart carrying an inverted pendulum. The cart can move freely along a one-dimensional track and the pendulum is free to move within the vertical plan. <p> The state space defined by the state variables of the system is continuous, and ideally we would like to express the agent's policy directly in terms of its continuous input. This is the approach taken in control theory where a linear feedback controller would typically be used <ref> [21] </ref>. It is also the approach taken by Anderson [1], who implemented the controller as a two layer "neural" network that receives the values of the state variables as input. As noted earlier, however, most reinforcement learning algorithms assume that the state space is finite.
Reference: [22] <author> J. A. Clouse and P. E. Utgoff. </author> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 92-101. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Since their introduction, a number of variations and extensions of the Q-learning and TD algorithms have been proposed in the literature. Most of these variations seek to speed-up the learning process. Examples are the methods proposed by McCallum [64], Mahadevan [61], Clouse and Utgoff <ref> [22] </ref>, and Koenig and Simmons [59]. Whitehead and Ballard [108] describe an extension of Q-learning that integrates the control of an active sensory system with the decision-action system. Sutton's DYNA architecture [95] augments Q-learning with a mechanism that learns a model of the state-action transitions. <p> It has been used by several authors to illustrate and validate different learning and control techniques <ref> [20, 69, 8, 21, 1, 22] </ref>. wheeled cart carrying an inverted pendulum. The cart can move freely along a one-dimensional track and the pendulum is free to move within the vertical plan.
Reference: [23] <author> R. H. Crites and A. G. Barto. </author> <title> Improving elevator performance using reinforcement learning. </title> <editor> In D. Touretzky, M. Moser, and M. Hasselmo, editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 8. </volume> <year> 1996. </year>
Reference-contexts: In the last five to ten years there has been a rapidly growing interest in reinforcement learning techniques as a base for intelligent control architectures [94, 104, 37, 108, 54]. Many methods have been proposed and a number of very successful applications have been developed <ref> [98, 88, 23] </ref>. However, as noted by several authors [97, 51], little theoretical guidance is available. The theoretical results either address very restricted types of problems, like learning a shortest path to a goal in a deterministic environment [59], or are based on assumptions inadequate for practical use. <p> Successful applications have been developed in a variety of domains, ranging from robotics [1, 88, 63], to industrial process control <ref> [23] </ref>, to computer game playing [85, 97, 98].
Reference: [24] <author> Peter Dayan. </author> <title> Temporal differences: TD() for general . Machine Learning, </title> <booktitle> 8 </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference-contexts: Similar techniques were used by Samuel [85] in its famous checker playing program, and by Witten [110]. Sutton [94] and Dayan <ref> [24] </ref> proved the convergence in the limit of the TD procedure in a number of cases.
Reference: [25] <author> T. Dean, D. Angluin, K. Basye, S. Engelson, L. Keabling, E. Kokkevis, and O. Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 208-214. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Using this representation, they give versions of their inference algorithms that run in time polynomial in the diversity of the automaton (instead of in the number of states). Recently, there have been attempts to extend some of these results to stochastic automata. Dean et al. <ref> [25] </ref> consider the problem of inferring DFAs when there is uncertainty in the observed outputs. They assume that, in any state, the learner observes an incorrect output with some probability p &lt; 1=2. <p> In reinforcement learning, in contrast, the system to learn is considered inherently stochastic and the policy learned has to take the randomness into account. In particular, what makes the problem easier in <ref> [25] </ref> and [10] is that there is always a "correct" result that is observed more than half of the time.
Reference: [26] <author> A. E. Eiben, E. H. L. Aarts, and K. M. Van Hee. </author> <title> Global convergence of genetic algorithms: A Markov chain analysis. In Parallel Problem Solving from Nature: </title> <booktitle> First Workshop, </booktitle> <pages> pages 4-12. </pages> <address> Springler-Verlag, </address> <year> 1991. </year>
Reference-contexts: Convergence in the limit of genetic search methods under a number of conditions was proved by Eiben et al. <ref> [26] </ref>, and Ros [84] recently described a PAC analysis of a class of genetic algorithms for concept learning. Unlike the Q-learning and TD-based algorithms, genetic algorithms are general optimization algorithms and they do not directly take advantage of the particular structure 12 of the reinforcement learning problem.
Reference: [27] <author> Claude-Nicolas Fiechter. </author> <title> Efficient reinforcement learning. </title> <booktitle> In Proceedings of the Seventh Annual Conference on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference: [28] <author> Claude-Nicolas Fiechter. </author> <title> Expected mistake bound model for on-line reinforcement learning. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year> <month> 121 </month>
Reference: [29] <author> Claude-Nicolas Fiechter. </author> <title> PAC adaptive control of linear systems. </title> <booktitle> In Proceedings of the Tenth Annual Conference on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1997. </year>
Reference: [30] <author> K. S. Fu and M. D. Waltz. </author> <title> A heuristic approach to reinforcement-learning control systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 10 </volume> <pages> 390-398, </pages> <year> 1965. </year>
Reference-contexts: Instead, the agent executes some actions, observes their consequences and tries to directly adjust its policy to improve its performances. Examples of such methods are those of Fu and Waltz <ref> [30] </ref>, Mendel and McLaren [66], Witten [110], and Wheeler and Narendra [106]. Some of these methods are purely heuristical, others are based on results on the collective behavior of stochastic learning automata (see [74]) and are shown to converge in the limit.
Reference: [31] <author> M. Fulk and J. </author> <title> Case, </title> <editor> editors. </editor> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: This overview is by no mean intended to be exhaustive, and the interested reader is referred to the proceedings of the annual conference on computational learning theory <ref> [31, 101, 41, 77] </ref> for a more representative sampling of the wide variety of recent results in the PAC model. We begin by defining three basic representation classes of Boolean formulae.
Reference: [32] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability. </title> <publisher> Freeman, </publisher> <address> New York, NY, </address> <year> 1979. </year>
Reference-contexts: We assume that the domain instances x 2 X and representation c 2 C are encoded according to some efficient standard encoding scheme (see, for instance, <ref> [49, 32] </ref>), and we denote by jxj and jcj the length of these encodings (measured in bits or any other reasonable measure of length).
Reference: [33] <author> E. Mark Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: We give here a brief overview of some of this work. For a much more detailed overview the reader is referred to the excellent survey paper by Pitt [79]. The theoretical study of DFA inference goes back to Gold's seminal paper on formal language identification <ref> [33] </ref>. Gold showed that DFAs are identifiable in the limit, from an infinite sequence of input-output examples, by a simple enumeration technique. Angluin [5], however, showed that this could not be done efficiently.
Reference: [34] <author> D. E. Goldberg. </author> <title> Genetic algorithms and rule learning in dynamic system control. </title> <booktitle> In Proceedings of the First International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 8-15. </pages> <institution> Carnegie-Mellon Institute, </institution> <year> 1985. </year>
Reference-contexts: The environment model is then used to generate hypothetical experiments, thus reducing the number of actual experiments needed. Another family of methods that have been used extensively for solving sequential decision problems are genetic algorithms <ref> [34, 48, 37, 39] </ref>.
Reference: [35] <author> Graham C. Goodwin and Kwai Sang Sin. </author> <title> Adaptive Filtering Prediction and Control. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1984. </year>
Reference-contexts: The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. Results in the control literature typically consider the convergence of an adaptive control scheme to an optimal control in the limit, as time goes to infinity <ref> [35, 73] </ref>. <p> The idea of using linear regression to estimate the unknown parameters of the systems and to use these estimates to compute a control law as if the parameters were exactly known is not new in adaptive control. It is the approach taken in the self-tuning regulator control scheme <ref> [35, 73] </ref>. The difference here is that the learning algorithm will actively explore the state-space to quickly obtain a good approximation of the unknown parameters. Self-tuning regulators, on the contrary, always choose the control that looks best from an exploitation point of view and only learn "passively".
Reference: [36] <author> G. J. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <address> San Fransisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Boyan and Moore [19] show that this will not work in all cases, and that even on simple environments that are completely known, value iteration combined with function approximation can diverge and fail to produce an optimal policy. Some recent results by Gordon <ref> [36] </ref> and by Tsitsiklis and Van Roy [100], however, show how using some special kinds of function approximator can guarantee convergence, though not necessarily to the optimal values.
Reference: [37] <author> J. J. Grefenstette. </author> <title> Credit assignment in rule discovery systems based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-382, </pages> <year> 1989. </year>
Reference-contexts: In the last five to ten years there has been a rapidly growing interest in reinforcement learning techniques as a base for intelligent control architectures <ref> [94, 104, 37, 108, 54] </ref>. Many methods have been proposed and a number of very successful applications have been developed [98, 88, 23]. However, as noted by several authors [97, 51], little theoretical guidance is available. <p> It then has to decide which action (s) to credit (or to blame) for the result. This is known as the credit assignment problem <ref> [71, 37] </ref>. An other important characteristic of reinforcement learning which distinguishes it from supervised learning is that it is an on-line learning problem. The agent needs to learn at the same time that it performs the task. This leads to the so-called trade-off between exploration and exploitation [99, 51]. <p> The environment model is then used to generate hypothetical experiments, thus reducing the number of actual experiments needed. Another family of methods that have been used extensively for solving sequential decision problems are genetic algorithms <ref> [34, 48, 37, 39] </ref>.
Reference: [38] <author> J. J. Grefenstette and C. L. Ramsey. </author> <title> An approach to anytime-learning. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 189-195. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: To analyze the relation between the PAC model and the mistake bound model it is convenient to introduce an additional model of off-line reinforcement learning that relates to the notion of anytime learning proposed by Grefenstette <ref> [38] </ref>. In this model the learning phase is not limited in time. The agent keeps learning forever, but can be asked at any time for its current hypothesis (solution). The longer the learning algorithm is given the better the solution is expected to be.
Reference: [39] <author> J. J. Grefenstette, C. L. Ramsey, and A. C. Shultz. </author> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381, </pages> <year> 1990. </year>
Reference-contexts: The environment model is then used to generate hypothetical experiments, thus reducing the number of actual experiments needed. Another family of methods that have been used extensively for solving sequential decision problems are genetic algorithms <ref> [34, 48, 37, 39] </ref>. <p> They use a relatively gross quantization (yielding only 162 states) chosen so as to produce "what seemed like a physically realistic control problem". The pole balancing problem shares some similarities with the more realistic and challenging evasive maneuvers problem studied by Grefenstette et al. <ref> [39] </ref>. In this problem, the objective is to learn a policy or tactical plan for a plane to avoid an approaching missile. <p> Like in the pole balancing, the state variables can be discretized to make the state-space finite, but here the number of states is very large (over 25 million distinct states in <ref> [39] </ref>). Therefore, specifying which action (turn) to execute for each state is not practical and a more compact way of expressing the policy is needed.
Reference: [40] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Inform. Comput., </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Haussler et al. [42] show that these models are all equivalent. A number of extensions have also been proposed to capture aspects of real application not addressed in the basic model, like allowing noise in the examples <ref> [40] </ref> or a slow drift in the target concept or sampling distribution [45]. In addition, several other definitions of learnability have been shown to be equivalent to the PAC learnability, including polynomial predictability [42], group learnability 20 [57] and weak learnability [89].
Reference: [41] <author> D. Haussler, </author> <title> editor. </title> <booktitle> Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, PA, July 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: This overview is by no mean intended to be exhaustive, and the interested reader is referred to the proceedings of the annual conference on computational learning theory <ref> [31, 101, 41, 77] </ref> for a more representative sampling of the wide variety of recent results in the PAC model. We begin by defining three basic representation classes of Boolean formulae.
Reference: [42] <author> D. Haussler, M. Kearns, N. Littlestone, and M. K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95 </volume> <pages> 129-161, </pages> <year> 1991. </year>
Reference-contexts: Several variants of this definition and of the PAC model can be found in the literature. In particular, the model is sometimes defined with two distinct oracles POS and NEG that produce exclusively positive and negative examples respectively. Haussler et al. <ref> [42] </ref> show that these models are all equivalent. A number of extensions have also been proposed to capture aspects of real application not addressed in the basic model, like allowing noise in the examples [40] or a slow drift in the target concept or sampling distribution [45]. <p> In addition, several other definitions of learnability have been shown to be equivalent to the PAC learnability, including polynomial predictability <ref> [42] </ref>, group learnability 20 [57] and weak learnability [89].
Reference: [43] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Expected mistake bounds for online learning algorithms. </title> <type> Technical report, </type> <institution> Department of Computer and Information Sciences, University of California, </institution> <address> Santa Cruz, CA, </address> <year> 1987. </year>
Reference-contexts: In this chapter we consider a related model of efficient reinforcement learning that captures the on-line nature of the problem. It is based on the expected mistake bound framework introduced by Haussler, Littlestone and Warmuth for supervised learning <ref> [43] </ref>. The learning takes place in a series of trials and the measure of performance on a trial is the expected difference between the total reward the agent gets compared to that of an agent behaving optimally. We call that the "mistake" of the learning algorithm. <p> Showing that (iv) implies (i) is more involved, but can be done in a manner analogous to the proof given by Haussler, Littlestone and Warmuth in the case of concept learning <ref> [43] </ref>. The proof is given in Section 4.4. 4.3 Conversion from PAC to On-Line Learner In this section we show how an off-line PAC reinforcement learning algorithm for some class C of environments can be converted into an efficient on-line algorithm for C in a simple and practical manner.
Reference: [44] <author> David Haussler. </author> <title> Quantifying inductive bias: AI learning and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference-contexts: A Boolean formula can then represent a complex condition on the state, and we can use such a condition to specify when a particular action has to be executed. Furthermore, results from Haussler <ref> [44] </ref> extend this to real-valued attributes. <p> For instance, it appears that "buying confidence" (i.e., decreasing ffi), is much less expensive than requiring a higher accuracy (i.e., decreasing "), as observed in other learning problems <ref> [44] </ref>. However, in the analysis the emphasis was put on proving a polynomial bound rather than getting a tight bound. In practice we expect that the actual number of steps needed to achieve some fixed accuracy and confidence levels would be much less than that predicted by the worst-case analysis.
Reference: [45] <author> David P. Helmbold and Philip M. </author> <title> Long. Tracking drifting concepts by minimizing disagreements. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 27-45, </pages> <year> 1994. </year> <month> 122 </month>
Reference-contexts: Haussler et al. [42] show that these models are all equivalent. A number of extensions have also been proposed to capture aspects of real application not addressed in the basic model, like allowing noise in the examples [40] or a slow drift in the target concept or sampling distribution <ref> [45] </ref>. In addition, several other definitions of learnability have been shown to be equivalent to the PAC learnability, including polynomial predictability [42], group learnability 20 [57] and weak learnability [89]. <p> Consequently, the value function estimates cannot be used to safely compute an improved policy, like in the policy iteration algorithm. Some recent results by Helmbold and Long about PAC learning of "drifting" concepts <ref> [45] </ref> might be helpful to address that problem. On the other hand, it is precisely the fact that the distribution of states visited depends on the policy used that has us believing that strong positive results about generalization in reinforcement learning are possible.
Reference: [46] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sum of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <year> 1963. </year>
Reference-contexts: The first result, known as the Hoeffding inequality <ref> [46] </ref>, provides a bound on the 26 difference between the expected value of a bounded random variable and a sample mean of that variable.
Reference: [47] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year>
Reference-contexts: Contrary to the methods described above that try to estimate the utility of taking a particular action in a particular state, genetic algorithms work by searching the space of possible behaviors to find one that performs well in the environment, in a way that mimics evolutionary processes <ref> [47] </ref>. Behaviors are typically described by a set of rules that maps some features of the states of the environment into actions. The learning proceeds in successive stages or generations.
Reference: [48] <author> J. H. Holland. </author> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artficial Intelligence Approach (Volume II). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference-contexts: The environment model is then used to generate hypothetical experiments, thus reducing the number of actual experiments needed. Another family of methods that have been used extensively for solving sequential decision problems are genetic algorithms <ref> [34, 48, 37, 39] </ref>.
Reference: [49] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: We assume that the domain instances x 2 X and representation c 2 C are encoded according to some efficient standard encoding scheme (see, for instance, <ref> [49, 32] </ref>), and we denote by jxj and jcj the length of these encodings (measured in bits or any other reasonable measure of length). <p> In this case, an input string a is said to be accepted by the automaton if fl (q 0 a) = 1. (Additional background material on DFAs can be found in <ref> [49] </ref>). The problem of inferring a DFA is related to reinforcement learning because when the environment for a reinforcement learning task is deterministic it can be naturally modeled as a DFA.
Reference: [50] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1960. </year>
Reference-contexts: The task that the learning agent is trying to learn in a reinforcement learning problem typically involves a sequence of actions or decisions. Such a task, as well as the interaction between the agent and its environment, can be mathematically formulated as a discrete time Markov decision process <ref> [50, 70] </ref>. We describe this model below. <p> One approach is to express the problem as a linear program and then solve it using general linear programming algorithms (see for instance [75]) or specialized policy iteration techniques <ref> [50] </ref>. Alternatively, the problem can be solved by successive approximations, using dynamic programming techniques [11]. 7 In most real situations however the agent does not have a perfect knowledge of its environment and does not know exactly the consequences of its actions. <p> The estimate can then be used to adjust the policy, in a manner reminiscent of Howard's policy improvement technique <ref> [50] </ref>. Schapire and Warmuth recently analyzed a variant of the TD algorithm and bounded the error that the algorithm makes in the worst-case when it estimates the value of the policy it uses [90].
Reference: [51] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: In this dissertation we focus on a different kind of machine learning problem, known as reinforcement learning <ref> [9, 51] </ref>. Reinforcement learning considers the problem of learning 1 2 a task or behavior by interacting with one's environment. <p> Many methods have been proposed and a number of very successful applications have been developed [98, 88, 23]. However, as noted by several authors <ref> [97, 51] </ref>, little theoretical guidance is available. The theoretical results either address very restricted types of problems, like learning a shortest path to a goal in a deterministic environment [59], or are based on assumptions inadequate for practical use. <p> 2 describes Valiant's model in some details and gives an overview of other relevant results in computational learning theory, notably results on the inference of finite automaton. 1.1 Reinforcement Learning Reinforcement learning is a computational approach to the problem of learning a task by trial-and-error interaction with a dynamic environment <ref> [51, 96] </ref>. It dates back to the early days of computer science and has its roots in the psychology of animal learning, from which it takes its name. <p> An other important characteristic of reinforcement learning which distinguishes it from supervised learning is that it is an on-line learning problem. The agent needs to learn at the same time that it performs the task. This leads to the so-called trade-off between exploration and exploitation <ref> [99, 51] </ref>. On the one hand the agent needs to try all the actions to gather information about the different actions and states of the environment and determine which action is best in which state. <p> This approach is sometimes referred to as the certainty equivalence method <ref> [51] </ref>. The key idea of the algorithm, however, is that it also uses the statistics it collects to guide the exploration so as to minimize the number of experiments it needs to build an adequate model of the environment.
Reference: [52] <author> Leslie P. Kaelbling. </author> <title> Associative reinforcement learning: A generate and test algorithm. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 299-319, </pages> <year> 1994. </year>
Reference-contexts: As a first step toward characterizing when and how such efficient reinforcement learning is possible, we consider in this chapter a restricted class of reinforcement learning problems known as associative reinforcement learning <ref> [53, 52] </ref>. In this kind of reinforcement learning the assumption is made that each trial is independent and that the reinforcement signal at time t + 1 only reflects the success of the action taken at time t. In other words, actions have short-term consequences but no long-term consequences. <p> We considered two sets of tasks in our experiments. The first set consists of three problems described by Kaelbling <ref> [53, 52] </ref> and the second consists of a large number of randomly generated tasks. In all the experiments, the input consists of n Boolean attributes and the state space X is the set f0; 1g n of all assignments to the n attributes. <p> maximum value of the policy learned on 100 runs with m = 1500. 2 The table also shows the expected value of the optimal and random policy (choosing action 0 and 1 with equal probability), as well as Kaelbling's results for her interval estimation k-DNF (iekdnf) and generate-and-test (gtrl) algorithms <ref> [53, 52] </ref>. We cannot, however, directly compare our results with those of Kaelbling, because, as a performance metric, she uses the average reward obtained over an entire run, whereas we are using the expected value of the policy learned.
Reference: [53] <author> Leslie P. Kaelbling. </author> <title> Associative reinforcement learning: Functions in k-DNF. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 279-298, </pages> <year> 1994. </year>
Reference-contexts: As a first step toward characterizing when and how such efficient reinforcement learning is possible, we consider in this chapter a restricted class of reinforcement learning problems known as associative reinforcement learning <ref> [53, 52] </ref>. In this kind of reinforcement learning the assumption is made that each trial is independent and that the reinforcement signal at time t + 1 only reflects the success of the action taken at time t. In other words, actions have short-term consequences but no long-term consequences. <p> In this kind of reinforcement learning the assumption is made that each trial is independent and that the reinforcement signal at time t + 1 only reflects the success of the action taken at time t. In other words, actions have short-term consequences but no long-term consequences. Kaelbling <ref> [53] </ref> de 74 75 scribes this learning scenario in detail and discusses how results for this particular case of reinforcement learning could be extended to situations in which the above assumption does not hold. <p> This representation seems to be quite natural and general. Different bases of functions can be chosen to accommodate all kinds of input spaces and to express a wide variety of learning biases. Notably, as a special case, the representation includes the class of k-DNF functions, as considered by Kaelbling <ref> [53] </ref>. Moreover, as discussed in Section 2.2, decision lists form in some sense the most general class of formula that are known to be PAC learnable. <p> This implies in particular a polynomial-time algorithm for learning policies expressible in k-DNF, for k a fixed constant. We also give some experimental results that show that the algorithm performs well in practice. 76 5.1 Learning Model We consider a simple generalization of Kaelbling's model of associative reinforcement learning <ref> [53] </ref> that corresponds to the model of efficient reinforcement learning in Chapter 4 when the length of each trial is fixed to M = 1. In other words, each trial consists of a single action and reward. <p> We considered two sets of tasks in our experiments. The first set consists of three problems described by Kaelbling <ref> [53, 52] </ref> and the second consists of a large number of randomly generated tasks. In all the experiments, the input consists of n Boolean attributes and the state space X is the set f0; 1g n of all assignments to the n attributes. <p> maximum value of the policy learned on 100 runs with m = 1500. 2 The table also shows the expected value of the optimal and random policy (choosing action 0 and 1 with equal probability), as well as Kaelbling's results for her interval estimation k-DNF (iekdnf) and generate-and-test (gtrl) algorithms <ref> [53, 52] </ref>. We cannot, however, directly compare our results with those of Kaelbling, because, as a performance metric, she uses the average reward obtained over an entire run, whereas we are using the expected value of the policy learned.
Reference: [54] <author> Leslie P. Kaelbling, </author> <title> editor. </title> <journal> Machine Learning, Special Issue on Reinforcement Learning, </journal> <month> January </month> <year> 1996. </year>
Reference-contexts: In the last five to ten years there has been a rapidly growing interest in reinforcement learning techniques as a base for intelligent control architectures <ref> [94, 104, 37, 108, 54] </ref>. Many methods have been proposed and a number of very successful applications have been developed [98, 88, 23]. However, as noted by several authors [97, 51], little theoretical guidance is available.
Reference: [55] <author> M. J. Kearns and R. E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48 </volume> <pages> 464-497, </pages> <year> 1994. </year>
Reference-contexts: A similar generalization of Rivest's decision lists was used by Kearns and Schapire <ref> [55] </ref> to represent probabilistic concepts. This representation seems to be quite natural and general. Different bases of functions can be chosen to accommodate all kinds of input spaces and to express a wide variety of learning biases. <p> that the learning algorithm produces a policy such that Prob n fi fi v fl fi fi " 1 ffi where fl is the optimal policy above. 5.2 Representation We are interested in policies that can be represented by a general form of decision list introduced by Kearns and Schapire <ref> [55] </ref>. <p> The learning algorithm is shown below. It uses a strategy similar to that of Kearns and Schapire's algorithm for learning probabilistic decision lists <ref> [55] </ref>. The algorithm starts by exploring the environment, collecting samples S k of input-reward pairs for each action k. 1 These samples are then used to construct an hypothesis decision list h that approximates the list c fl . The intuition behind the algorithm is the following.
Reference: [56] <author> M. J. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444. </pages> <institution> Association for Computing Machinery, </institution> <year> 1989. </year>
Reference-contexts: He then extends this algorithm to show that, for any fixed k, the representation classes k-DNF and k-CNF are PAC learnable in time polynomial in O (n k ). In contrast, Kearns and Valiant <ref> [57, 56] </ref> show that the class of all Boolean formulae is not PAC learnable, regardless of the hypotheses class used, under some standard assumptions about the security of cryptographic schemes. <p> Pitt and Warmuth [78] showed that even finding an approximation of the smallest DFA consistent with a sample of input-output pairs is intractable (assuming P 6= N P ). In the PAC model, Kearns and Valiant <ref> [57, 56] </ref> showed that DFAs are not PAC identifiable from examples only, regardless of the hypotheses class used, under some standard cryptographic assumptions.
Reference: [57] <author> Michael J. Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: We start by precisely defining the notion of concept. The following definitions are borrowed from <ref> [57] </ref> and [89]. Variants of these definitions can be found in most PAC related literature. Let X be a set that contains an encoding of all objects of interest in a 17 18 particular learning task. We call X the instance space or domain of the learning problem. <p> In addition, several other definitions of learnability have been shown to be equivalent to the PAC learnability, including polynomial predictability [42], group learnability 20 <ref> [57] </ref> and weak learnability [89]. <p> These classes are all parameterized classes and their domain X n is the set f0; 1g n of all assignments to n Boolean variables. We only define the subclass C n and the general class is defined by C = n1 C n . The following definitions are from <ref> [57] </ref>. <p> He then extends this algorithm to show that, for any fixed k, the representation classes k-DNF and k-CNF are PAC learnable in time polynomial in O (n k ). In contrast, Kearns and Valiant <ref> [57, 56] </ref> show that the class of all Boolean formulae is not PAC learnable, regardless of the hypotheses class used, under some standard assumptions about the security of cryptographic schemes. <p> Pitt and Warmuth [78] showed that even finding an approximation of the smallest DFA consistent with a sample of input-output pairs is intractable (assuming P 6= N P ). In the PAC model, Kearns and Valiant <ref> [57, 56] </ref> showed that DFAs are not PAC identifiable from examples only, regardless of the hypotheses class used, under some standard cryptographic assumptions.
Reference: [58] <editor> Y. Kodratoff and R. S. Michalski, editors. </editor> <booktitle> Machine Learning: An Artficial Intelligence Approach (Volume III). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: An overview of this diversity can be found, for instance, in the survey paper by Mitchell et al. [72] and in different available collections of learning papers <ref> [67, 68, 58, 91] </ref>. The approach taken in this dissertation is that of computational learning theory. The goal of this area of research is to develop a sound theoretical foundation for the study of machine learning.
Reference: [59] <author> S. Koenig and R. G. Simmons. </author> <title> Complexity analysis of real-time reinforcement learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 99-105. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: However, as noted by several authors [97, 51], little theoretical guidance is available. The theoretical results either address very restricted types of problems, like learning a shortest path to a goal in a deterministic environment <ref> [59] </ref>, or are based on assumptions inadequate for practical use. Notably, the convergence of the algorithms is usually guaranteed only "in the limit", when every action in every possible state in the environment is executed infinitely many times (see Section 1.2). <p> Since their introduction, a number of variations and extensions of the Q-learning and TD algorithms have been proposed in the literature. Most of these variations seek to speed-up the learning process. Examples are the methods proposed by McCallum [64], Mahadevan [61], Clouse and Utgoff [22], and Koenig and Simmons <ref> [59] </ref>. Whitehead and Ballard [108] describe an extension of Q-learning that integrates the control of an active sensory system with the decision-action system. Sutton's DYNA architecture [95] augments Q-learning with a mechanism that learns a model of the state-action transitions. <p> Similar navigation tasks have been considered by many authors (e.g., <ref> [9, 94, 108, 59] </ref>). 13 Consider the mobile robot of Figure 1.2 that has just been put in its "grid-world" environment. The robot has no a priori knowledge of the environment and does not know the consequences of its actions, where the obstacles are, and where the goal location is. <p> There are several possible ways of formulating this as the problem of finding a policy that maximizes the total reward. Maybe the simplest formulation, sometimes referred to as the goal-reward representation <ref> [59] </ref>, is to reward the agent when it enters the goal location and not to reward or punish it otherwise. That is, the immediate expected reward is 1 for the actions that move the robot from an adjacent location into the goal state and 0 for all other actions. <p> Even though the two formulations describe the same task, they are not equivalent from a learning point of view, as shown by Koenig and Simmons <ref> [59] </ref> for the general goal finding problem. The general goal finding problem, sometimes called the real-time search 14 problem [60], is a generalization of the described navigation task, in which the environment is no longer restricted to a grid but can be represented by an arbitrary directed graph. <p> As for the navigation task, the objective is to determine for each location a shortest path or sequence of actions that lead to a distinguished goal node. For this specific problem, Koenig and Simmons <ref> [59] </ref> show that Q-learning (see Section 1.2) takes polynomial time in the number of states in the worst case using the action-penalty representation, whereas the same algorithm can require exponential time when the goal-reward representation is used [107]. <p> This illustrates the fact that the complexity of a reinforcement learning algorithm on a given problem can dramatically depend on the chosen task formulation. Koenig and Simmons <ref> [59] </ref> also show that any algorithm for the general goal finding problem that does not know the effect of an action before it has tried it once will need to perform (n 3 ) actions in some cases to find a goal in an n-state environment. <p> The difficulty here is in accessing the accepting (goal) state, which can only be reached by a single sequence of actions (the "combination" of the DFA). This contrasts with the results of Koenig and Simmons <ref> [59] </ref> discussed in Section 1.3.1 showing that the general goal-finding problem can be solved in polynomial time.
Reference: [60] <author> R. E. Korf. </author> <title> Real-time heuristic search. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 441-450, </pages> <year> 1990. </year>
Reference-contexts: Even though the two formulations describe the same task, they are not equivalent from a learning point of view, as shown by Koenig and Simmons [59] for the general goal finding problem. The general goal finding problem, sometimes called the real-time search 14 problem <ref> [60] </ref>, is a generalization of the described navigation task, in which the environment is no longer restricted to a grid but can be represented by an arbitrary directed graph.
Reference: [61] <author> Shridar Mahadevan. </author> <title> Enhancing transfer in reinforcement learning by building stochastic model of robot actions. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 290-299. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Since their introduction, a number of variations and extensions of the Q-learning and TD algorithms have been proposed in the literature. Most of these variations seek to speed-up the learning process. Examples are the methods proposed by McCallum [64], Mahadevan <ref> [61] </ref>, Clouse and Utgoff [22], and Koenig and Simmons [59]. Whitehead and Ballard [108] describe an extension of Q-learning that integrates the control of an active sensory system with the decision-action system. Sutton's DYNA architecture [95] augments Q-learning with a mechanism that learns a model of the state-action transitions.
Reference: [62] <author> J. J. Martin. </author> <title> Bayesian Decision Problems and Markov Chains. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1967. </year> <month> 123 </month>
Reference-contexts: Most methods proposed in the control theory and operations research literature are of this type. They are known there as adaptive control of Markov processes. Examples are provided by Silver [92], Martin <ref> [62] </ref>, Satia and Lave [86], Borkar and Varaiya [17], and Sato, Abe and Takeda [87].
Reference: [63] <author> M. J. Mataric. </author> <title> Reward functions for accelerated learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Successful applications have been developed in a variety of domains, ranging from robotics <ref> [1, 88, 63] </ref>, to industrial process control [23], to computer game playing [85, 97, 98].
Reference: [64] <author> R. Andrew McCallum. </author> <title> Using transitional proximity for faster reinforcement learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 316-321. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Since their introduction, a number of variations and extensions of the Q-learning and TD algorithms have been proposed in the literature. Most of these variations seek to speed-up the learning process. Examples are the methods proposed by McCallum <ref> [64] </ref>, Mahadevan [61], Clouse and Utgoff [22], and Koenig and Simmons [59]. Whitehead and Ballard [108] describe an extension of Q-learning that integrates the control of an active sensory system with the decision-action system.
Reference: [65] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 387-395. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: This is sometimes referred to as the perceptual aliasing [108] or hidden state <ref> [65] </ref> problem. When the agent does not have a complete and noise-free perception of its environment the system can no longer be modeled adequately using a a simple Markov decision process.
Reference: [66] <author> J. M. Mendel and R. W. McLaren. </author> <title> Reinforcement learning control and pattern recognition. </title> <editor> In J. M. Mendel and K. S. Fu, editors, </editor> <title> Adaptive, </title> <journal> Learning and Pattern Recognition Systems: Theory and Applications, </journal> <pages> pages 287-318. </pages> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1970. </year>
Reference-contexts: Instead, the agent executes some actions, observes their consequences and tries to directly adjust its policy to improve its performances. Examples of such methods are those of Fu and Waltz [30], Mendel and McLaren <ref> [66] </ref>, Witten [110], and Wheeler and Narendra [106]. Some of these methods are purely heuristical, others are based on results on the collective behavior of stochastic learning automata (see [74]) and are shown to converge in the limit.
Reference: [67] <author> R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, </author> <title> editors. Machine Learning: An Artficial Intelligence Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1983. </year>
Reference-contexts: An overview of this diversity can be found, for instance, in the survey paper by Mitchell et al. [72] and in different available collections of learning papers <ref> [67, 68, 58, 91] </ref>. The approach taken in this dissertation is that of computational learning theory. The goal of this area of research is to develop a sound theoretical foundation for the study of machine learning.
Reference: [68] <author> R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, </author> <title> editors. Machine Learning: An Artficial Intelligence Approach (Volume II). </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1986. </year>
Reference-contexts: An overview of this diversity can be found, for instance, in the survey paper by Mitchell et al. [72] and in different available collections of learning papers <ref> [67, 68, 58, 91] </ref>. The approach taken in this dissertation is that of computational learning theory. The goal of this area of research is to develop a sound theoretical foundation for the study of machine learning.
Reference: [69] <author> D. Michie and R. A. Chambers. </author> <title> BOXES: an experiment in adaptative control. </title> <booktitle> In Machine Intelligence 2. </booktitle> <publisher> Oliver and Boyd, Edinburgh, </publisher> <year> 1968. </year>
Reference-contexts: It has been used by several authors to illustrate and validate different learning and control techniques <ref> [20, 69, 8, 21, 1, 22] </ref>. wheeled cart carrying an inverted pendulum. The cart can move freely along a one-dimensional track and the pendulum is free to move within the vertical plan. <p> On the other hand, we would expect the learning to take less time with the coarse quantization, since there are less states to consider. Barto et al. [8], following Michie and Chambers <ref> [69] </ref>, assume that the quantization is given from the start and that the controller directly receives a unary encoding of the region corresponding to the current state of the system.
Reference: [70] <author> H. Mine and S. Osaki. </author> <title> Markovian Decision Processes. Modern Analytic and Computational Methods in Science and Mathematics. </title> <publisher> Elsevier, </publisher> <address> New York, NY, </address> <year> 1970. </year>
Reference-contexts: The task that the learning agent is trying to learn in a reinforcement learning problem typically involves a sequence of actions or decisions. Such a task, as well as the interaction between the agent and its environment, can be mathematically formulated as a discrete time Markov decision process <ref> [50, 70] </ref>. We describe this model below. <p> We formally define this decision process here for the finite-state case. We essentially follow <ref> [70] </ref> in their definitions and notations. 3.1.1 Sequential decision task Let the state space S be a finite set of states labeled by the integers i = 1; 2; : : : ; N . <p> It is a well-known result that for any finite-state Markov decision process as defined above there is a deterministic stationary policy fl that is optimal simultaneously from every state of the environment. We state this result here as a theorem (see <ref> [70] </ref>, for instance, for the demonstration). 32 Theorem 3.1 For a given discount factor fl, the stationary policy fl defined by fl (i) = a s.t. w fl k2K i (i; k) is optimal.
Reference: [71] <author> M. L. Minsky. </author> <booktitle> Steps toward artificial intelligence. Proceedings of the Institute of Radio Engineers, </booktitle> <volume> 49 </volume> <pages> 8-30, </pages> <year> 1961. </year>
Reference-contexts: It then has to decide which action (s) to credit (or to blame) for the result. This is known as the credit assignment problem <ref> [71, 37] </ref>. An other important characteristic of reinforcement learning which distinguishes it from supervised learning is that it is an on-line learning problem. The agent needs to learn at the same time that it performs the task. This leads to the so-called trade-off between exploration and exploitation [99, 51].
Reference: [72] <author> T. M. Mitchell, B. Buchanan, G. DeJong, T. G. Dietterich, P. Rosenbloom, and A. Waibel. </author> <title> Machine learning. </title> <editor> In J. F. Traub, B. J. Grosz, B. W. Lampson, and N. J. Nillson, editors, </editor> <booktitle> Annual Review of Computer Science, </booktitle> <volume> volume 4, </volume> <pages> pages 417-433. </pages> <note> Annual Review, </note> <year> 1990. </year>
Reference-contexts: The variety of sources and purposes for studying machine learning is reflected in the extremely rich diversity of approaches that have been applied to this problem. An overview of this diversity can be found, for instance, in the survey paper by Mitchell et al. <ref> [72] </ref> and in different available collections of learning papers [67, 68, 58, 91]. The approach taken in this dissertation is that of computational learning theory. The goal of this area of research is to develop a sound theoretical foundation for the study of machine learning.
Reference: [73] <author> Edoardo Mosca. </author> <title> Optimal, Predictive, and Adaptive Control. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1995. </year>
Reference-contexts: Such linear systems are widely used to model real-world control applications and have been extensively studied in the optimal and adaptive control literature <ref> [13, 6, 73] </ref>. The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. <p> The main difference between our approach and those found in the adaptive control literature is our emphasis on computational and learning efficiency. Results in the control literature typically consider the convergence of an adaptive control scheme to an optimal control in the limit, as time goes to infinity <ref> [35, 73] </ref>. <p> The idea of using linear regression to estimate the unknown parameters of the systems and to use these estimates to compute a control law as if the parameters were exactly known is not new in adaptive control. It is the approach taken in the self-tuning regulator control scheme <ref> [35, 73] </ref>. The difference here is that the learning algorithm will actively explore the state-space to quickly obtain a good approximation of the unknown parameters. Self-tuning regulators, on the contrary, always choose the control that looks best from an exploitation point of view and only learn "passively".
Reference: [74] <author> K. S. Narendra and M. A. Thathachar. </author> <title> Learning automata|a survey. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 4, </volume> <year> 1974. </year>
Reference-contexts: Examples of such methods are those of Fu and Waltz [30], Mendel and McLaren [66], Witten [110], and Wheeler and Narendra [106]. Some of these methods are purely heuristical, others are based on results on the collective behavior of stochastic learning automata (see <ref> [74] </ref>) and are shown to converge in the limit. In the last few years, a family of direct methods, based on Sutton's temporal difference (TD) procedure [93, 94], have attracted much attention. The TD procedure is an incremental learning technique to predict a value from sequences of observations.
Reference: [75] <author> C. H. Papadimitriou and K. Steiglitz. </author> <title> Combinatorial Optimization: Algorithms and Complexity. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ., </address> <year> 1982. </year>
Reference-contexts: One approach is to express the problem as a linear program and then solve it using general linear programming algorithms (see for instance <ref> [75] </ref>) or specialized policy iteration techniques [50]. Alternatively, the problem can be solved by successive approximations, using dynamic programming techniques [11]. 7 In most real situations however the agent does not have a perfect knowledge of its environment and does not know exactly the consequences of its actions.
Reference: [76] <author> C. H. Papadimitriou and J. N. Tsitsiklis. </author> <title> The complexity of Markov decision process. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12 </volume> <pages> 441-450, </pages> <year> 1987. </year>
Reference-contexts: Unfortunately, this technique is impractical except on extremely small problems, because, in general, it transforms a problem with N states into a continuous N -dimensional problem. Papadimitriou and Tsitsiklis <ref> [76] </ref> show that in general determining an optimal policy for a partially observable Markov decision process is PSPACE-hard, even when the horizon is finite and the system is completely known.
Reference: [77] <editor> L. Pitt, editor. </editor> <booktitle> Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <address> Santa Cruz, CA, </address> <month> July </month> <year> 1993. </year> <note> ACM Press. 124 </note>
Reference-contexts: This overview is by no mean intended to be exhaustive, and the interested reader is referred to the proceedings of the annual conference on computational learning theory <ref> [31, 101, 41, 77] </ref> for a more representative sampling of the wide variety of recent results in the PAC model. We begin by defining three basic representation classes of Boolean formulae.
Reference: [78] <author> L. Pitt and M. K. Warmuth. </author> <title> The minimum consistent DFA problem cannot be approximated within any polynomial. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing. Association for Computing Machinery, </booktitle> <year> 1989. </year>
Reference-contexts: Angluin [5], however, showed that this could not be done efficiently. She proved that no algorithm can identify DFAs exactly from examples alone in time polynomial in the number of states of the target DFA. Pitt and Warmuth <ref> [78] </ref> showed that even finding an approximation of the smallest DFA consistent with a sample of input-output pairs is intractable (assuming P 6= N P ).
Reference: [79] <author> Leonard Pitt. </author> <title> Inductive inference, DFAs, and computational complexity. </title> <editor> In K. P. Jantke, editor, </editor> <booktitle> Proceedings of the 1989 International Workshop on Analogical and Inductive Inference. </booktitle> <address> Springler-Verlag, </address> <year> 1989. </year>
Reference-contexts: In spite of these differences, some important ideas and lessons can be drawn from the theoretical work on DFA learning. We give here a brief overview of some of this work. For a much more detailed overview the reader is referred to the excellent survey paper by Pitt <ref> [79] </ref>. The theoretical study of DFA inference goes back to Gold's seminal paper on formal language identification [33]. Gold showed that DFAs are identifiable in the limit, from an infinite sequence of input-output examples, by a simple enumeration technique. Angluin [5], however, showed that this could not be done efficiently.
Reference: [80] <author> A. Renyi. </author> <title> Probability Theory. </title> <publisher> North Holland, </publisher> <address> London, </address> <year> 1970. </year>
Reference-contexts: The Markov inequality can be used with any positive random variable, whereas the Bienayme-Chebyshev inequality applies when the variance of the random variable is known and bounded (see e.g. <ref> [80] </ref>). 27 Lemma 2.3 (Markov inequality) Let X be a positively-valued random variable with a bounded first moment = E [X].
Reference: [81] <author> R. L. Rivest and R. E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 411-420. </pages> <institution> Association for Computing Machinery, </institution> <year> 1989. </year>
Reference-contexts: However, in [4], she proved that a combination of membership and equivalence queries makes polynomial time identification of DFAs feasible. Since it uses membership queries, Angluin's algorithm implicitly assumes that a reset operation is available that returns the DFA to its initial state. Rivest and Schapire <ref> [81, 89] </ref> consider the case in which such a reset is not available and the learning has to be performed in a single continuous experiment (they restrict themselves to strongly connected DFAs, since, otherwise, the learner would get trapped in the first strongly connected component it enters and would never be <p> It, therefore, allows the learning agent to distinguish the state in which it ends up after executing it. Rivest and Schapire <ref> [81, 89] </ref> also consider the related concept of a distinguishing sequence. A distinguishing sequence is a sequence of actions d that has the property that, for all q 1 ; q 2 2 Q, q 1 hdi = q 2 hdi implies q 1 = q 2 . <p> It would therefore be interesting to characterize when and how we can dispense with the requirement for a reset operation. The results of Schapire and Rivest on how homing and distinguishing sequences can be used in DFAs in lieu of a reset <ref> [81, 82] </ref> could probably be useful there. We believe, however, that the most important open questions about computation-ally efficient reinforcement learning are those that concern the issue of generalization. The results presented in Chapter 5 and Chapter 6 shed some light on the subject, but much remains to be done.
Reference: [82] <author> R. L. Rivest and R. E. Schapire. </author> <title> Diversity-based inference of finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <year> 1993. </year>
Reference-contexts: They show how to dispense with both the reset and the equivalence queries when a distinguishing sequence for the automaton is provided. In addition, they show how such a 25 distinguishing sequence can be found in polynomial time for the special class of permutation automata. Rivest and Schapire <ref> [82, 89] </ref> also introduced the concept of diversity of a DFA, which is a measure of the complexity of the automaton with respect to its input-output behavior. They show that in some cases a diversity-based representation of an automaton is more natural and more compact than the traditional state-based representation. <p> It would therefore be interesting to characterize when and how we can dispense with the requirement for a reset operation. The results of Schapire and Rivest on how homing and distinguishing sequences can be used in DFAs in lieu of a reset <ref> [81, 82] </ref> could probably be useful there. We believe, however, that the most important open questions about computation-ally efficient reinforcement learning are those that concern the issue of generalization. The results presented in Chapter 5 and Chapter 6 shed some light on the subject, but much remains to be done.
Reference: [83] <author> Ronald R. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: also PAC learnable when the atoms can specify intervals on real-valued attributes, that is, when the "literals" in the Boolean formula have the form value attribute value: A generalization of the k-DNF and k-CNF representation classes that is of interest here, is the class of decision lists studied by Rivest <ref> [83] </ref>. <p> We denote the class of all such representation by k-DL n and define k-DL = S Rivest <ref> [83] </ref> proves that the class k-DL properly contains the classes k-DNF and k-CNF, and gives a polynomial-time algorithm for learning k-DL, for any fixed k. Decision lists thus form the most general class of Boolean formula that is known to be PAC learnable.
Reference: [84] <author> Johannes P. Ros. </author> <title> Learning Boolean Functions with Genetic Algorithms: a PAC Analysis. </title> <type> PhD thesis, </type> <institution> University of Pittsburgh, </institution> <address> Pittsburgh, PA, </address> <year> 1992. </year>
Reference-contexts: Convergence in the limit of genetic search methods under a number of conditions was proved by Eiben et al. [26], and Ros <ref> [84] </ref> recently described a PAC analysis of a class of genetic algorithms for concept learning. Unlike the Q-learning and TD-based algorithms, genetic algorithms are general optimization algorithms and they do not directly take advantage of the particular structure 12 of the reinforcement learning problem.
Reference: [85] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <pages> pages 210-229, </pages> <year> 1959. </year> <note> Reprinted in [91]. </note>
Reference-contexts: Unlike conventional techniques that adjust their prediction model by means of the difference between a prediction and the actual output (e.g., Widrow-Hoff rule [109]), the TD procedure adjusts its prediction model by means of the difference between temporally successive predictions. Similar techniques were used by Samuel <ref> [85] </ref> in its famous checker playing program, and by Witten [110]. Sutton [94] and Dayan [24] proved the convergence in the limit of the TD procedure in a number of cases. <p> Successful applications have been developed in a variety of domains, ranging from robotics [1, 88, 63], to industrial process control [23], to computer game playing <ref> [85, 97, 98] </ref>.
Reference: [86] <author> J. K. Satia and R. E. Lave, Jr. </author> <title> Markovian decision processes with uncertain transition probabilities. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 728-740, </pages> <year> 1973. </year>
Reference-contexts: Most methods proposed in the control theory and operations research literature are of this type. They are known there as adaptive control of Markov processes. Examples are provided by Silver [92], Martin [62], Satia and Lave <ref> [86] </ref>, Borkar and Varaiya [17], and Sato, Abe and Takeda [87]. These methods are usually based on a Bayesian formulation of the Markov decision process and differ on the details of the formulation, on the prior distributions assumed and on how the policy is updated based on the estimated probabilities.
Reference: [87] <author> M. Sato, K. Abe, and H. Takeda. </author> <title> Learing control of finite Markov chains with unknown transition probabilities. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 27 </volume> <pages> 502-505, </pages> <year> 1982. </year>
Reference-contexts: Most methods proposed in the control theory and operations research literature are of this type. They are known there as adaptive control of Markov processes. Examples are provided by Silver [92], Martin [62], Satia and Lave [86], Borkar and Varaiya [17], and Sato, Abe and Takeda <ref> [87] </ref>. These methods are usually based on a Bayesian formulation of the Markov decision process and differ on the details of the formulation, on the prior distributions assumed and on how the policy is updated based on the estimated probabilities.
Reference: [88] <author> S. Schaal and C. Atkeson. </author> <title> Robot juggling: An implementation of memory-based learning. </title> <journal> Control System Magazine, </journal> <volume> 14, </volume> <year> 1994. </year>
Reference-contexts: In the last five to ten years there has been a rapidly growing interest in reinforcement learning techniques as a base for intelligent control architectures [94, 104, 37, 108, 54]. Many methods have been proposed and a number of very successful applications have been developed <ref> [98, 88, 23] </ref>. However, as noted by several authors [97, 51], little theoretical guidance is available. The theoretical results either address very restricted types of problems, like learning a shortest path to a goal in a deterministic environment [59], or are based on assumptions inadequate for practical use. <p> Successful applications have been developed in a variety of domains, ranging from robotics <ref> [1, 88, 63] </ref>, to industrial process control [23], to computer game playing [85, 97, 98].
Reference: [89] <author> Robert E. Schapire. </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: We start by precisely defining the notion of concept. The following definitions are borrowed from [57] and <ref> [89] </ref>. Variants of these definitions can be found in most PAC related literature. Let X be a set that contains an encoding of all objects of interest in a 17 18 particular learning task. We call X the instance space or domain of the learning problem. <p> EX is assumed to run in unit time. In Valiant's model the quality of the hypothesis h 2 H produced by the learning algorithm after seeing a number of examples is judged using the same distribution D that was used to generate the examples. Following <ref> [89] </ref>, we write Prob x2D f (x)g to indicate the probability of predicate holding on instances x drawn from X according to distribution D. In particular, Prob x2D fh (x) 6= c (x)g is the probability that h will misclassify an instance x chosen at random according to D. <p> In addition, several other definitions of learnability have been shown to be equivalent to the PAC learnability, including polynomial predictability [42], group learnability 20 [57] and weak learnability <ref> [89] </ref>. <p> However, in [4], she proved that a combination of membership and equivalence queries makes polynomial time identification of DFAs feasible. Since it uses membership queries, Angluin's algorithm implicitly assumes that a reset operation is available that returns the DFA to its initial state. Rivest and Schapire <ref> [81, 89] </ref> consider the case in which such a reset is not available and the learning has to be performed in a single continuous experiment (they restrict themselves to strongly connected DFAs, since, otherwise, the learner would get trapped in the first strongly connected component it enters and would never be <p> It, therefore, allows the learning agent to distinguish the state in which it ends up after executing it. Rivest and Schapire <ref> [81, 89] </ref> also consider the related concept of a distinguishing sequence. A distinguishing sequence is a sequence of actions d that has the property that, for all q 1 ; q 2 2 Q, q 1 hdi = q 2 hdi implies q 1 = q 2 . <p> They show how to dispense with both the reset and the equivalence queries when a distinguishing sequence for the automaton is provided. In addition, they show how such a 25 distinguishing sequence can be found in polynomial time for the special class of permutation automata. Rivest and Schapire <ref> [82, 89] </ref> also introduced the concept of diversity of a DFA, which is a measure of the complexity of the automaton with respect to its input-output behavior. They show that in some cases a diversity-based representation of an automaton is more natural and more compact than the traditional state-based representation.
Reference: [90] <author> Robert E. Schapire and Manfred K. Warmuth. </author> <title> On the worst-case analysis of temporal-difference learning algorithms. </title> <booktitle> In Proc. 11th International Conference on Machine Learning, </booktitle> <pages> pages 266-274. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Schapire and Warmuth recently analyzed a variant of the TD algorithm and bounded the error that the algorithm makes in the worst-case when it estimates the value of the policy it uses <ref> [90] </ref>. Their analysis does not however carry over to the case where the current value estimates are used to update the policy. A slightly different use of the TD technique for sequential task learning was proposed by Watkins [104] in his Q-learning algorithm. <p> The main challenge in trying to perform a PAC analysis of such methods (and of reinforcement learning in general) is that the distribution of the states visited in the environment depends on the policy that is used and changes with each policy. As shown by Schapire and Warmuth <ref> [90] </ref>, it is not too difficult to efficiently learn a PAC approximation ^v of the value of a policy while using that particular policy .
Reference: [91] <author> J. W. Shavlik and T. G. Dietterich, </author> <title> editors. </title> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: An overview of this diversity can be found, for instance, in the survey paper by Mitchell et al. [72] and in different available collections of learning papers <ref> [67, 68, 58, 91] </ref>. The approach taken in this dissertation is that of computational learning theory. The goal of this area of research is to develop a sound theoretical foundation for the study of machine learning.
Reference: [92] <author> Edward A. Silver. </author> <title> Markovian decision processes with uncertain transition probabilities or rewards. </title> <type> Technical Report 1, </type> <institution> Operations Research Center, Massachusetts Institute of Technology, </institution> <year> 1963. </year>
Reference-contexts: Most methods proposed in the control theory and operations research literature are of this type. They are known there as adaptive control of Markov processes. Examples are provided by Silver <ref> [92] </ref>, Martin [62], Satia and Lave [86], Borkar and Varaiya [17], and Sato, Abe and Takeda [87].
Reference: [93] <author> R. S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1984. </year>
Reference-contexts: Some of these methods are purely heuristical, others are based on results on the collective behavior of stochastic learning automata (see [74]) and are shown to converge in the limit. In the last few years, a family of direct methods, based on Sutton's temporal difference (TD) procedure <ref> [93, 94] </ref>, have attracted much attention. The TD procedure is an incremental learning technique to predict a value from sequences of observations.
Reference: [94] <author> R. S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year> <month> 125 </month>
Reference-contexts: In the last five to ten years there has been a rapidly growing interest in reinforcement learning techniques as a base for intelligent control architectures <ref> [94, 104, 37, 108, 54] </ref>. Many methods have been proposed and a number of very successful applications have been developed [98, 88, 23]. However, as noted by several authors [97, 51], little theoretical guidance is available. <p> Some of these methods are purely heuristical, others are based on results on the collective behavior of stochastic learning automata (see [74]) and are shown to converge in the limit. In the last few years, a family of direct methods, based on Sutton's temporal difference (TD) procedure <ref> [93, 94] </ref>, have attracted much attention. The TD procedure is an incremental learning technique to predict a value from sequences of observations. <p> Similar techniques were used by Samuel [85] in its famous checker playing program, and by Witten [110]. Sutton <ref> [94] </ref> and Dayan [24] proved the convergence in the limit of the TD procedure in a number of cases. <p> Similar navigation tasks have been considered by many authors (e.g., <ref> [9, 94, 108, 59] </ref>). 13 Consider the mobile robot of Figure 1.2 that has just been put in its "grid-world" environment. The robot has no a priori knowledge of the environment and does not know the consequences of its actions, where the obstacles are, and where the goal location is.
Reference: [95] <author> R. S. Sutton. </author> <title> Integrating architectures for learning, planning and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Examples are the methods proposed by McCallum [64], Mahadevan [61], Clouse and Utgoff [22], and Koenig and Simmons [59]. Whitehead and Ballard [108] describe an extension of Q-learning that integrates the control of an active sensory system with the decision-action system. Sutton's DYNA architecture <ref> [95] </ref> augments Q-learning with a mechanism that learns a model of the state-action transitions. The environment model is then used to generate hypothetical experiments, thus reducing the number of actual experiments needed.
Reference: [96] <author> R. S. Sutton and A. G. Barto. </author> <title> Reinforcement learning. Course notes, </title> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1997. </year>
Reference-contexts: 2 describes Valiant's model in some details and gives an overview of other relevant results in computational learning theory, notably results on the inference of finite automaton. 1.1 Reinforcement Learning Reinforcement learning is a computational approach to the problem of learning a task by trial-and-error interaction with a dynamic environment <ref> [51, 96] </ref>. It dates back to the early days of computer science and has its roots in the psychology of animal learning, from which it takes its name. <p> We describe this model below. A detailed justification of its use a framework for the study of reinforcement learning can be found in [9] and <ref> [96] </ref>. 4 PSfrag replacements Agent Environment input x t reward R t action a t state s t 1.1.1 Sequential decision tasks Humans and animals constantly have to make sequences of actions to achieve some goals or to bring about circumstances favorable to their survival.
Reference: [97] <author> Gerald Tesauro. </author> <title> Temporal difference learning of backgammon strategy. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 451-457. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Many methods have been proposed and a number of very successful applications have been developed [98, 88, 23]. However, as noted by several authors <ref> [97, 51] </ref>, little theoretical guidance is available. The theoretical results either address very restricted types of problems, like learning a shortest path to a goal in a deterministic environment [59], or are based on assumptions inadequate for practical use. <p> Successful applications have been developed in a variety of domains, ranging from robotics [1, 88, 63], to industrial process control [23], to computer game playing <ref> [85, 97, 98] </ref>.
Reference: [98] <author> Gerald Tesauro. </author> <title> TD-gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 215-219, </pages> <year> 1994. </year>
Reference-contexts: In the last five to ten years there has been a rapidly growing interest in reinforcement learning techniques as a base for intelligent control architectures [94, 104, 37, 108, 54]. Many methods have been proposed and a number of very successful applications have been developed <ref> [98, 88, 23] </ref>. However, as noted by several authors [97, 51], little theoretical guidance is available. The theoretical results either address very restricted types of problems, like learning a shortest path to a goal in a deterministic environment [59], or are based on assumptions inadequate for practical use. <p> Successful applications have been developed in a variety of domains, ranging from robotics [1, 88, 63], to industrial process control [23], to computer game playing <ref> [85, 97, 98] </ref>. <p> Many practical applications of reinforcement learning are based on this approach and use a TD algorithm or Q-learning where the value function is represented by a function approximator (often a simple linear combination or a neural network) that maps the state representation into a value <ref> [104, 98, 19] </ref>. Boyan and Moore [19] show that this will not work in all cases, and that even on simple environments that are completely known, value iteration combined with function approximation can diverge and fail to produce an optimal policy.
Reference: [99] <author> Sebastian B. Thrun. </author> <title> The role of exploration in learning control. </title> <editor> In D. A. White and D. A. Sofge, editors, </editor> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: An other important characteristic of reinforcement learning which distinguishes it from supervised learning is that it is an on-line learning problem. The agent needs to learn at the same time that it performs the task. This leads to the so-called trade-off between exploration and exploitation <ref> [99, 51] </ref>. On the one hand the agent needs to try all the actions to gather information about the different actions and states of the environment and determine which action is best in which state.
Reference: [100] <author> J. N. Tsitsiklis and B. Van Roy. </author> <title> Feature-based methods for large scale dynamic programming. </title> <journal> Machine Learning, </journal> <volume> 22(1), </volume> <year> 1996. </year>
Reference-contexts: Some recent results by Gordon [36] and by Tsitsiklis and Van Roy <ref> [100] </ref>, however, show how using some special kinds of function approximator can guarantee convergence, though not necessarily to the optimal values.
Reference: [101] <author> L. G. Valiant and M. K. Warmuth, </author> <title> editors. </title> <booktitle> Proceedings of the Fourth Annual Workshop on Computational Learning Theory. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: This overview is by no mean intended to be exhaustive, and the interested reader is referred to the proceedings of the annual conference on computational learning theory <ref> [31, 101, 41, 77] </ref> for a more representative sampling of the wide variety of recent results in the PAC model. We begin by defining three basic representation classes of Boolean formulae.
Reference: [102] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communication of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: We also 3 consider the problem of generalization in reinforcement learning and show how in some cases the underlying structure of the environment can be exploited to achieve faster learning. Our reinforcement learning model is based on the notion of probably approximately correct (PAC) learning, introduced by Valiant <ref> [102] </ref> in the context of concept learning from examples. Valiant's learning framework defines a formal and general standard by which learning algorithms can be evaluated and since its introduction in 1984 it has taken a predominant place in computational learning theory. <p> Finally, Section 2.4 states a number of well-known mathematical results that we are going to use repeatedly in the analysis of our reinforcement learning algorithms. 2.1 PAC Learning In 1984 Valiant <ref> [102] </ref> introduced a new formal model of concept learning, the so-called distribution-free or probably approximately correct (PAC) learning model. <p> In his original paper defining the PAC model, Valiant <ref> [102] </ref> gives a polynomial-time algorithm for learning the representation class of monomials (in term of themselves). He then extends this algorithm to show that, for any fixed k, the representation classes k-DNF and k-CNF are PAC learnable in time polynomial in O (n k ).
Reference: [103] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16 </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: A Boolean formula can then represent a complex condition on the state, and we can use such a condition to specify when a particular action has to be executed. Furthermore, results from Haussler [44] extend this to real-valued attributes. Using the Vapnik-Chervonenkis dimension <ref> [103] </ref> to quantify the expressive power of infinite domain concepts, he showed that the representation classes of monomials, k-DNF and k-CNF are also PAC learnable when the atoms can specify intervals on real-valued attributes, that is, when the "literals" in the Boolean formula have the form value attribute value: A generalization
Reference: [104] <author> C. Watkins. </author> <title> Learning From Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: In the last five to ten years there has been a rapidly growing interest in reinforcement learning techniques as a base for intelligent control architectures <ref> [94, 104, 37, 108, 54] </ref>. Many methods have been proposed and a number of very successful applications have been developed [98, 88, 23]. However, as noted by several authors [97, 51], little theoretical guidance is available. <p> Their analysis does not however carry over to the case where the current value estimates are used to update the policy. A slightly different use of the TD technique for sequential task learning was proposed by Watkins <ref> [104] </ref> in his Q-learning algorithm. <p> In other words, when action k is taken in state i, the estimate Q (i; k) is made to resemble more the reward actually received on that step plus the reward that the agent expects to receive from the new state of the environment. Watkins <ref> [104] </ref> and Watkins and Dayan [105] prove that Q-learning converges to an optimal policy with probability one in the limit, when every action is executed infinitely many times in every state of the environment. <p> Many practical applications of reinforcement learning are based on this approach and use a TD algorithm or Q-learning where the value function is represented by a function approximator (often a simple linear combination or a neural network) that maps the state representation into a value <ref> [104, 98, 19] </ref>. Boyan and Moore [19] show that this will not work in all cases, and that even on simple environments that are completely known, value iteration combined with function approximation can diverge and fail to produce an optimal policy.
Reference: [105] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Watkins [104] and Watkins and Dayan <ref> [105] </ref> prove that Q-learning converges to an optimal policy with probability one in the limit, when every action is executed infinitely many times in every state of the environment.
Reference: [106] <author> R. M. Wheeler and K. S. Narendra. </author> <title> Decentralized learning in finite Markov chains. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 31 </volume> <pages> 519-526, </pages> <year> 1986. </year>
Reference-contexts: Instead, the agent executes some actions, observes their consequences and tries to directly adjust its policy to improve its performances. Examples of such methods are those of Fu and Waltz [30], Mendel and McLaren [66], Witten [110], and Wheeler and Narendra <ref> [106] </ref>. Some of these methods are purely heuristical, others are based on results on the collective behavior of stochastic learning automata (see [74]) and are shown to converge in the limit.
Reference: [107] <author> S. D. Whitehead. </author> <title> A complexity analysis of cooperative mechanism in reinforcement learning. </title> <booktitle> In Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> pages 607-613, </pages> <year> 1991. </year>
Reference-contexts: Watkins [104] and Watkins and Dayan [105] prove that Q-learning converges to an optimal policy with probability one in the limit, when every action is executed infinitely many times in every state of the environment. Whitehead <ref> [107] </ref>, however, shows that even on simple deterministic problems, Q-learning can require exponential time in the number of states to start to converge to an optimal policy. Since their introduction, a number of variations and extensions of the Q-learning and TD algorithms have been proposed in the literature. <p> For this specific problem, Koenig and Simmons [59] show that Q-learning (see Section 1.2) takes polynomial time in the number of states in the worst case using the action-penalty representation, whereas the same algorithm can require exponential time when the goal-reward representation is used <ref> [107] </ref>. This illustrates the fact that the complexity of a reinforcement learning algorithm on a given problem can dramatically depend on the chosen task formulation.
Reference: [108] <author> S. D. Whitehead and D. H. Ballard. </author> <title> Learning to perceive and act by trial and error. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 45-83, </pages> <year> 1991. </year>
Reference-contexts: In the last five to ten years there has been a rapidly growing interest in reinforcement learning techniques as a base for intelligent control architectures <ref> [94, 104, 37, 108, 54] </ref>. Many methods have been proposed and a number of very successful applications have been developed [98, 88, 23]. However, as noted by several authors [97, 51], little theoretical guidance is available. <p> For instance, a robot equipped with a camera might be able to recognize whether it is facing a wall, a hall or a door, but might not be able to tell which wall or door it is facing. This is sometimes referred to as the perceptual aliasing <ref> [108] </ref> or hidden state [65] problem. When the agent does not have a complete and noise-free perception of its environment the system can no longer be modeled adequately using a a simple Markov decision process. <p> Most of these variations seek to speed-up the learning process. Examples are the methods proposed by McCallum [64], Mahadevan [61], Clouse and Utgoff [22], and Koenig and Simmons [59]. Whitehead and Ballard <ref> [108] </ref> describe an extension of Q-learning that integrates the control of an active sensory system with the decision-action system. Sutton's DYNA architecture [95] augments Q-learning with a mechanism that learns a model of the state-action transitions. <p> Similar navigation tasks have been considered by many authors (e.g., <ref> [9, 94, 108, 59] </ref>). 13 Consider the mobile robot of Figure 1.2 that has just been put in its "grid-world" environment. The robot has no a priori knowledge of the environment and does not know the consequences of its actions, where the obstacles are, and where the goal location is.
Reference: [109] <author> B. Widrow and M. E. Hoff. </author> <title> Adaptive switching circuits. </title> <booktitle> In 1960 WESCON Convention Record, Part IV, </booktitle> <pages> pages 96-104, </pages> <year> 1960. </year>
Reference-contexts: The TD procedure is an incremental learning technique to predict a value from sequences of observations. Unlike conventional techniques that adjust their prediction model by means of the difference between a prediction and the actual output (e.g., Widrow-Hoff rule <ref> [109] </ref>), the TD procedure adjusts its prediction model by means of the difference between temporally successive predictions. Similar techniques were used by Samuel [85] in its famous checker playing program, and by Witten [110].
Reference: [110] <author> Ian H. Witten. </author> <title> An adaptive optimal controller for discrete-time Markov environments. </title> <journal> Information and Control, </journal> <volume> 34 </volume> <pages> 286-295, </pages> <year> 1977. </year>
Reference-contexts: Instead, the agent executes some actions, observes their consequences and tries to directly adjust its policy to improve its performances. Examples of such methods are those of Fu and Waltz [30], Mendel and McLaren [66], Witten <ref> [110] </ref>, and Wheeler and Narendra [106]. Some of these methods are purely heuristical, others are based on results on the collective behavior of stochastic learning automata (see [74]) and are shown to converge in the limit. <p> Similar techniques were used by Samuel [85] in its famous checker playing program, and by Witten <ref> [110] </ref>. Sutton [94] and Dayan [24] proved the convergence in the limit of the TD procedure in a number of cases.
References-found: 111

