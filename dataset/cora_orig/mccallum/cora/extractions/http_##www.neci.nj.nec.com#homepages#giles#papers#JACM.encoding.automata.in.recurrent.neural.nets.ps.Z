URL: http://www.neci.nj.nec.com/homepages/giles/papers/JACM.encoding.automata.in.recurrent.neural.nets.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: http://www.neci.nj.nec.com
Email: giles]@research.nj.nec.com.  
Title: Constructing Deterministic FiniteState Automata in Recurrent Neural Networks  
Author: CHRISTIAN W. OMLIN AND C. LEE GILES 
Keyword: General Terms: Algorithms, Theory, Verification Additional Key Words and Phrases: Automata, connectionism, knowledge encoding, neural networks, nonlinear dynamics, recurrent neural networks, rules, stability  
Note: Authors' present addresses: C. W. Omlin,  C. L. Giles,  Journal of the ACM, Vol. 43, No. 6, November 1996, pp. 937-972.  
Address: Princeton, New Jersey  College Park, Maryland  4 Independence Way, Princeton, NJ 08540;  College Park, MD 20742, email:[omlin,  
Affiliation: NEC Research Institute,  NEC Research Institute, Princeton, New Jersey, and UMIACS, University of Maryland,  NEC Research Institute,  UMIACS, University of Maryland,  
Abstract: Recurrent neural networks that are trained to behave like deterministic finitestate automata (DFAs) can show deteriorating performance when tested on long strings. This deteriorating performance can be attributed to the instability of the internal representation of the learned DFA states. The use of a sigmoidal discriminant function together with the recurrent structure contribute to this instability. We prove that a simple algorithm can construct second-order recurrent neural networks with a sparse interconnection topology and sigmoidal discriminant function such that the internal DFA state representations are stable, that is, the constructed network correctly classifies strings of arbitrary length. The algorithm is based on encoding strengths of weights directly into the neural network. We derive a relationship between the weight strength and the number of DFA states for robust string classification. For a DFA with n states and m input alphabet symbols, the constructive algorithm generates a programmed neural network with O(n) neurons and O(mn) weights. We compare our algorithm to other methods proposed in the literature. Categories and Subject Descriptors: B.2.2 [Arithmetic and Logic Structures]: Performance Analysis and Design Aids-simulation, verification; F.1.1 [Computation by Abstract Devices]: Models of Computation-automata; relations among models; self-modifying machine; G.1.0 [Numerical Analysis]: Generalstability; G.1.2 [Numerical Analysis]: Approximation-nonlinear approximation; I.1.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-representations; I.1.5.1 [Pattern Recognition]: Modes-neural nets Permission to make digital / hard copy of part or all of this work for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication, and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery (ACM), Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and / or a fee. 
Abstract-found: 1
Intro-found: 1
Reference: <author> ALON, N., DEWDNEY, A. K., AND OTT, T. J. </author> <year> 1991. </year> <title> Efficient simulation of finite automata by neural nets, </title> <journal> JACM 38, </journal> <volume> 2 (Apr.), </volume> <pages> 495-514. </pages>
Reference: <author> BARNSLEY, M. </author> <year> 1988. </year> <title> Fractals Everywhere. </title> <publisher> Academic Press, </publisher> <address> San Diego, Calif. </address>
Reference: <author> CASEY, M. </author> <year> 1996. </year> <title> The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction, </title> <journal> Neural Comput. </journal> <volume> 8, 6, </volume> <pages> 1135-1178. </pages>
Reference: <author> ELMAN, J. </author> <year> 1990. </year> <title> Finding structure in time. </title> <journal> Cogn. Sci. </journal> <volume> 14, 179 -211. </volume>
Reference: <author> FRASCONI, P., GORI, M., MAGGINI, M., AND SODA, G. </author> <year> 1996. </year> <title> Representation of finite state automata in recurrent radial basis function networks, </title> <journal> Mach. Learn. </journal> <volume> 23, </volume> <pages> 5-32. </pages>
Reference: <author> FRASCONI, P., GORI, M., MAGGINI, M., AND SODA, G. </author> <year> 1991. </year> <title> A unified approach for integrating explicit knowledge and learning by example in recurrent networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> vol. 1. </volume> <publisher> IEEE, </publisher> <address> New York, p. </address> <month> 811. </month>
Reference: <author> FRASCONI, P., GORI, M., AND SODA, G. </author> <year> 1993. </year> <title> Injecting nondeterministic finite state automata into recurrent networks. </title> <type> Tech. Rep. </type> <institution> Dipartimento di Sistemi e Informatica, Universita di Firenze, Italy, Florence, Italy. </institution>
Reference: <author> GEMAN, S., BIENENSTOCK, E., AND DOURSTAT, R. </author> <year> 1992. </year> <title> Neural networks and the bias/variance dilemma, </title> <journal> Neural Comput. </journal> <volume> 4, 1, </volume> <pages> 1-58. </pages>
Reference: <author> GILES, C., CHEN, D., MILLER, C., CHEN, H., SUN, G., AND LEE, Y. </author> <year> 1991. </year> <title> Second-order recurrent neural networks for grammatical inference. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks 1991, vol. II. IEEE, </booktitle> <address> New York, </address> <pages> pp. 273-281. </pages>
Reference: <author> GILES, C., KUHN, G., AND WILLIAMS, R. </author> <year> 1994. </year> <title> Special issue on Dynamic recurrent neural networks: </title> <journal> Theory and applications, IEEE Trans. Neural Netw. </journal> <volume> 5, </volume> <pages> 2. </pages>
Reference: <author> GILES, C., MILLER, C., CHEN, D., CHEN, H., SUN, G., AND LEE, Y. </author> <year> 1992. </year> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Comput. </journal> <volume> 4, 3, </volume> <pages> 380. </pages>
Reference: <author> GILES, C., AND OMLIN, C. </author> <year> 1992. </year> <title> Inserting rules into recurrent neural networks. In Neural Networks for Signal Processing II, </title> <booktitle> Proceedings of the 1992 IEEE Workshop (S. </booktitle> <editor> Kung, F. Fallside, J. </editor> <publisher> A. </publisher>
Reference: <editor> Sorenson, and C. Kamm, eds.) </editor> <publisher> IEEE, </publisher> <address> New York, </address> <pages> pp. </pages> <month> 13-22. </month> <title> 971Finite-State Automata in Neural Networks GILES, </title> <editor> C., AND OMLIN, C. </editor> <year> 1993. </year> <title> Rule refinement with recurrent neural networks. </title> <booktitle> In Proceedings IEEE International Conference on Neural Networks (ICNN'93), vol. II. IEEE, </booktitle> <address> New York, </address> <pages> pp. </pages> <address> 801- 806. </address>
Reference: <author> HAYKIN, S. </author> <year> 1994. </year> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> MacMillan, </publisher> <address> New York. </address>
Reference: <author> HIRSCH, M. </author> <year> 1989. </year> <title> Convergent activation dynamics in continuous-time neural networks. </title> <journal> Neural Netw. </journal> <volume> 2, </volume> <pages> 331-351. </pages>
Reference: <author> HIRSCH, M. </author> <year> 1994. </year> <title> Saturation at high gain in discrete time recurrent networks. </title> <journal> Neural Netw. </journal> <volume> 7, 3, 449 - 453. </volume>
Reference: <author> HOPCROFT, J., AND ULLMAN, J. </author> <year> 1979. </year> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address>
Reference: <author> HORNE, B., AND HUSH, D. </author> <year> 1996. </year> <title> Bounds on the complexity of recurrent neural network implementations of finite state machines. </title> <journal> Neural Netw. </journal> <volume> 9, 2, </volume> <pages> 243-252. </pages>
Reference: <author> MACLIN, R., AND SHAVLIK, J. </author> <year> 1993. </year> <title> Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. </title> <journal> Mach. Learn. </journal> <volume> 11, </volume> <pages> 195-215. </pages>
Reference: <author> MEAD, C. </author> <year> 1989. </year> <title> Analog VLSI and Neural Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address>
Reference: <author> MINSKY, M. </author> <year> 1967. </year> <title> Computation: Finite and Infinite Machines. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, N.J., </address> <pages> pp. </pages> <address> 32- 66 (Chap. </address> <month> 3). </month>
Reference: <author> OMLIN, C., AND GILES, C. </author> <year> 1996a. </year> <title> Rule revision with recurrent neural networks. </title> <journal> IEEE Trans. Knowl. Data Eng. </journal> <volume> 8, 1, </volume> <pages> 183-188. </pages>
Reference: <author> OMLIN, C., AND GILES, C. </author> <year> 1996b. </year> <title> Stable encoding of large finitestate automata in recurrent neural networks with sigmoid discriminants. </title> <journal> Neural Comput. </journal> <volume> 8, 4, 675- 696. </volume>
Reference: <author> OMLIN, C., AND GILES, C. </author> <year> 1992. </year> <title> Training second-order recurrent neural networks using hints, </title> <booktitle> in Proceedings of the 9th International Conference on Machine Learning (San Mateo, </booktitle> <address> Calif.), D. </address>
Reference: <editor> Sleeman and P. Edwards, eds. </editor> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, Calif., </address> <pages> pp. 363-368. </pages>
Reference: <author> POLLACK, J. </author> <year> 1991. </year> <title> The induction of dynamical recognizers. </title> <journal> Mach. Learn. </journal> <volume> 7, </volume> <pages> 227-252. </pages>
Reference: <author> SERVAN-SCHREIBER, D., CLEEREMANS, A., AND MCCLELLAND, J. </author> <year> 1991. </year> <title> Graded state machine: The representation of temporal contingencies in simple recurrent networks. </title> <journal> Mach. Learn. </journal> <volume> 7, </volume> <pages> 161. </pages>
Reference: <author> SHAVLIK, J. </author> <year> 1994. </year> <title> Combining symbolic and neural learning. </title> <journal> Mach. Learn. </journal> <volume> 14, 3, </volume> <pages> 321-331. </pages>
Reference: <author> SHEU, B. J. </author> <year> 1995. </year> <title> Neural Information Processing and VLSI. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Mass. </address>
Reference: <author> TINO, P., HORNE, B. AND GILES, C. </author> <year> 1995. </year> <title> Fixed points in two-neuron discrete time recurrent networks: Stability and bifurcation considerations. </title> <type> Tech. Rep. </type> <institution> UMIACS-TR-95-51. Institute for Advanced Computer Studies, Univ. Maryland, College Park, Md. </institution>
Reference: <author> TOWELL, G., SHAVLIK, J., AND NOORDEWIER, M. </author> <year> 1990. </year> <title> Refinement of approximately correct domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the 8th National Conference on Artificial Intelligence (San Mateo, Calif.) </booktitle> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, Calif., p. </address> <month> 861. </month>
Reference: <author> WATROUS, R., AND KUHN, G. </author> <year> 1992. </year> <title> Induction of finitestate languages using second-order recurrent networks. </title> <journal> Neural Comput. </journal> <volume> 4, 3, </volume> <pages> 406. </pages>
Reference: <author> ZENG, Z., GOODMAN, R., AND SMYTH, P. </author> <year> 1993. </year> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Comput. </journal> <volume> 5, </volume> <pages> 6, </pages> <note> 976 -990. RECEIVED MAY 1994; REVISED DECEMBER 1995; ACCEPTED MAY 1996 Journal of the ACM, Vol. 43, No. 6, </note> <month> November </month> <year> 1996. </year> <note> 972 C. </note> <editor> W. OMLIN AND C. L. </editor> <publisher> GILES </publisher>
References-found: 33

