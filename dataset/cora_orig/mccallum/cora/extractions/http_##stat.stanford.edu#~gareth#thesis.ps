URL: http://stat.stanford.edu/~gareth/thesis.ps
Refering-URL: http://stat.stanford.edu/~gareth/
Root-URL: 
Title: MAJORITY VOTE CLASSIFIERS: THEORY AND APPLICATIONS  
Author: Gareth James 
Degree: A DISSERTATION SUBMITTED TO THE DEPARTMENT OF STATISTICS AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY  
Date: May 1998  
Abstract-found: 0
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <title> Bagging predictors. </title> <journal> Machine Learning 26, </journal> <volume> No. </volume> <pages> 2 : 123-140. </pages>
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Bias, variance, and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> Statistics Department, University of California Berkeley. </institution>
Reference-contexts: CLASSICAL THEORIES 76 and Kong's it is possible for the Bayes Classifier to have positive bias * and the variance term will be negative whenever the bias is non zero. Breiman Breiman's definitions <ref> (Breiman, 1996b) </ref> are in terms of an "aggregated" classifier which is the equivalent of SC for a 0-1 loss function. He defines a classifier as unbiased at X if SY = SC and lets U be the set of all X at which C is unbiased.
Reference: <author> Breiman, L. </author> <year> (1997). </year> <note> Arcing the edge. Unpublished </note> . 
Reference-contexts: Despite these problems it seems clear that the Normal Model has potential which deserves further exploration. 4.4.3 Other Modern Theories The Modern theories are still at an early stage. Breiman has written a couple of papers <ref> (Breiman, 1997 and Breiman, 1998) </ref> with a similar approach to that of Schapire's. He defines a quantity which he calls the Edge. The edge is equivalent to the margin for a two CHAPTER 4. MODERN THEORIES 97 class case but is slightly different for larger numbers of classes.
Reference: <author> Breiman, L. </author> <year> (1998). </year> <title> Prediction games and arcing algorithms. </title> <note> Unpublished </note> . 
Reference: <author> Breiman, L., J. Friedman, R. Olshen, and C. </author> <title> Stone (1984). Classification and Regresion Trees. </title> <publisher> Wadsworth. </publisher>
Reference: <author> Dietterich, T. and G. </author> <title> Bakiri (1995). Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research 2 : 263-286. </journal>
Reference-contexts: Dietterich's belief that by using a matrix with a large degree of redundancy it would be possible to produce a classifier that made very few overall classification errors even if some of the individual Base Classifiers were incorrect. 2.1.3 The One vs Rest PICT A simple but commonly used method <ref> (see for example Dietterich and Bakiri, 1995 or Nilsson, 1965) </ref> for handling a multi-class problem, when one has only a binary classifier, is to do the following. CHAPTER 2.
Reference: <author> Dietterich, T. G. and E. B. </author> <title> Kong (1995). Error-correcting output coding corrects bias and variance. </title> <booktitle> Proceedings of the 12th International Conference on Machine Learning: </booktitle> <publisher> 313-321 Morgan Kaufmann. </publisher>
Reference-contexts: Dietterich's belief that by using a matrix with a large degree of redundancy it would be possible to produce a classifier that made very few overall classification errors even if some of the individual Base Classifiers were incorrect. 2.1.3 The One vs Rest PICT A simple but commonly used method <ref> (see for example Dietterich and Bakiri, 1995 or Nilsson, 1965) </ref> for handling a multi-class problem, when one has only a binary classifier, is to do the following. CHAPTER 2.
Reference: <author> Freund, Y. and R. </author> <title> Schapire (1996). Experiments with a new boosting algorithm. </title> <booktitle> Machine Learning : Proceedings of the Thirteenth International Conference. </booktitle>
Reference-contexts: There are several different algorithms to implement the Boosting procedure. The most common one is known as AdaBoost <ref> (Freund and Schapire, 1996) </ref>. It is possible to use AdaBoost in either a resampling or deterministic mode. We will describe here the resampling version. CHAPTER 2.
Reference: <author> Friedman, J. </author> <year> (1996a). </year> <title> Another approach to polychotomous classification. </title> <type> Technical Report, </type> <institution> Department of Statistics, Stanford University. </institution>
Reference-contexts: MaVLs which were introduced in the previous section are examples of PICTs. However, there are many PICTs which are not MaVLs. For example the Pairwise Coupling procedure, first suggested by Friedman <ref> (Friedman, 1996a) </ref> and later extended by Hastie and Tibshirani (Hastie and Tibshirani, 1996), is not a MaVL but is a PICT. Most of the classifiers introduced in Chapter 2 are examples of PICTs. CHAPTER 1.
Reference: <author> Friedman, J. </author> <year> (1996b). </year> <title> On bias, variance, 0/1-loss, and the curse of dimensionality. </title> <type> Technical Report, </type> <institution> Department of Statistics, Stanford University. </institution>
Reference-contexts: This observation alone can not provide the answer, however, because it has been clearly demonstrated <ref> (see for example Friedman, 1996b) </ref> that a reduction in variance of the probability estimates does not necessarily correspond to a reduction in the error rate. The quantity that we are interested in is not the individual probabilities but arg max j p j .
Reference: <author> Hastie, T. and R. </author> <month> Tibshirani </month> <year> (1994). </year> <title> Handwritten digit recognition via deformable prototypes. </title> <type> Technical Report, </type> <institution> AT&T Bell Labs. </institution> <note> 111 BIBLIOGRAPHY 112 Hastie, </note> <author> T. and R. </author> <month> Tibshirani </month> <year> (1996). </year> <title> Classification by pairwise coupling. </title> <type> Technical Report, </type> <institution> Department of Statistics, Stanford University. </institution>
Reference-contexts: Introduction Consider the digits in Figure 1.1 <ref> (Reprinted from Hastie and Tibshirani, 1994) </ref>. These are scanned in images of hand written zip codes from the US Postal Service Zip Code Data Set. The US postal service wishes to design a system to read hand written zip codes on mail.
Reference: <author> Hoeffding, W. </author> <year> (1963). </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association. </journal>
Reference-contexts: Note that Theorem 5 does not depend on Assumption 2.3. This tells us that the error rate for the ECOC PICT is equal to the error rate using arg max i i plus a term which decreases exponentially in the limit. The result can be proved using Hoeffding's inequality <ref> (Hoeffding, 1963) </ref>. As a simple corollary of Theorem 5 it is possible to show that, when the Bayes Classifier is used as the Base Classifier, the ECOC error rate approaches the Bayes error rate exponentially fast. CHAPTER 2.
Reference: <author> Kohavi, R. and D. </author> <title> Wolpert (1996). Bias plus variance decomposition for zero-one loss functions. </title> <booktitle> Machine Learning : Proceedings of the Thirteenth International Conference. </booktitle>
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning Machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Schapire, R., Y. Freund, P. Bartlett, and W. </author> <title> Lee (1997). Boosting the margin : A new explanation for the effectiveness of voting methods. </title> <note> (available at http://www.research.att.com/~yoav). </note>
Reference-contexts: Section 4.3 details an alternative method of utilizing the margin which we call the Normal model. The final section provides a summary and conclusion. 4.1 Margins In their paper Boosting the Margin : A new explanation for the effectiveness of voting methods <ref> (Schapire et al., 1997) </ref>, Schapire et al. introduce a quantity which they call the Margin. Based on this quantity they prove a bound on the expected test error rate and then use this bound to develop a theory to explain the success of Boosting. 83 CHAPTER 4. <p> MODERN THEORIES 85 4.1.2 A Bound on the Expected Test Error Rate Based on these definitions it is possible to prove a bound on the expected test error rate in terms of the training margin. Theorem 9 <ref> (Schapire et al., 1997) </ref> Let D be the test distribution of interest and let S be a sample of n elements drawn independently at random from D. <p> CHAPTER 4. MODERN THEORIES 86 Theorem 10 <ref> (Schapire et al., 1997) </ref> Suppose the Base Classifier, when called by AdaBoost, generates classifiers with weighted training errors * 1 ; : : : ; * B . <p> This last result is interesting because it has been demonstrated empirically that while AdaBoost works well at increasing low margins it tends to compensate by decreasing margins that are close to one <ref> (see Schapire et al., 1997) </ref>. These results would explain why that is a good trade off to achieve in practice. 4.4 Conclusion 4.4.1 The Schapire Theories Schapire et al.'s results proving bounds on the test error rate and also the training margin are very interesting.
Reference: <author> Tibshirani, R. </author> <year> (1996). </year> <title> Bias, variance and prediction error for classification rules. </title> <type> Technical Report, </type> <institution> Department of Statistics, University of Toronto. </institution>
Reference-contexts: MaVLs which were introduced in the previous section are examples of PICTs. However, there are many PICTs which are not MaVLs. For example the Pairwise Coupling procedure, first suggested by Friedman (Friedman, 1996a) and later extended by Hastie and Tibshirani <ref> (Hastie and Tibshirani, 1996) </ref>, is not a MaVL but is a PICT. Most of the classifiers introduced in Chapter 2 are examples of PICTs. CHAPTER 1. INTRODUCTION 9 1.6 Summary of Chapters In Chapter 2 we study a recently proposed classifier that is motivated by Error Correcting Output Coding ideas.
Reference: <author> Venables, W. and B. </author> <title> Ripley (1994). Modern Applied Statistics with S-Plus. 1st edn., </title> <publisher> Springer-Verlag. </publisher>
References-found: 17

