URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-405.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs7322_98_spring/readings.html
Root-URL: 
Email: brand@media.mit.edu  
Title: Coupled hidden Markov models for modeling interacting processes  
Author: Matthew Brand 
Address: 20 Ames Street Cambridge, MA 02139 USA  
Affiliation: The Media Lab, MIT http://www.media.mit.edu/~brand  
Abstract: c flMIT Media Lab Perceptual Computing / Learning and Common Sense Technical Report 405 3nov96, revised 3jun97 Abstract We present methods for coupling hidden Markov models (hmms) to model systems of multiple interacting processes. The resulting models have multiple state variables that are temporally coupled via matrices of conditional probabilities. We introduce a deterministic O(T (CN ) 2 ) approximation for maximum a posterior (MAP) state estimation which enables fast classification and parameter estimation via expectation maximization. An "N-heads" dynamic programming algorithm samples from the highest probability paths through a compact state trellis, minimizing an upper bound on the cross entropy with the full (combinatoric) dynamic programming problem. The complexity is O(T (CN ) 2 ) for C chains of N states apiece observing T data points, compared with O(T N 2C ) for naive (Cartesian product), exact (state clustering), and stochastic (Monte Carlo) methods applied to the same inference problem. In several experiments examining training time, model likelihoods, classification accuracy, and robustness to initial conditions, coupled hmms compared favorably with conventional hmms and with energy-based approaches to coupled inference chains. We demonstrate and compare these algorithms on synthetic and real data, including interpretation of video.
Abstract-found: 1
Intro-found: 1
Reference: [Baldi and Chauvin, 1994] <author> Baldi, P. and Chauvin, Y. </author> <year> (1994). </year> <title> Smooth on-line learning algorithms for hidden Markov models. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 305-316. </pages>
Reference-contexts: to the conventional hmm formul (equation (7)); the formula for coupling matrices between hmms is similar: P s 0 t =i;s t1 =jjO = t )fi i 0 ;t (19) P T P T (20) Alternately, the statistics obtained by the N-heads algorithm can be used by gradient descent methods <ref> [Baldi and Chauvin, 1994, Binder et al., 1997a] </ref> to yield an on-line training procedure. 6.4 Complexity of multiple couplings These procedures generalize directly to graphs containing multiple HMMs (figure 6): first associate sidekicks to antecedent paths, then associate antecedent paths to new heads.
Reference: [Baldi and Chauvin, 1996] <author> Baldi, P. and Chauvin, Y. </author> <year> (1996). </year> <title> Hybrid modeling, HMM/NN architectures, and protein applications. </title> <journal> Neural Computation, </journal> <volume> 8(7) </volume> <pages> 1541-1565. </pages>
Reference-contexts: This Cartesian product solution rapidly becomes untenable. Other variations on hmm structure aimed at multiple channel modeling include using neural networks for outputs <ref> [Baldi and Chauvin, 1996] </ref>, input-output models [Bengio and Frasconi, 1996], and per-state mixture models. However, these are all based on the traditional Markov formulation of a single process dynamic. Signals from systems of multiple processes are common and interesting modeling problems.
Reference: [Baum, 1972] <author> Baum, L. </author> <year> (1972). </year> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8. </pages>
Reference-contexts: Given these two values, we can estimate the probability of any transition given an observation (figure 2c): ^ P s t =i;s t1 =jjO = P (O) from which we can re-estimate the conditional transition probabilities: ^ P ijj = t=2 P T (7) This is Baum-Welch re-estimation <ref> [Baum, 1972] </ref>. Forward-backward analysis provides the expected state values given the observations and model parameters; re-estimation adjusts the parameters to maximize model likelihood given the state probabilities and the observation.
Reference: [Bengio and Frasconi, 1996] <author> Bengio, Y. and Frasconi, P. </author> <year> (1996). </year> <title> Input/output hmms for sequence processing. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(5) </volume> <pages> 1231-1249. </pages>
Reference-contexts: This Cartesian product solution rapidly becomes untenable. Other variations on hmm structure aimed at multiple channel modeling include using neural networks for outputs [Baldi and Chauvin, 1996], input-output models <ref> [Bengio and Frasconi, 1996] </ref>, and per-state mixture models. However, these are all based on the traditional Markov formulation of a single process dynamic. Signals from systems of multiple processes are common and interesting modeling problems.
Reference: [Binder et al., 1997a] <author> Binder, J., Koller, D., Russell, S., and Kanazawa, K. </author> <year> (1997a). </year> <title> Adaptive probabilistic networks with hidden variables. Machine Learning. in press. 26 Coupled Hidden Markov Models </title>
Reference-contexts: to the conventional hmm formul (equation (7)); the formula for coupling matrices between hmms is similar: P s 0 t =i;s t1 =jjO = t )fi i 0 ;t (19) P T P T (20) Alternately, the statistics obtained by the N-heads algorithm can be used by gradient descent methods <ref> [Baldi and Chauvin, 1994, Binder et al., 1997a] </ref> to yield an on-line training procedure. 6.4 Complexity of multiple couplings These procedures generalize directly to graphs containing multiple HMMs (figure 6): first associate sidekicks to antecedent paths, then associate antecedent paths to new heads.
Reference: [Binder et al., 1997b] <author> Binder, J., Murphy, K., and Russell, S. </author> <year> (1997b). </year> <title> Space-efficient inference in dynamic probabilistic networks. </title> <booktitle> In Proceedings, International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: These algorithms improve over random sampling by discarding, weighting, and/or varying sample state sequences according to their posteriors ([Henrion, 1988, Fung and Chang, 1989, Kanazawa et al., 1995], respectively). The space and time complexities are also typically exponential in the number of simultaneous state variables <ref> [Binder et al., 1997b] </ref>. While some of these algorithms are consistent (they converge asymptotically to the true distribution), it is not known whether they are efficient (they produce the best estimates given the order of computation).
Reference: [Brand, 1997a] <author> Brand, M. </author> <year> (1997a). </year> <title> Coupled hidden Markov models at multiple time scales. Vision and Modeling TR 428, </title> <publisher> MIT Media Lab. </publisher>
Reference-contexts: Models for this problem have been proposed by [Saul and Jordan, 1996, Hihi and Bengio, 1996]. Two inference graphs for 2-1 time scales are shown in figure (12c,d). Such architectures can be evaluated using O (T (DN ) 2 ) variants of the N-heads algorithm <ref> [Brand, 1997a] </ref>, where D is the number of differently conditioned hidden nodes in the inference graph. The procedure generalizes to multiple chains with doubling time scales (figure 12e). 10.
Reference: [Brand, 1997b] <author> Brand, M. </author> <year> (1997b). </year> <title> The "Inverse Hollywood Problem": From video to scripts and storyboards via causal analysis. </title> <booktitle> In Proceedings, Conference on Artificial Intelligence, AAAI. Also MIT Media Lab Vision and Modeling TR 410, </booktitle> <pages> 12/96. </pages>
Reference-contexts: Novel action sequences were processed and the map state sequence through the composite chmm was examined to obtain a sequence of recognized actions. Hit, miss, false alarms, precision, and recall statistics were compiled for four variants of this system systems, all shown in table 3. Further details are in <ref> [Brand, 1997b] </ref>. 9. Variations and extensions A number of interesting chmm variations are possible within the O (T (CN ) 2 ) complexity of the N-heads algorithm. We briefly mention two extensions motivated by problems in acoustical signal processing, relating to the inference graphs in figure 12. 3.
Reference: [Brand, 1997c] <author> Brand, M. </author> <year> (1997c). </year> <title> Source separation with coupled hidden Markov models. Vision and Modeling TR 427, </title> <publisher> MIT Media Lab. </publisher>
Reference-contexts: For example, the acoustical waveform of a string quartet. We have derived an extension to N-heads algorithm that performs source separation hand-in-hand with state estimation <ref> [Brand, 1997c] </ref>. Briefly, the posterior is expressed in terms of the choice of sidekicks, as before, and also as a choice of component signals, which must sum to the actual observed signal. Constrained maximization of this posterior yields a fixed-point equation for each component signal.
Reference: [Brand and Oliver, 1997] <author> Brand, M. and Oliver, N. </author> <year> (1997). </year> <title> Coupled hidden markov models for complex action recognition. </title> <booktitle> In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 994-999. </pages> <note> Also MIT Media Lab Vision and Modeling TR 407, 11/96. </note>
Reference-contexts: The results are plotted as fitted normal curves in figure 11, where the chmm is distinguished by its sharp, high-likelihood peak| contrasting the broad spread of low-likelihood hmm results. Further details can be found in <ref> [Brand and Oliver, 1997] </ref>. In a second video interpretation task, chmms were trained to recognize continuous sequences of dextrous manipulation|picking up, putting down, pushing, batting, tool 20 Coupled Hidden Markov Models use, etc.
Reference: [Cooper, 1990] <author> Cooper, G. F. </author> <year> (1990). </year> <title> The computational complexity of probablistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 393-405. </pages>
Reference-contexts: Full couplings of large numbers of hmms result in very densely connected graphs. Exact estimation in these graphs is NP-hard <ref> [Cooper, 1990] </ref>; for major classes of dense graphs approximation methods are NP-hard as well [Dagum and Luby, 1993]. The 13 Brand N-heads method exploits a regularity in the structure of the inference graph to avoid these problems.
Reference: [Dagum and Luby, 1993] <author> Dagum, P. and Luby, M. </author> <year> (1993). </year> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60 </volume> <pages> 141-153. </pages>
Reference-contexts: Full couplings of large numbers of hmms result in very densely connected graphs. Exact estimation in these graphs is NP-hard [Cooper, 1990]; for major classes of dense graphs approximation methods are NP-hard as well <ref> [Dagum and Luby, 1993] </ref>. The 13 Brand N-heads method exploits a regularity in the structure of the inference graph to avoid these problems. The approximation will of course weaken as more and more hmms are coupled, since an exponentially small fraction of the paths are being sampled.
Reference: [Dean and Kanazawa, 1989] <author> Dean, T. and Kanazawa, K. </author> <year> (1989). </year> <title> Probabilistic temporal reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150. </pages>
Reference-contexts: In recent years researchers have adopted Monte Carlo sampling methods for estimation problems in a class of chmm-like inference graphs called dynamic probabilistic networks <ref> [Dean and Kanazawa, 1989] </ref>. These algorithms improve over random sampling by discarding, weighting, and/or varying sample state sequences according to their posteriors ([Henrion, 1988, Fung and Chang, 1989, Kanazawa et al., 1995], respectively).
Reference: [Dempster et al., 1977] <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: Together, they form an expectation-maximization procedure that, when repeated until convergence, finds hmm parameters corresponding to a local maximum in model likelihood <ref> [Dempster et al., 1977] </ref>. The efficiency of this training procedure makes hmms very attractive for applications time-series modeling and classification. 3.
Reference: [Forney, 1973] <author> Forney, G. </author> <year> (1973). </year> <title> The Viterbi algorithm. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 6 </volume> <pages> 268-278. </pages>
Reference-contexts: The most likely full state sequence is then ^ SjO = argmax i ^ S i;T jO. This is the Viterbi algorithm <ref> [Forney, 1973] </ref>. If we replace "argmax" with " P " and consider the sum likelihood for all possible paths we obtain P [M jO], the probability of the model given an observation .
Reference: [Fung and Chang, 1989] <author> Fung, R. and Chang, K. C. </author> <year> (1989). </year> <title> Weighting and integrating evidence for stochastic simulation in Bayesian networks. </title> <booktitle> In Proceedings, Conference on Uncertainty in Artificial Intelligence, volume 5, </booktitle> <address> Ontario, Canada. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Ghahramani and Jordan, 1996] <author> Ghahramani, Z. and Jordan, M. I. </author> <year> (1996). </year> <title> Factorial hidden Markov models. </title> <editor> In [Touretzky et al., </editor> <year> 1996]. </year>
Reference-contexts: Tractable approximations are available using mean field theory from statistical physics [Williams and Hinton, 1990, Parisi, 1988]. Ghahramani and Jordan <ref> [Ghahramani and Jordan, 1996] </ref> have developed an elegant "structured mean field" approximation in which the inference graph is partitioned into subgraphs that can be tractably estimated via exact methods (e.g., forward-backward analysis of the independent hmms). <p> Mean field approximations of chmm inference graphs are O (T N 4 ); we followed the suggestion, "Rather than specifying these higher-order couplings through probability transition matrices, one can introduce second-order interaction terms in the energy function." <ref> [Ghahramani and Jordan, 1996] </ref> We independently compared the mean field algorithm with and without coupling terms in its energy function, and found that these terms reduce train-test divergence in cross-validation, increasing test data likelihoods but worsening training data likelihoods by a somewhat larger amount.
Reference: [Henrion, 1988] <author> Henrion, M. </author> <year> (1988). </year> <title> Propagation of uncertainty in Bayesian networks by probabilistic logic sampling. </title> <editor> In Lemmer, J. F. and Kanal, L. N., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, volume 2. </booktitle> <address> Elsevier/North-Holland, Amsterdam. </address>
Reference: [Hihi and Bengio, 1996] <author> Hihi, S. E. and Bengio, Y. </author> <year> (1996). </year> <title> Hierarchical recurrent networks for long-term dependencies. </title> <editor> In [Touretzky et al., </editor> <year> 1996]. </year> <note> 27 Brand </note>
Reference-contexts: Such is the case in speech-reading, where the voicebox modulates the acoustic signal faster than lip articulations modulate the visual signal. Models for this problem have been proposed by <ref> [Saul and Jordan, 1996, Hihi and Bengio, 1996] </ref>. Two inference graphs for 2-1 time scales are shown in figure (12c,d).
Reference: [Jensen et al., 1990] <author> Jensen, F. V., Lauritzen, S. L., and Olesen, K. G. </author> <year> (1990). </year> <title> Bayesian updating in recursive graphical models by local computations. </title> <journal> Computational Statistical Quarterly, </journal> <volume> 4 </volume> <pages> 269-282. </pages>
Reference-contexts: Inference in chmms has the same time complexity as the Cartesian product hmms, thus exact algorithms are not attractive. E.g., graphical analysis of the independence structure of a two-chain chmm reveals that exact maximum a posterior (map) inference is an O (T N 4 ) computation <ref> [Jensen et al., 1990, Kjaerulff, 1992, Smyth et al., 1997] </ref>. In recent years researchers have adopted Monte Carlo sampling methods for estimation problems in a class of chmm-like inference graphs called dynamic probabilistic networks [Dean and Kanazawa, 1989].
Reference: [Jordan et al., 1996] <author> Jordan, M. I., Ghahramani, Z., and Saul, L. K. </author> <year> (1996). </year> <title> Hidden Markov decision trees. </title> <editor> In [Touretzky et al., </editor> <year> 1996]. </year>
Reference-contexts: A limited class of graphs can be recursively decimated, obtaining correlations for any connected pair of nodes. This includes the two-chain ladder graph shown in the figure. processes on "slave" processes. Hidden Markov decision trees <ref> [Jordan et al., 1996] </ref> use structured mean field approximations to break the graph into hmm or decision tree subgraphs. This independence structure is suitable for representing hierarchical structure in a signal, for example the baseline of a song constrains the melody, and both constrain the harmony. <p> If we have prior knowledge that intrachain dynamics dominate interchain dynamics, we may also use the heuristic method of <ref> [Jordan et al., 1996] </ref>, in which we perform a conventional Viterbi analysis in one chain while holding state assignments constant in all others, and cycle through the chains until convergence to a local maximum. 6.3 Re-estimation After collecting statistics using the N-heads method, transition matrices within chains are re-estimated according to
Reference: [Kanazawa et al., 1995] <author> Kanazawa, K., Koller, D., and Russell, S. </author> <year> (1995). </year> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <booktitle> In Proceedings, Conference on Uncertainty in Artificial Intelligence, volume 11, </booktitle> <address> Montreal, Canada. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Kjaerulff, 1992] <author> Kjaerulff, U. </author> <year> (1992). </year> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <booktitle> In Proceedings, Conference on Uncertainty in Artificial Intelligence, </booktitle> <volume> volume 8, </volume> <pages> pages 121-129. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Inference in chmms has the same time complexity as the Cartesian product hmms, thus exact algorithms are not attractive. E.g., graphical analysis of the independence structure of a two-chain chmm reveals that exact maximum a posterior (map) inference is an O (T N 4 ) computation <ref> [Jensen et al., 1990, Kjaerulff, 1992, Smyth et al., 1997] </ref>. In recent years researchers have adopted Monte Carlo sampling methods for estimation problems in a class of chmm-like inference graphs called dynamic probabilistic networks [Dean and Kanazawa, 1989].
Reference: [Parisi, 1988] <author> Parisi, G. </author> <year> (1988). </year> <title> Statistical Field Theory. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA. </address>
Reference-contexts: Tractable approximations are available using mean field theory from statistical physics <ref> [Williams and Hinton, 1990, Parisi, 1988] </ref>. Ghahramani and Jordan [Ghahramani and Jordan, 1996] have developed an elegant "structured mean field" approximation in which the inference graph is partitioned into subgraphs that can be tractably estimated via exact methods (e.g., forward-backward analysis of the independent hmms).
Reference: [Rabiner, 1989] <author> Rabiner, L. R. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-286. </pages>
Reference-contexts: HMMs and Markov representation We begin with a short review of hidden Markov models. Readers familiar with the model may wish to skip to section 3 on page 5, but will find the key in figure 1 useful. An extended tutorial is available in <ref> [Rabiner, 1989] </ref>.
Reference: [Saul and Jordan, 1995] <author> Saul, L. K. and Jordan, M. I. </author> <year> (1995). </year> <title> Boltzmann chains and hidden Markov models. </title> <editor> In [Tesauro et al., </editor> <year> 1995]. </year>
Reference-contexts: Its independence structure is suitable for expressing symmetrical synchronous constraints such as the fact that it is rare to see tennis opponents playing net at the same time. <ref> [Saul and Jordan, 1995] </ref> present an O (T N 3 ) exact algorithm for training an equivalent two-chain Boltzmann machine based on decimation, a method from statistical mechanics in which the marginal distributions of singly or doubly connected nodes are integrated out.
Reference: [Saul and Jordan, 1996] <author> Saul, L. K. and Jordan, M. I. </author> <year> (1996). </year> <title> Exploiting tractable substructures in intractable networks. </title> <editor> In [Touretzky et al., </editor> <year> 1996]. </year>
Reference-contexts: Such is the case in speech-reading, where the voicebox modulates the acoustic signal faster than lip articulations modulate the visual signal. Models for this problem have been proposed by <ref> [Saul and Jordan, 1996, Hihi and Bengio, 1996] </ref>. Two inference graphs for 2-1 time scales are shown in figure (12c,d).
Reference: [Smyth et al., 1997] <author> Smyth, P., Heckerman, D., and Jordan, M. </author> <year> (1997). </year> <title> Probabilistic independence networks for hidden Markov probability models. </title> <journal> Neural Computation, </journal> <volume> 7(2) </volume> <pages> 227-269. </pages>
Reference-contexts: graph for 5 time slices of an hmm c. output variable g. path through state trellis of a 3-state hmm for 5 time slices d. joint probability h. coupling of 3 hmms Probabilistic inference graphs are a convenient way of representing the independence structure of a distribution over random variables <ref> [Smyth et al., 1997] </ref>, which we will use throughout this article. Graphically, hidden Markov models are depicted "rolled out" in time with the state variable taking on a new value in each time slice (figure 1f). <p> Inference in chmms has the same time complexity as the Cartesian product hmms, thus exact algorithms are not attractive. E.g., graphical analysis of the independence structure of a two-chain chmm reveals that exact maximum a posterior (map) inference is an O (T N 4 ) computation <ref> [Jensen et al., 1990, Kjaerulff, 1992, Smyth et al., 1997] </ref>. In recent years researchers have adopted Monte Carlo sampling methods for estimation problems in a class of chmm-like inference graphs called dynamic probabilistic networks [Dean and Kanazawa, 1989].
Reference: [Stork and Hennecke, 1996] <author> Stork, D. G. and Hennecke, M. E., </author> <title> editors (1996). Speachreading by humans and machines, </title> <booktitle> volume 150 of NATO ASI Series, F: Computer and Systems Sciences. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: In true couplings, the processes are dependent and interact by influencing each other's states (figure 3b). This is generally known as the sensor fusion problem: Multiple channels carry complementary information about different components of a system, e.g., acoustical signals from speech and visual features from lip-tracking <ref> [Stork and Hennecke, 1996] </ref>. (Source separation can also be done with dependent processes; at risk of splitting hairs, this is probably more appropriate for interesting cocktail parties.) 6 Coupled Hidden Markov Models a. independent b. dependent slices.
Reference: [Tesauro et al., 1995] <editor> Tesauro, G., Touretzky, D. S., and Leen, T., editors (1995). </editor> <booktitle> Advances in Neural Information Processing Systems, volume 7, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: [Touretzky et al., 1996] <author> Touretzky, D. S., Mozer, M. C., and Hasselmo, M., </author> <title> editors (1996). </title> <booktitle> Advances in Neural Information Processing Systems, volume 8, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: [Williams and Hinton, 1990] <author> Williams, C. and Hinton, G. E. </author> <year> (1990). </year> <title> Mean field networks that learn to discriminate temporally distorted strings. </title> <booktitle> In Proceedings, Connectionist models summer school, </booktitle> <pages> pages 18-22, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 28 </pages>
Reference-contexts: Tractable approximations are available using mean field theory from statistical physics <ref> [Williams and Hinton, 1990, Parisi, 1988] </ref>. Ghahramani and Jordan [Ghahramani and Jordan, 1996] have developed an elegant "structured mean field" approximation in which the inference graph is partitioned into subgraphs that can be tractably estimated via exact methods (e.g., forward-backward analysis of the independent hmms).
References-found: 32

