URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/reinforcement/papers/fastem.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/Web/People/awm/papers.html
Root-URL: http://www.cs.cmu.edu
Email: awm@cs.cmu.edu  
Phone: (412) 268-7599,  
Title: Very Fast EM-based Mixture Model Clustering using Multiresolution  
Author: kd-trees Andrew Moore 
Address: Pittsburgh, PA 15213,  
Affiliation: Robotics Institute and School of Computer Science Carnegie Mellon University  
Abstract: Clustering is important in many fields including manufacturing, biology, finance, and astronomy. Mixture models are a popular approach due to their statistical foundations, and EM is a very popular method for finding mixture models. EM, however, requires many accesses of the data, and thus has been dismissed as impractical (e.g. (Zhang, Ra-makrishnan, & Livny, 1996)) for data mining of enormous datasets. We present a new algorithm, based on the multiresolution kd-trees of (Moore, Schneider, & Deng, 1997), which dramatically reduces the cost of EM-based clustering, with savings rising linearly with the number of datapoints. Although presented here for maximum likelihood estimation of Gaussian mixture models, it is also applicable to non-Gaussian models (provided class densities are monotonic in Maha-lanobis distance), mixed categorical/numeric clusters, and Bayesian methods such as Autoclass (Cheeseman & Oldford, 1994).
Abstract-found: 1
Intro-found: 1
Reference: <author> Cheeseman, P., & Oldford, R. </author> <year> (1994). </year> <title> Selecting Models from Data: </title> <booktitle> Artificial Intelligence and Statistics IV. Lecture Notes in Statistics, </booktitle> <volume> vol. 89. </volume> <publisher> Springer Verlag. </publisher>
Reference: <author> Deng, K., & Moore, A. W. </author> <year> (1995). </year> <title> Multiresolution Instance-based Learning. </title> <booktitle> In Proceedings of IJCAI-95. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus each iteration of EM visits every datapoint-class pair, meaning NR evaluations of a M-dimensional Gaussian, and so needing O (M 2 NR) arithmetic operations per iteration. This paper aims to reduce that cost. An mrkd-tree (Multiresolution KD-tree), introduced in <ref> (Deng & Moore, 1995) </ref> and developed further in (Moore et al., 1997), is a binary tree in which each node is associated with a subset of the datapoints. The root node 2 owns all the datapoints.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: 1 Learning Mixture Models In a Gaussian mixture model (e.g. <ref> (Duda & Hart, 1973) </ref>), we assume that datapoints fx 1 : : : x R g have been generated independently by the following process.
Reference: <author> Ester, M., Kriegel, H. P., & Xu, X. </author> <year> (1995). </year> <title> A Database Interface for Clustering in Large Spatial Databases. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press. </publisher>
Reference: <author> Moore, A. W., Schneider, J., & Deng, K. </author> <year> (1997). </year> <title> Efficient Locally Weighted Polynomial Regression Predictions. </title> <editor> In D. Fisher (Ed.), </editor> <booktitle> Proceedings of the 1997 International Machine Learning Conference. </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Thus each iteration of EM visits every datapoint-class pair, meaning NR evaluations of a M-dimensional Gaussian, and so needing O (M 2 NR) arithmetic operations per iteration. This paper aims to reduce that cost. An mrkd-tree (Multiresolution KD-tree), introduced in (Deng & Moore, 1995) and developed further in <ref> (Moore et al., 1997) </ref>, is a binary tree in which each node is associated with a subset of the datapoints. The root node 2 owns all the datapoints. Each non-leaf-node has two children, defined by a splitting dimension Nd.splitdim and a splitting value Nd.splitval. <p> To do this, we will compute, for each j, the minimum and maximum w ij that any point inside the node could have. This procedure is more complex than in the case of locally weighted regression <ref> (Moore et al., 1997) </ref>. <p> In earlier work we provided evidence that this statement may not apply for locally weighted regression <ref> (Moore et al., 1997) </ref> or Bayesian network learning (Moore & Lee, 1998), and we hope this paper provides some evidence that it also needn't apply to clustering. 9
Reference: <author> Moore, A. W., & Lee, M. S. </author> <year> (1998). </year> <title> Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 8. </volume>
Reference-contexts: In earlier work we provided evidence that this statement may not apply for locally weighted regression (Moore et al., 1997) or Bayesian network learning <ref> (Moore & Lee, 1998) </ref>, and we hope this paper provides some evidence that it also needn't apply to clustering. 9
Reference: <author> Omohundro, S. M. </author> <year> (1987). </year> <title> Efficient Algorithms with Neural Network Be-haviour. </title> <journal> Journal of Complex Systems, </journal> <volume> 1 (2), </volume> <pages> 273-347. </pages>
Reference: <author> Omohundro, S. M. </author> <year> (1991). </year> <title> Bumptrees for Efficient Function, Constraint, and Classification Learning. </title> <editor> In Lippmann, R. P., Moody, J. E., & Touretzky, D. S. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Zhang, T., Ramakrishnan, R., & Livny, M. </author> <year> (1996). </year> <title> BIRCH: An Efficient Data Clustering Method for Very Large Databases. </title> <booktitle> In Proceedings of the Fifteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems : PODS 1996. Assn for Computing Machinery. </booktitle> <pages> 10 </pages>
References-found: 9

