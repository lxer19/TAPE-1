URL: http://www.cs.berkeley.edu/~culler/papers/sort.ps
Refering-URL: http://www.cs.berkeley.edu/~culler/papers/
Root-URL: 
Email: rmarting@CS.Berkeley.EDU  
Title: Fast Parallel Sorting under LogP: Experience with the CM-5  
Author: Andrea C. Dusseau, David E. Culler, Klaus Erik Schauser, Richard P. Martin fdusseau, culler, schauser, 
Affiliation: Computer Science Division University of California, Berkeley  
Abstract: In this paper, the LogP model is used to analyze four parallel sorting algorithms (bitonic, column, radix, and sample sort). LogP characterizes the performance of modern parallel machines with a small set of parameters: the communication latency (L), overhead (o), bandwidth (g), and the number of processors (P ). We develop implementations of these algorithms in Split-C, a parallel extension to C, and compare the performance predicted by LogP to actual performance on a CM-5 of 32 to 512 processors for a range of problem sizes and input sets. The sensitivity of the algorithms is evaluated by varying the distribution of key values and the rank ordering of the input. The LogP model is shown to be a valuable guide in the development of parallel algorithms and a good predictor of implementation performance. The model encourages the use of data layouts which minimize communication and balanced communication schedules which avoid contention. Using an empirical model of local processor performance, LogP predictions closely match observed execution times on uniformly distributed keys across a broad range of problem and machine sizes for all four algorithms. Communication performance is oblivious to the distribution of the keys values, whereas the local sort performance is not. The communication phases in radix and sample sort are sensitive to the ordering of keys, because certain layouts result in contention. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken, </author> <title> LogP: Towards a Realistic Model of Parallel Computation, </title> <booktitle> in Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: processors (P ) is comparable to the number of data elements (N ), and then an efficient simulation of the algorithm is provided for the case where P &lt; N . 1 In this paper, we study fast parallel sorting from the perspective of a new realistic parallel model, LogP <ref> [1] </ref>, which captures the key performance characteristics of modern large scale multiprocessors, such as the Thinking Machines CM-5. In particular, the model reflects the technological reality that these machines are essentially a collection of workstation-class nodes which communicate by point-to-point messages that travel through a dedicated, high performance network. <p> The predictions of the model are compared to measurements of the algorithms on the CM-5 for a variety of input sets. Finally, in Section 8, we compare the performance of the four algorithms. 2 LogP The LogP model <ref> [1] </ref> reflects the convergence of parallel machines towards systems formed by a collection of complete computers, each consisting of a powerful microprocessor, cache, and large DRAM memory, 2 connected by a communication network. <p> Each node consists of a 33 MHz Sparc RISC processor chip-set and a network interface. The nodes are interconnected in two identical disjoint incomplete fat trees, and a broadcast/scan/prefix control network. The implementations of the sorting algorithms do not use the vector accelerators. In previous experiments on the CM-5 <ref> [10, 1] </ref>, we determined that o 2:2s and, on an unloaded 2 Our random number generator produces numbers in the range 0 through 2 31 1. 6 network, L 6s. The communication word size, w, is equal to four (32-bit) processor words. <p> The standard approach to implementing bitonic sort is to simulate the individual steps in the butterfly. However, we derive a more efficient data placement that was inspired by the mapping used for large FFTs <ref> [1] </ref>. Our bitonic sort starts with a blocked layout.
Reference: [2] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick, </author> <title> Parallel Programming in Split-C, </title> <booktitle> in Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: Our implementation language, Split-C <ref> [2] </ref>, provides an attractive basis for this study, because it exposes the capabilities modeled by LogP through a rich set of assignment operators in a distributed global address space. <p> We then discuss the probability distribution of the input keys used in our measurements. Next, we characterize the CM-5 in terms of the LogP parameters. Finally, we discuss our model of the local computation, focusing on the local sort. 3.1 Split-C Our sorting algorithms are written in Split-C <ref> [2] </ref>, a parallel extension of the C programming language that can express the capabilities offered by the LogP model. The language follows a SPMD (single program multiple data) model. Processors are distinguished by the value of the special constant, MYPROC.
Reference: [3] <author> G. Blelloch, C. Leiserson, and B. Maggs, </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2, </title> <booktitle> in Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: Split-C, like C, provides a straightforward machine independent programming system, without attempting to hide the underlying performance characteristics of the machine. We were strongly influenced in this study by a previous comparison of sorting algorithms, which examined bitonic, radix, and sample sort implemented in microcode on the CM-2 <ref> [3] </ref>. We augment the comparison to include column sort, address a more general class of machines, formalized by LogP, and implement the algorithms in a language that can be ported to a variety of parallel machines. This paper is organized as follows. In Section 2, the LogP model is described. <p> In the next four sections, we examine four sorting algorithms: bitonic sort [4], column sort [5], radix sort <ref> [3, 6] </ref>, and sample sort [3]. The order in which we discuss the sorts is based on the increasing complexity of their communication phases. <p> In the next four sections, we examine four sorting algorithms: bitonic sort [4], column sort [5], radix sort [3, 6], and sample sort <ref> [3] </ref>. The order in which we discuss the sorts is based on the increasing complexity of their communication phases. <p> that for uniformly distributed keys. 11 Note that not all input sets which are initially sorted cause processors to store only to themselves, since the keys must be in sorted order for each of the b passes. 20 7 Sample Sort An interesting recent algorithm, called sample (or splitter) sort <ref> [3, 13] </ref>, pushes the pattern of alternating phases of local computation, destination setup, and key distribution to the extreme it performs only one of each. The key distribution phase in sample sort exhibits the most complicated structure of any in the four sorts: irregular, unbalanced all-to-all communication. <p> Assuming a large sample size, a large number of keys per processor, and a random input key distribution, the expansion factor can be bounded by a small constant with high probability <ref> [3] </ref>. In our analysis of the distribution of keys in radix sort, we ignored the destination contention that occurs when multiple processors send to the same destination processor. The distribution phase for sample sort is similar, so we continue to ignore the potential contention.
Reference: [4] <author> K. Batcher, </author> <title> Sorting Networks and their Applications, </title> <booktitle> in Proceedings of the AFIPS Spring Joint Computing Conference, </booktitle> <year> 1986. </year>
Reference-contexts: In the next four sections, we examine four sorting algorithms: bitonic sort <ref> [4] </ref>, column sort [5], radix sort [3, 6], and sample sort [3]. The order in which we discuss the sorts is based on the increasing complexity of their communication phases. <p> Table 1 shows the local computational rates used in all of the sorts. 4 Bitonic Sort In this section, we discuss a variant of Batcher's bitonic sort <ref> [4] </ref>. After describing the general algorithm, we present a data layout that reduces communication and enables optimizations for the local computation. We then describe how the LogP model guides us to an efficient implementation of the important communication operations: remaps between cyclic and blocked layouts.
Reference: [5] <author> T. Leighton, </author> <title> Tight Bounds on the Complexity of Parallel Sorting, </title> <journal> IEEE Transactions on Computers, </journal> <month> Apr. </month> <year> 1985. </year>
Reference-contexts: In the next four sections, we examine four sorting algorithms: bitonic sort [4], column sort <ref> [5] </ref>, radix sort [3, 6], and sample sort [3]. The order in which we discuss the sorts is based on the increasing complexity of their communication phases. <p> step. 8 Therefore, the 8 With low entropies, our implementation for finding the minimum element in the bitonic sequence is slower because more keys 12 input key distributions. overall time for bitonic sort is relatively stable with different input key entropies, varying by only 12%. 5 Column Sort Column sort <ref> [5] </ref>, like bitonic sort, alternates between local sort and key distribution phases, but only four phases of each are required. Two key distribution phases use an all-to-all communication pattern and two use a one-to-one pattern.
Reference: [6] <author> M. Zagha and G. Blelloch, </author> <title> Radix Sort for Vector Multiprocessors, </title> <booktitle> in Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: In the next four sections, we examine four sorting algorithms: bitonic sort [4], column sort [5], radix sort <ref> [3, 6] </ref>, and sample sort [3]. The order in which we discuss the sorts is based on the increasing complexity of their communication phases.
Reference: [7] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam, </author> <title> The Stanford Dash Multiprocessor, </title> <journal> IEEE Computer, </journal> <volume> vol. 25, </volume> <pages> pp. 63-79, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: After all messages have been sent, each processor receives messages at the sending rate. 1 On machines with hardware for shared memory access, the remote end may be serviced by an auxiliary processor that is part of the memory controller <ref> [7] </ref>. 4 Therefore, the total time for this operation is 2L + 2o + (n 1 L=g) max (g; 2o). Note that this assumes the processors alternate between sending and receiving after the first message arrives.
Reference: [8] <author> C. Shannon and W. Weaver, </author> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press: </publisher> <address> Urbana, </address> <year> 1949. </year>
Reference-contexts: The probability distribution of each input set is characterized by its Shannon entropy <ref> [8] </ref>, defined as i=1 where p i is the probability associated with key i. To generate input data sets with various entropies, we produce keys whose individual bits have between 0 and 1 bits of entropy.
Reference: [9] <author> K. Thearling and S. Smith, </author> <title> An Improved Supercomputer Sorting Benchmark, </title> <type> tech. rep., </type> <institution> Thinking Machines Corporation, </institution> <year> 1991. </year>
Reference-contexts: To generate input data sets with various entropies, we produce keys whose individual bits have between 0 and 1 bits of entropy. Multiple keys from a uniform distribution are combined into a single key having a non-uniform distribution, as suggested in <ref> [9] </ref>. For example, if the binary AND operator is applied to two independent keys generated from a uniform distribution, then each bit in the resulting key has a 0.75 chance of being a zero and a 0.25 chance of being a one. <p> In <ref> [9] </ref>, similar results are presented for the execution time of the phases in radix sort as the entropy of keys is varied: a decrease in the total execution time as the entropy decreases, with a marked decrease with an entropy of 0.
Reference: [10] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser, </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation, </title> <booktitle> in Proc. of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Each node consists of a 33 MHz Sparc RISC processor chip-set and a network interface. The nodes are interconnected in two identical disjoint incomplete fat trees, and a broadcast/scan/prefix control network. The implementations of the sorting algorithms do not use the vector accelerators. In previous experiments on the CM-5 <ref> [10, 1] </ref>, we determined that o 2:2s and, on an unloaded 2 Our random number generator produces numbers in the range 0 through 2 31 1. 6 network, L 6s. The communication word size, w, is equal to four (32-bit) processor words.
Reference: [11] <author> P. Liu, W. Aiello, and S. Bhatt, </author> <title> An atomic model for message-passing, </title> <booktitle> in Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1993. </year>
Reference-contexts: Determining the expected slowdown due to contention of a random permutation under LogP is an interesting open problem. Our simulations and recent theoretical results <ref> [11] </ref> suggest that the slow-down is bounded by a small constant, but a thorough treatment of this problem is beyond the scope of this paper.
Reference: [12] <author> R. Karp, A. Sahay, E. Santos, and K. E. Schauser, </author> <title> Optimal Broadcast and Summation in the LogP Model, </title> <booktitle> in 5th Symp. on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Modifications could be made to the algorithm such that keys destined for the same processor are first gathered and then stored in bulk. 10 In <ref> [12] </ref> an optimal broadcast strategy is developed where the root sends each data element only once, but alternates among recipients in order to retain the logarithmic depth of a tree broadcast. 17 theory each processor could refuse to receive the next value until it has forwarded the present one and the
Reference: [13] <author> J. H. Reif and L. G. Valiant, </author> <title> A Logarithmic time Sort for Linear Size Networks, </title> <journal> Journal of the ACM, </journal> <volume> vol. 34, </volume> <pages> pp. 60-76, </pages> <month> Jan. </month> <year> 1987. </year> <month> 27 </month>
Reference-contexts: that for uniformly distributed keys. 11 Note that not all input sets which are initially sorted cause processors to store only to themselves, since the keys must be in sorted order for each of the b passes. 20 7 Sample Sort An interesting recent algorithm, called sample (or splitter) sort <ref> [3, 13] </ref>, pushes the pattern of alternating phases of local computation, destination setup, and key distribution to the extreme it performs only one of each. The key distribution phase in sample sort exhibits the most complicated structure of any in the four sorts: irregular, unbalanced all-to-all communication.
References-found: 13

