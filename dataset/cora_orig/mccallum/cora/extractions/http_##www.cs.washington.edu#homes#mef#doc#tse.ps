URL: http://www.cs.washington.edu/homes/mef/doc/tse.ps
Refering-URL: http://www.cs.washington.edu/homes/mef/
Root-URL: 
Title: Region Analysis: A Parallel Elimination Method for Data Flow Analysis  
Author: Yong-fong Lee Barbara G. Ryder Marc E. Fiuczynski 
Keyword: Index Terms Data flow analysis, elimination algorithms, interval analysis, parallel algorithms, program optimization.  
Abstract: Parallel data flow analysis methods offer the promise of calculating detailed semantic information about a program at compile-time more efficiently than sequential techniques. Previous work on parallel elimination methods [1, 2] has been hampered by the lack of control over interval size; this can prohibit effective parallel execution of these methods. To overcome this problem, we have designed the region analysis method, a new elimination method for data flow analysis. Region analysis emphasizes flow graph partitioning to enable better load balancing in a more effective parallel algorithm. In this paper, we present the design of region analysis and the empirical results we have obtained that indicate (1) the prevalence of large intervals in flow graphs derived from real programs, and (2) the performance improvement of region analysis over parallel Allen-Cocke interval analysis. Our implementation analyzed programs from the Perfect Benchmarks [3] and netlib [4] running on a Sequent Symmetry S81. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Zobel, </author> <title> "Parallel interval analysis of data flow equations," </title> <booktitle> in Proceedings of the 1990 International Conference on Parallel Processing, Vol.II, </booktitle> <pages> pp. 9-16, </pages> <publisher> The Penn State University Press, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: A. Parallel Allen-Cocke Interval Analysis Interval analysis offers natural parallelism for us to exploit. In the elimination phase, we can concurrently summarize data flow information in independent intervals. Similar parallelism exists in the propagation phase. Both Zobel <ref> [1] </ref> and Gupta et al. [2] have parallelized Allen-Cocke interval analysis in this manner. 5 More precisely, an interval contains both nodes and edges among these nodes. 5 6 To formally capture the parallelism, we define an interval containment tree or ic-tree [1]. Let G be the original flow graph. <p> Both Zobel <ref> [1] </ref> and Gupta et al. [2] have parallelized Allen-Cocke interval analysis in this manner. 5 More precisely, an interval contains both nodes and edges among these nodes. 5 6 To formally capture the parallelism, we define an interval containment tree or ic-tree [1]. Let G be the original flow graph. Then the ic-tree of G is specified by T ic = (N ic ; E ic ), where N ic contains all the nodes appearing in any derived graphs.
Reference: [2] <author> R. Gupta, L. Pollock, and M. L. Soffa, </author> <title> "Parallelizing data flow analysis," </title> <booktitle> in Proceedings of the Workshop on Parallel Compilation, </booktitle> <address> (Kingston, Ontario, Canada), </address> <month> May </month> <year> 1990. </year>
Reference-contexts: A. Parallel Allen-Cocke Interval Analysis Interval analysis offers natural parallelism for us to exploit. In the elimination phase, we can concurrently summarize data flow information in independent intervals. Similar parallelism exists in the propagation phase. Both Zobel [1] and Gupta et al. <ref> [2] </ref> have parallelized Allen-Cocke interval analysis in this manner. 5 More precisely, an interval contains both nodes and edges among these nodes. 5 6 To formally capture the parallelism, we define an interval containment tree or ic-tree [1]. Let G be the original flow graph.
Reference: [3] <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck, </author> <title> "Supercomputer performance evaluation and the perfect benchmarks," </title> <booktitle> in Proceedings of 1990 International Conference on Supercomputing, </booktitle> <pages> pp. 254-266, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The relatively formidable size of I 26 can inhibit the effectiveness of parallel interval analysis. Normally, we hope all the intervals are of comparable size. 6 III. Interval Structure of Control Flow Graphs We investigated programs from the Perfect Benchmarks <ref> [3] </ref> and five netlib libraries [4]. 7 Table 1 reports the size statistics for 854 reducible control flow graphs; we divide these graphs into four groups by number of nodes and use these groups to report our results.
Reference: [4] <author> J. J. Dongarra and E. Grosse, </author> <title> "Distribution of mathematical software via electronic mail," </title> <journal> Communications of the ACM, </journal> <volume> vol. 30, </volume> <pages> pp. 403-407, </pages> <year> 1987. </year>
Reference-contexts: The relatively formidable size of I 26 can inhibit the effectiveness of parallel interval analysis. Normally, we hope all the intervals are of comparable size. 6 III. Interval Structure of Control Flow Graphs We investigated programs from the Perfect Benchmarks [3] and five netlib libraries <ref> [4] </ref>. 7 Table 1 reports the size statistics for 854 reducible control flow graphs; we divide these graphs into four groups by number of nodes and use these groups to report our results.
Reference: [5] <author> M. S. Hecht, </author> <title> Flow Analysis of Computer Programs. </title> <address> Amsterdam, Netherlands: </address> <publisher> Elsevier North-Holland, </publisher> <year> 1977. </year> <month> 32 </month>
Reference-contexts: A flow graph is either reducible or irreducible depending on the simplicity of its loop structure <ref> [19, 5] </ref>. Intuitively, an irreducible flow graph contains a cycle with multiple entry nodes; by contrast, any loop in a reducible flow graph has only one entry node. <p> In interprocedural analysis, a program is abstracted as a call multigraph, in which each node represents a procedure and an edge, a possible procedure call <ref> [5] </ref>. <p> In a reducible flow graph, edge (u; v) is a back edge if v dominates u; we call node v a back-targeted node. The set of back-targeted nodes in a reducible flow graph is unique <ref> [5, 27] </ref>. In our algorithm descriptions we will refer to parent/child nodes in a dominator tree and predecessor/successor nodes in a flow graph. There are several classical bit vector problems solved by data flow analysis to enable machine independent program optimizations [19]. <p> Data flow analysis algorithms calculate the possible propagation of information through paths in a flow graph. This analysis is performed intraprocedurally and/or interprocedurally. For the purposes of compile-time analysis, the standard assumption is that all such paths are executable <ref> [5] </ref>. <p> data flow algorithms: elimination methods and fixed-point iteration methods. 4 The former can be compared to a Gaussian elimination-like solution procedure of the data flow equations defined on the flow graph [29], whereas the latter involves finding a fixed point of this system of equations from a reasonable initial value <ref> [5] </ref>. Considering a forward data flow problem (i.e, one in which the data flow information is flowing along the direction of execution), we can intuitively describe elimination methods as having two phases: elimination and propagation. <p> Therefore, access to every region internal node in N 1 fhg must be through the region head node h <ref> [5] </ref>. We can define the region partitioning problem for a reducible flow graph as follows: The Region Partitioning Problem.
Reference: [6] <author> M. J. Harrold and M. L. Soffa, </author> <title> "Efficient computation of interprocedural definition-use chains," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 16, </volume> <pages> pp. 175-204, </pages> <month> March </month> <year> 1994. </year>
Reference: [7] <author> T. Ostrand and E. Weyuker, </author> <title> "Data flow based test adequecy analysis for languages with pointers," </title> <booktitle> in Proceedings of the 1991 Symposium on Software Testing, Analysis and Verification (TAV4), </booktitle> <month> October </month> <year> 1991. </year>
Reference: [8] <author> M. Weiser, </author> <title> "Program slicing," </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. SE-10, </volume> <pages> pp. 352-357, </pages> <month> July </month> <year> 1984. </year>
Reference: [9] <author> K. J. Ottenstein and L. M. Ottenstein, </author> <title> "The program dependence graph in a software development environment," </title> <booktitle> in Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pp. 177-184, </pages> <month> May </month> <year> 1984. </year>
Reference: [10] <author> S. Horwitz, T. Reps, and D. Binkley, </author> <title> "Interprocedural slicing using dependence graphs," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 12, </volume> <pages> pp. 26-60, </pages> <month> January </month> <year> 1990. </year>
Reference: [11] <author> G. A. Venkatesh, </author> <title> "The semantic approach to program slicing," </title> <booktitle> in Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 107-119, </pages> <month> June </month> <year> 1991. </year>
Reference: [12] <author> B. G. Ryder, </author> <title> "ISMM: Incremental software maintenance manager," </title> <booktitle> in Proceedings of the IEEE Computer Society Conference on Software Maintenance, </booktitle> <pages> pp. 142-164, </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1989. </year> <title> Miami, </title> <address> Florida. </address>
Reference: [13] <author> S. Horwitz, J. Prins, and T. Reps, </author> <title> "Integrating non-interfering versions of programs," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 11, </volume> <pages> pp. 345-387, </pages> <month> July </month> <year> 1989. </year>
Reference: [14] <author> D. Padua and M. Wolfe, </author> <title> "Advanced compiler optimizations for supercomputers," </title> <journal> Communications of the ACM, </journal> <volume> vol. 29, </volume> <pages> pp. 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference: [15] <author> F. E. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante, </author> <title> "An Overview of the PTRAN Analysis System for Multiprocessing," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 5, </volume> <pages> pp. 617-640, </pages> <year> 1988. </year>
Reference: [16] <author> D. Callahan, K. Cooper, K. Kennedy, L. Torczon, and R. Hood, </author> <title> "Parascope: A parallel programming environment," </title> <journal> International Journal of Supercomputing, </journal> <volume> vol. 2, </volume> <month> December </month> <year> 1988. </year>
Reference: [17] <author> J. M. Anderson and M. S. Lam, </author> <title> "Global optimizations for parallelism and locality on scalable parallel machines," </title> <booktitle> in Proceedings of the SIGPLAN '93 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pp. 112-125, </pages> <publisher> ACM Press, </publisher> <address> June 1993. Albuquerque, New Mexico. </address>
Reference: [18] <author> H. Zima and B. Chapman, </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference: [19] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Moreover, the worst case time complexity of useful data flow techniques are not observed in practice. 2 The available expressions problem involves information necessary for common subexpression elimination <ref> [19] </ref>. 2 This is the second parallel data flow analysis technique we have investigated. Our first experi-ments were with a family of parallel hybrid algorithms, which combine fixed point iteration within regions and an elimination-like propagation between regions. <p> A flow graph is either reducible or irreducible depending on the simplicity of its loop structure <ref> [19, 5] </ref>. Intuitively, an irreducible flow graph contains a cycle with multiple entry nodes; by contrast, any loop in a reducible flow graph has only one entry node. <p> For intraprocedural analysis, a control flow graph node represents a single-entry, single-exit block of code, and an edge, a possible control transfer from one block to another <ref> [19] </ref>. In interprocedural analysis, a program is abstracted as a call multigraph, in which each node represents a procedure and an edge, a possible procedure call [5]. <p> In our algorithm descriptions we will refer to parent/child nodes in a dominator tree and predecessor/successor nodes in a flow graph. There are several classical bit vector problems solved by data flow analysis to enable machine independent program optimizations <ref> [19] </ref>. We have used one of these, the reaching definitions problem, in testing our approach. The solution to reaching definitions collects for each variable at each program point, those definitions which might have set the value of that variable when execution reaches this program point.
Reference: [20] <author> T. Reps, </author> <title> "On the sequential nature of interprocedural program-analysis problems," </title> <note> 1995. To appear in Acta Informatica. </note>
Reference-contexts: 1 In <ref> [20] </ref>, Reps proves hardness results for some interprocedural data flow problems, assuming the "meet-overall-valid-paths" precision, and uses them to assert the non-existence of fast (NC-class) parallel algorithms for these problems.
Reference: [21] <author> R. Kramer, R. Gupta, and M. L. Soffa, </author> <title> "The combining DAG: A technique for parallel data flow analysis," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 805-813, </pages> <month> August </month> <year> 1994. </year>
Reference: [22] <author> F. E. Allen and J. Cocke, </author> <title> "A program data flow analysis procedure," </title> <journal> Communications of the ACM, </journal> <volume> vol. 19, no. 3, </volume> <pages> pp. 137-147, </pages> <year> 1976. </year>
Reference-contexts: Elimination algorithms partition the flow graph nodes (and their corresponding data flow equations) into single-entry regions, according to some graph 4 Consult [28] for the use of elimination methods in interprocedural analysis. 4 decomposition (Allen-Cocke intervals were the original partitioning <ref> [22] </ref>). During elimination, the algorithm summarizes the data flow within an interval in terms of the data flow solution at the entry node; this calculation gathers those parts of the solution generated locally. <p> The forward algorithm is based on the Allen-Cocke interval-finding algorithm <ref> [22] </ref>. The bottom-up algorithm uses the dominator tree of a flow graph and topological order on flow graph nodes. Both algorithms are greedy; their common heuristic is to make a region as large as possible while not violating the size constraint.
Reference: [23] <author> Y. Lee, T. J. Marlowe, and B. G. Ryder, </author> <title> "Experiences with a parallel algorithm for data flow analysis," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 5, </volume> <pages> pp. 163-188, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Using the parallel hybrid algorithm on the reaching definitions problem, we already have demonstrated the potential of using a parallel analysis technique on a distributed memory machine, the Intel iPSC/2 <ref> [23, 24] </ref>. The regions used in hybrid algorithms are single-entry clusters of one or more strongly connected components. While the data flow solution procedure of hybrid algorithms combines both iteration and elimination, region analysis is essentially an elimination method. Paper Overview.
Reference: [24] <author> Y. Lee and B. G. Ryder, </author> <title> "Effectively exploiting parallelism in data flow analysis," </title> <journal> The Journal of Supercomputing, </journal> <volume> vol. 8, </volume> <pages> pp. 233-262, </pages> <year> 1994. </year>
Reference-contexts: Using the parallel hybrid algorithm on the reaching definitions problem, we already have demonstrated the potential of using a parallel analysis technique on a distributed memory machine, the Intel iPSC/2 <ref> [23, 24] </ref>. The regions used in hybrid algorithms are single-entry clusters of one or more strongly connected components. While the data flow solution procedure of hybrid algorithms combines both iteration and elimination, region analysis is essentially an elimination method. Paper Overview. <p> This clustering reduces the degree of parallelism in parallel hybrid algorithms, but can improve their performance by reducing scheduling and communication overhead <ref> [24] </ref>. In solving the region partitioning problem, we seek to minimize the number of regions whose sizes are constrained by a certain size limit. This objective requires us to create as large a region as possible while not violating the size constraint. <p> problem, this elimination cost is even higher for a cyclic interval or region. 15 No attempt was made to automatically select the region size limit in this study. 28 29 In practice, by solving several data flow problems on the same derived graphs, the parallel execu-tion overhead can be amortized <ref> [24] </ref>. Furthermore, other data flow problems are more computation-intensive than the reaching definitions problem that we used in these experiments. Thus, we can expect the speedups obtained in solving these problems to be more significant. Region analysis is not guaranteed to outmatch parallel interval analysis in all the test cases.
Reference: [25] <author> V. Sgro and B. G. Ryder, </author> <title> "Differences in algorithmic parallelism in control flow and call multigraphs," </title> <booktitle> in Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 217-233, </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1994. </year> <note> Lecture Notes in Computer Science #892. </note>
Reference-contexts: In this paper, we use the term flow graph to mean either control flow graph or call multigraph, although the experiments reported here were conducted with only control flow graphs. 3 3 In <ref> [25] </ref> Sgro and Ryder studied the differences in algorithmic parallelism in control flow graphs and call multigraphs. 3 Let G = (N; E; ) be a flow graph.
Reference: [26] <author> T. Lengauer and R. E. Tarjan, </author> <title> "A fast algorithm for finding dominators in a flowgraph," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 1, </volume> <pages> pp. 121-141, </pages> <month> July </month> <year> 1979. </year> <month> 34 </month>
Reference-contexts: A node u dominates another node v 6= u in G, if every path from to v in the flow graph contains u. Node u is the immediate dominator of v, denoted u = idom (v), if u dominates v and every other dominator of v dominates u <ref> [26] </ref>. Every node v 6= in G has a unique immediate dominator, so we can construct the dominator tree for G; T = (N; f (idom (v ); v) j v 2 N fgg; ); that is, idom (v) is par (v), the parent of v in the dominator tree. <p> enable us to design a new elimination method called region analysis, which differs from Allen-Cocke interval analysis mainly in its partitioning of the flow 9 However, the fixed-point iteration is trivial for fast data flow problems, such as reaching definitions. 10 However, the O (e log n) algorithm described in <ref> [26] </ref> is widely used in implementation. 17 The bottom-up algorithm for region partitioning Input: flow graph G=(N,E,) and size limit S. Output: a set of head nodes defining regions that partition G.
Reference: [27] <author> R. E. Tarjan, </author> <title> "Testing flow graph reducibility," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> vol. 9, </volume> <pages> pp. 355-365, </pages> <year> 1974. </year>
Reference-contexts: In a reducible flow graph, edge (u; v) is a back edge if v dominates u; we call node v a back-targeted node. The set of back-targeted nodes in a reducible flow graph is unique <ref> [5, 27] </ref>. In our algorithm descriptions we will refer to parent/child nodes in a dominator tree and predecessor/successor nodes in a flow graph. There are several classical bit vector problems solved by data flow analysis to enable machine independent program optimizations [19].
Reference: [28] <author> M. Burke, </author> <title> "An interval-based approach to exhaustive and incremental interprocedural data-flow analysis," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 12, </volume> <pages> pp. 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Elimination algorithms partition the flow graph nodes (and their corresponding data flow equations) into single-entry regions, according to some graph 4 Consult <ref> [28] </ref> for the use of elimination methods in interprocedural analysis. 4 decomposition (Allen-Cocke intervals were the original partitioning [22]). <p> This second approach changes the region analysis elimination phase to include a fixed-point iteration within each region containing a back 16 edge, rather than a one-pass forward substitution <ref> [28] </ref>. 9 The (second approach of the) bottom-up algorithm for region partitioning of flow graphs is shown in Figure 6. Two supporting functions compress and mark-region are given in Figure 7. Note that the stacks child-stack (v) keep children in topological order from stack top to bottom.
Reference: [29] <author> B. G. Ryder and M. C. Paull, </author> <title> "Elimination algorithms for data flow analysis," </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 18, </volume> <pages> pp. 277-316, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: There are two main families of general-purpose data flow algorithms: elimination methods and fixed-point iteration methods. 4 The former can be compared to a Gaussian elimination-like solution procedure of the data flow equations defined on the flow graph <ref> [29] </ref>, whereas the latter involves finding a fixed point of this system of equations from a reasonable initial value [5].
Reference: [30] <author> Y. Lee, </author> <title> Performing Data Flow Analysis in Parallel. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rutgers University, </institution> <month> May </month> <year> 1992. </year> <note> Also Technical Report LCSR-TR-178. </note>
Reference-contexts: Then the number of intervals in G i is exactly n (G i+1 ), 1 i &lt; d 1. For each control flow graph, we use the following attributes to study its interval structure: * DG: number of derived graphs. 6 Our previous studies of Fortran call multigraphs <ref> [30] </ref> from the Perfect Benchmarks showed them to be acyclic and, therefore, consisting of only one large interval. We also observed that C call multigraphs have similarly large acyclic regions even though they normally contains cycles [30]. 7 Among these netlib libraries, eispack, fftpack, fishpack and linpack are collections of library <p> * DG: number of derived graphs. 6 Our previous studies of Fortran call multigraphs <ref> [30] </ref> from the Perfect Benchmarks showed them to be acyclic and, therefore, consisting of only one large interval. We also observed that C call multigraphs have similarly large acyclic regions even though they normally contains cycles [30]. 7 Among these netlib libraries, eispack, fftpack, fishpack and linpack are collections of library routines, whereas paranoia is a program. 8 * LIS: the largest interval size found in all the derived graphs. * FINT: number of intervals in the first derived graph; that is, n (G 2 ). * <p> The size limit is intended to refine large intervals. When the flow graph is a tree, we can solve the problem efficiently in low-order polynomial (arguably linear) time <ref> [30] </ref>; however, in general, this problem is provably N P-hard [30] by reducing into it the PARTITION problem in [31]. 12 In the following, we present two approximation algorithms for region partitioning. The forward algorithm is based on the Allen-Cocke interval-finding algorithm [22]. <p> The size limit is intended to refine large intervals. When the flow graph is a tree, we can solve the problem efficiently in low-order polynomial (arguably linear) time <ref> [30] </ref>; however, in general, this problem is provably N P-hard [30] by reducing into it the PARTITION problem in [31]. 12 In the following, we present two approximation algorithms for region partitioning. The forward algorithm is based on the Allen-Cocke interval-finding algorithm [22].
Reference: [31] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <address> New York: </address> <publisher> W.H. Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: The size limit is intended to refine large intervals. When the flow graph is a tree, we can solve the problem efficiently in low-order polynomial (arguably linear) time [30]; however, in general, this problem is provably N P-hard [30] by reducing into it the PARTITION problem in <ref> [31] </ref>. 12 In the following, we present two approximation algorithms for region partitioning. The forward algorithm is based on the Allen-Cocke interval-finding algorithm [22]. The bottom-up algorithm uses the dominator tree of a flow graph and topological order on flow graph nodes.
Reference: [32] <author> R. E. Tarjan, </author> <title> "Fast algorithms for solving path problems," </title> <journal> Journal of the ACM, </journal> <volume> vol. 28, </volume> <pages> pp. 594-614, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: The parent of v, par (v), is the immediate dominator of v, idom (v). Thus, we can visit the dominator tree nodes in a bottom-up fashion by visiting them in reverse topological order. Any immediate predecessor of v is either its parent or dominated by its parent <ref> [32] </ref>. Therefore, we have all the necessary information regarding the entry constraint to decide the merger of R v into R par (v) if we visit dominator tree nodes in a bottom-up manner and visit sibling nodes in topological order.
Reference: [33] <author> D. Harel, </author> <title> "A linear time algorithm for finding dominators in flow graphs and related problems," </title> <booktitle> in Proceedings of Seventeen Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 185-194, </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1985. </year> <month> 35 </month>
Reference-contexts: The compress (v) function makes a linked list to keep all the internal nodes in region R v . We can analyze the worst case time complexity of the bottom-up algorithm as follows. The construction of the dominator tree takes O (e) time using an algorithm described in <ref> [33] </ref>. 10 The calculation of topological order takes O (n + e) time. The bottom-up phase takes O (n) time to visit each node once. The compress (v) function facilitates the construction of regions in O (n) time once we have obtained the set of head nodes.
References-found: 33

