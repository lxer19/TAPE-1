URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.tr528.Scalability_of_atomic_primitives.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Email: fmichael,scottg@cs.rochester.edu  
Title: Scalability of Atomic Primitives on Distributed Shared Memory Multiprocessors  
Author: Maged M. Michael Michael L. Scott 
Keyword: synchronization, scalability, fetch-and-, compare-and-swap, load-linked, store-conditional, cache coherence  
Date: July 1994  
Address: Rochester, NY 14627-0226  
Affiliation: Computer Science Department University of Rochester  
Abstract: Many hardware primitives have been proposed for synchronization and atomic memory update on shared-memory multiprocessors. In this paper, we focus on general-purpose primitives that have proven popular on small-scale bus-based machines, but have yet to become widely available on large-scale, distributed-memory machines. Specifically, we propose several alternative implementations of fetch and , compare and - swap, and load linked/store conditional. We then analyze the performance of these implementations for various data sharing patterns, in both real and synthetic applications. Our results indicate that good overall performance can be obtained by implementing compare and swap in a multiprocessor's cache controllers, and by providing an additional instruction to load an exclusive copy of a line. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Examples include test and set with special semantics on the DASH multiprocessor [17], the QOLB primitives on the Wisconsin Multicube [6] and the IEEE Scalable Coherent Interface standard [24], the full/empty bits on the Alewife <ref> [1] </ref> and Tera machines [3], and the primitives for locking and unlocking cache lines on the KSR1 [15]. While it is possible to implement arbitrary synchronization mechanisms on top of special-purpose locks, greater concurrency, efficiency, and fault-tolerance may be achieved by using more general-purpose primitives.
Reference: [2] <institution> Alpha Architecture Handbook. Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: The pair load linked/store conditional, proposed by Jensen et al. [13], are implemented on the MIPS II [14] and the DEC Alpha <ref> [2] </ref> architectures. They must be used together to read, modify, and write a shared location. Load linked returns the value stored at the shared location and sets a reservation associated with the location and the processor. Store conditional checks the reservation.
Reference: [3] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June 11-15, </month> <year> 1990. </year>
Reference-contexts: Examples include test and set with special semantics on the DASH multiprocessor [17], the QOLB primitives on the Wisconsin Multicube [6] and the IEEE Scalable Coherent Interface standard [24], the full/empty bits on the Alewife [1] and Tera machines <ref> [3] </ref>, and the primitives for locking and unlocking cache lines on the KSR1 [15]. While it is possible to implement arbitrary synchronization mechanisms on top of special-purpose locks, greater concurrency, efficiency, and fault-tolerance may be achieved by using more general-purpose primitives.
Reference: [4] <author> R. P. Case and A. Padegs. </author> <title> Architecture of the IBM System 370. </title> <journal> Comm. of the ACM, </journal> <volume> 21(1) </volume> <pages> 73-96, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: Examples of fetch and primitives include test and set, fetch and store, fetch and add, and fetch and or. The compare and swap primitive was first provided on the IBM System/370 <ref> [4] </ref>. Compare - and swap takes three parameters: the address of the destination operand, an expected value, and a new value.
Reference: [5] <author> S. J. Eggers and R. H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: which load linked/store - conditional simulates compare and swap. 8 NOC EXC UPD LocusRoute 1.83 1.79 1.70 Cholesky 1.62 1.68 1.59 Table 1: Average write-run length in LocusRoute and Cholesky. 4.2 Sharing Patterns Performance of atomic primitives is affected by two main sharing pattern parameters: contention and average write-run length <ref> [5] </ref>. In this context, we define the level of contention as the number of processors that concurrently try to access an atomically accessed shared location.
Reference: [6] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Several atomic primitives have been proposed and implemented on DSM architectures. Most of them are special-purpose primitives that are designed to support some particular style of synchronization operations. Examples include test and set with special semantics on the DASH multiprocessor [17], the QOLB primitives on the Wisconsin Multicube <ref> [6] </ref> and the IEEE Scalable Coherent Interface standard [24], the full/empty bits on the Alewife [1] and Tera machines [3], and the primitives for locking and unlocking cache lines on the KSR1 [15].
Reference: [7] <author> A. Gottlieb and C. P. Kruskal. </author> <title> Coordinating Parallel Processors: A Parallel Unification. </title> <journal> Computer Architecture News, </journal> <volume> 9(6) </volume> <pages> 16-24, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: In section 3 we present several implementation options for these primitives on DSM multiprocessors. Then we present our experimental results and discuss their implications in section 4, and conclude with recommendations in section 5. 2 Atomic Primitives 2.1 Functionality A fetch and primitive <ref> [7] </ref> takes (conceptually) two parameters: the address of the destination operand, and a value parameter. It atomically reads the original value of the destination operand, computes the new value as a function of the original value and the value parameter, stores this new value, and returns the original value.
Reference: [8] <author> A. Gottlieb, B. D. Lubachevsky, and L. Rudolph. </author> <title> Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: Although we do not recommend it as the sole atomic primitive, we find fetch and add to be useful with lock-free counters (and with many other objects <ref> [8] </ref>). We recommend implementing it in uncached memory as an extra atomic primitive. 15 Acknowledgment We thank Jack Veenstra for his invaluable effort in developping and supporting MINT and for his comments on this paper, and Leonidas Kontothanassis for his important suggestions.
Reference: [9] <author> M. P. Herlihy. </author> <title> Impossibility and Universality Results for Wait-Free Synchronization. </title> <booktitle> In Proceedings of the 7th Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 276-290, </pages> <address> Toronto, Canada, </address> <month> August 15-17, </month> <year> 1988. </year>
Reference-contexts: Depending on the processor, these things may include loads, stores, and incorrectly-predicted branches. 2.2 Expressive Power Herlihy introduced an impossibility and universality hierarchy <ref> [9] </ref> that ranks atomic operations according to their relative power. The hierarchy is based on the concepts of lock-freedom and wait-freedom.
Reference: [10] <author> M. P. Herlihy. </author> <title> A Methodology for Implementing Highly Concurrent Data Objects. </title> <booktitle> In Proceedings of the Second Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 197-206, </pages> <address> Seattle, WA, March 14-16, </address> <year> 1990. </year> <month> 16 </month>
Reference-contexts: Herlihy presented methodologies for implementing lock-free (and wait-free) implementations of concurrent data objects using compare and swap <ref> [10] </ref> and load linked/store conditional [12]. The compare and swap algorithms are less efficient and conceptually more complex than the load linked/store - conditional algorithms due to the pointer problem [12].
Reference: [11] <author> M. P. Herlihy. </author> <title> Wait-Free Synchronization. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(1) </volume> <pages> 124-149, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Atomic reads, loads, and stores are at level 1. The primitives fetch and store, fetch and add, and test and set are at level 2. Compare and swap is a universal primitive|it is at level 1 of the hierarchy <ref> [11] </ref>. Load linked/store conditional can also be shown to be universal if we assume that reservations are invalidated if and only if the corresponding shared location is written.
Reference: [12] <author> M. P. Herlihy. </author> <title> A Methodology for Implementing Highly Concurrent Data Objects. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 15(5) </volume> <pages> 745-770, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Herlihy presented methodologies for implementing lock-free (and wait-free) implementations of concurrent data objects using compare and swap [10] and load linked/store conditional <ref> [12] </ref>. The compare and swap algorithms are less efficient and conceptually more complex than the load linked/store - conditional algorithms due to the pointer problem [12]. On the other hand, there are several algorithms that need or benefit from compare and - swap [18, 19, 20, 27]. <p> Herlihy presented methodologies for implementing lock-free (and wait-free) implementations of concurrent data objects using compare and swap [10] and load linked/store conditional <ref> [12] </ref>. The compare and swap algorithms are less efficient and conceptually more complex than the load linked/store - conditional algorithms due to the pointer problem [12]. On the other hand, there are several algorithms that need or benefit from compare and - swap [18, 19, 20, 27]. A simulation of compare and swap using load linked and store - conditional is less efficient than providing compare and swap in hardware.
Reference: [13] <author> E. H. Jensen, G. W. Hagensen, and J. M. Broughton. </author> <title> A New Approch to Exclusive Data Access in Shared Memory Multiprocessors. </title> <type> Technical Report UCRL-97663, </type> <institution> Lawrence Livermore National Lab, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: If the original value of the destination operand is equal to the expected 2 value, the former is replaced by the latter (atomically) and the return value indicates suc-cess, otherwise the return value indicates failure. The pair load linked/store conditional, proposed by Jensen et al. <ref> [13] </ref>, are implemented on the MIPS II [14] and the DEC Alpha [2] architectures. They must be used together to read, modify, and write a shared location. Load linked returns the value stored at the shared location and sets a reservation associated with the location and the processor.
Reference: [14] <author> G. Kane. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1989. </year>
Reference-contexts: The pair load linked/store conditional, proposed by Jensen et al. [13], are implemented on the MIPS II <ref> [14] </ref> and the DEC Alpha [2] architectures. They must be used together to read, modify, and write a shared location. Load linked returns the value stored at the shared location and sets a reservation associated with the location and the processor. Store conditional checks the reservation.
Reference: [15] <institution> KSR1 Principles of Operation. Kendall Square Research Corporation, </institution> <year> 1991. </year>
Reference-contexts: test and set with special semantics on the DASH multiprocessor [17], the QOLB primitives on the Wisconsin Multicube [6] and the IEEE Scalable Coherent Interface standard [24], the full/empty bits on the Alewife [1] and Tera machines [3], and the primitives for locking and unlocking cache lines on the KSR1 <ref> [15] </ref>. While it is possible to implement arbitrary synchronization mechanisms on top of special-purpose locks, greater concurrency, efficiency, and fault-tolerance may be achieved by using more general-purpose primitives.
Reference: [16] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceeedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> Seattle, WA, </address> <month> May 28-30, </month> <year> 1990. </year>
Reference-contexts: EXC and UPD implementations are embedded in the cache coherence protocols. Our protocols are mainly based on the directory-based protocol of the DASH multiprocessor <ref> [16] </ref>. For fetch and , EXC obtains an exclusive copy of the data and performs the operation locally. NOC sends a request to the memory to perform the operation on uncached data.
Reference: [17] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Several atomic primitives have been proposed and implemented on DSM architectures. Most of them are special-purpose primitives that are designed to support some particular style of synchronization operations. Examples include test and set with special semantics on the DASH multiprocessor <ref> [17] </ref>, the QOLB primitives on the Wisconsin Multicube [6] and the IEEE Scalable Coherent Interface standard [24], the full/empty bits on the Alewife [1] and Tera machines [3], and the primitives for locking and unlocking cache lines on the KSR1 [15].
Reference: [18] <author> H. Massalin. </author> <title> A Lock-Free Multiprocessor OS Kernel. </title> <type> Technical Report CUCS-005-91, </type> <institution> Computer Science Department, Columbia University, </institution> <year> 1991. </year>
Reference-contexts: The compare and swap algorithms are less efficient and conceptually more complex than the load linked/store - conditional algorithms due to the pointer problem [12]. On the other hand, there are several algorithms that need or benefit from compare and - swap <ref> [18, 19, 20, 27] </ref>. A simulation of compare and swap using load linked and store - conditional is less efficient than providing compare and swap in hardware.
Reference: [19] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The compare and swap algorithms are less efficient and conceptually more complex than the load linked/store - conditional algorithms due to the pointer problem [12]. On the other hand, there are several algorithms that need or benefit from compare and - swap <ref> [18, 19, 20, 27] </ref>. A simulation of compare and swap using load linked and store - conditional is less efficient than providing compare and swap in hardware. <p> This capability is useful for algorithms such as the MCS queue-based spin lock <ref> [19] </ref>, in which it reduces by one the number of memory accesses required to relinquish the lock. It is not even necessary that the serial number reside in special memory: load linked and store conditional could be designed to work on doubles. <p> The second uses a counter protected by a test-and-test-and-set lock with bounded exponential backoff to cover the case in which all three primitives are used in a similar manner. The third uses a counter protected by an MCS lock <ref> [19] </ref> to cover the case in which load linked/store - conditional simulates compare and swap. 8 NOC EXC UPD LocusRoute 1.83 1.79 1.70 Cholesky 1.62 1.68 1.59 Table 1: Average write-run length in LocusRoute and Cholesky. 4.2 Sharing Patterns Performance of atomic primitives is affected by two main sharing pattern parameters:
Reference: [20] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Scalable Reader-Writer Synchronization for Shared-Memory Multiprocessors. </title> <booktitle> In Proceeedings of the Third Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 106-113, </pages> <address> Williamsburg, VA, </address> <month> April 21-24, </month> <year> 1991. </year>
Reference-contexts: The compare and swap algorithms are less efficient and conceptually more complex than the load linked/store - conditional algorithms due to the pointer problem [12]. On the other hand, there are several algorithms that need or benefit from compare and - swap <ref> [18, 19, 20, 27] </ref>. A simulation of compare and swap using load linked and store - conditional is less efficient than providing compare and swap in hardware.
Reference: [21] <author> B. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <journal> Computer, </journal> <volume> 15(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Distributed shared memory multiprocessors combine the scalability of network-based architectures and the intuitive programming model provided by shared memory <ref> [21] </ref>. To ensure the consistency of shared data structures, processors perform synchronization operations using hardware-supported primitives. Synchronization overhead (especially atomic update) is one of the obstacles to scalable performance on shared memory multiprocessors. fl This work was supported in part by NSF grants nos.
Reference: [22] <institution> MIPS R4000 Microprocessor User's Manual. MIPS Computer Systems, Inc., </institution> <year> 1991. </year>
Reference-contexts: In practice, processors are generally limited to one outstanding reservation, and reservations may be invalidated even if the variable is not written. On the MIPS R4000 <ref> [22] </ref>, for example, reservations are invalidated on context switches and TLB exceptions. <p> If load linked and store conditional are implemented in the caches, one reservation bit and one reservation address register are needed to maintain ideal semantics, assuming 6 that load linked and store conditional pairs are not allowed to nest. On the MIPS R4000 processor <ref> [22] </ref> there is an LLbit and an on-chip system control processor register LLAddr. The LLAddr register is used only for diagnostic purposes, and serves no function during normal operation. Thus, invalidation of any cache line causes LLbit to be reset.
Reference: [23] <author> L. Rudolph and Z. Segall. </author> <title> Dynamic Decentralized Cache Schemes for MIMD Parallel Processors. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: We began by studying two lock-based applications from the SPLASH suite [25]|LocusRoute and Cholesky| in order to identify typical sharing patterns of atomically accessed data. We replaced the library locks with an assembly language implementation of the test-and-test-and-set lock <ref> [23] </ref> with bounded exponential backoff implemented using the atomic primitives and auxiliary instructions under study. Our three synthetic applications served to explore the parameter space and to provide controlled performance measurements. The first uses lock-free concurrent counters to cover the case in which load linked/store conditional simulates fetch and .
Reference: [24] <institution> IEEE Standard for Scalable Coherent Interface (SCI). IEEE, Inc., </institution> <year> 1993. </year>
Reference-contexts: Most of them are special-purpose primitives that are designed to support some particular style of synchronization operations. Examples include test and set with special semantics on the DASH multiprocessor [17], the QOLB primitives on the Wisconsin Multicube [6] and the IEEE Scalable Coherent Interface standard <ref> [24] </ref>, the full/empty bits on the Alewife [1] and Tera machines [3], and the primitives for locking and unlocking cache lines on the KSR1 [15].
Reference: [25] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year> <month> 17 </month>
Reference: [26] <author> J. E. Veenstra and R. J. Fowler. MINT: </author> <title> A Front End for Efficient Simulation of Shared-Memory M ultiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, </booktitle> <pages> pages 201-207, </pages> <year> 1994. </year>
Reference-contexts: The results were collected from an execution driven cycle-by-cycle simulator. The simulator uses MINT (Mips INTerpreter) <ref> [26] </ref>, which simulates MIPS R4000 object code, as a front end. The back end simulates a 64 node multiprocessor with directory-based caches, 32-byte blocks, queued memory, and a 2-D worm-hole mesh network. The simulator supports directory-based cache coherence protocols with write-invalidate and write-update coherence policies.
Reference: [27] <author> R. W. Wisniewski, L. Kontothanassis, and M. L. Scott. </author> <title> Scalable Spin Locks for Mul-tiprogrammed Systems. </title> <booktitle> In Proceedings of the Eigth International Parallel Processing Symposium, </booktitle> <address> Cancun, Mexico, </address> <month> April 26-29, </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: The compare and swap algorithms are less efficient and conceptually more complex than the load linked/store - conditional algorithms due to the pointer problem [12]. On the other hand, there are several algorithms that need or benefit from compare and - swap <ref> [18, 19, 20, 27] </ref>. A simulation of compare and swap using load linked and store - conditional is less efficient than providing compare and swap in hardware.
References-found: 27

