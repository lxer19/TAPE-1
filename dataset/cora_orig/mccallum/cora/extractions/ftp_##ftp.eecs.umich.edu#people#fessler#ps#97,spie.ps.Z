URL: ftp://ftp.eecs.umich.edu/people/fessler/ps/97,spie.ps.Z
Refering-URL: http://www.eecs.umich.edu/~fessler/papers/conf.html
Root-URL: http://www.cs.umich.edu
Title: SPIE 97, IMAGE RECONSTRUCTION AND RESTORATION II,  Grouped Coordinate Descent Algorithms for Robust Edge-Preserving Image Restoration  
Author: Jeffrey A. Fessler 
Keyword: Image restoration, non-Gaussian noise, deconvolution, Bayesian methods.  
Address: Ann Arbor, MI 48109-2122  
Affiliation: 4240 EECS Bldg., University of Michigan,  
Date: JULY 29, 1997 1  
Note: VOL. 3170, PRESENTED  
Abstract: We present a new class of algorithms for edge-preserving restoration of piecewise-smooth images measured in non-Gaussian noise under shift-variant blur. The algorithms are based on minimizing a regularized objective function, and are guaranteed to monotonically decrease the objective function. The algorithms are derived by using a combination of two previously unconnected concepts: A. De Pierro's convexity technique for optimization transfer, and P. Huber's iteration for M-estimation. Convergence to the unique global minimum is guaranteed for strictly convex objective functions. The convergence rate is very fast relative to conventional gradient-based iterations. The proposed algorithms are flexibly parallelizable, and easily accommodate nonnegativity constraints and arbitrary neighborhood structures. Implementation in Matlab is remarkably simple, requiring no cumbersome line searches or tolerance parameters. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A H Delaney and Y Bresler. </author> <title> A fast and accurate Fourier algorithm for iterative parallel-beam tomography. </title> <journal> IEEE Tr. Im. Proc., </journal> <volume> 5(5) </volume> <pages> 740-53, </pages> <month> May </month> <year> 1996. </year>
Reference: [2] <author> Gabor T Herman. </author> <title> Image reconstruction from projections: The fundamentals of computerized tomography. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference: [3] <author> R M Lewitt. </author> <title> Multidimensional digital image representations using generalized Kaiser-Bessel window functions. </title> <journal> J. Opt. Soc. Amer. Ser. A, </journal> <volume> 7(10) </volume> <pages> 1834-1846, </pages> <month> October </month> <year> 1990. </year>
Reference: [4] <author> P J Huber. </author> <title> Robust statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: SPIE 97, IMAGE RECONSTRUCTION AND RESTORATION II, VOL. 3170, PRESENTED JULY 29, 1997 6 D. Convergence Proof Sketch In discussing the convergence of (9) or equivalently (11), Huber <ref> [4, p. 187] </ref> first showed that the iteration monotonically decreases the objective function, i.e. (x n+1 ) (x n ); by showing that WLS is a "comparison function" that satisfies WLS (x; x n ) WLS (x n ; x n ) (x) (x n ): (13) This is precisely the <p> The monotonicity property ensures that the sequence f (x n )g converges (since is bounded below by P m However, Huber did not proceed to prove that the sequence of iterates fx n g converges. Instead he established convergence of a somewhat different algorithm called the modified residuals method <ref> [4] </ref>. <p> Huber's convergence theorems <ref> [4] </ref> established that the mapping M () monotonically decreases , but did not establish convergence to a fixed point of M (). A. Nonexpansive The following result shows that the iteration M is nonexpansive, i.e., the sequence of iterates "gets closer" to a minimizer each iteration.
Reference: [5] <author> J A Fessler. </author> <title> Penalized weighted least-squares image reconstruction for positron emission tomography. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 13(2) </volume> <pages> 290-300, </pages> <month> June </month> <year> 1994. </year>
Reference: [6] <author> D L Snyder, C W Helstrom, A D Lanterman, M Faisal, and R L White. </author> <title> Compensation for readout noise in CCD images. </title> <journal> J. Opt. Soc. Amer. Ser. A, </journal> <volume> 12(2) </volume> <pages> 272-83, </pages> <month> February </month> <year> 1995. </year>
Reference: [7] <author> J A Fessler and W L Rogers. </author> <title> Spatial resolution properties of penalized-likelihood image reconstruction methods: </title> <journal> Space-invariant tomographs. IEEE Tr. Im. Proc., </journal> <volume> 5(9) </volume> <pages> 1346-58, </pages> <month> September </month> <year> 1996. </year>
Reference: [8] <author> J A Fessler, E P Ficaro, N H Clinthorne, and K Lange. </author> <title> Grouped-coordinate ascent algorithms for penalized-likelihood transmission image reconstruction. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 16(2) </volume> <pages> 166-75, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: IV. GROUPED COORDINATE DESCENT ALGORITHM In <ref> [8] </ref> we presented a new algorithm for penalized-likelihood tomographic image reconstruction from Poisson transmission measurements. The basic principles of the algorithm apply much more broadly than to the specific problem addressed in [8]. <p> IV. GROUPED COORDINATE DESCENT ALGORITHM In <ref> [8] </ref> we presented a new algorithm for penalized-likelihood tomographic image reconstruction from Poisson transmission measurements. The basic principles of the algorithm apply much more broadly than to the specific problem addressed in [8]. Here we use those principles to derive a general algorithm suitable for finding a minimizer of objective functions of the form (5). <p> In a grouped-coordinate descent (GCD) algorithm 2 , we update x S while holding x n ~ S fixed at the nth update <ref> [8, 14] </ref>. <p> if r (x) = Bx c; then r i ([x S ; x n X ff ij b ij (x j x n for any choice 5 of ff ij 0 that satisfies the constraint X ff ij = 1; 8i: (17) We discussed specific choices for ff ij in <ref> [8] </ref>. <p> Each j only depends on one x j , so the minimization step in (14) reduces to jSj separate 1D minimizations. Thus (14) becomes the parallelizable operations: x n+1 x Note that because the minimization is separable, it is trivial to incorporate nonnegativity constraints <ref> [8] </ref>. Borrowing from the expectation-maximization algorithm [13, 14], we refer to these minimizations as the "M-step" of the algorithm. (Computing r i (x n ) can be considered the "E-step.") 4 Separable surrogate functions are very convenient for enforcing the nonnegativity constraint. <p> Thus we can apply the successive-substitutions method described by (9) in Section 3 to the minimization required in (21). This new approach is a significant improvement over the M-step methods described in <ref> [8] </ref>. To simplify notation, consider the update of a particular pixel j for a particular iteration n, so that we can drop the dependence on j and n. <p> The next section analyzes this iteration. V. THE 1-D SUBITERATION A key component of several algorithms for image reconstruction and restoration (often part of the so-called M-step) is the minimization of a scalar functional of the form (x) = i=1 Examples include <ref> [8, 19, 20] </ref> and (22) above.
Reference: [9] <author> C Bouman and K Sauer. </author> <title> A generalized Gaussian image model for edge-preserving MAP estimation. </title> <journal> IEEE Tr. Im. Proc., </journal> <volume> 2(3) </volume> <pages> 296-310, </pages> <month> July </month> <year> 1993. </year>
Reference: [10] <author> H L Royden. </author> <title> Real analysis. </title> <publisher> Macmillan, </publisher> <address> New York, 3 edition, </address> <year> 1988. </year>
Reference: [11] <author> P Charbonnier, L Blanc-Feraud, G Aubert, and M Barlaud. </author> <title> Deterministic edge-preserving regularization in computed imaging. </title> <journal> IEEE Tr. Im. Proc., </journal> <volume> 6(2), </volume> <month> February </month> <year> 1997. </year>
Reference-contexts: Charbonnier et al. addressed this problem by applying iterative algorithms such as conjugate-gradient and Gauss-Siedel iterations to find an approximate minimum of the quadratic form in (11) <ref> [11] </ref>. Such use of iterative linear-equation solvers as subiterations within the main iterations is fairly expensive computa-tionally, and may therefore be suboptimal for imaging problems. Furthermore the Huber iteration does not easily accommodate nonnegativity constraints for x, which are often important in imaging problems. <p> Furthermore the Huber iteration does not easily accommodate nonnegativity constraints for x, which are often important in imaging problems. An additional minor consideration is that the use of approximate solutions to (11) raises questions about the guarantee of convergence. (The convergence proofs in <ref> [11] </ref> assume that (11) is exactly minimized each iteration, even though this is not achievable in practice.) Therefore, for imaging problems it is desirable to find new algorithms that are globally convergent but that require less expensive subiterations. IV. <p> But this choice leads to very slow convergence rates. Instead, we focus here on the special case where ! i (t) = _ i (t)=t; as defined by C4. That choice has its origins in Huber's iteration, as well as in the "half-quadratic" method for nonquadratic penalty functions <ref> [11, 21, 22] </ref>.
Reference: [12] <author> P J Green. </author> <title> Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant alternatives. </title> <journal> J. Royal Stat. Soc. Ser. B, </journal> <volume> 46(2) </volume> <pages> 149-192, </pages> <year> 1984. </year>
Reference-contexts: n ) = i=1 1 ([Bx c] i ) 2 = 2 then one can easily show that the successive substitutions iteration (9) can also be expressed as follows: x n+1 = arg min WLS (x; x n ): (11) This type of iteration is known as iteratively reweighted least-squares <ref> [12] </ref>.
Reference: [13] <author> A P Dempster, N M Laird, and D B Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Stat. Soc. Ser. B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Thus (14) becomes the parallelizable operations: x n+1 x Note that because the minimization is separable, it is trivial to incorporate nonnegativity constraints [8]. Borrowing from the expectation-maximization algorithm <ref> [13, 14] </ref>, we refer to these minimizations as the "M-step" of the algorithm. (Computing r i (x n ) can be considered the "E-step.") 4 Separable surrogate functions are very convenient for enforcing the nonnegativity constraint.
Reference: [14] <author> J A Fessler and A O Hero. </author> <title> Space-alternating generalized expectation-maximization algorithm. </title> <journal> IEEE Tr. Sig. Proc., </journal> <volume> 42(10) </volume> <pages> 2664-77, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In a grouped-coordinate descent (GCD) algorithm 2 , we update x S while holding x n ~ S fixed at the nth update <ref> [8, 14] </ref>. <p> The GCD update (cf SAGE algorithm <ref> [14, 15] </ref>) is then: x n+1 x S x n+1 j ; j 2 ~ S: 2 In a GCD method, S varies with n. To simplify notation, we leave this dependence implicit. <p> Thus (14) becomes the parallelizable operations: x n+1 x Note that because the minimization is separable, it is trivial to incorporate nonnegativity constraints [8]. Borrowing from the expectation-maximization algorithm <ref> [13, 14] </ref>, we refer to these minimizations as the "M-step" of the algorithm. (Computing r i (x n ) can be considered the "E-step.") 4 Separable surrogate functions are very convenient for enforcing the nonnegativity constraint.
Reference: [15] <author> J A Fessler and A O Hero. </author> <title> Penalized maximum-likelihood image reconstruction using space-alternating generalized EM algorithms. </title> <journal> IEEE Tr. Im. Proc., </journal> <volume> 4(10) </volume> <pages> 1417-29, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Instead he established convergence of a somewhat different algorithm called the modified residuals method [4]. Using arguments very similar to those in <ref> [15, 16] </ref>, we have shown that if has a unique minimizer ^x, then fx n g converges globally to ^x. (Strict convexity of is sufficient but certainly not necessary to ensure has a unique minimizer.) The details will be provided elsewhere, hopefully after resolving whether the sequence convergences to a minimizer <p> The GCD update (cf SAGE algorithm <ref> [14, 15] </ref>) is then: x n+1 x S x n+1 j ; j 2 ~ S: 2 In a GCD method, S varies with n. To simplify notation, we leave this dependence implicit. <p> The overall algorithm is summarized in Table I, where [x] + = x for x &gt; 0 and is 0 otherwise. This [] + operator enforces the nonnegativity constraint, which is optional. The symbol ":=" denotes assignment. C. Convergence We have extended the proof given in <ref> [15] </ref> to establish convergence of the grouped coordinate descent algorithm described above, provided that the minimizer of is unique and that the groups S are chosen such that every pixel is updated periodically, as discussed in [15]. <p> C. Convergence We have extended the proof given in <ref> [15] </ref> to establish convergence of the grouped coordinate descent algorithm described above, provided that the minimizer of is unique and that the groups S are chosen such that every pixel is updated periodically, as discussed in [15]. The extension of the proof accommodates the fact that the surrogate functions j (; x n ) are not exactly minimized by a finite number of subiterations of the form (24).
Reference: [16] <author> K Lange and R Carson. </author> <title> EM reconstruction algorithms for emission and transmission tomography. </title> <journal> J. Comp. Assisted Tomo., </journal> <volume> 8(2) </volume> <pages> 306-316, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Instead he established convergence of a somewhat different algorithm called the modified residuals method [4]. Using arguments very similar to those in <ref> [15, 16] </ref>, we have shown that if has a unique minimizer ^x, then fx n g converges globally to ^x. (Strict convexity of is sufficient but certainly not necessary to ensure has a unique minimizer.) The details will be provided elsewhere, hopefully after resolving whether the sequence convergences to a minimizer
Reference: [17] <author> A R De Pierro. </author> <title> On the relation between the ISRA and the EM algorithm for positron emission tomography. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 12(2) </volume> <pages> 328-333, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: A. Choosing Surrogate Functions We restrict attention here to additively separable 4 surrogate functions (; x n ) satisfying (x S ; x n ) = j2S To choose these j 's, we use modifications of De Pierro's convexity method <ref> [17, 18] </ref>.
Reference: [18] <author> A R De Pierro. </author> <title> A modified expectation maximization algorithm for penalized likelihood estimation in emission tomography. </title> <journal> IEEE Tr. Med. Im., </journal> <volume> 14(1) </volume> <pages> 132-137, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: A. Choosing Surrogate Functions We restrict attention here to additively separable 4 surrogate functions (; x n ) satisfying (x S ; x n ) = j2S To choose these j 's, we use modifications of De Pierro's convexity method <ref> [17, 18] </ref>.
Reference: [19] <author> C A Bouman and K Sauer. </author> <title> A unified approach to statistical tomography using coordinate descent optimization. </title> <journal> IEEE Tr. Im. Proc., </journal> <volume> 5(3) </volume> <pages> 480-92, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: The next section analyzes this iteration. V. THE 1-D SUBITERATION A key component of several algorithms for image reconstruction and restoration (often part of the so-called M-step) is the minimization of a scalar functional of the form (x) = i=1 Examples include <ref> [8, 19, 20] </ref> and (22) above.
Reference: [20] <author> J A Fessler and S D Booth. </author> <title> Conjugate-gradient preconditioning methods for shift-variant image reconstruction. </title> <journal> IEEE Tr. </journal> <note> Im. Proc., 1997. Submitted. </note>
Reference-contexts: The next section analyzes this iteration. V. THE 1-D SUBITERATION A key component of several algorithms for image reconstruction and restoration (often part of the so-called M-step) is the minimization of a scalar functional of the form (x) = i=1 Examples include <ref> [8, 19, 20] </ref> and (22) above.
Reference: [21] <author> D Geman and G Reynolds. </author> <title> Constrained restoration and the recovery of discontinuities. </title> <journal> IEEE Tr. Patt. Anal. Mach. Int., </journal> <volume> 14(3) </volume> <pages> 367-383, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: But this choice leads to very slow convergence rates. Instead, we focus here on the special case where ! i (t) = _ i (t)=t; as defined by C4. That choice has its origins in Huber's iteration, as well as in the "half-quadratic" method for nonquadratic penalty functions <ref> [11, 21, 22] </ref>.
Reference: [22] <author> D Geman and C Yang. </author> <title> Nonlinear image recovery with half-quadratic regularization. </title> <journal> IEEE Tr. Im. Proc., </journal> <volume> 4(7) </volume> <pages> 932-46, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: But this choice leads to very slow convergence rates. Instead, we focus here on the special case where ! i (t) = _ i (t)=t; as defined by C4. That choice has its origins in Huber's iteration, as well as in the "half-quadratic" method for nonquadratic penalty functions <ref> [11, 21, 22] </ref>.
References-found: 22

