URL: http://www.cs.toronto.edu/~dale/papers/uai97.ps.gz
Refering-URL: http://www.cs.toronto.edu/~dale/
Root-URL: 
Email: greiner@scr.siemens.com  grove@research.nj.nec.com  daes@linc.cis.upenn.edu  
Title: Learning Bayesian Nets that Perform Well  
Author: Russell Greiner Adam J. Grove Dale Schuurmans 
Date: August 1997.  
Note: Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI-97), Providence, RI,  
Address: 755 College Road East Princeton, NJ 08540-6632  4 Independence Way Princeton, NJ 08540  Philadelphia, PA 19104-6228  
Affiliation: Siemens Corporate Research  NEC Research Institute  Inst. for Research in Cognitive Science University of Pennsylvania  
Abstract: A Bayesian net (BN) is more than a succinct way to encode a probabilistic distribution; it also corresponds to a function used to answer queries. A BN can therefore be evaluated by the accuracy of the answers it returns. Many algorithms for learning BNs, however, attempt to optimize another criterion (usually likelihood, possibly augmented with a regularizing term), which is independent of the distribution of queries that are posed. This paper takes the "performance criteria" seriously, and considers the challenge of computing the BN whose performance | read "accuracy over the distribution of queries" | is optimal. We show that many aspects of this learning task are more difficult than the corresponding subtasks in the standard model. 
Abstract-found: 1
Intro-found: 1
Reference: [Bun96] <author> Wray Buntine. </author> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <year> 1996. </year>
Reference-contexts: Often the underlying distribution, which should be used to map questions to appropriate responses, is not known a priori. In such cases, if we have access to training examples, we can try to learn the model. There are currently many algorithms for learning BNs <ref> [Hec95, Bun96] </ref>. Each such learning algorithm fl Also: NEC Research Institute, Princeton, NJ tries to determine which BN is optimal, usually based on some measure such as log-likelihood (possibly augmented with a "regularizing" term, leading to measures like MDL [LB94], and Bayesian Information Criterion (BIC) [Sch78]).
Reference: [CH92] <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: speaking, #P is the class of problems corresponding to counting the number of satisfiable assignments to a satisfiability problem, and thus #P -hard problems are at least as difficult as problems in NP. in the general SQ case, or to estimate KL D ( B ) from incomplete tuples D <ref> [CH92, RBKK95] </ref>; as here the Bayesian net computations are inherently intractable. We will see these parallels again below. Another challenge is computing the sample complexity of gathering the information required to compute the score for a network.
Reference: [DL93] <author> P. Dagum and M. Luby. </author> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60 </volume> <pages> 141-153, </pages> <month> April </month> <year> 1993. </year>
Reference: [FG96] <author> Nir Friedman and Moises Goldszmidt. </author> <title> Building classifers using Bayesian networks. </title> <booktitle> In Proc. AAAI-96, </booktitle> <year> 1996. </year>
Reference-contexts: We first close this section by discussing how our results compare with others. Related Results: The framework closest to ours is Friedman and Goldszmidt <ref> [FG96] </ref>, as they also consider finding the BN that is best for some distribution of queries, and also explain why the BN with (say) maximal log-likelihood may not be the one with optimal performance on a specific task. <p> This means that systems that use LL (BjD) to rank BNs could do poorly if the contributions of second summation dominate those of the first. The <ref> [FG96] </ref> paper, however, considers only building BNs for classification, i.e., where every query is of the specific form p ( C = c j A 1 = a 1 ; : : : ; A n = a n ) where C is the only "consequent" variable, and fA i g
Reference: [FY96] <author> Nir Friedman and Z. Yakhini. </author> <title> On the sample complexity of learning Bayesian networks. </title> <booktitle> In Proc. Uncertainty in AI, </booktitle> <year> 1996. </year>
Reference-contexts: In our model, by contrast, the learner is observing which such queries are posed by the "environment", as it will be evaluated based on its accuracy with respect to these queries. Other researchers, including <ref> [FY96, Hof93] </ref>, also compute the sample complexity for learning good BNs. <p> Of course, as can be arbitrarily small (e.g., o (1=2 n ) or worse), this M D bound can be arbitrarily large, in terms of the size of the Bayesian net. Note also that the Friedman and Yakhini <ref> [FY96] </ref> bound similarly depends on "skewness" of the distribution, which they define as the smallest non-zero probability of an event, over all atomic events. 5 Two final comments: (1) Recall that these bounds describe only how many examples are required; not how much work is required, given this information.
Reference: [GGS97] <author> Russell Greiner, Adam Grove, and Dale Schuurmans. </author> <title> Learning Bayesian nets that perform well. </title> <type> Technical report, </type> <institution> Siemens Corporate Research, </institution> <year> 1997. </year>
Reference-contexts: We can still estimate the score of a BN, in the following on-line fashion: Theorem 4 First, let S SQ = fsq ( x i ; y i )g i be a set of 2 4 4 Proofs for all new theorems, lemmas and corollaries appear in <ref> [GGS97] </ref>. unlabeled statistical queries drawn randomly from the sq ( ; ) distribution. <p> Only then can we be confident about B fl 's accuracy. (See proof in <ref> [GGS97] </ref>.) As in Section 3.1, we can also consider the slightly more complex task of learning the CP-table entries from unlabeled statistical queries sq ( X = x ; Y = y ), augmented with examples of the underlying distribution p ( ). <p> A detailed statement and proof of this re sult is a straightforward extension of Theorem 7, so we omit the details here. (See <ref> [GGS97] </ref>.) The point is that, from a sample complexity perspective, it is feasible to learn near optimal settings for the CP-table entries in a fixed Bayesian network structure under our model.
Reference: [Gre96] <author> Russell Greiner. </author> <title> PALO: A probabilistic hill-climbing algorithm. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 83(1-2), </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: In general, of course, we may have to use the examples to learn that structure as well. The obvious approach is to hill-climb in the discrete, but combinatorial space of BN structures, perhaps using a system like palo <ref> [Gre96] </ref>, after augmenting it to climb from one structure S i to a "neighboring" structure S i+1 , if S i+1 , filled with some CP-table entries, appears better than S i with (near) optimal CP-table values, over a distribution of queries.
Reference: [Hec95] <author> David E. Heckerman. </author> <title> A tutorial on learning with Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <year> 1995. </year>
Reference-contexts: Often the underlying distribution, which should be used to map questions to appropriate responses, is not known a priori. In such cases, if we have access to training examples, we can try to learn the model. There are currently many algorithms for learning BNs <ref> [Hec95, Bun96] </ref>. Each such learning algorithm fl Also: NEC Research Institute, Princeton, NJ tries to determine which BN is optimal, usually based on some measure such as log-likelihood (possibly augmented with a "regularizing" term, leading to measures like MDL [LB94], and Bayesian Information Criterion (BIC) [Sch78]). <p> There is an obvious parallel between estimating err Q 0 ( B ) when dealing with SQ B queries Q 0 , and es timating KL D 0 ( B ) from complete tuples D 0 <ref> [Hec95] </ref>: both tasks are quite straightforward, basically because their respective Bayesian net computations are simple. <p> can always return a B 00 hV;Ei;Q such that err Q ( B 00 In contrast, notice that the analogous task is trivial in the log-likelihood framework: Given complete train-ing examples (and some benign assumptions), the CP-table that produces the optimal maximal-likelihood BN corresponds simply to the observed frequency estimates <ref> [Hec95] </ref>. However, the news is not all bad in our case. Although the problem may be computationally hard, the sample complexity can be polynomial.
Reference: [Hoe63] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: Another challenge is computing the sample complexity of gathering the information required to compute the score for a network. It is easy to collect a sufficient number of examples if we are considering learning from labeled statistical queries. Here, a simple application of Hoeffding's Inequality <ref> [Hoe63] </ref> shows 4 Theorem 3 Let err Q ( B ) = M LSQ hq;pi2S LSQ be the empirical score of the Bayesian net B, based on a set S LSQ of 1 2 labeled statistical queries, drawn randomly from the sq ( ) distribution and labeled by p ( ).
Reference: [Hof93] <author> Klaus-U. Hoffgen. </author> <title> Learning and robust learning of product distributions. </title> <booktitle> In Proc. COLT-93, </booktitle> <pages> pages 77-83, </pages> <year> 1993 1993. </year> <note> ACM Press. </note>
Reference-contexts: In our model, by contrast, the learner is observing which such queries are posed by the "environment", as it will be evaluated based on its accuracy with respect to these queries. Other researchers, including <ref> [FY96, Hof93] </ref>, also compute the sample complexity for learning good BNs. <p> Unfortunately, using these examples to compute the score of a BN requires solving a #P -hard problem; see Theorem 2. (2) The sample complexity results hold for estimating the accuracy of any system for representing arbitrary distributions; not just BNs. 5 Hoffgen <ref> [Hof93] </ref> was able to avoid this dependency, in certain "log-loss" contexts, by "tilting" the empirical distribution to avoid 0-probability atomic events.
Reference: [Kea93] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. STOC-93, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: As such, they resemble the standard class of "statistical queries", discussed by Kearns and others <ref> [Kea93] </ref> in the context of noise-tolerant learners. 1 In that model, however, the learner is posing such queries to gather information about the underlying distribution, and the learner's score depends its accuracy with respect to some other specific set of queries (here the same p ( C = c j A
Reference: [KR94] <author> Roni Khardon and Dan Roth. </author> <title> Learning to reason. </title> <booktitle> In Proc. AAAI-94, </booktitle> <pages> pages 682-687, </pages> <year> 1994. </year>
Reference-contexts: the example queries it has seen; it should be able to "extend" the BN based on the event distribution.) Contributions: As noted repeatedly in Machine Learning and elsewhere, the goal of a learning algorithm should be to produce a "performance element" that will work well on its eventual performance task <ref> [SMCB77, KR94] </ref>. This paper considers the task of learning an effective Bayesian net within this frame work, and argues that the goal of a BN-learner should be to produce a BN whose error, over the distribution of queries, is minimal.
Reference: [LB94] <author> Wai Lam and Fahiem Bacchus. </author> <title> Learning Bayesian belief networks: An approach based on the MDL principle. </title> <journal> Computation Intelligence, </journal> <volume> 10(4) </volume> <pages> 269-293, </pages> <year> 1994. </year>
Reference-contexts: There are currently many algorithms for learning BNs [Hec95, Bun96]. Each such learning algorithm fl Also: NEC Research Institute, Princeton, NJ tries to determine which BN is optimal, usually based on some measure such as log-likelihood (possibly augmented with a "regularizing" term, leading to measures like MDL <ref> [LB94] </ref>, and Bayesian Information Criterion (BIC) [Sch78]). However, these typical measures are independent of the queries that will be posed.
Reference: [Pea88] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Each node also includes a conditional-probability-table that specifies how the node's values depends (stochastically) on the values of its parents. (Readers unfamiliar with these ideas are referred to <ref> [Pea88] </ref>.) In general, we assume there is a stationary underlying distribution P over the N variables V = fV 1 ; : : : ; V N g. (I.e., p ( V 1 = v 1 ; : : : ; V N = v N ) 0 P example, perhaps <p> Note, however, that this computation is much easier in the SQ B case, because there is an trivial way to evaluate a Bayesian net on any Markov-blanket query <ref> [Pea88] </ref>; and hence to compute the score. <p> Notice Equation 4 is simple to compute, as it involves no non-trivial Bayesian net computation; see the simple algorithms in <ref> [Pea88] </ref>. 4 HOW CAN THE QUERY DISTRIBUTION HELP? Our intuition throughout this paper is that having access to the distribution of queries should allow us to learn better and more efficiently than if we only get to see domain tuples alone.
Reference: [RBKK95] <author> Stuart Russell, John Binder, Daphne Koller, and Keiji Kanazawa. </author> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proc. IJCAI-95, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference-contexts: speaking, #P is the class of problems corresponding to counting the number of satisfiable assignments to a satisfiability problem, and thus #P -hard problems are at least as difficult as problems in NP. in the general SQ case, or to estimate KL D ( B ) from incomplete tuples D <ref> [CH92, RBKK95] </ref>; as here the Bayesian net computations are inherently intractable. We will see these parallels again below. Another challenge is computing the sample complexity of gathering the information required to compute the score for a network. <p> 3.3 HILL CLIMBING It should not be surprising that finding the optimal CP-tables was computationally hard, as this problem has a lot in common with the challenge of learning the KL ( )-best network, given partially specified tuples; a task for which people often use iterative steepest ascent climbing methods <ref> [RBKK95] </ref>. We now briefly consider the analogous approach in our setting. Given a single labeled statistical query "hx; y; p ( x j y )i", consider how to change the value of the CP-table entry [qjr], whose current value is e qjr . <p> Unfortunately, we see that evaluating the gradient requires computing conditional probabilities in a BN. This is analogous to to the known result in the traditional model <ref> [RBKK95] </ref>. It thus follows that it can be #P -hard to evaluate this gradient in general (see Theorem 2). However, in special cases | i.e., BNs for which inference is tractable | efficient computation is possible.
Reference: [Rot96] <author> D. Roth. </author> <title> On the hardness of approximate reasoning. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 82(1-2), </pages> <month> April </month> <year> 1996. </year>
Reference: [Sch78] <author> G. Schwartz. </author> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: Each such learning algorithm fl Also: NEC Research Institute, Princeton, NJ tries to determine which BN is optimal, usually based on some measure such as log-likelihood (possibly augmented with a "regularizing" term, leading to measures like MDL [LB94], and Bayesian Information Criterion (BIC) <ref> [Sch78] </ref>). However, these typical measures are independent of the queries that will be posed.
Reference: [SMCB77] <author> Reid G. Smith, Thomas M. Mitchell, R. Chestek, and Bruce G. Buchanan. </author> <title> A model for learning systems. </title> <booktitle> In Proc. IJCAI-77, </booktitle> <pages> pages 338-343, </pages> <publisher> MIT. Morgan Kaufmann. </publisher>
Reference-contexts: the example queries it has seen; it should be able to "extend" the BN based on the event distribution.) Contributions: As noted repeatedly in Machine Learning and elsewhere, the goal of a learning algorithm should be to produce a "performance element" that will work well on its eventual performance task <ref> [SMCB77, KR94] </ref>. This paper considers the task of learning an effective Bayesian net within this frame work, and argues that the goal of a BN-learner should be to produce a BN whose error, over the distribution of queries, is minimal.
References-found: 18

