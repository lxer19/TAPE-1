URL: http://www.cs.jhu.edu/salzberg/critique.ps
Refering-URL: http://www.cs.bham.ac.uk/~anp/dm_docs/old_papers.html
Root-URL: 
Title: Data Mining and Knowledge Discovery,  On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach  
Author: STEVEN L. SALZBERG Editor: Usama Fayyad 
Keyword: classification, comparative studies, statistical methods  
Address: Baltimore, MD 21218, USA  
Affiliation: Department of Computer Science, Johns Hopkins University,  
Note: c 1997 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 1,  
Email: salzberg@cs.jhu.edu  
Date: 317327 (1997)  
Abstract: An important component of many data mining projects is finding a good classification algorithm, a process that requires very careful thought about experimental design. If not done very carefully, comparative studies of classification and other types of algorithms can easily result in statistically invalid conclusions. This is especially true when one is using data mining techniques to analyze very large databases, which inevitably contain some statistically unlikely data. This paper describes several phenomena that can, if ignored, invalidate an experimental comparison. These phenomena and the conclusions that follow apply not only to classification, but to computational experiments in almost any aspect of data mining. The paper also discusses why comparative analysis is more important in evaluating some types of algorithms than for others, and provides some suggestions about how to avoid the pitfalls suffered by many experimental studies. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Aha. </author> <title> Generalizing from case studies: A case study. </title> <booktitle> In Proc. Ninth Intl. Workshop on Machine Learning, </booktitle> <pages> pages 110, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: (p-values) would have to be, e.g., 0.005 in order to obtain levels comparable to a single experiment using a level of 0.05. (This assumes, unrealistically, that the experiments are independent.) But few if any experimenters keep careful count of how many adjustments they consider. (Kibler and Langley [15] and Aha <ref> [1] </ref> suggest, as an alternative, that the parameter settings themselves be studied as independent variables, and that their effects be measured on artificial data. A greater problem occurs when one uses an algorithm that has been used before: that algorithm may already have been tuned on 324 STEVEN L.
Reference: 2. <author> W. Cochran and G. Cox. </author> <title> Experimental Designs. </title> <publisher> Wiley, </publisher> <address> 2nd edition, </address> <year> 1957. </year>
Reference-contexts: As pointed out by Feelders and Verkooijen [8], finding the proper statistical procedure to compare two or more classification algorithms can be quite difficult, and requires more than an introductory level knowledge of statistics. A good general reference for experimental design is Cochran and Cox <ref> [2] </ref>, and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand [11]. Jensen [13, 14] discusses a framework for experimental comparison of classifiers and addresses significance testing, and Cohen and Jensen [3] discuss some specific ways to remove optimistic statistical bias from such experiments.
Reference: 3. <author> P. R. Cohen and D. Jensen. </author> <title> Overfitting explained. </title> <booktitle> In Prelim. Papers Sixth Intl. Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 115122, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: Here I focus on design of experiments, which has been the subject of little concern in the machine learning community until recently (with some exceptions, such as [15] and <ref> [3] </ref>). Included in the comparative study category are papers that neither introduce a new algorithm nor improve an old one; instead, they consider one or more known algorithms and conduct experiments on known datasets. They may also include variations on known algorithms. <p> A good general reference for experimental design is Cochran and Cox [2], and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand [11]. Jensen [13, 14] discusses a framework for experimental comparison of classifiers and addresses significance testing, and Cohen and Jensen <ref> [3] </ref> discuss some specific ways to remove optimistic statistical bias from such experiments. One important innovation they discuss is randomization testing, in which one derives a reference distribution as follows. For each trial, the data set is copied and class labels are replaced with random class labels.
Reference: 4. <author> F. </author> <title> Denton. Data mining as an industry. </title> <journal> Review of Economics and Statistics, </journal> <volume> 67:124127, </volume> <year> 1985. </year>
Reference-contexts: Denton <ref> [4] </ref> made similar observations about how the reviewing and publication process can skew results. Although the data mining community is much broader than the classification community, it is likely that benchmark databases will emerge, and that different researchers will test their mining techniques on them.
Reference: 5. <author> T. Dietterich. </author> <title> Statistical tests for comparing supervised learning algorithms. </title> <type> Technical report, </type> <institution> Oregon State University, Corvallis, </institution> <address> OR, </address> <year> 1996. </year>
Reference-contexts: This problem is widespread in comparative machine learning studies. (One of the authors of the study cited above has written recently that the paired t-test has a high probability of Type I error ... and should never be used <ref> [5] </ref>.) It is worth noting here that even statisticians have difficulty agreeing on the correct framework for hypothesis testing in complex experimental designs. For example, the whole framework of using alpha levels and p-values has been questioned when more than two hypotheses are under consideration [20]. 3.1.
Reference: 6. <author> B. Everitt. </author> <title> The Analysis of Contingency Tables. </title> <publisher> Chapman and Hall, </publisher> <address> London., </address> <year> 1977. </year>
Reference-contexts: Another, nearly identical form of this test is known as McNemar's test <ref> [6] </ref>, which uses the 2 distribution.
Reference: 7. <author> U. M. Fayyad and K. B. Irani. </author> <title> Multi-interval discretization of continuous valued attributes for classification learning. </title> <booktitle> In Proc. 13th Intl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 10221027, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Equally important for many problems is the representation of the data, which may vary from one study to the next even when the same basic dataset is used. For example, numeric values are sometimes converted to a discrete set of intervals, especially when using decision tree algorithms <ref> [7] </ref>. Whenever tuning takes place, every adjustment should really be considered a separate experiment.
Reference: 8. <author> A. Feelders and W. Verkooijen. </author> <title> Which method learns most from the data? In Prelim. </title> <booktitle> Papers Fifth Intl. Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 219225, </pages> <address> Fort Lauderdale, Florida, </address> <year> 1995. </year>
Reference-contexts: Statisticians have been aware of this problem for a very long time; it is known as the multiplicity effect. At least two recent papers have focused their attention nicely on how classification researchers might address this effect <ref> [10, 8] </ref>. In particular, let ff be the probability that if no differences exist among our algorithms, we will make at least one mistake; i.e., we will find at least one significant difference. Thus ff is the percent of the time in which we (the experimenters) make an error. <p> If N is the number of agreements and N &gt;> n, then it can be argued that our belief that the algorithms are doing the same thing should increase regardless of the pattern of disagreement. As pointed out by Feelders and Verkooijen <ref> [8] </ref>, finding the proper statistical procedure to compare two or more classification algorithms can be quite difficult, and requires more than an introductory level knowledge of statistics.
Reference: 9. <author> A. Flexer. </author> <title> Statistical evaluation of neural network experiments: Minimum requirements and current practice. </title> <editor> In R. Trappl, editor, </editor> <booktitle> Cybernetics and Systems '96: Proc. 13th European Meeting on Cybernetics and Systems Res., </booktitle> <pages> pages 10051008. </pages> <institution> Austrian Society for Cybernetic Studies, </institution> <year> 1996. </year>
Reference-contexts: His survey found that a strikingly high percentage of new algorithms (29%) were not evaluated on any real problem at all, and that very few (only 8%) were compared to more than one alternative on real data. In a survey by Flexer <ref> [9] </ref> of experimental neural network papers, only 3 out of 43 studies in leading journals used a separate data set for parameter tuning, which leaves open the possibility that many of the reported results were overly optimistic. <p> This point will seem obvious to many experimental researchers, but the fact is that papers are still appearing in which this methodology is not followed. In the survey by Flexer <ref> [9] </ref>, only 3 out of 43 experimental papers in leading neural network journals used a separate data set for parameter tuning; the remaining 40 papers either did not explain how they adjusted parameters or else did their adjustments after using the test set.
Reference: 10. <author> O. Gascuel and G. Caraux. </author> <title> Statistical significance in inductive learning. </title> <booktitle> In Proc. of the European Conf. on Artificial Intelligence (ECAI), </booktitle> <pages> pages 435439, </pages> <address> New York, 1992. </address> <publisher> Wiley. </publisher>
Reference-contexts: Statisticians have been aware of this problem for a very long time; it is known as the multiplicity effect. At least two recent papers have focused their attention nicely on how classification researchers might address this effect <ref> [10, 8] </ref>. In particular, let ff be the probability that if no differences exist among our algorithms, we will make at least one mistake; i.e., we will find at least one significant difference. Thus ff is the percent of the time in which we (the experimenters) make an error.
Reference: 11. <author> D. Hildebrand. </author> <title> Statistical Thinking for Behavioral Scientists. </title> <publisher> Duxbury Press, </publisher> <address> Boston, MA, </address> <year> 1986. </year>
Reference-contexts: A good general reference for experimental design is Cochran and Cox [2], and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand <ref> [11] </ref>. Jensen [13, 14] discusses a framework for experimental comparison of classifiers and addresses significance testing, and Cohen and Jensen [3] discuss some specific ways to remove optimistic statistical bias from such experiments. One important innovation they discuss is randomization testing, in which one derives a reference distribution as follows. <p> Thus a p-value of 0.05 indicates that there was a 5% probability that the observed results were merely random variation of some kind, and do not indicate a true difference between the treatments. For a description of how to perform a paired t-test, see reference <ref> [11] </ref> or another introductory statistics text. 3. Cross validation refers to a widely used experimental testing procedure. The idea is to break a data set up into k disjoint subsets of approximately equal size.
Reference: 12. <author> R. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning, </booktitle> <address> 11(1):6390, </address> <year> 1993. </year>
Reference-contexts: The NetTalk dataset of English pronunciation data (introduced by Sejnowski and Rosenberg, [21] has been used in numerous experiments, as has the protein secondary structure data (introduced by Qian and Sejnowski [19]), to cite just two examples. Holte <ref> [12] </ref> collected results on 16 different datasets, and found as many as 75 different published accuracy figures for some of them. Any new experiments on these and other UCI datasets run the risk of finding significant results that are no more than statistical accidents, as explained in Section 3.2. <p> It is not valid to make general statements about other datasets. The only way this might be valid would be if the UCI repository were known to represent a larger population of classification problems. In fact, though, as argued persuasively by Holte <ref> [12] </ref> and others, the UCI repository is a very limited sample of problems, many of which are quite easy for a classifier. (Many of them may represent the same concept class, for example many might be almost linearly separable, as suggested by the strong performance of the perceptron algorithm in one
Reference: 13. <author> D. Jensen. </author> <title> Knowledge discovery through induction with randomization testing. </title> <editor> In G. Piatetsky-Shapiro, editor, </editor> <booktitle> Proc. 1991 Knowledge Discovery in Databases Workshop, </booktitle> <pages> pages 148159, </pages> <address> Menlo Park, CA, 1991. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A good general reference for experimental design is Cochran and Cox [2], and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand [11]. Jensen <ref> [13, 14] </ref> discusses a framework for experimental comparison of classifiers and addresses significance testing, and Cohen and Jensen [3] discuss some specific ways to remove optimistic statistical bias from such experiments. One important innovation they discuss is randomization testing, in which one derives a reference distribution as follows.
Reference: 14. <author> D. Jensen. </author> <title> Labeling space: A tool for thinking about significance testing in knowledge discovery. Office of Technology Assessment, U.S. </title> <booktitle> Congress, </booktitle> <year> 1995. </year>
Reference-contexts: A good general reference for experimental design is Cochran and Cox [2], and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand [11]. Jensen <ref> [13, 14] </ref> discusses a framework for experimental comparison of classifiers and addresses significance testing, and Cohen and Jensen [3] discuss some specific ways to remove optimistic statistical bias from such experiments. One important innovation they discuss is randomization testing, in which one derives a reference distribution as follows.
Reference: 15. <author> D. Kibler and P. Langley. </author> <title> Machine learning as an experimental science. </title> <booktitle> In Proc. of 1988 Euro. Working Session on Learning, </booktitle> <pages> pages 8192, </pages> <year> 1988. </year>
Reference-contexts: Here I focus on design of experiments, which has been the subject of little concern in the machine learning community until recently (with some exceptions, such as <ref> [15] </ref> and [3]). Included in the comparative study category are papers that neither introduce a new algorithm nor improve an old one; instead, they consider one or more known algorithms and conduct experiments on known datasets. They may also include variations on known algorithms. <p> then significance levels (p-values) would have to be, e.g., 0.005 in order to obtain levels comparable to a single experiment using a level of 0.05. (This assumes, unrealistically, that the experiments are independent.) But few if any experimenters keep careful count of how many adjustments they consider. (Kibler and Langley <ref> [15] </ref> and Aha [1] suggest, as an alternative, that the parameter settings themselves be studied as independent variables, and that their effects be measured on artificial data.
Reference: 16. <author> R. Kohavi and D. Sommerfield. </author> <title> Oblivious decision trees, graphs, and top-down pruning. </title> <booktitle> In Proc. 14th Intl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 10711077, </pages> <address> Montreal, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The views expressed here are the author's own, and do not necessarily represent the views of those acknowledged above, the National Science Foundation, or the National Institutes of Health. Notes 1. This study was never published; a similar study is <ref> [16] </ref>, which used 22 data sets and 4 algorithms. 2. A p-value is simply the probability that a result occurred by chance.
Reference: 17. <author> P. M. Murphy. </author> <title> UCI repository of machine learning databases a machine-readable data repository. </title> <institution> Maintained at the Department of Information and Computer Science, University of California, Irvine. </institution> <note> Anonymous FTP from ics.uci.edu in the directory pub/machine-learning-databases, </note> <year> 1995. </year>
Reference-contexts: This is a 318 STEVEN L. SALZBERG healthy development and it represents an important step in the maturation of the field. One indication of this maturation is the creation and maintenance of the UC Irvine repository of machine learning databases <ref> [17] </ref>, which now contains over 100 datasets that have appeared in published work. This repository makes it very easy for machine learning researchers to compare new algorithms to previous work.
Reference: 18. <author> L. Prechelt. </author> <title> A quantitative study of experimental evaluations of neural network algorithms: </title> <booktitle> Current research practice. Neural Networks, </booktitle> <volume> 9, </volume> <year> 1996. </year>
Reference-contexts: Classification research, which is a component of data mining as well as a subfield of machine learning, has always had a need for very specific, focused studies that compare algorithms carefully. The evidence to date is that good evaluations are not done nearly enoughfor example, Prechelt <ref> [18] </ref> recently surveyed nearly 200 experimental papers on neural network learning algorithms and found most of them to have serious experimental deficiencies.
Reference: 19. <author> N. Qian and T. Sejnowski. </author> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> Journal of Molecular Biology, </journal> <volume> 202:65884, </volume> <year> 1988. </year>
Reference-contexts: The NetTalk dataset of English pronunciation data (introduced by Sejnowski and Rosenberg, [21] has been used in numerous experiments, as has the protein secondary structure data (introduced by Qian and Sejnowski <ref> [19] </ref>), to cite just two examples. Holte [12] collected results on 16 different datasets, and found as many as 75 different published accuracy figures for some of them.
Reference: 20. <author> A. Raftery. </author> <title> Bayesian model selection in social research (with discussion by Andrew Gelman, </title> <editor> Donald B. Rubin, and Robert M. Hauser). In Peter Marsden, editor, </editor> <booktitle> Sociological Methodology 1995, </booktitle> <pages> pages 111196. </pages> <address> Blackwells, Oxford, UK, </address> <year> 1995. </year>
Reference-contexts: For example, the whole framework of using alpha levels and p-values has been questioned when more than two hypotheses are under consideration <ref> [20] </ref>. 3.1. Alternative statistical tests One obvious problem with the experimental design cited above is that it only considers overall accuracy on a test set.
Reference: 21. <author> T. Sejnowski and C. Rosenberg. </author> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1:145168, </volume> <year> 1987. </year>
Reference-contexts: For example, Fisher's iris data has been around for 60 years and has been used in hundreds (maybe thousands) of studies. The NetTalk dataset of English pronunciation data (introduced by Sejnowski and Rosenberg, <ref> [21] </ref> has been used in numerous experiments, as has the protein secondary structure data (introduced by Qian and Sejnowski [19]), to cite just two examples. Holte [12] collected results on 16 different datasets, and found as many as 75 different published accuracy figures for some of them.
Reference: 22. <author> J. Shavlik, R. Mooney, and G. Towell. </author> <title> Symbolic and neural learning algorithms: An experimental compar ison. </title> <booktitle> Machine Learning, </booktitle> <address> 6:111143, </address> <year> 1991. </year>
Reference-contexts: UCI repository is a very limited sample of problems, many of which are quite easy for a classifier. (Many of them may represent the same concept class, for example many might be almost linearly separable, as suggested by the strong performance of the perceptron algorithm in one well-known comparative study <ref> [22] </ref>.) Thus the evidence is strong that results on the UCI datasets do not apply to all classification problems, and the repository is not an unbiased sample of classification problems. This is not by any means to say that the UCI repository should not exist.
Reference: 23. <author> D. Wettschereck and T. Dietterich. </author> <title> An experimental comparison of the nearest-neighbor and nearest hyperrectangle algorithms. </title> <booktitle> Machine Learning, </booktitle> <address> 19(1):528, </address> <year> 1995. </year>
Reference-contexts: The use of the wrong p-value makes it even more likely that some experiments will find significance where none exists. Nonetheless, many researchers proceed with using a simple t-test to compare multiple algorithms on multiple datasets from the UCI repository; see, e.g., Wettschereck and Dietterich <ref> [23] </ref>. Although easy to conduct, the t-test is simply the wrong test for such an experimental design. The t-test assumes that the test sets for each treatment (each algorithm) are independent.
Reference: 24. <author> D. Wolpert. </author> <title> On the connection between in-sample testing and generalization error. </title> <journal> Complex Systems, </journal> <volume> 6:4794, </volume> <year> 1992. </year>
Reference-contexts: The same observation can be made for data mining: no single technique is likely to work best on all databases. Recent theoretical work has shown that, with certain assumptions, no classifier is always better than another one <ref> [24] </ref>. However, experimental science is concerned with data that occurs in the real world, and it is not clear that these theoretical limitations are relevant. Comparative studies typically include at least one new algorithm and several known methods; these studies must be very careful about their methods and their claims.
References-found: 24

