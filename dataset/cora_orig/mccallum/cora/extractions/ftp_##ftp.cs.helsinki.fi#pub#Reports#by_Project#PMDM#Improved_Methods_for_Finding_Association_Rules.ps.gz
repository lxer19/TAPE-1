URL: ftp://ftp.cs.helsinki.fi/pub/Reports/by_Project/PMDM/Improved_Methods_for_Finding_Association_Rules.ps.gz
Refering-URL: http://www.cs.bham.ac.uk/~anp/bibtex/kdd.bib.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Improved Methods for Finding Association Rules  
Author: Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo P. O. Box (Teollisuuskatu ) 
Address: Helsinki, Finland  
Date: Helsinki, December 1993 (Revised February 1994)  
Note: Series of Publications C, No. C-1993-65  The papers in the series are intended for internal use and are distributed by the author. Copies may be ordered from the library  
Affiliation: University of Helsinki Department of Computer Science  Department of Computer Science  University of  of Department of Computer Science.  
Pubnum: FIN-00014  
Abstract-found: 0
Intro-found: 1
Reference: [AIS93] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proceedings of the 1993 International Conference on Management of Data (SIGMOD 93), </booktitle> <pages> pages 207 - 216, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The area can be loosely defined as finding interesting rules or exceptions from large collections of data. Recently, Agrawal, Imielinski, and Swami introduced a class of regularities, association rules, and gave an algorithm for finding such rules from a database with binary data <ref> [AIS93] </ref>. An association rule is an expression W ) B, where W is a set of attributes and B a single attribute. <p> Applications of association rules include customer behavior analysis for example in a supermarket or banking environment, and telecommunications alarm diagnosis and prediction. In this paper we study the properties of association rule discovery in relations. We give a new algorithm for the problem that outperforms the method in <ref> [AIS93] </ref> by a factor of 5. The algorithm is based on the same basic idea of repeated passes over the database as the method in [AIS93]. <p> We give a new algorithm for the problem that outperforms the method in <ref> [AIS93] </ref> by a factor of 5. The algorithm is based on the same basic idea of repeated passes over the database as the method in [AIS93]. The difference is that our algorithm makes careful use of the combinatorial information obtained from previous passes and in this way avoids considering many unnecessary sets in the process of finding the association rules. <p> The rest of this paper is organized as follows. Section 2 introduces the 1 problem and the notations. Section 3 describes our algorithm for finding association rules. The analysis of sampling is given in Section 4. Empirical results and a comparison to the results of <ref> [AIS93] </ref> are given in Section 5. Section 6 shortly discusses association rules in sequential data. Related work is presented in Section 7. Section 8 is a short conclusion. Appendix A contains the probabilistic analyses of random relations and the lower bound result. <p> Related work is presented in Section 7. Section 8 is a short conclusion. Appendix A contains the probabilistic analyses of random relations and the lower bound result. Appendix B gives an overview of the implementation. 2 Problem First we introduce some basic concepts, using the formalism presented in <ref> [AIS93] </ref>. Let R = fI 1 ; I 2 ; : : : ; I m g be a set of attributes, also called items, over the binary domain f0; 1g. <p> covering 1 (with respect to the database and the given support threshold ), if jfi j t i [X] = 1gj n: That is, at least a fraction of the rows in the relation have 1's in all the attributes of X. 1 Agrawal et al. use the term large <ref> [AIS93] </ref>. 2 As an example, suppose support threshold = 0:3 and confidence thresh-old fl = 0:9, and consider the example database ABCD; ABEF G; ABHIJ; BK: Now, three of four rows contain the set fABg, so the support is jfi j t i [AB] = 1gj = 0:75; supports of A, <p> In the first case the rule W A ) B does not necessarily have sufficient support, and in the second case the rule W ) B does not necessarily hold with sufficient confidence. 3 Finding association rules Basic algorithm The approach in <ref> [AIS93] </ref> to finding association rules is to first find all covering attribute sets X, and then separately test whether the rule X n fBg ) B holds with sufficient confidence. 2 We follow this approach and concentrate on the algorithms that search for covering subsets. <p> The extreme method would be to do just one pass and check for each of the 2 m subsets of R whether they are covering or not. This is infeasible for all but the smallest values of m. The method of <ref> [AIS93] </ref> makes multiple passes over the database. During a database pass, new candidates for covering sets are generated, and support information is collected to evaluate which of the candidates actually are covering. The candidates are derived from the database tuples by extending previously found covering sets in the frontier. <p> The expected support required for this decision is derived from the frequency information of the items of the set. Originally the frontier only contains the empty set. An essential property of the method of <ref> [AIS93] </ref> is that both candidate generation and evaluation is performed during the database pass. The method of [AIS93] further uses two techniques to prune the candidate space during the database pass. These are briefly described in Appendix B. We take a slightly different approach. <p> Originally the frontier only contains the empty set. An essential property of the method of <ref> [AIS93] </ref> is that both candidate generation and evaluation is performed during the database pass. The method of [AIS93] further uses two techniques to prune the candidate space during the database pass. These are briefly described in Appendix B. We take a slightly different approach. <p> We call this latter method the e-extension method. This system is especially advantageous if the number of sets is so small that the coverage of all the sets in these families can be checked in a single pass, so that the idea of pass minimization from <ref> [AIS93] </ref> is used. For example, if we know immediately that L 5 = ;, since 2 = 10 &gt; jL 2 j: The sets that can possibly be in L 3 are ABC; BCD; ACD; ABD; and the only possible member of L 4 is ABCD. <p> close to the number of sets whose coverage can be checked in a single pass. (Again, note that these computations are relatively cheap, as they do not involve the database.) An essential improvement is that candidates are only generated once and between the database passes, unlike in the algorithm of <ref> [AIS93] </ref>. The computational complexity of the algorithms can be analyzed in terms of the quantities jL s j, jC s j, jC 0 s j, and the size n of the database. <p> Appendix A contains an analysis of covering sets in random relations and a lower bound result for the problem of finding association rules. 5 Experiments To evaluate the efficiency of our methods, we compare the original algorithm in <ref> [AIS93] </ref> to our algorithm. Candidate generation is performed by extending sets in L s with other sets in L s to achieve (at most) e-extensions. <p> more aggressive strategy with e = s, where the size of the candidate sets is doubled during each iteration step. (We refer to our algorithm as off-line candidate determination; the variants are noted in the following as OCD 1 and OCD s .) In addition to the original algorithm of <ref> [AIS93] </ref> (noted in the following by AIS orig ), we also implemented a minor modification of it that refrains from extending any set with an item that is not a covering set by itself (noted in the following by AIS mod ). <p> In comparison to the unincremental method, the gain of this method grows linearly with the window size. In practical experiments, the efficiency has been 25 to 100 fold. 7 Related work The basic inspiration for this paper is naturally <ref> [AIS93] </ref>. In this section we discuss briefly how finding association rules differs from traditional machine learning. The discovery of rules from empirical data is, of course, a major topic in artificial intelligence and machine learning [Mug90, KM90]. <p> The PAC-model of Valiant [Val84, Ang92] is a robust setting for the study of theoretical properties of machine learning tasks. It is an interesting open question whether the task of finding association rules can be formulated within this framework. We refer to <ref> [AIS93] </ref> for additional references. 8 Concluding remarks Association rules are a simple and natural class of database regularities, useful e.g. in various analysis or prediction tasks. We have considered the problem of finding the association rules that hold in a given relation. Following the work of [AIS93], we have given an <p> We refer to <ref> [AIS93] </ref> for additional references. 8 Concluding remarks Association rules are a simple and natural class of database regularities, useful e.g. in various analysis or prediction tasks. We have considered the problem of finding the association rules that hold in a given relation. Following the work of [AIS93], we have given an algorithm that uses all existing information between database passes to avoid checking the coverage of redundant sets. The algorithm gives clear empirical improvement when compared against the previous results, and it is simple to implement. <p> In Appendix A we give some additional theoretical results. We also give a simple lower bound for a special case of the problem, and note that an algorithm from the different framework of [Lov87] actually matches this bound. Several problems remain open. Some of the pruning ideas in <ref> [AIS93] </ref> are probably quite useful in certain situations; recognizing when to use such methods would help in practice. An algorithmic problem is how to find out as efficiently as possible what candidate sets occur in a given database row.
Reference: [Ang92] <author> D. Angluin. </author> <title> Computational learning theory: survey and selected bibliography. </title> <booktitle> In Proc. 24th Annu. ACM Sympos. Theory Com-put., </booktitle> <pages> pages 351-369. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Second, the examples we have are all positive; no negative information is given. In the area of data mining [FPSM91, IJI92, PS93] the algorithms [PS91] have some similarities to the methods above. The PAC-model of Valiant <ref> [Val84, Ang92] </ref> is a robust setting for the study of theoretical properties of machine learning tasks. It is an interesting open question whether the task of finding association rules can be formulated within this framework.
Reference: [AS92] <author> Noga Alon and Joel H. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley Inc., </publisher> <address> New York, </address> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Then the number of rows in the sample that contain X is a random variable x distributed according to B (h; t ), i.e., binomial distribution of h trials, each having success probability t . The Chernoff bounds <ref> [AS92, HR90] </ref> state that for all a we have P r [x &gt; ht + a] &lt; e 2a 2 =h : That is, the probability that the estimated support is off by at least ff is P r [x &gt; h (t + ff)] &lt; e 2ff 2 h 2
Reference: [Bol86] <author> Bela Bollobas. </author> <title> Combinatorics. </title> <publisher> Cambridge University Press, </publisher> <address> Cam--bridge, </address> <year> 1986. </year>
Reference-contexts: checking the inclusion condition. (Note that the work done in generating C s+1 does not depend on the size of the database, but only on the size of the collection L s .) 3 Results on the possible relative sizes of L s and C s+1 can be found in <ref> [Bol86] </ref>. 5 Refinements Instead of computing C s+1 from L s , one can compute several families C s+1 ; : : : ; C s+e for some e &gt; 1 directly from L s . That is, we first compute C s+1 as above.
Reference: [ES90] <author> Margaret Ellis and Bjarne Stroustrup. </author> <title> The Annotated C++ Reference Manual. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference: [FPSM91] <author> William J. Frawley, Gregory Piatetsky-Shapiro, and Christo-pher J. Matheus. </author> <title> Knowledge discovery in databases: An overview. </title> <editor> In Gregory Piatetsky-Shapiro and William J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pages 1 - 27. </pages> <publisher> AAAI Press / The MIT Press, </publisher> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: Second, the examples we have are all positive; no negative information is given. In the area of data mining <ref> [FPSM91, IJI92, PS93] </ref> the algorithms [PS91] have some similarities to the methods above. The PAC-model of Valiant [Val84, Ang92] is a robust setting for the study of theoretical properties of machine learning tasks.
Reference: [HR90] <author> Torben Hagerup and Christine Rub. </author> <title> A guided tour of Chernoff bounds. </title> <journal> Information Processing Letters, </journal> <volume> 33 </volume> <pages> 305-308, </pages> <year> 1989/90. </year>
Reference-contexts: Then the number of rows in the sample that contain X is a random variable x distributed according to B (h; t ), i.e., binomial distribution of h trials, each having success probability t . The Chernoff bounds <ref> [AS92, HR90] </ref> state that for all a we have P r [x &gt; ht + a] &lt; e 2a 2 =h : That is, the probability that the estimated support is off by at least ff is P r [x &gt; h (t + ff)] &lt; e 2ff 2 h 2
Reference: [IJI92] <institution> International Journal of Intelligent Systems, </institution> <month> 7(7) </month> <pages> 587-704, </pages> <month> September </month> <year> 1992. </year> <editor> Gregory Piatetsky-Shapiro, editor. </editor> <title> Special Issue: Knowledge Discovery in Data- and Knowledge Bases. </title>
Reference-contexts: Second, the examples we have are all positive; no negative information is given. In the area of data mining <ref> [FPSM91, IJI92, PS93] </ref> the algorithms [PS91] have some similarities to the methods above. The PAC-model of Valiant [Val84, Ang92] is a robust setting for the study of theoretical properties of machine learning tasks.
Reference: [KM90] <editor> Y. Kodratoff and R. S. Michalski, editors. </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, volume III. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, California, </address> <year> 1990. </year>
Reference-contexts: In this section we discuss briefly how finding association rules differs from traditional machine learning. The discovery of rules from empirical data is, of course, a major topic in artificial intelligence and machine learning <ref> [Mug90, KM90] </ref>. The problem of finding association rules is, however, quite different from most machine learning tasks. First, we are not interested in obtaining a complete description of the data. Rather, we are content with some information that describes some (interesting) parts of the data.
Reference: [Lov87] <author> Donald W. Loveland. </author> <title> Finding critical sets. </title> <journal> Journal of Algorithms, </journal> <volume> 8:362 - 371, </volume> <year> 1987. </year>
Reference-contexts: We also give a simple information-theoretic lower bound for finding one rule, and show that an algorithm suggested by Loveland in <ref> [Lov87] </ref> in a different framework actually meets this lower bound. The rest of this paper is organized as follows. Section 2 introduces the 1 problem and the notations. Section 3 describes our algorithm for finding association rules. The analysis of sampling is given in Section 4. <p> In Appendix A we give some additional theoretical results. We also give a simple lower bound for a special case of the problem, and note that an algorithm from the different framework of <ref> [Lov87] </ref> actually matches this bound. Several problems remain open. Some of the pruning ideas in [AIS93] are probably quite useful in certain situations; recognizing when to use such methods would help in practice.
Reference: [Meh84] <author> Kurt Mehlhorn. </author> <title> Data Structures and Algorithms, Volumes 1-3. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference: [Mug90] <author> Stephen Muggleton. </author> <title> Inductive Acquisition of Expert Knowledge. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: In this section we discuss briefly how finding association rules differs from traditional machine learning. The discovery of rules from empirical data is, of course, a major topic in artificial intelligence and machine learning <ref> [Mug90, KM90] </ref>. The problem of finding association rules is, however, quite different from most machine learning tasks. First, we are not interested in obtaining a complete description of the data. Rather, we are content with some information that describes some (interesting) parts of the data.
Reference: [Nah92] <author> Stefan Naher. </author> <title> LEDA user manual, version 3.0. </title> <type> Technical report, </type> <institution> Max-Planck-Institut fur Informatik, Im Stadtwald, D-6600 Saar-brucken, </institution> <year> 1992. </year>
Reference: [PS91] <author> Gregory Piatetsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. </title> <editor> In Gregory Piatetsky-Shapiro and William J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <year> 1991. </year>
Reference-contexts: Second, the examples we have are all positive; no negative information is given. In the area of data mining [FPSM91, IJI92, PS93] the algorithms <ref> [PS91] </ref> have some similarities to the methods above. The PAC-model of Valiant [Val84, Ang92] is a robust setting for the study of theoretical properties of machine learning tasks. It is an interesting open question whether the task of finding association rules can be formulated within this framework.
Reference: [PS93] <editor> Gregory Piatetsky-Shapiro, editor. </editor> <booktitle> Knowledge Discovery in Databases, AAAI-93 workshop working notes distributed to attendees, </booktitle> <address> Washington, D.C., </address> <month> July 11-15 </month> <year> 1993. </year> <month> 17 </month>
Reference-contexts: Second, the examples we have are all positive; no negative information is given. In the area of data mining <ref> [FPSM91, IJI92, PS93] </ref> the algorithms [PS91] have some similarities to the methods above. The PAC-model of Valiant [Val84, Ang92] is a robust setting for the study of theoretical properties of machine learning tasks.
Reference: [PSF91] <editor> Gregory Piatetsky-Shapiro and William J. Frawley, editors. </editor> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI Press / The MIT Press, </publisher> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Data mining (database mining, knowledge discovery in databases) has recently been recognized as a promising new field in the intersection of databases, artificial intelligence, and machine learning (see, e.g., <ref> [PSF91] </ref>). The area can be loosely defined as finding interesting rules or exceptions from large collections of data. Recently, Agrawal, Imielinski, and Swami introduced a class of regularities, association rules, and gave an algorithm for finding such rules from a database with binary data [AIS93].
Reference: [Qui86] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: First, we are not interested in obtaining a complete description of the data. Rather, we are content with some information that describes some (interesting) parts of the data. This can be compared to, e.g., the task of inferring decision trees or other classifiers <ref> [Qui86] </ref>, where even in the presence of noise the task is still to obtain a reasonably small description of the whole data. Second, the examples we have are all positive; no negative information is given.
Reference: [Val84] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Second, the examples we have are all positive; no negative information is given. In the area of data mining [FPSM91, IJI92, PS93] the algorithms [PS91] have some similarities to the methods above. The PAC-model of Valiant <ref> [Val84, Ang92] </ref> is a robust setting for the study of theoretical properties of machine learning tasks. It is an interesting open question whether the task of finding association rules can be formulated within this framework.
References-found: 18

