URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/94.toplas.Control_Abstraction.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/leblanc/pubs.html
Root-URL: 
Title: Parallel Programming with Control Abstraction  
Author: Lawrence A. Crowl Thomas J. LeBlanc 
Keyword: General terms: Design, Languages Additional Key Words and Phrases: parallel programming languages, control abstraction, data abstraction, closures, early reply, multiprocessors, architectural adaptability, performance tuning.  
Address: Rochester  
Affiliation: Oregon State University and  University of  
Abstract: Parallel programming involves finding the potential parallelism in an application and mapping it to the architecture at hand. Since a typical application has more potential parallelism than any single architecture can exploit effectively, programmers usually limit their focus to the parallelism that the available control constructs express easily and that the given architecture exploits efficiently. This approach produces programs that exhibit much less parallelism than exists in the application, and whose performance depends critically on the underlying hardware and software. We argue for an alternative approach based on control abstraction. Control abstraction is the process by which programmers define new control constructs, specifying constraints on statement ordering separately from an implementation of that ordering. With control abstraction programmers can define and use a rich variety of control constructs to represent an algorithm's potential parallelism. Since control abstraction separates the definition of a construct from its implementation, a construct may have several different implementations, each exploiting a different subset of the parallelism admitted by the construct. By selecting an implementation for each control construct using annotations, a programmer can vary the parallelism in a program to best exploit the underlying hardware without otherwise changing the source code. This approach produces programs that exhibit most of the potential parallelism in an algorithm, and whose performance can be tuned simply by choosing among the various implementations for the control constructs in use. We use several example applications to illustrate the use of control abstraction in parallel programming and performance tuning, and describe our implementation of a prototype programming language based on these ideas on the BBN Butterfly. Categories and Subject Descriptors: D.1.3 [Programming Techniques]: Concurrent Programming|parallel programming; D.2.m [Software Engineering]: Miscellaneous|reusable software; D.3.2 [Programming Languages]: Language Classifications|concurrent, distributed, and parallel languages, Matroshka, Natasha; D.3.3 [Programming Languages]: Language Constructs and Features|abstract data types, concurrent programming structures, control structures, procedures, functions, and subroutines; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs|control primitives An early version of some of this material appeared in the Proceedings of the 1992 International Conference on Computer Languages (Crowl and LeBlanc 1992). Authors' addresses: Lawrence A. Crowl, Computer Science Department, Oregon State University, Corvallis, Oregon, 97331-3202, crowl@cs.orst.edu; Thomas J. LeBlanc, Computer Science Department, University of Rochester, Rochester, New York, 14627-0226, leblanc@cs.rochester.edu. This work was supported by the National Science Foundation under research grant CDA-8822724, and the Office of Naval Research under research contract N00014-92-J-1801 (jointly funded by ARPA, HPCC Software Science and Technology program, ARPA Order No. 8930). The Government has certain rights in this material. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Albert, Eugene, Kathleen Knobe, Joan D. Lukas, and Guy L. Steele, J. </author> <year> 1988 </year> <month> (July). </month> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Proceedings of the ACM/SIGPLAN PPEALS 1988, </booktitle> <pages> pages 42-56. </pages>
Reference-contexts: Data-parallel languages provide high-level data structures and data operations that allow programmers to operate on large amounts of data in an SIMD fashion. The compilers for these languages generate parallel or sequential code, as appropriate for the target machine. APL (Budd 1984), Fortran 8x <ref> (Albert et al. 1988) </ref>, and its descendent Fortran 90 (ANSI 1990, Metcalf and Reid 1990) provide operators that act over entire arrays, which can have parallel implementations. The Seymor language (Miller and Stout 1989) provides prefix, broadcast, sort, and divide-and-conquer operations, which also have parallel implementations.
Reference: <author> Alverson, Gail A. and David Notkin. </author> <year> 1992. </year> <title> Abstracting data-representation and partitioning-scheduling in parallel programs. In Suzuki, </title> <editor> N., editor, </editor> <booktitle> Shared Memory Multiprocessing, </booktitle> <pages> pages 315-338. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Alverson, Gail A. </author> <year> 1990 </year> <month> (October). </month> <title> Abstraction for effectively portable shared memory parallel programs. </title> <type> Technical Report 90-10-09, </type> <institution> Department of Computer Science and Engineering, University of Washington. </institution> <type> Ph.D. Dissertation. </type>
Reference-contexts: Thus, generators are a form of control abstraction. The Uniform System provides built-in generators for manipulating arrays and matrices, but allows customized generators to be implemented by calling ActivateGen directly. Chameleon <ref> (Alverson 1990, Alverson and Notkin 1992) </ref> extends this form of control abstraction by separating the partitioning and scheduling policy from task generation.
Reference: <author> Andrews, Gregory R., Ronald A. Olsson, Michael H. Coffin, Irving J. P. Elshoff, Kelvin Nilsen, Titus Purdin, and G. Townsend. </author> <year> 1988 </year> <month> (January). </month> <title> An overview of the SR language and implementation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 51-86. </pages> <institution> American National Standards Institute. </institution> <year> 1990 </year> <month> (June). </month> <title> American National Standard Programming Language: Fortran 90, </title> <publisher> X3J3/s8.115. </publisher>
Reference-contexts: Compilers on different architectures may make different choices, thus providing a limited degree of architectural independence. The Par language (Coffin and Andrews 1989, Coffin 1990, Coffin 1992) (based on SR <ref> (Andrews et al. 1988) </ref>) extends the concept of multiple implementations for a construct to user-defined implementations. Par's primary parallel control construct is the co statement, which is a combination of cobegin and parallel for loops. <p> This mechanism, called early reply, is the sole source of parallelism in Matroshka. 3 This mechanism is not new (for example, see <ref> (Andrews et al. 1988, Liskov, Herlihy, and Gilbert 1986, Scott 1987) </ref>), but its expressive power does not appear to be widely recognized. We require that in any implementation of early reply, both processes (that is, the return to the caller and the continuation of the invocation) make finite progress.
Reference: <author> Black, Andrew P., Norman Hutchinson, Eric Jul, Henry Levy, and Larry Carter. </author> <year> 1987 </year> <month> (January). </month> <title> Distribution and abstract types in Emerald. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 13(1) </volume> <pages> 65-76. </pages>
Reference-contexts: Our approach is compatible with techniques developed by others to address mapping (Hudak 1986, Snyder 1984), distribution (Coffin and Andrews 1989, Coffin 1992, Alverson and Notkin 1992), and communication <ref> (Black et al. 1987) </ref>. 1.1.1 Parallel Function Evaluation. Functional programs have no side effects, so expressions may be evaluated in any order. As a consequence, parallelism in functional programs is implicit, in that expressions can be evaluated in parallel.
Reference: <author> Budd, Timothy A. </author> <year> 1984 </year> <month> (July). </month> <title> An APL compiler for a vector processor. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 297-313. </pages>
Reference-contexts: Data-parallel languages provide high-level data structures and data operations that allow programmers to operate on large amounts of data in an SIMD fashion. The compilers for these languages generate parallel or sequential code, as appropriate for the target machine. APL <ref> (Budd 1984) </ref>, Fortran 8x (Albert et al. 1988), and its descendent Fortran 90 (ANSI 1990, Metcalf and Reid 1990) provide operators that act over entire arrays, which can have parallel implementations. The Seymor language (Miller and Stout 1989) provides prefix, broadcast, sort, and divide-and-conquer operations, which also have parallel implementations.
Reference: <author> Burton, F. Warren. </author> <year> 1984 </year> <month> (April). </month> <title> Annotations to control parallelism and reduction order in the distributed evaluation of functional programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(2) </volume> <pages> 159-174. </pages>
Reference-contexts: Owing to the difficulty of automatically finding and exploiting the optimal sources of parallelism in a functional program, several researchers have suggested the use of annotations to specify lazy, eager, parallel, and distributed function evaluation <ref> (Burton 1984, Halstead 1985, Hudak 1986) </ref>. 6 Crowl and LeBlanc ParAlfl (Hudak 1986, Hudak 1988) is a functional language that provides annotations to select eager evaluation over lazy evaluation, resulting in parallel execution, and to map expression evaluation to processors.
Reference: <author> Coffin, Michael H. and Gregory R. Andrews. </author> <year> 1989 </year> <month> (September). </month> <title> Towards architecture-independent parallel programming. </title> <type> Technical Report 89-21a, </type> <institution> Department of Computer Science, University of Arizona. </institution>
Reference-contexts: Our approach is compatible with techniques developed by others to address mapping (Hudak 1986, Snyder 1984), distribution <ref> (Coffin and Andrews 1989, Coffin 1992, Alverson and Notkin 1992) </ref>, and communication (Black et al. 1987). 1.1.1 Parallel Function Evaluation. Functional programs have no side effects, so expressions may be evaluated in any order. <p> Programmers use do across to specify potential parallelism, and the compiler can choose either a sequential or parallel implementation as appropriate. Compilers on different architectures may make different choices, thus providing a limited degree of architectural independence. The Par language <ref> (Coffin and Andrews 1989, Coffin 1990, Coffin 1992) </ref> (based on SR (Andrews et al. 1988)) extends the concept of multiple implementations for a construct to user-defined implementations. Par's primary parallel control construct is the co statement, which is a combination of cobegin and parallel for loops.
Reference: <author> Coffin, Michael H. </author> <year> 1990 </year> <month> (August). </month> <title> Par: An approach to architecture-independent parallel programming. </title> <type> PhD thesis, </type> <institution> University of Arizona. </institution> . <year> 1992. </year> <title> Parallel Programming: A New Approach. Summit, New Jersey: Silicon Press. Parallel Programming with Control Abstraction 51 Crovella, </title> <editor> Mark and Thomas LeBlanc. </editor> <year> 1993 </year> <month> (May). </month> <title> Performance debugging using parallel performance predicates. </title> <booktitle> In Proceedings of the 3rd ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 140-150. </pages>
Reference: <author> Crowl, Lawrence A. and Thomas J. LeBlanc. </author> <year> 1992 </year> <month> (April). </month> <booktitle> Control abstraction in parallel programming languages. In Proceedings of the 1992 International Conference on Computer Languages, </booktitle> <pages> pages 44-53. </pages>
Reference: <author> Crowl, Lawrence, Mark Crovella, Thomas LeBlanc, and Michael Scott. </author> <year> 1993 </year> <month> (April). </month> <title> Beyond data parallelism: The advantages of multiple parallelizations in combinatorial search. </title> <type> Technical Report 451, </type> <institution> Computer Science Department, University of Rochester. </institution>
Reference-contexts: Searching for 128 solutions. There are a variety of reasons why each machine performs best using a particular parallelization under particular circumstances, and a complete discussion of these results is beyond the scope of this paper. (See <ref> (Crowl et al. 1993) </ref> for a complete analysis of subgraph isomorphism and an explanation for each of these performance results, and (Crovella and LeBlanc 1993) for a description of the tool we developed to aid in this analysis.) Here we note only that a primary source of overhead under graph parallelization
Reference: <author> Crowl, Lawrence A. </author> <year> 1991 </year> <month> (May). </month> <title> Architectural adaptability in parallel programming. </title> <type> Technical Report 381, </type> <institution> Computer Science Department, University of Rochester. </institution> <type> Ph.D. Dissertation. </type>
Reference-contexts: Operation invocation is synchronous with respect to the caller; the caller waits for the operation to return a result before proceeding. 1 A complete description of the Matroshka programming model and the Natasha programming language is beyond the scope of this paper; see <ref> (Crowl 1991) </ref> for additional details. 10 Crowl and LeBlanc As in nearly all imperative programming languages, we require that all arguments be evaluated in sequence before invoking the operation. This requirement results in sequential evaluation of expressions, without limiting the potential for parallelism in control flow. <p> Programmers use parallelism to improve performance however, and if control abstraction is to be used frequently, it must be cheap. We now consider the performance implications of control abstraction using our prototype implementation of the Natasha programming language <ref> (Crowl 1991) </ref>. Natasha uses the primitive mechanisms for control abstraction described in section 2. The Natasha compiler uses the C language as an intermediate form, and relies on GNU's gcc compiler to generate machine code.
Reference: <author> Goldberg, Adele and David Robson. </author> <year> 1983. </year> <title> Smalltalk-80, The Language and Its Implementation. </title> <address> Reading, Massachusetts: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Busy-waiting synchronization may prevent a process from terminating, and therefore cannot be used in tandem with a non-preemptive scheduler, unless knowledge of the scheduling policy can be used to ensure that both processes make finite progress. 2.1.5 Conditional Execution. For conditional execution we adopt the approach of Smalltalk <ref> (Goldberg and Robson 1983) </ref> and depend on a Boolean type and built-in if operation that conditionally executes a closure.
Reference: <author> Goldman, Ron, Richard P. Gabriel, and Carol Sexton. </author> <year> 1990. </year> <title> Qlisp: An interim report. </title> <editor> In Ito, Takayasu and Robert H. Halstead, J, editors, </editor> <booktitle> Parallel Lisp: Languages and Systems, number 441 in Springer-Verlag Lecture Notes in Computer Science, </booktitle> <pages> pages 161-181. </pages>
Reference-contexts: Although pure Lisp is functional, most Lisp-based programming languages are imperative. Like ParAlfl, an imperative Lisp can exploit parallelism in function evaluation by selecting either lazy or eager (and potentially parallel) evaluation. For example, Multilisp (Halstead 1985) (and Qlisp <ref> (Goldman, Gabriel, and Sexton 1990) </ref>) provides the function pcall for parallel argument evaluation, and future for parallel expression evaluation. Unlike ParAlfl, Multilisp is an imperative language with assignment.
Reference: <author> Halstead, Robert H., J. </author> <year> 1985 </year> <month> (October). </month> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538. </pages> . <year> 1990. </year> <title> New ideas in parallel Lisp: Language design, implementation, and programming tools. </title> <editor> In Ito, Takayasu and Robert H. Halstead, J, editors, </editor> <booktitle> Parallel Lisp: Languages and Systems, number 441 in Springer-Verlag Lecture Notes in Computer Science, </booktitle> <pages> pages 2-57. </pages>
Reference-contexts: Although pure Lisp is functional, most Lisp-based programming languages are imperative. Like ParAlfl, an imperative Lisp can exploit parallelism in function evaluation by selecting either lazy or eager (and potentially parallel) evaluation. For example, Multilisp <ref> (Halstead 1985) </ref> (and Qlisp (Goldman, Gabriel, and Sexton 1990)) provides the function pcall for parallel argument evaluation, and future for parallel expression evaluation. Unlike ParAlfl, Multilisp is an imperative language with assignment. <p> All of our implementations of the distance filter are based on a set abstract data type, which must export the remove_element operation. As in Multilisp <ref> (Halstead 1985) </ref>, we assume that data synchronization is embedded in data abstractions. Thus, the remove_element implementation must provide any synchronization needed to manage multiple invocations of remove_element.
Reference: <author> Hewitt, Carl E. and Russel R. Atkinson. </author> <year> 1979 </year> <month> (January). </month> <title> Specification and proof techniques for serializers. </title> <journal> IEEE Transactions on Software Engineering, SE-5(1):10-23. </journal>
Reference-contexts: We use the precedes relation to describe constraints on the order of execution imposed by a control construct. Our definition of precedes is similar to Lamport's happened before relation (Lamport 1978) and Hewitt's and Atkinson's necessarily precedes relation <ref> (Hewitt and Atkinson 1979) </ref>, which are statements about causal ordering of execution events. Informally, we say that "a precedes b" (written a ! b) if event a must occur before event b. We determine whether one event must occur before another using the semantics of our primitive mechanisms.
Reference: <author> Hilfinger, Paul N. </author> <year> 1982. </year> <title> Abstraction Mechanisms And Language Design. </title> <publisher> ACM Distinguished Dissertation. MIT Press. </publisher>
Reference: <author> Hudak, Paul. </author> <year> 1986 </year> <month> (August). </month> <title> Para-functional programming. </title> <journal> Computer, </journal> <volume> 19(8) </volume> <pages> 60-70. </pages> . <year> 1988 </year> <month> (January). </month> <title> Exploring parafunctional programming: Separating the what from the how. </title> <journal> IEEE Software, </journal> 5(1):54-61. <volume> 52 </volume> Crowl and LeBlanc Kranz, David, Richard Kelsey, Jonathan Rees, Paul Hudak, James Philbin, and Norman Adams. 1986 (June). ORBIT: An optimizing compiler for Scheme. In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, pages <pages> 219-233. </pages>
Reference-contexts: Our goal is to expose all of the potentially useful sources of parallelism in the source code of a program, while allowing the user to select the parallelism to exploit in the implementation. Our approach is compatible with techniques developed by others to address mapping <ref> (Hudak 1986, Snyder 1984) </ref>, distribution (Coffin and Andrews 1989, Coffin 1992, Alverson and Notkin 1992), and communication (Black et al. 1987). 1.1.1 Parallel Function Evaluation. Functional programs have no side effects, so expressions may be evaluated in any order. <p> Owing to the difficulty of automatically finding and exploiting the optimal sources of parallelism in a functional program, several researchers have suggested the use of annotations to specify lazy, eager, parallel, and distributed function evaluation (Burton 1984, Halstead 1985, Hudak 1986). 6 Crowl and LeBlanc ParAlfl <ref> (Hudak 1986, Hudak 1988) </ref> is a functional language that provides annotations to select eager evaluation over lazy evaluation, resulting in parallel execution, and to map expression evaluation to processors. A mapped expression in ParAlfl can dynamically select the processor on which it executes.
Reference: <author> Lamport, Leslie. </author> <year> 1978 </year> <month> (July). </month> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565. </pages>
Reference-contexts: We use the precedes relation to describe constraints on the order of execution imposed by a control construct. Our definition of precedes is similar to Lamport's happened before relation <ref> (Lamport 1978) </ref> and Hewitt's and Atkinson's necessarily precedes relation (Hewitt and Atkinson 1979), which are statements about causal ordering of execution events. Informally, we say that "a precedes b" (written a ! b) if event a must occur before event b.
Reference: <author> Leasure, B. </author> <year> 1990 </year> <month> (August). </month> <title> PCF Fortran: Language Definition 3.1. Parallel Computing Forum, </title> <type> Champagne, </type> <institution> Illinois. </institution>
Reference-contexts: We can construct a sequential implementation of cobegin using statement sequencing: implement cobegin $SEQUENTIAL ( stmt1, stmt2: closure () ) - stmt1 (); stmt2 () - 6 This last constraint imposes an ordering on iterations analogous to the ORDERED qualifier for PARALLEL DO in PCF Fortran <ref> (Leasure 1990) </ref>. We exploit this property of forall in sections 3.2 and 3.3. 20 Crowl and LeBlanc Note that the precedence rules in the definition of cobegin require that stmt1 precede stmt2 in any sequential implementation.
Reference: <author> LeBlanc, Thomas J. </author> <year> 1988. </year> <title> Problem decomposition and communication tradeoffs in a shared-memory multiprocessor. </title> <editor> In Schultz, Martin, editor, </editor> <booktitle> Numerical Algorithms for Modern Parallel Computer Architectures, number 13 in IMA Volumes in Mathematics and its Applications, </booktitle> <pages> pages 145-163. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: queue is more than compensated by the efficiency of executing larger subtrees of tasks locally (Mohr 1991). 5.3 Performance Evaluation To evaluate the effectiveness of our parallel implementation of Natasha, we compared the performance of the Natasha program for Gaussian elimination against an existing hand-tuned parallel program written in C <ref> (LeBlanc 1988) </ref>. As seen in figure 7, the unoptimized Natasha program executes between three and four times slower than the hand-tuned C program.
Reference: <author> Liskov, Barbara H., Alan Snyder, R. R. Atkinson, and J. C. Schaffert. </author> <year> 1977 </year> <month> (August). </month> <title> Abstraction mechanisms in CLU. </title> <journal> Communications of the ACM, </journal> <volume> 20(8) </volume> <pages> 564-576. </pages>
Reference-contexts: This history does not mention control abstraction, although the mechanisms for control abstraction are present in Lisp. Control abstraction has been used in several sequential languages to support data abstraction. For example, CLU iterators <ref> (Liskov et al. 1977) </ref> (or generators) are a limited form of control abstraction that allows the user of an abstract type to operate on the elements of the type without knowing the underlying representation.
Reference: <author> Liskov, Barbara H., Maurice P. Herlihy, and Lucy Gilbert. </author> <year> 1986 </year> <month> (January). </month> <title> Limitations of synchronous communication with static process structure in languages for distributed computing. </title> <booktitle> In Conference Record of the Thirteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 150-159. </pages>
Reference: <author> Metcalf, Michael and John Reid. </author> <year> 1990. </year> <title> Fortran 90 Explained. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: The compilers for these languages generate parallel or sequential code, as appropriate for the target machine. APL (Budd 1984), Fortran 8x (Albert et al. 1988), and its descendent Fortran 90 <ref> (ANSI 1990, Metcalf and Reid 1990) </ref> provide operators that act over entire arrays, which can have parallel implementations. The Seymor language (Miller and Stout 1989) provides prefix, broadcast, sort, and divide-and-conquer operations, which also have parallel implementations. These languages restrict parallelism to a particular set of operations on data structures.
Reference: <author> Miller, Russ and Quentin F. Stout. </author> <year> 1989 </year> <month> (September). </month> <title> An introduction to the portable parallel programming language Seymor. </title> <booktitle> In Proceedings of the Thirteenth Annual International Computer Software and Applications Conference, </booktitle> <pages> pages 94-101. </pages> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: APL (Budd 1984), Fortran 8x (Albert et al. 1988), and its descendent Fortran 90 (ANSI 1990, Metcalf and Reid 1990) provide operators that act over entire arrays, which can have parallel implementations. The Seymor language <ref> (Miller and Stout 1989) </ref> provides prefix, broadcast, sort, and divide-and-conquer operations, which also have parallel implementations. These languages restrict parallelism to a particular set of operations on data structures.
Reference: <author> Mohr, Eric. </author> <year> 1991 </year> <month> (October). </month> <title> Dynamic partitioning of parallel Lisp programs. </title> <type> Technical Report YALEU/DCS/RR-869, </type> <institution> Department of Computer Science, Yale University. </institution> <type> (Ph.D. dissertation). </type>
Reference-contexts: Mohr has shown that the increased cost of queuing operations that results from removing tasks from both ends of the queue is more than compensated by the efficiency of executing larger subtrees of tasks locally <ref> (Mohr 1991) </ref>. 5.3 Performance Evaluation To evaluate the effectiveness of our parallel implementation of Natasha, we compared the performance of the Natasha program for Gaussian elimination against an existing hand-tuned parallel program written in C (LeBlanc 1988).
Reference: <author> Polychronopoulos, Constantine D. and David J. Kuck. </author> <year> 1987 </year> <month> (December). </month> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Software Engineering, C-36(12). </journal>
Reference-contexts: One implementation based on this approach follows; other implementations based on dynamic loop scheduling algorithms, such as guided self-scheduling <ref> (Polychronopoulos and Kuck 1987) </ref>, could be implemented in a similar fashion. 7 implement forall $BLOCKED ( lower, upper: integer; body: closure ( iteration: integer ) ) - ifelse ( lower+N &gt; upper, for ( lower, upper, body ) -, - cobegin $PARALLEL ( for ( lower, lower+N-1, body ) -, -
Reference: <author> Sabot, Gary Wayne. </author> <year> 1988. </year> <title> The Paralation Model: Architecture-Independent Parallel Programming. </title> <publisher> MIT Press. </publisher>
Reference-contexts: The Seymor language (Miller and Stout 1989) provides prefix, broadcast, sort, and divide-and-conquer operations, which also have parallel implementations. These languages restrict parallelism to a particular set of operations on data structures. Parallel Programming with Control Abstraction 7 The Paralation model <ref> (Sabot 1988) </ref> and Connection Machine Lisp (Steele and Hillis 1986) support data parallelism through high-level control operations such as iteration and reduction on parallel data structures.
Reference: <author> Scott, Michael L. </author> <year> 1987 </year> <month> (January). </month> <title> Language support for loosely-coupled distributed programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> SE-13(1):88-103. </month> <title> Parallel Programming with Control Abstraction 53 Snyder, </title> <publisher> Lawrence. </publisher> <year> 1984 </year> <month> (July). </month> <title> Parallel programming and the Poker programming environment. </title> <journal> Computer, </journal> <volume> 17(7) </volume> <pages> 27-36. </pages> . <year> 1986. </year> <title> Type architectures, shared memory, and the corollary of modest potential. </title> <booktitle> In Annual Review of Computer Science. </booktitle>
Reference-contexts: However, if a task cannot block, either because it accesses no synchronization variables or because it invokes no operations containing an early reply, we avoid allocating a separate stack and use the scheduler's stack to execute the task. The Lynx implementation also uses this technique <ref> (Scott 1987) </ref>. 17 This optimization, when performed in isolation, can increase execution time because the compiler no longer eliminates some common sub-expressions. 18 Both this optimization and the previous one are required to promote Natasha activation variables to registers. 46 Crowl and LeBlanc Direct access to scheduler queues: We expect the
Reference: <author> Steele, Guy L., J and W. Daniel Hillis. </author> <year> 1986 </year> <month> (August). </month> <title> Connection Machine Lisp: Fine-grained parallel symbolic processing. </title> <booktitle> In Proceedings of the 1986 ACM Conference on Lisp and Fuctional Programming, </booktitle> <pages> pages 279-297. </pages>
Reference-contexts: The Seymor language (Miller and Stout 1989) provides prefix, broadcast, sort, and divide-and-conquer operations, which also have parallel implementations. These languages restrict parallelism to a particular set of operations on data structures. Parallel Programming with Control Abstraction 7 The Paralation model (Sabot 1988) and Connection Machine Lisp <ref> (Steele and Hillis 1986) </ref> support data parallelism through high-level control operations such as iteration and reduction on parallel data structures.
Reference: <author> Thimbleby, Harold. </author> <year> 1988 </year> <month> (May). </month> <title> Delaying commitment. </title> <journal> IEEE Software, </journal> <volume> 5(3) </volume> <pages> 78-86. </pages>
Reference-contexts: Just as data abstraction requires a change in programming methodology, so does the introduction of control abstraction. Our experience with parallel programming using control abstraction has led to the following insights. Use control abstraction early in the programming process. In principle, abstraction is always good because it delays commitment <ref> (Thimbleby 1988) </ref>, which localizes the program's assumptions and reduces the effort needed to change a program. In practice, abstraction mechanisms have a cost, and delayed commitment may not be worth this cost if there is an obvious best implementation.
Reference: <author> Thomas, Robert H. and Will Crowther. </author> <year> 1988 </year> <month> (August). </month> <title> The Uniform System: An approach to runtime support for large scale shared memory parallel processors. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 245-254. </pages>
Reference-contexts: Thus, even though Multilisp and Paralation use Lisp closures in the implementation of the parallel programming constructs presented to users, there is little or no recognition of the benefits of user-defined control abstractions as a parallel programming tool. 1.1.5 User-Defined Control Constructs for Parallel Programming. BBN's Uniform System <ref> (Thomas and Crowther 1988) </ref> represents one approach to user-defined control constructs for parallel programming. The Uniform System provides a global shared memory and a general-purpose task activation routine, called ActivateGen. This routine takes as parameters a task generation procedure and a work procedure. <p> As before, we can replace forall $DIVIDED with forall $BLOCKED or forall $CYCLIC to obtain triangulate $SYNCHED_BLOCKED and triangulate $SYNCHED_CYCLIC. This triangulate construct is similar to the built-in task generator GenOnHalfArray in BBN's Uniform System <ref> (Thomas and Crowther 1988) </ref>. A Uniform System task generator accepts a pointer to a procedure and executes the procedure in parallel for each value produced by the generator. Thus, generators are a limited form of control abstraction.
Reference: <author> Ullman, Jeffrey R. </author> <year> 1976. </year> <title> An algorithm for subgraph isomorphism. </title> <journal> Journal of the ACM, </journal> <volume> 23 </volume> <pages> 31-42. </pages>
References-found: 33

