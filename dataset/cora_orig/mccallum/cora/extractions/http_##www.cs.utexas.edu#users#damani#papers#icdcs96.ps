URL: http://www.cs.utexas.edu/users/damani/papers/icdcs96.ps
Refering-URL: http://www.cs.utexas.edu/users/damani/papers/papers.html
Root-URL: 
Title: How to Recover Efficiently and Asynchronously when Optimism Fails  
Author: Om P. Damani Vijay K. Garg 
Web: http://maple.ece.utexas.edu/  
Address: Austin, Austin, TX, 78712  
Affiliation: Dept. of Computer Sciences  Dept. of Electrical and Computer Engineering University of Texas at  
Abstract: We propose a new algorithm for recovering asynchronously from failures in a distributed computation. Our algorithm is based on two novel concepts a fault-tolerant vector clock to maintain causality information in spite of failures, and a history mechanism to detect orphan states and obsolete messages. These two mechanisms together with checkpointing and message-logging are used to restore the system to a consistent state after a failure of one or more processes. Our algorithm is completely asynchronous. It handles multiple failures, does not assume any message ordering, causes the minimum amount of rollback and restores the maximum recoverable state with low overhead. Earlier optimistic protocols lack one or more of the above properties. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Alvisi, B. Hoppe, and K. Marzullo. </author> <title> Nonblocking and Orphan-Free Message Logging Protocols. </title> <booktitle> Proc. 23rd Fault-Tolerant Computing Symp. </booktitle> , <pages> 145-154, </pages> <year> 1993. </year>
Reference-contexts: The main drawback of their algorithm is the size of its vector clock, resulting in high overhead during failure-free operations. Another drawback is that erroneous computation may continue for long time. An optimistic protocol for fast output to environment is presented in [8]. Causal logging <ref> [1, 5] </ref> protocols are non-blocking and orphan free. They log message in processes other than the receiver. So synchronization is required during recovery. Alvisi and Marzullo [2] present a theoretical framework for different message logging protocols. 2. Our Model of Computation A distributed computation is a set of process executions.
Reference: [2] <author> L. Alvisi and K. Marzullo. </author> <title> Message Logging: Pessimistic, Optimistic, and Causal. </title> <booktitle> Proc. 15th Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> 229-236, </pages> <year> 1995. </year>
Reference-contexts: An optimistic protocol for fast output to environment is presented in [8]. Causal logging [1, 5] protocols are non-blocking and orphan free. They log message in processes other than the receiver. So synchronization is required during recovery. Alvisi and Marzullo <ref> [2] </ref> present a theoretical framework for different message logging protocols. 2. Our Model of Computation A distributed computation is a set of process executions.
Reference: [3] <author> F. Cristian and F. Jahanian. </author> <title> A Timestamp-Based Check-pointing Protocol for Long-Lived Distributed Computations. </title> <booktitle> Proc. 10th IEEE Symp. on Reliable Distributed Systems, </booktitle> <pages> 12-20, </pages> <year> 1991. </year>
Reference-contexts: A consistent state is one where the send of a message must be recorded in the sender's state if the receipt of the message has been recorded in the receiver's state. In consistent checkpointing, different processes synchronize their checkpointing actions <ref> [3, 11] </ref>. After a process fails, some or all of the processes rollback to their last checkpoint such that the resulting system state is consistent.
Reference: [4] <author> O. P. Damani and V. K. Garg, </author> <title> How to Recover Efficiently and Asynchronously when Optimism Fails, </title> <type> Technical Report, </type> <institution> TR-PDS-1995-014, Electrical and Computer Engineering Department, University of Texas at Austin, </institution> <month> August </month> <year> 1995, </year> <note> http://maple.ece.utexas.edu/ vijay/dist/om.ps.Z </note> . 
Reference-contexts: Then, s 6! u ) u:c [s:p] &lt; s:c [s:p] Proof of this lemma can be found in <ref> [4] </ref>. As shown in the next theorem, the above condition is also sufficient for `6!' relation. It shows that despite failures, FTVC keeps track of causality for the useful states. This may be of interest in applications other than recovery, for example, in predicate detection [6].
Reference: [5] <author> E. N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> Transparent Rollback Recovery with Low Overhead, Limited Rollback and Fast Output Commit. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 41 (5): </volume> <pages> 526-531, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The main drawback of their algorithm is the size of its vector clock, resulting in high overhead during failure-free operations. Another drawback is that erroneous computation may continue for long time. An optimistic protocol for fast output to environment is presented in [8]. Causal logging <ref> [1, 5] </ref> protocols are non-blocking and orphan free. They log message in processes other than the receiver. So synchronization is required during recovery. Alvisi and Marzullo [2] present a theoretical framework for different message logging protocols. 2. Our Model of Computation A distributed computation is a set of process executions.
Reference: [6] <author> V. K. Garg and B. Waldecker. </author> <title> Detection of Weak Unstable Predicates in Distributed Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 5, No. 3, </volume> <pages> pp. 299-307, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: As shown in the next theorem, the above condition is also sufficient for `6!' relation. It shows that despite failures, FTVC keeps track of causality for the useful states. This may be of interest in applications other than recovery, for example, in predicate detection <ref> [6] </ref>. Theorem 1 Let s and u be useful states in a distributed computation. Then, s ! u iff s:c &lt; u:c Proof: If s = u, then the theorem is trivially true. Let s ! u. <p> The following overheads are involved in this protocol: 1. FTVC: The protocol tags a FTVC to every application message. The FTVC might be needed for purposes other than recovery, for example predicate detection <ref> [6] </ref>. Let the maximum number of failures of any process be f . The protocol adds log f bits to each times-tamp in vector clock. Since we expect the number of failures to be small, log f should be small. 2.
Reference: [7] <author> V. Hadzilacos and S. Toueg. </author> <title> Fault-Tolerant Broadcasts and Related Problems. Distributed Systems, Edited by S. </title> <publisher> Mullen-der, Publishers Addison-Wesley,97-146,1993. </publisher>
Reference-contexts: P i , then restarts computing as normal. 5.1. Remarks The following issues are relevant to all the optimistic protocols including ours. We just mention them and do not discuss them any further. 1. Efficient failure detection and reliable broadcast in asynchronous distributed system are challenging problems in themselves <ref> [7] </ref>.
Reference: [8] <author> D. B. Johnson. </author> <title> Efficient Transparent Optimistic Rollback Recovery for Distributed Application Programs. </title> <booktitle> Proc. 12th IEEE Symp. on Reliable Distributed Systems, </booktitle> <pages> 86-95, </pages> <year> 1993. </year>
Reference-contexts: The main drawback of their algorithm is the size of its vector clock, resulting in high overhead during failure-free operations. Another drawback is that erroneous computation may continue for long time. An optimistic protocol for fast output to environment is presented in <ref> [8] </ref>. Causal logging [1, 5] protocols are non-blocking and orphan free. They log message in processes other than the receiver. So synchronization is required during recovery. Alvisi and Marzullo [2] present a theoretical framework for different message logging protocols. 2.
Reference: [9] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> Sender-Based Message Logging. </title> <booktitle> Proc. 17th Intl. Symp. on Fault-Tolerant Computing, </booktitle> <pages> 14-19, </pages> <year> 1987. </year>
Reference-contexts: Theoretically, message logging alone is sufficient, but checkpointing speeds up the recovery. Messages can be logged either by the sender or by the receiver. In pessimistic logging, messages are logged either as soon as they are received, or before the receiver sends a new message <ref> [9] </ref>. When a process fails, its last checkpoint is restored and the logged messages that were received after the check-pointed state are replayed in the order they were received. Pessimism in logging ensures that no other process needs to be rolled back.
Reference: [10] <author> D.B. Johnson and W. Zwaenpeoel. </author> <title> Recovery in Distributed Systems using Optimistic Message Logging and Checkpoint-ing. </title> <journal> Journal of Algorithms, </journal> <volume> 11: </volume> <pages> 462-491, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: For fl supported in part by the NSF Grants CCR-9520540 and ECS-9414780, a TRW faculty assistantship award, a General Motors Fellowship, and an IBM grant. large systems, the cost of this synchronization is prohibitive. Furthermore, these protocols may not restore the maximum recoverable state <ref> [10] </ref>. If along with checkpoints, messages are logged to the stable storage, then the maximum recoverable state can always be restored [10]. Theoretically, message logging alone is sufficient, but checkpointing speeds up the recovery. Messages can be logged either by the sender or by the receiver. <p> Furthermore, these protocols may not restore the maximum recoverable state <ref> [10] </ref>. If along with checkpoints, messages are logged to the stable storage, then the maximum recoverable state can always be restored [10]. Theoretically, message logging alone is sufficient, but checkpointing speeds up the recovery. Messages can be logged either by the sender or by the receiver. In pessimistic logging, messages are logged either as soon as they are received, or before the receiver sends a new message [9]. <p> Pessimism in logging ensures that no other process needs to be rolled back. Although this recovery mechanism is simple, it reduces the speed of the computation. Therefore, it is not a desirable scheme in an environment where failures are rare and message activity is high. In optimistic recovery schemes <ref> [10, 14, 15, 16, 17] </ref>, it is assumed that failures are rare. A process stores the received messages in volatile memory and logs it to stable storage at infrequent intervals. Since volatile memory is lost in a failure, some of the messages can not be replayed after the failure. <p> A recovery protocol should make as weak assumptions as possible about the ordering of messages. * Handle concurrent failures: It is possible that more than one processes fail concurrently in a distributed computation. A recovery protocol should handle this situation correctly and efficiently <ref> [10, 15] </ref>. * Low overhead: The algorithm should have a low overhead in terms of number of control messages or the amount of control information piggybacked on application messages, both during a failure-free operation and during recovery. * Recover maximum recoverable state: No computation should be needlessly rolled back. <p> Previous protocols lack one or more of these properties. Table 1 shows a comparison of our work with some other optimistic recovery schemes. Strom and Yemini [17] initiated the area of optimistic recovery using checkpointing. Their scheme, however, suffers from the domino effect. Johnson and Zwaenepoel <ref> [10] </ref> present a centralized protocol to optimistically recover the maximum recoverable state. Other distributed protocols for optimistic recovery can be found in [14, 15, 16]. Peterson and Kearns [14] give a synchronous protocol based on vector clock. Their protocol cannot handle multiple failures. <p> Internal events do not cause state transitions; we ignore them for the rest of the paper. Processes are assumed to be piecewise deterministic. This means that MO AR R T F Strom et. al. [17] FIFO Yes fi (2 n ) O (n) 1 Johnson et. al. <ref> [10] </ref> None No 1 O (1) n Sistla et. al. [16] FIFO No 1 O (n) 1 Peterson et. al. [14] FIFO No 1 O (n) 1 Smith et. al. [15] None Yes 1 O (n 2 f ) n This Paper None Yes 1 O (n) n Table 1.
Reference: [11] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and Rollback Recovery for Distributed Systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 13(1): </volume> <pages> 23-31, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: A consistent state is one where the send of a message must be recorded in the sender's state if the receipt of the message has been recorded in the receiver's state. In consistent checkpointing, different processes synchronize their checkpointing actions <ref> [3, 11] </ref>. After a process fails, some or all of the processes rollback to their last checkpoint such that the resulting system state is consistent.
Reference: [12] <author> L. Lamport. </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> vol. 21, no. 7, </volume> <pages> 558-565, </pages> <year> 1978. </year>
Reference-contexts: We extend the Lamport's happen before (! ) relation <ref> [12] </ref>.
Reference: [13] <author> B. W. Lampson. </author> <title> Atomic Transactions. </title> <editor> B. W. Lampson, M. Paul, and H. J. Siegert, editors, </editor> <booktitle> Distributed Systems-Architecture and Implementation, Spring-Verlag, </booktitle> <pages> 246-265, </pages> <year> 1981. </year>
Reference: [14] <author> S.L. Peterson and P. Kearns. </author> <title> Rollback Based on Vector Time. </title> <booktitle> Proc. 12th IEEE Symp. on Reliable Distributed Systems, </booktitle> <pages> 68-77, </pages> <year> 1993. </year>
Reference-contexts: Pessimism in logging ensures that no other process needs to be rolled back. Although this recovery mechanism is simple, it reduces the speed of the computation. Therefore, it is not a desirable scheme in an environment where failures are rare and message activity is high. In optimistic recovery schemes <ref> [10, 14, 15, 16, 17] </ref>, it is assumed that failures are rare. A process stores the received messages in volatile memory and logs it to stable storage at infrequent intervals. Since volatile memory is lost in a failure, some of the messages can not be replayed after the failure. <p> This is called the domino effect. A process should rollback at most once in response to each failure. * No assumptions about the ordering of messages: If assumptions are made about the ordering of messages such as FIFO, then we lose the asynchronous character of the computation <ref> [14] </ref>. A recovery protocol should make as weak assumptions as possible about the ordering of messages. * Handle concurrent failures: It is possible that more than one processes fail concurrently in a distributed computation. <p> Strom and Yemini [17] initiated the area of optimistic recovery using checkpointing. Their scheme, however, suffers from the domino effect. Johnson and Zwaenepoel [10] present a centralized protocol to optimistically recover the maximum recoverable state. Other distributed protocols for optimistic recovery can be found in <ref> [14, 15, 16] </ref>. Peterson and Kearns [14] give a synchronous protocol based on vector clock. Their protocol cannot handle multiple failures. Smith, Johnson and Tygar [15] present the first completely asynchronous, optimistic protocol which can handle multiple failures. <p> Their scheme, however, suffers from the domino effect. Johnson and Zwaenepoel [10] present a centralized protocol to optimistically recover the maximum recoverable state. Other distributed protocols for optimistic recovery can be found in [14, 15, 16]. Peterson and Kearns <ref> [14] </ref> give a synchronous protocol based on vector clock. Their protocol cannot handle multiple failures. Smith, Johnson and Tygar [15] present the first completely asynchronous, optimistic protocol which can handle multiple failures. <p> This means that MO AR R T F Strom et. al. [17] FIFO Yes fi (2 n ) O (n) 1 Johnson et. al. [10] None No 1 O (1) n Sistla et. al. [16] FIFO No 1 O (n) 1 Peterson et. al. <ref> [14] </ref> FIFO No 1 O (n) 1 Smith et. al. [15] None Yes 1 O (n 2 f ) n This Paper None Yes 1 O (n) n Table 1. <p> In Figure 1, r20.c &lt; s22.c, even though r20 6! s22. To detect causality for lost or orphan states, we use history, as explained in Section 4. 4. History Mechanism We first give some definitions which are similar to those in <ref> [14] </ref>. A state is called lost, if it cannot be restored from the stable storage after a process fails. To define a lost state more formally, let restored (u) denote the state that is restored after a failure.
Reference: [15] <author> S. W. Smith, D. B. Johnson, and J. D. Tygar. </author> <title> Completely Asynchronous Optimistic Recovery with Minimal Rollbacks. </title> <booktitle> Proc. 25th Intl. Symp. on Fault-Tolerant Computing, </booktitle> <pages> 361-370, </pages> <year> 1995. </year>
Reference-contexts: Pessimism in logging ensures that no other process needs to be rolled back. Although this recovery mechanism is simple, it reduces the speed of the computation. Therefore, it is not a desirable scheme in an environment where failures are rare and message activity is high. In optimistic recovery schemes <ref> [10, 14, 15, 16, 17] </ref>, it is assumed that failures are rare. A process stores the received messages in volatile memory and logs it to stable storage at infrequent intervals. Since volatile memory is lost in a failure, some of the messages can not be replayed after the failure. <p> States in other processes that depend on these lost states become orphan. A recovery protocol must rollback these orphan states to non-orphan states. The following properties are desirable for an optimistic recovery protocol: * Asynchronous recovery: A process should be able to restart immediately after a failure <ref> [15, 17] </ref>. It should not have to wait for messages from other processes. * Minimal amount of rollback: In some algorithms, processes which causally depend on the lost computation might rollback more than once. In the worst case, they may rollback an exponential number of times. <p> A recovery protocol should make as weak assumptions as possible about the ordering of messages. * Handle concurrent failures: It is possible that more than one processes fail concurrently in a distributed computation. A recovery protocol should handle this situation correctly and efficiently <ref> [10, 15] </ref>. * Low overhead: The algorithm should have a low overhead in terms of number of control messages or the amount of control information piggybacked on application messages, both during a failure-free operation and during recovery. * Recover maximum recoverable state: No computation should be needlessly rolled back. <p> Strom and Yemini [17] initiated the area of optimistic recovery using checkpointing. Their scheme, however, suffers from the domino effect. Johnson and Zwaenepoel [10] present a centralized protocol to optimistically recover the maximum recoverable state. Other distributed protocols for optimistic recovery can be found in <ref> [14, 15, 16] </ref>. Peterson and Kearns [14] give a synchronous protocol based on vector clock. Their protocol cannot handle multiple failures. Smith, Johnson and Tygar [15] present the first completely asynchronous, optimistic protocol which can handle multiple failures. <p> Other distributed protocols for optimistic recovery can be found in [14, 15, 16]. Peterson and Kearns [14] give a synchronous protocol based on vector clock. Their protocol cannot handle multiple failures. Smith, Johnson and Tygar <ref> [15] </ref> present the first completely asynchronous, optimistic protocol which can handle multiple failures. They maintain information about two levels of partial order: one for the application and the other for the recovery. <p> AR R T F Strom et. al. [17] FIFO Yes fi (2 n ) O (n) 1 Johnson et. al. [10] None No 1 O (1) n Sistla et. al. [16] FIFO No 1 O (n) 1 Peterson et. al. [14] FIFO No 1 O (n) 1 Smith et. al. <ref> [15] </ref> None Yes 1 O (n 2 f ) n This Paper None Yes 1 O (n) n Table 1.
Reference: [16] <author> A. P. Sistla and J. L. Welch. </author> <title> Efficient Distributed Recovery Using Message Logging. </title> <booktitle> Proc. 8th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> 223-238, </pages> <year> 1989. </year>
Reference-contexts: Pessimism in logging ensures that no other process needs to be rolled back. Although this recovery mechanism is simple, it reduces the speed of the computation. Therefore, it is not a desirable scheme in an environment where failures are rare and message activity is high. In optimistic recovery schemes <ref> [10, 14, 15, 16, 17] </ref>, it is assumed that failures are rare. A process stores the received messages in volatile memory and logs it to stable storage at infrequent intervals. Since volatile memory is lost in a failure, some of the messages can not be replayed after the failure. <p> Strom and Yemini [17] initiated the area of optimistic recovery using checkpointing. Their scheme, however, suffers from the domino effect. Johnson and Zwaenepoel [10] present a centralized protocol to optimistically recover the maximum recoverable state. Other distributed protocols for optimistic recovery can be found in <ref> [14, 15, 16] </ref>. Peterson and Kearns [14] give a synchronous protocol based on vector clock. Their protocol cannot handle multiple failures. Smith, Johnson and Tygar [15] present the first completely asynchronous, optimistic protocol which can handle multiple failures. <p> Processes are assumed to be piecewise deterministic. This means that MO AR R T F Strom et. al. [17] FIFO Yes fi (2 n ) O (n) 1 Johnson et. al. [10] None No 1 O (1) n Sistla et. al. <ref> [16] </ref> FIFO No 1 O (n) 1 Peterson et. al. [14] FIFO No 1 O (n) 1 Smith et. al. [15] None Yes 1 O (n 2 f ) n This Paper None Yes 1 O (n) n Table 1.
Reference: [17] <author> R. E. Strom and S. Yemini. </author> <title> Optimistic Recovery in Distributed Systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Pessimism in logging ensures that no other process needs to be rolled back. Although this recovery mechanism is simple, it reduces the speed of the computation. Therefore, it is not a desirable scheme in an environment where failures are rare and message activity is high. In optimistic recovery schemes <ref> [10, 14, 15, 16, 17] </ref>, it is assumed that failures are rare. A process stores the received messages in volatile memory and logs it to stable storage at infrequent intervals. Since volatile memory is lost in a failure, some of the messages can not be replayed after the failure. <p> States in other processes that depend on these lost states become orphan. A recovery protocol must rollback these orphan states to non-orphan states. The following properties are desirable for an optimistic recovery protocol: * Asynchronous recovery: A process should be able to restart immediately after a failure <ref> [15, 17] </ref>. It should not have to wait for messages from other processes. * Minimal amount of rollback: In some algorithms, processes which causally depend on the lost computation might rollback more than once. In the worst case, they may rollback an exponential number of times. <p> We present an optimistic recovery protocol which has all the above features. Previous protocols lack one or more of these properties. Table 1 shows a comparison of our work with some other optimistic recovery schemes. Strom and Yemini <ref> [17] </ref> initiated the area of optimistic recovery using checkpointing. Their scheme, however, suffers from the domino effect. Johnson and Zwaenepoel [10] present a centralized protocol to optimistically recover the maximum recoverable state. Other distributed protocols for optimistic recovery can be found in [14, 15, 16]. <p> Internal events do not cause state transitions; we ignore them for the rest of the paper. Processes are assumed to be piecewise deterministic. This means that MO AR R T F Strom et. al. <ref> [17] </ref> FIFO Yes fi (2 n ) O (n) 1 Johnson et. al. [10] None No 1 O (1) n Sistla et. al. [16] FIFO No 1 O (n) 1 Peterson et. al. [14] FIFO No 1 O (n) 1 Smith et. al. [15] None Yes 1 O (n 2 f
Reference: [18] <author> Y. M. Wang, P. Y. Chung, I. J. Lin, and W. K. Fuchs. </author> <title> Check-pointing Space Reclamation for Uncoordinated Checkpoint-ing in Message Passing Systems. </title> <journal> IEEE Trans. on Parallel and distributed Systems, </journal> <volume> vol. 6, no. 5, </volume> <pages> 546-554, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Before rolling back, it can log all the messages and so no message is lost. 3. Some form of garbage collection is also required for reclaiming space. Space required for checkpoints and message logs can be bounded by using the scheme presented in <ref> [18] </ref>. Before committing an output to the environment, a process must make sure that it will never rollback the current state or lose it in a failure. 5.2. An Example In Figure 5, ci is the checkpoint of process P i.
References-found: 18

