URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-358.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: picard@media.mit.edu,  
Title: Toward a Visual Thesaurus  
Author: Rosalind W. Picard 
Web: http://www.media.mit.edu/~picard/  
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: MIT Media Laboratory,  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 358 Also to appear: Springer Verlag Workshops in Computing, MIRO 95, Invited Paper, Glasgow, Sep. 95 Abstract A thesaurus is a book containing synonyms in a given language; it provides similarity links when trying to retrieve articles or stories about a particular topic. A "visual thesaurus" works with pictures, not words. It aids in recognizing visually similar events, "visual synonyms," including both spatial and motion similarity. This paper describes a method for building such a tool, and recent research results in the MIT Media Lab which contribute toward this goal. The heart of the method is a learning system which gathers information by interacting with a user of a database. The learning system is also capable of incorporating audio and other perceptual information, ultimately constructing a representation of common sense knowledge.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> "Wordnet," </author> <year> 1995. </year> <note> http://www.cogsci.princeton.edu/~wn/. </note>
Reference-contexts: thousand words," i.e able to be uniquely described by those words, then image retrieval would be relatively easy for computers: form an index by compressing the thousand words (which occupy far fewer bytes than most pictures), and apply existing text-based query methods, including an online text thesaurus such as Wordnet <ref> [1] </ref>.
Reference: [2] <author> R. W. Picard and T. P. Minka, </author> <title> "Vision texture for annotation," </title> <journal> Journal of Multimedia Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 3-14, </pages> <year> 1995. </year>
Reference-contexts: But, this presumes a solution to the problem of generating the best set of a thousand words; which words uniquely describe the picture, and who will decide what they should be for all pictures in the world? Although progress is being made with computer vision tools to assist in annotation <ref> [2] </ref>, [3], [4], the choice of the right words for a picture is still up to an individual. The words are domain-dependent, knowledge-dependent, and may also depend on subjective influences or visual associations.
Reference: [3] <author> E. Saber, A. M. Tekalp, R. Eschbach, and K. Knox, </author> <title> "Annotation of natural scenes using adaptive color segmentation," IS&T/SPIE Electronic Imaging, </title> <address> Feb. 1995. San Jose, CA. </address>
Reference-contexts: this presumes a solution to the problem of generating the best set of a thousand words; which words uniquely describe the picture, and who will decide what they should be for all pictures in the world? Although progress is being made with computer vision tools to assist in annotation [2], <ref> [3] </ref>, [4], the choice of the right words for a picture is still up to an individual. The words are domain-dependent, knowledge-dependent, and may also depend on subjective influences or visual associations.
Reference: [4] <author> T. P. Minka and R. W. </author> <title> Picard, "Interactive learning using a `society of models'," </title> <note> Submitted for Publication, 1995. Also appears as MIT Media Lab Perceptual Computing TR#349. </note>
Reference-contexts: presumes a solution to the problem of generating the best set of a thousand words; which words uniquely describe the picture, and who will decide what they should be for all pictures in the world? Although progress is being made with computer vision tools to assist in annotation [2], [3], <ref> [4] </ref>, the choice of the right words for a picture is still up to an individual. The words are domain-dependent, knowledge-dependent, and may also depend on subjective influences or visual associations. <p> Our latest efforts in this area appear in <ref> [4] </ref>, a paper describing the new FourEyes system which learns groupings in interactive-time based on positive and negative visual examples provided by a user. <p> The FourEyes system adaptively computes weights for all the groupings generated by all the models, and then combines the ones which best match the user's positive examples, without including negative examples. (Criteria for "best" and other details are given in <ref> [4] </ref>). In this way, FourEyes implicitly chooses the most relevant features, combining features from multiple models if that gives the best result.
Reference: [5] <author> S. Intille and A. Bobick, </author> <title> "Exploiting contextual information for tracking by using closed-worlds," </title> <booktitle> in Proceedings of the Workshop on Context-based Vision, </booktitle> <address> (Cambridge, MA), </address> <pages> pp. 87-98, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: This high-level language, coupled with computer vision techniques, can be used for example to simplify the retrieval of similar plays from digital video, an important aid in analyzing successful games for improving winning strategies <ref> [5] </ref>. Another domain where there is syntax is photography. Romer [6] has described a number of useful syntactical components which occur repeatedly in photographs, such as horizontal structure (e.g. sunset photos), or aerial view (e.g. looking down from high buildings or from airplanes).
Reference: [6] <author> D. Romer, </author> <title> "The Kodak picture exchange," </title> <month> April </month> <year> 1995. </year> <institution> seminar at MIT Media Lab. </institution>
Reference-contexts: This high-level language, coupled with computer vision techniques, can be used for example to simplify the retrieval of similar plays from digital video, an important aid in analyzing successful games for improving winning strategies [5]. Another domain where there is syntax is photography. Romer <ref> [6] </ref> has described a number of useful syntactical components which occur repeatedly in photographs, such as horizontal structure (e.g. sunset photos), or aerial view (e.g. looking down from high buildings or from airplanes).
Reference: [7] <author> S. Mann and R. </author> <title> Picard, "Video orbits of the projective group: A new perspective on image mosaicing," </title> <note> Submitted for Publication, 1995. Also appears as MIT Media Lab Perceptual Computing TR#338. </note>
Reference-contexts: In these two cases, all the photos taken by the camera essentially lie in the same "orbit of the projective group" so that they are related by a simple coordinate transformation <ref> [7] </ref>. Visual synonyms can also occur with patterns, colors, shapes, and textures, including motion patterns or temporal textures [8], [9]. An arrangement of chairs at an outdoor wedding viewed from above may have the same pattern as rows of hedges and flowerbeds in a formal garden. <p> Stage 1 does more than necessary since it allows visual as well as other groupings. Visual synonyms may be grouped either in Stage 1, if there is a model (such as the video orbits <ref> [7] </ref>) that can group them, or in the latter units, if a combination of models is required. Stage 2 weights combinations of groupings that serve together in 2 Werbos was actually inspired by Freud's idea of cathexis, a feedback of emotional energy. 4 useful ways discovered by Stage 3.
Reference: [8] <author> R. Polana and R. C. Nelson, </author> <title> "Recognition of motion from temporal texture," </title> <booktitle> in Proceedings CVPR '92 (C. </booktitle> <editor> Harris, ed.), </editor> <address> (Champaign, IL), </address> <pages> pp. </pages> <month> 129-134, </month> <title> Computer Vision and Pattern Recognition, </title> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: In these two cases, all the photos taken by the camera essentially lie in the same "orbit of the projective group" so that they are related by a simple coordinate transformation [7]. Visual synonyms can also occur with patterns, colors, shapes, and textures, including motion patterns or temporal textures <ref> [8] </ref>, [9]. An arrangement of chairs at an outdoor wedding viewed from above may have the same pattern as rows of hedges and flowerbeds in a formal garden.
Reference: [9] <author> M. Szummer, </author> <title> "Temporal texture modeling," </title> <type> Master's thesis, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Visual synonyms can also occur with patterns, colors, shapes, and textures, including motion patterns or temporal textures [8], <ref> [9] </ref>. An arrangement of chairs at an outdoor wedding viewed from above may have the same pattern as rows of hedges and flowerbeds in a formal garden. A crowd of people pouring out of a stadium exhibits motion flow similar to candies flowing down a chute in a candy factory.
Reference: [10] <author> P. Werbos, </author> <title> "The brain as a neurocontroller: New hypotheses and new experimental possibilities," in Origins: </title> <editor> Brain and Self-Organization (K. H. Pribram, ed.), </editor> <publisher> Erlbaum, </publisher> <year> 1994. </year>
Reference-contexts: After the learner in Stage 3 collects combinations of weighted groupings and learns which the user likes best, it enhances the winning weights in Stage 2. This feedback is similar to that 2 which inspired Werbos in creating back-propagation <ref> [10] </ref>, although the mathematical update rule here is different. Thus, Stages 2-3 are necessary for the multiple models to form a society, interacting to give more powerful and efficient descriptions than any one model can provide, and learning as they interact.
Reference: [11] <author> F. Liu and R. W. </author> <title> Picard, "Periodicity, directionality, and randomness: Wold features for perceptual pattern recognition," </title> <booktitle> in Proc. Int. Conf. Pat. Rec., vol. II, (Jerusalem, Israel), </booktitle> <pages> pp. 184-185, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: In some cases, where model features have semantic associations, e.g, the features in the Wold model correspond to the adjectives of periodicity, directionality, and randomness <ref> [11] </ref>, then the labels might be inferred from the model features directly. The only addition remaining to make a visual thesaurus using FourEyes is to allow directed associations for the hierarchical relations mentioned above (Section 2.1.2).
Reference: [12] <author> Y. J. Gao, J. J. Lim, and A. D. Narasimhalu, </author> <title> "Fuzzy multilinkage thesaurus builder in multimedia information systems," 1995. </title> <institution> Institute of Systems, Science, National University of Singapore. </institution>
Reference-contexts: For example, weights on links arise in Stage 2, allowing more useful links to receive higher values and consequently be found faster. This importance of weighted links has been argued by others; for example, Gao et al. have developed a "fuzzy" text thesaurus for help retrieving trademark images <ref> [12] </ref>. The FourEyes-based visual thesaurus automatically provides this multiple-membership advantage. FourEyes also automatically updates the weightings and groupings as users interact with it; it thus accumulates new knowledge. The FourEyes advantage also extends to the third category of associative relations, which includes user-defined relations and subjective associations.
Reference: [13] <author> I. Hunter, </author> <year> 1995. </year> <type> Personal Communication. </type>
Reference-contexts: For instance, there is a huge industry associated with olfactory/taste vocabularies, e.g. perfumes, wines, cleaning products, food products, even the "new-car" smell. As computers become equipped with "artificial noses" <ref> [13] </ref>, they can construct an olfactory thesaurus that allows one to compare similar odors, and retrieve products with those odors.
Reference: [14] <author> F. Matsumoto, </author> <title> "Using simple controls to manipulate complex objects: Application to the drum-boy interactive percussion system," </title> <type> Master's thesis, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: As computers become equipped with "artificial noses" [13], they can construct an olfactory thesaurus that allows one to compare similar odors, and retrieve products with those odors. The most progress on perceptual similarities appears to have been done with retrieval of similar audio patterns (e.g. <ref> [14] </ref>, [15]), which provide an important aid to musicians and sound effects artists. Note that the same models may be used for different senses.
Reference: [15] <author> N. Saint-Arnaud, </author> <title> "Classification of sound textures," </title> <type> Master's thesis, </type> <institution> MIT, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: As computers become equipped with "artificial noses" [13], they can construct an olfactory thesaurus that allows one to compare similar odors, and retrieve products with those odors. The most progress on perceptual similarities appears to have been done with retrieval of similar audio patterns (e.g. [14], <ref> [15] </ref>), which provide an important aid to musicians and sound effects artists. Note that the same models may be used for different senses. After all, one human brain processes all the perceptions, so that re-use of a descriptive model for vision and audition would suggest some efficiency in the brain. <p> Note that the same models may be used for different senses. After all, one human brain processes all the perceptions, so that re-use of a descriptive model for vision and audition would suggest some efficiency in the brain. The work of <ref> [15] </ref> demonstrates this cross-over by successfully using a model for audio patterns that was previously used successfully for visual patterns [16]. The society of models approach used in FourEyes allows the same or different models to be combined into one set of groupings.
Reference: [16] <author> K. Popat and R. W. </author> <title> Picard, "Novel cluster-based probability models for texture synthesis, classification, and compression," </title> <booktitle> in Proc. SPIE Visual Communication and Image Proc., vol. 2094, (Boston), </booktitle> <pages> pp. 756-768, </pages> <month> Nov. </month> <year> 1993. </year> <month> 6 </month>
Reference-contexts: The work of [15] demonstrates this cross-over by successfully using a model for audio patterns that was previously used successfully for visual patterns <ref> [16] </ref>. The society of models approach used in FourEyes allows the same or different models to be combined into one set of groupings. This effortless mingling of cross-sensory groupings is a feature shared by the human brain.
References-found: 16

