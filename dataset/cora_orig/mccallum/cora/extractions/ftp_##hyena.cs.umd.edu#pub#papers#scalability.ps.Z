URL: ftp://hyena.cs.umd.edu/pub/papers/scalability.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/appl/chem2.html
Root-URL: 
Email: Email fraja, uysal, saltz, shing@cs.umd.edu  
Title: Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures 3  
Author: Raja Das Mustafa Uysal Joel Saltz Yuan-Shin Hwang 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: This paper describes a number of optimizations that can be used to support the efficient execution of irregular problems on distributed memory parallel machines. These primitives (1) coordinate inter-processor data movement, (2) manage the storage of, and access to, copies of off-processor data, (3) minimize interprocessor communication requirements and (4) support a shared name space. We present a detailed performance and scalability analysis of the communication primitives. This performance and scalability analysis is carried out using a workload generator, kernels from real applications and a large unstructured adaptive application (the molecular dynamics code CHARMM). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Harry Berryman, Joel Saltz, and Jeffrey Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory machines. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(3) </volume> <pages> 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: paper we give a detailed description of the communication optimizations that prove to be useful for optimizing irregular problem performance but do not further address compilation issues. 1.1 Overview of PARTI In this section, we give an overview of the functionality of the PARTI primitives described in previous publications [22], <ref> [1] </ref>, [17]. In many algorithms data produced or input during program initialization plays a large role in determining the nature of the subsequent computation. In the PARTI approach, when the data structures that define a computation have been initialized, a preprocessing phase follows. <p> The class of problem with which we are dealing consists of a sequence of clearly demarcated concurrent computational phases where data access patterns cannot be anticipated until runtime. They are called static irregular concurrent computations <ref> [1] </ref>. In these problems, once runtime information is available, 1) data access patterns are known before each computational phase and 2) the same data access patterns occur many times. Adaptive problems can fall into this class of problem as long as data access patterns change relatively infrequently. <p> The executor loop then uses the information from the inspector to implement the actual computation. We have developed a suite of primitives that can be used directly by programmers to generate inspector/executor pairs. These primitives are named PARTI [5], <ref> [1] </ref>; they carry out the distribution and retrieval of globally indexed but irregularly distributed data-sets over the numerous local processor memories.
Reference: [2] <author> S. Bokhari. </author> <title> Communication overhead on the intel ipsc-860 hypercube. </title> <type> Report 90-10, ICASE Interim Report, </type> <year> 1990. </year>
Reference-contexts: The object of communication coalescing is to reduce the number of message startups. For many distributed memory systems there is a substantial latency associated with message passing. For instance, Bokhari <ref> [2] </ref> measured the time to communicate a message of size k (bytes) between two nodes of an Intel iPSC/860 as: T = 65.0 + 0.425k + 10.0h, for 0 &lt; k 100, and where T is the time in secs and h is the number of hops between the communicating processors.
Reference: [3] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: In Section 2.1 we describe a representative kernel from the explicit Euler solver [14], [5] and Section 2.2 briefly describes the kernel from the molecular dynamics code CHARMM <ref> [3] </ref>, [7]. 2.1 Unstructured Euler Kernel Unstructured meshes provide a great deal of flexibility in discretizing complex domains and offer the possibility of easily performing adaptive meshing. However, unstructured meshes result in random data-sets and large sparse matrices which require runtime preprocessing to be executed on a distributed memory machine.
Reference: [4] <author> B. R. Brooks and M. Hodoscek. </author> <title> Parallelization of charmm for mimd machines. Chemical Design Automation News, </title> <address> 7:16, </address> <year> 1992. </year>
Reference-contexts: Data Replication Processors Unweighted Weighted Replicated 1 (sec/iteration) (sec/iteration) (sec/iteration) 16 5.38 4.86 4.95 64 2.27 1.35 1.53 1 Results from <ref> [4] </ref> and Table 10. We present these results to make clear that we have found that our runtime support and optimizations can be, and are, being used to port challenging application codes. These results are comparable to all other implementations of which we are aware [4]. <p> 1.35 1.53 1 Results from <ref> [4] </ref> and Table 10. We present these results to make clear that we have found that our runtime support and optimizations can be, and are, being used to port challenging application codes. These results are comparable to all other implementations of which we are aware [4]. We should also note that the optimization scalability function for the weighted coordinate bisection load balancing optimization function takes on values of 1.11, 1.35, 1.69 and 1.65 for 16,32,64 and 128 processor respectively.
Reference: [5] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <note> In To appear AIAA Journal,AIAA-92-0562, </note> <month> Jan </month> <year> 1992. </year>
Reference-contexts: The executor loop then uses the information from the inspector to implement the actual computation. We have developed a suite of primitives that can be used directly by programmers to generate inspector/executor pairs. These primitives are named PARTI <ref> [5] </ref>, [1]; they carry out the distribution and retrieval of globally indexed but irregularly distributed data-sets over the numerous local processor memories. <p> In this section we briefly describe two kernels that have been extracted from these codes and how they stress the primitives quite differently. In Section 2.1 we describe a representative kernel from the explicit Euler solver [14], <ref> [5] </ref> and Section 2.2 briefly describes the kernel from the molecular dynamics code CHARMM [3], [7]. 2.1 Unstructured Euler Kernel Unstructured meshes provide a great deal of flexibility in discretizing complex domains and offer the possibility of easily performing adaptive meshing.
Reference: [6] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems data copy reuse and runtime partitioning. In Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehrotra Editors, </editor> <address> Amsterdam, The Netherlands, 1992. </address> <publisher> Elsevier. </publisher>
Reference-contexts: The gather on each processor fetches all the necessary y references that reside off-processor. The scatter add calls accumulates the off-processor x values. A detailed description of the functionality of these primitives are given in <ref> [6] </ref>. 5 do i = 1, NATOM do index = 1, INB (i) j = Partners (i, index) Calculate dF (x, y and z components). Subtract dF from F j .
Reference: [7] <author> R. Das and J. Saltz. </author> <title> Parallelizing molecular dynamics codes using parti software primitives. </title> <booktitle> In Parallel Processing for Scientific Computation, Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Norfolk VA, </address> <month> March </month> <year> 1993, 1993. </year>
Reference-contexts: In Section 2.1 we describe a representative kernel from the explicit Euler solver [14], [5] and Section 2.2 briefly describes the kernel from the molecular dynamics code CHARMM [3], <ref> [7] </ref>. 2.1 Unstructured Euler Kernel Unstructured meshes provide a great deal of flexibility in discretizing complex domains and offer the possibility of easily performing adaptive meshing. However, unstructured meshes result in random data-sets and large sparse matrices which require runtime preprocessing to be executed on a distributed memory machine.
Reference: [8] <author> R. Das, J. Saltz, D. Mavriplis, and R. Ponnusamy. </author> <title> The incremental scheduler. In Unstructured Scientific Computation on Scalable Multiprocessors, </title> <address> Cambridge Mass, 1992. </address> <publisher> MIT Press. </publisher> <pages> 29 </pages>
Reference-contexts: 2) + z (ic (i), 2) do i = 1, n x (ib (i), 2) = x (ib (i), 2) + z (id (i), 2) Accumulate x using the single schedule for arrays ia and ib PARTI supported a translation table that was partitioned between processors in a blocked fashion <ref> [8] </ref>, [22]. This was accomplished by putting the first N/P elements on the first processor, the second N/P elements of the table on the second processor, etc ., where P is the number of processors.
Reference: [9] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: As long as it is known that the values assigned to off-processor memory locations remain unmodified, it is possible to reuse stored off-processor data. A mixture of compile-time and run-time analysis can be used to generate efficient code for irregular problems <ref> [9] </ref>, [11], [15]. <p> In either case, the same off-processor data may be accessed multiple times, but only a single copy of that data must be fetched from off-processor. In this paper, we do not address issues associated with identifying when these prefetches can be carried out; See von Hanxladen [11] and Das <ref> [9] </ref> for a detailed discussion. <p> Consequently, when we include the inspector overhead, the optimization scalability function decreases with the number of processors. We have developed a substantial body of compile time analysis dedicated to identifying such situations [11], <ref> [9] </ref>. In the case of the test loops, it is clear that we do not have to repeat our preprocessing for identically distributed array sections.
Reference: [10] <author> J. L. Gustafson, G. R. Montry, and R. E. Benner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Under constant problem size scalability model, the application is run on varying sized systems without changing any of its parameters, including problem size. In the memory-constrained scalability model, the application parameters are scaled so that the memory requirements 17 grow linearly with the increase in system size <ref> [10] </ref>. Finally, in time-constrained scaling, the application parameters are scaled so that the running time does not change while the system size changes. All of these models have some deficiencies and none of them are universal for all applications as pointed out by [19].
Reference: [11] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: As long as it is known that the values assigned to off-processor memory locations remain unmodified, it is possible to reuse stored off-processor data. A mixture of compile-time and run-time analysis can be used to generate efficient code for irregular problems [9], <ref> [11] </ref>, [15]. <p> In either case, the same off-processor data may be accessed multiple times, but only a single copy of that data must be fetched from off-processor. In this paper, we do not address issues associated with identifying when these prefetches can be carried out; See von Hanxladen <ref> [11] </ref> and Das [9] for a detailed discussion. <p> Consequently, when we include the inspector overhead, the optimization scalability function decreases with the number of processors. We have developed a substantial body of compile time analysis dedicated to identifying such situations <ref> [11] </ref>, [9]. In the case of the test loops, it is clear that we do not have to repeat our preprocessing for identically distributed array sections.
Reference: [12] <author> S. Hiranandani, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> Performance of hashed cache data migration schemes on multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 415-422, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The primitives that build schedules use hash tables to generate communication calls that, for each loop nest, transmit only a single copy of each off-processor datum <ref> [12] </ref>, [22]. The schedules are used in the executor by PARTI primitives to gather, scatter and accumulate data to/from off-processor memory locations. In this paper, the idea of eliminating duplicates has been taken a step further.
Reference: [13] <author> V. Kumar and A. Gupta. </author> <title> Analysis of scalability of parallel algorithms and architectures : A survey. </title> <booktitle> In Proc. of 1991 International Conference on Parallel Processing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: On each processor, we choose the most heavily accessed pages as the dynamically assigned pages. 6 Optimization Scalability Various models, metrics and analysis techniques for scalability of parallel programs have been proposed in the literature <ref> [20, 21, 13] </ref>. Under constant problem size scalability model, the application is run on varying sized systems without changing any of its parameters, including problem size.
Reference: [14] <author> D. J. Mavriplis. </author> <title> Three dimensional multigrid for the Euler equations. </title> <journal> AIAA paper 91-1549CP, </journal> <pages> pages 824-831, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In this section we briefly describe two kernels that have been extracted from these codes and how they stress the primitives quite differently. In Section 2.1 we describe a representative kernel from the explicit Euler solver <ref> [14] </ref>, [5] and Section 2.2 briefly describes the kernel from the molecular dynamics code CHARMM [3], [7]. 2.1 Unstructured Euler Kernel Unstructured meshes provide a great deal of flexibility in discretizing complex domains and offer the possibility of easily performing adaptive meshing.
Reference: [15] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choudhary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <institution> Technical Report CS-TR-3055 and UMIACS-TR-93-32, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> April </month> <year> 1993. </year> <note> To Appear Supercomputing '93. </note>
Reference-contexts: As long as it is known that the values assigned to off-processor memory locations remain unmodified, it is possible to reuse stored off-processor data. A mixture of compile-time and run-time analysis can be used to generate efficient code for irregular problems [9], [11], <ref> [15] </ref>.
Reference: [16] <author> A. Pothen, H. D. Simon, and K. P. Liou. </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 11 </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: When the data and the indirection arrays are block partitioned the number of off-processor data is extremely high, and nearly all processors have to communicate with each other. When the data is partitioned based on the connectivity of the mesh (spectral bisection <ref> [16] </ref>), we get low volumes of communication between neighboring processors. We have used geometric partitioners with fair amount of success. 6 2.2 Molecular Dynamics This kernel from the molecular dynamics code CHARMM has a very different data access pattern from that of the Euler code kernel.
Reference: [17] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <year> 1991. </year>
Reference-contexts: we give a detailed description of the communication optimizations that prove to be useful for optimizing irregular problem performance but do not further address compilation issues. 1.1 Overview of PARTI In this section, we give an overview of the functionality of the PARTI primitives described in previous publications [22], [1], <ref> [17] </ref>. In many algorithms data produced or input during program initialization plays a large role in determining the nature of the subsequent computation. In the PARTI approach, when the data structures that define a computation have been initialized, a preprocessing phase follows.
Reference: [18] <author> J. Saltz, K. Crowley, R. Mirchandaney, and Harry Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: When targeted at distributed memory architectures, the naive implementation is extremely inefficient <ref> [18] </ref>. The row labeled "simple communication aggregation" gives the communication characteristics associated with the optimization described in Section 4.2.1. This optimization reduces the number of messages that must be transmitted.
Reference: [19] <author> J. P. Singh, J. L. Hennessy, and A. Gupta. </author> <title> Scaling parallel programs for multiprocessors: Methodology and examples. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 42-50, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Finally, in time-constrained scaling, the application parameters are scaled so that the running time does not change while the system size changes. All of these models have some deficiencies and none of them are universal for all applications as pointed out by <ref> [19] </ref>. There are a wide range of optimizations designed to improve the performance of problems on multiprocessor architectures. Any given optimization targets a certain class of problems and the effectiveness of the optimization varies with problem characteristics.
Reference: [20] <author> X. Sun and D. T. </author> <title> Rover. Scalability of parallel algorithm-machine combinations. </title> <type> Report IS-5057, </type> <month> April </month> <year> 1991. </year>
Reference-contexts: On each processor, we choose the most heavily accessed pages as the dynamically assigned pages. 6 Optimization Scalability Various models, metrics and analysis techniques for scalability of parallel programs have been proposed in the literature <ref> [20, 21, 13] </ref>. Under constant problem size scalability model, the application is run on varying sized systems without changing any of its parameters, including problem size.
Reference: [21] <author> Patrick H. Worley. </author> <title> The effect of time constraints on scaled speedup. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 11(5) </volume> <pages> 838-858, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: On each processor, we choose the most heavily accessed pages as the dynamically assigned pages. 6 Optimization Scalability Various models, metrics and analysis techniques for scalability of parallel programs have been proposed in the literature <ref> [20, 21, 13] </ref>. Under constant problem size scalability model, the application is run on varying sized systems without changing any of its parameters, including problem size.
Reference: [22] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 26-30, </pages> <year> 1991. </year> <month> 30 </month>
Reference-contexts: this paper we give a detailed description of the communication optimizations that prove to be useful for optimizing irregular problem performance but do not further address compilation issues. 1.1 Overview of PARTI In this section, we give an overview of the functionality of the PARTI primitives described in previous publications <ref> [22] </ref>, [1], [17]. In many algorithms data produced or input during program initialization plays a large role in determining the nature of the subsequent computation. In the PARTI approach, when the data structures that define a computation have been initialized, a preprocessing phase follows. <p> The primitives that build schedules use hash tables to generate communication calls that, for each loop nest, transmit only a single copy of each off-processor datum [12], <ref> [22] </ref>. The schedules are used in the executor by PARTI primitives to gather, scatter and accumulate data to/from off-processor memory locations. In this paper, the idea of eliminating duplicates has been taken a step further. <p> + z (ic (i), 2) do i = 1, n x (ib (i), 2) = x (ib (i), 2) + z (id (i), 2) Accumulate x using the single schedule for arrays ia and ib PARTI supported a translation table that was partitioned between processors in a blocked fashion [8], <ref> [22] </ref>. This was accomplished by putting the first N/P elements on the first processor, the second N/P elements of the table on the second processor, etc ., where P is the number of processors.
References-found: 22

