URL: http://dimacs.rutgers.edu/techps/1993/93-58.ps
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1993.html
Root-URL: http://www.cs.rutgers.edu
Title: Matrix Searching with the Shortest Path Metric  
Author: John Hershberger Subhash Suri 
Keyword: Key Words: Shortest paths, matrix searching, geodesic diameter, farthest neighbors, geometric matching.  
Date: August 16, 1993  
Address: 130 Lytton Avenue, Palo Alto, California 94301  445 South Street, Morristown, New Jersey 07960  
Affiliation: DEC/SRC  Bellcore  
Abstract: We present an O(n) time algorithm for computing row-wise maxima or minima of an implicit, totally-monotone n fi n matrix whose entries represent shortest-path distances between pairs of vertices in a simple polygon. We apply this result to derive improved algorithms for several well-known problems in computational geometry. Most prominently, we obtain linear-time algorithms for computing the geodesic diameter, all farthest neighbors, and external farthest neighbors of a simple polygon, improving the previous best result by a factor of O(log n) in each case. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. K. Agarwal, A. Aggarwal, B. Aronov, S. R. Kosaraju, B. Schieber, and S. Suri. </author> <title> Computing external-furthest neighbors for a simple polygon. </title> <journal> Discrete Applied Mathematics, </journal> <volume> 31 </volume> <pages> 97-111, </pages> <year> 1991. </year>
Reference-contexts: The following pseudocode gives a top-level view of the algorithm. MSEARCH (M ) (fl Compute the position and value of the leftmost maximum in each row of M fl) 1. A REDUCE (M ) 2. if jAj = 1 then return the value of A <ref> [1; 1] </ref> and its position in M 3. B the matrix consisting of even rows of A 4. MSEARCH (B) 5. MFILL (A; B) (fl Finds maxima in the odd rows of A fl) 6. Compute the positions and values of the maxima in M from their positions in A. <p> The following code avoids some of the matrix probes in the version of REDUCE above by reusing previously computed values. NEW-REDUCE (M ) for j 1 to m do pred [j] j 1; ncols m; j 2; k 1; value <ref> [1] </ref> M (1; 1); while ncols &gt; p do (fl Note that value [pred [j]] = M (k; pred [j]) fl) if value [pred [j]] M (k; j) then if k &lt; p then k k + 1; value [j] M (k; j); else pred [j + 1] pred [j]; ncols <p> Our algorithm makes critical use of the order in which the matrix-searching algorithm evaluates entries. This order is easier to analyze in NEW-REDUCE than in the equivalent routine REDUCE . The first matrix entry evaluated by REDUCE (or NEW-REDUCE) in the kth phase is A k <ref> [1; 1] </ref>. In each new evaluation, either the row or the column index is changed by one. In particular, the movement in the matrix consists of steps that belong to fup; down; rightg, where up and down decrement and increment the row-index and right increments the column-index. <p> Theorem 6.2 Given a simple polygon P on n vertices, we can compute a geodesic farthest neighbor for each of its vertices in O (n) total time. Consequently, the geodesic diameter of P can also be computed in linear time. 6.2 The external farthest neighbors Agarwal et al. <ref> [1] </ref> gave an O (n log n) time algorithm for computing an external farthest neighbor for all vertices of a simple polygon. (The external farthest neighbors are computed using the external shortest path metric: the distance between two points is the length of a shortest path in the plane minus the
Reference: [2] <author> A. Aggarwal, M. Klawe, S. Moran, P. Shor, and R. Wilber. </author> <title> Geometric applications of a matrix searching algorithm. </title> <journal> Algorithmica, </journal> <volume> 2 </volume> <pages> 195-208, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Matrix-searching is the popular term for a technique introduced by Aggarwal et al. <ref> [2] </ref> for computing row-wise maxima in a totally monotone matrix. A matrix M is called totally monotone if M (i; k) &lt; M (i; l) =) M (j; k) &lt; M (j; l); for any i &lt; j and k &lt; l. Aggarwal et al. [2] discovered the importance of totally <p> introduced by Aggarwal et al. <ref> [2] </ref> for computing row-wise maxima in a totally monotone matrix. A matrix M is called totally monotone if M (i; k) &lt; M (i; l) =) M (j; k) &lt; M (j; l); for any i &lt; j and k &lt; l. Aggarwal et al. [2] discovered the importance of totally monotone matrices and showed that many problems in computational geometry can be formulated as finding maxima in a totally monotone matrix. <p> Thus, we can find for all vertices of U a corresponding farthest neighbor in V by computing the row-wise maxima in M . The main result of Aggarwal et al. <ref> [2] </ref> says that the row-wise maxima of a totally monotone n fi n matrix can be found with only O (n) comparisons and evaluations of the matrix entries. The matrix is defined implicitly|an entry is evaluated only when needed by the algorithm. <p> In this section, we introduce the problem, define terminology, and provide a brief review of the matrix-searching framework of Aggarwal et al. <ref> [2] </ref>. <p> We use the matrix-searching framework of Aggarwal et al. <ref> [2] </ref>, which depends crucially 3 on a matrix property called total monotonicity. The matrix M is totally monotone if M (i; k) &lt; M (i; l) =) M (j; k) &lt; M (j; l); (2) for any 1 i &lt; j p and 1 k &lt; l m. <p> We use the term matrix operation to describe either of these two steps. In general, the comparison step can always be performed in constant time, but the evaluation step can be more complicated. The main result of Aggarwal et al. <ref> [2] </ref> is the following lemma. Lemma 2.1 ([2]) The leftmost maximum in each row of a totally monotone n fi n matrix can be determined using O (n) matrix operations. If each matrix operation takes f (n) time, then the row-maxima problem can be solved in time O (nf (n)). <p> Our contribution in this paper is an algorithm that reduces f (n) to amortized O (1), giving an optimal linear-time algorithm. 2.2 The matrix-searching algorithm Our algorithm assumes some familiarity with the matrix-searching algorithm of <ref> [2] </ref>. This section gives a brief overview of the matrix-searching technique; however, the reader is encouraged to refer to the original paper for details. 4 The matrix-searching algorithm of [2] finds the leftmost maximum in each row of a totally monotone matrix using only a linear number of matrix operations. <p> O (1), giving an optimal linear-time algorithm. 2.2 The matrix-searching algorithm Our algorithm assumes some familiarity with the matrix-searching algorithm of <ref> [2] </ref>. This section gives a brief overview of the matrix-searching technique; however, the reader is encouraged to refer to the original paper for details. 4 The matrix-searching algorithm of [2] finds the leftmost maximum in each row of a totally monotone matrix using only a linear number of matrix operations. The algorithm is recursive; it proceeds in O (log n) phases. <p> The code above, which is essentially in the form given by Aggarwal et al. <ref> [2] </ref>, operates on a dynamic matrix: when a column is deleted, all columns to its right are renumbered. This property makes it somewhat difficult to identify which entries of the original matrix are examined. <p> The pred [fl] list that NEW-REDUCE returns encodes the reduced matrix A returned by REDUCE . In an implementation, MSEARCH and MFILL would also use pred pointers; however, in order to preserve a close parallel with the basic algorithm of <ref> [2] </ref>, we will use NEW-REDUCE only in the next subsection, and use REDUCE in the remainder of the paper. <p> 1 to dp=2e do C 2i1 C 2i2 ; if A (2i 1; j) &gt; max then max A (2i 1; j); endif Evaluate A (2i; C 2i ); (fl Extra work, for accounting purposes fl) endfor; Our algorithm has exactly the same form as that of Aggarwal et al. <ref> [2] </ref>; only the details of evaluating a matrix entry are different. In [2], each matrix evaluation takes constant time. In our case, the evaluations are trickier, and require a careful implementation of a shortest path algorithm. <p> 1; j) &gt; max then max A (2i 1; j); endif Evaluate A (2i; C 2i ); (fl Extra work, for accounting purposes fl) endfor; Our algorithm has exactly the same form as that of Aggarwal et al. <ref> [2] </ref>; only the details of evaluating a matrix entry are different. In [2], each matrix evaluation takes constant time. In our case, the evaluations are trickier, and require a careful implementation of a shortest path algorithm.
Reference: [3] <author> A. Aggarwal, S. Moran, P. Shor, and S. Suri. </author> <title> Computing the minimum visible vertex distance between two polygons. </title> <booktitle> In Proc. of 1st Workshop on Algorithms and Data Structures, </booktitle> <pages> pages 115-134. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Although this problem does not involve the shortest path metric in its formulation, we apply matrix-searching with shortest paths to get a conceptually simple algorithm for the problem that runs in optimal linear time. Previous linear-time algorithms were quite complicated <ref> [3, 4] </ref>. In a separate paper [12] we apply this result, along with some additional techniques, to obtain a linear-time algorithm for computing a shortest diagonal of a simple polygon, improving the previous O (n log n) algorithm. <p> The visibility constraint implies that some vertex-pairs are not legal candidates, and thus the associated distance matrix is not fully defined. Aggarwal et al. <ref> [3] </ref> gave a complicated linear-time algorithm for this problem that exploits the special structure of the legal entries in the matrix. A linear-time algorithm due to Amato [4] corrects minor problems with the algorithm of Aggarwal et al.
Reference: [4] <author> N. Amato. </author> <title> Computing the minimum visible vertex distance between two nonintersecting simple polygons. </title> <booktitle> In Proceedings of the Conference on Information Sciences and Systems, </booktitle> <volume> volume II, </volume> <pages> pages 800-805, </pages> <address> Princeton, NJ, </address> <year> 1992. </year>
Reference-contexts: Although this problem does not involve the shortest path metric in its formulation, we apply matrix-searching with shortest paths to get a conceptually simple algorithm for the problem that runs in optimal linear time. Previous linear-time algorithms were quite complicated <ref> [3, 4] </ref>. In a separate paper [12] we apply this result, along with some additional techniques, to obtain a linear-time algorithm for computing a shortest diagonal of a simple polygon, improving the previous O (n log n) algorithm. <p> Aggarwal et al. [3] gave a complicated linear-time algorithm for this problem that exploits the special structure of the legal entries in the matrix. A linear-time algorithm due to Amato <ref> [4] </ref> corrects minor problems with the algorithm of Aggarwal et al. Amato's algorithm can also be implemented on a CREW PRAM in time O (log n) using O (n) processors. However, like the algorithm of Aggarwal et al., Amato's algorithm is quite complicated.
Reference: [5] <author> B. Chazelle. </author> <title> Triangulating a simple polygon in linear time. </title> <journal> Discrete and Computational Geometry, </journal> <volume> 6 </volume> <pages> 485-524, </pages> <year> 1991. </year>
Reference-contexts: We first triangulate the polygon in linear time using Chazelle's algorithm <ref> [5] </ref>, and then build the shortest path trees T a and T b also in linear time using an algorithm of Guibas et al. [7] or Hershberger and Snoeyink [11].
Reference: [6] <author> L. Guibas and J. Hershberger. </author> <title> Optimal shortest path queries in a simple polygon. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 39(2) </volume> <pages> 126-152, </pages> <year> 1989. </year>
Reference-contexts: Observe, however, that unlike the Euclidean distance, the geodesic distance in general cannot be computed in constant time. Unless one precomputes all the distances, which requires too much preprocessing, the best data structure for computing shortest path distances is one due to Guibas and Hershberger <ref> [6, 10] </ref>, which achieves f (n) = O (log n). (After an O (n) time and space preprocessing step, this data structure computes the geodesic distance between two arbitrary points of a simple polygon in O (log n) time; n denotes the number of vertices in the polygon.) A straightforward combination <p> If each matrix operation takes f (n) time, then the row-maxima problem can be solved in time O (nf (n)). If we use shortest path queries <ref> [6, 10] </ref> to compute the entries of M , then f (n) = O (log n), and we get an O (n log n) time algorithm. <p> ; p n in counterclockwise order around the boundary. 6.1 The geodesic diameter and farthest neighbors This section describes a linear-time algorithm for computing a farthest neighbor for each vertex of P under the shortest path metric; the previous best time bound for this problem was O (n log n) <ref> [6, 16] </ref>.
Reference: [7] <author> L. Guibas, J. Hershberger, D. Leven, M. Sharir, and R. Tarjan. </author> <title> Linear time algorithms for visibility and shortest path problems inside triangulated simple polygons. </title> <journal> Algorithmica, </journal> <volume> 2 </volume> <pages> 209-233, </pages> <year> 1987. </year> <month> 26 </month>
Reference-contexts: We first triangulate the polygon in linear time using Chazelle's algorithm [5], and then build the shortest path trees T a and T b also in linear time using an algorithm of Guibas et al. <ref> [7] </ref> or Hershberger and Snoeyink [11]. Lemma 3.1 ([7, 11]) After linear-time preprocessing, the distances d (x; a) and d (x; b), for any vertex x 2 P , can be computed in O (1) time. <p> Since the farthest neighbors p j and p k can be computed in linear time, using shortest path trees <ref> [7, 11] </ref>, Lemma 6.1 gives a linear-time reduction from the farthest neighbors problem of P to three instances of the farthest neighbors problem between disjoint chains.
Reference: [8] <author> L. Guibas, J. Hershberger, and J. Snoeyink. </author> <title> Compact interval trees: A data structure for convex hulls. </title> <journal> International Journal of Computational Geometry & Applications, </journal> <volume> 1(1) </volume> <pages> 1-22, </pages> <year> 1991. </year>
Reference-contexts: Each internal node represents the subpath of its leaf descendants; it stores the edge linking its left and right subtrees' paths. Our tangent-finding algorithm starts its search from two start vertices u s 2 F (u) and v s 2 F (v). The algorithm is a standard binary search <ref> [8, 14] </ref>, with a modified search 11 sequence.
Reference: [9] <author> D. Harel and R. E. Tarjan. </author> <title> Fast algorithms for finding nearest common ancestors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 13(2) </volume> <pages> 338-355, </pages> <year> 1984. </year>
Reference-contexts: We use the linear-time algorithm of Harel and Tarjan <ref> [9] </ref> or that of Schieber and Vishkin [15] to preprocess our trees for lowest common ancestor queries.
Reference: [10] <author> J. Hershberger. </author> <title> A new data structure for shortest path queries in a simple polygon. </title> <journal> Information Processing Letters, </journal> <volume> 38 </volume> <pages> 231-235, </pages> <year> 1991. </year>
Reference-contexts: Observe, however, that unlike the Euclidean distance, the geodesic distance in general cannot be computed in constant time. Unless one precomputes all the distances, which requires too much preprocessing, the best data structure for computing shortest path distances is one due to Guibas and Hershberger <ref> [6, 10] </ref>, which achieves f (n) = O (log n). (After an O (n) time and space preprocessing step, this data structure computes the geodesic distance between two arbitrary points of a simple polygon in O (log n) time; n denotes the number of vertices in the polygon.) A straightforward combination <p> If each matrix operation takes f (n) time, then the row-maxima problem can be solved in time O (nf (n)). If we use shortest path queries <ref> [6, 10] </ref> to compute the entries of M , then f (n) = O (log n), and we get an O (n log n) time algorithm.
Reference: [11] <author> J. Hershberger and J. Snoeyink. </author> <title> Computing minimum length paths of a given homotopy class. </title> <booktitle> In Proceedings of the 2nd Workshop on Algorithms and Data Structures, </booktitle> <pages> pages 331-342. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year> <note> Lecture Notes in Computer Science 519. </note>
Reference-contexts: We first triangulate the polygon in linear time using Chazelle's algorithm [5], and then build the shortest path trees T a and T b also in linear time using an algorithm of Guibas et al. [7] or Hershberger and Snoeyink <ref> [11] </ref>. Lemma 3.1 ([7, 11]) After linear-time preprocessing, the distances d (x; a) and d (x; b), for any vertex x 2 P , can be computed in O (1) time. <p> Since the farthest neighbors p j and p k can be computed in linear time, using shortest path trees <ref> [7, 11] </ref>, Lemma 6.1 gives a linear-time reduction from the farthest neighbors problem of P to three instances of the farthest neighbors problem between disjoint chains.
Reference: [12] <author> J. Hershberger and S. Suri. </author> <title> Finding a shortest diagonal of a simple polygon in linear time. </title> <type> Manuscript, </type> <year> 1993. </year>
Reference-contexts: Although this problem does not involve the shortest path metric in its formulation, we apply matrix-searching with shortest paths to get a conceptually simple algorithm for the problem that runs in optimal linear time. Previous linear-time algorithms were quite complicated [3, 4]. In a separate paper <ref> [12] </ref> we apply this result, along with some additional techniques, to obtain a linear-time algorithm for computing a shortest diagonal of a simple polygon, improving the previous O (n log n) algorithm. <p> That result will be reported elsewhere <ref> [12] </ref>.
Reference: [13] <author> O. Marcotte and S. Suri. </author> <title> Fast matching algorithms for points on a polygon. </title> <journal> SIAM Journal on Computing, </journal> <volume> 20 </volume> <pages> 405-422, </pages> <year> 1991. </year>
Reference-contexts: This improves the complexity of computing an optimal matching on the vertices of a polygon from O (n log 2 n) to O (n log n) <ref> [13] </ref>, matching the best bound for a convex polygon. Our paper is organized in seven main sections. In Section 2, we formulate the key problem of our paper, and provide a brief summary of the matrix searching technique. <p> The problem of computing a weighted nearest neighbor for each vertex of U in V is equivalent to computing row-wise minima of M . This problem arises in an algorithm of Marcotte and Suri <ref> [13] </ref> for solving the Euclidean matching problem for points on a polygon. That algorithm finds an optimal matching for the vertices of a simple polygon, under the constraint that each matching edge lies inside the polygon. <p> The algorithm is based on divide-and-conquer, and the dominating term in the conquer step is the complexity of solving the weighted 24 nearest neighbors problem as defined above. Our algorithm of Section 4 solves the row--minima problem for M in linear time, thus improving the result in <ref> [13] </ref> from O (n log 2 n) to O (n log n). Theorem 6.4 The algorithm of Marcotte and Suri [13] for computing an optimal matching of the vertices of a simple polygon can be implemented in O (n log n) time, where n denotes the number of vertices of the <p> Our algorithm of Section 4 solves the row--minima problem for M in linear time, thus improving the result in <ref> [13] </ref> from O (n log 2 n) to O (n log n). Theorem 6.4 The algorithm of Marcotte and Suri [13] for computing an optimal matching of the vertices of a simple polygon can be implemented in O (n log n) time, where n denotes the number of vertices of the polygon. 6.4 Closest visible vertex pair between two chains Given two disjoint polygonal chains U and V , the closest
Reference: [14] <author> M. Overmars and J. van Leeuwen. </author> <title> Maintenance of configurations in the plane. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 23 </volume> <pages> 166-204, </pages> <year> 1981. </year>
Reference-contexts: Each internal node represents the subpath of its leaf descendants; it stores the edge linking its left and right subtrees' paths. Our tangent-finding algorithm starts its search from two start vertices u s 2 F (u) and v s 2 F (v). The algorithm is a standard binary search <ref> [8, 14] </ref>, with a modified search 11 sequence.
Reference: [15] <author> B. Schieber and U. Vishkin. </author> <title> On finding lowest common ancestors: Simplification and parallelization. </title> <booktitle> In Proceedings of the Third Aegean Workshop on Computing, </booktitle> <pages> pages 111-123. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year> <note> Lecture Notes in Computer Science 319. </note>
Reference-contexts: We use the linear-time algorithm of Harel and Tarjan [9] or that of Schieber and Vishkin <ref> [15] </ref> to preprocess our trees for lowest common ancestor queries.
Reference: [16] <author> S. Suri. </author> <title> Computing geodesic furthest neighbors in simple polygons. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 39 </volume> <pages> 220-235, </pages> <year> 1989. </year> <month> 27 </month>
Reference-contexts: ; p n in counterclockwise order around the boundary. 6.1 The geodesic diameter and farthest neighbors This section describes a linear-time algorithm for computing a farthest neighbor for each vertex of P under the shortest path metric; the previous best time bound for this problem was O (n log n) <ref> [6, 16] </ref>. <p> The geodesic diameter of P , denoted D (P ), is the maximum length of a shortest path in P : D (P ) = maxfd (x; y) j x; y 2 P g: The diameter is always realized by two vertices of P <ref> [16] </ref>, and so finding it is a special case of the problem of computing farthest neighbors for all vertices of P . <p> We use a lemma from <ref> [16] </ref> to decompose the all-farthest neighbor problem into at most three instances of maxima-finding in totally monotone matrices. <p> Figure 9 shows a schematic diagram of this construction. The following lemma is established in <ref> [16] </ref>. 23 Lemma 6.1 ([16]) Every vertex of the chain P i has a farthest neighbor in chain Q i , for i = 1; 2; 3.
References-found: 16

