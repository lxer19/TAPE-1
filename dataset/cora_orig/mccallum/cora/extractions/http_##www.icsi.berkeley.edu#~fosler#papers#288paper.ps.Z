URL: http://www.icsi.berkeley.edu/~fosler/papers/288paper.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~fosler/papers/
Root-URL: http://www.icsi.berkeley.edu
Title: Unification Grammars and Semantics within the Berkeley Restaurant Project  
Author: Dan Gildea Eric Fosler 
Date: December 14, 1994  
Abstract: In this paper we describe our CS 288 class project, implementation of a unification parser within the Berke-ley Restaurant Project. The system is a work in progress; what is described here is a snapshot of the current system. We also present several ideas for improvements we have had while working on the project. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Gerald Gazdar and Chris Mellish. </editor> <booktitle> Natural Language Processing in LISP. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The idea behind restriction is to use other features in addition to the "cat" feature for parsing, which would allow us to perform this filtering. Gadzar and Mellish <ref> [1] </ref> give an overview of how this can be done. Another interesting possibility that we may consider is probabilistic unification. Instead of having features unify binarily, we could have them unify with a certain probability.
Reference: [2] <author> Chung Hee Hwang and Lenhart K. Schubert. EL: </author> <title> A formal, yet natural, comprehensive knowledge representation. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI-93), </booktitle> <pages> pages 676-682. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: [3] <author> Daniel Jurafsky, Chuck Wooters, Gary Tajchman, Jonathan Segal, Andreas Stolcke, Eric Fosler, and Nelson Morgan. </author> <title> The Berkeley restaurant project. </title> <booktitle> In ICSLP-94, </booktitle> <address> Yokohama, Japan, </address> <year> 1994. </year>
Reference-contexts: Since the utility of on-line unification is grammar-specific, the tradeoff between the amount of pruning and the overhead of on-line unification must be taken into account. The bottom up chart parser uses a probabalistic version of the CKY algorithm <ref> [3] </ref>, whereas the top down parser is an implementation of probablistic extensions to the Earley parser.[6] The unification DAGs ornament each arc of the chart parser.
Reference: [4] <author> Daniel Jurafsky, Chuck Wooters, Gary Tajchman, Jonathan Segal, Andreas Stolcke, Eric Fosler, and Nelson Morgan. </author> <title> Using a stochastic contex-free grammar as a language model for speech recognition. </title> <booktitle> In IEEE ICASSP-95, </booktitle> <year> 1995. </year> <note> to appear. </note>
Reference: [5] <author> Peter Norvig. </author> <booktitle> Paradigms of Artificial Intelligence Programming. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1992. </year>
Reference-contexts: = INFINITIVAL) ((0 head subject cat) = NP) ((0 head sem category) = want)) The first valence structure occurs in sentences like "I want Chinese food," the second structure is found in "I want to eat Thai food." The structure of the grammar uses Peter Norvig's formalism for unification grammars <ref> [5] </ref>. The unification grammar lets features do much of the work performed by ad-hoc syntactic constituents and rules in the previous grammar. Because of this, our unification grammar is more soundly based in linguistic theory than the previous grammar.
Reference: [6] <author> Andreas Stolcke. </author> <title> An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. </title> <type> Technical Report TR-93-065, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <month> November </month> <year> 1993. </year> <note> To appear in Computational Linguistics. </note>
Reference: [7] <author> Andreas Stolcke and Jonathan Segal. </author> <title> Precise n-gram probabilities from stochastic context-free grammars. </title> <booktitle> In Proceedings of the 32nd ACL, </booktitle> <pages> pages 74-79, </pages> <address> Las Cruces, NM, </address> <year> 1994. </year>
Reference-contexts: Another issue, more on the theoretical side, is the generation of bigrams from the UG for use as a language model within the speech recognizer. Jurafsky et al.[4] show that smoothing bigrams with a SCFG improved performance by 14% relative error. Stol-cke and Segal <ref> [7] </ref> show how to generate a bigram from a SCFG directly, by computing its characteristic n-gram in closed form. The algorithm computes bigram probabilities by solving a system of linear equations derived from the probabilities of the grammar rules.
Reference: [8] <author> V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, J. Polifroni, and S. Seneff. </author> <title> Integration of speech recognition and natural language processing in the MIT VOYAGER system. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 713-716, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
References-found: 8

