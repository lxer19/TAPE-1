URL: http://www.isi.edu/~faber/pubs/infocom96.ps
Refering-URL: http://www.isi.edu/atomic2/NFS.html
Root-URL: http://www.isi.edu
Email: faber@isi.edu  
Title: Adapting Network File System Data Transfer Protocols to High Speed Networks: the Design of the
Author: Theodore Faber 
Address: 4676 Admiralty Way Marina del Rey, CA 90292  
Affiliation: University of Southern California/ Information Sciences Institute  
Note: DRAFT: please do not distribute!  
Abstract: This paper presents studies of the server/client data transfer rates of two commercial network file systems, the Andrew File System(AFS) and Suns Network File System (NFS). The purpose of these studies is to develop a design for the ATOMIC file server, a high speed file server for the 640 Mbps ATOMIC LAN, designed around a high-speed solid-state disk. The ATOMIC file server is intended to transfer data from client to server at the full bandwidth of the network. AFS and NFS support file data transfer rates of less than one third the rates of other reliable data transfer protocols on the ATOMIC network. This is due to the use of blocking serial remote procedure calls, and attempts to pipeline them through the use of multiple processes, which incur high context switching overhead. This paper presents an alternative protocol for the ATOMIC file server that is based on single-process pipelined remote procedure calls, and aggressive data transfer. The goal of this enhanced protocol is to improve file system performance by transferring file data between client and server at the full bandwidth of the ATOMIC LAN. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert Felderman, Annette DeSchon, Danny Cohen, and Gregory Finn, </author> <title> Atomic: A High Speed Local Communication Architecture, </title> <journal> Journal of High Speed Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 1-29, </pages> <year> 1994. </year>
Reference-contexts: The ATOMIC Environment The ATOMIC network is a local area network based on the Mosaic supercomputer designed at CalTech. The network was designed and implemented at USC/ISI <ref> [1] </ref>, and is currently being produced commercially by Myricom [2]. All ATOMIC network components used in this research are Myricom products. The ATOMIC-2 research group at USC/ISI is installing an ATOMIC network for production use in a distributed computing environment [12].
Reference: [2] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Selzovic, Wen-King Su, and Myricom, Inc., Myrinet: </author> <title> A Gigabit-per-second Local Area Network, </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 29-36, </pages> <publisher> IEEE, </publisher> <month> February </month> <year> 1995. </year>
Reference-contexts: The ATOMIC Environment The ATOMIC network is a local area network based on the Mosaic supercomputer designed at CalTech. The network was designed and implemented at USC/ISI [1], and is currently being produced commercially by Myricom <ref> [2] </ref>. All ATOMIC network components used in this research are Myricom products. The ATOMIC-2 research group at USC/ISI is installing an ATOMIC network for production use in a distributed computing environment [12]. Among the issues of interest are building an ATOMIC/ATM gateway, high speed authentication algorithms, and high performance protocols.
Reference: [3] <author> Sun Microsystems, Inc., </author> <title> Network Filesystem Specification, </title> <institution> RFC-1094, DDN Network Information Center, SRI International, </institution> <address> Menlo Park, CA, March 1, </address> <year> 1989. </year>
Reference-contexts: This requires a dramatic reduction in the cost of a cache miss. This paper presents studies of the cold cache throughput of two commercial file systems, Suns Network File System (NFS) <ref> [3, 4] </ref> and Transarcs Andrew File System (AFS) [5,6]. The purpose 1. This work is supported by the Advanced Research Projects Agency through Ft. Huachuca contract #DABT63-93-C-0062 entitled Netstation Architecture and Advanced Atomic Network.
Reference: [4] <author> X/Open Company, Ltd., </author> <title> X/Open CAE Specification: Protocols for X/Open Internetworking: </title> <publisher> XNFS, X/Open Company, Ltd., Apex Plaza, </publisher> <address> Forbury Road, Reading Berkshire, RG1 1AX, UK, </address> <year> 1991. </year>
Reference-contexts: This requires a dramatic reduction in the cost of a cache miss. This paper presents studies of the cold cache throughput of two commercial file systems, Suns Network File System (NFS) <ref> [3, 4] </ref> and Transarcs Andrew File System (AFS) [5,6]. The purpose 1. This work is supported by the Advanced Research Projects Agency through Ft. Huachuca contract #DABT63-93-C-0062 entitled Netstation Architecture and Advanced Atomic Network.
Reference: [5] <institution> Transarc Corporation, AFS Systems Administrators Guide, Transarc Corporation, </institution> <address> Pittsburgh, Pa., </address> <year> 1993. </year>
Reference-contexts: This provides the same cache-to-cache behavior that was studied in NFS. 4. 1 AFS Overview The Andrew File System <ref> [5, 6] </ref> is another distributed file system with a client/server model.
Reference: [6] <author> J. H. Howard, M. L. Lazar, S. G. Menees, D. A. Nichols, M. Satyanarayanan, R. N. Sidebotham, and M. J. West, </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 6(1), </volume> <pages> pp. 51-81, </pages> <publisher> ACM, </publisher> <month> February </month> <year> 1988. </year>
Reference-contexts: This provides the same cache-to-cache behavior that was studied in NFS. 4. 1 AFS Overview The Andrew File System <ref> [5, 6] </ref> is another distributed file system with a client/server model.
Reference: [7] <author> Michael Nelson, Brent Welch, John Osterhout, </author> <title> Caching in the Sprite Network File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6, no. 1, </volume> <pages> pp. 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference: [8] <author> Michael Dahlin, Rudolph Wang, Thomas Anderson, and David Patterson, </author> <title> Cooperative Caching: Using Remote Client Memory to Improve File System Performance, </title> <booktitle> Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994. </year>
Reference: [9] <author> Michael Dahlin, Clifford Mather, Rudolph Wang, Thomas Anderson, and David Patterson, </author> <title> A Quantitative Analysis of Cache Policies for Scalable Network File Systems, </title> <booktitle> Proceedings of the SIGMETRICS, </booktitle> <year> 1994. </year>
Reference: [10] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French, Extensible File Systems (ELFS): An Object-Oriented Approach to High Performance File I/O, </title> <booktitle> Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <month> October </month> <year> 1994. </year> <title> DRAFT: please do not distribute! </title>
Reference: [11] <author> Michael N. Nelson, Yousef A. Khalidi, Peter W. Madany, </author> <title> The Spring File System, </title> <institution> Sun Microsystems Technical Report SMLI TR-93-10, </institution> <month> February </month> <year> 1993. </year>
Reference: [12] <institution> The Atomic-2 Project, &lt;http://www.isi.edu/atomic2&gt;. </institution>
Reference-contexts: The network was designed and implemented at USC/ISI [1], and is currently being produced commercially by Myricom [2]. All ATOMIC network components used in this research are Myricom products. The ATOMIC-2 research group at USC/ISI is installing an ATOMIC network for production use in a distributed computing environment <ref> [12] </ref>. Among the issues of interest are building an ATOMIC/ATM gateway, high speed authentication algorithms, and high performance protocols. This paper discusses the groups research in providing remote file access at network bandwidth.
Reference: [13] <author> H. Xu, T. Fisher, </author> <title> Improving PVM Performance Using the ATOMIC User-Level Protocol, </title> <booktitle> to appear in the proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: API is the bandwidth Table 1: ATOMIC LAN user-user bandwidth Protocol Bandwidth (Mb/sec) ATP (reliable) 162 UDP 150 DRAFT: please do not distribute! that memory mapped I/O direct to the host interface can achieve, and ATP is the reliable protocol tuned to the Myricom hardware developed by Xu and Fisher <ref> [13] </ref> (the values given in Table 2 are more recent measurements than in [13] and show the results of tuning the protocols). The unreliable protocols can transfer data at 150 Mb/sec, and data can be moved reliably at 140 Mb/sec on the ATOMIC LAN. <p> ATP (reliable) 162 UDP 150 DRAFT: please do not distribute! that memory mapped I/O direct to the host interface can achieve, and ATP is the reliable protocol tuned to the Myricom hardware developed by Xu and Fisher <ref> [13] </ref> (the values given in Table 2 are more recent measurements than in [13] and show the results of tuning the protocols). The unreliable protocols can transfer data at 150 Mb/sec, and data can be moved reliably at 140 Mb/sec on the ATOMIC LAN.
Reference: [14] <author> Sun Microsystems, Inc., </author> <title> Remote Procedure Call Specification, </title> <institution> RFC-1057, DDN Network Information Center, SRI International, </institution> <address> Menlo Park, CA, </address> <month> June 1, </month> <year> 1988. </year>
Reference-contexts: NFS is designed to allow users to share files across a network simply, preserving UNIX file semantics as closely as possible. Client workstations can access files on a server workstation via Suns RPC protocol <ref> [14] </ref>, transported via UDP 1 . Although the RPC specification [14] does not require RPC to be serial, the implementation in SunOS 4.1.3 is serial. NFS is implemented as a virtual file system [15,16] under SunOS 4.1.3. The behavior described in this section is that of NFS version 2. <p> NFS is designed to allow users to share files across a network simply, preserving UNIX file semantics as closely as possible. Client workstations can access files on a server workstation via Suns RPC protocol <ref> [14] </ref>, transported via UDP 1 . Although the RPC specification [14] does not require RPC to be serial, the implementation in SunOS 4.1.3 is serial. NFS is implemented as a virtual file system [15,16] under SunOS 4.1.3. The behavior described in this section is that of NFS version 2.
Reference: [15] <author> S.R. Kleiman, Vnodes: </author> <title> An Architecture for Multiple File System Types in Sun UNIX, </title> <booktitle> USENIX Association Conference Proceedings, </booktitle> <pages> pp. 238-247, </pages> <month> June </month> <year> 1986. </year>
Reference: [16] <author> D. Rosenthal, </author> <title> Evolving the Vnode Interface, </title> <booktitle> USENIX Association Conference Proceedings, </booktitle> <pages> pp. 107-118, </pages> <month> June </month> <year> 1990. </year>
Reference: [17] <author> Sun Microsystems, Inc., NFS: </author> <title> Network File System Version 3 Protocol Specification, Sun Microsystems, </title> <publisher> Inc., </publisher> <address> Mountain View, CA, </address> <month> Feb 16, </month> <year> 1994. </year>
Reference-contexts: In other experiments, the server did not write data to the disk at all to study the limits inherent in the NFS implementation. To use NFS safely with asynchronous server writes requires an extension to the NFS protocol, such as the extension in the NFS 3.0 specification <ref> [17] </ref>. NFS 3.0 adds a commit action that forces the asynchronously written blocks to the server disk. Sending an RPC request is less straightforward than mentioned above, and the details are important to understanding NFS bottlenecks.
Reference: [18] <author> R. Braden, </author> <title> Extending TCP for Transactions, </title> <type> RFC 1379, </type> <month> November </month> <year> 1992. </year>
Reference-contexts: It is this request/ response model that we are interested in using, and therefore may investigate other methods of implementation, such as Transaction TCP <ref> [18] </ref>, should RPC not prove satisfactory. The remaining discussion assumes RPC will be the model used. The ATOMIC file server adopts a single-process pipelined RPC communication model. RPC is used to communicate between server and client, but many simultaneous asynchronous RPCs may be outstanding to a single process.
Reference: [19] <author> J. Osterhout, H. Da Costa, D. Harrison, J. Kunze, M. Kupfer, J. Thompson, </author> <title> A trace driven analysis of the 4.2 BSD UNIX file system, </title> <booktitle> in Proceedings of the 10th Symposium on Operating Systems Principles, </booktitle> <address> Orcas Island, Wash., </address> <pages> pp. 15-24, </pages> <year> 1985. </year>
Reference-contexts: This avoids the extra overhead of establishing a connection for each RPC request. The ATOMIC file server will aggressively read ahead and cache blocks from files as they are opened for reading, because several studies, e.g. <ref> [19] </ref>, have shown that files are frequently read in their entirety. As a file is written, it will be cached and written through to the server, although, in accordance with our loose consistency policy, the writes may be done asynchronously, via pipe DRAFT: please do not distribute! lined RPC.
Reference: [20] <author> L. McVoy and S. Kleiman, </author> <title> Extent-like Performance from a UNIX File System, </title> <booktitle> Proceedings of USENIX, </booktitle> <pages> pp. 33-44, </pages> <month> Winter </month> <year> 1991 </year>
Reference-contexts: This aggressive prefetching of file blocks is similar to the clustering work done by Mc-Voy and Kleiman on Suns UFS, a local file system <ref> [20] </ref>. They reported doubling the bandwidth of their system. Even in a system where caching provides the bulk of the performance, this aggressive prefetching strategy, and increased data transfer bandwidth will improve performance.
References-found: 20

