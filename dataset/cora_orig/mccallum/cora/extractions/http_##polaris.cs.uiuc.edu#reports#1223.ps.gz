URL: http://polaris.cs.uiuc.edu/reports/1223.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: The Design of Automatic Parallelizers for Symbolic and Numeric Programs  
Author: Williams Ludwell Harrison III and Zahira Ammarguellat 
Date: May 7, 1992  
Abstract-found: 0
Intro-found: 1
Reference: [Amm90] <author> Zahira Ammarguellat. </author> <title> Normalization of program control flow. </title> <type> Technical Report 885, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: A program arrives to this form by three phases of rewriting. First, the program is translated from a source language (Scheme, C, Fortran) into an unstructured dialect of the intermediate form. From here, a radical control-flow normalization <ref> [Amm90] </ref> leaves only properly nested ifs, single-entry, single-exit while loops, and begin forms as the control structures within a procedure.
Reference: [Ban76] <author> Uptal D. Banerjee. </author> <title> Data dependence in ordinary programs. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1976. </year>
Reference-contexts: In particular, the analysis cannot benefit from even the simplest test for independence among subscript expressions <ref> [Ban76, Ban79] </ref>. <p> This allows one to extract medium- and fine-grained parallelism from a traversal of a data structure that modifies its constituent objects, when the traversal follows a linear path through the structure. Third, it employs a gcd test <ref> [Ban76, Ban79] </ref> to establish the independence of references that fall within a single block of storage. What results is a three-level analysis of side-effects and dependences.
Reference: [Ban79] <author> Uptal D. Banerjee. </author> <title> Speedup of Ordinary Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: In particular, the analysis cannot benefit from even the simplest test for independence among subscript expressions <ref> [Ban76, Ban79] </ref>. <p> This allows one to extract medium- and fine-grained parallelism from a traversal of a data structure that modifies its constituent objects, when the traversal follows a linear path through the structure. Third, it employs a gcd test <ref> [Ban76, Ban79] </ref> to establish the independence of references that fall within a single block of storage. What results is a three-level analysis of side-effects and dependences.
Reference: [BC86] <author> Michael Burke and Ronald G. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN 1986 Symposium on Compiler Construction, </booktitle> <pages> pages 162-175. </pages> <institution> Association for Computing Machinery, </institution> <month> July </month> <year> 1986. </year>
Reference-contexts: Interprocedural analysis, if performed at all, was ordinarily done to strengthen dependence analysis of loops, to infer aliasing among parameters, and to fold constants across procedure boundaries, but seldom with the goal of extracting high-level parallelism from a Fortran program. (There are exceptions; see <ref> [Tri84, BC86] </ref>.) For this reason, such compilers are typically quite capable of extracting fine-grained parallelism, but do less well in extracting large-grained, high-level parallelism.
Reference: [CF89] <author> R. Cartwright and M. Felleisen. </author> <title> The semantics of program dependence. </title> <booktitle> In Proceedings of the 1989 ACM SIGPLAN Conference on Programming Language Design and Implementation. ACM, </booktitle> <publisher> ACM Press, </publisher> <month> jun </month> <year> 1989. </year>
Reference-contexts: In this form, the only control structures in the program are begins, properly nested ifs, and procedure calls. Recent work on intermediate forms for parallelizing compilers has put forth several, similar graph-based representations that incorporate control and data dependences in a single representation of the program. See <ref> [FOW87, CF89] </ref>. These representations have the advantage that updates to the program representation automatically update the control and dependence graphs.
Reference: [Cho90] <author> Jhy-Herng Chow. </author> <title> Run-time support for automatically parallelized lisp programs. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: When the nesting of these parallel threads is judged to be sufficient to saturate the machine (this judgement is made according to one of several experimental strategies; see <ref> [Cho90] </ref>), the sequential versions of procedures are invoked, so that no further creation of parallel activity will occur beyond that nesting depth. A traditional microtasking environment [Cra82, EHJP90] associates a stack with every processor. <p> The last processor to finish work on the parallel loop will return its stack to the pool, seize the stack abandoned by the initiating processor, and execute the continuation of the loop. This scheme is described in <ref> [Cho90] </ref>. 5 (define copy-integer (lambda (n) (copy-integer-aux 1 n))) (define copy-integer-aux (lambda (i s) (if (&gt;= i s) (copy-integer-aux (1+ i) s)))) (define copy (lambda (x) (if (atom? x) (if (integer? x) (copy-integer x) nil) (cons (copy (car x)) (copy (cdr x)))))) (write (copy (read))) As an example of the transformations <p> The program achieves a maximum speedup of around 6:5, at a nesting depth of about 4, with 8 processors active. We are experimenting with a number of strategies for automatically selecting between the parallel and sequential procedure versions during execution; the results of these experiments are reported in <ref> [Cho90] </ref>. 10 2.2 Shortcomings While Parcel was successful in a number of respects, including those mentioned above, it also has a number of shortcomings, in its interprocedural analysis, its parallelizing transformations, and its run-time system. 2.2.1 Interprocedural analysis Intrinsic procedures Scheme has a fairly large collection of built-in (intrinsic) procedures, including
Reference: [Cra82] <institution> Cray Research, Mendota Heights, MN. Cray X-MP Series Mainframe Reference Manual (HR-0032), </institution> <year> 1982. </year>
Reference-contexts: A traditional microtasking environment <ref> [Cra82, EHJP90] </ref> associates a stack with every processor. When a processor initiates a parallel loop, the continuation of that loop is on its stack; it must wait for all iterations of the initiated loop to terminate, before it executes the continuation.
Reference: [EHJP90] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> Cedar fortran and its compiler. </title> <type> Technical Report 966, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> jan </month> <year> 1990. </year>
Reference-contexts: A traditional microtasking environment <ref> [Cra82, EHJP90] </ref> associates a stack with every processor. When a processor initiates a parallel loop, the continuation of that loop is on its stack; it must wait for all iterations of the initiated loop to terminate, before it executes the continuation.
Reference: [FOW87] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <year> 1987. </year>
Reference-contexts: In this form, the only control structures in the program are begins, properly nested ifs, and procedure calls. Recent work on intermediate forms for parallelizing compilers has put forth several, similar graph-based representations that incorporate control and data dependences in a single representation of the program. See <ref> [FOW87, CF89] </ref>. These representations have the advantage that updates to the program representation automatically update the control and dependence graphs.
Reference: [Har89] <author> Williams Ludwell Harrison III. </author> <title> The interprocedural analysis and automatic parallelization of scheme programs. Lisp and Symbolic Computation: </title> <journal> an International Journal, </journal> <volume> 2(3), </volume> <year> 1989. </year>
Reference-contexts: 1 Introduction Parcel is a compiler and run-time system that automatically parallelizes a Scheme program for execution on a shared-memory multiprocessor; it is described at length in <ref> [Har89] </ref>. <p> (list-of-sums (cdr l)))))) (define sum (lambda (l ctr) (if (null l) 0 (begin (sum (cdr l) ctr) (ctr))))) (define counter (lambda (x) (lambda () (set! x (1+ x)) x))) computation that contains the lifetime of the object affected. (The interested reader will find this analysis described and proven correct in <ref> [Har89] </ref>.) From this information, Parcel constructs dependence graphs of individual procedures, and thereafter parallelizes their control-flow structures regardless of whether they are "outermost" or "innermost" procedures (indeed, such a characterization may be meaningless, as a procedure may be invoked in many different contexts, as well as recursively). <p> This lifetime information can also be used to guide the placement of objects in the hierarchical shared memory of a machine like Cedar. These ideas are developed in detail in <ref> [Har89] </ref>. As an example of this analysis, consider the program in Figure 1. The procedure counter returns an object (a closure that increments a free variable x and returns its updated value).
Reference: [Tri84] <author> Remi Triolet. </author> <title> Contributions to Automatic Parallelization of Fortran Programs with Procedure Calls. </title> <type> PhD thesis, </type> <institution> University of Paris VI (I.P.), </institution> <year> 1984. </year>
Reference-contexts: Interprocedural analysis, if performed at all, was ordinarily done to strengthen dependence analysis of loops, to infer aliasing among parameters, and to fold constants across procedure boundaries, but seldom with the goal of extracting high-level parallelism from a Fortran program. (There are exceptions; see <ref> [Tri84, BC86] </ref>.) For this reason, such compilers are typically quite capable of extracting fine-grained parallelism, but do less well in extracting large-grained, high-level parallelism.
References-found: 11

