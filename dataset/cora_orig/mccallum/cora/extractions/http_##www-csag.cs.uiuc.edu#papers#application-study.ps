URL: http://www-csag.cs.uiuc.edu/papers/application-study.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: Evaluating High Level Parallel Programming Support for Irregular Applications in ICC++  
Author: Andrew A. Chien Julian Dolby Bishwaroop Ganguly Vijay Karamcheti Xingbin Zhang 
Note: The authors can be contacted by e-mail at concert@cs.uiuc.edu, by phone at (217) 244-7116 and by FAX at (217) 333-3501  
Address: 1304 W. Springfield Avenue Urbana, IL 61801  
Affiliation: Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(N log N) force calculation algorithm. </title> <type> Technical report, </type> <institution> The Institute for Advanced Study, Princeton, </institution> <address> New Jersey, </address> <year> 1986. </year>
Reference-contexts: We summarize the description for each application by discussing the extent to which support in the Concert system helps manage the programmer concerns. 4.1 Barnes-Hut The Barnes-Hut code computes the interactions of a system of particles in 3-D using the Barnes-Hut hierarchical N-body method <ref> [1] </ref>. 4.1.1 Algorithmic Structure The Barnes-Hut method exploits the physical insight that the interactive force between two particles decreases at a rate proportional to the square of their distance to reduce computational complexity. <p> We use a sequential algorithm described by Jones [20] as our starting point. 25 4.6.1 Algorithmic Structure Phylogenetic trees are constructed by considering the characters exhibited by the species. Given a set of species, with each species u represented as a vector of character values, u <ref> [1] </ref>; : : :; u [c max ] (c max is the maximum number of characters to be considered), a character is compatible with a phylogenetic tree if no value for that character arises more than once in any path in the tree.
Reference: [2] <author> Peter Beckman, Dennis Gannon, and Elizabeth Johnson. </author> <title> Portable parallel programming in HPC++. </title> <note> Available online at http://www.extreme.indiana.edu/hpc%2b%2b/docs/ppphpc++/icpp.ps, 1996. </note>
Reference-contexts: While there are a few message passing versions of Barnes-Hut, FMM, SAMR, and Radiosity, they are considered to require heroic programming chiefly due to difficulties of implementing sophisticated distributed data structures. 36 Data-parallel approaches <ref> [13, 24, 2] </ref> provide a global namespace but require structured data parallelism, often a poor match for irregular applications. The alignment adds a significant task to program formulation, and generally forces the programmer to build structures quite different from sequential program versions. This increases the programming effort.
Reference: [3] <author> G. Blelloch and G. Sabot. </author> <title> Compiling collection-oriented languages onto massively parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 119-34, </pages> <year> 1990. </year>
Reference-contexts: The alignment adds a significant task to program formulation, and generally forces the programmer to build structures quite different from sequential program versions. This increases the programming effort. More flexible models of data parallels such as nested parallelism <ref> [3, 8] </ref> are more promising. Another body of related work are a series of application studies for cache-coherent shared memory systems, which employ a model of shared address space and threads [42, 37].
Reference: [4] <author> Greg L. Bryan. </author> <title> The Numerical Simulation of X-ray Clusters. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: Structured Adaptive Mesh Refinement (SAMR) methods use dynamically created meshes to discretize such simulations, conserving computational resources. For our investigation, we have based our parallel method on a sequential C++ code that is in use by the Computational Cosmology Group at the National Center for Supercomputing Applications <ref> [4] </ref>. 4.3.1 Algorithmic Structure The basic idea in SAMR is to adapt a hierarchy of grids (eq. meshes) over time, in order to model a physical space efficiently. In this context, hierarchy means a logical arrangement of the grids into a series of levels.
Reference: [5] <author> B. </author> <title> Buchberger. Multidimensional Systems Theory, chapter Grobner Basis: an Algorithmic Method in Polynomial Ideal Theory, </title> <address> pages 184-232. D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: The programming was easier than on a shared-memory system, as the programmer did not need to do any explicit locking. 4.5 Grobner The Grobner application is from the symbolic algebra domain and computes the Grobner basis of a set of polynomials. We use a sequential algorithm due to Buchberger <ref> [5] </ref> as our starting point. 4.5.1 Algorithmic Structure The algorithm starts off with an initial basis set of polynomials equal to the input set. Then, each possible pair of polynomials is evaluated, ranked according to a heuristic metric.
Reference: [6] <author> S. Chakrabarti and K. Yelick. </author> <title> Implementing an irregular application on a distributed memory multiprocessor. </title> <booktitle> In Proceedings of the Fourth ACM/SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 169-179, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: FMM computational cosmology [42] Computes the interaction of a system of particles in 2D using the fast multipole method. SAMR computational fluid dynamics [14] Solves hyperbolic partial differential equations using structured adaptive mesh refinement. Grobner computational algebra <ref> [6] </ref> Computes grobner basis. Phylogeny evolutionary history [20] Determines the evolutionary history of a set of species. Radiosity computer graphics [42] Computes graph illumination using the hierarchical ra diosity method. <p> queue (required due to the heuristic evaluation metric): computation consists of dequeuing the next pair of the priority queue and evaluating it until the queue is empty. 4.5.2 Program Parallelization Our parallel algorithm is derived from a previous parallelization of the Grobner basis problem on distributed memory machines by Chakrabarti <ref> [6] </ref>. Concurrency and Synchronization Specification The parallelism in the application arises from parallel evaluation of polynomial pairs. This is naturally expressed in Concert using a conc loop around the pair generation code in the sequential program. Each pair creates a dynamic task which accesses both the basis and polynomial objects. <p> These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [6, 34, 36, 18, 35] </ref>. Polyover's speedup is against the best sequential algorithm. The speedup levels off because of insufficient work in the parallel algorithm but is comparable to other reported numbers [41]. <p> Finally, the speedups for both Grobner and Phylogeny compare favorably with other implementations <ref> [20, 6] </ref> despite not fully replicating data structures as in the latter cases. The above competitive speedups were made possible by the various transformations described in Section 4 to enhance data locality and load balance.
Reference: [7] <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: Of the many concurrent object-oriented programming systems, nearly all have chosen to ignore support for fine-grained parallelism. This greatly complicates the task of programming irregular applications by forcing programmers to map irregular concurrency into grains large enough to achieve efficiency. For example, systems such as pC++ [24], CC++ <ref> [7] </ref> (these two languages have been combined into HPC++), and Mentat [16] (now a basis for the Legion project) all use heavyweight threads.
Reference: [8] <author> S. Chatterjee. </author> <title> Compiling nested data parallel programs for shared memory multiprocessors. </title> <journal> ACM Transactions of Programming Languages and Systems, </journal> <volume> 15(3), </volume> <year> 1993. </year>
Reference-contexts: The alignment adds a significant task to program formulation, and generally forces the programmer to build structures quite different from sequential program versions. This increases the programming effort. More flexible models of data parallels such as nested parallelism <ref> [3, 8] </ref> are more promising. Another body of related work are a series of application studies for cache-coherent shared memory systems, which employ a model of shared address space and threads [42, 37].
Reference: [9] <author> A. A. Chien, W. Feng, V. Karamcheti, and J. Plevyak. </author> <title> Techniques for efficient execution of fine-grained concurrent programs. </title> <booktitle> In Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <pages> pages 103-13, </pages> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10].
Reference: [10] <author> A. A. Chien, M. Straka, J. Dolby, V. Karamcheti, J. Plevyak, and X. Zhang. </author> <title> A case study in irregular parallel programming. </title> <booktitle> In DIMACS Workshop on Specification of Parallel Algorithms, </booktitle> <month> May </month> <year> 1994. </year> <note> Also available as Springer-Verlag LNCS. </note>
Reference-contexts: While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [23, 45, 10] </ref>. In effect, the Concert system automatically addresses many of the concerns which programmers explicitly manage in lower level programming models (e.g. explicit threads or message passing). <p> While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [23, 45, 10] </ref>. In effect, the Concert system automatically addresses many of the concerns which programmers must manage in lower level programming models: Procedure Granularity and Virtual Function Call Overhead Aggressive interprocedural optimization and cloning chooses efficient granularities and essentially eliminates virtual function call overhead [28].
Reference: [11] <author> E. Chu, A. George, J. Liu, and E. Ng. Sparspak: </author> <title> Waterloo sparse matrix package user's guide for sparspak-a. </title> <type> Technical Report CS-84-36, </type> <institution> Department of Computer Science, University of Waterloo, Waterloo, </institution> <address> Ontario, Canada, </address> <year> 1984. </year> <month> 39 </month>
Reference-contexts: The 2 Multipol library provides a set of tools to build irregular applications, but no uniform high level interface [39]. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes <ref> [11, 27] </ref>. Of the many concurrent object-oriented programming systems, nearly all have chosen to ignore support for fine-grained parallelism. This greatly complicates the task of programming irregular applications by forcing programmers to map irregular concurrency into grains large enough to achieve efficiency. <p> The inspector-executor approach supports iterative irregular communication (such as in finite element simulations) [19], but because it is designed for SPMD execution, provides little support for irregular control flow. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes <ref> [11, 27] </ref>, but are basically solver libraries, not a general programming interface for irregular applications. The closest related work is Yelick's Multipol libraries which provide a set of tools to build irregular applications.
Reference: [12] <author> Julian Dolby. </author> <title> Automatic inline allocation of objects. </title> <booktitle> In Proceedings of the 1997 ACM SIGPLAN Conference on Program--ming Language Design and Implementation, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> Compound Object Structuring Object data inlining based on interprocedural analysis allows the Concert compiler to reorganize data structures with inline allocation, reducing storage management cost and eliminating associated pointer dereferences <ref> [12] </ref>. Resulting code efficiency can surpass hand-tuned C or C++ manual inline optimization.
Reference: [13] <author> High Performance Fortran Forum. </author> <title> High performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: While the decomposition of computation and data layout is similar conceptually to that in HPF <ref> [13] </ref>, the decompositions used for our applications are irregular. Finally, while quantitative metrics are only moderately indicative of programming effort, they nonetheless provide a tangible point of comparison. <p> However, these libraries provide no uniform high level interface, perhaps because the project did not include optimizing compiler support [39]. The best known parallel programming approaches are message passing embodied in MPI [26] and data parallel embodied in <ref> [13] </ref>. While both of these approaches are widely used, and could be used to build these irregular applications. We believe neither provides appropriate support. <p> While there are a few message passing versions of Barnes-Hut, FMM, SAMR, and Radiosity, they are considered to require heroic programming chiefly due to difficulties of implementing sophisticated distributed data structures. 36 Data-parallel approaches <ref> [13, 24, 2] </ref> provide a global namespace but require structured data parallelism, often a poor match for irregular applications. The alignment adds a significant task to program formulation, and generally forces the programmer to build structures quite different from sequential program versions. This increases the programming effort.
Reference: [14] <author> Bishwaroop Ganguly, Greg Bryan, Michael Norman, and Andrew Chien. </author> <title> Exploring structured adaptive mesh refinement (SAMR) methods with the Illinois Concert system. </title> <booktitle> In Proceedings of the Eighth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Minneapolis, Minnesota, </address> <month> March </month> <year> 1997. </year>
Reference-contexts: Barnes-hut computational cosmology [42] Computes the interaction of a system of particles in 3D using the Barnes-Hut hierarchical N-body method. FMM computational cosmology [42] Computes the interaction of a system of particles in 2D using the fast multipole method. SAMR computational fluid dynamics <ref> [14] </ref> Solves hyperbolic partial differential equations using structured adaptive mesh refinement. Grobner computational algebra [6] Computes grobner basis. Phylogeny evolutionary history [20] Determines the evolutionary history of a set of species. Radiosity computer graphics [42] Computes graph illumination using the hierarchical ra diosity method.
Reference: [15] <author> Leslie Greengard. </author> <title> Fast Algorithms for Classical Physics. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: In all, less than 5% of the lines require changes for the parallel versions: 107 out of 2,631 lines. 4.2 FMM The FMM code computes the interactions of a system of particles in 2-D using the fast multipole (FMM) hierarchical N-body method <ref> [15] </ref>. 4.2.1 Algorithmic Structure The FMM method also exploits the physical insight that the interactions between two particles decreases at a rate proportional to the square of their distance to reduce the computational complexity.
Reference: [16] <author> A. Grimshaw. </author> <title> Easy-to-use object-oriented parallel processing with Mentat. </title> <journal> IEEE Computer, </journal> <volume> 5(26) </volume> <pages> 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This greatly complicates the task of programming irregular applications by forcing programmers to map irregular concurrency into grains large enough to achieve efficiency. For example, systems such as pC++ [24], CC++ [7] (these two languages have been combined into HPC++), and Mentat <ref> [16] </ref> (now a basis for the Legion project) all use heavyweight threads. Of the few systems which have explored fine-grained concurrency (ABCL/f [43], MPC++, and the Illinois Concert system), this study is the first systematic evaluation of programming effort for a significant range of irregular applications.
Reference: [17] <author> P. Hanrahan, D. Salzman, and L. Aupperle. </author> <title> A rapid hierarchical radiosity algorithm. </title> <booktitle> Computer Graphics (Proc Siggraph), </booktitle> <volume> 25(4) </volume> <pages> 197-206, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: As our starting point, we use a sequential algorithm due to Hanrahan <ref> [17] </ref>, modeled after hierarchical N-body methods. 4.7.1 Algorithmic Structure Traditional radiosity approaches first subdivide the polygonal surfaces that describe a scene into small elemental patches with roughly uniform radiosity and then compute the radiosity of a patch as the aggregation of pair-wise interactions between the patch and every other patch.
Reference: [18] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Bernard Brooks, and Milan Hodoscek. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines. </title> <journal> IEEE Computational Science and Engineering, </journal> <pages> pages 18-29, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [6, 34, 36, 18, 35] </ref>. Polyover's speedup is against the best sequential algorithm. The speedup levels off because of insufficient work in the parallel algorithm but is comparable to other reported numbers [41].
Reference: [19] <editor> J.H. Saltz, et al. </editor> <title> A manual for the CHAOS runtime library. </title> <type> Technical Report CS-TK-3437, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1995. </year>
Reference-contexts: Research efforts have largely focused on building special libraries which provide runtime support for one or several dimensions of irregularity. For example, the inspector-executor approach uses a library to support iterated irregular communication (such as in finite element simulations) <ref> [19] </ref>. The 2 Multipol library provides a set of tools to build irregular applications, but no uniform high level interface [39]. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes [11, 27]. <p> Programming techniques for irregular applications is an important body of related work. All of these techniques pursue a library-based approach, and often consider only irregularity in an single dimension. The inspector-executor approach supports iterative irregular communication (such as in finite element simulations) <ref> [19] </ref>, but because it is designed for SPMD execution, provides little support for irregular control flow. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes [11, 27], but are basically solver libraries, not a general programming interface for irregular applications.
Reference: [20] <author> Jeff A. Jones. </author> <title> Parallelizing the phylogeny problem. </title> <type> Master's thesis, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: FMM computational cosmology [42] Computes the interaction of a system of particles in 2D using the fast multipole method. SAMR computational fluid dynamics [14] Solves hyperbolic partial differential equations using structured adaptive mesh refinement. Grobner computational algebra [6] Computes grobner basis. Phylogeny evolutionary history <ref> [20] </ref> Determines the evolutionary history of a set of species. Radiosity computer graphics [42] Computes graph illumination using the hierarchical ra diosity method. Table 1: The Irregular Applications Suite The complexity of programming for our applications implies that many are not in widespread use on 8 scalable parallel machines. <p> We use a sequential algorithm described by Jones <ref> [20] </ref> as our starting point. 25 4.6.1 Algorithmic Structure Phylogenetic trees are constructed by considering the characters exhibited by the species. <p> Our algorithm for the general phylogeny problem is based on a method known as character compatibility and uses a solution to the perfect phylogeny problem (determining whether or not a perfect phylogenetic tree exists) (see <ref> [20] </ref> for details) as a subroutine. The essence of the character compatibility method is a search for the largest compatible subset of characters, the rationale being that if the subset is large enough, the corresponding perfect phylogeny will be a good estimate of the evolutionary history of the species. <p> Each solution node maintains a character subset, and child and parent pointers of the trie. Given the sophisticated nature of the data structures, and the pruning-based search procedure, the algorithm is complicated to express even on sequential platforms. 4.6.2 Program Parallelization Our parallel algorithm follows the strategy of <ref> [20] </ref> and evaluates multiple character subsets in parallel. Given that adequate parallelism exists at this level, we have chosen to utilize a bundled solution to the parallel 26 phylogeny problem as a (sequential) leaf subroutine. <p> Finally, the speedups for both Grobner and Phylogeny compare favorably with other implementations <ref> [20, 6] </ref> despite not fully replicating data structures as in the latter cases. The above competitive speedups were made possible by the various transformations described in Section 4 to enhance data locality and load balance.
Reference: [21] <author> L. V. Kale and W. Shu. </author> <title> The chare-kernel language for parallel programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1990. </year>
Reference-contexts: Of course, given the diversity of applications studied, several required more detailed control over data locality and load balance. In applications employing heuristic methods, task ordering control is required <ref> [21] </ref>. And, in several applications, explicit thread placement (complementary to data placement and owner computes) was a more convenient perspective. Finally, the radiosity, grobner, and phylogeny applications benefited from annotations for relaxed data consistency.
Reference: [22] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> View caching: Efficient software shared memory for dynamic computations. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1997. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> However, patch locality requires demand-driven caching. The Concert implementation optimizes the caching performance by utilizing additional information about relaxed data consistency semantics. As the simplified visibility calculation code below shows, the programmer uses the with local annotation to specify an application-specific access semantics a view <ref> [22] </ref> to the compiler and runtime system. In this case, both input polygonal patches are specified to be read-only, allowing the Concert system to efficiently cache the data to exploit data reuse. Over the entire program, the programmer needs to specify access information about 3 kinds of views.
Reference: [23] <author> Vijay Karamcheti, John Plevyak, and Andrew A. Chien. </author> <title> Runtime mechanisms for efficient dynamic multithreading. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1) </volume> <pages> 21-40, </pages> <year> 1996. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/ rtperf.ps. </note>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [23, 45, 10] </ref>. In effect, the Concert system automatically addresses many of the concerns which programmers explicitly manage in lower level programming models (e.g. explicit threads or message passing). <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [23, 45, 10] </ref>. In effect, the Concert system automatically addresses many of the concerns which programmers must manage in lower level programming models: Procedure Granularity and Virtual Function Call Overhead Aggressive interprocedural optimization and cloning chooses efficient granularities and essentially eliminates virtual function call overhead [28]. <p> Resulting code efficiency can surpass hand-tuned C or C++ manual inline optimization. Thread Granularity Speculative stack-heap execution reduces cost by allowing threads to execute as procedure calls on the stack, lazily creating heap threads only as needed. 70% or greater processor efficiency at 10s thread granularity has been demonstrated <ref> [23] </ref>. 7 Distributed Name Management Concert provides a global namespace on both distributed memory and shared memory systems. Fast name translation and data movement (a few microseconds) enable efficient manipulation of remote names and data [23]. <p> only as needed. 70% or greater processor efficiency at 10s thread granularity has been demonstrated <ref> [23] </ref>. 7 Distributed Name Management Concert provides a global namespace on both distributed memory and shared memory systems. Fast name translation and data movement (a few microseconds) enable efficient manipulation of remote names and data [23]. Initial data placement, Data consistency and sharing (sometimes) A fast and flexible software object caching system can eliminate the importance of initial data placement for some applications. Compiler analyses can provide coarse sharing information which in some cases allows automatic management of data consistency and prefetching.
Reference: [24] <author> J. Lee and D. Gannon. </author> <title> Object oriented parallel programming. </title> <booktitle> In Proceedings of the ACM/IEEE Conference on Supercomputing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Of the many concurrent object-oriented programming systems, nearly all have chosen to ignore support for fine-grained parallelism. This greatly complicates the task of programming irregular applications by forcing programmers to map irregular concurrency into grains large enough to achieve efficiency. For example, systems such as pC++ <ref> [24] </ref>, CC++ [7] (these two languages have been combined into HPC++), and Mentat [16] (now a basis for the Legion project) all use heavyweight threads. <p> While there are a few message passing versions of Barnes-Hut, FMM, SAMR, and Radiosity, they are considered to require heroic programming chiefly due to difficulties of implementing sophisticated distributed data structures. 36 Data-parallel approaches <ref> [13, 24, 2] </ref> provide a global namespace but require structured data parallelism, often a poor match for irregular applications. The alignment adds a significant task to program formulation, and generally forces the programmer to build structures quite different from sequential program versions. This increases the programming effort.
Reference: [25] <author> Stephen P. Masticola and Barbara G. Ryder. </author> <title> Non-concurrency analysis. </title> <booktitle> In Proceedings of Fourth Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 129-138, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: First, compiler technology which aggressively propagates locality information for pointer-based data structures has the potential to further reduce the program optimization effort for irregular applications. Second, the flexible concurrency structures expressible in ICC++ require global concurrency analysis in general, an open research problem <ref> [25] </ref>, to enable synchronization optimizations. At present, this also increases the explicit annotations required to achieve high performance. This is another promising direction for research.
Reference: [26] <author> Message Passing Interface Forum. </author> <title> The MPI message passing interface standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <month> April </month> <year> 1994. </year> <note> Available from http://www.mcs.anl.gov/mpi/mpi-report.ps. </note>
Reference-contexts: However, these libraries provide no uniform high level interface, perhaps because the project did not include optimizing compiler support [39]. The best known parallel programming approaches are message passing embodied in MPI <ref> [26] </ref> and data parallel embodied in [13]. While both of these approaches are widely used, and could be used to build these irregular applications. We believe neither provides appropriate support.
Reference: [27] <author> R. Parsons and D. Quinlan. </author> <title> A++/P++ array classes for architecture independent finite difference computations. </title> <booktitle> In Proceedings of the Second Annual Object-Oriented Numerics Conference, </booktitle> <address> Sunriver, Oregon, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The 2 Multipol library provides a set of tools to build irregular applications, but no uniform high level interface [39]. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes <ref> [11, 27] </ref>. Of the many concurrent object-oriented programming systems, nearly all have chosen to ignore support for fine-grained parallelism. This greatly complicates the task of programming irregular applications by forcing programmers to map irregular concurrency into grains large enough to achieve efficiency. <p> The inspector-executor approach supports iterative irregular communication (such as in finite element simulations) [19], but because it is designed for SPMD execution, provides little support for irregular control flow. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes <ref> [11, 27] </ref>, but are basically solver libraries, not a general programming interface for irregular applications. The closest related work is Yelick's Multipol libraries which provide a set of tools to build irregular applications.
Reference: [28] <author> John Plevyak. </author> <title> Optimization of Object-Oriented and Concurrent Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <year> 1996. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> In effect, the Concert system automatically addresses many of the concerns which programmers must manage in lower level programming models: Procedure Granularity and Virtual Function Call Overhead Aggressive interprocedural optimization and cloning chooses efficient granularities and essentially eliminates virtual function call overhead <ref> [28] </ref>. Compound Object Structuring Object data inlining based on interprocedural analysis allows the Concert compiler to reorganize data structures with inline allocation, reducing storage management cost and eliminating associated pointer dereferences [12]. Resulting code efficiency can surpass hand-tuned C or C++ manual inline optimization.
Reference: [29] <author> John Plevyak and Andrew Chien. </author> <title> Incremental inference of concrete types. </title> <type> Technical Report UIUCDCS-R-93-1829, </type> <institution> Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10].
Reference: [30] <author> John Plevyak and Andrew A. Chien. </author> <title> Precise concrete type inference of object-oriented programs. </title> <booktitle> In Proceedings of OOPSLA'94, Object-Oriented Programming Systems, Languages and Architectures, </booktitle> <pages> pages 324-340, </pages> <year> 1994. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10].
Reference: [31] <author> John Plevyak and Andrew A. Chien. </author> <title> Type directed cloning for object-oriented programs. </title> <booktitle> In Proceedings of the Workshop for Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 566-580, </pages> <year> 1995. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10].
Reference: [32] <author> John Plevyak, Vijay Karamcheti, and Andrew Chien. </author> <title> Analysis of dynamic structures for efficient parallel execution. </title> <booktitle> In Proceedings of the Sixth Workshop for Languages and Compilers for Parallel Machines, </booktitle> <pages> pages 37-56, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10].
Reference: [33] <author> John Plevyak, Xingbin Zhang, and Andrew A. Chien. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 311-321, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10].
Reference: [34] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [6, 34, 36, 18, 35] </ref>. Polyover's speedup is against the best sequential algorithm. The speedup levels off because of insufficient work in the parallel algorithm but is comparable to other reported numbers [41].
Reference: [35] <author> Jaswinder Pal Singh. </author> <title> Parallel Hierarchical N-Body Methods and Their Implications For Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University Department of Computer Science, Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: We ensure data locality and load balance using a spatial partitioning of the tree data structure and a history-based load balancing algorithm, adopted from <ref> [35] </ref>. The spatial partitioning constructs a Peano-Hilbert ordering of all the particles according their spatial coordinates. This linear ordering is then divided into blocks and assigned to processors. <p> In addition, the lengths of interaction lists are highly irregular across boxes, making both data locality and load balancing essential for good performance. Similar to Barnes-Hut, we ensure data locality and load balance using a spatial partitioning of particles and boxes and a history-based load balancing algorithm, adopted from <ref> [35] </ref>. The spatial partitioning constructs a Peano-Hilbert ordering of all the particles according their spatial coordinates. This linear ordering is then divided into blocks and assigned to processors. <p> These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [6, 34, 36, 18, 35] </ref>. Polyover's speedup is against the best sequential algorithm. The speedup levels off because of insufficient work in the parallel algorithm but is comparable to other reported numbers [41].
Reference: [36] <author> Jaswinder Pal Singh, Anoop Gupta, and Marc Levoy. </author> <title> Parallel visualization algorithms: Performance and architectural implications. </title> <journal> IEEE Computer, </journal> <volume> 27(7) </volume> <pages> 45-56, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes <ref> [6, 34, 36, 18, 35] </ref>. Polyover's speedup is against the best sequential algorithm. The speedup levels off because of insufficient work in the parallel algorithm but is comparable to other reported numbers [41]. <p> The Radiosity speedup of 23 on 32 T3D processors compares well with the previously reported speedup of 26 on 32 processors of the DASH machine <ref> [36] </ref>, despite DASH's hardware support for cache-coherent shared memory and an order of magnitude faster communication (in terms of processor clocks), which better facilitate scalable performance.
Reference: [37] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <type> Technical report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, Stanford University, </institution> <address> CA 94305, </address> <month> April </month> <year> 1991. </year> <note> Available from ftp://mojave.stanford.edu/pub/splash/report/splash.ps. </note>
Reference-contexts: This increases the programming effort. More flexible models of data parallels such as nested parallelism [3, 8] are more promising. Another body of related work are a series of application studies for cache-coherent shared memory systems, which employ a model of shared address space and threads <ref> [42, 37] </ref>. While we have leveraged these studies, borrowing the algorithmic structure and data locality and load balance optimization for Barnes-Hut, FMM, and Radiosity, the real goal of those studies is architectural evaluation, not programmability.
Reference: [38] <author> M. Warren and J. Salmon. </author> <title> A parallel hashed oct-tree N-body algorithm. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 12-21, </pages> <year> 1993. </year> <month> 40 </month>
Reference-contexts: Radiosity computer graphics [42] Computes graph illumination using the hierarchical ra diosity method. Table 1: The Irregular Applications Suite The complexity of programming for our applications implies that many are not in widespread use on 8 scalable parallel machines. For those that are <ref> [42, 38] </ref>, extraordinary programming using low level interfaces have previously been required to achieve good performance. To our knowledge, our study is the first study which evaluates a high level, general purpose programming interface for such a challenging suite of parallel applications. <p> The speedup levels off because of insufficient work in the parallel algorithm but is comparable to other reported numbers [41]. Both the Barnes speedup of 42 and FMM speedup of 54 on 64 nodes are competitive with shared-memory versions [42] and hand-optimized versions <ref> [38] </ref> reported elsewhere in literature.
Reference: [39] <author> Chih-Po Wen, Soumen Chakrabarti, Etienne Deprit, Arvind Krishnamurthy, and Katherine Yelick. </author> <title> Run-time support for portable distributed data structures. </title> <booktitle> In Third Workshop on Languages, Compilers, and Run-Time Systems for Scalable Computers, </booktitle> <pages> pages 111-120, </pages> <address> Boston, May 1995. </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: For example, the inspector-executor approach uses a library to support iterated irregular communication (such as in finite element simulations) [19]. The 2 Multipol library provides a set of tools to build irregular applications, but no uniform high level interface <ref> [39] </ref>. Libraries such as Sparspak, Kelp, and A++/P++ support particular classes of algorithms on sparse matrices and adaptive meshes [11, 27]. Of the many concurrent object-oriented programming systems, nearly all have chosen to ignore support for fine-grained parallelism. <p> The closest related work is Yelick's Multipol libraries which provide a set of tools to build irregular applications. However, these libraries provide no uniform high level interface, perhaps because the project did not include optimizing compiler support <ref> [39] </ref>. The best known parallel programming approaches are message passing embodied in MPI [26] and data parallel embodied in [13]. While both of these approaches are widely used, and could be used to build these irregular applications. We believe neither provides appropriate support.
Reference: [40] <author> Gregory V. Wilson and Paul Lu, </author> <title> editors. Parallel Programming Using C++. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: This bodes well for supporting both sequential and parallel performance in a single code base. keywords: conncurent object-oriented programming, high-level parallel models 1 Overview Object-oriented techniques have been proffered as aids for managing complexity, enhancing reuse, and im proving readability of parallel applications. Numerous variants have been explored (e.g. <ref> [40] </ref>) with a great volume of technical papers published. Concurrent object systems appear particularly promising for irregular applications where modularity and encapsulation help manage the complex data and parallelism struc tures.
Reference: [41] <author> Gregory V. Wilson and Paul Lu, </author> <title> editors. Parallel Programming Using C++, chapter ICC++. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Then, concurrency can be added as appropriate. The basic model is based on C++, providing a single namespace, scalars and objects, and single inheritance. For further details on the programming model (Illinois Concert C++ or ICC++) see <ref> [41] </ref>. Simple extensions for concurrency include annotating standard blocks (i.e. compound statements) and loops with the conc keyword. A conc block defines a partial order amongst its statements, allowing concur-rency while preserving local data dependences. Thus, conc can often be applied to existing blocks. <p> In short, these are applications which are complex to program even on sequential systems, using sophisticated pointer based data structures and efficient algorithms. Applications Description Polyover computer graphics <ref> [41] </ref> Computes the overlay of two polygon maps. Barnes-hut computational cosmology [42] Computes the interaction of a system of particles in 3D using the Barnes-Hut hierarchical N-body method. FMM computational cosmology [42] Computes the interaction of a system of particles in 2D using the fast multipole method. <p> The collection data type ensures cyclic distribution, and concurrent method calls with synchronization happen transparently in concurrent loops. 4.4 Polyover Polyover computes the overlay of two polygon maps. As our starting point we use a fast sequential algorithm described in <ref> [41] </ref>. 4.4.1 Algorithmic Structure The two polygon maps, referred to as the left and right map, are represented as a vector and a link list respectively. <p> These speedups compare favorably with the best speedups reported elsewhere for hand-optimized codes [6, 34, 36, 18, 35]. Polyover's speedup is against the best sequential algorithm. The speedup levels off because of insufficient work in the parallel algorithm but is comparable to other reported numbers <ref> [41] </ref>. Both the Barnes speedup of 42 and FMM speedup of 54 on 64 nodes are competitive with shared-memory versions [42] and hand-optimized versions [38] reported elsewhere in literature.
Reference: [42] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <year> 1995. </year>
Reference-contexts: In short, these are applications which are complex to program even on sequential systems, using sophisticated pointer based data structures and efficient algorithms. Applications Description Polyover computer graphics [41] Computes the overlay of two polygon maps. Barnes-hut computational cosmology <ref> [42] </ref> Computes the interaction of a system of particles in 3D using the Barnes-Hut hierarchical N-body method. FMM computational cosmology [42] Computes the interaction of a system of particles in 2D using the fast multipole method. <p> Applications Description Polyover computer graphics [41] Computes the overlay of two polygon maps. Barnes-hut computational cosmology <ref> [42] </ref> Computes the interaction of a system of particles in 3D using the Barnes-Hut hierarchical N-body method. FMM computational cosmology [42] Computes the interaction of a system of particles in 2D using the fast multipole method. SAMR computational fluid dynamics [14] Solves hyperbolic partial differential equations using structured adaptive mesh refinement. Grobner computational algebra [6] Computes grobner basis. <p> SAMR computational fluid dynamics [14] Solves hyperbolic partial differential equations using structured adaptive mesh refinement. Grobner computational algebra [6] Computes grobner basis. Phylogeny evolutionary history [20] Determines the evolutionary history of a set of species. Radiosity computer graphics <ref> [42] </ref> Computes graph illumination using the hierarchical ra diosity method. Table 1: The Irregular Applications Suite The complexity of programming for our applications implies that many are not in widespread use on 8 scalable parallel machines. <p> Radiosity computer graphics [42] Computes graph illumination using the hierarchical ra diosity method. Table 1: The Irregular Applications Suite The complexity of programming for our applications implies that many are not in widespread use on 8 scalable parallel machines. For those that are <ref> [42, 38] </ref>, extraordinary programming using low level interfaces have previously been required to achieve good performance. To our knowledge, our study is the first study which evaluates a high level, general purpose programming interface for such a challenging suite of parallel applications. <p> If the distance is within the cutoff, the traversal continues with the children cells of the current cell. Therefore, the data access pattern of each tree traversal is highly data-dependent and irregular. 4.1.2 Program Parallelization Our parallel algorithm is derived from the Barnes-Hut application in the SPLASH-2 benchmark suite <ref> [42] </ref>. Concurrency and Synchronization Specification There are two levels of parallelism in the dominant force computation phase. Oct-tree traversals for all particles are concurrent, and within a traversal, the traversal of subtrees are also concurrent. <p> The force computation phase dominates the computation, representing more than 90% of the sequential computation time. 4.2.2 Program Parallelization Our parallel algorithm is derived from the FMM application in the SPLASH-2 benchmark suite <ref> [42] </ref>. Concurrency and Synchronization Specification There are three levels of parallelism in the dom inant force computation phase: across interactions corresponding to separate boxes, across computations with different interaction lists for a single box, and across interactions with boxes within the same inter action list. <p> The iterations terminate when the change in the total radiosity (weighted sum of all input patches) falls below a threshold. 4.7.2 Program Parallelization Our parallel algorithm is derived from the radiosity application in the SPLASH-2 benchmark suite <ref> [42] </ref>. Concurrency and Synchronization Specification There are three levels of parallelism in each iteration: across all input patches (radiosity calculation), across child patches of a subdivided patch (radiosity calculation), and across neighbor patches stored in the interaction list (visibility, error, and radiosity calculation). <p> Polyover's speedup is against the best sequential algorithm. The speedup levels off because of insufficient work in the parallel algorithm but is comparable to other reported numbers [41]. Both the Barnes speedup of 42 and FMM speedup of 54 on 64 nodes are competitive with shared-memory versions <ref> [42] </ref> and hand-optimized versions [38] reported elsewhere in literature. <p> This increases the programming effort. More flexible models of data parallels such as nested parallelism [3, 8] are more promising. Another body of related work are a series of application studies for cache-coherent shared memory systems, which employ a model of shared address space and threads <ref> [42, 37] </ref>. While we have leveraged these studies, borrowing the algorithmic structure and data locality and load balance optimization for Barnes-Hut, FMM, and Radiosity, the real goal of those studies is architectural evaluation, not programmability.
Reference: [43] <editor> Akinori Yonezawa, editor. </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: For example, systems such as pC++ [24], CC++ [7] (these two languages have been combined into HPC++), and Mentat [16] (now a basis for the Legion project) all use heavyweight threads. Of the few systems which have explored fine-grained concurrency (ABCL/f <ref> [43] </ref>, MPC++, and the Illinois Concert system), this study is the first systematic evaluation of programming effort for a significant range of irregular applications.
Reference: [44] <author> Xingbin Zhang and Andrew A. Chien. </author> <title> Dynamic pointer alignment: Tiling and communication optimizations for parallel pointer-based computations. </title> <booktitle> In Proceedings of ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Las Vegas, Nevada, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10]. <p> !scheduler GLOBAL PRIORITY ); ... 2.3 Illinois Concert System Our application study is done in the context of the Illinois Concert system, a high performance compiler and runtime for parallel computers which has been the vehicle for extensive research on compiler optimization and runtime techniques over the past five years <ref> [9, 32, 29, 30, 33, 28, 31, 12, 44, 23, 22] </ref>. While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications [23, 45, 10].
Reference: [45] <author> Xingbin Zhang, Vijay Karamcheti, Tony Ng, and Andrew Chien. </author> <title> Optimizing COOP languages: Study of a protein dynamics program. </title> <booktitle> In IPPS'96, </booktitle> <year> 1996. </year> <month> 41 </month>
Reference-contexts: While no system contains all known optimizations, the Concert system contains a wide range of aggressive optimizations, and has been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [23, 45, 10] </ref>. In effect, the Concert system automatically addresses many of the concerns which programmers explicitly manage in lower level programming models (e.g. explicit threads or message passing). <p> While no system contains all known optimizations, the Concert system contains a wide range of them, and has been used to demonstrate high performance in absolute terms on a wide range of applications <ref> [23, 45, 10] </ref>. In effect, the Concert system automatically addresses many of the concerns which programmers must manage in lower level programming models: Procedure Granularity and Virtual Function Call Overhead Aggressive interprocedural optimization and cloning chooses efficient granularities and essentially eliminates virtual function call overhead [28].
References-found: 45

