URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR418.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Email: dswise@cs.indiana.edu  
Title: Undulant-Block Elimination and Integer-Preserving Matrix Inversion 1  
Author: David S. Wise 
Date: Revised: August 1995  
Address: Bloomington, Indiana 474054101  
Affiliation: Computer Science Department Indiana University  
Pubnum: Technical Report 418  
Abstract: 1 c fl 1994,1995 by the author. This work has been accepted for publication by Science of Computer Programming where it is anticipated to appear in volume 32, number 1, in mid-1998. Copyright will be transferred the publisher without further notice, and this version may no longer be accessible. This document is made available by the author as a means to ensure timely dissemination of scholarly and technical work on a noncommercial basis. Copyright and all other rights are maintained by the author or by other copyright holders, notwithstanding that they have offered their work electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by the author's copyright. This work may not be reposted without the explicit permission of the copyright holder. 2 Research reported herein was sponsored, in part, by the National Science Foundation under Grant Number DCR 90-027092. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. K.Abdali and D. S. Wise. </author> <title> Experiments with quadtree representation of matrices. </title> <editor> In P. Gianni (ed.), </editor> <booktitle> Proceedings ISAAC 88, Lecture Notes in Computer Science 358, </booktitle> <address> Berlin, </address> <publisher> Springer (1989), </publisher> <pages> 96-108. </pages>
Reference-contexts: An alternative readingthat the processes are dispatched bottom-up on sixteen scalar additions at a time, but a million timesis possible, though silly. 1.5 Operation counts. The results entitled OpCount below present bounds on the uniprocessor behavior of these algorithms <ref> [1] </ref>. They measure the total number of multiplications or divisions, the traditional metric for linear systems, even though it can be misleading for multiprocessing. Like theorems or corollaries each can be established from the relevant code and previous opCounts, usually by inspection.
Reference: [2] <author> E. R. Bareiss. </author> <title> Sylvester's identity and multistep integer-preserving Gaussian elimination. </title> <journal> Math. Comp. </journal> <volume> 22, </volume> <month> 103 (July </month> <year> 1968), </year> <pages> 565-578. </pages>
Reference-contexts: The usual algorithm to solve this problem is due to Bareiss. A perspective at the end of his introduction <ref> [2, p. 567] </ref> characterizes the quadtree algorithm below. It is a fraction-free version of GE that eliminates blocks of undulant size: 1; 2; 4; : : : with quadtree matrices that Bareiss would recognize each as a 2 p -step. 50 4 EXACT-ARITHMETIC DECOMPOSITION. <p> Although he describes a version with invariant p, he explicitly suggests degrees higher than the two-step algorithms that he presents <ref> [2, p. 570] </ref>. That is, Bareiss anticipates the parallel nature of his elimination steps that is motivational here. The quadtree solution, however, brings new attributes to this problem. Implicitly, it offers a uniform method for both sparse and dense problems that implicitly responds to sparseness. <p> Implicitly, it offers a uniform method for both sparse and dense problems that implicitly responds to sparseness. The parallel decomposition is married to a structural decomposition, as well. The tree structure offers good pivoting strategies, a subject treated only briefly by Bareiss <ref> [2, xV] </ref>. Keeping the accumulated determinants small can be as important as sustaining sparseness; both constrain the extra time and space needed for the residual problem. Finally, the following algorithm provides undulant pivoting, allowing better pivot selection and even more parallelism, if only at a few intermediate steps. <p> OpCount 4.2 Algorithm 2.6 inverts an n fi n matrix within 13 12 n 3 2n 2 =3 multi plications, exclusive of permutations. It is usual <ref> [2] </ref> to characterize algorithms of this genre by counting divisions with the following results: OpCount 4.3 Algorithm 4.2, eliminating 1 fi 1 blocks, decomposes n fi n integer matrices within n 3 =3 n 2 =2 + O (n) integer divisions.
Reference: [3] <author> P. Beckman. </author> <title> Parallel LU Decomposition for Sparse Matrices Using Quadtrees on a Shared-Heap Multiprocessor. </title> <type> Ph.D. dissertation. </type> <institution> Indiana University, Bloomington (1993). </institution>
Reference-contexts: Unlike other decorations, however, these 49 numbers can change as a result of elimination elsewhere in the matrix; that is, such decorations go stale without traversing the decorated submatrix. Experiments with such decorations <ref> [3, x4.4] </ref> have, nevertheless, yielded very good results. 3.3.3 Decorating to avoid bignums. The problem of the next section suggests an opposite use of the known-determinant, as introduced above. Exact-arithmetic GE can generate huge integers in the intermediate results even though the final inverse may be quite tame. <p> Prolonged contact with the computer turns mathematicians into clerks and vice versa. [18, -80] Present implementations <ref> [3, 14] </ref> of these algorithms do not order the basis of the vector space before elimination. The remarks in this section, therefore, are speculative. A good preordering algorithm decomposes the domain into nearly independent subdomains, assembling zeros into off-diagonal blocks [7]. <p> Sketched in Section 3.3, this work is ongoing. The algorithms described here have now been implemented in various languages and run on various computers, both uniprocessors and a multiprocessor. The languages are C, SCHEME, and HASKELL [14]; the computers include BBN Butterflies <ref> [3] </ref>, a NeXT, SGIs, Suns, and Macintoshes. Test data has been taken from the HarwellBoeing collection [10]. Performance is favorable, within the ability of each implementation to mimic an idealized environment.
Reference: [4] <author> F. W. Burton & J. G. Kollias. </author> <title> Comment on `The explicit quad tree as a structure for computer graphics.' </title> <journal> Comput. J. </journal> <volume> 26, </volume> <month> 2 (May </month> <year> 1983), </year> <month> 188. </month>
Reference-contexts: Burton and Kollias <ref> [4] </ref> use an Ahnentafel-like indexing generalized from binary to quaternary trees, without convenient stripe/colonnade indexing. The only stripes 2 Reverse-Ahnentafel indexing, like floating-point numbers and even the quadtree representation, itself, is an internal representation that speeds computation.
Reference: [5] <author> H. G. </author> <title> Cragon A historical note on binary tree. </title> <booktitle> ACM Computer Architecture News 18, </booktitle> <month> 4 (Dec </month> <year> 1990), </year> <month> 3. </month>
Reference-contexts: Definition. A stripe 1 is a set of adjacent rows in a matrix. A colonnade is a set of adjacent columns. 1 as in Stars and Stripes with apologies to tigers and zebras. 1.2 Indexing. 7 1.2 Indexing. Definition. The Ahnentafel index <ref> [5] </ref> of an entire vector is 1. If i is the Ahnentafel index of a subvector, then the Ahnentafel index of its north half is 2i, and the Ahnentafel index of its south half is 2i + 1.
Reference: [6] <author> D. Culler, R. Karp, D. Patterson, A. Sahay,K. E. Schauser, E. Santos, R. Sub-ramonian, & T. von Eicken. </author> <title> LogP: towards a realistic model of parallel computation. </title> <booktitle> Proc. 4th ACM Symp. on Principles & Practice of Parallel Programming; SIGPLAN Notices 28, </booktitle> <month> 7 (July </month> <year> 1993), </year> <month> 112. </month>
Reference-contexts: Before fetching from memory, the second allows only 2m 2 scalar multiplications to complete two entries in the product; but the first readily computes m products added to each of m 2 entries. Under parallel processing, block manipulation reduces the granularity of both scheduling and communication <ref> [6] </ref>. Such improvement from block operations is well known; matrix-matrix operations are commonly called Level 3 (BLAS) [9], with matrix-vector operations rated Level 2. Statically sized blocks are used by existing block algorithms for GE under one of two philosophies. <p> If 4 fi 4 leaves of a quadtree were stored serially, moreover, the time to fetch and to multiply them will be little different from the time to fetch and to 15 multiply two scalars; effects from latency and caching make the actual multiplication time trivial <ref> [6] </ref>. However, only (n=4) 3 block multiplications of the first sort are necessary to multiply n fi n matrices, compared to n 3 of the second. So the coefficients that appear these uniprocessor-operation counts may not themselves be useful, except for relative comparison. <p> Thus, #*@ is matrix product for an undecorated matrix times a decorated matrix; @*# is matrix product for a decorated matrix times an undecorated matrix; #+@ is matrix sum 36 3 DECOMPOSITION OF QUADTREE MATRICES. infixl <ref> [6] </ref> #+% infixl [7] #*@ , @*# type DecorQuadrants a = (DecorMatrx a, DecorMatrx a, DecorMatrx a, DecorMatrx a) data DecorMatrx a = ZeroD | ScalarD a | MtxD (Decoration a) (DecorQuadrants a) instance (Num a) =&gt; Num (DecorMatrx a) decorate:: (DecorQuadrants a) -&gt; (DecorMatrx a) decorate (ZeroD,ZeroD,ZeroD,ZeroD) = ZeroD decorate <p> Counts of (exact) divisions are more important than that of multiplications for the integer algorithms, because extended precision or symbolic algebra requires long-division algorithms. Yet, even these counts are artificial under multiprocessing, where costs of communication dominate <ref> [6] </ref>. Under those rules it can be better to recompute a scalar result locally than to share one stored remotely. <p> For instance, how would analysts of algorithms measure Algorithm 2.6 to be superior to Algorithm 2.5? A difference, based on relative locality, certainly exists. How should we measure the complexity of these algorithms? The op-Counts offered here should be replaced by something reflecting locality <ref> [6] </ref> and read-only cache use, but what? Can good implementationseven on uniproces-sors using row-major array representationexpose the greater locality of Algorithm 2.6? And can we use the new measures to identify better algorithms for related problemsespecially those that we think are already solved.
Reference: [7] <author> I. S. Duff, A. M. Erisman, & J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Clarendon Press, </publisher> <address> Oxford (1989). </address>
Reference-contexts: The quadtree representation of matrices [21] motivated the algorithms to follow. Conceptually, it decomposes full matrices as balanced quaternary trees. Sparse matrices are accommodated by providing a distinguished representation for an entirely zero submatrix, essentially a null pointer (cf. hypermatrices <ref> [7] </ref>). The same convention makes it easy to handle a matrix whose order, n, is not a power of two by padding with zero blocks to order 2 dlg ne . <p> Thus, #*@ is matrix product for an undecorated matrix times a decorated matrix; @*# is matrix product for a decorated matrix times an undecorated matrix; #+@ is matrix sum 36 3 DECOMPOSITION OF QUADTREE MATRICES. infixl [6] #+% infixl <ref> [7] </ref> #*@ , @*# type DecorQuadrants a = (DecorMatrx a, DecorMatrx a, DecorMatrx a, DecorMatrx a) data DecorMatrx a = ZeroD | ScalarD a | MtxD (Decoration a) (DecorQuadrants a) instance (Num a) =&gt; Num (DecorMatrx a) decorate:: (DecorQuadrants a) -&gt; (DecorMatrx a) decorate (ZeroD,ZeroD,ZeroD,ZeroD) = ZeroD decorate quads = MtxD <p> So the net overhead of parallel processing is reduced by eliminating larger blocks, in proportion to their sizes. In order to choose a larger pivot block, decor need only favor shallower blocks; signposts already contain the necessary information. Algorithms for sparse matrices often use a Markowitz count <ref> [7, x9.2] </ref> to avoid fill-in. It predicts the fill caused by eliminating any candidate, based on two vectors: the row counts and column counts, whose size is the order of the matrix. <p> The remarks in this section, therefore, are speculative. A good preordering algorithm decomposes the domain into nearly independent subdomains, assembling zeros into off-diagonal blocks <ref> [7] </ref>. Under block-oriented matrix representations, moreover, it should try to compact either zeroes or nonzeroes within each block. In the case of the quadtree representation, these blocks are the subtrees.
Reference: [8] <author> J. W. Demmel & N. J. Higham. </author> <title> Stability of block algorithms with fast Level 3 BLAS. </title> <journal> ACM Trans. Math. </journal> <volume> Software 18, </volume> <month> 3 (September </month> <year> 1992), </year> <month> 274291. </month>
Reference-contexts: Most importantly, the order of a block-operand is here allowed to vary irregularly from one step to the next; this so-called undulant blocking is the major distinction between these algorithms and others' <ref> [16, 8] </ref> that impose a uniform order. Three interrelated contributions can be found in this paper. 4 0 INTRODUCTION. * The collection of block algorithms for undulant-block decomposition along with associated sequels for solving a linear system and for inverting a matrix.
Reference: [9] <author> J. Dongarra, J. DuCroz, S. Hammarling, & R. Hanson. </author> <title> An extended set of FORTRAN basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw. </journal> <volume> 14 (1988), 117. </volume> <pages> 70 REFERENCES </pages>
Reference-contexts: Under parallel processing, block manipulation reduces the granularity of both scheduling and communication [6]. Such improvement from block operations is well known; matrix-matrix operations are commonly called Level 3 (BLAS) <ref> [9] </ref>, with matrix-vector operations rated Level 2. Statically sized blocks are used by existing block algorithms for GE under one of two philosophies.
Reference: [10] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. </journal> <volume> Software 15, 1, </volume> <pages> 114. </pages>
Reference-contexts: The languages are C, SCHEME, and HASKELL [14]; the computers include BBN Butterflies [3], a NeXT, SGIs, Suns, and Macintoshes. Test data has been taken from the HarwellBoeing collection <ref> [10] </ref>. Performance is favorable, within the ability of each implementation to mimic an idealized environment. The leaf nodes may be the 1 fi 1 scalars as defined here, or conventional arrays 66 6 CONCLUSIONS. sized to fit in a line of cache.
Reference: [11] <author> V. N. </author> <title> Faddeeva Computational Methods of Linear Algebra. </title> <publisher> Dover Publications, </publisher> <address> New York (1959). </address> <note> Translated from Russian, originally published Moscow (1950). </note>
Reference-contexts: Not so long before the serial addressing of a FORTRAN array fixed our attention on row/column operations, undulant-block decomposition was recognized <ref> [11] </ref> as an important way to divide-and-conquer matrix problems. More recently, block organization re-emerged [17] for sparse matrices [16] and as a tactic to improve locality, a problem that manifested itself earlier as page faults [13], and lately as block transfers among local memories of a multiprocessor [15].
Reference: [12] <editor> J. Fasel, P. Hudak, S. Peyton Jones, & P. Wadler (eds.) </editor> <title> HASKELL special issue. </title> <journal> ACM SIGPLAN Notices 27, </journal> <month> 5 (May </month> <year> 1992). </year>
Reference-contexts: The next section shows how addition and multiplication, for instance, decompose to independent, parallel processes on subtrees. The quadtree structure encourages such divide-and-conquer algorithms, which descend into their operand-trees, and probe to leaves from levels deep in the tree, from twigs near the leaves. 1.4 Ring algorithms. HASKELL <ref> [12] </ref> is the programming language used in this paper. HASKELL is the international functional-programming language and, therefore, its code is devoid of any sequentiality except that implicit in the dependence of one result upon another. <p> No permutations are performed until Algorithm 2.2. Versions of these algorithms for quadtrees (cf. Section 3) have been 20 2 L + U 0 DECOMPOSITION. programmed in SCHEME, C, and HASKELL, the international functional programming language <ref> [12] </ref>. A transformation from one sextuple hA l ; d l ; S l ; l ; l ; l i to the next, hA l+1 ; d l+1 ; S l+1 ; l+1 ; l+1 ; l+1 i, is described below. An example is given in the others. <p> The data dependencies under strict evaluation suggest that the northwest work is completed, then northeast and southwest can be done in parallel, and southeast follows the completion of northeast. Through lazy evaluation <ref> [12] </ref>, however, one can imagine northwest and northeast eliminations proceeding almost simultaneously, with southeast and southwest lagging not too far behind; depending on communication patterns, all four can advance at once. 3.2 Padding via permutation. <p> Contemporary programming styles can better manipulate both functions that return multiple results <ref> [12] </ref> and tree structures that stash intermediate decorations at internal nodes. The impact of this distribution on locality in large systems is stark. Not only does it eliminate the extra traversal (communication) usually associated with separate pivoting, but also it admits complete pivoting at the cost of little space.
Reference: [13] <author> P. C. Fischer & R. L. Probert. </author> <title> Storage reorganization techniques for matrix computation in a paging environment. </title> <journal> Comm. ACM 22, </journal> <month> 7 (July </month> <year> 1979), </year> <pages> 405 415. </pages>
Reference-contexts: Non-trivial blocking uses hierarchical <ref> [15, 13] </ref> and distributed memory more effectively than row-based methods. <p> this formulation holds invariant the blocks in the pivot stripe (A mw and A me ) during the elimination step there, and those in the pivot column (L) later during the last step of inversion (cf. base cases of f; h in Algorithm 2.6.) With remote memory, caching, or paging <ref> [13] </ref>, the pages of the pivot stripe or colonnade become dirty in just one of the two steps, and under parallelism the processes there similarly encapsulate more local results or none at all. <p> More recently, block organization re-emerged [17] for sparse matrices [16] and as a tactic to improve locality, a problem that manifested itself earlier as page faults <ref> [13] </ref>, and lately as block transfers among local memories of a multiprocessor [15].
Reference: [14] <author> J. Frens & D. S. Wise. </author> <title> Matrix inversion Using quadtrees implemented in GOFER. </title> <type> Technical Report 433, </type> <institution> Computer Science Dept., </institution> <note> Indiana University (May 1995). </note>
Reference-contexts: Prolonged contact with the computer turns mathematicians into clerks and vice versa. [18, -80] Present implementations <ref> [3, 14] </ref> of these algorithms do not order the basis of the vector space before elimination. The remarks in this section, therefore, are speculative. A good preordering algorithm decomposes the domain into nearly independent subdomains, assembling zeros into off-diagonal blocks [7]. <p> Sketched in Section 3.3, this work is ongoing. The algorithms described here have now been implemented in various languages and run on various computers, both uniprocessors and a multiprocessor. The languages are C, SCHEME, and HASKELL <ref> [14] </ref>; the computers include BBN Butterflies [3], a NeXT, SGIs, Suns, and Macintoshes. Test data has been taken from the HarwellBoeing collection [10]. Performance is favorable, within the ability of each implementation to mimic an idealized environment.
Reference: [15] <author> K. A. Gallivan, R. J. Plemmons, & A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <note> SIAM Review 32, </note> <month> 1 (March </month> <year> 1990), </year> <pages> 54-135. </pages>
Reference-contexts: Non-trivial blocking uses hierarchical <ref> [15, 13] </ref> and distributed memory more effectively than row-based methods. <p> More recently, block organization re-emerged [17] for sparse matrices [16] and as a tactic to improve locality, a problem that manifested itself earlier as page faults [13], and lately as block transfers among local memories of a multiprocessor <ref> [15] </ref>.
Reference: [16] <author> A. George. </author> <title> On block elimination for sparse linear systems. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 11, </volume> <month> 3 (June </month> <year> 1974), </year> <month> 585603. </month>
Reference-contexts: Most importantly, the order of a block-operand is here allowed to vary irregularly from one step to the next; this so-called undulant blocking is the major distinction between these algorithms and others' <ref> [16, 8] </ref> that impose a uniform order. Three interrelated contributions can be found in this paper. 4 0 INTRODUCTION. * The collection of block algorithms for undulant-block decomposition along with associated sequels for solving a linear system and for inverting a matrix. <p> It is required that the use and recycling of such nodes not mandate global garbage-collection. Indeed, these algorithms have also been used quite successfully to test a hardware-based reference-counting memory [24] with great success; all storage is recycled in real time and garbage collection is never needed. George <ref> [16] </ref> observed that block pivoting offers a middle ground between the complicated programming necessary for sparse matrix techniques, and the fill-in that results from straightforward code using a band or band-like ordering. <p> Not so long before the serial addressing of a FORTRAN array fixed our attention on row/column operations, undulant-block decomposition was recognized [11] as an important way to divide-and-conquer matrix problems. More recently, block organization re-emerged [17] for sparse matrices <ref> [16] </ref> and as a tactic to improve locality, a problem that manifested itself earlier as page faults [13], and lately as block transfers among local memories of a multiprocessor [15].
Reference: [17] <author> G. H. Golub & C. F. Van Loan. </author> <title> Matrix Computations 2nd edition. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore (1989). </address>
Reference-contexts: OpCount 1.3 Figure 2's multiplication of two triangular n fi n matrices requires at most (n 3 + n 2 )=3 scalar multiplications. An alternative to the product algorithm in Figure 2 is Strassen's algorithm <ref> [17, x1.3.8] </ref>, for which quadtrees are the ideal structure. It is not included here both for simplicity and because its pre-additions eradicate sparseness [21, 23] that might accelerate later multiplications. <p> Notation. Upper-case Greek letters denote lists of integers. 16 2 L + U 0 DECOMPOSITION. 2.1 Quasi-triangular matrices. The indexing in the next few definitions is the ordinary cartesian row/column indexing usual for scalar entries in a matrix. Familiarity with the triangular LU decomposition via scalar elimination is assumed <ref> [17, x3.2] </ref>. Definition. A matrix A is properly-lower (or -upper) triangular if a i;j = 0 for i j (respectively, i j). Corollary 2.1 If L is properly-lower triangular, then I L is unit-lower triangular. <p> A square matrix is quasi-diagonal if it has square submatrices (cells) along its main diagonal with its remaining elements equal to zero. The older term, quasi-diagonal [11][x1.5], is used, instead of block-diagonal <ref> [17, x1.3.1] </ref>, to emphasize that the diagonal blocks are square. The square cells, of varying or undulating size will coincide with eliminated blocks, in the order in which they are eliminated; since the cells' sizes do undulate, their sizes must be recorded. Notation. <p> pairwise. l+1 = l + +[k]; l+1 = l + +[RA (i; k; n)]; l+1 = l + +[RA (j; k; n)]: Then compute hS mc ; ti = hA 1 G s = A nc The intermediate result, G should be viewed as a single colonnade (a Gauss vector <ref> [17, x3.2.1] </ref>) in the spanning space. d l+1 = t d l ; S l+1 = B S nw S nc G n S ne S sw S sc G s S se C 2.3 Decomposition algorithm. 21 The direct sum, , can be used here because its terms are respectively <p> If decorations were to identify the largest (in magnitude) scalar element that had not yet been eliminated (to assure the smallest possible multipliers in the Gauss transform <ref> [17] </ref>, of Figures 9 and 10), then it would suffice that each decoration also include the maximum magnitude of all scalars in that quadrant. That is, More in than max applied to the four local maxima, or the four absolute values when the quadrants are scalars. <p> Not so long before the serial addressing of a FORTRAN array fixed our attention on row/column operations, undulant-block decomposition was recognized [11] as an important way to divide-and-conquer matrix problems. More recently, block organization re-emerged <ref> [17] </ref> for sparse matrices [16] and as a tactic to improve locality, a problem that manifested itself earlier as page faults [13], and lately as block transfers among local memories of a multiprocessor [15].
Reference: [18] <author> A. J. Perlis. </author> <title> Epigrams on programming. </title> <journal> ACM SIGPLAN Notices 17, </journal> <month> 9 (September </month> <year> 1982), </year> <month> 713. </month>
Reference-contexts: General Term: Algorithms. Additional Key Words and Phrases: Gaussian elimination, block algorithm, LU factorization, quadtree matrices, quaternary trees, pivoting, exact arithmetic. 0 Introduction. You can measure a programmer's perspective by noting his attitude on the continuing vitality of FORTRAN. <ref> [18, -42] </ref> Problems from linear algebra, like solving linear systems and inverting matrices, occupy very important roles in the development of computing. <p> Section 4 presents the exact-arithmetic analogs of these algorithms, and Section 5 anticipates improvements from ordering the basis. Finally, the last section offers conclusions. 1 Quadtree representation. Symmetry is a complexity-reducing concept (co-routines include subroutines); seek it everywhere. <ref> [18, -6] </ref> 1.1 Arrays as trees. The quadtree representation of matrices [21] motivated the algorithms to follow. Conceptually, it decomposes full matrices as balanced quaternary trees. Sparse matrices are accommodated by providing a distinguished representation for an entirely zero submatrix, essentially a null pointer (cf. hypermatrices [7]). <p> That relative cost is reflected by these coefficients. 2 L + U decomposition. Functions delay binding; data structures induce binding. Moral: Structure data late in the programming process. <ref> [18, -2] </ref> An undulant version of LU decomposition is presented independently of the quadtree representation of matrices. While the theory here is not new, this presentation replaces the traditional row/column decomposition of matrices with arbitrary-block decomposition. <p> Make no mistake about it: Computers process numbersnot symbols. We measure our understanding (and control) by the extent to which we can arithmetize an activity. <ref> [18, -65] </ref> The algorithms of Section 2 have a common philosophy of decomposing matrices into blocks. This section marries those algorithms to the quadtree representation 34 3 DECOMPOSITION OF QUADTREE MATRICES. of Section 1. <p> That is, the 5-way Wilkinsonmax for decorating floating-point matrices (4 subtrees plus 1 enclosing tree) becomes a 5-way min for decorating integer matrices excluding, of course, zero trees. 4 Exact-arithmetic decomposition. Simplicity does not precede complexity, but follows it. <ref> [18, -31] </ref> 4.1 Integer-matrix inversion. Integer solutions for linear systems are important in rational arithmetic and symbolic computation. <p> Prolonged contact with the computer turns mathematicians into clerks and vice versa. <ref> [18, -80] </ref> Present implementations [3, 14] of these algorithms do not order the basis of the vector space before elimination. The remarks in this section, therefore, are speculative. A good preordering algorithm decomposes the domain into nearly independent subdomains, assembling zeros into off-diagonal blocks [7]. <p> If we believe in data structures, we must believe in independent (hence simultaneous) processing. For why else would we collect items within a structure? Why do we tolerate languages that give us the one without the other? <ref> [18, -68] </ref> 6.1 History. This work began as an exercise in applying lessons from functional programming to a problem that was so well refined that customized hardware (pipelined processors) had already found a large market.
Reference: [19] <author> H. Samet. </author> <title> Algorithms for the conversion of quadtrees to rasters. </title> <journal> IEEE Trans. Pattern Anal. and Machine Intelligence PAMI3, </journal> <month> 1 (January </month> <year> 1981), </year> <month> 9395. </month>
Reference-contexts: The only stripes 2 Reverse-Ahnentafel indexing, like floating-point numbers and even the quadtree representation, itself, is an internal representation that speeds computation. All three are isomorphic to alternative representations that are more easily read by humans, but translations often are compu-tationally difficult <ref> [19] </ref>. Such translations never occur during routine computation, however, and are overlapped with trudging input/output whenever they become necessary. 10 1 QUADTREE REPRESENTATION. and colonnades of interest in quadtree matrices will be those that have Ahnentafel indices.
Reference: [20] <author> G. W. </author> <title> Stewart Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York (1973). </address>
Reference-contexts: The idea of distributing the search across the preceding pivot [22] is not new here <ref> [20, p. 154] </ref>, but it is remarkable that its desertion was not attributed to limitations of architecture (at that time, before RISC), but rather to the shadow that the higher-level programming language cast over it.
Reference: [21] <author> D. S. Wise. </author> <title> Representing matrices as quadtrees for parallel processors (extended abstract). </title> <journal> ACM SIGSAM Bulletin 18, </journal> <note> 3 (August 1984), 2425. REFERENCES 71 </note>
Reference-contexts: Section 4 presents the exact-arithmetic analogs of these algorithms, and Section 5 anticipates improvements from ordering the basis. Finally, the last section offers conclusions. 1 Quadtree representation. Symmetry is a complexity-reducing concept (co-routines include subroutines); seek it everywhere. [18, -6] 1.1 Arrays as trees. The quadtree representation of matrices <ref> [21] </ref> motivated the algorithms to follow. Conceptually, it decomposes full matrices as balanced quaternary trees. Sparse matrices are accommodated by providing a distinguished representation for an entirely zero submatrix, essentially a null pointer (cf. hypermatrices [7]). <p> An alternative to the product algorithm in Figure 2 is Strassen's algorithm [17, x1.3.8], for which quadtrees are the ideal structure. It is not included here both for simplicity and because its pre-additions eradicate sparseness <ref> [21, 23] </ref> that might accelerate later multiplications. If it were included, however, all the n 3 factors in these counts could be replaced by n 2:81 with the same coefficients.
Reference: [22] <author> D. S. Wise. </author> <title> Parallel decomposition of matrix inversion using quadtrees. </title> <booktitle> Proc. 1986 International Conference on Parallel Processing (IEEE Cat. </booktitle> <volume> No. 86CH23556), </volume> <pages> 9299. </pages>
Reference-contexts: More recently, block organization re-emerged [17] for sparse matrices [16] and as a tactic to improve locality, a problem that manifested itself earlier as page faults [13], and lately as block transfers among local memories of a multiprocessor [15]. The idea of distributing the search across the preceding pivot <ref> [22] </ref> is not new here [20, p. 154], but it is remarkable that its desertion was not attributed to limitations of architecture (at that time, before RISC), but rather to the shadow that the higher-level programming language cast over it.
Reference: [23] <author> D. S. Wise & J. Franco. </author> <title> Costs of quadtree representation of non-dense matrices. </title> <journal> J. Parallel Distrib. Comput. </journal> <volume> 9, </volume> <month> 3 (July </month> <year> 1990), </year> <month> 282296. </month>
Reference-contexts: Similarly, no representation of a quadtree-permutation matrix has both northeast and southwest quadrants 0 while both its northwest and southeast are simultaneously either 0 or I. Table 1 summarizes space and access-time asymptotes extracted from the analytic results of Wise and Franco <ref> [23] </ref>. They show how familiarly patterned matrices are uniformly represented in expectedly shrinking space, albeit with proportional overhead beyond case-specific data structures. The expected path here reflects the cost to access a random [i; j] element of a matrix, from the root of the entire tree. <p> An alternative to the product algorithm in Figure 2 is Strassen's algorithm [17, x1.3.8], for which quadtrees are the ideal structure. It is not included here both for simplicity and because its pre-additions eradicate sparseness <ref> [21, 23] </ref> that might accelerate later multiplications. If it were included, however, all the n 3 factors in these counts could be replaced by n 2:81 with the same coefficients.
Reference: [24] <author> D. S. Wise, B. Heck, C. Hess, W. Hunt & E. </author> <title> Ost. Research demonstration of a hardware reference-counting heap. LISP & SYMBOLIC COMPUTATION (to appear). </title> <type> Technical Report 401, </type> <institution> Computer Science Dept., </institution> <note> Indiana University (May 1994). </note>
Reference-contexts: It is required that the use and recycling of such nodes not mandate global garbage-collection. Indeed, these algorithms have also been used quite successfully to test a hardware-based reference-counting memory <ref> [24] </ref> with great success; all storage is recycled in real time and garbage collection is never needed. George [16] observed that block pivoting offers a middle ground between the complicated programming necessary for sparse matrix techniques, and the fill-in that results from straightforward code using a band or band-like ordering.
References-found: 24

