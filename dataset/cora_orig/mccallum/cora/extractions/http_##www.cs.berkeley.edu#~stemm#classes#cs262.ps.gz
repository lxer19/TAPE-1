URL: http://www.cs.berkeley.edu/~stemm/classes/cs262.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~stemm/pubs.html
Root-URL: 
Email: dbanks@cs.berkeley.edu  stemm@cs.berkeley.edu  
Title: Investigating Virtual Memory Compression on Portable Architectures  
Author: Doug Banks Mark Stemm 
Keyword: mobile computing, PDAs, virtual memory, compression, wireless communication.  
Date: January 19, 1995  
Address: Berkeley  Berkeley  
Affiliation: Computer Science Division University of California at  Computer Science Division University of California at  
Abstract: The largest cost in servicing virtual memory page faults is the time needed to swap pages to and from backing store. With the increase in portable machines connected to a network across slow wireless links, this cost could become unacceptable. One solution is to use a compression cache, keeping some virtual memory pages in compressed form rather than sending them to the backing store. We have implemented a compression cache similar to previous work with some additional optimizations designed to manage the compressed pages more effectively. We measured page fault and compression counts using a variety of test programs, and applied architecture-specific numbers to determine the effectiveness of a compression cache under many different situations. We also determine the effectiveness of our optimizations. Also measured is the usefulness of virtual memory compression when there is no access to a backing store at all. Results show that a compression cache is useful under a wide variety of portable architectures, and can improve page fault response time by a factor of two. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter J Denning. </author> <title> The working set model for program behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-332, </pages> <year> 1968. </year>
Reference-contexts: Because this is generally impossible to predict, various page replacement algorithms, for example the clock algorithm (LRU) or the working-set model <ref> [1] </ref>, have been designed to approximate the optimal case based on past behavior. The problem with these algorithms is that they each perform rather poorly for some particular referencing pattern.
Reference: [2] <author> Fred Douglis. </author> <title> The compression cache: Using on-line compression to extend physical memory. </title> <booktitle> USENIX Proceedings, </booktitle> <month> Winter </month> <year> 1993. </year>
Reference-contexts: An alternative approach to reducing backing store traffic which we investigate here is to increase the number of pages that can be held in memory by using data compression. This idea was originally explored in depth in <ref> [2] </ref> for a workstation environment, in which the swap file was managed on a local disk. <p> They further suggest that it might be useful to allow user programs to manage this compression, influencing how and when pages are compressed. Fred Douglis implemented this basic idea without user-level control and presented this results in <ref> [2] </ref>. He augmented the Sprite operating system to compress swapped-out pages using the LZRW1 compression algorithm and store them on-memory in a compression cache, managing the cache as a variable-sized circular buffer. The compressed VM system was then tested for a range of applications on DECstation 5000/200 workstations.
Reference: [3] <author> A. W. Appel and K. Li. </author> <title> Virtual memory primitives for user programs. </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Operating Systems, </booktitle> <pages> pages 96-107, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We were unable to obtain a copy of this paper, but our work was largely influenced by conversations with Marvin Theimer, one of the authors. In <ref> [3] </ref>, a paper discussing alternative uses for various virtual memory primitives, Appel % Li briefly introduce the idea of compressing pages on-line and caching them in memory to avoid backing store traffic.
Reference: [4] <author> Mark Weiser. </author> <title> Some computer science issues in ubiquitous computing. </title> <journal> Communications of the ACM, </journal> <volume> 36(7) </volume> <pages> 74-83, </pages> <year> 1993. </year>
Reference-contexts: He mentions wireless networks as a sample environment with a much slower time to backing store. This suggestion is especially interesting in light of the advent of portable machines and ubiquitous computing <ref> [4] </ref>.
Reference: [5] <author> Michael Culbert. </author> <title> Low power hardware for a high performance pda. </title> <journal> IEEE Computer Society, </journal> <pages> pages 144-155, </pages> <month> Spring </month> <year> 1994. </year>
Reference-contexts: We decided to use the ARM710 as the CPU for the PDA device because its predecessor, the ARM610, was used in the Apple Newton <ref> [5] </ref>. 1 Because we were not able to directly measure the performance of the compression algorithm on the ARM 710, we calculated these numbers by measuring the speed of the compression code on the HP712/80 and multiplying this by the ratio of the dhrystones benchmark of the two machines. 6 Architecture
Reference: [6] <author> George H. Forman and John Zahorjan. </author> <title> The challenges of mobile computing. </title> <type> Technical Report 93-11-03, </type> <institution> University of Washington, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Although the idea of a diskless laptop may seem unrealistic at first, we felt it was worth considering because the hard drive motor on a laptop is the second largest consumer of energy after the display <ref> [6] </ref>, and replacing the disk by a wireless connection would greatly extend the battery life. 6.1 PDA Infrared, PDA Radio, Laptop Infrared The extremely slow network time allowed the system with the compression cache to outperform the system without the compression cache for every communication pattern (including locality 2 ) for
Reference: [7] <author> Rashid et al. </author> <title> Machine independent virtual memory management for paged uniprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 896-908, </pages> <year> 1988. </year>
Reference-contexts: We also found it impossible to find one set of parameters for the optimized compression cache that worked well for all applications. It could be possible to design a user-level compressor similar to the user-level pagers in Mach <ref> [7] </ref> or V++[8] where applications can have more control in how VM compression was done. This could be combined with some of the different compression methods above to allow user programs to tune their memory usage depending on the application.
Reference: [8] <author> Kieran Harty and David R. Cheriton. </author> <title> Application-controlled physical memory using external page-cache management. </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Operating Systems, </booktitle> <pages> pages 187-199, </pages> <month> Oct </month> <year> 1992. </year> <month> 11 </month>
References-found: 8

