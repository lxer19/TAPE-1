URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-339.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: picard@media.mit.edu,  
Title: Video and Image Libraries of the Future  
Author: Rosalind W. Picard 
Web: http://www.media.mit.edu/~picard/  
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: MIT Media Laboratory,  
Note: Light-years from Lena:  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 339 Also appearing: Int. Conf. on Image Proc., Washington DC, Oct. 1995. Abstract The average consumer with a personal computer will soon have access to the world's collections of digital video and images. However, the theory and tools that facilitate browsing, querying, retrieval, and manipulation of imagery are still in their infancy. For example, people would like to access content in movies, e.g. "fast forward to where they bicycle through the sky." This new application area reveals an abundance of unsolved scientific problems for image processing. In this paper I overview key technical challenges that the image processing community should em brace.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. M. Haralick and L. G. Shapiro, </author> <title> "Image segmentation techniques," Comp. Vis., Graph., </title> <journal> and Img. Proc., </journal> <volume> vol. 29, </volume> <pages> pp. 100-132, </pages> <year> 1985. </year>
Reference-contexts: Although problems in these two domains might be posed with the same Bayesian theory, the different cost criteria warrant a fresh investigation for the new consumer applications. 2.2 Find interest regions, not one segmentation Segmentation has been a topic in computer vision for decades <ref> [1] </ref>, and has recently become an area of image processing research. Traditionally, an image segmentation is defined as a partition of the image into homogeneous regions, where "region" depends on a specified definition of homogeneity.
Reference: [2] <author> R. W. Picard and T. P. Minka, </author> <title> "Vision texture for annotation," </title> <journal> Journal of Multimedia Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 3-14, </pages> <year> 1995. </year>
Reference-contexts: As the user interacts with the system, the system dynamically determines which model or combination of models best describes the regions of interest. This is the "society of models" approach of Picard and Minka <ref> [2] </ref>. This approach is similar in spirit to the learning algorithm of Delanoy and Sasiela [3]. In both cases, the system accepts positive and negative examples of regions from a user, then determines what is the best model or set of features for subsequent classification and identification. <p> In contrast, distance measures applied to suitable color and texture features can perform surprisingly well for image retrieval [13], [14]. Nonetheless similarity is complicated by many factors, and no one criterion of similarity, e.g. perceptual or semantic, will be appropriate for all applications <ref> [2] </ref>. More research is needed to determine suitable features and similarity measures for comparing visual information. 2.7 Motion analysis: camera, action Motion analysis can be divided into scene motion (actions) and camera motion.
Reference: [3] <author> R. L. Delanoy and R. J. Sasiela, </author> <title> "Machine learning for a toolkit for image mining," </title> <institution> Lincoln Laboratory 1017, MIT, Lexington, </institution> <address> MA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: As the user interacts with the system, the system dynamically determines which model or combination of models best describes the regions of interest. This is the "society of models" approach of Picard and Minka [2]. This approach is similar in spirit to the learning algorithm of Delanoy and Sasiela <ref> [3] </ref>. In both cases, the system accepts positive and negative examples of regions from a user, then determines what is the best model or set of features for subsequent classification and identification.
Reference: [4] <author> F. Arman, A. Hsu, and M.-Y. Chiu, </author> <title> "Feature management for large video databases," in Storage and Retrieval for Image and Video Databases (W. Niblack, </title> <editor> ed.), </editor> <address> (San Jose, CA), </address> <pages> pp. 2-12, SPIE, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: A "segment" is a sequence of scenes that forms a story unit, e.g., a flashback. Video parsing research has been directed so far toward the problem of detecting shots, e.g. Araman et al. <ref> [4] </ref>, Tonomura et al. [5], and Zhang et al. [6]. Most of the methods have relied either on differencing (high-pass filtering) all the pixels or a subset of them in two frames, or on differences of gray-level or color statistics. <p> However, if the goal is both compression and content-access, then the extra complexity is justified, for it saves enormous work during the searching stage. Shot change detection has already been run successfully on compressed MPEG and JPEG data <ref> [4] </ref> [6].
Reference: [5] <author> Y. Tonomura, A. Akutsu, Y. Taniguchi, and G. Suzuki, </author> <title> "Structured video computing," </title> <journal> IEEE Multimedia, </journal> <volume> vol. 1, </volume> <pages> pp. 34-43, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: A "segment" is a sequence of scenes that forms a story unit, e.g., a flashback. Video parsing research has been directed so far toward the problem of detecting shots, e.g. Araman et al. [4], Tonomura et al. <ref> [5] </ref>, and Zhang et al. [6]. Most of the methods have relied either on differencing (high-pass filtering) all the pixels or a subset of them in two frames, or on differences of gray-level or color statistics. In both cases a close analogy exists to early work in spatial edge detection. <p> See http://www-white.media.mit.edu/~steve/orbits/orbits.html for a closer look. 2.8 Browsing video It is unclear at this stage what will be the best way to browse video. Tonomura et al. have outlined several possible solutions <ref> [5] </ref>. One of these options, the stroboscopic browser, attempts to make a still image out of a shot or scene.
Reference: [6] <author> H.-J. Zhang, C. Y. Low, and S. W. Smoliar, </author> <title> "Video parsing and browsing using compressed data," </title> <booktitle> Multimedia Tools and Applications, </booktitle> <volume> vol. 1, </volume> <pages> pp. 80-111, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: A "segment" is a sequence of scenes that forms a story unit, e.g., a flashback. Video parsing research has been directed so far toward the problem of detecting shots, e.g. Araman et al. [4], Tonomura et al. [5], and Zhang et al. <ref> [6] </ref>. Most of the methods have relied either on differencing (high-pass filtering) all the pixels or a subset of them in two frames, or on differences of gray-level or color statistics. In both cases a close analogy exists to early work in spatial edge detection. <p> However, if the goal is both compression and content-access, then the extra complexity is justified, for it saves enormous work during the searching stage. Shot change detection has already been run successfully on compressed MPEG and JPEG data [4] <ref> [6] </ref>. Smith and 3 Chang have also run retrieval algorithms directly on com-pressed data [22]. 3 Summary: Hard questions In this short paper I have tried to overview key image processing research problems which must be solved to give people access to the content of digital video and image libraries.
Reference: [7] <author> A. Hampapur, R. Jain, and T. T. Weymouth, </author> <title> "Production model based digital video segmentation," </title> <booktitle> Multimedia Tools and Applications, </booktitle> <volume> vol. 1, </volume> <pages> pp. 9-46, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: In both cases a close analogy exists to early work in spatial edge detection. Matched-filter methods are also useful, and have the ability to both detect and identify edited transitions <ref> [7] </ref>. Parsing video into scenes and segments is a harder process that has not been carefully examined. The transition from shots to segments parallels the transition between low-level and high-level processing in computer vision.
Reference: [8] <author> W. Niblack, R. Barber, W. Equitz, M. Flickner, E. Glasman, D. Petkovic, P. Yanker, C. Faloutsos, and G. Taubin, </author> <title> "The QBIC project: Querying images by content using color, texture, and shape," in Storage and Retrieval for Image and Video Databases (W. Niblack, </title> <editor> ed.), </editor> <address> (San Jose, CA), </address> <pages> pp. 173-181, SPIE, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Very little work has been done in this area, although it appears to be an important step toward characterizing video content. 2.5 Content-based retrieval and organization Research in image retrieval is relatively new, with the initial largest efforts by IBM <ref> [8] </ref>, ISS [9], and MIT [10]. These systems have emphasized representations of shape, color, and texture. For example, in Fig. 1, a screenful of fabric samples is shown, part of a database used in the apparel industry. Each sample is represented by a set of pre-computed color and texture features.
Reference: [9] <author> H.-J. Zhang, S. W. Smoliar, J. H. Wu, C. Y. Low, and A. Kankanhalli, </author> <title> "A video database system for digital libraries." </title> <type> ISS, </type> <institution> Nat. Univ. Singapore, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Very little work has been done in this area, although it appears to be an important step toward characterizing video content. 2.5 Content-based retrieval and organization Research in image retrieval is relatively new, with the initial largest efforts by IBM [8], ISS <ref> [9] </ref>, and MIT [10]. These systems have emphasized representations of shape, color, and texture. For example, in Fig. 1, a screenful of fabric samples is shown, part of a database used in the apparel industry. Each sample is represented by a set of pre-computed color and texture features.
Reference: [10] <author> A. Pentland, R. Picard, and S. Sclaroff, "Photo-book: </author> <title> Tools for content-based manipulation of image databases," </title> <journal> Int'l Journal of Computer Vision, </journal> <note> 1995. in press. </note>
Reference-contexts: Very little work has been done in this area, although it appears to be an important step toward characterizing video content. 2.5 Content-based retrieval and organization Research in image retrieval is relatively new, with the initial largest efforts by IBM [8], ISS [9], and MIT <ref> [10] </ref>. These systems have emphasized representations of shape, color, and texture. For example, in Fig. 1, a screenful of fabric samples is shown, part of a database used in the apparel industry. Each sample is represented by a set of pre-computed color and texture features.
Reference: [11] <author> L. A. Rowe, J. S. Boreczky, and C. A. Eads, </author> <title> "Indexes for user access to large video databases," in Storage and Retrieval for Image and Video Databases II (W. </title> <editor> Niblack and R. C. Jain, eds.), </editor> <address> (San Jose, CA), </address> <pages> pp. 150-161, SPIE, </pages> <month> Feb. </month> <year> 1994. </year> <note> Vol. 2185. </note>
Reference-contexts: An example of retrieval is shown at the right in Fig. 1, where the system has found images similar to the one at the upper left and displayed them in raster-scan order according to their texture similarity. The features needed for retrieval vary with the domain. Rowe et al. <ref> [11] </ref> identified three types of indices for queries in video on demand systems: bibliographic (title, genre, etc.), structural (shot, scene, segment, etc.), and content (actors and objects in scenes, etc.) Romer [12] identified three types of requests in stock photo searches: subject and action content (three stooges, throwing a pie), picture
Reference: [12] <author> D. Romer, </author> <title> "The Kodak picture exchange," </title> <address> April 1995. </address> <publisher> seminar at MIT. </publisher>
Reference-contexts: The features needed for retrieval vary with the domain. Rowe et al. [11] identified three types of indices for queries in video on demand systems: bibliographic (title, genre, etc.), structural (shot, scene, segment, etc.), and content (actors and objects in scenes, etc.) Romer <ref> [12] </ref> identified three types of requests in stock photo searches: subject and action content (three stooges, throwing a pie), picture syntax (horizontal structures, smooth open spaces), and subjective components (mood frolicsome, powerful). In commercial photography for advertising the subjective features were most important. <p> For example, useful categories exist for photographs with people: face, face & partial body, face & full body, two people, picture within a picture, group, crowd, one in front of many, etc. <ref> [12] </ref>. In some cases the person does not know what they want to retrieve; they say, "I'll know it when I see it." However, they do not have time to look at even a thumbnail version of the millions of images available.
Reference: [13] <author> M. J. Swain and D. H. Ballard, </author> <title> "Indexing via color histograms," </title> <booktitle> in Image Understanding Workshop, </booktitle> <address> (Pitts-burgh, PA), </address> <pages> pp. 623-630, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: In contrast, distance measures applied to suitable color and texture features can perform surprisingly well for image retrieval <ref> [13] </ref>, [14]. Nonetheless similarity is complicated by many factors, and no one criterion of similarity, e.g. perceptual or semantic, will be appropriate for all applications [2].
Reference: [14] <author> F. Liu and R. W. </author> <title> Picard, "Periodicity, directionality, and randomness: Wold features for perceptual pattern recognition," </title> <booktitle> in Proc. Int. Conf. Pat. Rec., vol. II, (Jerusalem, Israel), </booktitle> <pages> pp. 184-185, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: In contrast, distance measures applied to suitable color and texture features can perform surprisingly well for image retrieval [13], <ref> [14] </ref>. Nonetheless similarity is complicated by many factors, and no one criterion of similarity, e.g. perceptual or semantic, will be appropriate for all applications [2].
Reference: [15] <author> S. Mann and R. W. </author> <title> Picard, "Video orbits: characterizing the coordinate transformation between two images using the projective group," </title> <type> Tech. Rep. 278, </type> <institution> MIT Media Lab, Perceptual Computing, </institution> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: When the scene is static and the camera motion does not induce much parallax, then the camera parameters can be solved and used to transform the coordinates of one frame so that it aligns with another; this problem has recently been solved using the exact projective coordinate transform <ref> [15] </ref>. When there are multiple motions in the scene, these can be approximately found using affine methods [16] or methods with also account for parallax [17]. Recognition of the extracted motion is an important new area of research which merits increased effort. <p> Tonomura et al. have outlined several possible solutions [5]. One of these options, the stroboscopic browser, attempts to make a still image out of a shot or scene. These "dynamic mosaics" [17] or "summary frames" <ref> [15] </ref> combine lots of frames seamlessly into a new image using either an affine model with parallax correction [17] or a direct projective method [15]. An example of the latter is shown in Fig. 2. Here, frames of video were automatically coordinate-transformed to align them on top of each other. <p> These "dynamic mosaics" [17] or "summary frames" <ref> [15] </ref> combine lots of frames seamlessly into a new image using either an affine model with parallax correction [17] or a direct projective method [15]. An example of the latter is shown in Fig. 2. Here, frames of video were automatically coordinate-transformed to align them on top of each other. If a football player moved between frames, he shows up in a "stroboscopic" trajectory according to the path he moved.
Reference: [16] <author> M. Irani, B. Rousso, and S. Peleg, </author> <title> "Detecting and tracking multiple moving objects using temporal integration," </title> <booktitle> in Computer Vision - ECCV, </booktitle> <address> (Santa Margherita Ligure, Italy), </address> <pages> pp. 282-287, </pages> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: When there are multiple motions in the scene, these can be approximately found using affine methods <ref> [16] </ref> or methods with also account for parallax [17]. Recognition of the extracted motion is an important new area of research which merits increased effort.
Reference: [17] <author> H. S. Sawhney, S. Ayer, and M. Gorkani, </author> <title> "Model-based 2d & 3d dominant motion estimation for mo-saicing and video representation," </title> <booktitle> in ICCV, </booktitle> <address> (Cam-bridge, MA), </address> <month> June </month> <year> 1995. </year>
Reference-contexts: When there are multiple motions in the scene, these can be approximately found using affine methods [16] or methods with also account for parallax <ref> [17] </ref>. Recognition of the extracted motion is an important new area of research which merits increased effort. Sometimes low-level features can be very successful for recognition of motion textures (water, leaves, etc.) and even human activities (see Polana and Nelson [18]). frames of a noisy football video. <p> Tonomura et al. have outlined several possible solutions [5]. One of these options, the stroboscopic browser, attempts to make a still image out of a shot or scene. These "dynamic mosaics" <ref> [17] </ref> or "summary frames" [15] combine lots of frames seamlessly into a new image using either an affine model with parallax correction [17] or a direct projective method [15]. An example of the latter is shown in Fig. 2. <p> One of these options, the stroboscopic browser, attempts to make a still image out of a shot or scene. These "dynamic mosaics" <ref> [17] </ref> or "summary frames" [15] combine lots of frames seamlessly into a new image using either an affine model with parallax correction [17] or a direct projective method [15]. An example of the latter is shown in Fig. 2. Here, frames of video were automatically coordinate-transformed to align them on top of each other.
Reference: [18] <author> R. Polana and R. Nelson, </author> <title> "Low level recognition of human motion," </title> <booktitle> in IEEE Workshop on Motion of Non-rigid and Articulated Objects, </booktitle> <address> (Austin, TX), </address> <year> 1994. </year>
Reference-contexts: Recognition of the extracted motion is an important new area of research which merits increased effort. Sometimes low-level features can be very successful for recognition of motion textures (water, leaves, etc.) and even human activities (see Polana and Nelson <ref> [18] </ref>). frames of a noisy football video. See http://www-white.media.mit.edu/~steve/orbits/orbits.html for a closer look. 2.8 Browsing video It is unclear at this stage what will be the best way to browse video. Tonomura et al. have outlined several possible solutions [5].
Reference: [19] <author> R. W. </author> <title> Picard, "Content access for image/video coding: `The Fourth Criterion'," </title> <type> Tech. Rep. 295, </type> <institution> MIT Media Lab, Perceptual Computing, </institution> <address> Cambridge, MA, </address> <year> 1994. </year> <title> MPEG Doc. </title> <type> 127, </type> <institution> Lausanne, </institution> <year> 1995. </year>
Reference-contexts: The problem is that coding has focused for years on maximizing rate while minimizing distortion and cost. Often this results in an encoded form which prohibits accessing the content. If the encoded form is also to be "searchable" then that adds a fourth criterion to the optimization <ref> [19] </ref>. However, jointly optimizing with respect to these four criteria can lead to much better systems. For example, designing a system that simultaneously compresses and classifies was found by Oehler and Gray [20] to give excellent results.
Reference: [20] <author> K. L. Oehler and R. M. Gray, </author> <title> "Combining image compression and classification using vector quantization," </title> <journal> IEEE T. Patt. Analy. and Mach. Intell., </journal> <volume> vol. 17, </volume> <pages> pp. 461-473, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: However, jointly optimizing with respect to these four criteria can lead to much better systems. For example, designing a system that simultaneously compresses and classifies was found by Oehler and Gray <ref> [20] </ref> to give excellent results. The more one knows about the content of an image, the better one should be able to compress it. Homogeneous regions, when they exist in images, can be compressed more efficiently [21]; this approach forms the basis of second-generation and model-based coding efforts.
Reference: [21] <author> M. Kunt, A. Ikonomopoulos, and M. Kocher, </author> <title> "Second-generation image-coding techniques," </title> <journal> Proc. IEEE, </journal> <volume> vol. 73, no. 4, </volume> <pages> pp. 549-574, </pages> <year> 1985. </year>
Reference-contexts: The more one knows about the content of an image, the better one should be able to compress it. Homogeneous regions, when they exist in images, can be compressed more efficiently <ref> [21] </ref>; this approach forms the basis of second-generation and model-based coding efforts. But rarely do large homogeneous regions exist to justify the extra gain in complexity for the goal of compression alone.
Reference: [22] <author> J. R. Smith and S.-F. Chang, </author> <title> "Transform features for texture classification and discrimination in large image databases," </title> <booktitle> in Proceedings ICIP, </booktitle> <address> (Austin, TX), </address> <pages> pp. 407-411, </pages> <month> Nov. </month> <year> 1994. </year> <title> Vol. </title> <booktitle> III. </booktitle> <pages> 4 </pages>
Reference-contexts: Shot change detection has already been run successfully on compressed MPEG and JPEG data [4] [6]. Smith and 3 Chang have also run retrieval algorithms directly on com-pressed data <ref> [22] </ref>. 3 Summary: Hard questions In this short paper I have tried to overview key image processing research problems which must be solved to give people access to the content of digital video and image libraries.
References-found: 22

