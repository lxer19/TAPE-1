URL: http://www.dcs.qmw.ac.uk/~scott/papers/KDD94.ps.gz
Refering-URL: http://www.dcs.qmw.ac.uk/~scott/papers/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: tmonk@cs.waikato.ac.nz, rsm1@cs.waikato.ac.nz, las@waikato.ac.nz, geoff@waikato.ac.nz  
Title: Geometric Comparison of Classifications and Rule Sets*  
Author: Trevor J. Monk, R. Scott Mitchell, Lloyd A. Smith and Geoffrey Holmes 
Keyword: Machine Learning, Classification, Rules, Geometric Comparison  
Address: Hamilton, New Zealand  
Affiliation: Department of Computer Science University of Waikato  
Abstract: We present a technique for evaluating classifications by geometric comparison of rule sets. Rules are represented as objects in an n-dimensional hyperspace. The similarity of classes is computed from the overlap of the geometric class descriptions. The system produces a correlation matrix that indicates the degree of similarity between each pair of classes. The technique can be applied to classifications generated by different algorithms, with different numbers of classes and different attribute sets. Experimental results from a case study in a medical domain are included. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cendrowska, J., </author> <year> 1987. </year> <title> PRISM: An algorithm for inducing modular rules. </title> <journal> International Journal of Man-Machine Studies. </journal> <volume> 27, </volume> <pages> 349-370. </pages>
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W. & Freeman, D., </author> <year> 1988. </year> <title> AUTO-CLASS: A Bayesian Classification System. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 54-64. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California. </address>
Reference-contexts: WEKA runs under UNIX on Sun workstations. Figure 1 shows an example display presented by the workbench. 2.1. AutoClass AUTOCLASS is an unsupervised induction algorithm that automatically discovers classes in a database using a Bayesian statistical technique. The Bayesian approach has several advantages over other methods <ref> (Cheeseman, et. al., 1988) </ref>. The number of classes is determined automatically; examples are assigned with a probability to each class rather than absolutely to a single class; all attributes are potentially significant to the classification; and the example data can be real or discrete.
Reference: <author> Compton, P., Edwards, G., Srinivasan, A., Malor, R., Preston, P., Kang, B. & Lazarus, L., </author> <year> 1992. </year> <title> Ripple down rules: turning knowledge aquisition into knowledge maintenance. </title> <booktitle> Artificial Intelligence in Medicine 4, </booktitle> <pages> 47-59. </pages>
Reference-contexts: The resulting rules are then generalised to remove conditions that do not contribute to the accuracy of the classification. A side-effect of this process is that the rules are no longer exhaus tive or mutually exclusive (Quinlan, 1992). C4.5 copes with this by using `ripple-down rules' <ref> (Compton, et. al., 1992) </ref>. The rules are ordered, and any example is then classified by the first rule that covers it.
Reference: <author> Fisher, D., </author> <year> 1987. </year> <title> Knowledge Acquisition Via Incremental Concept Clustering. </title> <booktitle> Machine Learning 2, </booktitle> <pages> 139-172. </pages>
Reference-contexts: Other measures are based on the class descriptionsin the form of decision trees, rules or a `concept hierarchy' as used by UNIMEM (Lebowitz, 1987) or COBWEB <ref> (Fisher, 1987) </ref>. Some clustering systems produce class descriptions as part of their normal, operation while others, such as AUTOCLASS (Cheese-man, et. al., 1988) merely assign examples to classes. In this case, a supervised learning algorithm such as C4.5 (Quinlan, 1992) can be used to induce descriptions for the classification.
Reference: <author> Friedman, H. & Rubin, J., </author> <year> 1967. </year> <title> On some invariant criteria for grouping data. </title> <journal> Journal of the American Statistical Association 62, </journal> <pages> 1159-1178. </pages>
Reference: <author> Hanson, S. & Bauer, M., </author> <year> 1989. </year> <title> Conceptual Clustering, Categorization, </title> <booktitle> and Polymor-phy. Machine Learning 3, </booktitle> <pages> 343-372. </pages>
Reference: <author> Kononenko, I. & Bratko, I., </author> <year> 1991. </year> <title> Information-Based Evaluation Criterion for Classifier's Performance. </title> <booktitle> Machine Learning 6, </booktitle> <pages> 67-80. </pages>
Reference: <author> Lebowitz, M., </author> <year> 1987, </year> <title> Experiments with Incremental Concept Formation: </title> <booktitle> UNIMEM. Machine Learning 2, </booktitle> <pages> 103-138. </pages>
Reference-contexts: This cohesion metric evaluates clusters in terms of their within-class and between-class similarities, using the training examples that have been assigned to each class. Other measures are based on the class descriptionsin the form of decision trees, rules or a `concept hierarchy' as used by UNIMEM <ref> (Lebowitz, 1987) </ref> or COBWEB (Fisher, 1987). Some clustering systems produce class descriptions as part of their normal, operation while others, such as AUTOCLASS (Cheese-man, et. al., 1988) merely assign examples to classes.
Reference: <author> McQueen, R., Neal, D., DeWar, R. & Garner, S., </author> <year> 1994. </year> <title> Preparing and processing relational data through the WEKA machine learning workbench. </title> <type> Internal report, </type> <institution> Department of Computer Science, University of Waikato, Hamilton, </institution> <address> New Zealand. </address>
Reference-contexts: Section 6 contains some concluding remarks and ideas for further research in this area. 2. The WEKA Workbench WEKA, 1 the Waikato Environment for Knowledge Analysis, is a machine learning workbench currently under development at the University of Waikato <ref> (McQueen, et. al, 1994) </ref>. The purpose of the workbench is to give users access to many machine learning algorithms, and to apply these to real-world data.
Reference: <author> Michalski, R. & Stepp, R., </author> <year> 1983. </year> <title> Learning from Observation: Conceptual Clustering. </title> <editor> In Michalski, R., Carbonell, J. & Mitchell, T. (eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 331-363. </pages> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto, Cal-ifornia. </address>
Reference-contexts: Consequently, the demands for simplicity and good fit are conflicting, and the solution is to find a balance between the two. The CLUSTER/2 algorithm used a combined measure of cluster quality based on a number of elementary criteria including the `simplicity of description' and `goodness of fit' mentioned above <ref> (Michalski & Stepp, 1983) </ref>. Hansen & Bauer (1987) use an information-theoretic measure of cluster quality in their WITT system. This cohesion metric evaluates clusters in terms of their within-class and between-class similarities, using the training examples that have been assigned to each class.
Reference: <author> Mingers, J., </author> <year> 1989. </year> <title> An Empirical Comparison of Pruning Methods for Decision Tree Induction. </title> <booktitle> Machine Learning 4, </booktitle> <pages> 227-243. </pages>
Reference-contexts: Accuracy is a measure of the predictive ability of the class description when classifying unseen test cases. It is usually measured by the error ratethe proportion of incorrect predictions on the test set <ref> (Mingers, 1989) </ref>. Accuracy is often used to measure classification quality, but it is known to have several defects (Mingers, 1989; Kononenko & Bratko, 1991).
Reference: <author> Nie, N., Hull, H., Jenkins, J., Steinbrenner, K. & Bent, D., </author> <year> 1975. </year> <title> SPSS: Statistical Package for the Social Sciences, 2nd ed. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> New York. </address>
Reference-contexts: For example, the Glucose mean for the Normal class of the electronic classification (EClass) will be compared with the Glucose mean for the Normal class of the clinical classification (CClass). We used a two way t-test <ref> (Nie, et. al., 1975) </ref> to compare the class means. The null hypothesis was that there is no difference between the two means. The level of similarity between two classifications is given by the number of rejected t-tests. All tests were performed at the 95% level of significance.
Reference: <author> Ousterhout, J., </author> <year> 1993. </year> <title> An Introduction to TCL and TK. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Reading, Massachusetts. </address>
Reference: <author> Quinlan, J., </author> <year> 1986. </year> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference-contexts: C4.5 C4.5 (Quinlan, 1992) is a powerful tool for inducing decision trees and production rules from a set of examples. Much of C4.5 is derived from Quinlan's earlier induction system, ID3 <ref> (Quinlan, 1986) </ref>. The basic ID3 algorithm has been extensively described, tested and modified since its invention (Mingers, 1989; Utgoff, 1989) and will not be discussed in detail here. However, C4.5 adds a number of enhancements to ID3, which are worth examining.
Reference: <author> Quinlan, J., </author> <year> 1992. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Some clustering systems produce class descriptions as part of their normal, operation while others, such as AUTOCLASS (Cheese-man, et. al., 1988) merely assign examples to classes. In this case, a supervised learning algorithm such as C4.5 <ref> (Quinlan, 1992) </ref> can be used to induce descriptions for the classification. This is the method used in this study for evaluating AUTOCLASS clusterings. Mingers (1989) uses the two criteria size and accuracy for evaluating a decision tree (or an equivalent set of rules). <p> This allows the AUTOCLASS classification to be used as input to other programs, such as rule and decision tree inducers. 2.2. C4.5 C4.5 <ref> (Quinlan, 1992) </ref> is a powerful tool for inducing decision trees and production rules from a set of examples. Much of C4.5 is derived from Quinlan's earlier induction system, ID3 (Quinlan, 1986). <p> However, C4.5 adds a number of enhancements to ID3, which are worth examining. C4.5 uses a new `gain ratio' criterion to determine how to split the examples at each node of the decision tree. C4.5. This removes ID3's strong bias towards tests with many outcomes <ref> (Quinlan, 1992) </ref>. Additionally, C4.5 allows splits to be made on the values of continuous (real and integer) attributes as well as enumerations. Decision trees induced by ID3 are often very complex, with a tendency to `over-fit' the data (Quinlan, 1992). <p> C4.5. This removes ID3's strong bias towards tests with many outcomes <ref> (Quinlan, 1992) </ref>. Additionally, C4.5 allows splits to be made on the values of continuous (real and integer) attributes as well as enumerations. Decision trees induced by ID3 are often very complex, with a tendency to `over-fit' the data (Quinlan, 1992). C4.5 provides a solution to this problem in the form of pruned decision trees or production rules. These are derived from the original decision tree, and lead to structures that generally cover the training set less thoroughly but perform better on unseen cases. <p> The resulting rules are then generalised to remove conditions that do not contribute to the accuracy of the classification. A side-effect of this process is that the rules are no longer exhaus tive or mutually exclusive <ref> (Quinlan, 1992) </ref>. C4.5 copes with this by using `ripple-down rules' (Compton, et. al., 1992). The rules are ordered, and any example is then classified by the first rule that covers it. <p> The rules are ordered, and any example is then classified by the first rule that covers it. In fact, only the classes are ranked, with the twin advantages that the final rule set is more intelligible and the order of rules within each class becomes irrelevant <ref> (Quinlan, 1992) </ref>. C4.5 also defines a default class that is used to classify examples not covered by any of the rules. 3. Methodology 3.1. The Data Set Reaven and Miller (1979) examined the relationship between Chemical and Overt diabetes in 145 non-obese subjects.
Reference: <author> Reaven, G., Berstein, R., Davis, B. & Olef-sky, J., </author> <year> 1976. </year> <journal> Non-ketoic diabetes mel-litus: insulin deficiency or insulin resistance?. American Journal of Medicine 60, </journal> <pages> 80-88. </pages>
Reference: <author> Reaven, G. & Miller, R., </author> <year> 1979. </year> <title> An Attempt to Define the Nature of Chemical Diabetes Using a Multidimensional Analysis. </title> <booktitle> Dia-betologia 16, </booktitle> <pages> 17-24. </pages>
Reference: <author> Tierney, L., </author> <year> 1990. </year> <title> LISP-STAT: An Object-Oriented Environment for Statistical Computing and Dynamic Graphics. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: The purpose of the workbench is to give users access to many machine learning algorithms, and to apply these to real-world data. WEKA provides a uniform interactive interface to a variety of tools, including machine learning schemes, data manipulation programs, and the LISP-STAT statistics and graphics package <ref> (Tierney, 1990) </ref>. Data sets to be manipulated by the workbench use the ARFF (Attribute-Relation File Format) intermediate file format. An ARFF file records information about a relation such as its name, attribute names, types and values, and instances (examples).
Reference: <author> Utgoff, P., </author> <year> 1989. </year> <title> Incremental Induction of Decision Trees. </title> <booktitle> Machine Learning 4, </booktitle> <pages> 161-186. </pages>
References-found: 19

