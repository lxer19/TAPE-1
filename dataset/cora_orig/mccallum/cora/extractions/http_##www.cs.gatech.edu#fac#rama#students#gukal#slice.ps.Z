URL: http://www.cs.gatech.edu/fac/rama/students/gukal/slice.ps.Z
Refering-URL: http://www.cs.gatech.edu/fac/rama/students/gukal/index.html
Root-URL: 
Email: e-mail: gukal@informix.com  
Phone: Fax: (415) 926-5985  
Title: Transient Versioning for Consistency and Concurrency in Client-Server Systems  
Author: Sreenivas Gukal Edward Omiecinski Umakishore Ramachandran 
Address: 4100 Bohannon Drive, Menlo Park, CA 94025  Atlanta, GA 30332  
Affiliation: Informix Software, Inc.,  College of Computing, Georgia Institute of Technology,  
Abstract: Inexpensive processors and memories have made client-server systems popular. Data-shipping client-server systems are designed to scale by distributing work, making use of the clients' caches and processing power. However, synchronization and cache consistency among the clients become bottlenecks as the number of clients increases. Both the problems arise because all existing methods treat cached data as replicated data. This paper simplifies these problems by developing a new consistency method called Slice Consistency based on transient versioning concepts. In this method, copies of data in different client caches are treated as different versions of the data. Multiple versions reduce cache consistency overhead since updating a data page only creates a new version and does not require invalidating copies of that page in other caches. The transient versions are also used to increase concurrency by allowing multiple readers and one writer to simultaneously access the same page, which helps in supporting queries without affecting other concurrent transactions. The storage overhead is reduced by making the prior versions transient. The slice consistency method is easily adaptable to exhibit either pessimistic or optimistic behaviors. This makes switching to the optimal method easy when the data contention changes. It is also possible for each client to follow a different policy based on its workload. Simulation experiments show that slice consistency performs better than the best existing methods in different environments and is easily adaptable to mixed and/or changing workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [ABGS 87] <author> Agrawal, D., Bernstein, A., Gupta, P., Sengupta, S. </author> <title> Distributed Optimistic Concurrency Control with Reduced Rollback, </title> <journal> Journal of Distributed Computing, </journal> <month> January </month> <year> 1987. </year>
Reference-contexts: In all these methods, updates to data items generate new versions. Queries are executed on the prior versions based on some variation of the timestamp method. Transactions are supported using a single-version concurrency control method. The multiversion optimistic methods ([LaWi 84], <ref> [ABGS 87] </ref>), multiversion times-tamp ordering ([AgSe 89]) and multiversion two-phase locking ([Chan 82]) are examples of such algorithms. Transient versioning is a multiversioning technique, where the prior versions are maintained temporarily to increase concurrency. The transient versions reduce the storage overhead while providing the same benefits as multiversioning algorithms.
Reference: [AgSe 89] <author> Agrawal, D., Sengupta, S. </author> <title> Modular Synchronization in Multiversion Databases: Version Control and Concurrency Control, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1989. </year>
Reference: [BaHR 80] <author> Bayer, R., Heller, H., Reiser, A. </author> <title> Parallelism and Recovery in Database Systems, </title> <journal> ACM Transactions on Database Systems, </journal> <month> June </month> <year> 1980. </year>
Reference-contexts: Transient versioning is a multiversioning technique, where the prior versions are maintained temporarily to increase concurrency. The transient versions reduce the storage overhead while providing the same benefits as multiversioning algorithms. Transient versioning was first used by Bayer 4 et al. <ref> [BaHR 80] </ref>. The algorithm is based on the observation that in a shadowing environment an update to a data page makes a copy of the data page and modifies the copy.
Reference: [BeHG 87] <author> Bernstein, P.A., Hadzilacos, V., Goodman, N. </author> <title> Concurrency Control and Recovery in Database Systems, </title> <publisher> Addison-Wesley Pub. Co., </publisher> <year> 1987. </year> <month> 23 </month>
Reference-contexts: Multiversioning algorithms have been in existence for more than 15 years. We mention only the relevant work. Multiversion concurrency control algorithms were first introduced by Reed [Reed 78]. Reed's thesis describes a distributed timestamp ordering scheme, which is a multiversion extension of basic timestamp ordering. Bernstein et al. <ref> [BeHG 87] </ref> also describe two-version and multi-version two-phase locking methods. To efficiently support queries, several multiversion algorithms have been proposed. In all these methods, updates to data items generate new versions. Queries are executed on the prior versions based on some variation of the timestamp method.
Reference: [BoCa 92a] <author> Bober, P., Carey, M. </author> <title> On Mixing Queries and Transactions via Multiversion Locking, </title> <booktitle> Proc. 8th International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: A similar method that uses time-stamp based deadlock prevention has been developed for distributed databases by Stearns and Rosenkrantz [StRo 81]. Recently, transient versioning algorithms based on two-phase locking have been proposed ([MoPL 92], [Chan 82], <ref> [BoCa 92a] </ref>, [BoCa 92b], [GuOR 95]). In all these methods, prior versions are deleted when the queries do not need them. All the above methods apply to centralized databases, while [BoCa 92b] and [MoPL 92] also extend transient versioning to distributed databases. Transient-versioning is also implemented in commercial systems.
Reference: [BoCa 92b] <author> Bober, P., Carey, M. </author> <title> Multiversion query locking, </title> <booktitle> Proc. 18th International Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: A similar method that uses time-stamp based deadlock prevention has been developed for distributed databases by Stearns and Rosenkrantz [StRo 81]. Recently, transient versioning algorithms based on two-phase locking have been proposed ([MoPL 92], [Chan 82], [BoCa 92a], <ref> [BoCa 92b] </ref>, [GuOR 95]). In all these methods, prior versions are deleted when the queries do not need them. All the above methods apply to centralized databases, while [BoCa 92b] and [MoPL 92] also extend transient versioning to distributed databases. Transient-versioning is also implemented in commercial systems. <p> Recently, transient versioning algorithms based on two-phase locking have been proposed ([MoPL 92], [Chan 82], [BoCa 92a], <ref> [BoCa 92b] </ref>, [GuOR 95]). In all these methods, prior versions are deleted when the queries do not need them. All the above methods apply to centralized databases, while [BoCa 92b] and [MoPL 92] also extend transient versioning to distributed databases. Transient-versioning is also implemented in commercial systems. Prime and Oracle (through rollback segments) support transient versioning in their products, but the details are not published. <p> The problem becomes interesting if the queries wish to see a transaction consistent database. Degree 3 queries may also be modeled along a different dimension when multiple versions of data items are maintained ([GaWi 82], <ref> [BoCa 92b] </ref>). A query is said to be of strong consistency if it is serializable with respect to all other transactions and queries. A query is of weak consistency if it is serializable only with all other transactions. Figure 5 provides an example of strong and weak consistencies.
Reference: [Care 91] <author> Carey. M., et. al. </author> <title> Data Caching Tradeoffs in Client-Server DBMS Architectures, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: When copies of data are cached at different clients, all copies should be maintained consistent. Synchronization deals with how transactions executing on different clients are allowed to access shared data in a consistent manner. Several methods for providing cache consistency and synchronization have been proposed, implemented and evaluated (e.g., <ref> [Care 91] </ref>, [KGBW 90], [WaRo 91], [WiNe 90], [FrCa 92], [LLOW 91], [GoOR 96]). All these methods assume that the copies of data in different client caches are all copies of a single version of data. <p> Pessimistic and optimistic variations are possible in both classes of consistency protocols. Several detection-based algorithms have been proposed in the literature (<ref> [Care 91] </ref>, [KGBW 90], [WaRo 91], [WiNe 90]). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. <p> Several detection-based algorithms have been proposed in the literature (<ref> [Care 91] </ref>, [KGBW 90], [WaRo 91], [WiNe 90]). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. The pessimistic variations ([WaRo 91], [FrCa 92], [LLOW 91]) invalidate the copies in other caches immediately on a write fault, while the optimistic methods ([Care 91], [WiNe 90]) wait until <p> optimistic methods (<ref> [Care 91] </ref>, [WiNe 90]) wait until the commit time and either invalidate or update the copies in the other caches. We briefly discuss two algorithms to give a flavor of the design space. The first algorithm is a pessimistic detection-based method called "Caching Two-Phase Locking (C2PL)" ([KGBW 90], [Care 91], [WaRo 91]). C2PL is a variation of the primary copy locking algorithm. Transactions set read (or write) locks on the data pages they read (or modify). All lock requests are sent to the server which serves as the primary copy site.
Reference: [Chan 82] <author> Chan, A., et al, </author> <title> The Implementation of an Integrated Concurrency Control and Recovery Scheme, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> June </month> <year> 1982. </year>
Reference-contexts: A similar method that uses time-stamp based deadlock prevention has been developed for distributed databases by Stearns and Rosenkrantz [StRo 81]. Recently, transient versioning algorithms based on two-phase locking have been proposed ([MoPL 92], <ref> [Chan 82] </ref>, [BoCa 92a], [BoCa 92b], [GuOR 95]). In all these methods, prior versions are deleted when the queries do not need them. All the above methods apply to centralized databases, while [BoCa 92b] and [MoPL 92] also extend transient versioning to distributed databases.
Reference: [DeGr 92] <author> DeWitt, D., Gray, J. </author> <title> Parallel Database Systems: The Future of High Performance Database Systems, </title> <journal> Communications of the ACM, </journal> <month> June </month> <year> 1992. </year>
Reference-contexts: We use these two methods as benchmarks to evaluate the slice consistency method. 2.2 Transient Versioning One of the important concurrency control problems <ref> [DeGr 92] </ref> is to prevent the execution of queries from affecting concurrent update transactions. Data versioning helps to avoid data contention between queries and other transactions. Multiversioning algorithms have been in existence for more than 15 years. We mention only the relevant work.
Reference: [FrCa 92] <author> Franklin, M., Carey, M. </author> <title> Client-Server Caching Revisited, </title> <booktitle> Proc. International Workshop on Distributed Object Management, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Synchronization deals with how transactions executing on different clients are allowed to access shared data in a consistent manner. Several methods for providing cache consistency and synchronization have been proposed, implemented and evaluated (e.g., [Care 91], [KGBW 90], [WaRo 91], [WiNe 90], <ref> [FrCa 92] </ref>, [LLOW 91], [GoOR 96]). All these methods assume that the copies of data in different client caches are all copies of a single version of data. <p> Several detection-based algorithms have been proposed in the literature ([Care 91], [KGBW 90], [WaRo 91], [WiNe 90]). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], <ref> [FrCa 92] </ref>, [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. The pessimistic variations ([WaRo 91], [FrCa 92], [LLOW 91]) invalidate the copies in other caches immediately on a write fault, while the optimistic methods ([Care 91], <p> Avoidance-based methods ([WaRo 91], <ref> [FrCa 92] </ref>, [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. The pessimistic variations ([WaRo 91], [FrCa 92], [LLOW 91]) invalidate the copies in other caches immediately on a write fault, while the optimistic methods ([Care 91], [WiNe 90]) wait until the commit time and either invalidate or update the copies in the other caches.
Reference: [Fran 93] <author> Franklin, M.J. </author> <title> Caching and Memory Management in Client-Server Database Systems, </title> <type> Ph.D. Thesis, </type> <institution> Computer Sciences, University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: Hence these algorithms are variations of the consistency methods for replicated databases. A detailed taxonomy of the consistency protocols is presented by Franklin <ref> [Fran 93] </ref>. The cache consistency algorithms can be divided into two classes: detection-based and avoidance-based. Detection-based algorithms allow stale data to exist in client caches. Hence a transaction has to check the validity of the data it has accessed before it can commit. <p> The clients invalidate the cached copies. The server commits the transaction after all the caching clients have responded. These two algorithms are compared to other existing methods <ref> [Fran 93] </ref>. C2PL has better performance than other methods when the data contention is high and O2PL performs best when there is little or no data contention and a high locality of access. <p> When space becomes available, the transaction is restarted in a new slice. 12 4 Simulation Experiments and Results The performance of the cache consistency methods depends on the type of workload considered. We use the workload model described in ([Care 91], <ref> [Fran 93] </ref>) for client-server architectures. HotCold Each client has its own hot region where the majority of its accesses are directed. The remaining accesses refer to the rest of the database. This models a situation where different clients favor disjoint regions of the database, but where some read/write overlap exists. <p> The cost of sending a message is modeled as having two components, the time required to transmit the message and the processing required at the sender and the receiver for handling the message. The resource parameters common to all the experiments are listed in Table 2. As in <ref> [Fran 93] </ref>, a small database size is used to make the detailed simulations of the complex client-server system computationally feasible. The server and client caches are chosen to hold 30% and 15% of the database respectively.
Reference: [GaWi 82] <author> Garcia-Molina, H., Wiederhold, G. </author> <title> Read-Only Transactions in a Distributed Database, </title> <journal> ACM Transactions on Database Systems, </journal> <month> June </month> <year> 1982. </year>
Reference: [GLPT 76] <author> Gray, J.N., Lorie, R., Putzolu, F., Traiger, I. </author> <title> Granularity of Locks and Degrees of Consistency in a Shared Data Base, Modeling in Data Base Systems, </title> <publisher> North Holland Publishing, </publisher> <year> 1976. </year>
Reference-contexts: Queries are never blocked, never rolled back and do not affect concurrent transactions. One way to support queries is to violate serializability and run the queries at degree 2 or degree 1 isolation <ref> [GLPT 76] </ref>. The problem becomes interesting if the queries wish to see a transaction consistent database. Degree 3 queries may also be modeled along a different dimension when multiple versions of data items are maintained ([GaWi 82], [BoCa 92b]).
Reference: [GuOR 95] <author> Gukal, S., Omiecinski, E., Ramachandran, U. </author> <title> An Efficient Transient Versioning Method, </title> <booktitle> 13th British National Conference on Databases, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: A similar method that uses time-stamp based deadlock prevention has been developed for distributed databases by Stearns and Rosenkrantz [StRo 81]. Recently, transient versioning algorithms based on two-phase locking have been proposed ([MoPL 92], [Chan 82], [BoCa 92a], [BoCa 92b], <ref> [GuOR 95] </ref>). In all these methods, prior versions are deleted when the queries do not need them. All the above methods apply to centralized databases, while [BoCa 92b] and [MoPL 92] also extend transient versioning to distributed databases. Transient-versioning is also implemented in commercial systems.
Reference: [GoOR 96] <author> Gottemukkala, V., Omiecinski, E., Ramachandran, U. </author> <title> Relaxed Index Consistency for Data-only Locking in a Client-Server Database, </title> <booktitle> IEEE International Conference on Data Engineering, </booktitle> <year> 1996. </year>
Reference-contexts: Synchronization deals with how transactions executing on different clients are allowed to access shared data in a consistent manner. Several methods for providing cache consistency and synchronization have been proposed, implemented and evaluated (e.g., [Care 91], [KGBW 90], [WaRo 91], [WiNe 90], [FrCa 92], [LLOW 91], <ref> [GoOR 96] </ref>). All these methods assume that the copies of data in different client caches are all copies of a single version of data.
Reference: [KGBW 90] <author> Kim, W., Garza, J., Ballou, N., Woelk, D. </author> <title> The Architecture of the ORION Next-Generation Database System, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <month> March </month> <year> 1990. </year>
Reference-contexts: Synchronization deals with how transactions executing on different clients are allowed to access shared data in a consistent manner. Several methods for providing cache consistency and synchronization have been proposed, implemented and evaluated (e.g., [Care 91], <ref> [KGBW 90] </ref>, [WaRo 91], [WiNe 90], [FrCa 92], [LLOW 91], [GoOR 96]). All these methods assume that the copies of data in different client caches are all copies of a single version of data. <p> When a transaction commits, its updates are immediately propagated to all necessary client caches, thereby avoiding access to stale data. Pessimistic and optimistic variations are possible in both classes of consistency protocols. Several detection-based algorithms have been proposed in the literature ([Care 91], <ref> [KGBW 90] </ref>, [WaRo 91], [WiNe 90]). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). <p> Pessimistic and optimistic variations are possible in both classes of consistency protocols. Several detection-based algorithms have been proposed in the literature ([Care 91], <ref> [KGBW 90] </ref>, [WaRo 91], [WiNe 90]). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches.
Reference: [LaWi 84] <author> Lai, M., Wilkinson, K. </author> <title> Distributed Transaction Management in Jasmin, </title> <booktitle> Proc. 10th Int. Conf. on Very Large Data Bases, </booktitle> <year> 1984. </year> <month> 24 </month>
Reference: [LLOW 91] <author> Lamb, C., Landis, G., Orenstein, J., Weinred, D. </author> <title> The ObjectStore Database System, </title> <journal> Communications of the ACM, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: Synchronization deals with how transactions executing on different clients are allowed to access shared data in a consistent manner. Several methods for providing cache consistency and synchronization have been proposed, implemented and evaluated (e.g., [Care 91], [KGBW 90], [WaRo 91], [WiNe 90], [FrCa 92], <ref> [LLOW 91] </ref>, [GoOR 96]). All these methods assume that the copies of data in different client caches are all copies of a single version of data. <p> Several detection-based algorithms have been proposed in the literature ([Care 91], [KGBW 90], [WaRo 91], [WiNe 90]). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], <ref> [LLOW 91] </ref>, [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. The pessimistic variations ([WaRo 91], [FrCa 92], [LLOW 91]) invalidate the copies in other caches immediately on a write fault, while the optimistic methods ([Care 91], [WiNe 90]) <p> Avoidance-based methods ([WaRo 91], [FrCa 92], <ref> [LLOW 91] </ref>, [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. The pessimistic variations ([WaRo 91], [FrCa 92], [LLOW 91]) invalidate the copies in other caches immediately on a write fault, while the optimistic methods ([Care 91], [WiNe 90]) wait until the commit time and either invalidate or update the copies in the other caches. We briefly discuss two algorithms to give a flavor of the design space.
Reference: [MoPL 92] <author> Mohan, C., Pirahesh, H., Lorie, R. </author> <title> Efficient and Flexible Methods for Transient Ver-sioning of Records to Avoid Locking by Read-Only Transactions, </title> <booktitle> ACM SIGMOD Int. Conf. on Mgmt. of Data, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Recently, transient versioning algorithms based on two-phase locking have been proposed (<ref> [MoPL 92] </ref>, [Chan 82], [BoCa 92a], [BoCa 92b], [GuOR 95]). In all these methods, prior versions are deleted when the queries do not need them. All the above methods apply to centralized databases, while [BoCa 92b] and [MoPL 92] also extend transient versioning to distributed databases. Transient-versioning is also implemented in commercial systems. Prime and Oracle (through rollback segments) support transient versioning in their products, but the details are not published.
Reference: [PMCL 90] <author> Pirahesh, H., Mohan, C., Cheng, J., Liu, T.S., Selinger, P. </author> <title> Parallelism in Relational Database Systems: </title> <booktitle> Architectural Issues and Design Approaches, IEEE 2nd Int'l Symp. on Databases in Parallel and Distributed Systems, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: Single version data also requires global synchronization and reduces concurrency as read and write locks on the same data are incompatible. Centralized databases have a similar concurrency problem while supporting long-running queries (read-only transactions) <ref> [PMCL 90] </ref>. Queries hold the read locks and starve other transactions that request to update the same data. Transient-versioning algorithms have been proposed for centralized databases as an elegant way to support queries. These algorithms maintain prior versions of updated data items.
Reference: [RaRe 91] <author> Raghavan, A., Rengarajan, </author> <title> T.K. Database Availability for Transaction Processing, </title> <journal> Digital Technical Journal, </journal> <volume> Vol.3 No. 1, </volume> <month> Winter </month> <year> 1991. </year>
Reference-contexts: All the above methods apply to centralized databases, while [BoCa 92b] and [MoPL 92] also extend transient versioning to distributed databases. Transient-versioning is also implemented in commercial systems. Prime and Oracle (through rollback segments) support transient versioning in their products, but the details are not published. Raghavan and Rengarajan <ref> [RaRe 91] </ref> provide a brief description of a transient-versioning scheme implemented by DEC in Rdb relational database product. 3 Slice Consistency Serializability is the underlying concern for executing transactions. Slice consistency, by using transient-versioning and the notion of slices, enables transactions executing on different clients to be serializable.
Reference: [Reed 78] <author> Reed, D. </author> <title> Naming and Synchronization in a Decentralized Computer System, </title> <type> PhD Thesis, Technical Report MIT/LCS/TR-205, </type> <institution> MIT, </institution> <month> September </month> <year> 1978. </year>
Reference-contexts: Data versioning helps to avoid data contention between queries and other transactions. Multiversioning algorithms have been in existence for more than 15 years. We mention only the relevant work. Multiversion concurrency control algorithms were first introduced by Reed <ref> [Reed 78] </ref>. Reed's thesis describes a distributed timestamp ordering scheme, which is a multiversion extension of basic timestamp ordering. Bernstein et al. [BeHG 87] also describe two-version and multi-version two-phase locking methods. To efficiently support queries, several multiversion algorithms have been proposed.
Reference: [Schw 90] <author> Schwetman, H. </author> <title> CSIM Users Guide, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: The same sets of transactions are used for all methods. For each set of input parameters, the system is simulated on a number of transaction sets until the 90% confidence interval for the transaction throughput is within a few percent. The simulator is written in CSIM <ref> [Schw 90] </ref>, a process-oriented simulation package. 15 Table 3: Workload parameters Parameter SharedHot HotCold Private Uniform References 10 10 10 10 Hot Area 1000 pages all 200 pages each 100 pages each - Cold Area rest of DB rest of DB 5000 pages all DB Hot Access Prob. 0.8 0.8 0.6
Reference: [Silb 82] <author> Silberschatz, A. </author> <title> A Multi-version Concurrency Scheme with no Rollbacks, </title> <booktitle> ACM-SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1982. </year>
Reference: [StRo 81] <author> Stearns, R.E., Rosenkrantz, </author> <title> D.J. Distributed Database Concurrency Controls Using Before-Values, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> April </month> <year> 1981. </year>
Reference-contexts: Since the prior copies of the pages are maintained for recovery purposes, this algorithm proposes that the readers be allowed to read the "before" values. A similar method that uses time-stamp based deadlock prevention has been developed for distributed databases by Stearns and Rosenkrantz <ref> [StRo 81] </ref>. Recently, transient versioning algorithms based on two-phase locking have been proposed ([MoPL 92], [Chan 82], [BoCa 92a], [BoCa 92b], [GuOR 95]). In all these methods, prior versions are deleted when the queries do not need them.
Reference: [WaRo 91] <author> Wang, Y., Rowe, L. </author> <title> Cache Consistency and Concurrency Control in a Client/Server DBMS Architecture, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Synchronization deals with how transactions executing on different clients are allowed to access shared data in a consistent manner. Several methods for providing cache consistency and synchronization have been proposed, implemented and evaluated (e.g., [Care 91], [KGBW 90], <ref> [WaRo 91] </ref>, [WiNe 90], [FrCa 92], [LLOW 91], [GoOR 96]). All these methods assume that the copies of data in different client caches are all copies of a single version of data. <p> When a transaction commits, its updates are immediately propagated to all necessary client caches, thereby avoiding access to stale data. Pessimistic and optimistic variations are possible in both classes of consistency protocols. Several detection-based algorithms have been proposed in the literature ([Care 91], [KGBW 90], <ref> [WaRo 91] </ref>, [WiNe 90]). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of <p> Pessimistic and optimistic variations are possible in both classes of consistency protocols. Several detection-based algorithms have been proposed in the literature ([Care 91], [KGBW 90], <ref> [WaRo 91] </ref>, [WiNe 90]). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. The pessimistic variations ([WaRo 91], [FrCa 92], [LLOW 91]) invalidate the copies in <p> We briefly discuss two algorithms to give a flavor of the design space. The first algorithm is a pessimistic detection-based method called "Caching Two-Phase Locking (C2PL)" ([KGBW 90], [Care 91], <ref> [WaRo 91] </ref>). C2PL is a variation of the primary copy locking algorithm. Transactions set read (or write) locks on the data pages they read (or modify). All lock requests are sent to the server which serves as the primary copy site.
Reference: [WiNe 90] <author> Wilkinson, W., Neimat, M. </author> <title> Maintaining Consistency of Client Cached Data, </title> <booktitle> Proc. 16th International Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1990. </year> <month> 25 </month>
Reference-contexts: Synchronization deals with how transactions executing on different clients are allowed to access shared data in a consistent manner. Several methods for providing cache consistency and synchronization have been proposed, implemented and evaluated (e.g., [Care 91], [KGBW 90], [WaRo 91], <ref> [WiNe 90] </ref>, [FrCa 92], [LLOW 91], [GoOR 96]). All these methods assume that the copies of data in different client caches are all copies of a single version of data. <p> When a transaction commits, its updates are immediately propagated to all necessary client caches, thereby avoiding access to stale data. Pessimistic and optimistic variations are possible in both classes of consistency protocols. Several detection-based algorithms have been proposed in the literature ([Care 91], [KGBW 90], [WaRo 91], <ref> [WiNe 90] </ref>). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents <p> Several detection-based algorithms have been proposed in the literature ([Care 91], [KGBW 90], [WaRo 91], <ref> [WiNe 90] </ref>). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. <p> detection-based algorithms have been proposed in the literature ([Care 91], [KGBW 90], [WaRo 91], <ref> [WiNe 90] </ref>). Detection of stale pages can be either pessimistic (on initial access [Care 91], [KGBW 90], [WaRo 91]) or optimistic (deferred until commit [WiNe 90]). Avoidance-based methods ([WaRo 91], [FrCa 92], [LLOW 91], [Care 91], [WiNe 90]) are more complex as the server has to keep track of the contents of all client caches. The pessimistic variations ([WaRo 91], [FrCa 92], [LLOW 91]) invalidate the copies in other caches immediately on a write fault, while the optimistic methods ([Care 91], [WiNe 90]) wait until the commit <p> 92], [LLOW 91], [Care 91], <ref> [WiNe 90] </ref>) are more complex as the server has to keep track of the contents of all client caches. The pessimistic variations ([WaRo 91], [FrCa 92], [LLOW 91]) invalidate the copies in other caches immediately on a write fault, while the optimistic methods ([Care 91], [WiNe 90]) wait until the commit time and either invalidate or update the copies in the other caches. We briefly discuss two algorithms to give a flavor of the design space. The first algorithm is a pessimistic detection-based method called "Caching Two-Phase Locking (C2PL)" ([KGBW 90], [Care 91], [WaRo 91]).
References-found: 27

