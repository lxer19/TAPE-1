URL: http://iacoma.cs.uiuc.edu/iacoma-papers/coma_mem.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: lyang,torrella@csrd.uiuc.edu  
Title: Speeding up the Memory Hierarchy in Flat COMA Multiprocessors 1  
Author: Liuxi Yang and Josep Torrellas 
Web: http://www.csrd.uiuc.edu/iacoma  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: Scalable Flat Cache Only Memory Architectures (Flat COMA) are designed for reduced memory access latencies while minimizing programmer and operating system involvement. Indeed, to keep memory access latencies low, neither the programmer needs to perform clever data placement nor the operating system needs to perform page migration. The hardware automatically replicates the data and migrates it to the attraction memories of the nodes that use it. Unfortunately, part of the latency of memory accesses is superfluous. In particular, reads often perform unnecessary attraction memory accesses, require too many network hops, or perform necessary attraction memory accesses inefficiently. In this paper, we propose relatively inexpensive schemes that address these three problems. To eliminate unnecessary attraction memory accesses, we propose a small direct-mapped cache called Invalidation Cache (IVC). To reduce the number of network hops, the IVC is augmented with hint pointers to processors. These hint pointers are faster and have more applicability than in older hint schemes. Finally, to speed up necessary accesses to set-associative attraction memories, we optimize the locality of windows in page-mode DRAMs. We evaluate these optimizations with 32-processor simulations of 8 Splash and Perfect Suite applications. We show that these optimizations speed up the applications by an average of 20% at a modest cost. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Finally, to speed up necessary accesses to set-associative attraction memories, we optimize the locality of windows in page-mode DRAMs. We evaluate these three optimizations with 32-processor simulations of 8 Splash [15] and Perfect Suite <ref> [1] </ref> applications. We show that these optimizations speed up the applications by an average of 20% at a modest cost. <p> Tangolite is used for the Splash applications [15], while EPG-sim is used for the Perfect Suite applications <ref> [1] </ref>. The resulting tightly coupled execution-driven simulation systems ensure correct interleaving of memory accesses. To make the evaluation as representative as possible, we evaluate the architecture with eight parallel applications from the Splash and Perfect Suite sets.
Reference: [2] <author> M. Bjorkman, F. Dahlgren, and P. Stenstrom. </author> <title> Using Hints to Reduce the Read Miss Penalty for Flat COMA Protocols. </title> <booktitle> In Proceedings of the 28th Annual Hawaii International Conference of System Sciences, </booktitle> <pages> pages 242-251, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: IVC can not possibly eliminate all unnecessary attraction memory accesses: start-up effects and displacement of lines from the attraction memory in state shared will still cause unnecessary memory accesses. 3.2 Invalidation Caches with Hints To address the problem discussed in Section 2.2, Gupta et al [4] and Bjorkman et al <ref> [2] </ref> have proposed the hint schemes. The idea is for the lines in the attraction memory to store the ID of a processor that is likely to have an up-to-date copy of the line. This ID is called hint. <p> The latter node will supply the data. In this case, the requesting processor gets the data in 3 hops. Gupta et al [4] and Bjorkman et al <ref> [2] </ref> have proposed two types of hints, namely invalid and shared. An invalid hint is stored when a line is invalidated. The hint is the ID of the invalidating processor and it replaces the data in the line. <p> Finally, if the algorithm used is Joe and Hennessy's [8], the IVC stores the ID of the processor that supplies the line causing the displacement. Overall, the cost of this optimization is the extra complexity of the cache coherence protocol <ref> [2] </ref>. The size of the IVC, however, changes only slightly. For the 64-processor machine described in Section 3.1, the hint adds 6 bits to each entry. <p> HintRS is InvRS plus coherence and conflict hints, while HintInf is InvInf plus coherence and conflict hints. For the coherence hints, we use invalid hints <ref> [2, 4] </ref>. The execution time of the applications on the HintRS and HintInf architectures is shown in bars 4-5 of Figure 5. Overall, we see that the impact of IVCs with hints is larger: applications run, on average, 11% faster than in Base. <p> However, for the other four applications, namely Cholesky, Mp3d, Ocean and Arc2d, hints do not manage to reduce the 3Hop time much. This is consistent with the results of Bjorkman et al <ref> [2] </ref>, which we discuss in Section 6. To understand these results, we examine the outcome of 2-and 3-hop transactions (Figure 7). We focus on the HintRS bars. <p> Finally, for Mp3d, the IVC intercepts a relatively small fraction of the transactions (Figure 7). In addition, the hints are mostly incorrect. The low accuracy of the hints in Mp3d is due to the migratory behavior of its data, as pointed out by Bjorkman et al <ref> [2] </ref>. Mp3d, therefore, suffers in part from poor hints. To understand why larger IVCs do not accomplish much more, we compare HintRS to HintInf in Figure 7. From the figure, we see that infinite IVCs have a significant effect on Mp3d, Ocean and, to a lesser extent, Flo52 and Arc2d. <p> While this optimization requires changes to the memory controller, it does not affect the DRAM data paths or the memory read and write primitives. 6 Related Work There are several pieces of related work. Gupta et al [4] and Bjorkman et al <ref> [2] </ref> proposed the use of hints to transform three-hop transactions into two-hop ones. Furthermore, Bjorkman et al [2] proposed sending the request to both the node that is the home of the line and the hint node. While we have used their ideas, our approach differs in two ways. <p> Gupta et al [4] and Bjorkman et al <ref> [2] </ref> proposed the use of hints to transform three-hop transactions into two-hop ones. Furthermore, Bjorkman et al [2] proposed sending the request to both the node that is the home of the line and the hint node. While we have used their ideas, our approach differs in two ways. First, we use conflict hints in addition to coherence hints; the other authors used coherence hints only.
Reference: [3] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The latency of these transactions includes two attraction memory accesses. Finally, each network hop takes 30 cycles. In our simulations, we model all the contention for resources. 4.2 Simulation Environment and Appli cations The architecture simulator described is connected to two trace generation systems, namely Tangolite <ref> [3] </ref> and EPG-sim [13], to form two execution-driven simulation systems. Tangolite is used for the Splash applications [15], while EPG-sim is used for the Perfect Suite applications [1]. The resulting tightly coupled execution-driven simulation systems ensure correct interleaving of memory accesses.
Reference: [4] <author> A. Gupta, T. Joe, and P. Stenstrom. </author> <title> Performance Limitations of Cache-Coherent NUMA and COMA Multiprocessors and the Flat-COMA Solution. </title> <type> Technical Report CSL-TR-92-524, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Note, however, that the IVC can not possibly eliminate all unnecessary attraction memory accesses: start-up effects and displacement of lines from the attraction memory in state shared will still cause unnecessary memory accesses. 3.2 Invalidation Caches with Hints To address the problem discussed in Section 2.2, Gupta et al <ref> [4] </ref> and Bjorkman et al [2] have proposed the hint schemes. The idea is for the lines in the attraction memory to store the ID of a processor that is likely to have an up-to-date copy of the line. This ID is called hint. <p> The latter node will supply the data. In this case, the requesting processor gets the data in 3 hops. Gupta et al <ref> [4] </ref> and Bjorkman et al [2] have proposed two types of hints, namely invalid and shared. An invalid hint is stored when a line is invalidated. The hint is the ID of the invalidating processor and it replaces the data in the line. <p> HintRS is InvRS plus coherence and conflict hints, while HintInf is InvInf plus coherence and conflict hints. For the coherence hints, we use invalid hints <ref> [2, 4] </ref>. The execution time of the applications on the HintRS and HintInf architectures is shown in bars 4-5 of Figure 5. Overall, we see that the impact of IVCs with hints is larger: applications run, on average, 11% faster than in Base. <p> While this optimization requires changes to the memory controller, it does not affect the DRAM data paths or the memory read and write primitives. 6 Related Work There are several pieces of related work. Gupta et al <ref> [4] </ref> and Bjorkman et al [2] proposed the use of hints to transform three-hop transactions into two-hop ones. Furthermore, Bjorkman et al [2] proposed sending the request to both the node that is the home of the line and the hint node.
Reference: [5] <author> E. Hagersten. </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <type> Ph.D. Dissertation, </type> <institution> The Royal Institute of Technology, Stockholm, Sweden, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Their scheme and our window scheme address different problems: their scheme increases the number of hits, while our scheme speeds up accesses that already hit. Hagersten <ref> [5] </ref> discusses a scheme that was considered in the design of the DDM COMA machine [6]. The idea is to keep a special register for each set in a two-way set-associative attraction memory. The register points to the most recently-used partition in the set.
Reference: [6] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM a Cache-Only Memory Architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: One class of cache-coherent shared-memory machine that is designed for reduced memory access latencies while minimizing programmer and operating system support is called Flat Cache-Only Memory Architecture (Flat COMA) [16]. Flat COMA machines, like all other COMA machines <ref> [6, 8, 10, 14, 16] </ref>, organize the distributed memories as caches called attraction memories. Like caches, attraction memories are composed of data array and tag array. <p> In addition, to reduce the number of conflicts, attraction memories are usually associative, often 4- to 16-ways. Examples of COMA machines, also called All-Cache, are the Swedish Institute of Computer Science's DDM1 <ref> [6] </ref> and Kendall Square Research's KSR1 [10]. 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP 94-57436 and RIA MIP 93-08098, DARPA Contract No. DABT63-95-C-0097, NASA Contract No. NAG-1-613, and Intel Corporation. <p> In addition, since we simulate faster processors, the memory latencies in our system are higher than in theirs. This increases the relative impact of the hints. Muller et al [12] propose an optimization for the attraction memory of the DDM COMA machine <ref> [6] </ref>. The idea is to use a skewed-associative attraction memory to decrease the chances that hot memory lines conflict with each other in several sets. Their scheme and our window scheme address different problems: their scheme increases the number of hits, while our scheme speeds up accesses that already hit. <p> Their scheme and our window scheme address different problems: their scheme increases the number of hits, while our scheme speeds up accesses that already hit. Hagersten [5] discusses a scheme that was considered in the design of the DDM COMA machine <ref> [6] </ref>. The idea is to keep a special register for each set in a two-way set-associative attraction memory. The register points to the most recently-used partition in the set. When a set is accessed, the partition pointed to by the corresponding register is immediately checked.
Reference: [7] <author> W. C. Hsu and J. E. Smith. </author> <title> Performance of Cached DRAM Organizations in Vector Supercomputers. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 327-336, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: When a set is accessed, the partition pointed to by the corresponding register is immediately checked. If that partition hits, the access is satisfied with low latency. Our window scheme is more elaborate than this. Finally, Hsu and Smith <ref> [7] </ref> studied cached DRAM organizations for vector supercomputers. Their work is different in that it is focused on machines without caches running highly vectorized codes. 7 Summary Scalable Flat COMA multiprocessors are designed for reduced memory access latencies while minimizing programmer and operating system involvement.
Reference: [8] <author> T. Joe and J. Hennessy. </author> <title> Evaluating the Memory Overhead Required for COMA Architectures. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-93, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: One class of cache-coherent shared-memory machine that is designed for reduced memory access latencies while minimizing programmer and operating system support is called Flat Cache-Only Memory Architecture (Flat COMA) [16]. Flat COMA machines, like all other COMA machines <ref> [6, 8, 10, 14, 16] </ref>, organize the distributed memories as caches called attraction memories. Like caches, attraction memories are composed of data array and tag array. <p> Therefore, in the general case, a message sent to the home by a processor requesting data will be forwarded to a third node. Note, however, that no search of the memory hierarchy is necessary now to find a copy of the data. COMAs have several well-known sources of overhead <ref> [8] </ref>. First, each attraction memory line needs a tag. In addition, the attraction memory needs to be larger than the size of the application data to allow for the latter to replicate. <p> Otherwise, if the local node decides what node to send the line to, the IVC is updated accordingly. Finally, if the algorithm used is Joe and Hennessy's <ref> [8] </ref>, the IVC stores the ID of the processor that supplies the line causing the displacement. Overall, the cost of this optimization is the extra complexity of the cache coherence protocol [2]. The size of the IVC, however, changes only slightly.
Reference: [9] <editor> F. Jones et al. </editor> <title> A New Era of Fast Dynamic RAMs. </title> <journal> IEEE Spectrum, </journal> <pages> pages 43-49, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This is a 37% savings. 2.3 Inefficient Attraction Memory Reads In COMA machines, when a read request finally reaches the correct attraction memory, it often performs the memory access inefficiently. To see why, consider page-mode DRAMs <ref> [9] </ref>. These memories effectively have a cache inside. After a given memory location x in the DRAM is accessed, an active window of contiguous locations surrounding x can be accessed in very short times.
Reference: [10] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary. </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: One class of cache-coherent shared-memory machine that is designed for reduced memory access latencies while minimizing programmer and operating system support is called Flat Cache-Only Memory Architecture (Flat COMA) [16]. Flat COMA machines, like all other COMA machines <ref> [6, 8, 10, 14, 16] </ref>, organize the distributed memories as caches called attraction memories. Like caches, attraction memories are composed of data array and tag array. <p> In addition, to reduce the number of conflicts, attraction memories are usually associative, often 4- to 16-ways. Examples of COMA machines, also called All-Cache, are the Swedish Institute of Computer Science's DDM1 [6] and Kendall Square Research's KSR1 <ref> [10] </ref>. 1 This work was supported in part by the National Science Foundation under grants NSF Young Investigator Award MIP 94-57436 and RIA MIP 93-08098, DARPA Contract No. DABT63-95-C-0097, NASA Contract No. NAG-1-613, and Intel Corporation.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The attraction memory is 4-way set-associative. Processors stall on read misses and use release consistency. In most cases, we run the applications under 0.8 memory pressure. We use a DASH-like cache coherence protocol <ref> [11] </ref> with COMA extensions. Because of the small working sets of the applications, instead of using large active windows, we use 2-Kbyte active windows. In later experiments, we use even smaller windows (256 bytes).
Reference: [12] <author> H. Muller, P. Stallard, and D. Warren. </author> <title> The Application of Skewed-Associative Memories to Cache Only Memory Architectures. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <pages> pages I150-I154, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: In addition, since we simulate faster processors, the memory latencies in our system are higher than in theirs. This increases the relative impact of the hints. Muller et al <ref> [12] </ref> propose an optimization for the attraction memory of the DDM COMA machine [6]. The idea is to use a skewed-associative attraction memory to decrease the chances that hot memory lines conflict with each other in several sets.
Reference: [13] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Execution Driven Tools for Parallel Simulation of Parallel Architectures and Applications. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 860-869, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The latency of these transactions includes two attraction memory accesses. Finally, each network hop takes 30 cycles. In our simulations, we model all the contention for resources. 4.2 Simulation Environment and Appli cations The architecture simulator described is connected to two trace generation systems, namely Tangolite [3] and EPG-sim <ref> [13] </ref>, to form two execution-driven simulation systems. Tangolite is used for the Splash applications [15], while EPG-sim is used for the Perfect Suite applications [1]. The resulting tightly coupled execution-driven simulation systems ensure correct interleaving of memory accesses.
Reference: [14] <author> A. Saulsbury, T. Wilkinson, J. Carter, and A. Landin. </author> <title> An Argument for Simple COMA. </title> <booktitle> In Proceedings of the 1st International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 276-285, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: One class of cache-coherent shared-memory machine that is designed for reduced memory access latencies while minimizing programmer and operating system support is called Flat Cache-Only Memory Architecture (Flat COMA) [16]. Flat COMA machines, like all other COMA machines <ref> [6, 8, 10, 14, 16] </ref>, organize the distributed memories as caches called attraction memories. Like caches, attraction memories are composed of data array and tag array.
Reference: [15] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: To reduce the number of network hops, the IVC is augmented with hint pointers to processors. Finally, to speed up necessary accesses to set-associative attraction memories, we optimize the locality of windows in page-mode DRAMs. We evaluate these three optimizations with 32-processor simulations of 8 Splash <ref> [15] </ref> and Perfect Suite [1] applications. We show that these optimizations speed up the applications by an average of 20% at a modest cost. <p> In the meantime, the bus is busy with the data transfer. Finally, a third issue has to do with accesses to invalidated data. Our experiments show that such accesses often have much spatial locality. Consider, for example, Mp3d's cells <ref> [15] </ref>. These are small objects that account for many attraction memory accesses. The fields in each cell are accessed in sequence, which should be good for hitting in active windows. However, cells are heavily shared and, therefore, an access to the attraction memory often finds the line marked invalid. <p> In our simulations, we model all the contention for resources. 4.2 Simulation Environment and Appli cations The architecture simulator described is connected to two trace generation systems, namely Tangolite [3] and EPG-sim [13], to form two execution-driven simulation systems. Tangolite is used for the Splash applications <ref> [15] </ref>, while EPG-sim is used for the Perfect Suite applications [1]. The resulting tightly coupled execution-driven simulation systems ensure correct interleaving of memory accesses. To make the evaluation as representative as possible, we evaluate the architecture with eight parallel applications from the Splash and Perfect Suite sets.
Reference: [16] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: One class of cache-coherent shared-memory machine that is designed for reduced memory access latencies while minimizing programmer and operating system support is called Flat Cache-Only Memory Architecture (Flat COMA) <ref> [16] </ref>. Flat COMA machines, like all other COMA machines [6, 8, 10, 14, 16], organize the distributed memories as caches called attraction memories. Like caches, attraction memories are composed of data array and tag array. <p> One class of cache-coherent shared-memory machine that is designed for reduced memory access latencies while minimizing programmer and operating system support is called Flat Cache-Only Memory Architecture (Flat COMA) [16]. Flat COMA machines, like all other COMA machines <ref> [6, 8, 10, 14, 16] </ref>, organize the distributed memories as caches called attraction memories. Like caches, attraction memories are composed of data array and tag array. <p> Instead, the memory hierarchy had to be searched. Such search, even if done intelligently, was likely to cause some performance degradation. Flat COMA machines are unlike traditional COMA machines in that each individual memory line has a directory entry in an agreed-upon node <ref> [16] </ref>. This node is called the home. The home, however, does not have a repository for the data. Therefore, in the general case, a message sent to the home by a processor requesting data will be forwarded to a third node.
References-found: 16

