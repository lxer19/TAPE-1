URL: ftp://ftp.cs.toronto.edu/pub/reports/csri/327/TR327.ps.Z
Refering-URL: http://www.cs.toronto.edu/~mfalou/papers.html
Root-URL: http://www.cs.toronto.edu
Title: Creating Optimal Distributed Algorithms for Minimum Spanning Trees  
Author: Michalis Faloutsos and Mart Molle 
Date: May 9, 1995  
Affiliation: University of Toronto  
Abstract: This paper examines the complexity of distributed algorithms for finding a Minimum Spanning Tree in undirected graphs; the goal is to create algorithms optimal with respect to both communication O(E + N log N ) and time O(N ), where E; N is the number of edges and nodes respectively. A fundamental bad case that leads to non-optimal performance and the proposed techniques to overcome this problem are presented. We introduce new techniques based on the the idea that we call Distributed Information; nodes store information that summarizes properties of groups of nodes. The techniques (and the corresponding algorithms) are classified in communication optimal and time optimal ones. Finally, the structure of the algorithm proposed in [Awe87] and the above classification can lead to a pattern for creating optimal algorithms. In addition, a simple O(E) messages and O(N ) time algorithm for counting the nodes of the network is introduced; It can be used as part of the optimal algorithm or it may be of independent interest. 
Abstract-found: 1
Intro-found: 1
Reference: [Awe87] <author> B. Awerbuch. </author> <title> Optimal distributed algorithms for minimum weight spanning tree, counting, leader election and related problems. </title> <booktitle> Proc. 19th Symp. on Theory of Computing, </booktitle> <pages> pages 230-240, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The discussion concludes in a pattern introduced in <ref> [Awe87] </ref>, for creating optimal algorithms using non-optimal ones. An idea, that we call Distributed Information, is introduced in which nodes exchange and store information that summarizes properties of parts of the network. This information can be used to increase the efficiency of the algorithms. <p> that can create an optimal algorithm using non optimal ones. 1.2 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, namely, [CT85], [Gaf85], <ref> [Awe87] </ref>, [Fal95] and [FM95]. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. <p> We think that this convention makes things much clearer. 4 2.2 Estimating the Distance Two techniques were introduced in <ref> [Awe87] </ref> and modified in its revised version [FM95]. The more complex structure of the two algorithms will be discussed in the next section. First, the Root Distance technique (called Root Update in [Awe87]) ensures that the root of a level L fragment will never be blocked for longer than O (2 <p> think that this convention makes things much clearer. 4 2.2 Estimating the Distance Two techniques were introduced in <ref> [Awe87] </ref> and modified in its revised version [FM95]. The more complex structure of the two algorithms will be discussed in the next section. First, the Root Distance technique (called Root Update in [Awe87]) ensures that the root of a level L fragment will never be blocked for longer than O (2 L+1 ), waiting for the Finding and Reporting procedure to complete. Initiate messages have a counter initialized to 2 L+1 , where L is the level of the root. <p> Namely, we have two types of stored information. The first type of information concerns the previously reported MOEs and facilitates the Reporting procedure. The second type of 4 the Test Distance procedure in <ref> [Awe87] </ref> differs in some crucial points 5 For an explanation of factor 2 see [FM95] 5 decide (root) join inform new root report to father initiate (broadcast) initiate report decide (local)to father NO (broadcast) (b) join inform new root inform old root reach root YES information is some measure of the <p> In <ref> [Awe87] </ref> there is an algorithm that creates a spanning tree with O (E + N log N ) messages and O (N ) time units. We assume that we can originally activate one node, we can use the following algorithm. Note that this assumption seems to hold for real applications. <p> Note the techniques will be denoted by the names given in this paper. Communication: The optimal number of messages is a O (E + N log (N )). 1. Root Size ([CT85] [Gaf85]). 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST). 4. Root Distance ([FM95] <ref> [Awe87] </ref>). The communication-only optimal algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. Time: The optimal time is O (N ) time units. 1. Tentative (Tentative algorithm). 2. Leader Distance ( [FM95]). 3. Leader Size (optiMST). The time-only optimal algorithms are: the algorithm of the third phase in [Awe87], optiMST and <p> Distance ([FM95] <ref> [Awe87] </ref>). The communication-only optimal algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. Time: The optimal time is O (N ) time units. 1. Tentative (Tentative algorithm). 2. Leader Distance ( [FM95]). 3. Leader Size (optiMST). The time-only optimal algorithms are: the algorithm of the third phase in [Awe87], optiMST and Tentative. Note that the above time optimal techniques need the support of the communication optimal techniques. The former alone can not guarantee optimal time and they may also create cycles. <p> So far there does not exist a technique or a combination of techniques that could make [GHS83] optimal in both aspects; a more complicated solution to the problem is required and is presented next. In <ref> [Awe87] </ref>, Awerbuch suggested an innovative three-phase algorithm whose revised version [FM95] achieves optimal performance in terms of both message and time complexity. To elaborate, in the first Counting phase, the algorithm determines N . <p> N , that will be used as a switching criterion from phase II to phase III. If N is known then this phase can be skipped. We can use the Counting algorithm of <ref> [Awe87] </ref> or the Simple Count that we previously saw. II) Small fragment MST Phase: Size &lt; N log (N) . Small fragments join fast and therefore we do not need communication expensive techniques. <p> II) Small fragment MST Phase: Size &lt; N log (N) . Small fragments join fast and therefore we do not need communication expensive techniques. We can use any algorithm optimal with respect 10 to communication that for the "small" size fragments is also optimal to time. It was proven <ref> [Awe87] </ref> that [GHS83] has this property and so do the communication optimal algorithms we listed above, since they are based on it. Notice that every fragment that finds out that its size satisfies the above inequality, passes independently in the next phase.
Reference: [AZ89] <author> Mohan Ahuja and Yahui Zhu. </author> <title> Distributed algorithm for minimum weihgt spanning trees based on echo algorithms. </title> <booktitle> 9th International Conference on Distributed Computing Systems, publ. by IEEE, </booktitle> <year> 1989. </year> <month> 11 </month>
Reference-contexts: The results showed an increase in the efficiency that often exceeded our expectations (see [Fal95]). However, due to space limitations, it seemed impossible to include any of this information in this paper. It should be noted that algorithms with different approaches compared to [GHS83] exist; first we can mention <ref> [AZ89] </ref> where messages accumulate graph information (O (N log N ) bits per message are required) and the algorithm achieves O (E + N log N ) message complexity and O (D log N ) time units with small constants (D is the diameter of the network); second we can mention
Reference: [CT85] <author> F. Chin and H.F. Ting. </author> <title> An almost linear time and o(vlogv + e) messages distributed algorithm for minimum weight spanning trees. </title> <booktitle> Proceedings of 1985 FOCS Conference Portland, </booktitle> <address> Oregon, </address> <month> October </month> <year> 1985. </year>
Reference-contexts: the structure that can create an optimal algorithm using non optimal ones. 1.2 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, namely, <ref> [CT85] </ref>, [Gaf85], [Awe87], [Fal95] and [FM95]. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. <p> These techniques will be presented without repeating features that do not change. 2.1 The Size Estimating Improvement The technique which we will call Root Size 3 was introduced in <ref> [CT85] </ref> [Gaf85] and tries to make the fragment level a better estimate of the fragment size. It is obvious that any fragment of level L must have at least 2 L nodes. <p> Communication: The optimal number of messages is a O (E + N log (N )). 1. Root Size (<ref> [CT85] </ref> [Gaf85]). 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST). 4. Root Distance ([FM95] [Awe87]). The communication-only optimal algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. Time: The optimal time is O (N ) time units. 1. Tentative (Tentative algorithm). 2. Leader Distance ( [FM95]). 3. Leader Size (optiMST). The time-only optimal algorithms are: the algorithm of the third phase in [Awe87], optiMST and Tentative.
Reference: [Fal95] <author> Michalis Faloutsos. </author> <title> Corrections, improvements, simulations and optimstic algorithms for the distributed minimum spanning tree problem. </title> <institution> Master's Thesis University of Toronto, </institution> <year> 1995. </year>
Reference-contexts: The algorithm constructs a spanning tree and thus slightly modified it can solve the leader election problem with similar complexity. Simulations and experiments were conducted with different families of graphs and with variable network size. The results showed an increase in the efficiency that often exceeded our expectations (see <ref> [Fal95] </ref>). However, due to space limitations, it seemed impossible to include any of this information in this paper. <p> can create an optimal algorithm using non optimal ones. 1.2 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, namely, [CT85], [Gaf85], [Awe87], <ref> [Fal95] </ref> and [FM95]. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. <p> This technique which we call Fast Report can be incorporated in [GHS83] and give rise to the algorithm that we call the Fast Report algorithm with identical asymptotic complexity (see <ref> [Fal95] </ref> fro more details). <p> It is easy to see that this technique that we call Leader Size, is equivalent to the Leader Distance technique that we saw before, and leads to optimal time (O (N )) and non optimal communication (O (E + N 2 )) (see the analysis in Leader Distance and <ref> [Fal95] </ref> for more details). The difference is that the size-distance is estimated by the fragment that accepts a submission and the submitted fragment can benefit from it right away. <p> In general, it can be proven that the time delay between level increases is linear to the size of the fragments involved and so the termination time is O (N ) (for a detailed proof see <ref> [Fal95] </ref>). 3.5 Counting the Nodes of the Network We present here an algorithm that counts the nodes of the network by constructing a spanning tree. <p> The former alone can not guarantee optimal time and they may also create cycles. Briefly, we can mention that the Leader Distance/Size techniques have to be combined with the Root Distance/Size technique respectively (for more details see <ref> [Fal95] </ref> [FM95]). There is a tradeoff between communication and time; the techniques that achieve optimal time, require non-optimal number of messages.
Reference: [FM95] <author> Michalis Faloutsos and Mart Molle. </author> <title> Optimal distributed algorithm for minimum spanning trees revisited. </title> <note> to appear in PODC, </note> <year> 1995. </year>
Reference-contexts: an optimal algorithm using non optimal ones. 1.2 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, namely, [CT85], [Gaf85], [Awe87], [Fal95] and <ref> [FM95] </ref>. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. <p> We think that this convention makes things much clearer. 4 2.2 Estimating the Distance Two techniques were introduced in [Awe87] and modified in its revised version <ref> [FM95] </ref>. The more complex structure of the two algorithms will be discussed in the next section. <p> In addition, nodes that receive a testDistance message of a higher level are also allowed to increase their level. Several issues have to be taken care of in order to guarantee efficiency and correctness (see <ref> [FM95] </ref> for details). The symmetry of the two procedures is apparent; they explore the same path from different ends. The second technique can increase the communication complexity to O (E +N 2 ) in the following bad cases. <p> Namely, we have two types of stored information. The first type of information concerns the previously reported MOEs and facilitates the Reporting procedure. The second type of 4 the Test Distance procedure in [Awe87] differs in some crucial points 5 For an explanation of factor 2 see <ref> [FM95] </ref> 5 decide (root) join inform new root report to father initiate (broadcast) initiate report decide (local)to father NO (broadcast) (b) join inform new root inform old root reach root YES information is some measure of the distance from the root and helps submitted fragments increase their level by giving them <p> Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST). 4. Root Distance (<ref> [FM95] </ref> [Awe87]). The communication-only optimal algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. Time: The optimal time is O (N ) time units. 1. Tentative (Tentative algorithm). 2. Leader Distance ( [FM95]). 3. Leader Size (optiMST). The time-only optimal algorithms are: the algorithm of the third phase in [Awe87], optiMST and Tentative. Note that the above time optimal techniques need the support of the communication optimal techniques. The former alone can not guarantee optimal time and they may also create cycles. <p> The former alone can not guarantee optimal time and they may also create cycles. Briefly, we can mention that the Leader Distance/Size techniques have to be combined with the Root Distance/Size technique respectively (for more details see [Fal95] <ref> [FM95] </ref>). There is a tradeoff between communication and time; the techniques that achieve optimal time, require non-optimal number of messages. <p> So far there does not exist a technique or a combination of techniques that could make [GHS83] optimal in both aspects; a more complicated solution to the problem is required and is presented next. In [Awe87], Awerbuch suggested an innovative three-phase algorithm whose revised version <ref> [FM95] </ref> achieves optimal performance in terms of both message and time complexity. To elaborate, in the first Counting phase, the algorithm determines N .
Reference: [Gaf85] <author> Eli Gafni. </author> <title> Improvements in the time complexity of two message-optimal election algorithms. </title> <booktitle> Proceedings of 1985 PODC, Conference, </booktitle> <address> Minacki, Ontario(August), </address> <year> 1985. </year>
Reference-contexts: structure that can create an optimal algorithm using non optimal ones. 1.2 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper [GHS83], Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, namely, [CT85], <ref> [Gaf85] </ref>, [Awe87], [Fal95] and [FM95]. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. <p> These techniques will be presented without repeating features that do not change. 2.1 The Size Estimating Improvement The technique which we will call Root Size 3 was introduced in [CT85] <ref> [Gaf85] </ref> and tries to make the fragment level a better estimate of the fragment size. It is obvious that any fragment of level L must have at least 2 L nodes. <p> Note the techniques will be denoted by the names given in this paper. Communication: The optimal number of messages is a O (E + N log (N )). 1. Root Size ([CT85] <ref> [Gaf85] </ref>). 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST). 4. Root Distance ([FM95] [Awe87]). The communication-only optimal algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. Time: The optimal time is O (N ) time units. 1. Tentative (Tentative algorithm). 2. Leader Distance ( [FM95]). 3. <p> Communication: The optimal number of messages is a O (E + N log (N )). 1. Root Size ([CT85] <ref> [Gaf85] </ref>). 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST). 4. Root Distance ([FM95] [Awe87]). The communication-only optimal algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. Time: The optimal time is O (N ) time units. 1. Tentative (Tentative algorithm). 2. Leader Distance ( [FM95]). 3. Leader Size (optiMST). The time-only optimal algorithms are: the algorithm of the third phase in [Awe87], optiMST and Tentative.
Reference: [GHS83] <author> R.G. Gallager, </author> <title> P.A. Humblet, and P.M. Spira. A distributed algorithm for minimum weight spanning trees. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 5(1) </volume> <pages> 66-77, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: All the techniques and the algorithms discussed here are based on the algorithm of Gallager, Humblet and Spira <ref> [GHS83] </ref> which we refer to as the basic algorithm. The difficulty of achieving an optimal algorithm is studied through a bad case example. <p> The results showed an increase in the efficiency that often exceeded our expectations (see [Fal95]). However, due to space limitations, it seemed impossible to include any of this information in this paper. It should be noted that algorithms with different approaches compared to <ref> [GHS83] </ref> exist; first we can mention [AZ89] where messages accumulate graph information (O (N log N ) bits per message are required) and the algorithm achieves O (E + N log N ) message complexity and O (D log N ) time units with small constants (D is the diameter of <p> The structure of this paper is as follows. In the rest of this section, the basic <ref> [GHS83] </ref> algorithm and a bad case example are described. Section 2 lists the innovations introduced by previous algorithms. Section 3 presents new techniques and algorithms for the MST and counting problem. <p> Section 3 presents new techniques and algorithms for the MST and counting problem. Finally in section 4, we group all the techniques and examine the structure that can create an optimal algorithm using non optimal ones. 1.2 Basic Algorithm of Gallager, Humblet and Spira In their pioneering paper <ref> [GHS83] </ref>, Gallager, Humblet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of subsequent work in the area, namely, [CT85], [Gaf85], [Awe87], [Fal95] and [FM95]. <p> The leader sends a connect message along that edge and joins with the other fragment. Note that the leader is a "temporary root"; it makes the decisions for its fragment and its role ends when an initiate message from a root arrives. Even the basic algorithm presented in <ref> [GHS83] </ref> contains several subtleties. First, each fragment has a level, L, in addition to its unique fragment identifier F , denoted as a pair (F; L) for the rest of the paper. <p> It can be shown (e.g., <ref> [GHS83] </ref>) that the message complexity of this algorithm is O (E + N log (N )), and hence optimal. However, its time complexity is O (N log (N )) and hence not optimal. <p> This MOE information will update the edge-MOE of the corresponding edge of the receiver node. For example, a node that receives a report message will store the reported MOE as the edge-MOE of that edge. It is important to notice that under the Joining policy introduced in <ref> [GHS83] </ref>, the weight of the MOE reachable via a branch can only increase over time. In other words, edge-MOEs are non-decreasing in time and thus an edge-MOE, possibly a previous MOE, is a lower bound of the current MOE. These two observations are fundamental for the correctness of this approach. <p> Although this sounds trivial in practice, it speeds up the final time consuming Finding procedures and reduces the termination time considerably. This technique which we call Fast Report can be incorporated in <ref> [GHS83] </ref> and give rise to the algorithm that we call the Fast Report algorithm with identical asymptotic complexity (see [Fal95] fro more details). <p> Note that the decision is taken wherever this is possible, and at the worst case at the root. In other words, nodes have the ability to take advantage of a "good" configuration. Obviously, in non-favourable configurations this algorithm performs as <ref> [GHS83] </ref>, while in favourable configurations it is faster. However, the worst case complexity analysis of the two algorithms is identical. The conceptual differences of this algorithm compared to the [GHS83] algorithm are the following. <p> Obviously, in non-favourable configurations this algorithm performs as <ref> [GHS83] </ref>, while in favourable configurations it is faster. However, the worst case complexity analysis of the two algorithms is identical. The conceptual differences of this algorithm compared to the [GHS83] algorithm are the following. First the nodes can function autonomously and asynchronously, i.e., we do not have the strict phases Find-Report-Join but rather a more aggressive "join when you can" approach. <p> Time. As stated earlier, a fragment never gets "trapped" waiting at an outgoing edge that is not its Real MOE. In the bad case example (Fig.1), assume that e 1 &gt; e x (denoting the weights of the edges) and recall that in <ref> [GHS83] </ref> fragment (F; L) would have to wait for an answer from (F 1 ; L 1 ), but this would happen only if L 1 &gt; L, which may take a long time. <p> Note that this assumption seems to hold for real applications. In the beginning, all the nodes are Sleeping except for one Active node which will be the root of the spanning tree. We have two phases that bear some similarities with the Finding and Reporting procedures of <ref> [GHS83] </ref>: Exploration phase.The root broadcasts an activate message to all adjacent nodes. Every node that receives such a message marks itself Active, considers the sender of the message as parent and broadcasts the activate message to its children. <p> The root finds the node with minimum ID and if required, notifies that node adding at most N messages and N time units to the complexity. 9 4 Creating Optimal Algorithms So far we have seen a number of different techniques that can be incorporated in <ref> [GHS83] </ref> and give rise to new algorithms. We can now group these techniques according to whether they are optimal with respect to communication or time. Note the techniques will be denoted by the names given in this paper. <p> Communication: The optimal number of messages is a O (E + N log (N )). 1. Root Size ([CT85] [Gaf85]). 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST). 4. Root Distance ([FM95] [Awe87]). The communication-only optimal algorithms are: <ref> [GHS83] </ref>, [CT85], [Gaf85], Fast Report and autonoMST. Time: The optimal time is O (N ) time units. 1. Tentative (Tentative algorithm). 2. Leader Distance ( [FM95]). 3. Leader Size (optiMST). The time-only optimal algorithms are: the algorithm of the third phase in [Awe87], optiMST and Tentative. <p> There is a tradeoff between communication and time; the techniques that achieve optimal time, require non-optimal number of messages. So far there does not exist a technique or a combination of techniques that could make <ref> [GHS83] </ref> optimal in both aspects; a more complicated solution to the problem is required and is presented next. In [Awe87], Awerbuch suggested an innovative three-phase algorithm whose revised version [FM95] achieves optimal performance in terms of both message and time complexity. <p> Small fragments join fast and therefore we do not need communication expensive techniques. We can use any algorithm optimal with respect 10 to communication that for the "small" size fragments is also optimal to time. It was proven [Awe87] that <ref> [GHS83] </ref> has this property and so do the communication optimal algorithms we listed above, since they are based on it. Notice that every fragment that finds out that its size satisfies the above inequality, passes independently in the next phase. III) Large fragment MST phase: Size N log (N) . <p> Apart from the previous techniques that increase the efficiency of the basic algorithm <ref> [GHS83] </ref>, new techniques were presented that use distributed information that resides at the nodes. The techniques can be classified according to their asymptotic behavior and using an optimal pattern, we can combine them to create optimal algorithms.
Reference: [GKP93] <author> J.A. Garay, S. Kutten, and D. Peleg. </author> <title> A sub-linear time distributed algorithm for minimum-weight spanning trees. </title> <booktitle> FOCS, </booktitle> <year> 1993. </year> <month> 34. </month>
Reference-contexts: where messages accumulate graph information (O (N log N ) bits per message are required) and the algorithm achieves O (E + N log N ) message complexity and O (D log N ) time units with small constants (D is the diameter of the network); second we can mention <ref> [GKP93] </ref> and its improved version [KP95], where communication complexity is ignored, dominating sets and edge elimination techniques are used, and the algorithms achieve O (D + N 0:614 ) and O (N 1=2 G (N ) + D) 2 respectively. The structure of this paper is as follows.
Reference: [KP95] <author> S. Kutten and D. Peleg. </author> <title> Fast distributed construction of k-dominating sets ans applications. </title> <note> to appear in PODC, 1995. 12 </note>
Reference-contexts: (O (N log N ) bits per message are required) and the algorithm achieves O (E + N log N ) message complexity and O (D log N ) time units with small constants (D is the diameter of the network); second we can mention [GKP93] and its improved version <ref> [KP95] </ref>, where communication complexity is ignored, dominating sets and edge elimination techniques are used, and the algorithms achieve O (D + N 0:614 ) and O (N 1=2 G (N ) + D) 2 respectively. The structure of this paper is as follows.
References-found: 9

