URL: http://www.stat.washington.edu:80/tech.reports/tr335.ps
Refering-URL: http://www.stat.washington.edu:80/tech.reports/
Root-URL: 
Title: Bayesian Model Averaging  
Author: Jennifer A. Hoeting David Madigan, Adrian E. Raftery Chris T. Volinsky 
Keyword: Bayesian model averaging; Bayesian graphical models; Learning; Model uncertainty; Markov chain Monte Carlo  
Address: Fort Collins, CO 80526, USA  
Note: Research supported in part by the U.S. National Science Foundation and the U.S. Office of Naval Research (N00014-91-J-1014). The authors are grateful to David Lewis and Robert Schapire for helpful advice. Address for correspondence:  
Affiliation: Colorado State University  University of Washington  AT&T Labs  Department of Statistics University of Washington  Department of Statistics, Colorado State University,  
Pubnum: Technical Report 335  
Email: (jah@stat.colostate.edu).  
Date: May 28, 1998  
Abstract: Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently available BMA software. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> (1973). </year> <title> Information theory and an extension of the maximum likelihood principle. </title> <editor> In Petrox, B. and Caski, F., editors, </editor> <booktitle> Second International Symposium on Information Theory, </booktitle> <pages> page 267. </pages>
Reference-contexts: However, Freedman et al. (1988) have shown that this does not necessarily give a satisfactory 30 solution to the problem. Buckland et al. (1997) suggested several ad-hoc approaches to accounting for model uncertainty which are non-Bayesian. They suggest approximating model weights based on Akaike's information criterion (AIC) <ref> (Akaike, 1973) </ref>. This approach is similar to the BIC approximating strategies described above. Kass and Raftery (1995) discuss the relative merits of AIC and BIC in this context. To estimate model uncertainty, Buckland et al. suggest several bootstrapping methods.
Reference: <author> Ali, K. M. </author> <year> (1995). </year> <title> A comparison of methods for learning and combining evidence from multiple models. </title> <type> Technical Report 95-47, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference: <author> Barnard, G. A. </author> <year> (1963). </year> <title> New methods of quality control. </title> <journal> Journal of the Royal Statistical Society (Ser. </journal> <note> A), 126:255. </note>
Reference: <author> Bates, J. M. and Granger, C. W. J. </author> <year> (1969). </year> <title> The combination of forecasts. </title> <journal> Operational Research Quarterly, </journal> <volume> 20 </volume> <pages> 451-468. </pages>
Reference: <author> Berger, J. O. and Delampady, M. </author> <year> (1987). </year> <title> Testing precise hypotheses. </title> <journal> Statistical Science, </journal> <volume> 2 </volume> <pages> 317-352. </pages>
Reference-contexts: The posterior probability of "no effect" can be viewed as an approximation to the posterior probability of the effect being "small", namely P (jfij &lt; "), provided that " is at most about one-half of a standard error <ref> (Berger and Delampady, 1987) </ref>. 7.1.3 Predictive Performance For assessing predictive performance, we randomly split the data into two parts such that an equal number of events (61 deaths) occurred in each part.
Reference: <author> Berger, J. O. and Sellke, T. </author> <year> (1987). </year> <title> Testing a point null hypothesis (with discussion). </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 112-122. </pages>
Reference-contexts: The posterior probability of "no effect" can be viewed as an approximation to the posterior probability of the effect being "small", namely P (jfij &lt; "), provided that " is at most about one-half of a standard error <ref> (Berger and Delampady, 1987) </ref>. 7.1.3 Predictive Performance For assessing predictive performance, we randomly split the data into two parts such that an equal number of events (61 deaths) occurred in each part.
Reference: <author> Bernardo, J. and Smith, A. </author> <year> (1994). </year> <title> Bayesian Theory. </title> <publisher> Wiley: </publisher> <address> Chichester. </address>
Reference: <author> Besag, J. E., Green, P., Higdon, D., and Mengerson, K. </author> <year> (1995). </year> <title> Bayesian computation and stochastic systems. </title> <journal> Statistical Science, </journal> <volume> 10 </volume> <pages> 3-66. </pages>
Reference-contexts: Several authors have suggested alternative approaches to choosing the class of models for BMA. Draper (1995) suggested finding a good model and then averaging over an expanded class of models "near" the good model <ref> (see also, Besag et al., 1995, Section 5.6) </ref>. Within a single model structure, this approach is similar to the Madigan and Raftery (1994) suggestion to average over a parsimonious set of models supported by the data.
Reference: <author> Breiman, L. </author> <year> (1996). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 26 </volume> <pages> 123-140. </pages>
Reference: <author> Breiman, L. and Friedman, J. H. </author> <year> (1985). </year> <title> Estimating optimal transformations for multiple regression and correlation (with discussion). </title> <journal> Journal of the American Statistical Association, </journal> <volume> 80 </volume> <pages> 580-619. </pages>
Reference-contexts: For transformation of the predictors, HRM95 proposed a novel approach consisting of an initial exploratory use of the Alternating Conditional Expectation algorithm (ACE), followed by change point transformations if needed. The ACE algorithm <ref> (Breiman and Friedman, 1985) </ref> provides nonlinear transformations of the variables in a regression model. ACE chooses the transformations to maximize the correlation between the transformed response and the sum of the transformed predictors. HRM95 used ACE to suggest non-parametric transformations of the predictors.
Reference: <author> Brozek, J., Grande, F., Anderson, J., and Keys, A. </author> <year> (1963). </year> <title> Densitometric analysis of body composition: Revision of some quantitative assumptions. </title> <journal> Annals of the New York Academy of Sciences, </journal> <volume> 110 </volume> <pages> 113-140. </pages>
Reference-contexts: Percent body fat was determined using body density, the ratio of body mass to body volume. Body volume was measured using an underwater weighing technique (Katch and McArdle, 1993, p. 242-244). Body density was then used to estimate percent body fat using Brozek's equation <ref> (Brozek et al., 1963) </ref>, % body fat = 457=Density 414:2: (20) For more details on the derivation of Equation (20) see Johnson (1996) and Brozek et al. (1963).
Reference: <author> Buckland, S. T., Burnham, K. P., and Augustin, N. H. </author> <year> (1997). </year> <title> Model selection: An integral part of inference. </title> <journal> Biometrics, </journal> <volume> 53 </volume> <pages> 275-290. </pages>
Reference: <author> Buntine, W. </author> <year> (1992). </year> <title> Learning classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 2 </volume> <pages> 63-73. </pages> <note> 32 Carlin, </note> <author> B. P. and Chib, S. </author> <year> (1993). </year> <title> Bayesian model choice via Markov chain Monte Carlo. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> 55:473-484. 
Reference: <author> Carlin, B. P. and Polson, N. G. </author> <year> (1991). </year> <title> Inference for nonconjugate Bayesian models using the Gibbs sampler. </title> <journal> The Canadian Journal of Statistics, </journal> <volume> 19 </volume> <pages> 399-405. </pages>
Reference: <author> Chan, P. K. and Stolfo, S. J. </author> <year> (1996). </year> <title> On the accuracy of meta-learning for scalable data mining. </title> <journal> Journal of Intelligent Integration of Information. </journal>
Reference: <author> Chatfield, C. </author> <year> (1995). </year> <title> Model uncertainty, data mining, and statistical inference (with discussion). </title> <journal> Journal of the Royal Statistical Society (Ser. A), </journal> <volume> 158 </volume> <pages> 419-466. </pages>
Reference: <author> Clemen, R. T. </author> <year> (1989). </year> <title> Combining forecasts: a review and annotated bibliography. </title> <journal> International Journal of Forecasting, </journal> <volume> 5 </volume> <pages> 559-583. </pages>
Reference: <author> Clyde, M., DeSimone, H., and Parmigiani, G. </author> <year> (1996). </year> <title> Prediction via orthoganalized model mixing. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 91 </volume> <pages> 1197-1208. </pages>
Reference: <author> Coifman, R. R. and Donoho, D. L. </author> <year> (1996). </year> <title> Translation-invariant de-noising. </title> <booktitle> In Lecture Notes in Statistics (103): Wavelets and Statistics. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Cox, D. R. </author> <year> (1972). </year> <title> Regression models and life tables (with discussion). </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 34 </volume> <pages> 187-220. </pages>
Reference-contexts: The most popular way of doing this is to use the Cox proportional hazards model <ref> (Cox, 1972) </ref>, which allows different hazard rates for cases with different covariate vectors and leaves the underlying common baseline hazard rate unspecified.
Reference: <author> Dawid, A. P. </author> <year> (1984). </year> <title> Statistical theory-the prequential approach. </title> <journal> Journal of the Royal Statistical Society (Series A), </journal> <volume> 147 </volume> <pages> 278-292. </pages>
Reference-contexts: Ibrahim and Laud (1994) adopt a somewhat similar approach in the context of linear models. 6 Predictive Performance Before presenting two examples, we briefly discuss methods for assessing the success of various modeling strategies. A primary purpose of statistical analysis is to make forecasts <ref> (Dawid, 1984) </ref>. Similarly, Bernardo and Smith (1994, p. 238) argue that when comparing rival modeling strategies, all other things being equal, we are more impressed with a modeling strategy that consistently assigns higher probabilities to the events that actually occur.
Reference: <author> Dickinson, J. P. </author> <year> (1973). </year> <title> Some statistical results on the combination of forecasts. </title> <journal> Operational Research Quarterly, </journal> <volume> 24 </volume> <pages> 253-260. </pages>
Reference: <author> Dijkstra, T. K. </author> <year> (1988). </year> <title> On Model Uncertainty and its statistical implications. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: He also points out the fundamental idea that BMA accounts for the uncertainty involved in selecting the model. After Leamer's book was published little attention was given to BMA for some time. The drawbacks of ignoring model uncertainty were recognized by many authors <ref> (e.g., the collection of papers edited by Dijkstra, 1988) </ref>, but little progress was made until new theoretical developments and computational power enabled researchers to overcome the difficulties related to implementing BMA (Section 1). George (1999) reviews Bayesian model selection and discusses BMA in the context of decision theory.
Reference: <author> Draper, D. </author> <year> (1995). </year> <title> Assessment and propagation of model uncertainty. </title> <journal> Journal of the Royal Statistical Society, Ser. B., </journal> <volume> 57 </volume> <pages> 45-97. </pages>
Reference: <author> Draper, D., Gaver, D. P., Goel, P. K., Greenhouse, J. B., Hedges, L. V., Morris, C. N., Tucker, J., and Waternaux, C. </author> <year> (1993). </year> <title> Combining information: National Research Council Panel on Statistical Issues and Opportunities for Research in the Combination of Information. </title> <publisher> National Academy Press, </publisher> <address> Washington. </address>
Reference-contexts: We note that the logarithmic scoring rule is a proper scoring rule as defined by Matheson and Winkler (1976) and others. Several other measures of predictive performance are described in the examples below. For probabilistic predictions, there exist two types of discrepancies between observed and predicted values <ref> (Draper et al., 1993) </ref>: predictive bias (a systematic tendency to predict on the low or high side) and lack of calibration (a systematic tendency to over- or understate predictive accuracy). The predictive log score is a combined measure of bias and calibration.
Reference: <author> Draper, D., Hodges, J. S., Leamer, E. E., Morris, C. N., and Rubin, D. B. </author> <year> (1987). </year> <title> A research agenda for assessment and propagation of model uncertainty. </title> <type> Technical Report Rand Note N-2683-RC, </type> <institution> The RAND Corporation, </institution> <address> Santa Monica, California. </address>
Reference: <author> Edwards, W., Lindman, H., and Savage, L. J. </author> <year> (1963). </year> <title> Bayesian statistical inference for psychological research. </title> <journal> Psychological Review, </journal> <volume> 70 </volume> <pages> 193-242. </pages>
Reference: <author> Fernandez, C., Ley, E., and Steel, M. F. </author> <year> (1997). </year> <title> Statistical modeling of fishing activities in the north atlantic. </title> <type> Technical report, </type> <institution> Department of Econometrics, Tilburg Univeristy, the Netherlands. </institution>
Reference: <author> Fernandez, C., Ley, E., and Steel, M. F. </author> <year> (1998). </year> <title> Benchmark priors for Bayesian model averaging. </title> <type> Technical report, </type> <institution> Department of Econometrics, Tilburg Univeristy, the Netherlands. </institution>
Reference: <author> Fleming, T. R. and Harrington, D. H. </author> <year> (1991). </year> <title> Counting Processes and Survival Analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Freedman, D. A., Navidi, W., and Peters, S. C. </author> <year> (1988). </year> <title> On the impact of variable selection in fitting regression equations. In Dijkstra, </title> <editor> T. K., editor, </editor> <title> On Model Uncertainty and its statistical implications. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Freund, Y. </author> <year> (1995). </year> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <volume> 121 </volume> <pages> 256-285. </pages> <note> 33 Fried, </note> <author> L. P., Borhani, N. O., et al. </author> <year> (1991). </year> <title> The cardiovascular health study: Design and rationale. </title> <journal> Annals of Epidemiology, </journal> 1:263-276. 
Reference-contexts: As with the neural network ensembles, the greater the diversity in the predictors, the larger the gain in predictive performance. "Boosting" is like stacking in that a given learning algorithm is rerun many times with different training sets <ref> (Freund, 1995) </ref>. However, boosting uses a more sophisticated method for computing each training set in which it tries to focus the learning algorithm on the "hardest" parts of the distribution.
Reference: <author> Furnival, G. M. and Wilson, R. W. </author> <year> (1974). </year> <title> Regression by leaps and bounds. </title> <journal> Technometrics, </journal> <volume> 16 </volume> <pages> 499-511. </pages>
Reference-contexts: VMRK use the "leaps and bounds" algorithm <ref> (Furnival and Wilson, 1974) </ref> to rapidly identify models to be used in the summation of Equation (1). The second approach, Markov chain Monte Carlo model composition (MC 3 ), uses a Markov chain Monte Carlo method to directly approximate (1) (Madigan and York, 1995).
Reference: <author> Geisser, S. </author> <year> (1980). </year> <title> Discussion on sampling and Bayes' inference in scientific modeling and robustness (by g. </title> <editor> e. p. </editor> <title> box). </title> <journal> Journal of the Royal Statistical Society (Series A), </journal> <volume> 143 </volume> <pages> 416-417. </pages>
Reference-contexts: Performance is then measured on the second half of the data (test data, or D T ). One measure of predictive ability is the logarithmic scoring rule of Good (1952) which is based on the conditional predictive ordinate <ref> (Geisser, 1980) </ref>.
Reference: <author> George, E. and McCulloch, R. </author> <year> (1993). </year> <title> Variable selection via Gibbs sampling. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88 </volume> <pages> 881-889. </pages>
Reference: <author> George, E. I. </author> <year> (1999). </year> <title> Bayesian model selection. </title> <journal> In Encyclopedia of Statistical Sciences Update, </journal> <volume> volume 3. </volume> <publisher> Wiley, </publisher> <address> New York, </address> <note> to appear. </note>
Reference: <author> Good, I. J. </author> <year> (1950). </year> <title> Probability and the weighing of evidence. </title> <address> Charles Griffin, London. </address>
Reference-contexts: Madigan et al. (1995) provide a simple method for informative prior elicitation in discrete data applications and show that their approach provides improved predictive performance for their application. The method elicits an informative prior distribution on model space via "imaginary data" <ref> (Good, 1950) </ref>.
Reference: <author> Good, I. J. </author> <year> (1952). </year> <title> Rational decisions. </title> <journal> Journal of the Royal Statistical Society (Ser. B), </journal> <volume> 14 </volume> <pages> 107-114. </pages>
Reference: <author> Grambsch, P. M., Dickson, E. R., Kaplan, M., et al. </author> <year> (1989). </year> <title> Extramural cross-validation of the Mayo primary biliary cirrhosis survival model establishes its generalizability. </title> <journal> Hepa-tology, </journal> <volume> 10 </volume> <pages> 846-850. </pages>
Reference: <author> Green, P. J. </author> <year> (1995). </year> <title> Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. </title> <journal> Biometrika, </journal> <volume> 82 </volume> <pages> 711-732. </pages>
Reference: <author> Heckerman, D., Geiger, D., and Chickering, D. M. </author> <year> (1994). </year> <title> Learning Bayesian networks: the combination of knowledge and statistical data. </title> <editor> In de Mantaras, B. L. and Poole, D., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, Proceedings of the Tenth Conference, </booktitle> <pages> pages 293-301. </pages> <publisher> Morgan Kaufman: </publisher> <address> San Francisco. </address>
Reference: <author> Hodges, J. S. </author> <year> (1987). </year> <title> Uncertainty, policy analysis, </title> <journal> and statistics. Statistical Science, </journal> <volume> 2 </volume> <pages> 259-291. </pages>
Reference: <author> Hoeting, J. A. </author> <year> (1994). </year> <title> Accounting for Model Uncertainty in Linear Regression. </title> <type> PhD thesis, </type> <institution> University of Washington. </institution>
Reference-contexts: This two-step method is computationally feasible, and it allows for groups of observations to be considered simultaneously as potential outliers. HRM96 provided evidence that SVO successfully identifies masked outliers. A simultaneous variable, transformation, and outlier selection approach 8 (SVOT) which combines SVO and SVT has also been proposed <ref> (Hoeting, 1994) </ref>.
Reference: <author> Hoeting, J. A., Raftery, A. E., and Madigan, D. </author> <year> (1995). </year> <title> Simultaneous variable and transformation selection in linear regression. </title> <type> Technical Report 9506, </type> <institution> Department of Statistics, Colorado State University. </institution>
Reference: <author> Hoeting, J. A., Raftery, A. E., and Madigan, D. </author> <year> (1996). </year> <title> A method for simultaneous variable selection and outlier identification in linear regression. </title> <journal> Journal of Computational Statistics, </journal> <volume> 22 </volume> <pages> 251-271. </pages>
Reference: <author> Ibrahim, J. G. and Laud, P. W. </author> <year> (1994). </year> <title> A predictive approach to the analysis of designed experiments. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89 </volume> <pages> 309-319. </pages>
Reference: <author> Johnson, R. W. </author> <year> (1996). </year> <title> Fitting percentage of body fat to simple body measurements. </title> <journal> Journal of Statistics Education, </journal> <volume> 4. </volume>
Reference: <author> Kass, R. E. and Raftery, A. E. </author> <year> (1995). </year> <title> Bayes factors. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90 </volume> <pages> 773-795. </pages>
Reference: <author> Kass, R. E. and Wasserman, L. </author> <year> (1995). </year> <title> A reference Bayesian test for nested hypothese with large samples. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90 </volume> <pages> 928-934. </pages>
Reference: <author> Katch, F. and McArdle, W. </author> <year> (1993). </year> <title> Nutrition, Weight Control, and Exercise. </title> <editor> Williams and Wilkins, </editor> <booktitle> Philadelphia, 4th edition. </booktitle>
Reference-contexts: Percent body fat can be measured in a variety of ways including underwater weighing, skinfold calipers, and bioelectric impedance <ref> (Katch and McArdle, 1993) </ref>. One drawback with these methods is that they require specialized equipment or expertise on the part of the person taking the measurements. As a result, simpler methods for measuring body fat have been developed. <p> The response in the regression model is percent body fat. Percent body fat was determined using body density, the ratio of body mass to body volume. Body volume was measured using an underwater weighing technique <ref> (Katch and McArdle, 1993, p. 242-244) </ref>. Body density was then used to estimate percent body fat using Brozek's equation (Brozek et al., 1963), % body fat = 457=Density 414:2: (20) For more details on the derivation of Equation (20) see Johnson (1996) and Brozek et al. (1963).
Reference: <author> Kearns, M. J., Schapire, R. E., and Sellie, L. M. </author> <year> (1994). </year> <title> Toward efficient agnostic learning. </title> <journal> Machine Learning, </journal> <volume> 17 </volume> <pages> 115-142. </pages> <note> 34 Kivinen, </note> <author> J. and Warmuth, M. K. </author> <year> (1995). </year> <title> Exponentiated gradient versus gradient descent for linear predictors. </title> <type> Technical Report UCSC-CRL-94-16, </type> <institution> Computer Engineering and Information Science, University of California, Santa Cruz. </institution>
Reference: <author> Kononenko, I. and Kovacic, M. </author> <year> (1992). </year> <title> Learning as optimization: Stochastic generation of multiple knowledge. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 257-262. </pages>
Reference: <author> Kuk, A. Y. C. </author> <year> (1984). </year> <title> All subsets regression in a proportional hazards model. </title> <journal> Biometrika, </journal> <volume> 71 </volume> <pages> 587-592. </pages>
Reference: <author> Kwok, S. and Carter, C. </author> <year> (1990). </year> <title> Multiple decision trees. </title> <editor> In Shachter, R., Levitt, T., Kanal, L., and Lemmer, J., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 4, </booktitle> <pages> pages 323-349. </pages> <publisher> North Holland. </publisher>
Reference: <author> Laplace, P. S. d. </author> <type> (1818). </type> <institution> Deuxieme Supplement a la Theorie Analytique des Probabilites. </institution>
Reference: <institution> Gauthier-Villars, Paris. </institution> <note> reprinted (1847) in Oeuvres Completes de Laplace, Vol 7. </note>
Reference: <author> Lauritzen, S. L. </author> <year> (1996). </year> <title> Graphical Models. </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference-contexts: To date, most graphical models research has focused on acyclic digraphs, chordal undirected graphs, and chain graphs that allow both directed and undirected edges, but have no partially directed cycles <ref> (Lauritzen, 1996) </ref>. Here we focus on acyclic directed graphs (ADGs) and discrete random variables. In an ADG, all the edges are directed and appear as arrows in the figures. A directed graph is acyclic if it contains no directed cycles.
Reference: <author> Lauritzen, S. L., Thiesson, B., and Spiegelhalter, D. J. </author> <year> (1994). </year> <title> Diagnostic systems created by model selection methods a case study. In Cheeseman, </title> <editor> P. and Oldford, W., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 4, </booktitle> <pages> pages 143-152. </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Lawless, J. and Singhal, K. </author> <year> (1978). </year> <title> Efficient screening of nonnormal regression models. </title> <journal> Biometrics, </journal> <volume> 34 </volume> <pages> 318-327. </pages>
Reference: <author> Leamer, E. E. </author> <year> (1978). </year> <title> Specification Searches. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: Basing inferences on M fl alone is risky; presumably, ambiguity about model selection should dilute information about effect sizes and predictions, since "part of the evidence is spent to specify the model" <ref> (Leamer, 1978, p. 91) </ref> . Draper et al. (1987) and Hodges (1987) make essentially the same observation. Bayesian model averaging provides a way around this problem.
Reference: <author> Lohman, T. </author> <year> (1992). </year> <title> Advance in Body Composition Assessment, Current Issues in Exercise Science (Monograph Number 3). Human Kinetics Publishers, </title> <address> Champaign, IL. </address>
Reference-contexts: The BMA results also provide additional evidence that the p-values for the model selected using stepwise variable selection overstate confidence because they ignore model uncertainty. 7.2 Example 2: Predicting Percent Body Fat 7.2.1 Overview Percent body fat is now commonly used as an indicator of fitness or potential health problems <ref> (Lohman, 1992, p. 1) </ref>. Percent body fat can be measured in a variety of ways including underwater weighing, skinfold calipers, and bioelectric impedance (Katch and McArdle, 1993). One drawback with these methods is that they require specialized equipment or expertise on the part of the person taking the measurements.
Reference: <author> Madigan, D., Andersson, S. A., Perlman, M., and Volinsky, C. T. </author> <year> (1996). </year> <title> Bayesian model averaging and model selection for Markov equivalence classes of acyclic digraphs. </title> <journal> Communications in Statistics: Theory and Methods, </journal> <volume> 25 </volume> <pages> 2493-2520. </pages>
Reference: <author> Madigan, D., Gavrin, J., and Raftery, A. E. </author> <year> (1995). </year> <title> Elicting prior information to enhance the predictive performance of Bayesian graphical models. </title> <journal> Communications in Statistics Theory and Methods, </journal> <volume> 24 </volume> <pages> 2271-2292. </pages>
Reference-contexts: VMRK use the "leaps and bounds" algorithm (Furnival and Wilson, 1974) to rapidly identify models to be used in the summation of Equation (1). The second approach, Markov chain Monte Carlo model composition (MC 3 ), uses a Markov chain Monte Carlo method to directly approximate (1) <ref> (Madigan and York, 1995) </ref>. This generates a stochastic process which moves through model space. Specifically, let M denote the space of models under consideration. <p> For certain interesting classes of models such as discrete graphical models <ref> (e.g., Madigan and York, 1995) </ref> and linear regression (e.g., Raftery et al., 1997), closed form integrals for the marginal likelihood, Equation (3), are available.
Reference: <author> Madigan, D. and Raftery, A. E. </author> <year> (1991). </year> <title> Model selection and accounting for model uncertainty in graphical models using Occam's window. </title> <type> Technical Report 213, </type> <institution> University of Washington, </institution> <address> Seattle. </address>
Reference: <author> Madigan, D. and Raftery, A. E. </author> <year> (1994). </year> <title> Model selection and accounting for model uncertainty in graphical models using Occam's window. </title> <journal> J. American Statistical Association, </journal> <volume> 89 </volume> <pages> 1535-1546. </pages>
Reference-contexts: To construct the Markov chain, define a neighborhood nbd (M ) for each M 2 M. For example, with graphical models the neighborhood might be the set of models with either one link more or one link fewer than M and the model M itself <ref> (Madigan et al., 1994) </ref>. Define a transition matrix q by setting q (M ! M 0 ) = 0 for all M 0 62 nbd (M ) and q (M ! M 0 ) non-zero for all M 0 2 nbd (M ).
Reference: <author> Madigan, D., Raftery, A. E., York, J. C., Bradsahw, J. M., and Almond, R. G. </author> <year> (1994). </year> <title> Strategies for graphical model selection. In Cheeseman, </title> <editor> P. and Oldford, W., editors, </editor> <title> Selecting Models from Data: </title> <booktitle> Artificial Intelligence and Statistics IV, </booktitle> <pages> pages 91-100. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: To construct the Markov chain, define a neighborhood nbd (M ) for each M 2 M. For example, with graphical models the neighborhood might be the set of models with either one link more or one link fewer than M and the model M itself <ref> (Madigan et al., 1994) </ref>. Define a transition matrix q by setting q (M ! M 0 ) = 0 for all M 0 62 nbd (M ) and q (M ! M 0 ) non-zero for all M 0 2 nbd (M ).
Reference: <author> Madigan, D. and York, J. </author> <year> (1995). </year> <title> Bayesian graphical models for discrete data. </title> <journal> International Statistical Review, </journal> <volume> 63 </volume> <pages> 215-232. </pages>
Reference-contexts: VMRK use the "leaps and bounds" algorithm (Furnival and Wilson, 1974) to rapidly identify models to be used in the summation of Equation (1). The second approach, Markov chain Monte Carlo model composition (MC 3 ), uses a Markov chain Monte Carlo method to directly approximate (1) <ref> (Madigan and York, 1995) </ref>. This generates a stochastic process which moves through model space. Specifically, let M denote the space of models under consideration. <p> For certain interesting classes of models such as discrete graphical models <ref> (e.g., Madigan and York, 1995) </ref> and linear regression (e.g., Raftery et al., 1997), closed form integrals for the marginal likelihood, Equation (3), are available.
Reference: <author> Markus, B. H., Dickson, E. R., Grambsch, P. M., Fleming, T. R., Mazzaferro, V., Klintmalm, G., Weisner, R. H., Van Thiel, D. H., and Starzl, T. E. </author> <year> (1989). </year> <title> Efficacy of liver transplantation in patients with primary biliary cirrhosis. </title> <journal> New England Journal of Medicine, </journal> <volume> 320 </volume> <pages> 1709-1713. </pages>
Reference: <author> Matheson, J. E. and Winkler, R. L. </author> <year> (1976). </year> <title> Scoring rules for continuous probability distri 35 butions. </title> <journal> Management Science, </journal> <volume> 22 </volume> <pages> 1087-1096. </pages>
Reference: <author> McCullagh, P. and Nelder, J. </author> <year> (1989). </year> <title> Generalized Linear Models. </title> <publisher> Chapman & Hall, </publisher> <address> London, 2d edition. </address>
Reference-contexts: A faster but less exact implementation of BMA for variable selection in linear regression via the leaps-and-bound algorithm is available in the BICREG software (Section 4.5). 4.2 Generalized Linear Models Model-building for generalized linear models involves choosing the independent variables, the link function, and the variance function <ref> (McCullagh and Nelder, 1989) </ref>. Each possible combination of choices defines a different model. Raftery (1996) presents a series of methods for calculating approximate Bayes factors for generalized linear models.
Reference: <author> Miller, A. J. </author> <year> (1990). </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: The top 10 models by PMP account for 57% of the total posterior probability. We compare the Bayesian results with models that might be selected using standard techniques. We chose three popular variable selection techniques, Efroymson's stepwise method <ref> (Miller, 1990) </ref>, minimum Mallow's C p , and maximum adjusted R 2 (Weisberg, 1985). Efroymson's stepwise method is like forward selection except that when a new variable is added to the subset, partial correlations are considered to see if any of the variables currently in the subset should be dropped.
Reference: <author> Penrose, K., Nelson, A., and Fisher, A. </author> <year> (1985). </year> <title> Generalized body composition prediction equation for men using simple measurement techniques (abstract). Medicine and Science in Sports and Exercise, </title> <publisher> 17:189. </publisher>
Reference-contexts: To measure performance we split the complete data set into two subsets. We used the split of the data that was used by the original researchers for model building <ref> (Penrose et al., 1985) </ref>. The first 142 observations were used to do BMA and apply the model selection 27 Table 10: Body Fat Example: Performance Comparison. Predictive coverage % is the percentage of observations in the performance set that fall in the 90% prediction interval.
Reference: <author> Philips, D. B. and Smith, A. F. M. </author> <year> (1994). </year> <title> Bayesian model comparison via jump diffusions. </title> <type> Technical Report 94-20, </type> <institution> Imperial College, </institution> <address> London. </address>
Reference: <author> Raftery, A. E. </author> <year> (1993). </year> <title> Bayesian model selection in structural equation models. </title> <editor> In Bollen, K. and Long, J., editors, </editor> <title> Testing Structural Equation Models, number 163-180. </title> <publisher> Sage. </publisher>
Reference: <author> Raftery, A. E. </author> <year> (1995). </year> <title> Bayesian model selection in social research (with discussion). </title> <editor> In Mars-den, P. V., editor, </editor> <booktitle> Sociological Methodology 1995, </booktitle> <pages> pages 111-195. </pages> <publisher> Blackwells Publishers, </publisher> <address> Cambridge, Mass. </address>
Reference: <author> Raftery, A. E. </author> <year> (1996). </year> <title> Approximate Bayes factors and accounting for model uncertainty in generalised linear models. </title> <journal> Biometrika, </journal> <volume> 83 </volume> <pages> 251-266. </pages>
Reference: <author> Raftery, A. E., Madigan, D., and Hoeting, J. </author> <year> (1997). </year> <title> Bayesian model averaging for linear regression models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 92 </volume> <pages> 179-191. </pages>
Reference-contexts: For certain interesting classes of models such as discrete graphical models (e.g., Madigan and York, 1995) and linear regression <ref> (e.g., Raftery et al., 1997) </ref>, closed form integrals for the marginal likelihood, Equation (3), are available.
Reference: <author> Raftery, A. E., Madigan, D., and Volinsky, C. T. </author> <year> (1996). </year> <title> Accounting for model uncertainty in survival analysis improves predictive performance (with discussion). </title> <editor> In Bernardo, J., Berger, J., Dawid, A., and Smith, A., editors, </editor> <booktitle> Bayesian Statistics 5, </booktitle> <pages> pages 323-349. </pages> <publisher> Oxford University Press. </publisher>
Reference: <author> Rao, J. S. and Tibshirani, R. </author> <year> (1997). </year> <title> The out-of-bootstrap method for model averaging and selection. </title> <type> Technical report, </type> <institution> Department of Statistics, University of Toronto. </institution>
Reference: <author> Regal, R. and Hook, E. B. </author> <year> (1991). </year> <title> The effects of model selection on confidence intervals for the size of a closed population. </title> <journal> Statistical Medicine, </journal> <volume> 10 </volume> <pages> 717-721. </pages>
Reference: <author> Roberts, H. V. </author> <year> (1965). </year> <title> Probabilistic prediction. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 60 </volume> <pages> 50-62. </pages>
Reference: <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-46. </pages>
Reference: <author> Smith, A. F. M. and Roberts, G. O. </author> <year> (1993). </year> <title> Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods (with discussion). </title> <journal> Journal of the Royal Statistical Society (Ser. B), </journal> <volume> 55:3 - 23. </volume>
Reference-contexts: Applying standard Markov chain Monte Carlo results, ^ G ! E (g (M )) a:s: as N ! 1 <ref> (e.g., Smith and Roberts, 1993) </ref>. To compute (1) in this fashion set g (M ) = pr ( j M; D). To construct the Markov chain, define a neighborhood nbd (M ) for each M 2 M.
Reference: <author> Spiegelhalter, D. J. </author> <year> (1986). </year> <title> Probabilistic prediction in patient management and clinical trials. </title> <journal> Statistics in Medicine, </journal> <volume> 5 </volume> <pages> 421-433. </pages>
Reference: <author> Spiegelhalter, D. J., Dawid, A., Lauritzen, S., and Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems (with discussion). </title> <journal> Statistical Science, </journal> <volume> 8 </volume> <pages> 219-283. </pages>
Reference: <author> Spiegelhalter, D. J. and Lauritzen, S. </author> <year> (1990). </year> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 579-605. </pages>
Reference: <author> Stewart, L. </author> <year> (1987). </year> <title> Hierarchical Bayesian analysis using Monte Carlo integration: Computing posterior distributions when there are many possible models. </title> <journal> The Statistician, </journal> <volume> 36 </volume> <pages> 211-219. </pages>
Reference: <author> Stigler, S. M. </author> <year> (1973). </year> <title> Laplace, Fisher, and the discovery of the concept of sufficiency. </title> <journal> Biometrika, </journal> <volume> 60 </volume> <pages> 439-445. </pages>
Reference: <author> Taplin, R. H. </author> <year> (1993). </year> <title> Robust likelihood calculation for time series. </title> <journal> Journal of the Royal Statistical Society (Ser. B)., </journal> <volume> 55 </volume> <pages> 829-836. </pages> <note> 36 Thompson, </note> <author> E. A. and Wijsman, E. M. </author> <year> (1990). </year> <title> Monte Carlo methods for the genetic anal-ysis of complex traits. </title> <type> Technical Report 193, </type> <institution> Department of Statistics, University of Washington. </institution>
Reference: <author> Tierney, L. and Kadane, J. B. </author> <year> (1986). </year> <title> Accurate approximations for posterior moments and marginal densities. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 81 </volume> <pages> 82-86. </pages>
Reference-contexts: For certain interesting classes of models such as discrete graphical models (e.g., Madigan and York, 1995) and linear regression (e.g., Raftery et al., 1997), closed form integrals for the marginal likelihood, Equation (3), are available. The Laplace method <ref> (Tierney and Kadane, 1986) </ref> can provide an excellent approximation to pr (DjM k ); in certain circumstances this yields the very simple BIC approximation (Schwarz, 1978; Kass 6 and Wasserman, 1995; Raftery, 1995).
Reference: <author> Volinsky, C. T. </author> <year> (1997). </year> <title> Bayesian Model Averaging for Censored Survival Models. </title> <type> PhD thesis, </type> <institution> University of Washington. </institution>
Reference: <author> Volinsky, C. T., Madigan, D., Raftery, A. E., and Kronmal, R. A. </author> <year> (1997). </year> <title> Bayesian Model Averaging in proportional hazard models: Assessing the risk of a stroke. </title> <journal> Applied Statistics, </journal> <volume> 46(3). </volume>
Reference: <author> Weisberg, S. </author> <year> (1985). </year> <title> Applied Linear Regression. </title> <publisher> Wiley, </publisher> <address> New York, 2d edition. </address>
Reference-contexts: 7 hip -0.20 0.14 -1.44 0.15 X 8 thigh 0.24 0.14 1.74 0.08 X 9 knee -0.02 0.23 -0.09 0.93 X 10 ankle 0.17 0.21 0.81 0.42 X 11 biceps 0.16 0.16 0.98 0.33 X 12 forearm 0.43 0.18 2.32 0.02 X 13 wrist -1.47 0.50 -2.97 &lt;0.01 linear regression <ref> (Weisberg, 1985) </ref>. The standard approach to this analysis is to choose a single best subset of predictors using one of the many variable selection methods available. <p> We compare the Bayesian results with models that might be selected using standard techniques. We chose three popular variable selection techniques, Efroymson's stepwise method (Miller, 1990), minimum Mallow's C p , and maximum adjusted R 2 <ref> (Weisberg, 1985) </ref>. Efroymson's stepwise method is like forward selection except that when a new variable is added to the subset, partial correlations are considered to see if any of the variables currently in the subset should be dropped. Similar hybrid methods are found in most standard statistical computer packages. <p> For the stepwise procedure we used a 5% significance level which means that the significance levels for the F-to-enter and F-to-delete values were equal to 5%. Shortcomings of stepwise regression, Mallow's C p , and adjusted R 2 are well known <ref> (see, for example, Weisberg, 1985) </ref>. All three standard model selection methods selected the same eight predictor model (Table 8). There is clear agreement among the frequentist and BMA methods that the predictors abdomen circumference, weight, and wrist circumference are important predictors of percent body fat.
Reference: <author> Wolpert, D. H. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 241-259. </pages>
Reference-contexts: For example, Wolpert's stacked generalization classification method proceeds as follows: first, several classifiers are learned from the training data. The predictions made by these classifiers on the training data and the correct classifications form the training data for the next level classifier which provides the final classification <ref> (Wolpert, 1992) </ref>. Chan and Stolfo (1996) execute a number of processes on a number of data subsets in parallel to learn "base" classifiers, and then combine the collective results in a variety of different ways.
Reference: <author> York, J., Madigan, D., Heuch, I., and Lie, R. T. </author> <year> (1995). </year> <title> Estimating a proportion of birth defects by double sampling: a Bayesian approach incorporating covariates and model uncertainty. </title> <journal> Applied Statistics, </journal> <volume> 44 </volume> <pages> 227-242. 37 </pages>
Reference-contexts: VMRK use the "leaps and bounds" algorithm (Furnival and Wilson, 1974) to rapidly identify models to be used in the summation of Equation (1). The second approach, Markov chain Monte Carlo model composition (MC 3 ), uses a Markov chain Monte Carlo method to directly approximate (1) <ref> (Madigan and York, 1995) </ref>. This generates a stochastic process which moves through model space. Specifically, let M denote the space of models under consideration. <p> For certain interesting classes of models such as discrete graphical models <ref> (e.g., Madigan and York, 1995) </ref> and linear regression (e.g., Raftery et al., 1997), closed form integrals for the marginal likelihood, Equation (3), are available.
References-found: 95

