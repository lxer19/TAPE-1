URL: ftp://ftp.ics.hawaii.edu/pub/tr/ics-tr-95-08.ps.Z
Refering-URL: ftp://ftp.ics.hawaii.edu/pub/tr/INDEX.html
Root-URL: 
Email: dat@uhics.ics.hawaii.edu  
Title: Exploring The Effectiveness Of Formal Technical Review Factors With CSRS, A Collaborative Software Review System  
Author: Danu Tjahjono 
Date: June, 1996  
Address: 2565 The Mall Honolulu, HI 96822  
Affiliation: Department of Information and Computer Sciences University of Hawaii  
Pubnum: ICS-TR-95-08  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Frank Ackerman, Lynne S. Buchwald, and Frank H. Lewski. </author> <title> Software inspections: An effective verification process. </title> <journal> IEEE Software, </journal> <pages> pages 31-36, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: For example, some descriptions of Fagan's method explicitly advocate the use of paraphrasing for the inspection technique [52, 124, 34], while others disregard this technique [74]. Some advocate the use of common error checklists when examining source materials [74, 57], others also advocate the use of selective test cases <ref> [1, 43] </ref>. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection [74, 98, 142]. <p> Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion [127, 110]. Also, many review methods demand a separate preparation phase where reviewers learn source materials before starting error hunting <ref> [1, 74, 52] </ref>, while others consider it optional or unnecessary [92]. In conclusion, the wide variations of review practice, even within the same ostensible method, such as Fagan's method, indicate that the methods are defined ambiguously or unclear. <p> Myers and Parnas attribute the success of review to the synergy effect between the producer and the reviewers [106, 110], yet others discourage active participation of the producer during the meeting <ref> [1, 124] </ref>. Parnas and Weiss explicitly criticize the effectiveness of Fagan's inspection because it lacks specific roles/responsibilities and a systematic technique for individual reviewers to conduct the inspection. Thus, they consider these factors very important for a successful review [110]. <p> Thus, they consider these factors very important for a successful review [110]. Finally, some researchers attribute review effectiveness to paraphrasing, or the use of selective test cases, or checklists <ref> [124, 1, 43, 57] </ref>, or group interaction/composition [10, 97]. In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. <p> Like walkthroughs, the term Inspections is also used in the literature in many different ways. For example, many researchers associate inspections with paraphrasing techniques, in which a designated reader paraphrases the materials statement by statement <ref> [106, 3, 124, 1] </ref>, others downplay the role of reader [41, 101, 74, 60]. Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides [60]. <p> Some highlights from the inspection procedures described by Ackerman include a reader who is not the producer paraphrases and summarizes the materials being inspected, and the reader uses selected test cases to inspect the materials <ref> [1] </ref>. 19 * Hart reported a case study of design and code walkthroughs (i.e., Yourdon's Structured Walk-- throughs) at Sperry Univac during the development of PADS 1100 over a period of one and a half years. The code contained approximately 23K source statements. <p> is much cheaper than testing, and its detection efficiency is also relatively high (&gt;=75%). 21 Table 2.3: FTR experiences in industry References Organization Cost Efficiency Method Inspect Test Inspect Test Fagan76 [52] Aetna 82% 18% Fagan Fagan86 [53] IBM UK 93% Fagan Standard Bank &gt;50% Fagan AMEX &gt;50% Fagan Ackerman89 <ref> [1] </ref> Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 <p> Finally, most FTR experiences reported on Fagan style inspections. Only a few of them reported on non-Fagan inspections. One possible explanation is that Fagan inspection is often used as a competitive advantage in bidding for software contracts (as revealed by the Ackerman's survey <ref> [1] </ref>). Unfortunately, most of these published reports do not describe the actual inspection process performed, thus the organizations may or may not perform the real Fagan inspection. <p> In this case, the participants are expected to both fully understand the materials and find defects at the same time. Some researchers suggest that separating comprehension from examination activity makes a review process more effective and productive <ref> [52, 74, 1, 124] </ref>. For example, in the code inspection method, there is a phase called overview, where the producer educates participants about review materials, and preparation, where the reviewers learn the materials individually [52]. The stated objective of these phases is comprehension. <p> The stated objective of these phases is comprehension. As expressed by Ackerman, A half-hour or hour presentation by the producer can often save the inspectors two or three hours each in achieving the desired level of understanding of the inspection materials <ref> [1] </ref>. Fagan also stated that A team is most effective if it operates with only one objective at a time [52]. <p> Typical roles in FTR include moderator (who leads meetings), producer (who produces source materials), reviewer (who reviews source materials), reader (who paraphrases the materials) and recorder/scribe (who records the results) <ref> [52, 1] </ref>. Other domain specific roles include Designer (who checks for defects in the design), Tester (who checks the materials from the testing point of view) [52], and other specialists [110]. Many people consider review roles as an important review factor. <p> some FTR practices advocate the active participation of the producer during group meetings (i.e., have the producer play the role of presenter or reader) for self-debugging mechanism, others discourage the producer's participation to prevent them from brainwashing the reviewers into making the same erroneous assumptions about the product being reviewed <ref> [106, 1, 150] </ref>. Active Design Review [110] and Scenario based review method [115] advocate the use of specialists for an effective review [110] Synchronicity Synchronicity describes the physical interaction mode among participants. <p> Unfortunately, the literature does not provide a detailed description of these practices. Some highlights of review components used in industry include: * Ackerman's inspection uses paraphrasing with selective test cases for the group examination technique. The role of reader is given to a reviewer other than the producer <ref> [1] </ref>. * McKissick's inspection defines a number of specialists for review roles: system engineer, software architect, test engineer, and software quality assurance engineer. <p> 006: int numEmployees; 007: void insertion_sort (); 008:public: 009: Company1 (); 010: Company1 (); 011: int addEmployee (Employee* newWorker); 012: int deleteEmployee (char* SSN); 013: Employee* findEmployee (char* SSN); 014: void print (); 015:-; A.4 Employee::Employee Specification: Constructor to init Employee Source-code: 001:Employee::Employee () 002: 004: name = new char <ref> [1] </ref> = 0; //initialize name string to one space 165 005: socSecurity = new char [12];//initialize spaces for SSN 006: age = 17; // init age 007: numDependents = 0; // init number of dependents 008:- 010: A.5 Employee:: Employee Specification: Destructor for Employee Source-code: 001:Employee::Employee () 002: 004: delete [] <p> is initialized properly. */ 067:static REC_SYMTABTYPE SYMTAB [SYMTABLIMIT + 1]; 068: 069:static FILE *srcfile, *objfile, *lisfile, *intfile; 070:static int LOCCTR; 071:static CHAR6 PROGNAME; 072:static int PROGSTART; 073: 074:/* Error Messages */ 075:static CHAR50 ERRMSG [MAXERRORS] = - 076: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 077: "MISSING OPERATION CODE ",/*ERRMSG <ref> [1] </ref>*/ 078: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 188 079: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 080: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND <p> 055: i = 9; 056: if (isupper (source-&gt;line [i])) - /* THERE IS AN OPERATION CODE */ 057: while (i &lt;= 13 && (isupper (source-&gt;line [i]) || 058: isdigit (source-&gt;line [i]))) - 059: source-&gt;operation [i - 9] = source-&gt;line [i]; 060: i++; 062: else- 063: *errorsfound = true; 064: errorflags <ref> [1] </ref> = true; /* MISSING OPERATION CODE */ 065: - 067: for (j = i; j &lt;= 16; j++) - 068: if (source-&gt;line [j] != ' ') - 069: *errorsfound = true; 070: errorflags [2] = true; /* ILLEGAL OPERATION FIELD */ 071: - 073: 075: source-&gt;comment [i - 35] = <p> BY P2_ASSEMBLE_INST */ 100: 101:static BOOLEAN FIRSTSTMT, ENDFOUND; 102: 103:/* GLOBALS USED ONLY BY P2_WRITE_OBJ */ 104: 105:static int TEXTSTART, TEXTADDR, TEXTLENGTH; 106:static char TEXTARRAY [60]; 107: 108:/* ERROR MESSAGES */ 109:static CHAR50 ERRMSG [MAXERRORS] = - 110: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 111: "MISSING OPERATION CODE ",/*ERRMSG <ref> [1] </ref>*/ 112: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 113: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN
Reference: [2] <author> A. Frank Ackerman, Priscilla J. Fowler, and Robert G. Ebenau. </author> <title> Software Validation, </title> <booktitle> chap-ter Software Inspections and the Industrial Production of Software, </booktitle> <pages> pages 13-40. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1984. </year>
Reference-contexts: Many other studies, however, provide conflicting and/or anecdotal explanation of the causal factors underlying review effectiveness. For example, Basili attribute review effectiveness to a technique called stepwise abstraction [8, 7]. Dunn, Peele, Ackerman et al. attribute the success of software review to the presence of group synergy <ref> [43, 114, 2] </ref>, however, Humphrey reported that 75% of errors are found during individual preparation rather than in the group meeting [74]. Eick et al. also reported that 10% of errors are found during group meeting while 90% of errors are found during preparation [47]. <p> In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. For example, some studies have shown that in the inspection method, preparation time correlates positively with density of errors found <ref> [19, 2, 24] </ref>. However, as pointed out by Christenson et al. [24], the causal effects among these factors are not clear; one would think slower preparation would yield a higher number of errors found, but it may also be the presence of more errors that necessitates greater preparation time. <p> Ackerman also indicates that a potential of developing interpersonal tension in group inspections can be significant when inspections are initially introduced, but they tend to disappear as inspections become a routine procedure <ref> [2] </ref>. Collofello emphasizes the importance of developing a review culture to minimize personal stress [27]. <p> However, Dow and Murphy found comprehension (i.e., detailed product knowledge) is not a prerequisite for an effective inspection [42]. Similarly, some studies provide empirical results that relate preparation time to review outcomes <ref> [19, 2, 24, 74] </ref>. However, they generally do not specify whether reviewers should concentrate on simply learning the materials or on looking for errors during the preparation phase. In the same way, examination often implies consolidation. <p> Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy <ref> [43, 114, 2] </ref>. However, some review experiences/studies found that more errors are caught in individual process than group process [74, 142, 115, 47]. Many other review methods (namely, Fagan's code inspection and its variations) usually include some combination of individual and group processes. <p> EGSM will detect significantly more errors than EIAM. The rationale for this hypothesis is that the presence of group synergy (in EGSM) will lead to the discovery of more errors than in nominal groups (EIAM) as suggested by many FTR practices <ref> [43, 114, 2] </ref>. 2. H2: There will be significant differences in detection cost between EGSM and EIAM. EGSM will cost significantly more than EIAM. <p> + 1]; 068: 069:static FILE *srcfile, *objfile, *lisfile, *intfile; 070:static int LOCCTR; 071:static CHAR6 PROGNAME; 072:static int PROGSTART; 073: 074:/* Error Messages */ 075:static CHAR50 ERRMSG [MAXERRORS] = - 076: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 077: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 078: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG <ref> [2] </ref>*/ 188 079: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 080: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR <p> [i - 9] = source-&gt;line [i]; 060: i++; 062: else- 063: *errorsfound = true; 064: errorflags [1] = true; /* MISSING OPERATION CODE */ 065: - 067: for (j = i; j &lt;= 16; j++) - 068: if (source-&gt;line [j] != ' ') - 069: *errorsfound = true; 070: errorflags <ref> [2] </ref> = true; /* ILLEGAL OPERATION FIELD */ 071: - 073: 075: source-&gt;comment [i - 35] = source-&gt;line [i]; 076:- /* P1_Read_Source */ 077: 079: D.6 P1 Proc START Specification: Process start statement convert starting address and store in newlocctr. <p> 102: 103:/* GLOBALS USED ONLY BY P2_WRITE_OBJ */ 104: 105:static int TEXTSTART, TEXTADDR, TEXTLENGTH; 106:static char TEXTARRAY [60]; 107: 108:/* ERROR MESSAGES */ 109:static CHAR50 ERRMSG [MAXERRORS] = - 110: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 111: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 112: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG <ref> [2] </ref>*/ 113: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED <p> Set object type = TEXTREC. */ 018: 019: if (source.operand [0] = 'C') - 020: i = 1; 021: while (source.operand [i + 1] != QUOTE) - 022: asciival = ASCII [source.operand [i + 1]]; 023: numtohex (asciival, temp); 024: objct-&gt;objcode [i * 2 - 2] = temp <ref> [2] </ref>; 025: objct-&gt;objcode [i * 2 - 1] = temp [3]; 026: i++; 028: objct-&gt;objlength = i - 1; 029: objct-&gt;rectype = TEXTREC; 030: - 031: else - 032: /* Operand is X'...' -- Pack hex digits into object code and 033: set object type = TEXTREC. 034: Errors detected: 5 <p> 029: TEXTADDR = locctr; 030: TEXTSTART = locctr; 031: - 032: if (TEXTLENGTH + objct.objlength &gt; 60 && locctr != TEXTADDR) - 033: putc ('T', objfile); 034: numtohex (TEXTSTART, temp); 035: fprintf (objfile, "00%.4s", temp); 036: textbytes = TEXTLENGTH * 2; 037: numtohex (textbytes, temp); 038: fprintf (objfile, "%c%c", temp <ref> [2] </ref>, temp [3]); 039: forlim = TEXTLENGTH; 040: for (i = 0; i &lt; forlim; i++) 041: putc (TEXTARRAY [i], objfile); 042: putc ('"n', objfile); 043: TEXTLENGTH = 0; 044: TEXTSTART = locctr; 045: - 046: for (i = 0; i &lt;= objct.objlength; i++) 047: TEXTARRAY [TEXTLENGTH + i] = objct.objcode <p> record (if there is anything 054: in it) and then generate the end record */ 055: 056: if (TEXTLENGTH != 0) - 057: putc ('T', objfile); 058: numtohex (TEXTSTART, temp); 059: fprintf (objfile, "00%.4s", temp); 060: textbytes = TEXTLENGTH / 2; 061: numtohex (textbytes, temp); 062: fprintf (objfile, "%c%c", temp <ref> [2] </ref>, temp [3]); 063: forlim = TEXTLENGTH; 064: for (i = 0; i &lt; forlim; i++) 065: putc (TEXTARRAY [i], objfile); 066: putc ('"n', objfile); 067: - 068: putc ('E', objfile); 210 069: for (i = 0; i &lt; objct.objlength; i++) 070: putc (objct.objcode [i], objfile); 071: putc ('"n', objfile); 072:-
Reference: [3] <author> W. Richards Adrion, Martha A. Branstad, and John C. Cherniavsky. </author> <title> Validation, verification, and testing of computer software. </title> <journal> Computing Surveys, </journal> <volume> 14(2) </volume> <pages> 159-192, </pages> <year> 1982. </year>
Reference-contexts: al. described the main difference between inspection and walkthrough as follows: inspection involves a step-by-step reading of the product, with each step checked against a predetermined list of criteria (checklists), while walkthrough requires the producer or other participant to walk through the system manually with a set of test data <ref> [3] </ref>. Myers characterized walkthrough as playing computer in which the person designated as the tester comes to the meeting armed with a small set of test cases and mentally executes them [106]. <p> Programmers have used it since the time of the first program. The oldest practice of software review is called desk checking or code reading, which is, reading over a program by hand while sitting at one's desk <ref> [106, 3] </ref>. Desk checking is often viewed as ineffective and unproductive. One reason is that it is a completely undisciplined process. Another reason is that people are usually ineffective in catching their own mistakes. <p> Some refer it as structured walkthroughs [150]. Generally, the literature describes walkthrough as an undisciplined process without advanced preparation on the part of reviewers, and the meeting focuses on education purposes [27, 52]. However, many authors consider it a disciplined and formal review process <ref> [106, 150, 3] </ref>. 3. Round-robin reviews. <p> Like walkthroughs, the term Inspections is also used in the literature in many different ways. For example, many researchers associate inspections with paraphrasing techniques, in which a designated reader paraphrases the materials statement by statement <ref> [106, 3, 124, 1] </ref>, others downplay the role of reader [41, 101, 74, 60]. Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides [60]. <p> Boehm also stated that structured walkthrough or software inspection has been the most cost-effective technique to date (1987) for eliminating software errors [13]. Adrion et al. indicated that disciplined manual techniques, such as walkthroughs, reviews and inspections, have been the most successful V&V techniques <ref> [3] </ref>. The reason for this success is due to errors removal within the first stages of the development (requirement and design). <p> In general, V&V techniques can be classified into two categories: static analysis and dynamic 14 analysis (or testing). Static analysis can be further classified into manual and automated static analysis <ref> [3, 12, 11] </ref>. FTR is a manual static analysis technique. A similar technique is formal mathematical proofs. Some people consider FTR informal compared to mathematical proofs, however, some FTR methods are combined with mathematical proofs resulting in an even more effective review process. <p> Thus, one may have different techniques for individual examinations, group examination, individual comprehension, group comprehension, and so forth. Some examples of examination techniques include the paraphrasing technique used during the group meeting in Fagan's inspection [52], selective test cases (test data) <ref> [106, 3] </ref>, checklists of common errors (i.e., classes of software errors) used during the individual examination phase in Humphrey's method [74], active checklist used during individual review phase in Active Design Review [110], scenarios used in Porter and Votta's experiment [115], the stepwise abstraction technique used during the individual examination phase <p> LOCCTR; 071:static CHAR6 PROGNAME; 072:static int PROGSTART; 073: 074:/* Error Messages */ 075:static CHAR50 ERRMSG [MAXERRORS] = - 076: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 077: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 078: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 188 079: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG <ref> [3] </ref>*/ 080: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN <p> Errors detected: 3, 4 Source-code: 001:void P1_Proc_START (SOURCETYPE source, 002: int *newlocctr, 194 003: BOOLEAN *errorsfound, 004: BOOLEAN *errorflags) 005: 007: BOOLEAN converror; 008: int i, j, temploc; 009: 010: if (source.operand [0] == ' ') - 011: *errorsfound = true; 012: errorflags <ref> [3] </ref> = true; 013: /* MISSING OR MISPLACED OPERAND IN START STATEMENT */ 014: return; 015: - 016: temploc = hextonum (source.operand, 0, &i, &converror); 017: if (converror) - 018: *errorsfound = true; 019: errorflags [4] = true; /* ILLEGAL OPERAND IN START STATEMENT */ 020: - 022: if (source.operand [j] <p> (converror) - 018: *errorsfound = true; 019: errorflags [4] = true; /* ILLEGAL OPERAND IN START STATEMENT */ 020: - 022: if (source.operand [j] != ' ') - 023: *errorsfound = true; 024: errorflags [4] = true; 025: /* ILLEGAL OPERAND IN START STATEMENT */ 026: - 028: if (!errorflags <ref> [3] </ref> && !errorflags [4]) 029: *newlocctr = temploc; 030:- 032: D.7 P1 Proc RESW Specification: Process resw statement add the number of words reserved to locctr. <p> int TEXTSTART, TEXTADDR, TEXTLENGTH; 106:static char TEXTARRAY [60]; 107: 108:/* ERROR MESSAGES */ 109:static CHAR50 ERRMSG [MAXERRORS] = - 110: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 111: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 112: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 113: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG <ref> [3] </ref>*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN <p> if (source.operand [0] = 'C') - 020: i = 1; 021: while (source.operand [i + 1] != QUOTE) - 022: asciival = ASCII [source.operand [i + 1]]; 023: numtohex (asciival, temp); 024: objct-&gt;objcode [i * 2 - 2] = temp [2]; 025: objct-&gt;objcode [i * 2 - 1] = temp <ref> [3] </ref>; 026: i++; 028: objct-&gt;objlength = i - 1; 029: objct-&gt;rectype = TEXTREC; 030: - 031: else - 032: /* Operand is X'...' -- Pack hex digits into object code and 033: set object type = TEXTREC. 034: Errors detected: 5 */ 035: while (source.operand [i + 1] != QUOTE) - <p> = locctr; 030: TEXTSTART = locctr; 031: - 032: if (TEXTLENGTH + objct.objlength &gt; 60 && locctr != TEXTADDR) - 033: putc ('T', objfile); 034: numtohex (TEXTSTART, temp); 035: fprintf (objfile, "00%.4s", temp); 036: textbytes = TEXTLENGTH * 2; 037: numtohex (textbytes, temp); 038: fprintf (objfile, "%c%c", temp [2], temp <ref> [3] </ref>); 039: forlim = TEXTLENGTH; 040: for (i = 0; i &lt; forlim; i++) 041: putc (TEXTARRAY [i], objfile); 042: putc ('"n', objfile); 043: TEXTLENGTH = 0; 044: TEXTSTART = locctr; 045: - 046: for (i = 0; i &lt;= objct.objlength; i++) 047: TEXTARRAY [TEXTLENGTH + i] = objct.objcode [i]; 048: <p> there is anything 054: in it) and then generate the end record */ 055: 056: if (TEXTLENGTH != 0) - 057: putc ('T', objfile); 058: numtohex (TEXTSTART, temp); 059: fprintf (objfile, "00%.4s", temp); 060: textbytes = TEXTLENGTH / 2; 061: numtohex (textbytes, temp); 062: fprintf (objfile, "%c%c", temp [2], temp <ref> [3] </ref>); 063: forlim = TEXTLENGTH; 064: for (i = 0; i &lt; forlim; i++) 065: putc (TEXTARRAY [i], objfile); 066: putc ('"n', objfile); 067: - 068: putc ('E', objfile); 210 069: for (i = 0; i &lt; objct.objlength; i++) 070: putc (objct.objcode [i], objfile); 071: putc ('"n', objfile); 072:- 074: E.9
Reference: [4] <author> Lowell Jay Arthur. </author> <title> Improving Software Quality. </title> <publisher> Wiley Professional Computing, </publisher> <year> 1993. </year>
Reference-contexts: Many recent mishaps, from the loss of business to the loss of human life, can be attributed to quality problems in software products. For example, in 1992, a tiny software bug in the 4ESS switching system crashed a substantial portion of AT&T's telecommunication network <ref> [4] </ref>. In 1989, a UK bank accidently transferred $2 billion to US and UK companies because a software design flaw allowed payment instructions to be duplicated [26]. In 1986, a medical system for radiation therapy killed two patients due to improper design and implementation of its software. <p> Size of array of error flags */ 027:#define MAXOPS 25 /* Size of opcode table */ 187 028:#define SYMTABLIMIT 100 /* Size of symbol table */ 029:#define BLANK6 " " 030:#define BLANK8 " " 031:#define BLANK18 " " 032:#define BLANK31 " " 033: 034:typedef unsigned char BOOLEAN; 035:typedef char CHAR4 <ref> [4] </ref>; 036:typedef char CHAR6 [6]; 037:typedef char CHAR8 [8]; 038:typedef char CHAR18 [18]; 039:typedef char CHAR31 [31]; 040:typedef char CHAR50 [50]; 041:typedef char CHAR66 [66]; 042: 043:typedef struct SOURCETYPE - 044: /* source line and subfields */ 045: CHAR66 line; 046: BOOLEAN comline; 047: CHAR8 labl; 048: CHAR6 operation; 049: CHAR18 <p> 074:/* Error Messages */ 075:static CHAR50 ERRMSG [MAXERRORS] = - 076: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 077: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 078: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 188 079: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 080: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG <ref> [4] </ref>*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED <p> 009: 010: if (source.operand [0] == ' ') - 011: *errorsfound = true; 012: errorflags [3] = true; 013: /* MISSING OR MISPLACED OPERAND IN START STATEMENT */ 014: return; 015: - 016: temploc = hextonum (source.operand, 0, &i, &converror); 017: if (converror) - 018: *errorsfound = true; 019: errorflags <ref> [4] </ref> = true; /* ILLEGAL OPERAND IN START STATEMENT */ 020: - 022: if (source.operand [j] != ' ') - 023: *errorsfound = true; 024: errorflags [4] = true; 025: /* ILLEGAL OPERAND IN START STATEMENT */ 026: - 028: if (!errorflags [3] && !errorflags [4]) 029: *newlocctr = temploc; 030:- <p> STATEMENT */ 014: return; 015: - 016: temploc = hextonum (source.operand, 0, &i, &converror); 017: if (converror) - 018: *errorsfound = true; 019: errorflags <ref> [4] </ref> = true; /* ILLEGAL OPERAND IN START STATEMENT */ 020: - 022: if (source.operand [j] != ' ') - 023: *errorsfound = true; 024: errorflags [4] = true; 025: /* ILLEGAL OPERAND IN START STATEMENT */ 026: - 028: if (!errorflags [3] && !errorflags [4]) 029: *newlocctr = temploc; 030:- 032: D.7 P1 Proc RESW Specification: Process resw statement add the number of words reserved to locctr. <p> *errorsfound = true; 019: errorflags <ref> [4] </ref> = true; /* ILLEGAL OPERAND IN START STATEMENT */ 020: - 022: if (source.operand [j] != ' ') - 023: *errorsfound = true; 024: errorflags [4] = true; 025: /* ILLEGAL OPERAND IN START STATEMENT */ 026: - 028: if (!errorflags [3] && !errorflags [4]) 029: *newlocctr = temploc; 030:- 032: D.7 P1 Proc RESW Specification: Process resw statement add the number of words reserved to locctr. <p> 028:#define MAXERRORS 25 /* SIZE OF ARRAY OF ERROR FLAGS */ 029:#define MAXOPS 25 /* SIZE OF OPCODE TABLE */ 030: 031:#define BLANK6 " " 032:#define BLANK8 " " 033:#define BLANK18 " " 034:#define BLANK30 " " 035:#define BLANK31 " " 036: 037:typedef unsigned char BOOLEAN; 038: 039:typedef char CHAR4 <ref> [4] </ref>; 040:typedef char CHAR6 [6]; 041:typedef char CHAR8 [8]; 042:typedef char CHAR18 [18]; 043:typedef char CHAR30 [30]; 044:typedef char CHAR31 [31]; 045:typedef char CHAR50 [50]; 046:typedef char CHAR66 [66]; 047: 048:typedef struct SOURCETYPE - 049: /* SOURCE LINE AND SUBFIELDS */ 050: CHAR66 line; 051: BOOLEAN comline; 052: CHAR8 labl; 053: <p> 107: 108:/* ERROR MESSAGES */ 109:static CHAR50 ERRMSG [MAXERRORS] = - 110: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 111: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 112: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 113: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG <ref> [4] </ref>*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED
Reference: [5] <author> Emanual R. Baker. </author> <title> Total Quality Management For Software, </title> <booktitle> chapter TQM in Mission Critical Software Development, </booktitle> <pages> pages 3-36. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: The quality of the products stem, in large part, from the quality of the process used to create them [74].This principle is applied by the Department of Defense (DoD) under the name of Total Quality Management (TQM) <ref> [5] </ref>, and Software Engineering 16 Institute (SEI) under the name of Capability Maturity Model or Software Process Maturity [113, 74]. The underlying concepts of managing and improving software process can be traced back to Dr. W. <p> = - 076: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 077: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 078: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 188 079: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 080: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG <ref> [5] </ref>*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL <p> [MAXERRORS] = - 110: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 111: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 112: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 113: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG <ref> [5] </ref>*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL <p> Source-code: 001:void P2_Proc_BYTE (SOURCETYPE source, 002: int LOCCTR, 003: BOOLEAN *errorsfound, 004: BOOLEAN *errorflags, 005: OBJTYPE *objct) 006: 008: 010: CHAR4 temp; 011: char hexchar; 012: int asciival; 013: 014: if (!errorflags <ref> [5] </ref> && !errorflags [6] && !errorflags [7]) - 015: /* Operand is C'...' -- Use the ascii conversion table to 016: find the ascii code for each character and pack into 017: object code. <p> TEXTREC. 034: Errors detected: 5 */ 035: while (source.operand [i + 1] != QUOTE) - 036: hexchar = source.operand [i + 1]; 037: if (isdigit (hexchar) || (hexchar &gt;= 'A' && hexchar &lt;= 'F')) 038: objct-&gt;objcode [i - 1] = hexchar; 039: else - 040: *errorsfound = true; 041: errorflags <ref> [5] </ref> = true; /* ILLEGAL OPERAND IN BYTE */ 207 042: - 044: - 045: objct-&gt;objlength = i - 1; 046: objct-&gt;rectype = TEXTREC; 047: -/*else*/ 048: - 050:- 052: E.7 P2 Assemble Inst Specification: This procedure generates the object code (if any) for the source statement currently being processed.
Reference: [6] <author> Jack Barnard and Art Price. </author> <title> Managing code inspection information. </title> <journal> IEEE Software, </journal> <volume> 11(2):5969, </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: Barnard identified nine metrics to measure the inspection performance: total LOC of source materials inspected, average LOC inspected, average preparation rate, average inspection rate, average effort per KLOC, average effort per fault detected, average faults detected per KLOC, percentage of reinspections, and defect-removal efficiency <ref> [6] </ref>. Many studies have found a correlation between the effectiveness of inspections and certain inspection metrics. For example, the number of defects found decline with increasing inspection or preparation rates [53, 74, 24], with the optimum inspection rate is around 150 LOC/hour [124]. <p> Typical inspection metrics used to assess the effectiveness and productivity of the process itself are defect-removal efficiency (the percentage of faults removed by inspection compared with total reported faults), and detection costs (average effort per fault detected) <ref> [6] </ref>. In conclusion, inspection data can be used to improve the quality of the products under inspection, and the quality of the inspection process itself. Another aspect of software inspection that implements continuous process improvement is defect causal analysis [18]. <p> Chapter 4 will describe the design and implementation of such a system. 3.4 Work related to the FTR framework Existing theoretical work on FTR focuses primarily on modeling certain parameters of Fagan's inspection process in order to measure and predict its effectiveness <ref> [24, 6, 74] </ref>. For example, Christenson proposes an inspection model that relates input parameters, such as the preparation effort and inspection rate to the resulting fraction of errors found in the code [23, 24]. Using such a model, the effectiveness of inspections can be estimated and improvement can be made. <p> error flags */ 027:#define MAXOPS 25 /* Size of opcode table */ 187 028:#define SYMTABLIMIT 100 /* Size of symbol table */ 029:#define BLANK6 " " 030:#define BLANK8 " " 031:#define BLANK18 " " 032:#define BLANK31 " " 033: 034:typedef unsigned char BOOLEAN; 035:typedef char CHAR4 [4]; 036:typedef char CHAR6 <ref> [6] </ref>; 037:typedef char CHAR8 [8]; 038:typedef char CHAR18 [18]; 039:typedef char CHAR31 [31]; 040:typedef char CHAR50 [50]; 041:typedef char CHAR66 [66]; 042: 043:typedef struct SOURCETYPE - 044: /* source line and subfields */ 045: CHAR66 line; 046: BOOLEAN comline; 047: CHAR8 labl; 048: CHAR6 operation; 049: CHAR18 operand; 050: CHAR31 comment; <p> 077: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 078: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 188 079: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 080: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG <ref> [6] </ref>*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION <p> SIZE OF ARRAY OF ERROR FLAGS */ 029:#define MAXOPS 25 /* SIZE OF OPCODE TABLE */ 030: 031:#define BLANK6 " " 032:#define BLANK8 " " 033:#define BLANK18 " " 034:#define BLANK30 " " 035:#define BLANK31 " " 036: 037:typedef unsigned char BOOLEAN; 038: 039:typedef char CHAR4 [4]; 040:typedef char CHAR6 <ref> [6] </ref>; 041:typedef char CHAR8 [8]; 042:typedef char CHAR18 [18]; 043:typedef char CHAR30 [30]; 044:typedef char CHAR31 [31]; 045:typedef char CHAR50 [50]; 046:typedef char CHAR66 [66]; 047: 048:typedef struct SOURCETYPE - 049: /* SOURCE LINE AND SUBFIELDS */ 050: CHAR66 line; 051: BOOLEAN comline; 052: CHAR8 labl; 053: CHAR6 operation; 054: CHAR18 <p> [0]*/ 111: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 112: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 113: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG <ref> [6] </ref>*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION <p> Source-code: 001:void P2_Proc_BYTE (SOURCETYPE source, 002: int LOCCTR, 003: BOOLEAN *errorsfound, 004: BOOLEAN *errorflags, 005: OBJTYPE *objct) 006: 008: 010: CHAR4 temp; 011: char hexchar; 012: int asciival; 013: 014: if (!errorflags [5] && !errorflags <ref> [6] </ref> && !errorflags [7]) - 015: /* Operand is C'...' -- Use the ascii conversion table to 016: find the ascii code for each character and pack into 017: object code.
Reference: [7] <author> V. R. Basili and R. Selby. </author> <title> Comparing the effectiveness of software testing strategies. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> December </month> <year> 1987. </year>
Reference-contexts: Many other studies, however, provide conflicting and/or anecdotal explanation of the causal factors underlying review effectiveness. For example, Basili attribute review effectiveness to a technique called stepwise abstraction <ref> [8, 7] </ref>. Dunn, Peele, Ackerman et al. attribute the success of software review to the presence of group synergy [43, 114, 2], however, Humphrey reported that 75% of errors are found during individual preparation rather than in the group meeting [74]. <p> Myers used a combination of mentally executing test-cases and checking the logic 15 for common errors [105]. Basili & Selby used stepwise abstraction, in which the prime subprograms are identified, their individual functions are derived and compared against the specifications <ref> [7] </ref>. Collofello & Woodfield did not specify the processes used in reviews and testing. Overall, the findings suggest that reviews are more effective/efficient than testing at detecting errors. The published data from the software industry also supports these findings (see also Table 2.3). <p> Table 2.2: Research findings on Review v.s. Test References Detection Efficiency Cost Effectiveness Hetzel76 [70] code reading (37.3%) &lt; functional testing (47.7%) = structural testing (46.7%) Myers78 [105] code walkthrough (38%) = code walkthrough functional testing (30%) = most expensive structural testing (36%) Basili87 <ref> [7] </ref> code reading &gt; (16% more) code reading functional testing &gt; (11% more) least expensive structural testing Collofello89 [28] code review (64%)&gt; design review &gt; design review (54%) &gt; code review &gt; testing (38%) testing 2.4 FTR as a process management technique One typical criticism of traditional software development is its <p> However, the review methods in these studies investigate different components of the methods. No studies have investigated the interaction components of review methods. For example, Hetzel's, Basili's and Porter's studies <ref> [70, 7] </ref> investigated the examination technique components of review methods. In both Hetzel's and Basili's method, the examination technique (code reading) was compared to testing; no group interaction was involved. In Porter's study, three different examination (detection) techniques were compared: Ad Hoc, Checklist and Scenario [115]. <p> OPERATION FIELD ",/*ERRMSG [2]*/ 188 079: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 080: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG <ref> [7] </ref>*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG <p> IN OPERATION FIELD ",/*ERRMSG [2]*/ 113: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG <ref> [7] </ref>*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG <p> Source-code: 001:void P2_Proc_BYTE (SOURCETYPE source, 002: int LOCCTR, 003: BOOLEAN *errorsfound, 004: BOOLEAN *errorflags, 005: OBJTYPE *objct) 006: 008: 010: CHAR4 temp; 011: char hexchar; 012: int asciival; 013: 014: if (!errorflags [5] && !errorflags [6] && !errorflags <ref> [7] </ref>) - 015: /* Operand is C'...' -- Use the ascii conversion table to 016: find the ascii code for each character and pack into 017: object code.
Reference: [8] <author> Victor R. Basili and R. W. Selby. </author> <title> Comparing the effectiveness of software testing strategies. </title> <type> Technical Report TR-1501, </type> <institution> University of Maryland at College Park, Department of Computer Science, </institution> <year> 1985. </year>
Reference-contexts: Many other studies, however, provide conflicting and/or anecdotal explanation of the causal factors underlying review effectiveness. For example, Basili attribute review effectiveness to a technique called stepwise abstraction <ref> [8, 7] </ref>. Dunn, Peele, Ackerman et al. attribute the success of software review to the presence of group synergy [43, 114, 2], however, Humphrey reported that 75% of errors are found during individual preparation rather than in the group meeting [74]. <p> common errors (i.e., classes of software errors) used during the individual examination phase in Humphrey's method [74], active checklist used during individual review phase in Active Design Review [110], scenarios used in Porter and Votta's experiment [115], the stepwise abstraction technique used during the individual examination phase in Basili's method <ref> [8] </ref>, and statistical process control in Humphrey's Personal Reviews [76]. Many review practices, however, do not use any techniques for examination. They rely only on reviewers' intuition and experience to find errors. This dissertation refers to it as free technique; Porter et al. term it Ad Hoc detection method [115]. <p> MAXOPS 25 /* Size of opcode table */ 187 028:#define SYMTABLIMIT 100 /* Size of symbol table */ 029:#define BLANK6 " " 030:#define BLANK8 " " 031:#define BLANK18 " " 032:#define BLANK31 " " 033: 034:typedef unsigned char BOOLEAN; 035:typedef char CHAR4 [4]; 036:typedef char CHAR6 [6]; 037:typedef char CHAR8 <ref> [8] </ref>; 038:typedef char CHAR18 [18]; 039:typedef char CHAR31 [31]; 040:typedef char CHAR50 [50]; 041:typedef char CHAR66 [66]; 042: 043:typedef struct SOURCETYPE - 044: /* source line and subfields */ 045: CHAR66 line; 046: BOOLEAN comline; 047: CHAR8 labl; 048: CHAR6 operation; 049: CHAR18 operand; 050: CHAR31 comment; 051:- SOURCETYPE; 052: 053:typedef <p> IN START STATEMENT ",/*ERRMSG [3]*/ 080: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG <ref> [8] </ref>*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: <p> Errors detected: 8, 9 Source-code: 001:void P1_Proc_RESW (SOURCETYPE source, 002: int locctr, 003: int *newlocctr, 004: BOOLEAN *errorsfound, 005: BOOLEAN *errorflags) 006: 008:- 009: BOOLEAN converror; 010: int i, j, nwords; 011: 012: if (source.operand [0] == ' ') - 013: *errorsfound = true; 014: errorflags <ref> [8] </ref> = true; 015: /* MISSING OR MISPLACED OPERAND IN RESW STATEMENT */ 016: return; 017: - 018: nwords = dectonum (source.operand, 0, &i, &converror); 019: if (converror) - 020: *errorsfound = true; 021: errorflags [9] = true; /* ILLEGAL OPERAND IN RESW */ 022: - 024: if (source.operand [j] != <p> ERROR FLAGS */ 029:#define MAXOPS 25 /* SIZE OF OPCODE TABLE */ 030: 031:#define BLANK6 " " 032:#define BLANK8 " " 033:#define BLANK18 " " 034:#define BLANK30 " " 035:#define BLANK31 " " 036: 037:typedef unsigned char BOOLEAN; 038: 039:typedef char CHAR4 [4]; 040:typedef char CHAR6 [6]; 041:typedef char CHAR8 <ref> [8] </ref>; 042:typedef char CHAR18 [18]; 043:typedef char CHAR30 [30]; 044:typedef char CHAR31 [31]; 045:typedef char CHAR50 [50]; 046:typedef char CHAR66 [66]; 047: 048:typedef struct SOURCETYPE - 049: /* SOURCE LINE AND SUBFIELDS */ 050: CHAR66 line; 051: BOOLEAN comline; 052: CHAR8 labl; 053: CHAR6 operation; 054: CHAR18 operand; 055: CHAR31 comment; <p> IN START STATEMENT ",/*ERRMSG [3]*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG [4]*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG <ref> [8] </ref>*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125:
Reference: [9] <author> Tim Berners-Lee, Robert Cailliau, Ari Luotonen, Hennik Nielson, and Arthur Secrit. </author> <title> The world wide web. </title> <journal> Communications of the ACM, </journal> <volume> 37(8), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: Finally, hypertext systems generally do not support roles; all users have the same access privileges to all nodes and links. WWW World Wide Web <ref> [9] </ref> is the most widely accessed hypertext/hypermedia network. Unlike typical hypertext networks that serve on specific platforms, the WWW network supports an open-protocol architecture, that is, any WWW client/browser may access any WWW server in the network as long as they follow the standard protocols of the Web. <p> IN START STATEMENT ",/*ERRMSG [4]*/ 081: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG <ref> [9] </ref>*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: <p> 011: 012: if (source.operand [0] == ' ') - 013: *errorsfound = true; 014: errorflags [8] = true; 015: /* MISSING OR MISPLACED OPERAND IN RESW STATEMENT */ 016: return; 017: - 018: nwords = dectonum (source.operand, 0, &i, &converror); 019: if (converror) - 020: *errorsfound = true; 021: errorflags <ref> [9] </ref> = true; /* ILLEGAL OPERAND IN RESW */ 022: - 024: if (source.operand [j] != ' ') - 025: *errorsfound = true; 026: errorflags [9] = true; /* ILLEGAL OPERAND IN RESW */ 027: - 029: if (!errorflags [9]) 030: *newlocctr = locctr + nwords; 031:- 033: D.8 P1 Assign <p> RESW STATEMENT */ 016: return; 017: - 018: nwords = dectonum (source.operand, 0, &i, &converror); 019: if (converror) - 020: *errorsfound = true; 021: errorflags <ref> [9] </ref> = true; /* ILLEGAL OPERAND IN RESW */ 022: - 024: if (source.operand [j] != ' ') - 025: *errorsfound = true; 026: errorflags [9] = true; /* ILLEGAL OPERAND IN RESW */ 027: - 029: if (!errorflags [9]) 030: *newlocctr = locctr + nwords; 031:- 033: D.8 P1 Assign Loc Specification: This procedure updates the location counter value based on the type of statement being processed, placing the updated value in newlocctr. <p> &converror); 019: if (converror) - 020: *errorsfound = true; 021: errorflags <ref> [9] </ref> = true; /* ILLEGAL OPERAND IN RESW */ 022: - 024: if (source.operand [j] != ' ') - 025: *errorsfound = true; 026: errorflags [9] = true; /* ILLEGAL OPERAND IN RESW */ 027: - 029: if (!errorflags [9]) 030: *newlocctr = locctr + nwords; 031:- 033: D.8 P1 Assign Loc Specification: This procedure updates the location counter value based on the type of statement being processed, placing the updated value in newlocctr. <p> IN START STATEMENT ",/*ERRMSG [4]*/ 115: "ILLEGAL OPERAND IN BYTE STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG <ref> [9] </ref>*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126:
Reference: [10] <author> David B. Bisant and James R. Lyle. </author> <title> A two-person inspection method to improve programming productivity. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(10) </volume> <pages> 1294-1304, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: FTR also has the potential to improve the quality and productivity of software personnel. Using an experimental study, Bisant and Lyle showed that the groups who used a two-person FTR method improved their programming speed significantly <ref> [10] </ref>. Finally, FTR also plays a key role in defect prevention and process improvement [60]. The results of reviews can be used to institute continuous process improvement of the entire software development process. <p> Furthermore, both inspection and walkthrough involve a reader, but the reader for inspection must be someone other than the author; and, unlike walkthrough participants, inspection participants are provided with error checklists <ref> [10] </ref>. Fagan described the differences between inspection and walkthrough as a difference in their respective phases. First, walkthrough does not include an overview phase. Second, both inspection and walkthrough have a preparation phase followed by a meeting phase. <p> In fact, an earlier study by Zelkowitz found that 84% of the companies they surveyed agreed that reviews worked, but the ways reviews were conducted differed greatly <ref> [151, 10] </ref>. 4 1.3.2 Attribution of review outcomes to different review factors One reason why review methods are defined in so many ways is because the contribution of various review factors on review outcomes is not currently understood. <p> Thus, they consider these factors very important for a successful review [110]. Finally, some researchers attribute review effectiveness to paraphrasing, or the use of selective test cases, or checklists [124, 1, 43, 57], or group interaction/composition <ref> [10, 97] </ref>. In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. For example, some studies have shown that in the inspection method, preparation time correlates positively with density of errors found [19, 2, 24]. <p> Finally, the principle of continuous process improvement must also occur at a personal level. Bisant and Lyle's study has shown that software reviews can improve programmers' performance significantly <ref> [10] </ref>. The most significant development for self-improvement initiatives is Humphrey's Personal Software Process (PSP) [76]. The PSP takes a maturity framework developed for assessing and improving the development process of software organizations and applies it to individual developers. <p> In practice, this is also reflected in one survey that found that while 84% of organizations claimed to perform inspection, none performed it exactly as specified by Fagan <ref> [10, 60] </ref>. In conclusion, the most cited problems of FTR in the literature include high cost and confusion about the review procedures. The former problem is usually justified through analysis demonstrating that this investment is more than recouped through decreases in downstream rework. <p> One inspection study cited by Bisant and Lyle, however, found no difference in inspection performance between 3, 4, or 5-person inspection teams <ref> [10] </ref>, Compared to industrial data in Table 2.3, where the detection efficiency is mostly above 75%, the data in Table 2.4 generally shows smaller values. This number is also smaller than Boehm's claim that structured walkthroughs or inspections catch 60% of the errors [13]. <p> C:individual C:individual C:group S:ASYNC S:ASYNC S:ASYNC S:STSP R:clerical staff R:inspector R:inspector R:? T:checklist T:? T:passive&active T:Delphi E:? checklist X:check-marked X:? X:check-marked X:? 32 Table 3.3: Models of major FTR methods (cont.) Method Phase 1 Phase 2 Phase 3 Phase 4 Phase 5 Two-person Inspect1: Inspect2: Inspection O: examination O: examination <ref> [10] </ref> C: group C: group S: STSP S: STSP R: Reader2 R: Reader1 T: paraphrasing T: paraphrasing X: time X: time N-Fold Individual Meeting Consolidation Inspection O:examination O:examination O:consolidation [97] C: individual C:subgroup C:subgroup S:ASYNC S:STSP,ASYNC S:STSP,ASYNC R:reviewer R:? R:moderator T: checklist T:? T:? E/X: ? E/X: ? E/X:? Gilb's Indiv. <p> The multiple-inspector phase, however, is always carried out following the single-inspector phase. 3.2.8 Two-person Inspection Two-person inspection has been shown to improve programming productivity <ref> [10] </ref>. This method involves two programmers that review each other's design and code. This method was used as part of a controlled experiment, in which the subjects were given the same assignments. The review process consists of two phases. In the first phase, programmer-1's design or code is reviewed. <p> Martin's and Schneider's studies [97, 125] found that replicating the review group N fold can significantly improve review performance. Their findings are also substantiated by this experiment (see section 7.6.2). The effect of group size observed in this study seems to contradict the Buck's study cited in <ref> [10] </ref>. This study found that review performance would increase with a larger group size, whereas the latter study found no difference in performance between 3, 4, or 5-person inspection teams. <p> As described in the earlier chapters, the current state of FTR is characterized by ambiguities in its practice. Many practitioners believe that all FTR methods are basically the same <ref> [10, 151] </ref>. However, others do not agree. They usually argue that non-Fagan methods are ineffective, because they do 160 not focus on defect finding, do not use paraphrasing, do not have a trained moderator, do not have checklists, and so forth. <p> STATEMENT ",/*ERRMSG [5]*/ 082: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG <ref> [10] </ref>*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR <p> STATEMENT ",/*ERRMSG [5]*/ 116: "ODD LENGTH HEX STRING IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG <ref> [10] </ref>*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR
Reference: [11] <author> Bruce I. Blum. </author> <title> Software Engineering, A Holistic View. </title> <publisher> Oxford University Press, </publisher> <year> 1992. </year>
Reference-contexts: In general, V&V techniques can be classified into two categories: static analysis and dynamic 14 analysis (or testing). Static analysis can be further classified into manual and automated static analysis <ref> [3, 12, 11] </ref>. FTR is a manual static analysis technique. A similar technique is formal mathematical proofs. Some people consider FTR informal compared to mathematical proofs, however, some FTR methods are combined with mathematical proofs resulting in an even more effective review process. <p> IN BYTE STATEMENT ",/*ERRMSG [6]*/ 083: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG <ref> [11] </ref>*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: <p> IN BYTE STATEMENT ",/*ERRMSG [6]*/ 117: "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG <ref> [11] </ref>*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128:
Reference: [12] <author> B. Boehm. </author> <title> Verifying and validating software requirements and design specifications. </title> <journal> IEEE Software, </journal> <pages> pages 75-88, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Four of the techniques are related to FTR/software reviews, which include Formal review, Fagan's inspections, Peer review and Walkthrough. However, their detailed review processes are not described in the literature. Boehm identified a set of V&V techniques that are effective in performing software requirements and design V&V <ref> [12] </ref>. Those related to software reviews include Reading, Interviews (i.e., discussing the material with its originator) and Checklists. Boehm also stated that structured walkthrough or software inspection has been the most cost-effective technique to date (1987) for eliminating software errors [13]. <p> In general, V&V techniques can be classified into two categories: static analysis and dynamic 14 analysis (or testing). Static analysis can be further classified into manual and automated static analysis <ref> [3, 12, 11] </ref>. FTR is a manual static analysis technique. A similar technique is formal mathematical proofs. Some people consider FTR informal compared to mathematical proofs, however, some FTR methods are combined with mathematical proofs resulting in an even more effective review process. <p> SSN must be a string with format "XXX-XX-XXXX", where each X is a digit. Source-code: 001:int Employee::setSocSecurity (char* newSSN) 002: 004: int i = 0; 005: int validDigits = 0; //number of valid digits 006: 007: if (socSecurity == 0) 008: socSecurity = new char <ref> [12] </ref>; 009: 010: //Check for valid SSN format 011: while (newSSN [i] != '"0')- 012: if (newSSN [i] &gt;= '0' && newSSN [i] &lt;= '9')- 013: socSecurity [i] = newSSN [i]; 014: i++; 015: validDigits++; 016: - 017: else if ((newSSN [i] == '-') && (i == 3 || i == <p> "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG <ref> [12] </ref>*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END <p> &symtabret, source.labl, &address); 014: if (symtabret == NOTFOUND) - 015: address = locctr; 016: Access_Symtab (SEARCH, &symtabret, source.labl, &address); 017: if (symtabret = TABLEFULL) - 018: *errorsfound = true; 019: errorflags [13] = true; /* SYMBOL TABLE OVERFLOW */ 020: - 022: else - 023: *errorsfound = true; 024: errorflags <ref> [12] </ref> = true; /* DUPLICATE LABEL */ 025: - 197 027:- 029: D.10 Pass 1 Specification: This is the main procedure for pass 1. It uses P1 Read Source to read each input statement (until endofinput = true). For non-comment lines, it calls P1 Assign Loc and P1 Assign Sym. <p> "MISSING OR MISPLACED OPERAND IN BYTE STATEMENT ",/*ERRMSG [7]*/ 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG <ref> [12] </ref>*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END
Reference: [13] <author> B. Boehm. </author> <title> Industrial software metrics top 10 list. </title> <journal> IEEE Software, </journal> <pages> pages 84-85, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: Those related to software reviews include Reading, Interviews (i.e., discussing the material with its originator) and Checklists. Boehm also stated that structured walkthrough or software inspection has been the most cost-effective technique to date (1987) for eliminating software errors <ref> [13] </ref>. Adrion et al. indicated that disciplined manual techniques, such as walkthroughs, reviews and inspections, have been the most successful V&V techniques [3]. The reason for this success is due to errors removal within the first stages of the development (requirement and design). <p> This number is also smaller than Boehm's claim that structured walkthroughs or inspections catch 60% of the errors <ref> [13] </ref>. One may argue that this difference is due to a significantly larger number of group size in industry, assuming that one agrees with the effect of group size shown in Table 2.4. Most organizations who published their FTR data, unfortunately, did not mention the size of their groups. <p> 084: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG <ref> [13] </ref>*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT <p> 009: int address; 010: 011: if (!errorflags [0] && 012: !strncmp (source.labl, BLANK8, sizeof (CHAR8))) - 013: Access_Symtab (SEARCH, &symtabret, source.labl, &address); 014: if (symtabret == NOTFOUND) - 015: address = locctr; 016: Access_Symtab (SEARCH, &symtabret, source.labl, &address); 017: if (symtabret = TABLEFULL) - 018: *errorsfound = true; 019: errorflags <ref> [13] </ref> = true; /* SYMBOL TABLE OVERFLOW */ 020: - 022: else - 023: *errorsfound = true; 024: errorflags [12] = true; /* DUPLICATE LABEL */ 025: - 197 027:- 029: D.10 Pass 1 Specification: This is the main procedure for pass 1. <p> 118: "MISSING OR MISPLACED OPERAND IN RESW STATEMENT ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG <ref> [13] </ref>*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT
Reference: [14] <author> Barry W. Boehm. </author> <title> Software Engineering Economics. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1981. </year>
Reference-contexts: Boehm indicates that rework of erroneous requirement, design and code typically consumes 40-50 percent of the total cost of software development. Removing a requirements problem once it reaches the maintenance phase generally costs about 100 times more than it would take to remove the problem in the requirements phase <ref> [14] </ref>. Pressman uses a defect amplification model to characterize this situation: the number of defects is multiplied as the defects travel down the life cycle when no reviews are conducted [118]. Thus, one should avoid rework to improve productivity and focus on building the product right the first time. <p> ",/*ERRMSG [8]*/ 085: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG <ref> [14] </ref>*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG <p> ",/*ERRMSG [8]*/ 119: "ILLEGAL OPERAND IN RESW STATEMENT ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG <ref> [14] </ref>*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND <p> Source-code: 001:void P2_Proc_START (SOURCETYPE source, 002: BOOLEAN *errorsfound, 003: BOOLEAN *errorflags, 004: OBJTYPE *objct) 005: 007: int i; 009: if (!FIRSTSTMT) - 010: *errorsfound = true; 011: errorflags <ref> [14] </ref> = true;/*DUPLICATE OR MISPLACED START STATEMENT*/ 012: - 013: objct-&gt;rectype = HEADREC; 014: objct-&gt;objlength = 0; 015: for (i = 0; i &lt;= 5; i++) 016: objct-&gt;objcode [i] = source.labl [i]; 017: for (i = 6; i &lt;= 14; i++) 018: objct-&gt;objcode [i] = ' '; 019:- 021: E.6 P2
Reference: [15] <author> Nathaniel S. Borenstein. </author> <title> Computational mail as network infrastructure for computer-supported cooperative work. </title> <booktitle> Proceedings of the Conference on Computer-Supported Cooperative Work (CSCW'92), </booktitle> <pages> pages 67-74, </pages> <year> 1992. </year>
Reference-contexts: The messages themselves are typically typed messages and include a set of fields or attributes. The system compares these values with the user's supplied criteria in order to generate appropriate actions. Examples of message systems include Information Lens [95], Lotus Notes [35], Coordinator [55], Strudel [128], MailTray [120], Atomicmail <ref> [15] </ref>, and Active Mail [65]. Message systems are typically generic systems for building group applications, such as meeting schedulers, information finders, and tools for project management, task tracking, software defect tracking, and collaborative writing. <p> Error E19 is incorrect re-initialization of a global variable (error type C1). One example is as follows: void P2_Assemble_Inst (...) ... if (FIRSTSTMT && !strncmp (source.operation,"START ",sizeof (CHAR6)))- 129 /*the first source statement (except for comments) must be start*/ *errorsfound = true; errorflags <ref> [15] </ref> = true; /*missing or misplaced start statement*/ - /* missing reinitialization of FIRSTSTMT to false here*/ ... The global variable FIRSTSTMT is initialized in the main function to a boolean value true. <p> ",/*ERRMSG [9]*/ 086: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG <ref> [15] </ref>*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT <p> ",/*ERRMSG [9]*/ 120: "MISSING OR MISPLACED OPERAND IN RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG <ref> [15] </ref>*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END <p> 028: objct-&gt;rectype = NONE; 029: - 031: else if (!strncmp (source.operation, "END ", sizeof (CHAR6))) 032: P2_Proc_END (source,LOCCTR,errorsfound,errorflags,objct); 033: 034: else 035: P2_Proc_INST (source,LOCCTR,errorsfound,errorflags,objct); 036: 037: if (FIRSTSTMT && !strncmp (source.operation,"START ",sizeof (CHAR6)))- 038: /*THE FIRST SOURCE STATEMENT (EXCEPT FOR COMMENTS) MUST BE START*/ 039: *errorsfound = true; 040: errorflags <ref> [15] </ref> = true; /*MISSING OR MISPLACED START STATEMENT*/ 041: - 043: 045: E.8 P2 Write Obj Specification: This procedure places the generated object code into the object program.
Reference: [16] <author> R. N. Britcher. </author> <title> Using inspection to investigate program correctness. </title> <booktitle> IEEE Computer, </booktitle> <month> November </month> <year> 1988. </year>
Reference-contexts: This technique is used in the Cleanroom development method to produce zero defect software [94]. Britcher also proposes a similar approach of incorporating correctness arguments into the program which are later checked by inspections <ref> [16] </ref>. With respect to relative effectiveness of these methods, Jones provides the approximate ranges of detection efficiencies (percentage of total errors caught) for 10 common defect removal/detection methods obtained from uncontrolled observations (see Table 2.1) [87]. Table 2.1 generally suggests that reviews/inspections are more effective at finding defects than testing. <p> RESB STATEMENT ",/*ERRMSG [10]*/ 087: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG <ref> [16] </ref>*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ <p> RESB STATEMENT ",/*ERRMSG [10]*/ 121: "ILLEGAL OPERAND IN RESB STATEMENT ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG <ref> [16] </ref>*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG
Reference: [17] <author> L. Brothers, V. Sembugamoorthy, and M. Muller. ICICLE: </author> <title> Groupware for code inspection. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Computer Supported Cooperative Work, </booktitle> <pages> pages 169-181, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Only few studies have attempted to address this issue as described earlier [142, 116]. The experiment described in this thesis also investigates this question, although from a different perspective. 1.3.3 Variations and inflexibility in review systems implementation Recently, many computer-supported review systems have emerged, such as, ICICLE <ref> [17] </ref>, Scrutiny [61], CSI [98], InspeQ [92], etc. These systems implement a wide variety of FTR methods. For example, ICICLE and Scrutiny implement Fagan's method, CSI implements Humphrey's method, InspeQ implements Phased Inspection, etc. <p> Another difference between Management and Technical reviews is that in the former the attendance typically includes management, whereas in the latter it consists primarily of technical personnel/peers. 4. Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed <ref> [17, 84, 61, 98, 99, 92, 136] </ref>. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed [17, 84, 61, 98, 99, 92, 136]. The type of support varies from simple augmentation of the manual practices <ref> [17, 61] </ref> to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> First, it discusses existing computer supported review systems that implement specific FTR methods. These include ICICLE <ref> [17] </ref>, CIA/Scrutiny [61], CSI [98], CAIS [99], InspeQ [92], and QDA/Tammi [136]. The discussion highlights those facilities that are either similar to or different from CSRS. The following subsection compares CSRS with existing computer-supported cooperative work systems. <p> Although, these systems do not support the FTR process directly, they are included here because their unique features may exhibit some similarities with CSRS. The last subsection summarizes major strength and limitations of CSRS. 4.4.1 ICICLE ICICLE (Intelligent Code Inspection Environment in a C Language Environment) <ref> [17] </ref> is a review system that augments the process of formal code inspection (i.e., Fagan's method), with the goal of 70 a paperless review environment. The system allows inspectors to easily traverse source code in a windowed environment instead of rifling through hard copy of source files. <p> ",/*ERRMSG [11]*/ 088: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 089: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG <ref> [17] </ref>*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 099: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 100: "MISSING OR MISPLACED <p> ",/*ERRMSG [11]*/ 122: "DUPLICATE LABEL DEFINITION ",/*ERRMSG [12]*/ 123: "TOO MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG <ref> [17] </ref>*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 133: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 134: "MISSING OR
Reference: [18] <author> Bill Brykczynski. </author> <title> The software inspection process-applying the principles of Deming and Crosby. </title> <journal> Information and Systems Engineering, </journal> <volume> 1(1) </volume> <pages> 23-37, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: W. Edwards Deming's work on the industrial quality improvement in the post World War II [37]. Brykczynski noted that the total quality principles of Deming are remarkably similar to software inspection concepts, which generally include early defect detection, rework reduction, causal analysis and continuous process improvement <ref> [18] </ref>. Specifically, Brykczynski elaborated fourteen quality improvement principles set forth by Deming, and how the FTR process (i.e., software inspection) can be used to implement these principles. These principles include both the technical and management aspects of TQM. The two most significant technical principles include: 1. Cease dependence on Inspection. <p> These principles include both the technical and management aspects of TQM. The two most significant technical principles include: 1. Cease dependence on Inspection. The term Inspection in TQM refers to downstream testing <ref> [18] </ref>. This principle states that the quality must be built into the product, or it does not exist. It cannot be tested into it. Thus, one should no longer depend on downstream testing to achieve quality. <p> In conclusion, inspection data can be used to improve the quality of the products under inspection, and the quality of the inspection process itself. Another aspect of software inspection that implements continuous process improvement is defect causal analysis <ref> [18] </ref>. The goal of causal analysis is to determine the cause of each defect found and the suggested action to prevent it from occurring in the future. As stated by Humphrey, Every defect is an improvement opportunity [74]. <p> some of the data in Table 2.3 may be misleading, since the testing took place after the inspection which had removed a substantial percentage of errors (see also Table 2.2). 2.5.2 Reported FTR problems and limitations Despite numerous reports on successful FTR (mainly inspection), inspection has not been widely used <ref> [18] </ref>. Brykcysnki presented several possible reasons: * Technology transition to any new process is difficult and takes time. * Upfront cost of inspection is relatively high. Organizations may be reluctant to make the upfront investment in inspection. * Confusion with other review processes. <p> of opcode table */ 187 028:#define SYMTABLIMIT 100 /* Size of symbol table */ 029:#define BLANK6 " " 030:#define BLANK8 " " 031:#define BLANK18 " " 032:#define BLANK31 " " 033: 034:typedef unsigned char BOOLEAN; 035:typedef char CHAR4 [4]; 036:typedef char CHAR6 [6]; 037:typedef char CHAR8 [8]; 038:typedef char CHAR18 <ref> [18] </ref>; 039:typedef char CHAR31 [31]; 040:typedef char CHAR50 [50]; 041:typedef char CHAR66 [66]; 042: 043:typedef struct SOURCETYPE - 044: /* source line and subfields */ 045: CHAR66 line; 046: BOOLEAN comline; 047: CHAR8 labl; 048: CHAR6 operation; 049: CHAR18 operand; 050: CHAR31 comment; 051:- SOURCETYPE; 052: 053:typedef struct REC_SYMTABTYPE - 054: <p> MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 090: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG <ref> [18] </ref>*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 099: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 100: "MISSING OR MISPLACED OPERAND IN INSTRUCTION "/*ERRMSG [24]*/ 101:-; 103: D.2 hextonum Specification: <p> MAXOPS 25 /* SIZE OF OPCODE TABLE */ 030: 031:#define BLANK6 " " 032:#define BLANK8 " " 033:#define BLANK18 " " 034:#define BLANK30 " " 035:#define BLANK31 " " 036: 037:typedef unsigned char BOOLEAN; 038: 039:typedef char CHAR4 [4]; 040:typedef char CHAR6 [6]; 041:typedef char CHAR8 [8]; 042:typedef char CHAR18 <ref> [18] </ref>; 043:typedef char CHAR30 [30]; 044:typedef char CHAR31 [31]; 045:typedef char CHAR50 [50]; 046:typedef char CHAR66 [66]; 047: 048:typedef struct SOURCETYPE - 049: /* SOURCE LINE AND SUBFIELDS */ 050: CHAR66 line; 051: BOOLEAN comline; 052: CHAR8 labl; 053: CHAR6 operation; 054: CHAR18 operand; 055: CHAR31 comment; 056:- SOURCETYPE; 057: 058:typedef <p> MANY SYMBOLS IN SOURCE PROGRAM ",/*ERRMSG [13]*/ 124: "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG <ref> [18] </ref>*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 133: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 134: "MISSING OR MISPLACED OPERAND IN INSTRUCTION " /*ERRMSG [24]*/ 135: -; 137:
Reference: [19] <author> F. O. Buck. </author> <title> Indicators of quality inspections. </title> <type> Technical Report 21.802, </type> <institution> IBM Corporation, </institution> <month> September </month> <year> 1981. </year>
Reference-contexts: In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. For example, some studies have shown that in the inspection method, preparation time correlates positively with density of errors found <ref> [19, 2, 24] </ref>. However, as pointed out by Christenson et al. [24], the causal effects among these factors are not clear; one would think slower preparation would yield a higher number of errors found, but it may also be the presence of more errors that necessitates greater preparation time. <p> However, Dow and Murphy found comprehension (i.e., detailed product knowledge) is not a prerequisite for an effective inspection [42]. Similarly, some studies provide empirical results that relate preparation time to review outcomes <ref> [19, 2, 24, 74] </ref>. However, they generally do not specify whether reviewers should concentrate on simply learning the materials or on looking for errors during the preparation phase. In the same way, examination often implies consolidation. <p> "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG <ref> [19] </ref>*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 099: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 100: "MISSING OR MISPLACED OPERAND IN INSTRUCTION "/*ERRMSG [24]*/ 101:-; 103: D.2 hextonum Specification: This function examines the string str, beginning with <p> "DUPLICATE OR MISPLACED START STATEMENT ",/*ERRMSG [14]*/ 125: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG <ref> [19] </ref>*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 133: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 134: "MISSING OR MISPLACED OPERAND IN INSTRUCTION " /*ERRMSG [24]*/ 135: -; 137: 139: E.2 dectonum Specification: This functions scans the
Reference: [20] <author> Robert D. Buck and James H. </author> <title> Dobbins. </title> <booktitle> Software Validation, chapter Application of Software Inspection Methodology in Design and Code, </booktitle> <pages> pages 41-56. </pages> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1984. </year>
Reference-contexts: With such consistency, the data was also used for statistical quality control. The inspection process followed the Fagan's method <ref> [20] </ref>. * Bush reported experiences with 300 Fagan's inspections within a period of 21 months at Jet Propulsion Laboratory (JPL). The results showed that inspections found close to 75% of all defects before entering the test phase. The inspections were also proved to be cost effective. <p> 18% Fagan Fagan86 [53] IBM UK 93% Fagan Standard Bank &gt;50% Fagan AMEX &gt;50% Fagan Ackerman89 [1] Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 <ref> [20] </ref> IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 [107] IBM 85% Fagan Doolan92 [41] Shell Research Doolan Kaplan95 [88] IBM 3.5 15-25 Kaplan Again some of the data in Table 2.3 may be misleading, since the testing took place after the inspection <p> The producer participates as a silent observer [124]. * Buck and Dobbins' inspection uses statistical process control during the preparation and the meeting phases <ref> [20] </ref>. * Bush's inspection includes the third-hour meeting phase [21]. * Doolan's inspection uses the meeting phase for consolidation/ defects logging [41]. 3.2.16 Empirical studies of FTR FTR methods used for empirical studies also involve many variations of review components. <p> 091: "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 092: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG <ref> [20] </ref>*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 099: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 100: "MISSING OR MISPLACED OPERAND IN INSTRUCTION "/*ERRMSG [24]*/ 101:-; 103: D.2 hextonum Specification: This function examines the string str, beginning with the byte position indicated by first, for <p> "MISSING OR MISPLACED START STATEMENT ",/*ERRMSG [15]*/ 126: "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG <ref> [20] </ref>*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 133: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 134: "MISSING OR MISPLACED OPERAND IN INSTRUCTION " /*ERRMSG [24]*/ 135: -; 137: 139: E.2 dectonum Specification: This functions scans the string str, beginning at the byte position given
Reference: [21] <author> Marilyn Bush. </author> <title> Improving software quality: The use of formal inspections at the Jet Propulsion Laboratory. </title> <booktitle> In Proceedings of the 12th International Conference on Software Engineering, </booktitle> <pages> pages 196-199, </pages> <address> Nice, France, March 1990. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The inspection process involves additional phase called third-hour, in which the participants discuss solution ideas <ref> [21] </ref>. * Freedman and Weinberg reported a dramatic improvement in program quality before and after code reviews. The average error rates of the first 5 COBOL programs without code reviews was 4.50 Errors/100 LOC. <p> UK 93% Fagan Standard Bank &gt;50% Fagan AMEX &gt;50% Fagan Ackerman89 [1] Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 <ref> [21] </ref> JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 [107] IBM 85% Fagan Doolan92 [41] Shell Research Doolan Kaplan95 [88] IBM 3.5 15-25 Kaplan Again some of the data in Table 2.3 may be misleading, since the testing took place after the inspection which had removed a substantial <p> Gilb refers to consolidation as defects logging, and Votta and Porter refer to it as collection. Correlation can be found in Humphrey's method [74]. Discussion can be found in the third hour meeting phase of some inspection methods <ref> [21, 108] </ref>. In practice, a single phase may implicitly include multiple activities. For example, during Fagan's preparation phase, one may discover errors while attempting to comprehend the code. During defect finding, one may obtain additional comprehension while examining the code. <p> discussion since it often leads to discovery of new errors [69, 41] For the same reason, some review practices/methods include a separate phase, called third-hour meeting (so called because it takes place after the two-hour regular inspection meetings) where the objective of the phase is mainly to discuss solution ideas <ref> [21, 108] </ref>. 3.1.2 Interaction The interaction component of FTR framework specifies the group process that takes place among review participants. <p> The producer participates as a silent observer [124]. * Buck and Dobbins' inspection uses statistical process control during the preparation and the meeting phases [20]. * Bush's inspection includes the third-hour meeting phase <ref> [21] </ref>. * Doolan's inspection uses the meeting phase for consolidation/ defects logging [41]. 3.2.16 Empirical studies of FTR FTR methods used for empirical studies also involve many variations of review components. Hetzel's study Hetzel's study investigates the effectiveness of code reading (FTR method) compared to testing [70]. <p> "ILLEGAL OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG <ref> [21] </ref>*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 099: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 100: "MISSING OR MISPLACED OPERAND IN INSTRUCTION "/*ERRMSG [24]*/ 101:-; 103: D.2 hextonum Specification: This function examines the string str, beginning with the byte position indicated by first, for a hexadecimal value. <p> OPERAND IN WORD STATEMENT ",/*ERRMSG [16]*/ 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG <ref> [21] </ref>*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 133: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 134: "MISSING OR MISPLACED OPERAND IN INSTRUCTION " /*ERRMSG [24]*/ 135: -; 137: 139: E.2 dectonum Specification: This functions scans the string str, beginning at the byte position given by first, for the character representation of a decimal <p> In order to do this, it makes use of the global variables firststmt and endfound. Source-code: 001:void P2_Assemble_Inst (SOURCETYPE source, 002: int LOCCTR, 003: BOOLEAN *errorsfound, 004: BOOLEAN *errorflags, 005: OBJTYPE *objct) 006: 008:- 010: if (ENDFOUND) - 011: *errorsfound = true; 012: errorflags <ref> [21] </ref> = true; /* STATEMENT SHOULD NOT FOLLOW END */ 013: - 015: if (!strncmp (source.operation, "START ", sizeof (CHAR6))) 016: P2_Proc_START (source,errorsfound,errorflags,objct); 017: 018: else if (!strncmp (source.operation, "WORD ", sizeof (CHAR6))) 019: P2_Proc_WORD (source,LOCCTR,errorsfound,errorflags,objct); 020: 021: else if (!strncmp (source.operation, "BYTE ", sizeof (CHAR6))) 022: P2_Proc_BYTE (source,LOCCTR,errorsfound,errorflags,objct); 023: 024:
Reference: [22] <author> D.A. Christenson and S.T. Huang. </author> <title> Code inspection management using statistical control limits. </title> <booktitle> Proceedings of the National Communications Forum, </booktitle> <address> 41(II):1095-1100, </address> <year> 1987. </year>
Reference-contexts: Thus for example, if the pace of an inspection is well above the standard guideline 150 LOC/hour, the materials need to be reinspected. Control charts are also commonly used to evaluate the performance of an inspection process statistically <ref> [22, 74, 133] </ref>. Others use mathematical models to predict remaining errors after inspections [23, 47]. The principle of continuous process improvement also applies to the inspection process itself. This improvement can be achieved in the same way using statistical process control as discussed above. <p> [16]*/ 093: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG <ref> [22] </ref>*/ 099: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 100: "MISSING OR MISPLACED OPERAND IN INSTRUCTION "/*ERRMSG [24]*/ 101:-; 103: D.2 hextonum Specification: This function examines the string str, beginning with the byte position indicated by first, for a hexadecimal value. <p> 127: "MISSING OR MISPLACED OPERAND IN WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG <ref> [22] </ref>*/ 133: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 134: "MISSING OR MISPLACED OPERAND IN INSTRUCTION " /*ERRMSG [24]*/ 135: -; 137: 139: E.2 dectonum Specification: This functions scans the string str, beginning at the byte position given by first, for the character representation of a decimal value.
Reference: [23] <author> D.A. Christenson and S.T. Huang. </author> <title> A code inspection model for software quality management and prediction. </title> <booktitle> Proceedings of the IEEE Global Telecommunications Conference, </booktitle> <volume> 1 </volume> <pages> 468-471, </pages> <year> 1988. </year>
Reference-contexts: Control charts are also commonly used to evaluate the performance of an inspection process statistically [22, 74, 133]. Others use mathematical models to predict remaining errors after inspections <ref> [23, 47] </ref>. The principle of continuous process improvement also applies to the inspection process itself. This improvement can be achieved in the same way using statistical process control as discussed above. <p> For example, Christenson proposes an inspection model that relates input parameters, such as the preparation effort and inspection rate to the resulting fraction of errors found in the code <ref> [23, 24] </ref>. Using such a model, the effectiveness of inspections can be estimated and improvement can be made. However, this model does not incorporate other review techniques beyond Fagan's inspection. Furthermore, it does not describe how one should conduct the inspection process. <p> IN WORD STATEMENT ",/*ERRMSG [17]*/ 094: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 099: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG <ref> [23] </ref>*/ 100: "MISSING OR MISPLACED OPERAND IN INSTRUCTION "/*ERRMSG [24]*/ 101:-; 103: D.2 hextonum Specification: This function examines the string str, beginning with the byte position indicated by first, for a hexadecimal value. <p> WORD STATEMENT ",/*ERRMSG [17]*/ 128: "MISSING OR MISPLACED OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 133: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG <ref> [23] </ref>*/ 134: "MISSING OR MISPLACED OPERAND IN INSTRUCTION " /*ERRMSG [24]*/ 135: -; 137: 139: E.2 dectonum Specification: This functions scans the string str, beginning at the byte position given by first, for the character representation of a decimal value.
Reference: [24] <author> D.A. Christenson, S.T. Huang, A.J. Lamperez, and D.P. Smith. </author> <title> Total Quality Management For Software, </title> <booktitle> chapter Statistical Methods Applied To Software, </booktitle> <pages> pages 351-385. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. For example, some studies have shown that in the inspection method, preparation time correlates positively with density of errors found <ref> [19, 2, 24] </ref>. However, as pointed out by Christenson et al. [24], the causal effects among these factors are not clear; one would think slower preparation would yield a higher number of errors found, but it may also be the presence of more errors that necessitates greater preparation time. <p> For example, some studies have shown that in the inspection method, preparation time correlates positively with density of errors found [19, 2, 24]. However, as pointed out by Christenson et al. <ref> [24] </ref>, the causal effects among these factors are not clear; one would think slower preparation would yield a higher number of errors found, but it may also be the presence of more errors that necessitates greater preparation time. <p> Many studies have found a correlation between the effectiveness of inspections and certain inspection metrics. For example, the number of defects found decline with increasing inspection or preparation rates <ref> [53, 74, 24] </ref>, with the optimum inspection rate is around 150 LOC/hour [124]. This data is then used as a guideline for controlling the inspection process. Thus for example, if the pace of an inspection is well above the standard guideline 150 LOC/hour, the materials need to be reinspected. <p> However, Dow and Murphy found comprehension (i.e., detailed product knowledge) is not a prerequisite for an effective inspection [42]. Similarly, some studies provide empirical results that relate preparation time to review outcomes <ref> [19, 2, 24, 74] </ref>. However, they generally do not specify whether reviewers should concentrate on simply learning the materials or on looking for errors during the preparation phase. In the same way, examination often implies consolidation. <p> Chapter 4 will describe the design and implementation of such a system. 3.4 Work related to the FTR framework Existing theoretical work on FTR focuses primarily on modeling certain parameters of Fagan's inspection process in order to measure and predict its effectiveness <ref> [24, 6, 74] </ref>. For example, Christenson proposes an inspection model that relates input parameters, such as the preparation effort and inspection rate to the resulting fraction of errors found in the code [23, 24]. Using such a model, the effectiveness of inspections can be estimated and improvement can be made. <p> For example, Christenson proposes an inspection model that relates input parameters, such as the preparation effort and inspection rate to the resulting fraction of errors found in the code <ref> [23, 24] </ref>. Using such a model, the effectiveness of inspections can be estimated and improvement can be made. However, this model does not incorporate other review techniques beyond Fagan's inspection. Furthermore, it does not describe how one should conduct the inspection process. <p> In EIAM, paraphrasing was performed by individual participants silently. For the latter, however, I had no way of knowing whether the participants read every single statement as instructed in the review guideline. Several studies have found that paraphrasing rate is a good predictor of review effectiveness <ref> [124, 74, 24] </ref>. In fact, Russell suggested that the effective paraphrasing rate should be around 150 lines/hour. This section compares the paraphrasing rate of EGSM and EIAM. <p> OPERAND IN END STATEMENT ",/*ERRMSG [18]*/ 095: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 096: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 097: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 098: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 099: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 100: "MISSING OR MISPLACED OPERAND IN INSTRUCTION "/*ERRMSG <ref> [24] </ref>*/ 101:-; 103: D.2 hextonum Specification: This function examines the string str, beginning with the byte position indicated by first, for a hexadecimal value. <p> END STATEMENT ",/*ERRMSG [18]*/ 129: "ILLEGAL OPERAND IN END STATEMENT ",/*ERRMSG [19]*/ 202 130: "UNDEFINED SYMBOL IN OPERAND ",/*ERRMSG [20]*/ 131: "STATEMENT SHOULD NOT FOLLOW END STATEMENT ",/*ERRMSG [21]*/ 132: "ILLEGAL OPERAND FIELD ",/*ERRMSG [22]*/ 133: "UNRECOGNIZED OPERATION CODE ",/*ERRMSG [23]*/ 134: "MISSING OR MISPLACED OPERAND IN INSTRUCTION " /*ERRMSG <ref> [24] </ref>*/ 135: -; 137: 139: E.2 dectonum Specification: This functions scans the string str, beginning at the byte position given by first, for the character representation of a decimal value.
Reference: [25] <author> J.A. Clapp, S.F. Stanten, W.W. Peng, D.R. Wallace, D.A. Cerino, and Jr. </author> <title> R.J. Dziegiel. Software Quality Control, Error Analysis, And Testing. Noyes Data Corporation, </title> <address> Mill Road, Park Ridge, N.J., </address> <year> 1995. </year>
Reference-contexts: One thus cannot appreciate the enormous costs that can be avoided by finding and fixing defects before test. The biggest single problem is generally the combination of management inattention and schedule pressure [74, 75]. Clapp and Wallace identified two major issues concerning inspection <ref> [25] </ref>: 1. Inspection is viewed as low-level, manual work. People think testing is easier and faster. People need to be motivated by showing what they can learn as well as the savings to the project from inspections. 2.
Reference: [26] <author> Tony Collins. </author> <title> Bank error hands out 2bn pounds in half an hour. </title> <institution> Computer Weekly (UK), </institution> <month> October 19 </month> <year> 1989. </year>
Reference-contexts: For example, in 1992, a tiny software bug in the 4ESS switching system crashed a substantial portion of AT&T's telecommunication network [4]. In 1989, a UK bank accidently transferred $2 billion to US and UK companies because a software design flaw allowed payment instructions to be duplicated <ref> [26] </ref>. In 1986, a medical system for radiation therapy killed two patients due to improper design and implementation of its software. A report given to to the U.S. Food and Drug Administration (FDA) stated that the software failed to access the appropriate calibration data [79, 126].
Reference: [27] <author> James S. Collofello. </author> <title> The software technical review process. SEI Curriculum Module SEI-CM3-1.5, </title> <institution> Software Engineering Institute, </institution> <year> 1988. </year>
Reference-contexts: Finally, FTR also plays a key role in defect prevention and process improvement [60]. The results of reviews can be used to institute continuous process improvement of the entire software development process. The importance of software reviews is summarized by Collofello as follows <ref> [27] </ref>: * Software reviews allow errors to be detected early in the development stages before their cost for repair is escalated. * Reviews supplement testing. Testing is not sufficient. First, exhaustive testing of code is impossible. Second, technology does not currently exist for testing a specification or high level design. <p> Walkthroughs. Walkthroughs can be viewed as presentation reviews, in which a review participant, usually the developer of the software being reviewed, narrates a description of the software and the remainder of the review group provides their feedback throughout the presentation <ref> [56, 27, 60] </ref>. The term walkthrough itself has been used in the literature in a variety of ways. Some refer it as structured walkthroughs [150]. <p> The term walkthrough itself has been used in the literature in a variety of ways. Some refer it as structured walkthroughs [150]. Generally, the literature describes walkthrough as an undisciplined process without advanced preparation on the part of reviewers, and the meeting focuses on education purposes <ref> [27, 52] </ref>. However, many authors consider it a disciplined and formal review process [106, 150, 3]. 3. Round-robin reviews. <p> Formal reviews are a class of reviews characterized by carefully planned meetings in which reviewers are held accountable for their participation in the review and in which a review report containing action items is generated and acted upon <ref> [27] </ref>. For example, desk checking and walkthrough are informal reviews; inspection and structured walkthrough, on the other hand, are formal reviews; peer review, and round-robin review may be formal or informal depending upon whether there is a formal report being produced documenting the results of the review or not. 2. <p> Internal versus External reviews. These classes of reviews focus on the authority who manages the reviews. An external review implies that there is an independent central figure that has complete authority and ownership over the reviews and the resulting reports. One example of such a review is an Audit <ref> [72, 27] </ref>. The auditor is an independent organization that oversees the review process, but has no control over the corrective actions taken by the audited organization. <p> As such, some key issues from small group theory apply to FTR, such as group think (tendency to suppress dissent in the interests of group harmony), group deviants (influence by minority), and domination of the group by a single member <ref> [27] </ref>. Other key issues include social facilitation, where the presence of others may boost one's performance (akin to group synergy), and social loafing, where one member free rides on the group's effort [104]. <p> Ackerman also indicates that a potential of developing interpersonal tension in group inspections can be significant when inspections are initially introduced, but they tend to disappear as inspections become a routine procedure [2]. Collofello emphasizes the importance of developing a review culture to minimize personal stress <ref> [27] </ref>.
Reference: [28] <author> James S. Collofello and Scott N. Woodfield. </author> <title> Evaluating the effectiveness of reliability-assurance techniques. </title> <journal> Journal of Systems and Software, </journal> <month> March </month> <year> 1989. </year>
Reference-contexts: Effectiveness Hetzel76 [70] code reading (37.3%) &lt; functional testing (47.7%) = structural testing (46.7%) Myers78 [105] code walkthrough (38%) = code walkthrough functional testing (30%) = most expensive structural testing (36%) Basili87 [7] code reading &gt; (16% more) code reading functional testing &gt; (11% more) least expensive structural testing Collofello89 <ref> [28] </ref> code review (64%)&gt; design review &gt; design review (54%) &gt; code review &gt; testing (38%) testing 2.4 FTR as a process management technique One typical criticism of traditional software development is its dependency on testing to remove software defects.
Reference: [29] <author> ABACUS Concepts. StatView. </author> <title> The ultimate integrated data analysis and presentation system (for the Maciontosh). ABACUS Concepts, </title> <publisher> Inc., </publisher> <address> 1st edition, </address> <year> 1992. </year> <month> 224 </month>
Reference-contexts: If the p-value is less than the significance level ff = 0.05, then we reject the null hypothesis, or we may conclude that there is a significant difference between X and Y. The actual data analysis was done using the StatView program on the Macintosh <ref> [29] </ref> 6.3 EGSM and EIAM As described earlier, this study used EGSM (Experimental Group Synchronous Method) and EIAM (Experimental Individual Asynchronous Method) both as review methods and review systems to provide a controlled environment. <p> Thus, if two participants found the same issue, each participant's contribution for the issue would be 50%. EIAM for ICS-313 and ICS-411. Table 7.16 shows the Correlation Z Test <ref> [29] </ref>. The test shows that for all three cases, there was significant correlation (with P-value &lt; 0.05) between individual contributions in EGSM and EIAM. This result suggests that the performance of individual participants was constant no matter what method (EGSM or EIAM) was used.
Reference: [30] <author> Jeff Conklin and Michael L. Begeman. gIBIS: </author> <title> A hypertext tool for exploratory policy discus--sion. </title> <booktitle> In Proceedings of the 1988 ACM Conference on Computer-Supported Cooperative Work, </booktitle> <pages> pages 140-152, </pages> <address> Portland, Oregon, </address> <year> 1988. </year>
Reference-contexts: One of the goals is to provide the users with a medium to represent a collection of related ideas, and to discuss such ideas in a structured manner. Some examples of hypertext systems include NLS/Augment [50] NoteCards [68], gIBIS <ref> [30] </ref>, and Neptune [36]. These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor [67], Virtual Notebook [77], SEPIA [66], and Clare [144]. <p> OF OPCODE TABLE */ 030: 031:#define BLANK6 " " 032:#define BLANK8 " " 033:#define BLANK18 " " 034:#define BLANK30 " " 035:#define BLANK31 " " 036: 037:typedef unsigned char BOOLEAN; 038: 039:typedef char CHAR4 [4]; 040:typedef char CHAR6 [6]; 041:typedef char CHAR8 [8]; 042:typedef char CHAR18 [18]; 043:typedef char CHAR30 <ref> [30] </ref>; 044:typedef char CHAR31 [31]; 045:typedef char CHAR50 [50]; 046:typedef char CHAR66 [66]; 047: 048:typedef struct SOURCETYPE - 049: /* SOURCE LINE AND SUBFIELDS */ 050: CHAR66 line; 051: BOOLEAN comline; 052: CHAR8 labl; 053: CHAR6 operation; 054: CHAR18 operand; 055: CHAR31 comment; 056:- SOURCETYPE; 057: 058:typedef struct OBJTYPE - 059:
Reference: [31] <author> Stewart Crawford-Hines. </author> <title> Software inspections and technical reviews: Transcending the dogma. </title> <type> Technical report, </type> <institution> University of Colorado, Department of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: As stated by Crawford-Hines: Much of the discussion around procedural issues of FTR has been rather dogmatic. Many have defined a process in their one particular way and preach it as the One True Path from which no one may stray <ref> [31] </ref>. * FTR methods can be methodically investigated and improved. This thesis describes a systematic approach to FTR improvement. First, a framework for representing existing FTR methods was developed. <p> 187 028:#define SYMTABLIMIT 100 /* Size of symbol table */ 029:#define BLANK6 " " 030:#define BLANK8 " " 031:#define BLANK18 " " 032:#define BLANK31 " " 033: 034:typedef unsigned char BOOLEAN; 035:typedef char CHAR4 [4]; 036:typedef char CHAR6 [6]; 037:typedef char CHAR8 [8]; 038:typedef char CHAR18 [18]; 039:typedef char CHAR31 <ref> [31] </ref>; 040:typedef char CHAR50 [50]; 041:typedef char CHAR66 [66]; 042: 043:typedef struct SOURCETYPE - 044: /* source line and subfields */ 045: CHAR66 line; 046: BOOLEAN comline; 047: CHAR8 labl; 048: CHAR6 operation; 049: CHAR18 operand; 050: CHAR31 comment; 051:- SOURCETYPE; 052: 053:typedef struct REC_SYMTABTYPE - 054: CHAR8 symbol; 055: int <p> 030: 031:#define BLANK6 " " 032:#define BLANK8 " " 033:#define BLANK18 " " 034:#define BLANK30 " " 035:#define BLANK31 " " 036: 037:typedef unsigned char BOOLEAN; 038: 039:typedef char CHAR4 [4]; 040:typedef char CHAR6 [6]; 041:typedef char CHAR8 [8]; 042:typedef char CHAR18 [18]; 043:typedef char CHAR30 [30]; 044:typedef char CHAR31 <ref> [31] </ref>; 045:typedef char CHAR50 [50]; 046:typedef char CHAR66 [66]; 047: 048:typedef struct SOURCETYPE - 049: /* SOURCE LINE AND SUBFIELDS */ 050: CHAR66 line; 051: BOOLEAN comline; 052: CHAR8 labl; 053: CHAR6 operation; 054: CHAR18 operand; 055: CHAR31 comment; 056:- SOURCETYPE; 057: 058:typedef struct OBJTYPE - 059: /* OBJECT CODE, LENGTH,
Reference: [32] <author> John December and Neil Randall. </author> <title> The World Wide Web Unleashed. </title> <publisher> SAMS Publishing, </publisher> <year> 1994. </year>
Reference-contexts: The checklist items are presented to the user as fill-out forms. They must be written using HTML Fill-Out Form tags, for example, Input, Select, or Textarea tag (see <ref> [32] </ref> for further details).
Reference: [33] <author> L.E. Deimel and J.F. Naveda. </author> <title> Reading computer programs: Instructor's guide and exercises. SEI Curriculum Module CMU/SEI-90-EM-3, </title> <institution> Software Engineering Institute, </institution> <year> 1990. </year>
Reference-contexts: By answering these questions, the reviewers learn about the materials [110]. Another comprehension technique that has been shown to improve FTR effectiveness is the one suggested by Rifkin and Diemel <ref> [119, 33] </ref>. This technique involves basic top-down and bottom-up code reading strategies. A common technique for consolidation is presentation, where individual issues are presented one by one by the meeting leader, and the group confirms or rejects the issues as software defects [74].
Reference: [34] <author> Lionel E. Deimel. </author> <title> Scenes of Software Inspections. Video Dramatizations for the Classroom. </title> <institution> Software Engineering Institute, Carnegie Mellon University, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: In fact, the literature on FTR often describes conflicting review practices for the same method. For example, some descriptions of Fagan's method explicitly advocate the use of paraphrasing for the inspection technique <ref> [52, 124, 34] </ref>, while others disregard this technique [74]. Some advocate the use of common error checklists when examining source materials [74, 57], others also advocate the use of selective test cases [1, 43]. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. <p> Other key issues include social facilitation, where the presence of others may boost one's performance (akin to group synergy), and social loafing, where one member free rides on the group's effort [104]. The issue of domination in inspections has been well documented in the literature (i.e., moderator dominates inspection) <ref> [34] </ref>. Ackerman also indicates that a potential of developing interpersonal tension in group inspections can be significant when inspections are initially introduced, but they tend to disappear as inspections become a routine procedure [2]. Collofello emphasizes the importance of developing a review culture to minimize personal stress [27]. <p> Computer supported FTR may operate on all levels of synchronicity (STSP, STDP, DTSP, and DTDP). Again, synchronicity has been shown to play an important factor in FTR. For example, traditional face-to-face meetings (STSP) may cause meeting digression <ref> [109, 34] </ref>. Mashayekhi's study found that same-time different-place supported FTR meetings can be as effective as manual face-to-face meeting under certain conditions [98]. 3.1.3 Technique Technique describes the strategy that review participants use when following a specific interaction to achieve the stated objective. <p> Still another goal of FTArm as mentioned earlier is to explore a new FTR method that relies strongly on computer-mediated asynchronous group meetings, that is, group discussions are conducted through shared electronic notes. The aim is to reduce or eliminate common problems associated with face-to-face review meetings <ref> [34] </ref>: * The producer is under attack. In face-to-face meetings, reviewers sometimes verbally attack the producer for creating poor documents. This often leads to unnecessary arguments between the producer and the reviewers. <p> The paraphrasing process became very routine and tedious, and the reviewers often lost their concentration as the result. * Presenter is too fast. This is a common problem in paraphrasing. Several studies also cited this problem <ref> [124, 34] </ref>. My observation indicated that in these circumstances the presenters were generally very knowledgeable about the code being paraphrased.
Reference: [35] <author> David DeJean and Sally Blanning DeJean. </author> <title> Lotus Notes at Work. </title> <type> Brady, </type> <year> 1991. </year>
Reference-contexts: The messages themselves are typically typed messages and include a set of fields or attributes. The system compares these values with the user's supplied criteria in order to generate appropriate actions. Examples of message systems include Information Lens [95], Lotus Notes <ref> [35] </ref>, Coordinator [55], Strudel [128], MailTray [120], Atomicmail [15], and Active Mail [65]. Message systems are typically generic systems for building group applications, such as meeting schedulers, information finders, and tools for project management, task tracking, software defect tracking, and collaborative writing.
Reference: [36] <author> N. Delisle and M. Schwartz. Neptune: </author> <title> A hypertext system for cad applications. </title> <booktitle> Proceedings of ACM SIGMOD'86, </booktitle> <pages> pages 132-142, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: One of the goals is to provide the users with a medium to represent a collection of related ideas, and to discuss such ideas in a structured manner. Some examples of hypertext systems include NLS/Augment [50] NoteCards [68], gIBIS [30], and Neptune <ref> [36] </ref>. These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor [67], Virtual Notebook [77], SEPIA [66], and Clare [144].
Reference: [37] <author> W. Edwards Deming. </author> <title> Out Of The Crisis. </title> <institution> Massachusetts Institute of Technology, </institution> <year> 1986. </year>
Reference-contexts: The underlying concepts of managing and improving software process can be traced back to Dr. W. Edwards Deming's work on the industrial quality improvement in the post World War II <ref> [37] </ref>. Brykczynski noted that the total quality principles of Deming are remarkably similar to software inspection concepts, which generally include early defect detection, rework reduction, causal analysis and continuous process improvement [18].
Reference: [38] <author> Prasur Dewan and Rajiv Choudhary. </author> <title> Flexible user interface coupling in collaborative systems. </title> <booktitle> Proceedings of the ACM CHI'91 Conference on Human Factors in Computing Systems, </booktitle> <pages> pages 41-48, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In fact, the message systems described earlier may also be considered as groupware toolkits since they are used for building groupware applications (although mainly asynchronous applications). Some examples of these systems include GroupKit [122], Rendezvous [112], DistView [117], Conversation Builder (CB) [89], and Suite <ref> [38] </ref>. Some systems provide mainly toolkits for building synchronous collaborative applications, such as those that require shared window facilities, concur- rency controls, and session management (e.g., Group Kit, Rendezvous, DistView). Others provide additional facilities for supporting group processes (e.g., Suite and Conversation Builder).
Reference: [39] <author> M. Diehl and W. Stroebe. </author> <title> Productivity loss in brainstorming groups: Toward the solution of a riddle. </title> <journal> Journal of Personality and Social Psychology, </journal> (53):497-509, 1987. 
Reference-contexts: The experiment conducted as part of this research was an attempt to answer the latter question. 106 Similar studies in behavioral sciences have found that nominal groups generally perform better than real groups <ref> [39, 103] </ref>. The task given to the subjects in those studies, however, involves primarily idea generation.
Reference: [40] <author> James H. Dobbins. </author> <title> Handbook of Software Quality Assurance, chapter Inspections as an UpFront Quality Technique, </title> <address> pages 137-177. New York: </address> <publisher> Van Nostrand Reinhold, </publisher> <year> 1987. </year>
Reference-contexts: Another important and sensitive psychological aspect of FTR is the recording and dissemination of the data to the management. This must be done in such a way that individual programmers will not feel intimidated or threatened <ref> [40] </ref>. With respect to the positive psychological impact of FTR, Hart observed that reviews can make one more careful in writing programs (i.e., double checking his/her programs) in anticipation of having to present or share the programs with other participants.
Reference: [41] <author> E. P. Doolan. </author> <title> Experience with Fagan's inspection method. </title> <journal> Software Practice and Experience, </journal> <pages> pages 173-182, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Like walkthroughs, the term Inspections is also used in the literature in many different ways. For example, many researchers associate inspections with paraphrasing techniques, in which a designated reader paraphrases the materials statement by statement [106, 3, 124, 1], others downplay the role of reader <ref> [41, 101, 74, 60] </ref>. Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides [60]. <p> In most practices, the causal-analysis meeting is conducted as an extension to regular inspection, usually at the end of the inspection process <ref> [41, 60] </ref>. Finally, the principle of continuous process improvement must also occur at a personal level. Bisant and Lyle's study has shown that software reviews can improve programmers' performance significantly [10]. The most significant development for self-improvement initiatives is Humphrey's Personal Software Process (PSP) [76]. <p> With respect to the inspection process, there seemed to be no reader role unlike in Fagan's inspection. The inspection meeting was used mainly to log the defects found during individual preparation phase <ref> [41] </ref>. * Kaplan ran a case study of 61 code inspections involving 60 programmers on 14 teams at IBM Santa Teresa. <p> OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 [107] IBM 85% Fagan Doolan92 <ref> [41] </ref> Shell Research Doolan Kaplan95 [88] IBM 3.5 15-25 Kaplan Again some of the data in Table 2.3 may be misleading, since the testing took place after the inspection which had removed a substantial percentage of errors (see also Table 2.2). 2.5.2 Reported FTR problems and limitations Despite numerous reports on <p> Other researchers prefer to make these two activities separate (e.g., by having a separate defect-logging meeting) [74, 60]. Although consolidation does not involve examining new materials, some studies show that new errors are often discovered during this activity as a result of group interaction <ref> [41] </ref>. However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors [142, 116, 115]. Finally, group discussion to resolve issues or to find solutions is often considered detrimental to the review process and thus, should be forbidden [52, 124, 60]. <p> This happens, for example, during the inspection meeting phase. However, some review practices include this type of discussion since it often leads to discovery of new errors <ref> [69, 41] </ref> For the same reason, some review practices/methods include a separate phase, called third-hour meeting (so called because it takes place after the two-hour regular inspection meetings) where the objective of the phase is mainly to discuss solution ideas [21, 108]. 3.1.2 Interaction The interaction component of FTR framework specifies <p> The producer participates as a silent observer [124]. * Buck and Dobbins' inspection uses statistical process control during the preparation and the meeting phases [20]. * Bush's inspection includes the third-hour meeting phase [21]. * Doolan's inspection uses the meeting phase for consolidation/ defects logging <ref> [41] </ref>. 3.2.16 Empirical studies of FTR FTR methods used for empirical studies also involve many variations of review components. Hetzel's study Hetzel's study investigates the effectiveness of code reading (FTR method) compared to testing [70]. <p> However, other description of Fagan's method uses the preparation phase for both individual comprehension and examination activities, while the group meeting is used for consolidation (defects logging) <ref> [41] </ref>. With the framework, one can now compare and contrast similarities and differences between FTR methods by studying their review phases and the underlying review components that embody each phase. Furthermore, the impact of individual review factors can be empirically evaluated through proper experimental design.
Reference: [42] <author> Howard E. Dow and James S. Murphy. </author> <title> Detailed product knowledge is not required for a successful formal software inspection. </title> <booktitle> In Proceedings of the Seventh Software Engineering Process Group Meeting, </booktitle> <address> Boston, MA., </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Martin90 [97] 27% 4 N-Fold inspection (N=1) (Req Doc) Schneider92 [125] 35.1% 4 N-Fold inspection (N=1) (Req Doc) 77.8% 4 N=9 (Req Doc) Knight93 [92] 50% 2 Phased Inspection (Phase 6) (Code) Porter94 [115] 25% 3 Checklist (Req doc) 32% 3 AdHoc (Req doc) 50% 3 Scenario (Req doc) Dow94 <ref> [42] </ref> 20%-46% 4,7 Fagan (Code) Chapter 7 42.8% 3 EGSM (Code) 46.4% 3 EIAM (Code) 2.7 Summary and conclusions This chapter has presented a survey of software reviews, their differences in terminologies and actual practices, the psychological factors underlying the practices, and their roles in the software engineering field as a <p> However, some researchers believe that comprehension does correlate with examination. For example, Rifkin and Diemel found that applying a program comprehension technique can improve inspection effectiveness (finding more errors) [119]. However, Dow and Murphy found comprehension (i.e., detailed product knowledge) is not a prerequisite for an effective inspection <ref> [42] </ref>. Similarly, some studies provide empirical results that relate preparation time to review outcomes [19, 2, 24, 74]. However, they generally do not specify whether reviewers should concentrate on simply learning the materials or on looking for errors during the preparation phase. In the same way, examination often implies consolidation.
Reference: [43] <author> Robert H. Dunn. </author> <title> Software Defect Removal. </title> <type> McGraw-Hill, </type> <institution> Inc., ITT Advanced Technology Center, </institution> <year> 1984. </year>
Reference-contexts: For example, some descriptions of Fagan's method explicitly advocate the use of paraphrasing for the inspection technique [52, 124, 34], while others disregard this technique [74]. Some advocate the use of common error checklists when examining source materials [74, 57], others also advocate the use of selective test cases <ref> [1, 43] </ref>. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection [74, 98, 142]. <p> Many other studies, however, provide conflicting and/or anecdotal explanation of the causal factors underlying review effectiveness. For example, Basili attribute review effectiveness to a technique called stepwise abstraction [8, 7]. Dunn, Peele, Ackerman et al. attribute the success of software review to the presence of group synergy <ref> [43, 114, 2] </ref>, however, Humphrey reported that 75% of errors are found during individual preparation rather than in the group meeting [74]. Eick et al. also reported that 10% of errors are found during group meeting while 90% of errors are found during preparation [47]. <p> Thus, they consider these factors very important for a successful review [110]. Finally, some researchers attribute review effectiveness to paraphrasing, or the use of selective test cases, or checklists <ref> [124, 1, 43, 57] </ref>, or group interaction/composition [10, 97]. In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. <p> Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy <ref> [43, 114, 2] </ref>. However, some review experiences/studies found that more errors are caught in individual process than group process [74, 142, 115, 47]. Many other review methods (namely, Fagan's code inspection and its variations) usually include some combination of individual and group processes. <p> EGSM will detect significantly more errors than EIAM. The rationale for this hypothesis is that the presence of group synergy (in EGSM) will lead to the discovery of more errors than in nominal groups (EIAM) as suggested by many FTR practices <ref> [43, 114, 2] </ref>. 2. H2: There will be significant differences in detection cost between EGSM and EIAM. EGSM will cost significantly more than EIAM.
Reference: [44] <author> Michael Dyer. </author> <title> The Cleanroom Approach to Quality Software Development. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1992. </year>
Reference-contexts: A similar technique is formal mathematical proofs. Some people consider FTR informal compared to mathematical proofs, however, some FTR methods are combined with mathematical proofs resulting in an even more effective review process. One such example is Verification-Based Inspection that incorporates correctness verification method into the inspection practice <ref> [46, 44] </ref>. This technique is used in the Cleanroom development method to produce zero defect software [94]. Britcher also proposes a similar approach of incorporating correctness arguments into the program which are later checked by inspections [16]. <p> time T:pacing NASA's Overview Preparation Meeting Third-hour Inspection O:comprehension O:examination O:examination O:discussion [108] C:group C:individual C:group C:group S:STSP S:ASYNC S:STSP S:STSP R:author R:reviewer R:reader R:? T:presentation T:? T:paraphrasing T:? E/X:? E/X:? X:time X:time Humphrey's Review Personal O:examination Review I:no T:checklist,SPC [76] X:source reviewed& items checked Verification Inspection: ? Inspection O:examination <ref> [44] </ref> C:individual S:ASYNC R:reviewer T:stepwise verification E/X:? FTArm Orientation: Private: Public: Consolidation: Meeting: [83] O:comprehension O:comp,exam O:consol O:consol O:consol C:group C:indiv,group C:group C:individual C:group S:STSP S:ASYNC S:ASYNC S:ASYNC S:SYNC R:producer R:reviewer,producer R:moderator R:moderator R:moderator T:presentation T:question-answer, T:Delphi T:tools T:voting free exam E:unresolved E:source readiness E:source availability E:source nodes E:commentary issues remained <p> The SPC technique requires one to collect, measure, and analyze review metrics for future improvement. The exit criteria includes a fully reviewed source program and that all items in the checklist have been checked. 38 3.2.13 Verification Inspection Verification Inspection <ref> [44, 45] </ref> is a FTR method used in conjunction with Cleanroom software development methodology. The review process, unfortunately, is not well described in the literature. It consists of at least one phase, where individual inspectors examine the work product using a technique known as stepwise verification.
Reference: [45] <author> Michael Dyer. </author> <title> Verification-based inspection. </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <volume> 2 </volume> <pages> 418-427, </pages> <year> 1992. </year>
Reference-contexts: Over the past several years, many different forms of inspections or formal reviews have emerged. Some of these include: Structured Walkthrough [149], Active Design Reviews [110], Humphrey's inspection [74], Verification-Based Inspection <ref> [45] </ref>, and so forth. Throughout this dissertation, the term formal technical review (FTR) is used to refer to a general class of reviews, which includes the specific review techniques described above. <p> The SPC technique requires one to collect, measure, and analyze review metrics for future improvement. The exit criteria includes a fully reviewed source program and that all items in the checklist have been checked. 38 3.2.13 Verification Inspection Verification Inspection <ref> [44, 45] </ref> is a FTR method used in conjunction with Cleanroom software development methodology. The review process, unfortunately, is not well described in the literature. It consists of at least one phase, where individual inspectors examine the work product using a technique known as stepwise verification.
Reference: [46] <author> Michael Dyer and A. Kouchakdjian. </author> <title> Correctness verification: alternative to structural software testing. </title> <journal> Information and Software Technology, </journal> <volume> 32(1) </volume> <pages> 53-59, </pages> <month> January </month> <year> 1990. </year> <month> 225 </month>
Reference-contexts: A similar technique is formal mathematical proofs. Some people consider FTR informal compared to mathematical proofs, however, some FTR methods are combined with mathematical proofs resulting in an even more effective review process. One such example is Verification-Based Inspection that incorporates correctness verification method into the inspection practice <ref> [46, 44] </ref>. This technique is used in the Cleanroom development method to produce zero defect software [94]. Britcher also proposes a similar approach of incorporating correctness arguments into the program which are later checked by inspections [16].
Reference: [47] <author> Stephen G. Eick, Clive R. Loader, M. David Long, Scott A. Vander Wiel, and Lawrence G. Votta. </author> <title> Estimating software fault content before coding. </title> <booktitle> Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 59-65, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Eick et al. also reported that 10% of errors are found during group meeting while 90% of errors are found during preparation <ref> [47] </ref>. Myers and Parnas attribute the success of review to the synergy effect between the producer and the reviewers [106, 110], yet others discourage active participation of the producer during the meeting [1, 124]. <p> Control charts are also commonly used to evaluate the performance of an inspection process statistically [22, 74, 133]. Others use mathematical models to predict remaining errors after inspections <ref> [23, 47] </ref>. The principle of continuous process improvement also applies to the inspection process itself. This improvement can be achieved in the same way using statistical process control as discussed above. <p> Generally, the data in Table 2.4 shows that performance varies widely depending on the review method used. The performance also appears to vary according to group size. For example, Eick's design review found the average individual performance was only 12.5% compared to 75% with a group of size 8 <ref> [47] </ref>; the same is true for N-fold inspection where group replications can increase performance significantly [125] (see also Table 2.4). This observation is also consistent with Weller's study that found four-person teams were twice as effective, and more than twice as efficient as three- person teams [147]. <p> The FTR framework described in the next chapter is an approach for conducting such an investigation. 24 Table 2.4: Empirical findings of FTR References Efficiency Group-size Method Hetzel76 [70] 37.3% 1 Code reading Myers78 [105] 38% 3 Code walkthrough Eick92 <ref> [47] </ref> 75% 8 Design review Martin90 [97] 27% 4 N-Fold inspection (N=1) (Req Doc) Schneider92 [125] 35.1% 4 N-Fold inspection (N=1) (Req Doc) 77.8% 4 N=9 (Req Doc) Knight93 [92] 50% 2 Phased Inspection (Phase 6) (Code) Porter94 [115] 25% 3 Checklist (Req doc) 32% 3 AdHoc (Req doc) 50% 3 <p> Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy [43, 114, 2]. However, some review experiences/studies found that more errors are caught in individual process than group process <ref> [74, 142, 115, 47] </ref>. Many other review methods (namely, Fagan's code inspection and its variations) usually include some combination of individual and group processes. The Selective subgroup process has also been shown to provide unique benefits, such as a significant increase in review performance in N-Fold inspection [97]. <p> In other words, it has the objective of pure consolidation, where new faults are not expected to be found. Eick et al.'s study Eick et al.'s study investigates the number of undiscovered faults remaining in the document after inspection <ref> [47] </ref>. The inspection method involves two phases: preparation and meeting. In the preparation phase, each reviewer individually searches for faults in the documents. Thus, this phase has the objective of individual examination. No specific techniques are described. <p> They recommend that group meetings should be down-sized to include only the moderator and the producer in order to increase the cost effectiveness of FTR. In other words, the meeting should not involve all members of the group. Similar findings were also obtained by Eick et al. <ref> [47] </ref>. While these conclusions appear contradictory, they may result from the differing FTR methods used by Fagan and Votta. <p> has the potential to improve overall review performance, since the individual 152 participants/groups tend to discover different errors (see also section 7.6.2 ). 7.6.3 Related studies As described in Chapter 6, one objective of this study was to follow up the empirical study by Votta, Porter and Eick et al. <ref> [142, 116, 47] </ref>. These studies found that group meetings (collection meeting) are not as beneficial as claimed by Fagan [52]. However, as described in Chapter 6, the review methods used in these studies are slightly different from Fagan's method. <p> In both Hetzel's and Basili's method, the examination technique (code reading) was compared to testing; no group interaction was involved. In Porter's study, three different examination (detection) techniques were compared: Ad Hoc, Checklist and Scenario [115]. Myers', Eick's, and Knight's studies <ref> [105, 47, 92] </ref> investigated review methods as a whole (i.e., without focusing on specific review components). Myer's study compared code walkthrough/inspection method with testing. Eick's study involved a two-phase review method to estimate the number of software faults before coding; Knight's study investigated the effectiveness of Phased Inspection method.
Reference: [48] <author> C. Ellis, S. Gibbs, and G. Rein. </author> <title> Groupware: Some issues and experiences. </title> <journal> Communications of the ACM, </journal> <pages> pages 39-56, </pages> <month> Jan </month> <year> 1991. </year>
Reference-contexts: It involves syn- chronicity and the degree of collaboration among group members. The synchronicity component is compatible with the taxonomy of a group system or a meeting support system, which incorporates time and space dimensions (i.e., same-time same-place, same-time different-place, etc) <ref> [48] </ref>. The degree of collaboration is compatible with an extension of such taxonomy, which includes multiple individual sites, one group size, or multiple group sites [109]. However, unlike typical group systems, the Interaction component of FTR framework also includes Roles. The latter was again inspired by Fagan's inspection [52].
Reference: [49] <author> M. Elwart-Keys, D. Halonen, M. Horton, R. Kass, and P. Scott. </author> <title> User interface requirements for face to face groupware. </title> <booktitle> Proceedings of the ACM/SIGCHI Conference on Human Factors in Computing (CHI'90), </booktitle> <pages> pages 295-301, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: The goal of these systems is to facilitate meeting activities with proper computational support so as to make the meeting more productive. 75 Some examples of these systems include Electronic Meeting System or GroupSystem [109], Colab [132], and Capture Lab <ref> [49] </ref>. The major facilities provided by these systems include a shared electronic whiteboard (e.g., Colab), or a large screen with video projector (e.g., Capture Lab and EMS), and a set of software tools for supporting meeting activities, such as the brainstorming, organizing, evaluating, proposing, and arguing activities [132, 109]. <p> However, most systems also provide a private workspace, where the meeting participants may work individually and privately. Some of these private activities include taking notes, making short calculations, extending work on one piece of the group tasks <ref> [49] </ref>, or parallel communication where individual participants express their opinions simultaneously without waiting for someone else to finish speaking [109]. In general, meeting support systems bear some resemblance with CSRS.
Reference: [50] <editor> D. Engelbart. Authorship provisions in Augment. </editor> <booktitle> In Proceedings of the IEEE Computer Society International Conference-CompCon. IEEE, </booktitle> <year> 1984. </year>
Reference-contexts: One of the goals is to provide the users with a medium to represent a collection of related ideas, and to discuss such ideas in a structured manner. Some examples of hypertext systems include NLS/Augment <ref> [50] </ref> NoteCards [68], gIBIS [30], and Neptune [36]. These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor [67], Virtual Notebook [77], SEPIA [66], and Clare [144]. <p> /* Size of symbol table */ 029:#define BLANK6 " " 030:#define BLANK8 " " 031:#define BLANK18 " " 032:#define BLANK31 " " 033: 034:typedef unsigned char BOOLEAN; 035:typedef char CHAR4 [4]; 036:typedef char CHAR6 [6]; 037:typedef char CHAR8 [8]; 038:typedef char CHAR18 [18]; 039:typedef char CHAR31 [31]; 040:typedef char CHAR50 <ref> [50] </ref>; 041:typedef char CHAR66 [66]; 042: 043:typedef struct SOURCETYPE - 044: /* source line and subfields */ 045: CHAR66 line; 046: BOOLEAN comline; 047: CHAR8 labl; 048: CHAR6 operation; 049: CHAR18 operand; 050: CHAR31 comment; 051:- SOURCETYPE; 052: 053:typedef struct REC_SYMTABTYPE - 054: CHAR8 symbol; 055: int address; 056:- REC_SYMTABTYPE; 057: <p> " 032:#define BLANK8 " " 033:#define BLANK18 " " 034:#define BLANK30 " " 035:#define BLANK31 " " 036: 037:typedef unsigned char BOOLEAN; 038: 039:typedef char CHAR4 [4]; 040:typedef char CHAR6 [6]; 041:typedef char CHAR8 [8]; 042:typedef char CHAR18 [18]; 043:typedef char CHAR30 [30]; 044:typedef char CHAR31 [31]; 045:typedef char CHAR50 <ref> [50] </ref>; 046:typedef char CHAR66 [66]; 047: 048:typedef struct SOURCETYPE - 049: /* SOURCE LINE AND SUBFIELDS */ 050: CHAR66 line; 051: BOOLEAN comline; 052: CHAR8 labl; 053: CHAR6 operation; 054: CHAR18 operand; 055: CHAR31 comment; 056:- SOURCETYPE; 057: 058:typedef struct OBJTYPE - 059: /* OBJECT CODE, LENGTH, AND TYPE */ 060:
Reference: [51] <author> J.R. Ensor, S.R. Ahuja, D.N. Horn, and S.E. Lucco. </author> <title> The Rapport multimedia conferencing system A software overview. </title> <booktitle> Proceedings of the Second IEEE Conference on Computer Workstations, </booktitle> <pages> pages 52-58, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Conferencing systems Conferencing systems provide support for interactive, real-time, and distributed meeting activities, or same-time different-place meetings. Conceptually, they provide virtual meeting rooms <ref> [51] </ref>, where participants from dispersed locations can discuss and manipulate the materials/applications displayed on shared screens. Examples of such systems include Rapport [51], Mermaid [145], TeamWorkstation [78], and Dolphin [134]. Some systems, for example, Dolphin, also include same-time same-place meetings. <p> Conferencing systems Conferencing systems provide support for interactive, real-time, and distributed meeting activities, or same-time different-place meetings. Conceptually, they provide virtual meeting rooms <ref> [51] </ref>, where participants from dispersed locations can discuss and manipulate the materials/applications displayed on shared screens. Examples of such systems include Rapport [51], Mermaid [145], TeamWorkstation [78], and Dolphin [134]. Some systems, for example, Dolphin, also include same-time same-place meetings. The basic facilities of conferencing systems usually include real-time voice and video channels, which are implemented through separate communication networks (e.g., a wide area network).
Reference: [52] <author> Michael E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM System Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: When the artifacts involve mainly program code, this technique is similar to traditional testing, except that the code is analyzed instead of executed. The formal practice of FTR was first developed by Michael E. Fagan in IBM Kingston, NY <ref> [52] </ref> using the name inspection. The basic premise of inspection is to detect and resolve errors as early as possible in the development process, and thus, to prevent them from propagating to the next phase of the process. <p> Basically, it serves as an early detection mechanism for potential errors before reaching the later stages. In his original article, Fagan reported a 23% increase in coding productivity, and 25% reduction in schedule plans (i.e., shorten the development schedule) <ref> [52] </ref>. FTR also has the potential to improve the quality and productivity of software personnel. Using an experimental study, Bisant and Lyle showed that the groups who used a two-person FTR method improved their programming speed significantly [10]. <p> For example, the meeting review factor in Fagan's method <ref> [52] </ref> is intended for error hunting, while in Humphrey's method [74] it is used for error logging. The following subsections describe these problems in detail. 1.3.1 Variations and ambiguities in FTR methods and practices First, the literature does not define FTR consistently. <p> The objective of the preparation phase for both inspection and walkthrough is education. The objective of the meeting phase for inspection is solely to find errors, while the meeting phase of walkthrough has the objectives of education and discussion of alternative solutions <ref> [52] </ref>. Second, although all existing FTR methods provide some descriptions of their review procedures, in practice there can be many variations in the way in which they are performed. In fact, the literature on FTR often describes conflicting review practices for the same method. <p> In fact, the literature on FTR often describes conflicting review practices for the same method. For example, some descriptions of Fagan's method explicitly advocate the use of paraphrasing for the inspection technique <ref> [52, 124, 34] </ref>, while others disregard this technique [74]. Some advocate the use of common error checklists when examining source materials [74, 57], others also advocate the use of selective test cases [1, 43]. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. <p> Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection [74, 98, 142]. Regarding the participants' interaction, some state the importance of having group meetings <ref> [52] </ref>, while others intentionally eliminate the need for group discussion [127, 110]. Also, many review methods demand a separate preparation phase where reviewers learn source materials before starting error hunting [1, 74, 52], while others consider it optional or unnecessary [92]. <p> Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion [127, 110]. Also, many review methods demand a separate preparation phase where reviewers learn source materials before starting error hunting <ref> [1, 74, 52] </ref>, while others consider it optional or unnecessary [92]. In conclusion, the wide variations of review practice, even within the same ostensible method, such as Fagan's method, indicate that the methods are defined ambiguously or unclear. <p> The term walkthrough itself has been used in the literature in a variety of ways. Some refer it as structured walkthroughs [150]. Generally, the literature describes walkthrough as an undisciplined process without advanced preparation on the part of reviewers, and the meeting focuses on education purposes <ref> [27, 52] </ref>. However, many authors consider it a disciplined and formal review process [106, 150, 3]. 3. Round-robin reviews. <p> The latter is similar to walkthroughs, except that all review participants play the role of presenter as well. 4. Inspection. Inspection is a well-planned and well-defined group review process that requires a high degree of responsibility from the participants <ref> [52, 53] </ref>. It is the most cited review method in the literature. The method was first performed by Fagan at IBM [52]. Unlike other review methods, Inspection is often used as process control, namely, as a means to control the quality and productivity of the development process. <p> Inspection. Inspection is a well-planned and well-defined group review process that requires a high degree of responsibility from the participants [52, 53]. It is the most cited review method in the literature. The method was first performed by Fagan at IBM <ref> [52] </ref>. Unlike other review methods, Inspection is often used as process control, namely, as a means to control the quality and productivity of the development process. The method also produces a formal report detailing the defects found and the total effort expended by the participants. <p> Software inspection with its data collection mechanism implements this principle. The use of inspection for process control was first performed by Fagan <ref> [52] </ref>. Fagan decomposed a programming process into several levels (e.g., high-level design, low-level design, coding, testing, etc), then instituted inspections as checkpoints between these levels. An entry and exit criteria to and from these levels were also defined. <p> The results showed a 23% increase in the coding productivity. Another study at Aetna Life and Casualty found the detection efficiency of inspection was 82% compared to 18% in testing (the testing, however, was performed after the inspection) <ref> [52] </ref>. In another article, Fagan reported that the inspection process at IBM UK found 93% of all defects. At Standard Bank of South Africa and American Express, each found over 50% of all defects. However, the former also found 95% reduction in corrective maintenance cost. <p> The Test is included here for comparison purposes. Obviously, the data suggests inspection is much cheaper than testing, and its detection efficiency is also relatively high (&gt;=75%). 21 Table 2.3: FTR experiences in industry References Organization Cost Efficiency Method Inspect Test Inspect Test Fagan76 <ref> [52] </ref> Aetna 82% 18% Fagan Fagan86 [53] IBM UK 93% Fagan Standard Bank &gt;50% Fagan AMEX &gt;50% Fagan Ackerman89 [1] Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% <p> It does not include administrative procedures that are typical in industry practice, such as planning, selecting moderator, etc. It does not incorporate other software development activities, such as configuration management, coding, testing, etc. Some FTR methods, such as Fagan's <ref> [52] </ref> and Humphrey's [74] methods, include a rework phase as part of their FTR processes. However, this phase will not be included in the model, because it does not deal with the review activity per se. <p> Correlation is the activity that combines issues (without validating) raised by individual reviewers privately. Discussion is the activity intended to resolve issues and search for alternative solutions. In Fagan's method <ref> [52, 53] </ref>, the comprehension activity is known as preparation, whereas the examination activity is known as defect finding. Consolidation can be found, for example, in FTR methods suggested by Humphrey [74], Gilb [60], Votta and Porter [142, 116]. <p> During defect finding, one may obtain additional comprehension while examining the code. However, when the objective of a phase is stated as comprehension/education, reviewers will focus their efforts solely to understand the review materials, and errors found during this activity (if any) are usually insignificant <ref> [52] </ref>. As an illustration, when doing code comprehension, people focus on what are the purpose of these statements ?, instead of whether the statements are correct, which is what is asked for when looking for errors. <p> In this case, the participants are expected to both fully understand the materials and find defects at the same time. Some researchers suggest that separating comprehension from examination activity makes a review process more effective and productive <ref> [52, 74, 1, 124] </ref>. For example, in the code inspection method, there is a phase called overview, where the producer educates participants about review materials, and preparation, where the reviewers learn the materials individually [52]. The stated objective of these phases is comprehension. <p> For example, in the code inspection method, there is a phase called overview, where the producer educates participants about review materials, and preparation, where the reviewers learn the materials individually <ref> [52] </ref>. The stated objective of these phases is comprehension. As expressed by Ackerman, A half-hour or hour presentation by the producer can often save the inspectors two or three hours each in achieving the desired level of understanding of the inspection materials [1]. <p> Fagan also stated that A team is most effective if it operates with only one objective at a time <ref> [52] </ref>. However, many other inspection methods use the preparation phase for both comprehension and examination activities, namely, individual reviewers are asked to both understand the review materials and find errors during this phase [74, 60]. <p> However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors [142, 116, 115]. Finally, group discussion to resolve issues or to find solutions is often considered detrimental to the review process and thus, should be forbidden <ref> [52, 124, 60] </ref>. This happens, for example, during the inspection meeting phase. <p> Roles Roles describe the responsibility of individual group members. Fagan described the importance of roles as: The inspection team is best served when its members play their particular roles, assuming the particular vantage point of those roles <ref> [52] </ref>. Typical roles in FTR include moderator (who leads meetings), producer (who produces source materials), reviewer (who reviews source materials), reader (who paraphrases the materials) and recorder/scribe (who records the results) [52, 1]. <p> Typical roles in FTR include moderator (who leads meetings), producer (who produces source materials), reviewer (who reviews source materials), reader (who paraphrases the materials) and recorder/scribe (who records the results) <ref> [52, 1] </ref>. Other domain specific roles include Designer (who checks for defects in the design), Tester (who checks the materials from the testing point of view) [52], and other specialists [110]. Many people consider review roles as an important review factor. <p> Other domain specific roles include Designer (who checks for defects in the design), Tester (who checks the materials from the testing point of view) <ref> [52] </ref>, and other specialists [110]. Many people consider review roles as an important review factor. <p> Thus, one may have different techniques for individual examinations, group examination, individual comprehension, group comprehension, and so forth. Some examples of examination techniques include the paraphrasing technique used during the group meeting in Fagan's inspection <ref> [52] </ref>, selective test cases (test data) [106, 3], checklists of common errors (i.e., classes of software errors) used during the individual examination phase in Humphrey's method [74], active checklist used during individual review phase in Active Design Review [110], scenarios used in Porter and Votta's experiment [115], the stepwise abstraction technique <p> T:? E:completed form E:? Structured Preparation: Walkthrough: Walkthrough O:comprehension O:examination [150] C:individual, C:group selective subgroup S:ASYNC,SYNC S:SYNC R:reviewer,producer R:producer T:reading,discussion T:paraphrasing E/X:? E/X:? Round-robin Phase i Phase i+1 [69] O:examination O:examination C:subgroup C:subgroup S:ASYNC S:ASYNC R:reviewer i R:reviewer i+1 T:? T:? Fagan's Overview: Preparation: Inspection: Inspection O:comprehension O:comp (edu) O:examination <ref> [52] </ref> C:group C:individual C:group S:STSP S:ASYNC S:STSP R:producer R:reviewer R:producer/reader T:presentation T:reading&checklist T:paraphrasing E:clean compilation X:time X: time Humphrey's Overview: Preparation: Pre-meeting: Meeting: Inspection O:comprehension O:comp & exam O:correlation O:consolidation [74] C:group C:individual C:subgroup C:group S:STSP S:ASYNC S:ASYNC S:STSP R:producer R:reviewer R:moderator&producer R:producer T:presentation T:reading & T:? T:presentation std checklist E:clean <p> No specific examination techniques are described in the literature. Hart also pointed out that this method lacks any educational goal (i.e., there is no comprehension phase) [69]. The entry/exit criteria is not specified. 34 3.2.4 Fagan's Inspection The classical Fagan's Inspection <ref> [52] </ref> begins with the Overview phase. This phase has the objective of comprehension, and involves group interaction with STSP synchronicity. The comprehension technique used in this phase is presentation: the producer presents review materials and the reviewers ask questions about the materials. <p> The framework can also be used to find inconsistencies in the practice of the same review method. For example, in Fagan's original description of code inspection, the preparation phase involves only individual comprehension, and the examination activity is performed during group meeting <ref> [52] </ref>. However, other description of Fagan's method uses the preparation phase for both individual comprehension and examination activities, while the group meeting is used for consolidation (defects logging) [41]. <p> In general, there have been no studies that investigate the similarities and differences of FTR methods by looking at the underlying components that shape the review process, i.e., the FTR framework. The idea of this framework, however, was inspired by Fagan's inspection <ref> [52] </ref>. In his original article, Fagan stated that one should separate the objectives of inspection phases to keep the inspection team focused on one objective at a time. <p> The degree of collaboration is compatible with an extension of such taxonomy, which includes multiple individual sites, one group size, or multiple group sites [109]. However, unlike typical group systems, the Interaction component of FTR framework also includes Roles. The latter was again inspired by Fagan's inspection <ref> [52] </ref>. The Technique component was suggested by existing review methods that use different review tactics, such as checklist, paraphrasing, etc. Freedman and Weinberg also suggested different review tactics to achieve the overall review objectives. <p> The entry and exit criteria components of FTR framework was again inspired by Fagan's inspection <ref> [52] </ref>. However, in Fagan's inspection, the entry/exit criteria is applied to the entire review process, as supposed to the individual phases within the process. <p> Checklist nodes contain verification aids. In manual review practices, such as Fagan's method <ref> [52] </ref>, reviewers are often required to consult this checklist when examining source nodes. CSRS provides a definition language to configure the checklists descriptively (see section 4.1.3). <p> Fagan, in his pioneering work on code inspection, observed that group meetings are more effective in finding defects than individual activities (i.e., during the individual preparation phase) <ref> [52] </ref>: Sometimes flagrant errors are found during this operation (i.e., preparation phase), but in general, the number of errors found is not nearly as high as in the inspection operation (i.e., meeting phase) In contrast, recent empirical studies by Votta and Porter [142, 116, 115] found that group meetings may not <p> These studies found that group meetings (collection meeting) are not as beneficial as claimed by Fagan <ref> [52] </ref>. However, as described in Chapter 6, the review methods used in these studies are slightly different from Fagan's method. For example, in Votta's method, the objective of the meeting phase is collection/consolidation, whereas in Fagan's method, the objective is examination/defects finding. Furthermore, Fagan's method also involves a paraphrasing technique.
Reference: [53] <author> Michael E. Fagan. </author> <title> Advances in software inspections. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(7):744-751, </volume> <month> July </month> <year> 1986. </year>
Reference-contexts: Fagan reported that since the introduction of FTR (software inspection) in 1974, IBM has achieved significant improvements in quality. The number of defects per thousand lines of code of its Sys- tem/370 was reduced by two-thirds <ref> [53] </ref>. The Space Shuttle software produced by IBM Federal Systems Divisions has achieved very low error rates (0.11 errors per thousand lines of source code excluding comments). <p> The latter is similar to walkthroughs, except that all review participants play the role of presenter as well. 4. Inspection. Inspection is a well-planned and well-defined group review process that requires a high degree of responsibility from the participants <ref> [52, 53] </ref>. It is the most cited review method in the literature. The method was first performed by Fagan at IBM [52]. Unlike other review methods, Inspection is often used as process control, namely, as a means to control the quality and productivity of the development process. <p> Many studies have found a correlation between the effectiveness of inspections and certain inspection metrics. For example, the number of defects found decline with increasing inspection or preparation rates <ref> [53, 74, 24] </ref>, with the optimum inspection rate is around 150 LOC/hour [124]. This data is then used as a guideline for controlling the inspection process. Thus for example, if the pace of an inspection is well above the standard guideline 150 LOC/hour, the materials need to be reinspected. <p> At Standard Bank of South Africa and American Express, each found over 50% of all defects. However, the former also found 95% reduction in corrective maintenance cost. Fagan also noted that these two organizations did not use trained moderators, and Standard Bank of South Africa only conducted code inspections <ref> [53] </ref>. * Ackerman reported that two organizations that used inspections found inspection was cheaper than testing: a banking computer-services firm found that it took 4.5 hours to eliminate a defect by unit testing compared to 2.2 hours by inspection; an operating-system development organization for a large mainframe manufacturer found the average <p> Obviously, the data suggests inspection is much cheaper than testing, and its detection efficiency is also relatively high (&gt;=75%). 21 Table 2.3: FTR experiences in industry References Organization Cost Efficiency Method Inspect Test Inspect Test Fagan76 [52] Aetna 82% 18% Fagan Fagan86 <ref> [53] </ref> IBM UK 93% Fagan Standard Bank &gt;50% Fagan AMEX &gt;50% Fagan Ackerman89 [1] Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan <p> Thus, several questions remain unanswered: Is everyone performing the same FTR? Which underlying components differentiate one review method from the others? For example, Fagan hinted that the reason why one inspection was not so successful was because it could not get trained moderators <ref> [53] </ref>. In the case of the shuttle software experience which delivered a very low error rate, Mills indicated that formal correctness proofs may have contributed to the overall inspection quality [102]. <p> Correlation is the activity that combines issues (without validating) raised by individual reviewers privately. Discussion is the activity intended to resolve issues and search for alternative solutions. In Fagan's method <ref> [52, 53] </ref>, the comprehension activity is known as preparation, whereas the examination activity is known as defect finding. Consolidation can be found, for example, in FTR methods suggested by Humphrey [74], Gilb [60], Votta and Porter [142, 116].
Reference: [54] <author> George A. Ferguson and Yoshio Takane. </author> <title> Statistical Analysis In Psychology And Education. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> 6 edition, </address> <year> 1989. </year>
Reference-contexts: the total number of distinct valid errors detected by the groups. * Effort/Review Time: the total effort or review time spent by the groups. 110 6.2.5 Statistical tests The main hypotheses and some of the research questions were tested using non-parametric test of significance, specifically the Wilcoxon signed rank test <ref> [54] </ref>. This non-parametric test does not make any assumption regarding the underlying distribution of the data. It is based on the rank of differences between each pair of observations in the dataset. The data analysis proceeds in the following way.
Reference: [55] <author> F. Flores, M. Graves, B. Hartfield, and T Winograd. </author> <title> Computer systems and the design of organization interaction. </title> <journal> ACM Transaction on Office Information System, </journal> <volume> 6(2) </volume> <pages> 153-172, </pages> <year> 1988. </year>
Reference-contexts: The messages themselves are typically typed messages and include a set of fields or attributes. The system compares these values with the user's supplied criteria in order to generate appropriate actions. Examples of message systems include Information Lens [95], Lotus Notes [35], Coordinator <ref> [55] </ref>, Strudel [128], MailTray [120], Atomicmail [15], and Active Mail [65]. Message systems are typically generic systems for building group applications, such as meeting schedulers, information finders, and tools for project management, task tracking, software defect tracking, and collaborative writing.
Reference: [56] <author> D. P. Freedman and G. M. Weinberg. </author> <title> Handbook of Walkthroughs, Inspections and Technical Reviews. Little, </title> <publisher> Brown, </publisher> <address> 3 edition, </address> <year> 1982. </year>
Reference-contexts: Substantial credit was given to Fagan's inspection [107] Freedman and Weinberg reported that in large systems, reviews have reduced the number of errors reaching the testing stages by a factor of 10 <ref> [56] </ref>. This also translates to reduction in testing costs by 50% to 80% including review costs. Several studies also provide empirical evidence that FTR can be more effective in detecting errors than traditional testing [127, 59]. <p> Walkthroughs. Walkthroughs can be viewed as presentation reviews, in which a review participant, usually the developer of the software being reviewed, narrates a description of the software and the remainder of the review group provides their feedback throughout the presentation <ref> [56, 27, 60] </ref>. The term walkthrough itself has been used in the literature in a variety of ways. Some refer it as structured walkthroughs [150]. <p> In other variation of Round-robin review, all participants meet at one room, and each participant takes a turn presenting or walking through a small section of the materials <ref> [56] </ref>. The latter is similar to walkthroughs, except that all review participants play the role of presenter as well. 4. Inspection. Inspection is a well-planned and well-defined group review process that requires a high degree of responsibility from the participants [52, 53]. <p> After code reviews were instituted, the average error rates of the next 6 programs was 0.82 Errors/100 LOC. Unfortunately, the review procedure is only described in the literature as a formal code review with a Cobol checklist <ref> [56] </ref>. * At IBM Federal System Division in Houston, software development for the 500 KLOC used aboard the space shuttle achieved a very low error rate of 0.11 errors per KLOC. This success was later attributed to Fagan's inspection. <p> AMEX &gt;50% Fagan Ackerman89 [1] Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 <ref> [56] </ref> Client Code review Myers88 [107] IBM 85% Fagan Doolan92 [41] Shell Research Doolan Kaplan95 [88] IBM 3.5 15-25 Kaplan Again some of the data in Table 2.3 may be misleading, since the testing took place after the inspection which had removed a substantial percentage of errors (see also Table 2.2). <p> Freedman and Weinberg also suggested different review tactics to achieve the overall review objectives. According to them, tactics are the smaller plans that may contribute to the achievement of the grand plan <ref> [56] </ref>. Some of their tactics include playing devil's advocate in criticizing review materials, raising one positive and one negative issues; bebugging (leaving some number of known bugs in the materials to test whether the reviewers are thorough in reviewing the materials), etc.
Reference: [57] <author> D. P. Freedman and G. M. Weinberg. </author> <title> Handbook of Walkthroughs, Inspections and Technical Reviews. Little, Brown, </title> <booktitle> 4th edition, </booktitle> <year> 1990. </year>
Reference-contexts: For example, some descriptions of Fagan's method explicitly advocate the use of paraphrasing for the inspection technique [52, 124, 34], while others disregard this technique [74]. Some advocate the use of common error checklists when examining source materials <ref> [74, 57] </ref>, others also advocate the use of selective test cases [1, 43]. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. <p> Thus, they consider these factors very important for a successful review [110]. Finally, some researchers attribute review effectiveness to paraphrasing, or the use of selective test cases, or checklists <ref> [124, 1, 43, 57] </ref>, or group interaction/composition [10, 97]. In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood.
Reference: [58] <author> Thane J. Frivold, Ruth E. Lang, and Martin W. Fong. </author> <title> Extending WWW for synchronous collaboration. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 28 </volume> <pages> 69-75, </pages> <year> 1995. </year>
Reference-contexts: Typical features include support for group annotation and authoring. Examples of these systems include NCSA Mosaic 2.6 with group annotation, HyperNews (WWW + Usenet News) [93], BRIO (xMosaic + PRDM) [121], CoReview (Mosaic + XTV)[96], and WWW + COMET <ref> [58] </ref>. The latter two systems provide teleconferencing and a synchronous whiteboard facility as well. CoReview specifically supports a collaborative review process by spatially separated reviewers. It also provides support for two roles, the chair and reviewer.
Reference: [59] <author> Tom Gilb. </author> <booktitle> Principles of Software Engineering Management. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: This also translates to reduction in testing costs by 50% to 80% including review costs. Several studies also provide empirical evidence that FTR can be more effective in detecting errors than traditional testing <ref> [127, 59] </ref>. But more importantly, FTR can be applied to any intermediate software products that are untestable. In addition to reducing software defects, FTR can also improve other qualities of software products, such as portability, understandability, modifiability (maintainability), testability, etc.
Reference: [60] <author> Tom Gilb and Dorothy Graham. </author> <title> Software Inspection. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Using an experimental study, Bisant and Lyle showed that the groups who used a two-person FTR method improved their programming speed significantly [10]. Finally, FTR also plays a key role in defect prevention and process improvement <ref> [60] </ref>. The results of reviews can be used to institute continuous process improvement of the entire software development process. <p> Gilb and Graham described the difference between inspection and walkthrough as its focus: inspection focuses on defect identification, while walkthrough focuses on learning, i.e., walkthrough is generally a training process <ref> [60] </ref>. Bysant and Lyle described both inspection and walkthrough as having the main objective of error detection, however, walkthrough often degrades into a problem solving session. <p> Walkthroughs. Walkthroughs can be viewed as presentation reviews, in which a review participant, usually the developer of the software being reviewed, narrates a description of the software and the remainder of the review group provides their feedback throughout the presentation <ref> [56, 27, 60] </ref>. The term walkthrough itself has been used in the literature in a variety of ways. Some refer it as structured walkthroughs [150]. <p> Like walkthroughs, the term Inspections is also used in the literature in many different ways. For example, many researchers associate inspections with paraphrasing techniques, in which a designated reader paraphrases the materials statement by statement [106, 3, 124, 1], others downplay the role of reader <ref> [41, 101, 74, 60] </ref>. Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides [60]. <p> Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides <ref> [60] </ref>. <p> other review techniques, which include: (1) inspection focuses solely on defect findings instead of learning/education or group consensus; (2) metrics collection and analysis are always part of an inspection process; and (3) source artifacts to be inspected should be settled down technically, or otherwise it is a waste of time <ref> [60] </ref>. In addition to the specific review methods above, there are different classes of reviews: 1. Informal versus Formal reviews. Informal reviews are a class of reviews that typically occur spontaneously among peers and in which the reviewers have no responsibility and do not produce a review report. <p> In most practices, the causal-analysis meeting is conducted as an extension to regular inspection, usually at the end of the inspection process <ref> [41, 60] </ref>. Finally, the principle of continuous process improvement must also occur at a personal level. Bisant and Lyle's study has shown that software reviews can improve programmers' performance significantly [10]. The most significant development for self-improvement initiatives is Humphrey's Personal Software Process (PSP) [76]. <p> Because of this, one cannot afford to review all programs (either selected programs or some random portions). Gilb attributes the problems of inspection with incorrect execution of the process resulting from the confusion with other review techniques/terminologies <ref> [60] </ref>. Finally, most FTR experiences reported on Fagan style inspections. Only a few of them reported on non-Fagan inspections. One possible explanation is that Fagan inspection is often used as a competitive advantage in bidding for software contracts (as revealed by the Ackerman's survey [1]). <p> In practice, this is also reflected in one survey that found that while 84% of organizations claimed to perform inspection, none performed it exactly as specified by Fagan <ref> [10, 60] </ref>. In conclusion, the most cited problems of FTR in the literature include high cost and confusion about the review procedures. The former problem is usually justified through analysis demonstrating that this investment is more than recouped through decreases in downstream rework. <p> Most organizations who published their FTR data, unfortunately, did not mention the size of their groups. Gilb recommends that Inspection team size is four to five people including the leader for maximum effectiveness (percentage of total majors found in Inspections) <ref> [60] </ref>. This number, however, is more or less close to the one used in the empirical studies. <p> Discussion is the activity intended to resolve issues and search for alternative solutions. In Fagan's method [52, 53], the comprehension activity is known as preparation, whereas the examination activity is known as defect finding. Consolidation can be found, for example, in FTR methods suggested by Humphrey [74], Gilb <ref> [60] </ref>, Votta and Porter [142, 116]. Gilb refers to consolidation as defects logging, and Votta and Porter refer to it as collection. Correlation can be found in Humphrey's method [74]. Discussion can be found in the third hour meeting phase of some inspection methods [21, 108]. <p> However, many other inspection methods use the preparation phase for both comprehension and examination activities, namely, individual reviewers are asked to both understand the review materials and find errors during this phase <ref> [74, 60] </ref>. Unfortunately, no empirical studies have ever looked into the issue of whether separating comprehension from examination activity affects review effectiveness. However, some researchers believe that comprehension does correlate with examination. <p> For example, the meeting phase in Fagan's inspection, which has the objective of examination (hunting for defects), also includes consolidation implicitly, since defects are confirmed or rejected as they are discovered. Other researchers prefer to make these two activities separate (e.g., by having a separate defect-logging meeting) <ref> [74, 60] </ref>. Although consolidation does not involve examining new materials, some studies show that new errors are often discovered during this activity as a result of group interaction [41]. However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors [142, 116, 115]. <p> However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors [142, 116, 115]. Finally, group discussion to resolve issues or to find solutions is often considered detrimental to the review process and thus, should be forbidden <ref> [52, 124, 60] </ref>. This happens, for example, during the inspection meeting phase. <p> Another common technique is to let the meeting leader talk through review materials, while participants interrupt with the issues found during the private phase (sometimes called the document pacing technique) <ref> [60, 142, 115] </ref>. Still another technique is simply asking each participant in turn to report his/her issues [101], or to report and revise the issues as necessary following several rounds (i.e., Delphi technique [71]). 3.1.4 Entry/exit criteria The entry/exit criteria defines the starting or completion condition of a review phase. <p> S: STSP R: Reader2 R: Reader1 T: paraphrasing T: paraphrasing X: time X: time N-Fold Individual Meeting Consolidation Inspection O:examination O:examination O:consolidation [97] C: individual C:subgroup C:subgroup S:ASYNC S:STSP,ASYNC S:STSP,ASYNC R:reviewer R:? R:moderator T: checklist T:? T:? E/X: ? E/X: ? E/X:? Gilb's Indiv. check Logging meeting Inspection O:examination O:consolidation& <ref> [60] </ref> C:individual examination S:ASYNC C:group R:specialist S:STSP T:checklist,SPC R:moderator X: time T:pacing NASA's Overview Preparation Meeting Third-hour Inspection O:comprehension O:examination O:examination O:discussion [108] C:group C:individual C:group C:group S:STSP S:ASYNC S:STSP S:STSP R:author R:reviewer R:reader R:? T:presentation T:? T:paraphrasing T:? E/X:? E/X:? X:time X:time Humphrey's Review Personal O:examination Review I:no T:checklist,SPC [76] <p> As in Active Design Review, one may also view this phase as consisting of n additional subphases where the moderator meets with each subgroup leader. 37 3.2.10 Gilb's Inspection Gilb's Inspection <ref> [60] </ref> begins with the kickoff meeting, in which the inspector leader (i.e., moderator) describes administrative procedures of inspection. The actual inspection process begins with the Individual checking phase, where individual participants check the review materials for potential defects. Thus, this phase has the objective of examination. <p> FTArm allows the Reviewers to provide issue resolutions/actions for three reasons. First, they are generated asynchronously by individual Reviewers. Second, they often make the issue more understandable. Third, they eliminate the need for an additional third hour meeting to discuss issue resolutions as practiced by some FTR methods <ref> [60] </ref>. During this phase, the Reviewers are still allowed to ask questions to the Producer or other Reviewers concerning the artifacts that they find difficult to understand. The Reviewers may also consult checklists to help them look for potential defects. <p> 092:static int ASCII [256]; 093: 094:static FILE *srcfile, *objfile, *lisfile, *intfile; 095:static int LOCCTR; 096:static CHAR6 PROGNAME; 097:static int PROGSTART; 098: 099:/* GLOBALS USED ONLY BY P2_ASSEMBLE_INST */ 100: 101:static BOOLEAN FIRSTSTMT, ENDFOUND; 102: 103:/* GLOBALS USED ONLY BY P2_WRITE_OBJ */ 104: 105:static int TEXTSTART, TEXTADDR, TEXTLENGTH; 106:static char TEXTARRAY <ref> [60] </ref>; 107: 108:/* ERROR MESSAGES */ 109:static CHAR50 ERRMSG [MAXERRORS] = - 110: "ILLEGAL FORMAT IN LABEL FIELD ",/*ERRMSG [0]*/ 111: "MISSING OPERATION CODE ",/*ERRMSG [1]*/ 112: "ILLEGAL FORMAT IN OPERATION FIELD ",/*ERRMSG [2]*/ 113: "MISSING OR MISPLACED OPERAND IN START STATEMENT ",/*ERRMSG [3]*/ 114: "ILLEGAL OPERAND IN START STATEMENT ",/*ERRMSG
Reference: [61] <author> John W. Gintell, John Arnold, Michael Houde, Jacek Kruszelnicki, Roland McKenney, and Gerard Memmi. Scrutiny: </author> <title> A collaborative inspection and review system. </title> <booktitle> In Proceedings of the Fourth European Software Engineering Conference, </booktitle> <address> Garwisch-Partenkirchen, Germany, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Only few studies have attempted to address this issue as described earlier [142, 116]. The experiment described in this thesis also investigates this question, although from a different perspective. 1.3.3 Variations and inflexibility in review systems implementation Recently, many computer-supported review systems have emerged, such as, ICICLE [17], Scrutiny <ref> [61] </ref>, CSI [98], InspeQ [92], etc. These systems implement a wide variety of FTR methods. For example, ICICLE and Scrutiny implement Fagan's method, CSI implements Humphrey's method, InspeQ implements Phased Inspection, etc. <p> Another difference between Management and Technical reviews is that in the former the attendance typically includes management, whereas in the latter it consists primarily of technical personnel/peers. 4. Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed <ref> [17, 84, 61, 98, 99, 92, 136] </ref>. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed [17, 84, 61, 98, 99, 92, 136]. The type of support varies from simple augmentation of the manual practices <ref> [17, 61] </ref> to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> First, it discusses existing computer supported review systems that implement specific FTR methods. These include ICICLE [17], CIA/Scrutiny <ref> [61] </ref>, CSI [98], CAIS [99], InspeQ [92], and QDA/Tammi [136]. The discussion highlights those facilities that are either similar to or different from CSRS. The following subsection compares CSRS with existing computer-supported cooperative work systems. <p> A vote can be made anonymous by using an appropriate display function, such as by displaying only the number of users voting in a particular way instead of actual user-names. 4.4.2 CIA/Scrutiny CIA (Collaborative Inspection Agent)[62] (later called Scrutiny <ref> [61] </ref>) is a collaborative system for inspection and review of software engineering work products. It is also a distributed system that can be used by geographically separated users.
Reference: [62] <author> John W. Gintell and Gerard Memmi. CIA: </author> <title> Collaborative Inspection Agent. </title> <booktitle> In Proceedings of the CSCW'92 Workshop on Tools and Technologies, </booktitle> <month> September </month> <year> 1992. </year> <month> 226 </month>
Reference: [63] <author> Robert L. Glass. </author> <title> Software Conflict. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1991. </year>
Reference-contexts: Thus, to obtain high reliability, it is necessary to string together a well-planned series of reviews, inspections, and tests. Most researchers also agree that reviews must supplement testing; reviews tend to discover different types of errors than the those discovered by testing <ref> [105, 63] </ref>. In some cases, reviews can find errors that cannot be possibly found in testing [149]. Table 2.1: Defect detection efficiency for code [from Jones86] Removal step Lowest Med Highest eff. eff. eff. <p> Table 2.2 shows such studies. However, to a certain extent, these findings compare some varieties of apples with some varieties of oranges <ref> [63] </ref>. According to Glass, first, in the review process, the reviewers tend to not only identify a problem, they also isolate where the solution to the problem lies; in testing, the testers only identify that there is an error without isolating where the error can be fixed. <p> Second, in Collofello & Woodfield's research, which used project data (i.e., case study), the testing took place after the review processes had removed a fairly large percentage of the errors <ref> [63] </ref>. Review techniques also varied across studies.
Reference: [64] <author> Robert L. Glass. </author> <title> Building Quality Software. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1992. </year>
Reference-contexts: At the outset and near the end of the life cycle, managers, customers and users play a vital role in reviews, making sure that requirements and test results are correct. The intermediate reviews become more and more heavily peer oriented <ref> [64] </ref>. A new review method similar to desk checking called Personal Reviews has been recently developed by Humphrey, in which one examines one's own products to find as many defects as possible [76]. <p> Another typical reason why FTR is one of the favorite V&V techniques is that the technique can be applied to all phases of the development process and used to verify all quality attributes, such as portability, reliability, efficiency, testability, understandability and modifiability <ref> [64] </ref>. In general, V&V techniques can be classified into two categories: static analysis and dynamic 14 analysis (or testing). Static analysis can be further classified into manual and automated static analysis [3, 12, 11]. FTR is a manual static analysis technique. A similar technique is formal mathematical proofs. <p> Inspections also consume time early in development when customers and managers are anxious to see tangible results from software being tested. Again, the evidence of inspection benefits must be presented. Glass noted two problems with code review <ref> [64] </ref>: 1. Review is often drudgery; the methodical and thorough pace necessary for effective review is taxing. Because of this, motivation of participants is difficult. 2. Review is expensive. Only about 100 source statements may be reviewed in an hour, and the concentration usually wanes after an hour.
Reference: [65] <author> Yaron Goldberg, Marilyn Safran, and Ehud Shapiro. </author> <title> Active mail a framework for imple-menting groupware. </title> <booktitle> Proceedings of the Conference on Computer-Supported Cooperative Work (CSCW'92), </booktitle> <pages> pages 75-83, </pages> <year> 1992. </year>
Reference-contexts: The system compares these values with the user's supplied criteria in order to generate appropriate actions. Examples of message systems include Information Lens [95], Lotus Notes [35], Coordinator [55], Strudel [128], MailTray [120], Atomicmail [15], and Active Mail <ref> [65] </ref>. Message systems are typically generic systems for building group applications, such as meeting schedulers, information finders, and tools for project management, task tracking, software defect tracking, and collaborative writing.
Reference: [66] <author> Jorg M. Haake and Brian Wilson. </author> <title> Supporting collaborative writing of hyperdocuments in SEPIA. </title> <booktitle> Proceedings of the 1992 ACM Conference on Computer Supported Cooperative Work, </booktitle> <pages> pages 138-146, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Some examples of hypertext systems include NLS/Augment [50] NoteCards [68], gIBIS [30], and Neptune [36]. These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor [67], Virtual Notebook [77], SEPIA <ref> [66] </ref>, and Clare [144]. Still another systems provide unique support for real-time conferencing and/or face-to-face meetings, for example, Dolphin [134], as well as, CoAuthor and SEPIA. As explained in Chapter 3, the CSRS data modeling language is built around the concept of hypertext. <p> table */ 029:#define BLANK6 " " 030:#define BLANK8 " " 031:#define BLANK18 " " 032:#define BLANK31 " " 033: 034:typedef unsigned char BOOLEAN; 035:typedef char CHAR4 [4]; 036:typedef char CHAR6 [6]; 037:typedef char CHAR8 [8]; 038:typedef char CHAR18 [18]; 039:typedef char CHAR31 [31]; 040:typedef char CHAR50 [50]; 041:typedef char CHAR66 <ref> [66] </ref>; 042: 043:typedef struct SOURCETYPE - 044: /* source line and subfields */ 045: CHAR66 line; 046: BOOLEAN comline; 047: CHAR8 labl; 048: CHAR6 operation; 049: CHAR18 operand; 050: CHAR31 comment; 051:- SOURCETYPE; 052: 053:typedef struct REC_SYMTABTYPE - 054: CHAR8 symbol; 055: int address; 056:- REC_SYMTABTYPE; 057: 059:typedef enum -SEARCH, STORE- <p> " 033:#define BLANK18 " " 034:#define BLANK30 " " 035:#define BLANK31 " " 036: 037:typedef unsigned char BOOLEAN; 038: 039:typedef char CHAR4 [4]; 040:typedef char CHAR6 [6]; 041:typedef char CHAR8 [8]; 042:typedef char CHAR18 [18]; 043:typedef char CHAR30 [30]; 044:typedef char CHAR31 [31]; 045:typedef char CHAR50 [50]; 046:typedef char CHAR66 <ref> [66] </ref>; 047: 048:typedef struct SOURCETYPE - 049: /* SOURCE LINE AND SUBFIELDS */ 050: CHAR66 line; 051: BOOLEAN comline; 052: CHAR8 labl; 053: CHAR6 operation; 054: CHAR18 operand; 055: CHAR31 comment; 056:- SOURCETYPE; 057: 058:typedef struct OBJTYPE - 059: /* OBJECT CODE, LENGTH, AND TYPE */ 060: enum - HEADREC, TEXTREC,
Reference: [67] <author> Udo Hahn, Matthias Jarke, Stefan Eherer, and Klaus Kreplin. </author> <title> Studies in Computer Supported Cooperative Work. Theory, Practice, </title> <booktitle> and Design, chapter CoAuthor A Hypermedia Group Authoring Environment, </booktitle> <pages> pages 79-100. </pages> <publisher> Elsevier science publishers B.V, </publisher> <year> 1991. </year>
Reference-contexts: Some examples of hypertext systems include NLS/Augment [50] NoteCards [68], gIBIS [30], and Neptune [36]. These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor <ref> [67] </ref>, Virtual Notebook [77], SEPIA [66], and Clare [144]. Still another systems provide unique support for real-time conferencing and/or face-to-face meetings, for example, Dolphin [134], as well as, CoAuthor and SEPIA. As explained in Chapter 3, the CSRS data modeling language is built around the concept of hypertext.
Reference: [68] <author> Frank G. </author> <title> Halasz. Reflections on notecards: Seven issues for the next generation of hypermedia systems. </title> <booktitle> In Proceedings of the 1987 ACM Conference on Hypertext, </booktitle> <pages> pages 345-4365, </pages> <year> 1987. </year>
Reference-contexts: One of the goals is to provide the users with a medium to represent a collection of related ideas, and to discuss such ideas in a structured manner. Some examples of hypertext systems include NLS/Augment [50] NoteCards <ref> [68] </ref>, gIBIS [30], and Neptune [36]. These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor [67], Virtual Notebook [77], SEPIA [66], and Clare [144].
Reference: [69] <author> Jolene J. Hart. </author> <title> The effectiveness of design and code walkthroughs. </title> <booktitle> In Proceedings COMPSAC'82. The IEEE Computer Society's Eighth International Computer Software and Applications Conference, </booktitle> <address> Silver Spring, MD, </address> <pages> pages 515-522. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1982. </year>
Reference-contexts: Round-robin reviews. Round-robin review is a review process in which one copy of the review materials is made available and routed to each participant; the reviewers then write their comments/questions to the materials and pass them to another reviewer and to the moderator or the author eventually <ref> [69] </ref>. In other variation of Round-robin review, all participants meet at one room, and each participant takes a turn presenting or walking through a small section of the materials [56]. The latter is similar to walkthroughs, except that all review participants play the role of presenter as well. 4. Inspection. <p> Thus, errors are often eliminated even before the actual review sessions <ref> [69] </ref>. <p> Hart also observed that the maintenance of the reviewed portions of the systems were significantly easier than for the unreviewed portions; furthermore, more personnel were available to do error correction, since they understood how the fix could affect other parts of the system <ref> [69] </ref>. * McKissick reported on the use of inspection for a preliminary design of computer software for military electronic system at General Electric Company. The results showed that this method discovered substantial amount of defects (492 from 21 design inspections) in the early design. <p> This happens, for example, during the inspection meeting phase. However, some review practices include this type of discussion since it often leads to discovery of new errors <ref> [69, 41] </ref> For the same reason, some review practices/methods include a separate phase, called third-hour meeting (so called because it takes place after the two-hour regular inspection meetings) where the objective of the phase is mainly to discuss solution ideas [21, 108]. 3.1.2 Interaction The interaction component of FTR framework specifies <p> Phase 2 Phase 3 Phase 4 Peer Rating Review Summary [106, 129] O:examination O:consolidate C:individual C:individual S:ASYNC S:ASYNC R:reviewer R:administrator T:evaluation form T:? E:completed form E:? Structured Preparation: Walkthrough: Walkthrough O:comprehension O:examination [150] C:individual, C:group selective subgroup S:ASYNC,SYNC S:SYNC R:reviewer,producer R:producer T:reading,discussion T:paraphrasing E/X:? E/X:? Round-robin Phase i Phase i+1 <ref> [69] </ref> O:examination O:examination C:subgroup C:subgroup S:ASYNC S:ASYNC R:reviewer i R:reviewer i+1 T:? T:? Fagan's Overview: Preparation: Inspection: Inspection O:comprehension O:comp (edu) O:examination [52] C:group C:individual C:group S:STSP S:ASYNC S:STSP R:producer R:reviewer R:producer/reader T:presentation T:reading&checklist T:paraphrasing E:clean compilation X:time X: time Humphrey's Overview: Preparation: Pre-meeting: Meeting: Inspection O:comprehension O:comp & exam O:correlation <p> Following this phase, the coordinator performs post-walkthrough activities that are primarily administrative and clerical, such as preparing summary report. Since these activities are administrative in nature, they are not included in the model. 3.2.3 Round-robin reviews This subsection describes the Round-robin method as practiced by Hart <ref> [69] </ref> (see also section 2.1). In this method, one participant assumes the role of Moderator, and the rest are Reviewers. The number of review phases generally correspond to the number of participants. The review materials are routed to each reviewer and eventually to the moderator. <p> There is a limited interaction/collaboration among reviewers, in which the findings by one reviewer is passed to the next reviewer. No specific examination techniques are described in the literature. Hart also pointed out that this method lacks any educational goal (i.e., there is no comprehension phase) <ref> [69] </ref>. The entry/exit criteria is not specified. 34 3.2.4 Fagan's Inspection The classical Fagan's Inspection [52] begins with the Overview phase. This phase has the objective of comprehension, and involves group interaction with STSP synchronicity.
Reference: [70] <author> W.C. Hetzel. </author> <title> An experimental analysis of program verification methods. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1976. </year>
Reference-contexts: analysis, in which the lowest level of paragraphs/statements are first characterized in terms of variable used and set, their effects are summarized in a written form, then the entire program, viewed as a sequence of paragraphs, is characterized the same way; these characteristics are then compared with the program specifications <ref> [70] </ref>. Myers used a combination of mentally executing test-cases and checking the logic 15 for common errors [105]. Basili & Selby used stepwise abstraction, in which the prime subprograms are identified, their individual functions are derived and compared against the specifications [7]. <p> Table 2.2: Research findings on Review v.s. Test References Detection Efficiency Cost Effectiveness Hetzel76 <ref> [70] </ref> code reading (37.3%) &lt; functional testing (47.7%) = structural testing (46.7%) Myers78 [105] code walkthrough (38%) = code walkthrough functional testing (30%) = most expensive structural testing (36%) Basili87 [7] code reading &gt; (16% more) code reading functional testing &gt; (11% more) least expensive structural testing Collofello89 [28] code review <p> One of the research goals of this dissertation is to investigate these differences. The FTR framework described in the next chapter is an approach for conducting such an investigation. 24 Table 2.4: Empirical findings of FTR References Efficiency Group-size Method Hetzel76 <ref> [70] </ref> 37.3% 1 Code reading Myers78 [105] 38% 3 Code walkthrough Eick92 [47] 75% 8 Design review Martin90 [97] 27% 4 N-Fold inspection (N=1) (Req Doc) Schneider92 [125] 35.1% 4 N-Fold inspection (N=1) (Req Doc) 77.8% 4 N=9 (Req Doc) Knight93 [92] 50% 2 Phased Inspection (Phase 6) (Code) Porter94 [115] <p> Hetzel's study Hetzel's study investigates the effectiveness of code reading (FTR method) compared to testing <ref> [70] </ref>. The method includes only one phase, in which the experimental subjects individually search for program faults. There is no interaction involved. <p> However, the review methods in these studies investigate different components of the methods. No studies have investigated the interaction components of review methods. For example, Hetzel's, Basili's and Porter's studies <ref> [70, 7] </ref> investigated the examination technique components of review methods. In both Hetzel's and Basili's method, the examination technique (code reading) was compared to testing; no group interaction was involved. In Porter's study, three different examination (detection) techniques were compared: Ad Hoc, Checklist and Scenario [115].
Reference: [71] <author> Helmer Hirshberg. </author> <title> Social Technology. </title> <publisher> Basic Books, </publisher> <year> 1966. </year>
Reference-contexts: Still another technique is simply asking each participant in turn to report his/her issues [101], or to report and revise the issues as necessary following several rounds (i.e., Delphi technique <ref> [71] </ref>). 3.1.4 Entry/exit criteria The entry/exit criteria defines the starting or completion condition of a review phase. Specifically, it defines what conditions, states or properties can be expected from the review artifacts or review 30 participants before entering or after completing the review phase. <p> The latter technique is similar to the one used in Active Design Review [110]. The exit criteria is to ensure that all checklist items have been marked or inspected. The final phase is reconciliation, where the inspectors compare their findings in a face-to-face meeting using Delphi-like technique <ref> [71] </ref>. This corresponds to group consolidation. It is not clear from the literature who will lead the meeting. The exit criteria for this phase is also not specified. <p> The objective here is to consolidate individual issues into agreed upon actions. Thus, this phase corresponds to group consolidation activity. The reviewers, however, are still permitted to raise additional new issues if desired. The consolidation technique used in this phase is similar to Delphi approach <ref> [71] </ref>. Specifically, each reviewer is required to express his or her agreement on all issues (i.e., confirm, disconfirm or neutral) and provide some evidence if necessary. At some later time, the reviewer may change his or her position regarding the issue based on other reviewers' comments. <p> Generally, during the Public-review phase, the participants create Comment nodes to respond to Issue nodes, or to follow up Comment nodes created by other participants. The review technique used in this phase is similar to the Delphi method <ref> [71] </ref>, where the participants first read issues, then express their positions on each issue by voting and raising follow up comments. At some later time, they read someone else's response to the same issues, and then change their positions as necessary.
Reference: [72] <author> Charles P. Hollocker. </author> <title> Software Reviews and Audits Handbook. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1990. </year>
Reference-contexts: Internal versus External reviews. These classes of reviews focus on the authority who manages the reviews. An external review implies that there is an independent central figure that has complete authority and ownership over the reviews and the resulting reports. One example of such a review is an Audit <ref> [72, 27] </ref>. The auditor is an independent organization that oversees the review process, but has no control over the corrective actions taken by the audited organization. <p> The objective is to ensure progress according to plan. A technical review is a formal 11 evaluation of a specific software element to provide evidence that the software element satisfies its specifications and conforms to applicable standards <ref> [72] </ref>. Included in this technical review is inspection, walkthrough and other review methods described earlier. Another difference between Management and Technical reviews is that in the former the attendance typically includes management, whereas in the latter it consists primarily of technical personnel/peers. 4.
Reference: [73] <author> Charles P. Hollocker. </author> <title> System and Software Requirements Engineering, </title> <journal> chapter A Review Process Mix, </journal> <pages> pages 485-491. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year>
Reference-contexts: The review process itself may be based on inspections, walkthroughs or any other review methods. For example, Hollocker proposes mix review methods: walkthroughs, technical review and software inspection for SRR <ref> [73] </ref>. Pressman suggested a combination of informal and formal walkthroughs for design reviews [118]. Parnas suggested a new review method called Active Design Review for the design review (this method will be discussed in detail in the later chapter) [111].
Reference: [74] <author> Watts S. Humphrey. </author> <title> Managing the Software Process. </title> <publisher> Addison Wesley Publishing Company Inc., </publisher> <year> 1990. </year>
Reference-contexts: Over the past several years, many different forms of inspections or formal reviews have emerged. Some of these include: Structured Walkthrough [149], Active Design Reviews [110], Humphrey's inspection <ref> [74] </ref>, Verification-Based Inspection [45], and so forth. Throughout this dissertation, the term formal technical review (FTR) is used to refer to a general class of reviews, which includes the specific review techniques described above. <p> When the persons know their work will be critically examined by others, they are encouraged to work more carefully either to avoid being embarrassed by sloppy mistakes or through the pride of exhibiting a quality work product <ref> [74] </ref>. * Reviews improve software maintainability through better documentation, standardization and readability. 1.3 Important problems in current FTR research Despite its numerous benefits, current FTR research suffers from problems that hinder its progress. <p> For example, the meeting review factor in Fagan's method [52] is intended for error hunting, while in Humphrey's method <ref> [74] </ref> it is used for error logging. The following subsections describe these problems in detail. 1.3.1 Variations and ambiguities in FTR methods and practices First, the literature does not define FTR consistently. <p> In fact, the literature on FTR often describes conflicting review practices for the same method. For example, some descriptions of Fagan's method explicitly advocate the use of paraphrasing for the inspection technique [52, 124, 34], while others disregard this technique <ref> [74] </ref>. Some advocate the use of common error checklists when examining source materials [74, 57], others also advocate the use of selective test cases [1, 43]. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. <p> For example, some descriptions of Fagan's method explicitly advocate the use of paraphrasing for the inspection technique [52, 124, 34], while others disregard this technique [74]. Some advocate the use of common error checklists when examining source materials <ref> [74, 57] </ref>, others also advocate the use of selective test cases [1, 43]. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. <p> Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection <ref> [74, 98, 142] </ref>. Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion [127, 110]. <p> Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion [127, 110]. Also, many review methods demand a separate preparation phase where reviewers learn source materials before starting error hunting <ref> [1, 74, 52] </ref>, while others consider it optional or unnecessary [92]. In conclusion, the wide variations of review practice, even within the same ostensible method, such as Fagan's method, indicate that the methods are defined ambiguously or unclear. <p> Dunn, Peele, Ackerman et al. attribute the success of software review to the presence of group synergy [43, 114, 2], however, Humphrey reported that 75% of errors are found during individual preparation rather than in the group meeting <ref> [74] </ref>. Eick et al. also reported that 10% of errors are found during group meeting while 90% of errors are found during preparation [47]. <p> The field study conducted by Shneiderman suggests that peer ratings of programs are productive, enjoyable, and nonthreatening experiences [129]. Peer Ratings are often called Peer Reviews [129]. However, some authors use the term Peer Reviews as generic review methods involving peers <ref> [113, 74] </ref>. 2. Walkthroughs. Walkthroughs can be viewed as presentation reviews, in which a review participant, usually the developer of the software being reviewed, narrates a description of the software and the remainder of the review group provides their feedback throughout the presentation [56, 27, 60]. <p> Like walkthroughs, the term Inspections is also used in the literature in many different ways. For example, many researchers associate inspections with paraphrasing techniques, in which a designated reader paraphrases the materials statement by statement [106, 3, 124, 1], others downplay the role of reader <ref> [41, 101, 74, 60] </ref>. Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides [60]. <p> These two concepts have led to the justification of FTR groups, as well as the establishment of independent quality assurance groups that specialize in finding software defects in many software organizations <ref> [74] </ref>. Another important and sensitive psychological aspect of FTR is the recording and dissemination of the data to the management. This must be done in such a way that individual programmers will not feel intimidated or threatened [40]. <p> stem, in large part, from the quality of the process used to create them [74].This principle is applied by the Department of Defense (DoD) under the name of Total Quality Management (TQM) [5], and Software Engineering 16 Institute (SEI) under the name of Capability Maturity Model or Software Process Maturity <ref> [113, 74] </ref>. The underlying concepts of managing and improving software process can be traced back to Dr. W. Edwards Deming's work on the industrial quality improvement in the post World War II [37]. <p> The organizations should establish improvement objectives that focus on achieving higher levels of maturity. Peer review (walkthrough or inspection) is identified as one of the key process areas for advancing to level 3 (i.e., the defined process) <ref> [74] </ref>. Specifically, reviews should be used to monitor the quality of both the software product and the development process that produces it. <p> Many studies have found a correlation between the effectiveness of inspections and certain inspection metrics. For example, the number of defects found decline with increasing inspection or preparation rates <ref> [53, 74, 24] </ref>, with the optimum inspection rate is around 150 LOC/hour [124]. This data is then used as a guideline for controlling the inspection process. Thus for example, if the pace of an inspection is well above the standard guideline 150 LOC/hour, the materials need to be reinspected. <p> Thus for example, if the pace of an inspection is well above the standard guideline 150 LOC/hour, the materials need to be reinspected. Control charts are also commonly used to evaluate the performance of an inspection process statistically <ref> [22, 74, 133] </ref>. Others use mathematical models to predict remaining errors after inspections [23, 47]. The principle of continuous process improvement also applies to the inspection process itself. This improvement can be achieved in the same way using statistical process control as discussed above. <p> The goal of causal analysis is to determine the cause of each defect found and the suggested action to prevent it from occurring in the future. As stated by Humphrey, Every defect is an improvement opportunity <ref> [74] </ref>. The idea of using inspection for preventing defects was first suggested by Fagan. Fagan recommended that the defects caught by the inspection process should be analyzed and fed back to the developers (as well as to the inspectors in the form of checklists) to prevent them from recurring. <p> Inspections are perceived as improving quality at the expense of other goals (e.g., profit, schedule). 22 Furthermore, there are only few reported cases of poor FTR experiences. This may be partly due to people's natural reluctance to write about project failures <ref> [74] </ref>. According to Humphrey, the reasons why some inspections have not been effective were generally due to errors in the way they were conducted. Either the preparation was not adequate, too many people were involved, the wrong people attended, or too much material was covered at one time. <p> Another reason is inspection or review data is not managed. One thus cannot appreciate the enormous costs that can be avoided by finding and fixing defects before test. The biggest single problem is generally the combination of management inattention and schedule pressure <ref> [74, 75] </ref>. Clapp and Wallace identified two major issues concerning inspection [25]: 1. Inspection is viewed as low-level, manual work. People think testing is easier and faster. People need to be motivated by showing what they can learn as well as the savings to the project from inspections. 2. <p> It does not include administrative procedures that are typical in industry practice, such as planning, selecting moderator, etc. It does not incorporate other software development activities, such as configuration management, coding, testing, etc. Some FTR methods, such as Fagan's [52] and Humphrey's <ref> [74] </ref> methods, include a rework phase as part of their FTR processes. However, this phase will not be included in the model, because it does not deal with the review activity per se. The main goal of this framework is to clarify FTR methods by enumerating their subprocess components. <p> Discussion is the activity intended to resolve issues and search for alternative solutions. In Fagan's method [52, 53], the comprehension activity is known as preparation, whereas the examination activity is known as defect finding. Consolidation can be found, for example, in FTR methods suggested by Humphrey <ref> [74] </ref>, Gilb [60], Votta and Porter [142, 116]. Gilb refers to consolidation as defects logging, and Votta and Porter refer to it as collection. Correlation can be found in Humphrey's method [74]. Discussion can be found in the third hour meeting phase of some inspection methods [21, 108]. <p> Consolidation can be found, for example, in FTR methods suggested by Humphrey <ref> [74] </ref>, Gilb [60], Votta and Porter [142, 116]. Gilb refers to consolidation as defects logging, and Votta and Porter refer to it as collection. Correlation can be found in Humphrey's method [74]. Discussion can be found in the third hour meeting phase of some inspection methods [21, 108]. In practice, a single phase may implicitly include multiple activities. For example, during Fagan's preparation phase, one may discover errors while attempting to comprehend the code. <p> In this case, the participants are expected to both fully understand the materials and find defects at the same time. Some researchers suggest that separating comprehension from examination activity makes a review process more effective and productive <ref> [52, 74, 1, 124] </ref>. For example, in the code inspection method, there is a phase called overview, where the producer educates participants about review materials, and preparation, where the reviewers learn the materials individually [52]. The stated objective of these phases is comprehension. <p> However, many other inspection methods use the preparation phase for both comprehension and examination activities, namely, individual reviewers are asked to both understand the review materials and find errors during this phase <ref> [74, 60] </ref>. Unfortunately, no empirical studies have ever looked into the issue of whether separating comprehension from examination activity affects review effectiveness. However, some researchers believe that comprehension does correlate with examination. <p> However, Dow and Murphy found comprehension (i.e., detailed product knowledge) is not a prerequisite for an effective inspection [42]. Similarly, some studies provide empirical results that relate preparation time to review outcomes <ref> [19, 2, 24, 74] </ref>. However, they generally do not specify whether reviewers should concentrate on simply learning the materials or on looking for errors during the preparation phase. In the same way, examination often implies consolidation. <p> For example, the meeting phase in Fagan's inspection, which has the objective of examination (hunting for defects), also includes consolidation implicitly, since defects are confirmed or rejected as they are discovered. Other researchers prefer to make these two activities separate (e.g., by having a separate defect-logging meeting) <ref> [74, 60] </ref>. Although consolidation does not involve examining new materials, some studies show that new errors are often discovered during this activity as a result of group interaction [41]. However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors [142, 116, 115]. <p> Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy [43, 114, 2]. However, some review experiences/studies found that more errors are caught in individual process than group process <ref> [74, 142, 115, 47] </ref>. Many other review methods (namely, Fagan's code inspection and its variations) usually include some combination of individual and group processes. The Selective subgroup process has also been shown to provide unique benefits, such as a significant increase in review performance in N-Fold inspection [97]. <p> Some examples of examination techniques include the paraphrasing technique used during the group meeting in Fagan's inspection [52], selective test cases (test data) [106, 3], checklists of common errors (i.e., classes of software errors) used during the individual examination phase in Humphrey's method <ref> [74] </ref>, active checklist used during individual review phase in Active Design Review [110], scenarios used in Porter and Votta's experiment [115], the stepwise abstraction technique used during the individual examination phase in Basili's method [8], and statistical process control in Humphrey's Personal Reviews [76]. <p> This technique involves basic top-down and bottom-up code reading strategies. A common technique for consolidation is presentation, where individual issues are presented one by one by the meeting leader, and the group confirms or rejects the issues as software defects <ref> [74] </ref>. Another common technique is to let the meeting leader talk through review materials, while participants interrupt with the issues found during the private phase (sometimes called the document pacing technique) [60, 142, 115]. <p> O:examination C:subgroup C:subgroup S:ASYNC S:ASYNC R:reviewer i R:reviewer i+1 T:? T:? Fagan's Overview: Preparation: Inspection: Inspection O:comprehension O:comp (edu) O:examination [52] C:group C:individual C:group S:STSP S:ASYNC S:STSP R:producer R:reviewer R:producer/reader T:presentation T:reading&checklist T:paraphrasing E:clean compilation X:time X: time Humphrey's Overview: Preparation: Pre-meeting: Meeting: Inspection O:comprehension O:comp & exam O:correlation O:consolidation <ref> [74] </ref> C:group C:individual C:subgroup C:group S:STSP S:ASYNC S:ASYNC S:STSP R:producer R:reviewer R:moderator&producer R:producer T:presentation T:reading & T:? T:presentation std checklist E:clean compilation E/X:? E/X:? E/X:? Active Stage 1: Stage 2: Stage 3: Design O:comprehension O:comp & exam O:consolidation Review C:group C:individual C:selective subgroup [110] S:STSP S:ASYNC S:STSP,ASYNC R:producer R:reviewer (specialist) R:reviewer-producer <p> These, however, are not included in the model as they do not relate to specific review steps, although follow-up may be considered as instantiation of yet another series of review phases. 3.2.5 Humphrey's Inspection Humphrey's inspection <ref> [74] </ref> begins with the Overview phase. This phase has the objective of comprehension, and involves a face-to-face group interaction (STSP). The review participants review background materials presented by the producer. The entry criteria for this phase includes clean compilation, all text has been spell-checked, all pertinent documents have been generated. <p> Chapter 4 will describe the design and implementation of such a system. 3.4 Work related to the FTR framework Existing theoretical work on FTR focuses primarily on modeling certain parameters of Fagan's inspection process in order to measure and predict its effectiveness <ref> [24, 6, 74] </ref>. For example, Christenson proposes an inspection model that relates input parameters, such as the preparation effort and inspection rate to the resulting fraction of errors found in the code [23, 24]. Using such a model, the effectiveness of inspections can be estimated and improvement can be made. <p> In EIAM, paraphrasing was performed by individual participants silently. For the latter, however, I had no way of knowing whether the participants read every single statement as instructed in the review guideline. Several studies have found that paraphrasing rate is a good predictor of review effectiveness <ref> [124, 74, 24] </ref>. In fact, Russell suggested that the effective paraphrasing rate should be around 150 lines/hour. This section compares the paraphrasing rate of EGSM and EIAM. <p> This second phase can also increase participants' confidence and facilitate learning. Actually, this two-phase review method is commonly practiced <ref> [74, 142, 115] </ref> (see also Chapter 3). However, as pointed out by Votta [142], this second phase could be inefficient if not carefully administered. For example, instead of having the entire group members participate in the second phase, one might select only a subset of the participants.
Reference: [75] <author> Watts S. Humphrey. </author> <title> A personal commitment to software quality. </title> <publisher> Ed Yourdon's American Programmer, </publisher> <month> Dec </month> <year> 1994. </year>
Reference-contexts: The average yield of design and code reviews (i.e., percentage of defects found in the product) is between 50-100 percent, with an average of about 70 percent [76]. The productivity improvement in LOC/Hour also increases from 23%-136% <ref> [75] </ref>. 2.5 FTR experiences in industry This section surveys FTR experiences in industry as reported in the literature. It includes the experiences from organizations that published their FTR data or their distinct review processes. <p> Another reason is inspection or review data is not managed. One thus cannot appreciate the enormous costs that can be avoided by finding and fixing defects before test. The biggest single problem is generally the combination of management inattention and schedule pressure <ref> [74, 75] </ref>. Clapp and Wallace identified two major issues concerning inspection [25]: 1. Inspection is viewed as low-level, manual work. People think testing is easier and faster. People need to be motivated by showing what they can learn as well as the savings to the project from inspections. 2.
Reference: [76] <author> Watts S. Humphrey. </author> <title> A Discipline for Software Engineering. </title> <publisher> Addison-Wesley Publishing Company Inc., </publisher> <month> January </month> <year> 1995. </year>
Reference-contexts: The intermediate reviews become more and more heavily peer oriented [64]. A new review method similar to desk checking called Personal Reviews has been recently developed by Humphrey, in which one examines one's own products to find as many defects as possible <ref> [76] </ref>. However, unlike desk checking, Personal Review is a disciplined process used in conjunction with Personal Software Process (PSP) to improve one's own work. <p> Thus, errors are often eliminated even before the actual review sessions [69]. Humphrey provides other justifications or motivations for reviewing programs <ref> [76] </ref>: * Reviews help one to build a mental picture of what the program does and why, and thus, when the program does not do as expected, it is easier to find and fix the problem. * In reviews, one finds the defects directly. <p> Finally, the principle of continuous process improvement must also occur at a personal level. Bisant and Lyle's study has shown that software reviews can improve programmers' performance significantly [10]. The most significant development for self-improvement initiatives is Humphrey's Personal Software Process (PSP) <ref> [76] </ref>. The PSP takes a maturity framework developed for assessing and improving the development process of software organizations and applies it to individual developers. The result is a continuous self-improvement program to help one producing high quality software and improving one's productivity. <p> One case study from PSP suggests that the quality and productivity of participants can improve significantly. The average yield of design and code reviews (i.e., percentage of defects found in the product) is between 50-100 percent, with an average of about 70 percent <ref> [76] </ref>. The productivity improvement in LOC/Hour also increases from 23%-136% [75]. 2.5 FTR experiences in industry This section surveys FTR experiences in industry as reported in the literature. It includes the experiences from organizations that published their FTR data or their distinct review processes. <p> the individual examination phase in Humphrey's method [74], active checklist used during individual review phase in Active Design Review [110], scenarios used in Porter and Votta's experiment [115], the stepwise abstraction technique used during the individual examination phase in Basili's method [8], and statistical process control in Humphrey's Personal Reviews <ref> [76] </ref>. Many review practices, however, do not use any techniques for examination. They rely only on reviewers' intuition and experience to find errors. This dissertation refers to it as free technique; Porter et al. term it Ad Hoc detection method [115]. <p> [60] C:individual examination S:ASYNC C:group R:specialist S:STSP T:checklist,SPC R:moderator X: time T:pacing NASA's Overview Preparation Meeting Third-hour Inspection O:comprehension O:examination O:examination O:discussion [108] C:group C:individual C:group C:group S:STSP S:ASYNC S:STSP S:STSP R:author R:reviewer R:reader R:? T:presentation T:? T:paraphrasing T:? E/X:? E/X:? X:time X:time Humphrey's Review Personal O:examination Review I:no T:checklist,SPC <ref> [76] </ref> X:source reviewed& items checked Verification Inspection: ? Inspection O:examination [44] C:individual S:ASYNC R:reviewer T:stepwise verification E/X:? FTArm Orientation: Private: Public: Consolidation: Meeting: [83] O:comprehension O:comp,exam O:consol O:consol O:consol C:group C:indiv,group C:group C:individual C:group S:STSP S:ASYNC S:ASYNC S:ASYNC S:SYNC R:producer R:reviewer,producer R:moderator R:moderator R:moderator T:presentation T:question-answer, T:Delphi T:tools T:voting free exam <p> The last phase is the Third hour meeting phase. This optional phase is intended to discuss defects resolution, and resolutions to open issues identified during the previous meeting phase. 3.2.12 Humphrey's Personal Review Humphrey's Personal Review is a review method intended to support Personal Software Process (PSP) <ref> [76] </ref>. The method has only one phase where the software developer examines his/her own product for defects. Thus, no interaction is involved. The review technique is based on checklists and statistical process control (SPC). The SPC technique requires one to collect, measure, and analyze review metrics for future improvement.
Reference: [77] <author> Frank M. Shipman III, R. Jesse Chaney, and G. Anthony Gorry. </author> <title> Distributed hypertext for col-laborative research: The virtual notebook system. </title> <booktitle> In Proceedings of the 1989 ACM Conference on Hypertext, </booktitle> <pages> pages 129-135, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Some examples of hypertext systems include NLS/Augment [50] NoteCards [68], gIBIS [30], and Neptune [36]. These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor [67], Virtual Notebook <ref> [77] </ref>, SEPIA [66], and Clare [144]. Still another systems provide unique support for real-time conferencing and/or face-to-face meetings, for example, Dolphin [134], as well as, CoAuthor and SEPIA. As explained in Chapter 3, the CSRS data modeling language is built around the concept of hypertext.
Reference: [78] <author> H. Ishii and N. Miyake. Teamworkstation: </author> <title> Toward a seamless shared workspace. </title> <booktitle> Proceedings of the Conference on Computer-Supported Cooperative Work (CSCW'90), </booktitle> <pages> pages 13-26, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Conferencing systems Conferencing systems provide support for interactive, real-time, and distributed meeting activities, or same-time different-place meetings. Conceptually, they provide virtual meeting rooms [51], where participants from dispersed locations can discuss and manipulate the materials/applications displayed on shared screens. Examples of such systems include Rapport [51], Mermaid [145], TeamWorkstation <ref> [78] </ref>, and Dolphin [134]. Some systems, for example, Dolphin, also include same-time same-place meetings. The basic facilities of conferencing systems usually include real-time voice and video channels, which are implemented through separate communication networks (e.g., a wide area network).
Reference: [79] <author> J. Jacky. </author> <title> Programmed for disaster: Software errors that imperil lives. </title> <journal> The Sciences, </journal> <month> September </month> <year> 1989. </year>
Reference-contexts: In 1986, a medical system for radiation therapy killed two patients due to improper design and implementation of its software. A report given to to the U.S. Food and Drug Administration (FDA) stated that the software failed to access the appropriate calibration data <ref> [79, 126] </ref>. In addition to quality problems in software products, software development is often plagued by cost and schedule overruns.
Reference: [80] <author> Michael G. Jenner. </author> <title> Software Quality Management and ISO 9001. How to Make Them Work for You. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1995. </year>
Reference-contexts: The CMM defines Peer Review as the key process area for software organizations to mature to level 3 (Defined process) [113]. ISO 9001 states that All product is subject to one or more reviews, inspections and/or tests before being submitted for acceptance and release as a product <ref> [80] </ref>. Despite its importance and benefits, there are many aspects of FTR that are currently not understood. First, there are many variations and ambiguities in FTR methods. The same method is often practiced differently by different organizations. Second, variations in review outcomes are often attributed to different review factors.
Reference: [81] <author> Philip M. Johnson. </author> <title> ECS design reference. </title> <type> Technical Report ICS-TR-94-13, </type> <institution> University of Hawaii, Department of Information and Computer Sciences, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: It allows one to create and manipulate different types of nodes and links, and to provide different computational support for different entities. Finally, ECS also provides mechanisms to propagate state information among clients. The detailed design of ECS can be found in <ref> [81] </ref>. CSRS is built on top of ECS subsystem. It leverages on the services provided by ECS. Specifically, CSRS consists of three distinct levels (see figure 4.12): 1. Application level. At this top level of CSRS, the developer implements a new FTR method, such as FTArm. <p> The Gagent password is required, since it will spawn an Egret Gagent process in the background. When initialization is successful, two processes will be run in the background: the server process and the Gagent process (See Egret documentation for detailed explanation of these processes <ref> [81] </ref>). Furthermore, the database name will be installed into the database list of the client program (see Figure 5.3). At the end of the initialization step, the system will set the current review phase to Orientation. In other words, the Orientation phase can now begin.
Reference: [82] <author> Philip M. Johnson. </author> <title> Experiences with EGRET: An exploratory group work environment. </title> <journal> Collaborative Computing, </journal> <volume> 1(1), </volume> <month> January </month> <year> 1994. </year>
Reference-contexts: Furthermore, this operation will be invoked automatically by the system every time the Reviewer or the Producer connects to the database during the Public review phase. 4.2 System architecture CSRS is an application of Egret, a generic group work system for Unix and X-Windows environment <ref> [82] </ref>. Egret/CSRS implements a distributed client-server architecture (see Figure 4.12). Multiple clients send commands to a shared server, called HBS, to access the artifacts stored in the database.
Reference: [83] <author> Philip M. Johnson. </author> <title> An instrumented approach to improving software quality through formal technical review. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: C:group C:individual C:group C:group S:STSP S:ASYNC S:STSP S:STSP R:author R:reviewer R:reader R:? T:presentation T:? T:paraphrasing T:? E/X:? E/X:? X:time X:time Humphrey's Review Personal O:examination Review I:no T:checklist,SPC [76] X:source reviewed& items checked Verification Inspection: ? Inspection O:examination [44] C:individual S:ASYNC R:reviewer T:stepwise verification E/X:? FTArm Orientation: Private: Public: Consolidation: Meeting: <ref> [83] </ref> O:comprehension O:comp,exam O:consol O:consol O:consol C:group C:indiv,group C:group C:individual C:group S:STSP S:ASYNC S:ASYNC S:ASYNC S:SYNC R:producer R:reviewer,producer R:moderator R:moderator R:moderator T:presentation T:question-answer, T:Delphi T:tools T:voting free exam E:unresolved E:source readiness E:source availability E:source nodes E:commentary issues remained reviewed reviewed X:no more issue 33 3.2.1 Peer Rating The Peer Rating method <p> The author argues that the stepwise specification used in the verification technique may facilitate step by step reasoning or comprehension of the product at the same time. The entry and exit criteria are also not described in the literature. 3.2.14 FTArm FTArm (Formal Technical Asynchronous review method) <ref> [83] </ref> includes 5 major phases: orientation, private review, public review, consolidation, and group meeting. Most of the phases are performed asynchronously (ASYNC interaction) and supported by computational tools, such as hypertext style navigation, computer-mediated group meeting, etc. <p> This feature allows the process workflow to be started automatically, and can provide guidance to the participants about what they need to do when first connecting to the database. <ref> [84, 83] </ref>. The first construct l*process*define-method defines FTArm as a review method consisting of 6 phases: Orientation, Private, Public, Consolidation, Meeting, and Conclusion. <p> It implemented one review method called FTArm (Formal, Technical, Asynchronous review method) <ref> [83, 84] </ref>. Version 1 supported only asynchronous (different-time different-place) meetings. <p> They spent more time reading existing issues than finding new issues. Thus, the Public-Review phase may promote free-riding. To resolve this problem, a new Private review phase was added before the Public review phase. The resulting Csdl-Method was similar to FTArm <ref> [84, 83] </ref> except it has a slightly fewer number of phases. The FTArm itself was reimplemented for CSRS Version 3 afterwards. The next chapter will describe this method in detail.
Reference: [84] <author> Philip M. Johnson and Danu Tjahjono. </author> <title> Improving software quality through computer supported collaborative review. </title> <booktitle> In Proceedings of the Third European Conference on Computer Supported Cooperative Work, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Another difference between Management and Technical reviews is that in the former the attendance typically includes management, whereas in the latter it consists primarily of technical personnel/peers. 4. Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed <ref> [17, 84, 61, 98, 99, 92, 136] </ref>. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed [17, 84, 61, 98, 99, 92, 136]. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods <ref> [84, 92] </ref>. Chapter 4 will discuss these review systems in detail. As described in Chapter 1, this dissertation uses the term Formal Technical Review (FTR) to refer to generic classes of technical reviews as opposed to management reviews, and may include both manual and computer supported reviews. <p> Most FTR practices define the entry and exit criteria between phases according to an agreed upon time schedule, such as when to start or end the phase. Some FTR methods use the status of review artifacts as the entry/exit criteria <ref> [84] </ref>. Other FTR methods use the properties of review products (e.g., compliance with coding standards) as the entry/exit criteria. For example, Phased-Inspection uses exit criteria to ensure that the product possesses certain properties after leaving a phase [91, 92]. <p> This feature allows the process workflow to be started automatically, and can provide guidance to the participants about what they need to do when first connecting to the database. <ref> [84, 83] </ref>. The first construct l*process*define-method defines FTArm as a review method consisting of 6 phases: Orientation, Private, Public, Consolidation, Meeting, and Conclusion. <p> It implemented one review method called FTArm (Formal, Technical, Asynchronous review method) <ref> [83, 84] </ref>. Version 1 supported only asynchronous (different-time different-place) meetings. <p> They spent more time reading existing issues than finding new issues. Thus, the Public-Review phase may promote free-riding. To resolve this problem, a new Private review phase was added before the Public review phase. The resulting Csdl-Method was similar to FTArm <ref> [84, 83] </ref> except it has a slightly fewer number of phases. The FTArm itself was reimplemented for CSRS Version 3 afterwards. The next chapter will describe this method in detail.
Reference: [85] <author> Philip M. Johnson, Danu Tjahjono, Dadong Wan, and Robert S. Brewer. </author> <title> Experiences with CSRS: An instrumented software review environment. </title> <booktitle> In Proceedings of the Pacific Northwest Software Quality Conference, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: In other words, when the user is idle (for example, talking on the phone), no busy event will be generated. This allows one to collect and analyze not only the idle time, but also the finer-grained activity of the user in the order of minutes (see also <ref> [85] </ref>). The metrics subsystem also provides built-in tools to extract and analyze timestamps. The system incrementally abstracts the timestamps into more suitable forms for analysis. The first abstract form is activity, which is defined as the interval between two timestamp events.
Reference: [86] <author> C. L. Jones. </author> <title> A process-integrated approach to defect prevention. </title> <journal> IBM Systems Journal, </journal> <volume> 24(2) </volume> <pages> 150-166, </pages> <year> 1985. </year>
Reference-contexts: Jones and Mays then formalized this initiative as a defect prevention methodology, which included a causal-analysis meeting to identify the root cause of the defects and their preventive actions, and the establishment of an action team to implement the preventive actions and to provide feedback to the developers <ref> [86, 100] </ref>. In most practices, the causal-analysis meeting is conducted as an extension to regular inspection, usually at the end of the inspection process [41, 60]. Finally, the principle of continuous process improvement must also occur at a personal level.
Reference: [87] <author> Capers Jones. </author> <title> Programming Productivity. </title> <publisher> McGraw-Hill, </publisher> <year> 1986. </year>
Reference-contexts: With respect to relative effectiveness of these methods, Jones provides the approximate ranges of detection efficiencies (percentage of total errors caught) for 10 common defect removal/detection methods obtained from uncontrolled observations (see Table 2.1) <ref> [87] </ref>. Table 2.1 generally suggests that reviews/inspections are more effective at finding defects than testing. In addition, the medium efficiencies do not rise above 65%, and even the highest efficiencies do not rise about 80%.
Reference: [88] <author> Craig Kaplan, Ralph Clark, and Victor Tang. </author> <title> Secrets of Software Quality. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1995. </year>
Reference-contexts: The tool allows the participants to read the code on-line, make comments, and read other people's comments. The major advantage of the tool is education (people learn from each other before the actual inspection). Unfortunately, the detailed inspection meeting phase was not described in the literature <ref> [88] </ref>. Table 2.3 shows the summary of industrial findings of FTR in terms of cost (hours to find a defect) and efficiency (percentage of total defects found). The Test is included here for comparison purposes. <p> Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 [107] IBM 85% Fagan Doolan92 [41] Shell Research Doolan Kaplan95 <ref> [88] </ref> IBM 3.5 15-25 Kaplan Again some of the data in Table 2.3 may be misleading, since the testing took place after the inspection which had removed a substantial percentage of errors (see also Table 2.2). 2.5.2 Reported FTR problems and limitations Despite numerous reports on successful FTR (mainly inspection), inspection
Reference: [89] <author> Simon M. Kaplan, William J. Tolone, Douglas P. Bogia, and Celsina Bignoli. </author> <title> Flexible, active support for collaborative work with ConversationBuilder. </title> <booktitle> In Proceedings of the ACM 1992 Conference on Computer-supported Cooperative Work, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: In fact, the message systems described earlier may also be considered as groupware toolkits since they are used for building groupware applications (although mainly asynchronous applications). Some examples of these systems include GroupKit [122], Rendezvous [112], DistView [117], Conversation Builder (CB) <ref> [89] </ref>, and Suite [38]. Some systems provide mainly toolkits for building synchronous collaborative applications, such as those that require shared window facilities, concur- rency controls, and session management (e.g., Group Kit, Rendezvous, DistView). Others provide additional facilities for supporting group processes (e.g., Suite and Conversation Builder).
Reference: [90] <author> Ron S. Kenett. </author> <title> Total Quality Management For Software, </title> <booktitle> chapter Understanding the Software Process, </booktitle> <pages> pages 119-137. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: Kenett described that the key for ceasing dependence on inspection is to break down the process into subprocesses, identify their inputs and outputs, their internal suppliers and internal customers, and then to construct feedback loops to improve the subprocesses <ref> [90] </ref>. One example of such approach is the IEEE software development standards IEEE 730.1-1989, which stipulates eight types of reviews to be conducted for a software development process [90] (see also section 2.1). <p> subprocesses, identify their inputs and outputs, their internal suppliers and internal customers, and then to construct feedback loops to improve the subprocesses <ref> [90] </ref>. One example of such approach is the IEEE software development standards IEEE 730.1-1989, which stipulates eight types of reviews to be conducted for a software development process [90] (see also section 2.1). In conclusion, the basic premise of inspection is to discover defects as early as possible in the development process so as to de-amplify the cost of defects downstream and to reduce the overall rework cost. 2. Continuously improve processes.
Reference: [91] <author> John C. Knight and Ethella Ann Myers. </author> <title> Phased inspections and their implementation. </title> <journal> Software Engineering Notes, </journal> <volume> 16(3) </volume> <pages> 29-35, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Other FTR methods use the properties of review products (e.g., compliance with coding standards) as the entry/exit criteria. For example, Phased-Inspection uses exit criteria to ensure that the product possesses certain properties after leaving a phase <ref> [91, 92] </ref>. Table 3.1 shows the summary of FTR components and their numerous values. <p> However, in Fagan's inspection, the entry/exit criteria is applied to the entire review process, as supposed to the individual phases within the process. Only Phased Inspection uses the same concept of having an entry/exit criteria in every phase <ref> [91] </ref>. 3.5 Summary This chapter has presented a FTR framework to distinguish different variations of FTR methods. The framework organizes the review process of a review method into seven different components or dimensions: phase, objective, degree of collaboration, synchronicity, role, technique, entry/exit criteria.
Reference: [92] <author> John C. Knight and Ethella Ann Myers. </author> <title> An improved inspection technique. </title> <journal> Communications of the ACM, </journal> <volume> 11(11) </volume> <pages> 51-61, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Also, many review methods demand a separate preparation phase where reviewers learn source materials before starting error hunting [1, 74, 52], while others consider it optional or unnecessary <ref> [92] </ref>. In conclusion, the wide variations of review practice, even within the same ostensible method, such as Fagan's method, indicate that the methods are defined ambiguously or unclear. <p> The experiment described in this thesis also investigates this question, although from a different perspective. 1.3.3 Variations and inflexibility in review systems implementation Recently, many computer-supported review systems have emerged, such as, ICICLE [17], Scrutiny [61], CSI [98], InspeQ <ref> [92] </ref>, etc. These systems implement a wide variety of FTR methods. For example, ICICLE and Scrutiny implement Fagan's method, CSI implements Humphrey's method, InspeQ implements Phased Inspection, etc. <p> Another difference between Management and Technical reviews is that in the former the attendance typically includes management, whereas in the latter it consists primarily of technical personnel/peers. 4. Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed <ref> [17, 84, 61, 98, 99, 92, 136] </ref>. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed [17, 84, 61, 98, 99, 92, 136]. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods <ref> [84, 92] </ref>. Chapter 4 will discuss these review systems in detail. As described in Chapter 1, this dissertation uses the term Formal Technical Review (FTR) to refer to generic classes of technical reviews as opposed to management reviews, and may include both manual and computer supported reviews. <p> Empirical findings of FTR References Efficiency Group-size Method Hetzel76 [70] 37.3% 1 Code reading Myers78 [105] 38% 3 Code walkthrough Eick92 [47] 75% 8 Design review Martin90 [97] 27% 4 N-Fold inspection (N=1) (Req Doc) Schneider92 [125] 35.1% 4 N-Fold inspection (N=1) (Req Doc) 77.8% 4 N=9 (Req Doc) Knight93 <ref> [92] </ref> 50% 2 Phased Inspection (Phase 6) (Code) Porter94 [115] 25% 3 Checklist (Req doc) 32% 3 AdHoc (Req doc) 50% 3 Scenario (Req doc) Dow94 [42] 20%-46% 4,7 Fagan (Code) Chapter 7 42.8% 3 EGSM (Code) 46.4% 3 EIAM (Code) 2.7 Summary and conclusions This chapter has presented a survey <p> Other FTR methods use the properties of review products (e.g., compliance with coding standards) as the entry/exit criteria. For example, Phased-Inspection uses exit criteria to ensure that the product possesses certain properties after leaving a phase <ref> [91, 92] </ref>. Table 3.1 shows the summary of FTR components and their numerous values. <p> E/X:? Active Stage 1: Stage 2: Stage 3: Design O:comprehension O:comp & exam O:consolidation Review C:group C:individual C:selective subgroup [110] S:STSP S:ASYNC S:STSP,ASYNC R:producer R:reviewer (specialist) R:reviewer-producer T:presentation T:active checklist T:discussion E:? X:time X:no more issues Phased Phase 1: Phase 2: Phase 3: Phase 4: Inspection (single-inspector) (multiple-inspec) (multiple-inspec) (reconciliation) <ref> [92] </ref> O:examination O:exam docs O:comp & exam O:consolidation C:individual C:individual C:individual C:group S:ASYNC S:ASYNC S:ASYNC S:STSP R:clerical staff R:inspector R:inspector R:? T:checklist T:? T:passive&active T:Delphi E:? checklist X:check-marked X:? X:check-marked X:? 32 Table 3.3: Models of major FTR methods (cont.) Method Phase 1 Phase 2 Phase 3 Phase 4 Phase 5 <p> In other words, instead of having Stage 3, this phase actually includes Stage 3 i , where i = 1..n. The exit criteria basically specifies that no more issues remained to be addressed. 3.2.7 Phased Inspection Phased inspection <ref> [92] </ref> consists of a series of coordinated partial inspections termed phases. Each phase is intended to ensure that the product possesses certain properties. The property checked during a given phase is chosen to be intellectually manageable, otherwise it is split into multiple phases. <p> First, it discusses existing computer supported review systems that implement specific FTR methods. These include ICICLE [17], CIA/Scrutiny [61], CSI [98], CAIS [99], InspeQ <ref> [92] </ref>, and QDA/Tammi [136]. The discussion highlights those facilities that are either similar to or different from CSRS. The following subsection compares CSRS with existing computer-supported cooperative work systems. <p> Finally, CSI does not provide support for review roles. 4.4.4 InspeQ InspeQ (Inspecting software in phases to ensure quality) is a computer-based system to support the phased inspection method <ref> [92] </ref>. Phased inspection consists of a series of coordinated partial inspections termed phases. Each phase is intended to ensure that the product possesses certain properties. The property checked during a given phase must be simple enough to be intellectually manageable, otherwise it is split into multiple phases. <p> In both Hetzel's and Basili's method, the examination technique (code reading) was compared to testing; no group interaction was involved. In Porter's study, three different examination (detection) techniques were compared: Ad Hoc, Checklist and Scenario [115]. Myers', Eick's, and Knight's studies <ref> [105, 47, 92] </ref> investigated review methods as a whole (i.e., without focusing on specific review components). Myer's study compared code walkthrough/inspection method with testing. Eick's study involved a two-phase review method to estimate the number of software faults before coding; Knight's study investigated the effectiveness of Phased Inspection method.
Reference: [93] <author> D. M. LaLiberte. Hypernews. </author> <note> http://union.ncsa.uiuc.edu/HyperNews/get/ hypernews.html, </note> <year> 1994. </year>
Reference-contexts: Recently, many WWW clients have been augmented with separate programs or subsystems to support collaborative work. Typical features include support for group annotation and authoring. Examples of these systems include NCSA Mosaic 2.6 with group annotation, HyperNews (WWW + Usenet News) <ref> [93] </ref>, BRIO (xMosaic + PRDM) [121], CoReview (Mosaic + XTV)[96], and WWW + COMET [58]. The latter two systems provide teleconferencing and a synchronous whiteboard facility as well. CoReview specifically supports a collaborative review process by spatially separated reviewers. It also provides support for two roles, the chair and reviewer.
Reference: [94] <author> Richard C. Linger. </author> <title> Cleanroom software engineering for zero-defect software. </title> <booktitle> In Proceedings of the 15th International Conference on Software Engineering, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year> <month> 228 </month>
Reference-contexts: One such example is Verification-Based Inspection that incorporates correctness verification method into the inspection practice [46, 44]. This technique is used in the Cleanroom development method to produce zero defect software <ref> [94] </ref>. Britcher also proposes a similar approach of incorporating correctness arguments into the program which are later checked by inspections [16].
Reference: [95] <author> Thomas W. Malone, Kenneth R. Grant, Kum-Yew Lai, Ramana Rao, and David A. Rosenblitt. </author> <title> Technological Support For Work Group Collaboration, chapter The Information Lens: An Intelligent System For Information Sharing and Coordination, </title> <address> pages 65-88. </address> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1989. </year>
Reference-contexts: The messages themselves are typically typed messages and include a set of fields or attributes. The system compares these values with the user's supplied criteria in order to generate appropriate actions. Examples of message systems include Information Lens <ref> [95] </ref>, Lotus Notes [35], Coordinator [55], Strudel [128], MailTray [120], Atomicmail [15], and Active Mail [65]. Message systems are typically generic systems for building group applications, such as meeting schedulers, information finders, and tools for project management, task tracking, software defect tracking, and collaborative writing.
Reference: [96] <author> K.J. Maly, H. Abdel-Wahab, R. Mukkamala, A. Gupta, A. Prabhu, H. Syed, and C.S. Vemuru. </author> <title> Mosaic + xtv = coreview. </title> <address> http://www.igd.fhg.de/www/www95/ proceed-ings/papers/70/MAIN DOCUMENT.html, </address> <year> 1995. </year>
Reference: [97] <author> Johnny Martin and Wei-Tek Tsai. </author> <title> N-fold inspection: A requirement analysis technique. </title> <journal> Communications of the ACM, </journal> <volume> 33(2) </volume> <pages> 225-232, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Thus, they consider these factors very important for a successful review [110]. Finally, some researchers attribute review effectiveness to paraphrasing, or the use of selective test cases, or checklists [124, 1, 43, 57], or group interaction/composition <ref> [10, 97] </ref>. In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. For example, some studies have shown that in the inspection method, preparation time correlates positively with density of errors found [19, 2, 24]. <p> The FTR framework described in the next chapter is an approach for conducting such an investigation. 24 Table 2.4: Empirical findings of FTR References Efficiency Group-size Method Hetzel76 [70] 37.3% 1 Code reading Myers78 [105] 38% 3 Code walkthrough Eick92 [47] 75% 8 Design review Martin90 <ref> [97] </ref> 27% 4 N-Fold inspection (N=1) (Req Doc) Schneider92 [125] 35.1% 4 N-Fold inspection (N=1) (Req Doc) 77.8% 4 N=9 (Req Doc) Knight93 [92] 50% 2 Phased Inspection (Phase 6) (Code) Porter94 [115] 25% 3 Checklist (Req doc) 32% 3 AdHoc (Req doc) 50% 3 Scenario (Req doc) Dow94 [42] 20%-46% <p> An example of this is the meeting phase in Fagan's method. Finally, the subgroup type involves collaboration between selective group members. Some examples include the N-fold inspection method, where N subgroups perform inspections separately <ref> [97] </ref>, and active design review, where the producer engages in a one-on-one session with individual reviewers [110]. Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy [43, 114, 2]. <p> Many other review methods (namely, Fagan's code inspection and its variations) usually include some combination of individual and group processes. The Selective subgroup process has also been shown to provide unique benefits, such as a significant increase in review performance in N-Fold inspection <ref> [97] </ref>. Roles Roles describe the responsibility of individual group members. Fagan described the importance of roles as: The inspection team is best served when its members play their particular roles, assuming the particular vantage point of those roles [52]. <p> methods (cont.) Method Phase 1 Phase 2 Phase 3 Phase 4 Phase 5 Two-person Inspect1: Inspect2: Inspection O: examination O: examination [10] C: group C: group S: STSP S: STSP R: Reader2 R: Reader1 T: paraphrasing T: paraphrasing X: time X: time N-Fold Individual Meeting Consolidation Inspection O:examination O:examination O:consolidation <ref> [97] </ref> C: individual C:subgroup C:subgroup S:ASYNC S:STSP,ASYNC S:STSP,ASYNC R:reviewer R:? R:moderator T: checklist T:? T:? E/X: ? E/X: ? E/X:? Gilb's Indiv. check Logging meeting Inspection O:examination O:consolidation& [60] C:individual examination S:ASYNC C:group R:specialist S:STSP T:checklist,SPC R:moderator X: time T:pacing NASA's Overview Preparation Meeting Third-hour Inspection O:comprehension O:examination O:examination O:discussion [108] <p> The reader is the person other than the author of the design/code himself. The exit criteria is based on time, which was limited to 20 minutes in the experiment. 3.2.9 N-Fold Inspection N-Fold Inspection <ref> [97] </ref> has been shown to improve detection efficiency of a review method. The basic approach is to replicate a review method (in this case, Fagan's inspection) using N independent teams. With respect to the FTR framework, N independent teams can be viewed as N subgroups. <p> These findings are consistent with the literature. For example, Myers's study of code inspection found 38% of total errors discovered by the group of size 3 [105]. Martin and Tsai's study of user requirements document inspection found 27.2% of errors discovered by the group of size 4 <ref> [97] </ref> (see also Table 2.4). However, replicating the number of groups N fold shows dramatic improvement in the overall detection effectiveness. Figure 7.13 shows the detection effectiveness plots for ICS-411 for N=1,2..7. The data was obtained by combining the existing groups randomly, and counting their detection effectiveness. <p> As shown in the figure, the groups find almost all the errors (90%) after N=7. It also shows that both EGSM and EIAM yield more or less the same results using this technique. These results are consistent with Martin and Tsai's study <ref> [97] </ref>. <p> Eick's study involved a two-phase review method to estimate the number of software faults before coding; Knight's study investigated the effectiveness of Phased Inspection method. Other FTR studies investigated the effect of generic review attributes on review performance, 153 such as group size and replication. Martin's and Schneider's studies <ref> [97, 125] </ref> found that replicating the review group N fold can significantly improve review performance. Their findings are also substantiated by this experiment (see section 7.6.2). The effect of group size observed in this study seems to contradict the Buck's study cited in [10].
Reference: [98] <author> Vahid Mashayekhi, Janet Drake, Wei-Tek Tsai, and John Riedl. </author> <title> Distributed, collaborative software inspection. </title> <journal> IEEE Software, </journal> <volume> 10(5), </volume> <month> September </month> <year> 1993. </year>
Reference-contexts: Some advocate the use of common error checklists when examining source materials [74, 57], others also advocate the use of selective test cases [1, 43]. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) <ref> [98] </ref>. Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection [74, 98, 142]. Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion [127, 110]. <p> Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection <ref> [74, 98, 142] </ref>. Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion [127, 110]. <p> The experiment described in this thesis also investigates this question, although from a different perspective. 1.3.3 Variations and inflexibility in review systems implementation Recently, many computer-supported review systems have emerged, such as, ICICLE [17], Scrutiny [61], CSI <ref> [98] </ref>, InspeQ [92], etc. These systems implement a wide variety of FTR methods. For example, ICICLE and Scrutiny implement Fagan's method, CSI implements Humphrey's method, InspeQ implements Phased Inspection, etc. <p> Another difference between Management and Technical reviews is that in the former the attendance typically includes management, whereas in the latter it consists primarily of technical personnel/peers. 4. Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed <ref> [17, 84, 61, 98, 99, 92, 136] </ref>. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> Again, synchronicity has been shown to play an important factor in FTR. For example, traditional face-to-face meetings (STSP) may cause meeting digression [109, 34]. Mashayekhi's study found that same-time different-place supported FTR meetings can be as effective as manual face-to-face meeting under certain conditions <ref> [98] </ref>. 3.1.3 Technique Technique describes the strategy that review participants use when following a specific interaction to achieve the stated objective. Thus, one may have different techniques for individual examinations, group examination, individual comprehension, group comprehension, and so forth. <p> First, it discusses existing computer supported review systems that implement specific FTR methods. These include ICICLE [17], CIA/Scrutiny [61], CSI <ref> [98] </ref>, CAIS [99], InspeQ [92], and QDA/Tammi [136]. The discussion highlights those facilities that are either similar to or different from CSRS. The following subsection compares CSRS with existing computer-supported cooperative work systems. <p> However, the first review step/operation after the participant successfully connects to the database can be automatically invoked when it is specified through language definition. 4.4.3 CSI CSI (Collaborative Software Inspection) <ref> [98] </ref> is a collaborative inspection system that facilitates effective use of team resources for fault finding and analysis. The review method itself is based on Humphrey's method. Individual inspectors examine the work product and create a fault list during the preparation phase.
Reference: [99] <author> Vahid Mashayekhi, Chris Feuller, and John Riedl. </author> <title> CAIS: Collaborative asynchronous inspection of software. </title> <booktitle> Proceedings of the Second ACM SIGSOFT Symposium on Foundations of Software Engineering, </booktitle> <volume> 19(5) </volume> <pages> 21-34, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Another difference between Management and Technical reviews is that in the former the attendance typically includes management, whereas in the latter it consists primarily of technical personnel/peers. 4. Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed <ref> [17, 84, 61, 98, 99, 92, 136] </ref>. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> First, it discusses existing computer supported review systems that implement specific FTR methods. These include ICICLE [17], CIA/Scrutiny [61], CSI [98], CAIS <ref> [99] </ref>, InspeQ [92], and QDA/Tammi [136]. The discussion highlights those facilities that are either similar to or different from CSRS. The following subsection compares CSRS with existing computer-supported cooperative work systems.
Reference: [100] <author> R. G. Mays, C. L. Jones, G. J. Holloway, and D. P. Studinski. </author> <title> Experiences with defect prevention. </title> <journal> IBM Systems Journal, </journal> <volume> 29(1), </volume> <year> 1990. </year>
Reference-contexts: Jones and Mays then formalized this initiative as a defect prevention methodology, which included a causal-analysis meeting to identify the root cause of the defects and their preventive actions, and the establishment of an action team to implement the preventive actions and to provide feedback to the developers <ref> [86, 100] </ref>. In most practices, the causal-analysis meeting is conducted as an extension to regular inspection, usually at the end of the inspection process [41, 60]. Finally, the principle of continuous process improvement must also occur at a personal level.
Reference: [101] <author> John McKissick, Mark J. Somers, and Wilhelmina Marsh. </author> <title> Software design inspection for pre-liminary design. </title> <booktitle> In Proceedings of the 1984 Computer Software and Applications Conference (COMPSAC '84), </booktitle> <pages> pages 273-281, </pages> <address> Las Vegas, NV, </address> <month> July </month> <year> 1984. </year>
Reference-contexts: Like walkthroughs, the term Inspections is also used in the literature in many different ways. For example, many researchers associate inspections with paraphrasing techniques, in which a designated reader paraphrases the materials statement by statement [106, 3, 124, 1], others downplay the role of reader <ref> [41, 101, 74, 60] </ref>. Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides [60]. <p> The inspection process started with the distribution of inspection materials to individual inspectors for meeting preparation. During the meeting, all inspectors presented their findings and questions resulting from the review. The moderator/recorder recorded deficiencies. At the end of the meeting, the inspectors voted to accept or reject the design <ref> [101] </ref>. * Russell reported a case study with Fagan's inspection in ultralarge-scale software developments. This involved about 2.5 million lines of high-level code, at Bell-Northern Research for eight software releases over a two-year interval. <p> experiences in industry References Organization Cost Efficiency Method Inspect Test Inspect Test Fagan76 [52] Aetna 82% 18% Fagan Fagan86 [53] IBM UK 93% Fagan Standard Bank &gt;50% Fagan AMEX &gt;50% Fagan Ackerman89 [1] Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 <ref> [101] </ref> GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 [107] IBM 85% Fagan Doolan92 [41] Shell Research Doolan Kaplan95 [88] IBM 3.5 15-25 Kaplan Again some of <p> Another common technique is to let the meeting leader talk through review materials, while participants interrupt with the issues found during the private phase (sometimes called the document pacing technique) [60, 142, 115]. Still another technique is simply asking each participant in turn to report his/her issues <ref> [101] </ref>, or to report and revise the issues as necessary following several rounds (i.e., Delphi technique [71]). 3.1.4 Entry/exit criteria The entry/exit criteria defines the starting or completion condition of a review phase. <p> The meeting phase has the objective of consolidation using a reporting technique, in which each participant presents his/her findings in turn from individual reviews <ref> [101] </ref>. * Russell's inspection uses paraphrasing during the meeting phase and assigns the reader role to the inspector who is not the producer.
Reference: [102] <author> Harlan D. Mills. </author> <title> Total Quality Management For Software, chapter Cleanroom Engineering: </title> <journal> Engineering Software Under Statistical Quality Control, </journal> <pages> pages 463-483. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold, </publisher> <year> 1992. </year>
Reference-contexts: This leads the programmer by the nose from one problem to the next. This improvement in compile and test performance also makes the development process more predictable, and results in a better product. Mills described the importance of formal verification methods by drawing an analogy with touch typing methods <ref> [102] </ref>. This analogy applies to software reviews as well. The issue is, how can one type text without errors at reasonable rates of speed? One way is to go back and forth looking at keys and the text while typing. <p> In the case of the shuttle software experience which delivered a very low error rate, Mills indicated that formal correctness proofs may have contributed to the overall inspection quality <ref> [102] </ref>. The next chapter is an attempt to provide such answers by using the FTR framework. 25 Chapter 3 Formal Technical Review Framework As described in the previous chapters, existing FTR methods are defined somewhat ambiguously, and the different FTR terminologies are often confusing.
Reference: [103] <author> B. Mullen, C. Johnson, and E. Salas. </author> <title> Productivity loss in brainstorming groups. </title> <journal> Basic and Applied Social Psychology, </journal> (12):3-24, 1991. 
Reference-contexts: The experiment conducted as part of this research was an attempt to answer the latter question. 106 Similar studies in behavioral sciences have found that nominal groups generally perform better than real groups <ref> [39, 103] </ref>. The task given to the subjects in those studies, however, involves primarily idea generation.
Reference: [104] <author> David G. Myers. </author> <title> Social Psychology. </title> <publisher> McGraw-Hill Publishing Company, </publisher> <address> 3 edition, </address> <year> 1990. </year>
Reference-contexts: Other key issues include social facilitation, where the presence of others may boost one's performance (akin to group synergy), and social loafing, where one member free rides on the group's effort <ref> [104] </ref>. The issue of domination in inspections has been well documented in the literature (i.e., moderator dominates inspection) [34]. <p> In other words, whether individuals working in a group (using EGSM) contributed more or less the same percentage of errors as when working alone (using EIAM). Theoretically, working in a group and feeling the presence of others may boost one's performance, a phenomenon often called social facilitation <ref> [104] </ref>. In EGSM, individual contributions were measured by counting the percentage of valid issue nodes that were first suggested by the individuals regardless who inspired them (i.e., Suggested-by field value is equal to 1 or 2).
Reference: [105] <author> Glenford J. Myers. </author> <title> A controlled experiment in program testing and code walkthrough/inspection. </title> <journal> Communications of the ACM, </journal> <volume> 21(9) </volume> <pages> 760-768, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: Thus, to obtain high reliability, it is necessary to string together a well-planned series of reviews, inspections, and tests. Most researchers also agree that reviews must supplement testing; reviews tend to discover different types of errors than the those discovered by testing <ref> [105, 63] </ref>. In some cases, reviews can find errors that cannot be possibly found in testing [149]. Table 2.1: Defect detection efficiency for code [from Jones86] Removal step Lowest Med Highest eff. eff. eff. <p> Myers used a combination of mentally executing test-cases and checking the logic 15 for common errors <ref> [105] </ref>. Basili & Selby used stepwise abstraction, in which the prime subprograms are identified, their individual functions are derived and compared against the specifications [7]. Collofello & Woodfield did not specify the processes used in reviews and testing. <p> Table 2.2: Research findings on Review v.s. Test References Detection Efficiency Cost Effectiveness Hetzel76 [70] code reading (37.3%) &lt; functional testing (47.7%) = structural testing (46.7%) Myers78 <ref> [105] </ref> code walkthrough (38%) = code walkthrough functional testing (30%) = most expensive structural testing (36%) Basili87 [7] code reading &gt; (16% more) code reading functional testing &gt; (11% more) least expensive structural testing Collofello89 [28] code review (64%)&gt; design review &gt; design review (54%) &gt; code review &gt; testing (38%) <p> One of the research goals of this dissertation is to investigate these differences. The FTR framework described in the next chapter is an approach for conducting such an investigation. 24 Table 2.4: Empirical findings of FTR References Efficiency Group-size Method Hetzel76 [70] 37.3% 1 Code reading Myers78 <ref> [105] </ref> 38% 3 Code walkthrough Eick92 [47] 75% 8 Design review Martin90 [97] 27% 4 N-Fold inspection (N=1) (Req Doc) Schneider92 [125] 35.1% 4 N-Fold inspection (N=1) (Req Doc) 77.8% 4 N=9 (Req Doc) Knight93 [92] 50% 2 Phased Inspection (Phase 6) (Code) Porter94 [115] 25% 3 Checklist (Req doc) 32% <p> The study found that this method was inferior to testing. This is likely attributed to the examination technique used. 40 Myers' study Myers' study investigates the effectiveness of code walkthrough/inspection compared to testing <ref> [105] </ref>. The review method involves two phases: preparation and meeting. During the preparation phase, the subjects learn the programs individually. Thus, this phase has the objective of comprehension. <p> These findings are consistent with the literature. For example, Myers's study of code inspection found 38% of total errors discovered by the group of size 3 <ref> [105] </ref>. Martin and Tsai's study of user requirements document inspection found 27.2% of errors discovered by the group of size 4 [97] (see also Table 2.4). However, replicating the number of groups N fold shows dramatic improvement in the overall detection effectiveness. <p> In both Hetzel's and Basili's method, the examination technique (code reading) was compared to testing; no group interaction was involved. In Porter's study, three different examination (detection) techniques were compared: Ad Hoc, Checklist and Scenario [115]. Myers', Eick's, and Knight's studies <ref> [105, 47, 92] </ref> investigated review methods as a whole (i.e., without focusing on specific review components). Myer's study compared code walkthrough/inspection method with testing. Eick's study involved a two-phase review method to estimate the number of software faults before coding; Knight's study investigated the effectiveness of Phased Inspection method.
Reference: [106] <author> Glenford J. Myers. </author> <title> The Art of Software Testing. </title> <publisher> John Wiley and Sons, IBM Systems Research Institute, </publisher> <year> 1979. </year>
Reference-contexts: Myers characterized walkthrough as playing computer in which the person designated as the tester comes to the meeting armed with a small set of test cases and mentally executes them <ref> [106] </ref>. Gilb and Graham described the difference between inspection and walkthrough as its focus: inspection focuses on defect identification, while walkthrough focuses on learning, i.e., walkthrough is generally a training process [60]. <p> Myers once stated that software review is an effective means of finding software faults, however, no one knows why the technique works <ref> [106] </ref>. Only a few studies have attempted to investigate the role of specific review factors. For example, Votta found that the inspection meeting is not necessarily cost-effective [142]. <p> Eick et al. also reported that 10% of errors are found during group meeting while 90% of errors are found during preparation [47]. Myers and Parnas attribute the success of review to the synergy effect between the producer and the reviewers <ref> [106, 110] </ref>, yet others discourage active participation of the producer during the meeting [1, 124]. Parnas and Weiss explicitly criticize the effectiveness of Fagan's inspection because it lacks specific roles/responsibilities and a systematic technique for individual reviewers to conduct the inspection. <p> Programmers have used it since the time of the first program. The oldest practice of software review is called desk checking or code reading, which is, reading over a program by hand while sitting at one's desk <ref> [106, 3] </ref>. Desk checking is often viewed as ineffective and unproductive. One reason is that it is a completely undisciplined process. Another reason is that people are usually ineffective in catching their own mistakes. <p> Some of the most common review types or review methods include: 1. Peer Ratings. Peer Rating is a technique of evaluating anonymous programs in terms of their overall quality, maintainability, extensibility, usability and clarity by selected programmers who have similar backgrounds <ref> [106] </ref>. The concept of peer rating is similar to the rating process for office workers. It is anonymous and given by one's peers. The general procedure is as follows. First, each participant turns in his or her programs to the administrator. <p> Some refer it as structured walkthroughs [150]. Generally, the literature describes walkthrough as an undisciplined process without advanced preparation on the part of reviewers, and the meeting focuses on education purposes [27, 52]. However, many authors consider it a disciplined and formal review process <ref> [106, 150, 3] </ref>. 3. Round-robin reviews. <p> Like walkthroughs, the term Inspections is also used in the literature in many different ways. For example, many researchers associate inspections with paraphrasing techniques, in which a designated reader paraphrases the materials statement by statement <ref> [106, 3, 124, 1] </ref>, others downplay the role of reader [41, 101, 74, 60]. Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides [60]. <p> Yet another explanation is that one can never know the actual number of errors in the program. The industrial data is most likely based on the total errors found at the end of the testing process, which can be much smaller than the actual number of errors <ref> [106] </ref>. In the empirical studies, the number of seeded errors are known. In any case, the differences in review methods seem to affect the overall review performance. One of the research goals of this dissertation is to investigate these differences. <p> some FTR practices advocate the active participation of the producer during group meetings (i.e., have the producer play the role of presenter or reader) for self-debugging mechanism, others discourage the producer's participation to prevent them from brainwashing the reviewers into making the same erroneous assumptions about the product being reviewed <ref> [106, 1, 150] </ref>. Active Design Review [110] and Scenario based review method [115] advocate the use of specialists for an effective review [110] Synchronicity Synchronicity describes the physical interaction mode among participants. <p> Thus, one may have different techniques for individual examinations, group examination, individual comprehension, group comprehension, and so forth. Some examples of examination techniques include the paraphrasing technique used during the group meeting in Fagan's inspection [52], selective test cases (test data) <ref> [106, 3] </ref>, checklists of common errors (i.e., classes of software errors) used during the individual examination phase in Humphrey's method [74], active checklist used during individual review phase in Active Design Review [110], scenarios used in Porter and Votta's experiment [115], the stepwise abstraction technique used during the individual examination phase <p> A question mark (?) indicates that the corresponding review component is not described in the literature. 31 Table 3.2: Models of major FTR methods Method Phase 1 Phase 2 Phase 3 Phase 4 Peer Rating Review Summary <ref> [106, 129] </ref> O:examination O:consolidate C:individual C:individual S:ASYNC S:ASYNC R:reviewer R:administrator T:evaluation form T:? E:completed form E:? Structured Preparation: Walkthrough: Walkthrough O:comprehension O:examination [150] C:individual, C:group selective subgroup S:ASYNC,SYNC S:SYNC R:reviewer,producer R:producer T:reading,discussion T:paraphrasing E/X:? E/X:? Round-robin Phase i Phase i+1 [69] O:examination O:examination C:subgroup C:subgroup S:ASYNC S:ASYNC R:reviewer i R:reviewer i+1 <p> O:consol C:group C:indiv,group C:group C:individual C:group S:STSP S:ASYNC S:ASYNC S:ASYNC S:SYNC R:producer R:reviewer,producer R:moderator R:moderator R:moderator T:presentation T:question-answer, T:Delphi T:tools T:voting free exam E:unresolved E:source readiness E:source availability E:source nodes E:commentary issues remained reviewed reviewed X:no more issue 33 3.2.1 Peer Rating The Peer Rating method of Shneiderman and Myers <ref> [106, 129] </ref> (described in Chapter 2) defines two roles: Administrator and Reviewer, and two phases: Review and Summary. Prior to entering the review phase, the Administrator collects the programs to be reviewed from the programmers and distributes them to the reviewers (i.e., other programmers).
Reference: [107] <author> Ware Myers. </author> <title> Shuttle code achieves very low error rate. </title> <journal> IEEE Software, </journal> <pages> pages 93,95, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The Space Shuttle software produced by IBM Federal Systems Divisions has achieved very low error rates (0.11 errors per thousand lines of source code excluding comments). Substantial credit was given to Fagan's inspection <ref> [107] </ref> Freedman and Weinberg reported that in large systems, reviews have reduced the number of errors reaching the testing stages by a factor of 10 [56]. This also translates to reduction in testing costs by 50% to 80% including review costs. <p> This success was later attributed to Fagan's inspection. The detection efficiency of the inspections increased from about 50 percent in 1982 to 85 percent in 1985 <ref> [107] </ref>. * Doolan reported on the experience of instituting Fagan's inspection at Shell Research to validate requirements specifications. The primary motivation was to reduce maintenance costs associated with fixing fault-reports received from the users. Nearly half of these faults originated from incorrect requirements specifications. <p> Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 <ref> [107] </ref> IBM 85% Fagan Doolan92 [41] Shell Research Doolan Kaplan95 [88] IBM 3.5 15-25 Kaplan Again some of the data in Table 2.3 may be misleading, since the testing took place after the inspection which had removed a substantial percentage of errors (see also Table 2.2). 2.5.2 Reported FTR problems and
Reference: [108] <author> NASA. </author> <title> Software formal inspections standard (nasa-std-2202-93). </title> <address> http://satc.gsfc.nasa.gov /fi/std/fistdtxt.txt, </address> <year> 1993. </year>
Reference-contexts: Gilb refers to consolidation as defects logging, and Votta and Porter refer to it as collection. Correlation can be found in Humphrey's method [74]. Discussion can be found in the third hour meeting phase of some inspection methods <ref> [21, 108] </ref>. In practice, a single phase may implicitly include multiple activities. For example, during Fagan's preparation phase, one may discover errors while attempting to comprehend the code. During defect finding, one may obtain additional comprehension while examining the code. <p> discussion since it often leads to discovery of new errors [69, 41] For the same reason, some review practices/methods include a separate phase, called third-hour meeting (so called because it takes place after the two-hour regular inspection meetings) where the objective of the phase is mainly to discuss solution ideas <ref> [21, 108] </ref>. 3.1.2 Interaction The interaction component of FTR framework specifies the group process that takes place among review participants. <p> [97] C: individual C:subgroup C:subgroup S:ASYNC S:STSP,ASYNC S:STSP,ASYNC R:reviewer R:? R:moderator T: checklist T:? T:? E/X: ? E/X: ? E/X:? Gilb's Indiv. check Logging meeting Inspection O:examination O:consolidation& [60] C:individual examination S:ASYNC C:group R:specialist S:STSP T:checklist,SPC R:moderator X: time T:pacing NASA's Overview Preparation Meeting Third-hour Inspection O:comprehension O:examination O:examination O:discussion <ref> [108] </ref> C:group C:individual C:group C:group S:STSP S:ASYNC S:STSP S:STSP R:author R:reviewer R:reader R:? T:presentation T:? T:paraphrasing T:? E/X:? E/X:? X:time X:time Humphrey's Review Personal O:examination Review I:no T:checklist,SPC [76] X:source reviewed& items checked Verification Inspection: ? Inspection O:examination [44] C:individual S:ASYNC R:reviewer T:stepwise verification E/X:? FTArm Orientation: Private: Public: Consolidation: Meeting:
Reference: [109] <author> J. F. Nunamaker, Alan R. Dennis, Joseph S. Valacich, Douglas R. Vogel, and Joey F. George. </author> <title> Electronic meeting systems to support group work. </title> <journal> Communication of the ACM, </journal> <volume> 34(7) </volume> <pages> 42-61, </pages> <month> July </month> <year> 1991. </year> <month> 229 </month>
Reference-contexts: Computer supported FTR may operate on all levels of synchronicity (STSP, STDP, DTSP, and DTDP). Again, synchronicity has been shown to play an important factor in FTR. For example, traditional face-to-face meetings (STSP) may cause meeting digression <ref> [109, 34] </ref>. Mashayekhi's study found that same-time different-place supported FTR meetings can be as effective as manual face-to-face meeting under certain conditions [98]. 3.1.3 Technique Technique describes the strategy that review participants use when following a specific interaction to achieve the stated objective. <p> The degree of collaboration is compatible with an extension of such taxonomy, which includes multiple individual sites, one group size, or multiple group sites <ref> [109] </ref>. However, unlike typical group systems, the Interaction component of FTR framework also includes Roles. The latter was again inspired by Fagan's inspection [52]. The Technique component was suggested by existing review methods that use different review tactics, such as checklist, paraphrasing, etc. <p> The goal of these systems is to facilitate meeting activities with proper computational support so as to make the meeting more productive. 75 Some examples of these systems include Electronic Meeting System or GroupSystem <ref> [109] </ref>, Colab [132], and Capture Lab [49]. <p> The major facilities provided by these systems include a shared electronic whiteboard (e.g., Colab), or a large screen with video projector (e.g., Capture Lab and EMS), and a set of software tools for supporting meeting activities, such as the brainstorming, organizing, evaluating, proposing, and arguing activities <ref> [132, 109] </ref>. The basic design of such meeting support systems is the provision of public workspace that implements WYSIWIS (What You See Is What I See), or all meeting participants see exactly the same thing [132]. <p> Some of these private activities include taking notes, making short calculations, extending work on one piece of the group tasks [49], or parallel communication where individual participants express their opinions simultaneously without waiting for someone else to finish speaking <ref> [109] </ref>. In general, meeting support systems bear some resemblance with CSRS. They provide facilities for making the synchronous group process more productive, such as the provision of public and private workspaces, and specific meeting support tools. The meeting tools can be implemented in CSRS using the data modeling language facilities. <p> With asynchronous meetings, the reviewers read and inspect the materials individually at their own pace; no reader is required. Some studies in computer-supported collaborative work also indicate that asynchronous meetings have unique benefits compared to synchronous face-to-face meetings <ref> [109] </ref>. These include: * Reduce free riding. The participants cannot rely on others for contributing ideas. * Reduce production blocking. Production blocking prevents participants from contributing ideas as they occur, or pausing to think, because they have to constantly listen to others speaking. <p> Instead of having a face-to-face meeting, the group interacts asynchronously using computer. Many studies in collaborative work have found that this type of group interaction is more effective than a face-to-face meeting <ref> [109] </ref>. This experimental study can be easily performed using CSRS by implementing yet another review method: EGAM (Experimental Group Asynchronous Method). A three way comparative study of EGSM, EGSM and EIAM can then be performed. * EGSM/EIAM versus Manual reviews.
Reference: [110] <author> David L. Parnas and David M. Weiss. </author> <title> Active design reviews: </title> <booktitle> Principles and practices. Pro-ceedings of Eighth International Conference on Software Engineering, </booktitle> <address> London, England, </address> <pages> pages 132-136, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Over the past several years, many different forms of inspections or formal reviews have emerged. Some of these include: Structured Walkthrough [149], Active Design Reviews <ref> [110] </ref>, Humphrey's inspection [74], Verification-Based Inspection [45], and so forth. Throughout this dissertation, the term formal technical review (FTR) is used to refer to a general class of reviews, which includes the specific review techniques described above. <p> Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection [74, 98, 142]. Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion <ref> [127, 110] </ref>. Also, many review methods demand a separate preparation phase where reviewers learn source materials before starting error hunting [1, 74, 52], while others consider it optional or unnecessary [92]. <p> Eick et al. also reported that 10% of errors are found during group meeting while 90% of errors are found during preparation [47]. Myers and Parnas attribute the success of review to the synergy effect between the producer and the reviewers <ref> [106, 110] </ref>, yet others discourage active participation of the producer during the meeting [1, 124]. Parnas and Weiss explicitly criticize the effectiveness of Fagan's inspection because it lacks specific roles/responsibilities and a systematic technique for individual reviewers to conduct the inspection. <p> Parnas and Weiss explicitly criticize the effectiveness of Fagan's inspection because it lacks specific roles/responsibilities and a systematic technique for individual reviewers to conduct the inspection. Thus, they consider these factors very important for a successful review <ref> [110] </ref>. Finally, some researchers attribute review effectiveness to paraphrasing, or the use of selective test cases, or checklists [124, 1, 43, 57], or group interaction/composition [10, 97]. In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. <p> Finally, the subgroup type involves collaboration between selective group members. Some examples include the N-fold inspection method, where N subgroups perform inspections separately [97], and active design review, where the producer engages in a one-on-one session with individual reviewers <ref> [110] </ref>. Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy [43, 114, 2]. However, some review experiences/studies found that more errors are caught in individual process than group process [74, 142, 115, 47]. <p> Other domain specific roles include Designer (who checks for defects in the design), Tester (who checks the materials from the testing point of view) [52], and other specialists <ref> [110] </ref>. Many people consider review roles as an important review factor. <p> Active Design Review <ref> [110] </ref> and Scenario based review method [115] advocate the use of specialists for an effective review [110] Synchronicity Synchronicity describes the physical interaction mode among participants. <p> Active Design Review <ref> [110] </ref> and Scenario based review method [115] advocate the use of specialists for an effective review [110] Synchronicity Synchronicity describes the physical interaction mode among participants. <p> include the paraphrasing technique used during the group meeting in Fagan's inspection [52], selective test cases (test data) [106, 3], checklists of common errors (i.e., classes of software errors) used during the individual examination phase in Humphrey's method [74], active checklist used during individual review phase in Active Design Review <ref> [110] </ref>, scenarios used in Porter and Votta's experiment [115], the stepwise abstraction technique used during the individual examination phase in Basili's method [8], and statistical process control in Humphrey's Personal Reviews [76]. Many review practices, however, do not use any techniques for examination. <p> A rather unique technique for individual comprehension is active-checklist, where the producer develops a set of questions for reviewers to answer regarding the design rationale/assumptions in the source materials. By answering these questions, the reviewers learn about the materials <ref> [110] </ref>. Another comprehension technique that has been shown to improve FTR effectiveness is the one suggested by Rifkin and Diemel [119, 33]. This technique involves basic top-down and bottom-up code reading strategies. <p> Inspection O:comprehension O:comp & exam O:correlation O:consolidation [74] C:group C:individual C:subgroup C:group S:STSP S:ASYNC S:ASYNC S:STSP R:producer R:reviewer R:moderator&producer R:producer T:presentation T:reading & T:? T:presentation std checklist E:clean compilation E/X:? E/X:? E/X:? Active Stage 1: Stage 2: Stage 3: Design O:comprehension O:comp & exam O:consolidation Review C:group C:individual C:selective subgroup <ref> [110] </ref> S:STSP S:ASYNC S:STSP,ASYNC R:producer R:reviewer (specialist) R:reviewer-producer T:presentation T:active checklist T:discussion E:? X:time X:no more issues Phased Phase 1: Phase 2: Phase 3: Phase 4: Inspection (single-inspector) (multiple-inspec) (multiple-inspec) (reconciliation) [92] O:examination O:exam docs O:comp & exam O:consolidation C:individual C:individual C:individual C:group S:ASYNC S:ASYNC S:ASYNC S:STSP R:clerical staff R:inspector R:inspector <p> Thus, the consolidation technique used is presentation. The entry and exit criteria are not specified. Similar to Fagan's method, Humphrey's method in practice includes planning and post-inspection phases. Since these phases deal with administrative procedures, they are not included in the model. 3.2.6 Active Design Review Active Design Review <ref> [110] </ref> is a review method for inspecting design documents, in which the reviewers are forced to take an active role by making assertions about design decisions. Furthermore, only reviewers with specific expertise (specialists) participate in the review. <p> The techniques used for both activities are standard-checklist (domain specific checks) and active-checklist (application specific checks). The latter technique is similar to the one used in Active Design Review <ref> [110] </ref>. The exit criteria is to ensure that all checklist items have been marked or inspected. The final phase is reconciliation, where the inspectors compare their findings in a face-to-face meeting using Delphi-like technique [71]. This corresponds to group consolidation. <p> The nature of the items is passive, that is, they solicit a yes/no answer (e.g., Are the inputs and outputs for all interfaces sufficient?). The Scenario technique is the most systematic procedure for finding defects. It uses an active checklist similar to Active Design Review <ref> [110] </ref>, in which the reviewers are required to supply answers to the questions specified in the checklist (e.g., Identify all data objects mentioned in this module). The review process involves two phases: preparation and collection. <p> The checklist items can also be defined as required or optional. For required items, the reviewers must explicitly check off the items when examining the corresponding source nodes. One may also define active checklists similar to the ones suggested by Parnas <ref> [110] </ref>. An active checklist forces reviewers to elaborate responses to checklist questions, instead of simply providing yes/no or checked/unchecked answers. with the role of Reviewer during the Private-Review phase. The second language construct specifies how checklist items are installed.
Reference: [111] <author> David L. Parnas and David M. Weiss. </author> <title> Active design reviews: Principles and practices. </title> <journal> Journal of Systems and Software, </journal> <volume> 7 </volume> <pages> 259-265, </pages> <year> 1987. </year>
Reference-contexts: Pressman suggested a combination of informal and formal walkthroughs for design reviews [118]. Parnas suggested a new review method called Active Design Review for the design review (this method will be discussed in detail in the later chapter) <ref> [111] </ref>. With respect to review participants, Glass noted that the personnel makeup of the review changes dramatically as the life cycle proceeds.
Reference: [112] <author> J.F. Patterson, R.D. Hill, </author> <title> S.L. Rohall, and W.S. Meeks. Rendezvous: An architecture for synchronous multi-user applications. </title> <booktitle> Proceedings of the ACM Conference on ComputerSupported Cooperative Work, </booktitle> <pages> pages 317-328, </pages> <year> 1990. </year>
Reference-contexts: Groupware toolkits Groupware toolkits are generic systems to build groupware applications. In fact, the message systems described earlier may also be considered as groupware toolkits since they are used for building groupware applications (although mainly asynchronous applications). Some examples of these systems include GroupKit [122], Rendezvous <ref> [112] </ref>, DistView [117], Conversation Builder (CB) [89], and Suite [38]. Some systems provide mainly toolkits for building synchronous collaborative applications, such as those that require shared window facilities, concur- rency controls, and session management (e.g., Group Kit, Rendezvous, DistView).
Reference: [113] <author> Mark C. Paulk, Bill Curtis, and Mary Beth Chrissis. </author> <title> Capability maturity model, version 1.1. </title> <journal> IEEE Software, </journal> <pages> pages 18-27, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: For example, IEEE STD 730-1984 requires software contractors to use systematic and well-documented software development methods that include inspections/software reviews. The CMM defines Peer Review as the key process area for software organizations to mature to level 3 (Defined process) <ref> [113] </ref>. ISO 9001 states that All product is subject to one or more reviews, inspections and/or tests before being submitted for acceptance and release as a product [80]. Despite its importance and benefits, there are many aspects of FTR that are currently not understood. <p> The field study conducted by Shneiderman suggests that peer ratings of programs are productive, enjoyable, and nonthreatening experiences [129]. Peer Ratings are often called Peer Reviews [129]. However, some authors use the term Peer Reviews as generic review methods involving peers <ref> [113, 74] </ref>. 2. Walkthroughs. Walkthroughs can be viewed as presentation reviews, in which a review participant, usually the developer of the software being reviewed, narrates a description of the software and the remainder of the review group provides their feedback throughout the presentation [56, 27, 60]. <p> stem, in large part, from the quality of the process used to create them [74].This principle is applied by the Department of Defense (DoD) under the name of Total Quality Management (TQM) [5], and Software Engineering 16 Institute (SEI) under the name of Capability Maturity Model or Software Process Maturity <ref> [113, 74] </ref>. The underlying concepts of managing and improving software process can be traced back to Dr. W. Edwards Deming's work on the industrial quality improvement in the post World War II [37].
Reference: [114] <author> Ronald Peele. </author> <title> Code inspections at first union corporation. </title> <booktitle> In Proceedings of the IEEE Computer Society's Eighth International Computer Software and Applications Conference (COMPSAC'82), </booktitle> <pages> pages 445-446, </pages> <address> Silver Springs, MD., November 1982. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Many other studies, however, provide conflicting and/or anecdotal explanation of the causal factors underlying review effectiveness. For example, Basili attribute review effectiveness to a technique called stepwise abstraction [8, 7]. Dunn, Peele, Ackerman et al. attribute the success of software review to the presence of group synergy <ref> [43, 114, 2] </ref>, however, Humphrey reported that 75% of errors are found during individual preparation rather than in the group meeting [74]. Eick et al. also reported that 10% of errors are found during group meeting while 90% of errors are found during preparation [47]. <p> Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy <ref> [43, 114, 2] </ref>. However, some review experiences/studies found that more errors are caught in individual process than group process [74, 142, 115, 47]. Many other review methods (namely, Fagan's code inspection and its variations) usually include some combination of individual and group processes. <p> EGSM will detect significantly more errors than EIAM. The rationale for this hypothesis is that the presence of group synergy (in EGSM) will lead to the discovery of more errors than in nominal groups (EIAM) as suggested by many FTR practices <ref> [43, 114, 2] </ref>. 2. H2: There will be significant differences in detection cost between EGSM and EIAM. EGSM will cost significantly more than EIAM.
Reference: [115] <author> Adam A. Porter and Lawrence G. Votta. </author> <title> An experiment to assess different detection methods for software requirements inspections. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: [70] 37.3% 1 Code reading Myers78 [105] 38% 3 Code walkthrough Eick92 [47] 75% 8 Design review Martin90 [97] 27% 4 N-Fold inspection (N=1) (Req Doc) Schneider92 [125] 35.1% 4 N-Fold inspection (N=1) (Req Doc) 77.8% 4 N=9 (Req Doc) Knight93 [92] 50% 2 Phased Inspection (Phase 6) (Code) Porter94 <ref> [115] </ref> 25% 3 Checklist (Req doc) 32% 3 AdHoc (Req doc) 50% 3 Scenario (Req doc) Dow94 [42] 20%-46% 4,7 Fagan (Code) Chapter 7 42.8% 3 EGSM (Code) 46.4% 3 EIAM (Code) 2.7 Summary and conclusions This chapter has presented a survey of software reviews, their differences in terminologies and actual <p> Although consolidation does not involve examining new materials, some studies show that new errors are often discovered during this activity as a result of group interaction [41]. However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors <ref> [142, 116, 115] </ref>. Finally, group discussion to resolve issues or to find solutions is often considered detrimental to the review process and thus, should be forbidden [52, 124, 60]. This happens, for example, during the inspection meeting phase. <p> Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy [43, 114, 2]. However, some review experiences/studies found that more errors are caught in individual process than group process <ref> [74, 142, 115, 47] </ref>. Many other review methods (namely, Fagan's code inspection and its variations) usually include some combination of individual and group processes. The Selective subgroup process has also been shown to provide unique benefits, such as a significant increase in review performance in N-Fold inspection [97]. <p> Active Design Review [110] and Scenario based review method <ref> [115] </ref> advocate the use of specialists for an effective review [110] Synchronicity Synchronicity describes the physical interaction mode among participants. <p> meeting in Fagan's inspection [52], selective test cases (test data) [106, 3], checklists of common errors (i.e., classes of software errors) used during the individual examination phase in Humphrey's method [74], active checklist used during individual review phase in Active Design Review [110], scenarios used in Porter and Votta's experiment <ref> [115] </ref>, the stepwise abstraction technique used during the individual examination phase in Basili's method [8], and statistical process control in Humphrey's Personal Reviews [76]. Many review practices, however, do not use any techniques for examination. They rely only on reviewers' intuition and experience to find errors. <p> Many review practices, however, do not use any techniques for examination. They rely only on reviewers' intuition and experience to find errors. This dissertation refers to it as free technique; Porter et al. term it Ad Hoc detection method <ref> [115] </ref>. In contrast to examination, there are only a few techniques currently developed for comprehension. The most common technique used for group comprehension is presentation where the producer presents the materials, and the reviewers interrupt with questions. <p> Another common technique is to let the meeting leader talk through review materials, while participants interrupt with the issues found during the private phase (sometimes called the document pacing technique) <ref> [60, 142, 115] </ref>. Still another technique is simply asking each participant in turn to report his/her issues [101], or to report and revise the issues as necessary following several rounds (i.e., Delphi technique [71]). 3.1.4 Entry/exit criteria The entry/exit criteria defines the starting or completion condition of a review phase. <p> Porter and Votta's study Porter and Votta's study investigates the effectiveness of three different detection techniques: Ad Hoc, Checklist and Scenario <ref> [115, 116] </ref>. The Ad Hoc technique is a nonsystematic procedure to find defects. The reviewers rely on intuition and experience in finding errors. However, they are given a generic list of defect taxonomy, such as missing functionality, missing performance, etc. <p> For example, a new FTR method could use a Scenario-based checklist, instead of a regular checklist for its examination technique, since the technique has been tested empirically <ref> [115] </ref>. The FTR space also allows one to investigate the effectiveness of individual review factors for FTR improvement. For example, whether a group examination technique based on selective test cases would yield better performance than a traditional paraphrasing technique. <p> individual activities (i.e., during the individual preparation phase) [52]: Sometimes flagrant errors are found during this operation (i.e., preparation phase), but in general, the number of errors found is not nearly as high as in the inspection operation (i.e., meeting phase) In contrast, recent empirical studies by Votta and Porter <ref> [142, 116, 115] </ref> found that group meetings may not be cost effective compared to the preparation phase in which individual participants work alone. They found that meeting gains offset meeting losses; there was only 4% increase in faults found at the meeting [142]. <p> Russell referred to this term as detection efficiency [124]. Weller used detection rate to refer to the number of defects per KLOC [147]. Porter and Votta used detection rate to refer to the percentage of defects found per total defects in the source <ref> [115] </ref>. Only Selby used detection rate as number of faults detected per hour similar to the one used in this study [127]. Table 7.9 shows the resulting Wilcoxon analysis for ICS-313, ICS-411, and All groups. <p> This second phase can also increase participants' confidence and facilitate learning. Actually, this two-phase review method is commonly practiced <ref> [74, 142, 115] </ref> (see also Chapter 3). However, as pointed out by Votta [142], this second phase could be inefficient if not carefully administered. For example, instead of having the entire group members participate in the second phase, one might select only a subset of the participants. <p> In both Hetzel's and Basili's method, the examination technique (code reading) was compared to testing; no group interaction was involved. In Porter's study, three different examination (detection) techniques were compared: Ad Hoc, Checklist and Scenario <ref> [115] </ref>. Myers', Eick's, and Knight's studies [105, 47, 92] investigated review methods as a whole (i.e., without focusing on specific review components). Myer's study compared code walkthrough/inspection method with testing. <p> This eventually contributes toward an improvement of FTR field. For example, a new method could use a Scenario-based checklist, instead of a regular checklist for its examination technique, since the technique has been tested empirically <ref> [115] </ref>. 158 * It serves as requirements for implementing computer supported review systems. In other words, a review system must provide support for each component of the framework, such as support for multiple phases, multiple roles, different group synchronicity, and so forth.
Reference: [116] <author> Adam A. Porter, Lawrence G. Votta, and Victor R. Basili. </author> <title> Comparing detection methods for software requirements inspections: A replicated experiment. </title> <type> Technical report, </type> <institution> University of Maryland, Department of Computer Science, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Only a few studies have attempted to investigate the role of specific review factors. For example, Votta found that the inspection meeting is not necessarily cost-effective [142]. Porter found that Scenario based FTR method can have a higher defect detection rate than either Ad Hoc or Checklist-based FTR methods <ref> [116] </ref>. Many other studies, however, provide conflicting and/or anecdotal explanation of the causal factors underlying review effectiveness. For example, Basili attribute review effectiveness to a technique called stepwise abstraction [8, 7]. <p> Unfortunately, the studies did not make a distinction between the errors found for the first time during the meeting and the ones already found from the previous phase (i.e., preparation). As pointed out by Votta and Porter, the former errors are negligible compared to the latter errors <ref> [142, 116] </ref>. Understanding the impact of review factors on review outcomes is essential for the improvement of FTR methods. One example is the question of whether having a face-to-face group meeting is indeed effective. <p> Many FTR methods tend to include this meeting mode in their review process at any cost, because many people believe that this mode will increase group synergy in finding errors. Only few studies have attempted to address this issue as described earlier <ref> [142, 116] </ref>. The experiment described in this thesis also investigates this question, although from a different perspective. 1.3.3 Variations and inflexibility in review systems implementation Recently, many computer-supported review systems have emerged, such as, ICICLE [17], Scrutiny [61], CSI [98], InspeQ [92], etc. <p> Some FTR experiences have indicated that real group settings (i.e., face-to-face meeting) would outperform nominal group settings (no interaction among members) due to the presence of group synergy. However, recent empirical studies found that the meeting phase in Inspection method is not cost effective <ref> [142, 116] </ref>. The experimental study in this research is intended to follow up on this issue. 1.5 Contributions There are three major contributions of this research: 1. Theoretically, this research contributes a novel framework for FTR. This framework provides a new way of analyzing similarities and differences among FTR methods. <p> In Fagan's method [52, 53], the comprehension activity is known as preparation, whereas the examination activity is known as defect finding. Consolidation can be found, for example, in FTR methods suggested by Humphrey [74], Gilb [60], Votta and Porter <ref> [142, 116] </ref>. Gilb refers to consolidation as defects logging, and Votta and Porter refer to it as collection. Correlation can be found in Humphrey's method [74]. Discussion can be found in the third hour meeting phase of some inspection methods [21, 108]. <p> Although consolidation does not involve examining new materials, some studies show that new errors are often discovered during this activity as a result of group interaction [41]. However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors <ref> [142, 116, 115] </ref>. Finally, group discussion to resolve issues or to find solutions is often considered detrimental to the review process and thus, should be forbidden [52, 124, 60]. This happens, for example, during the inspection meeting phase. <p> Porter and Votta's study Porter and Votta's study investigates the effectiveness of three different detection techniques: Ad Hoc, Checklist and Scenario <ref> [115, 116] </ref>. The Ad Hoc technique is a nonsystematic procedure to find defects. The reviewers rely on intuition and experience in finding errors. However, they are given a generic list of defect taxonomy, such as missing functionality, missing performance, etc. <p> individual activities (i.e., during the individual preparation phase) [52]: Sometimes flagrant errors are found during this operation (i.e., preparation phase), but in general, the number of errors found is not nearly as high as in the inspection operation (i.e., meeting phase) In contrast, recent empirical studies by Votta and Porter <ref> [142, 116, 115] </ref> found that group meetings may not be cost effective compared to the preparation phase in which individual participants work alone. They found that meeting gains offset meeting losses; there was only 4% increase in faults found at the meeting [142]. <p> has the potential to improve overall review performance, since the individual 152 participants/groups tend to discover different errors (see also section 7.6.2 ). 7.6.3 Related studies As described in Chapter 6, one objective of this study was to follow up the empirical study by Votta, Porter and Eick et al. <ref> [142, 116, 47] </ref>. These studies found that group meetings (collection meeting) are not as beneficial as claimed by Fagan [52]. However, as described in Chapter 6, the review methods used in these studies are slightly different from Fagan's method.
Reference: [117] <author> Atul Prakash and Hyong Sop Shim. Distview: </author> <title> Support for building efficient collaborative applications using replicated objects. </title> <booktitle> Proceedings of the ACM Conference on ComputerSupported Cooperative Work, </booktitle> <pages> pages 153-164, </pages> <year> 1994. </year>
Reference-contexts: Groupware toolkits Groupware toolkits are generic systems to build groupware applications. In fact, the message systems described earlier may also be considered as groupware toolkits since they are used for building groupware applications (although mainly asynchronous applications). Some examples of these systems include GroupKit [122], Rendezvous [112], DistView <ref> [117] </ref>, Conversation Builder (CB) [89], and Suite [38]. Some systems provide mainly toolkits for building synchronous collaborative applications, such as those that require shared window facilities, concur- rency controls, and session management (e.g., Group Kit, Rendezvous, DistView). Others provide additional facilities for supporting group processes (e.g., Suite and Conversation Builder).
Reference: [118] <author> Roger S. Pressman. </author> <title> Software engineering : a practitioner's approach. </title> <publisher> McGraw-Hill, </publisher> <year> 1992. </year>
Reference-contexts: Furthermore, it is concerned primarily with formal and internal reviews as opposed to informal and external reviews. Some software engineering literature also uses this terminology <ref> [118] </ref>. There are also different review terminologies used during specific software development phases, such as requirement reviews, design reviews and code reviews instituted during the requirement specification, design and implementation phases respectively. <p> The review process itself may be based on inspections, walkthroughs or any other review methods. For example, Hollocker proposes mix review methods: walkthroughs, technical review and software inspection for SRR [73]. Pressman suggested a combination of informal and formal walkthroughs for design reviews <ref> [118] </ref>. Parnas suggested a new review method called Active Design Review for the design review (this method will be discussed in detail in the later chapter) [111]. With respect to review participants, Glass noted that the personnel makeup of the review changes dramatically as the life cycle proceeds. <p> Pressman uses a defect amplification model to characterize this situation: the number of defects is multiplied as the defects travel down the life cycle when no reviews are conducted <ref> [118] </ref>. Thus, one should avoid rework to improve productivity and focus on building the product right the first time. Generally, one should focus on improving the quality of the process for developing the products, in addition to improving the products themselves.
Reference: [119] <author> Stan Rifkin and Lionel E. Deimel. </author> <title> Applying program comprehension techniques to improve software inspections. </title> <booktitle> In Proceedings of the 19th Annual NASA Software Engineering Workshop, </booktitle> <address> Greenbelt, MD., </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Unfortunately, no empirical studies have ever looked into the issue of whether separating comprehension from examination activity affects review effectiveness. However, some researchers believe that comprehension does correlate with examination. For example, Rifkin and Diemel found that applying a program comprehension technique can improve inspection effectiveness (finding more errors) <ref> [119] </ref>. However, Dow and Murphy found comprehension (i.e., detailed product knowledge) is not a prerequisite for an effective inspection [42]. Similarly, some studies provide empirical results that relate preparation time to review outcomes [19, 2, 24, 74]. <p> By answering these questions, the reviewers learn about the materials [110]. Another comprehension technique that has been shown to improve FTR effectiveness is the one suggested by Rifkin and Diemel <ref> [119, 33] </ref>. This technique involves basic top-down and bottom-up code reading strategies. A common technique for consolidation is presentation, where individual issues are presented one by one by the meeting leader, and the group confirms or rejects the issues as software defects [74].
Reference: [120] <author> Tom Rodden and Ian Sommerville. </author> <title> Studies in Computer Supported Cooperative Work. Theory, Practice, and Design, chapter Building Conversation Using Mailtrays, </title> <address> pages 159-172. </address> <publisher> Elsevier science publishers B.V, </publisher> <year> 1991. </year>
Reference-contexts: The messages themselves are typically typed messages and include a set of fields or attributes. The system compares these values with the user's supplied criteria in order to generate appropriate actions. Examples of message systems include Information Lens [95], Lotus Notes [35], Coordinator [55], Strudel [128], MailTray <ref> [120] </ref>, Atomicmail [15], and Active Mail [65]. Message systems are typically generic systems for building group applications, such as meeting schedulers, information finders, and tools for project management, task tracking, software defect tracking, and collaborative writing.
Reference: [121] <author> M. Roscheisen, C. Mogensen, and T. Winograd. </author> <title> Beyond browsing: Shared com-ments, soaps, trails, and on-line communities. </title> <address> http://www.igd.fhg.de/www/www95/ proceed-ings/papers/88/TR/WWW95.html, </address> <year> 1995. </year>
Reference-contexts: Recently, many WWW clients have been augmented with separate programs or subsystems to support collaborative work. Typical features include support for group annotation and authoring. Examples of these systems include NCSA Mosaic 2.6 with group annotation, HyperNews (WWW + Usenet News) [93], BRIO (xMosaic + PRDM) <ref> [121] </ref>, CoReview (Mosaic + XTV)[96], and WWW + COMET [58]. The latter two systems provide teleconferencing and a synchronous whiteboard facility as well. CoReview specifically supports a collaborative review process by spatially separated reviewers. It also provides support for two roles, the chair and reviewer.
Reference: [122] <author> Mark Roseman and Saul Greenberg. Groupkit: </author> <title> A groupware toolkit for building real-time conferencing applications. </title> <booktitle> Proceedings of the ACM Conference on Computer-Supported Cooperative Work, </booktitle> <pages> pages 43-50, </pages> <year> 1992. </year>
Reference-contexts: Groupware toolkits Groupware toolkits are generic systems to build groupware applications. In fact, the message systems described earlier may also be considered as groupware toolkits since they are used for building groupware applications (although mainly asynchronous applications). Some examples of these systems include GroupKit <ref> [122] </ref>, Rendezvous [112], DistView [117], Conversation Builder (CB) [89], and Suite [38]. Some systems provide mainly toolkits for building synchronous collaborative applications, such as those that require shared window facilities, concur- rency controls, and session management (e.g., Group Kit, Rendezvous, DistView).
Reference: [123] <author> Jeffrey Rothfeder. </author> <title> It's late, costly, incompetent -but try firing a computer system. </title> <booktitle> Business Week, </booktitle> <month> November 7 </month> <year> 1993. </year> <month> 230 </month>
Reference-contexts: system was millions over budget and years behind schedule; the system was built in 1982 with a target completion date scheduled for 1987, and a target cost of $8 million; after spending $15 million, the completion date was pushed back to 1993 and the estimated cost grew to $100 million <ref> [123] </ref>. Research on tools and techniques to improve software quality has shown that Formal Technical Review (FTR) provides unique and important benefits. First, it has been shown to be an effective tool for defect removal.
Reference: [124] <author> Glen W. Russell. </author> <title> Experience with inspection in ultralarge-scale developments. </title> <journal> IEEE Software, </journal> <month> January </month> <year> 1991. </year>
Reference-contexts: In fact, the literature on FTR often describes conflicting review practices for the same method. For example, some descriptions of Fagan's method explicitly advocate the use of paraphrasing for the inspection technique <ref> [52, 124, 34] </ref>, while others disregard this technique [74]. Some advocate the use of common error checklists when examining source materials [74, 57], others also advocate the use of selective test cases [1, 43]. Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. <p> Myers and Parnas attribute the success of review to the synergy effect between the producer and the reviewers [106, 110], yet others discourage active participation of the producer during the meeting <ref> [1, 124] </ref>. Parnas and Weiss explicitly criticize the effectiveness of Fagan's inspection because it lacks specific roles/responsibilities and a systematic technique for individual reviewers to conduct the inspection. Thus, they consider these factors very important for a successful review [110]. <p> Thus, they consider these factors very important for a successful review [110]. Finally, some researchers attribute review effectiveness to paraphrasing, or the use of selective test cases, or checklists <ref> [124, 1, 43, 57] </ref>, or group interaction/composition [10, 97]. In addition to the lack of understanding of review factors, the relationships between various factors are also not completely understood. <p> Like walkthroughs, the term Inspections is also used in the literature in many different ways. For example, many researchers associate inspections with paraphrasing techniques, in which a designated reader paraphrases the materials statement by statement <ref> [106, 3, 124, 1] </ref>, others downplay the role of reader [41, 101, 74, 60]. Gilb specifically recommends not to use the role of reader, since it takes too much time for the benefits which it provides [60]. <p> Many studies have found a correlation between the effectiveness of inspections and certain inspection metrics. For example, the number of defects found decline with increasing inspection or preparation rates [53, 74, 24], with the optimum inspection rate is around 150 LOC/hour <ref> [124] </ref>. This data is then used as a guideline for controlling the inspection process. Thus for example, if the pace of an inspection is well above the standard guideline 150 LOC/hour, the materials need to be reinspected. <p> The results showed that each hour invested on inspection saved an average of 33 hours of subsequent maintenance effort. The study also showed that inspection was cheaper at finding defects than testing: 0.8-1 hour/defect compared to 2-4 hours/defect in testing <ref> [124] </ref>. * Weller reported a three-year study of inspections at Bull HN Information Systems, and some lessons learned from the study. The results showed a significant increase in the number of defects found, as well as the improvement in effectiveness and productivity of the inspection process itself. <p> Organization Cost Efficiency Method Inspect Test Inspect Test Fagan76 [52] Aetna 82% 18% Fagan Fagan86 [53] IBM UK 93% Fagan Standard Bank &gt;50% Fagan AMEX &gt;50% Fagan Ackerman89 [1] Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 <ref> [124] </ref> Bell-Northern 0.8-1 2-4 Fagan Weller93 [147] Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 [107] IBM 85% Fagan Doolan92 [41] Shell Research Doolan Kaplan95 [88] IBM 3.5 15-25 Kaplan Again some of the data in Table <p> In this case, the participants are expected to both fully understand the materials and find defects at the same time. Some researchers suggest that separating comprehension from examination activity makes a review process more effective and productive <ref> [52, 74, 1, 124] </ref>. For example, in the code inspection method, there is a phase called overview, where the producer educates participants about review materials, and preparation, where the reviewers learn the materials individually [52]. The stated objective of these phases is comprehension. <p> However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors [142, 116, 115]. Finally, group discussion to resolve issues or to find solutions is often considered detrimental to the review process and thus, should be forbidden <ref> [52, 124, 60] </ref>. This happens, for example, during the inspection meeting phase. <p> The producer participates as a silent observer <ref> [124] </ref>. * Buck and Dobbins' inspection uses statistical process control during the preparation and the meeting phases [20]. * Bush's inspection includes the third-hour meeting phase [21]. * Doolan's inspection uses the meeting phase for consolidation/ defects logging [41]. 3.2.16 Empirical studies of FTR FTR methods used for empirical studies also <p> In practice, detection rate is often used as a measure for the effectiveness of defect removal techniques, including testing. It refers to the number of defects found per man-hour invested. However, the term detection rate is not uniformly used. Russell referred to this term as detection efficiency <ref> [124] </ref>. Weller used detection rate to refer to the number of defects per KLOC [147]. Porter and Votta used detection rate to refer to the percentage of defects found per total defects in the source [115]. <p> In EIAM, paraphrasing was performed by individual participants silently. For the latter, however, I had no way of knowing whether the participants read every single statement as instructed in the review guideline. Several studies have found that paraphrasing rate is a good predictor of review effectiveness <ref> [124, 74, 24] </ref>. In fact, Russell suggested that the effective paraphrasing rate should be around 150 lines/hour. This section compares the paraphrasing rate of EGSM and EIAM. <p> The paraphrasing process became very routine and tedious, and the reviewers often lost their concentration as the result. * Presenter is too fast. This is a common problem in paraphrasing. Several studies also cited this problem <ref> [124, 34] </ref>. My observation indicated that in these circumstances the presenters were generally very knowledgeable about the code being paraphrased.
Reference: [125] <author> G. Michael Schneider, Johnny Martin, and Wei-Tek Tsai. </author> <title> An experimental study of fault detection in user requirements documents. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(2) </volume> <pages> 188-204, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The performance also appears to vary according to group size. For example, Eick's design review found the average individual performance was only 12.5% compared to 75% with a group of size 8 [47]; the same is true for N-fold inspection where group replications can increase performance significantly <ref> [125] </ref> (see also Table 2.4). This observation is also consistent with Weller's study that found four-person teams were twice as effective, and more than twice as efficient as three- person teams [147]. <p> the next chapter is an approach for conducting such an investigation. 24 Table 2.4: Empirical findings of FTR References Efficiency Group-size Method Hetzel76 [70] 37.3% 1 Code reading Myers78 [105] 38% 3 Code walkthrough Eick92 [47] 75% 8 Design review Martin90 [97] 27% 4 N-Fold inspection (N=1) (Req Doc) Schneider92 <ref> [125] </ref> 35.1% 4 N-Fold inspection (N=1) (Req Doc) 77.8% 4 N=9 (Req Doc) Knight93 [92] 50% 2 Phased Inspection (Phase 6) (Code) Porter94 [115] 25% 3 Checklist (Req doc) 32% 3 AdHoc (Req doc) 50% 3 Scenario (Req doc) Dow94 [42] 20%-46% 4,7 Fagan (Code) Chapter 7 42.8% 3 EGSM (Code) <p> Eick's study involved a two-phase review method to estimate the number of software faults before coding; Knight's study investigated the effectiveness of Phased Inspection method. Other FTR studies investigated the effect of generic review attributes on review performance, 153 such as group size and replication. Martin's and Schneider's studies <ref> [97, 125] </ref> found that replicating the review group N fold can significantly improve review performance. Their findings are also substantiated by this experiment (see section 7.6.2). The effect of group size observed in this study seems to contradict the Buck's study cited in [10].
Reference: [126] <author> G. Gordon Schulmeyer. </author> <title> Zero Defect Software. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1990. </year>
Reference-contexts: In 1986, a medical system for radiation therapy killed two patients due to improper design and implementation of its software. A report given to to the U.S. Food and Drug Administration (FDA) stated that the software failed to access the appropriate calibration data <ref> [79, 126] </ref>. In addition to quality problems in software products, software development is often plagued by cost and schedule overruns.
Reference: [127] <author> R. W. Selby. </author> <title> Evaluations of software technologies: Testing, CLEANROOM, and metrics. </title> <type> PhD thesis, </type> <institution> University of Maryland at College Park, Department of Computer Science, </institution> <year> 1985. </year>
Reference-contexts: This also translates to reduction in testing costs by 50% to 80% including review costs. Several studies also provide empirical evidence that FTR can be more effective in detecting errors than traditional testing <ref> [127, 59] </ref>. But more importantly, FTR can be applied to any intermediate software products that are untestable. In addition to reducing software defects, FTR can also improve other qualities of software products, such as portability, understandability, modifiability (maintainability), testability, etc. <p> Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection [74, 98, 142]. Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion <ref> [127, 110] </ref>. Also, many review methods demand a separate preparation phase where reviewers learn source materials before starting error hunting [1, 74, 52], while others consider it optional or unnecessary [92]. <p> Porter and Votta used detection rate to refer to the percentage of defects found per total defects in the source [115]. Only Selby used detection rate as number of faults detected per hour similar to the one used in this study <ref> [127] </ref>. Table 7.9 shows the resulting Wilcoxon analysis for ICS-313, ICS-411, and All groups.
Reference: [128] <author> Allan Shepherd, Niels Mayer, and Allan Kuchinsky. </author> <title> Strudel an extensible electronic con-versation toolkit. </title> <booktitle> Proceedings of the Conference on Computer-Supported Cooperative Work (CSCW'90), </booktitle> <pages> pages 93-104, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The messages themselves are typically typed messages and include a set of fields or attributes. The system compares these values with the user's supplied criteria in order to generate appropriate actions. Examples of message systems include Information Lens [95], Lotus Notes [35], Coordinator [55], Strudel <ref> [128] </ref>, MailTray [120], Atomicmail [15], and Active Mail [65]. Message systems are typically generic systems for building group applications, such as meeting schedulers, information finders, and tools for project management, task tracking, software defect tracking, and collaborative writing.
Reference: [129] <author> Ben Shneiderman. </author> <title> Software Psychology. </title> <publisher> Winthrop Publishers, Inc, </publisher> <year> 1980. </year>
Reference-contexts: Finally, the administrator collects these evaluation forms and produces summary reports. The field study conducted by Shneiderman suggests that peer ratings of programs are productive, enjoyable, and nonthreatening experiences <ref> [129] </ref>. Peer Ratings are often called Peer Reviews [129]. However, some authors use the term Peer Reviews as generic review methods involving peers [113, 74]. 2. Walkthroughs. <p> Finally, the administrator collects these evaluation forms and produces summary reports. The field study conducted by Shneiderman suggests that peer ratings of programs are productive, enjoyable, and nonthreatening experiences <ref> [129] </ref>. Peer Ratings are often called Peer Reviews [129]. However, some authors use the term Peer Reviews as generic review methods involving peers [113, 74]. 2. Walkthroughs. <p> A question mark (?) indicates that the corresponding review component is not described in the literature. 31 Table 3.2: Models of major FTR methods Method Phase 1 Phase 2 Phase 3 Phase 4 Peer Rating Review Summary <ref> [106, 129] </ref> O:examination O:consolidate C:individual C:individual S:ASYNC S:ASYNC R:reviewer R:administrator T:evaluation form T:? E:completed form E:? Structured Preparation: Walkthrough: Walkthrough O:comprehension O:examination [150] C:individual, C:group selective subgroup S:ASYNC,SYNC S:SYNC R:reviewer,producer R:producer T:reading,discussion T:paraphrasing E/X:? E/X:? Round-robin Phase i Phase i+1 [69] O:examination O:examination C:subgroup C:subgroup S:ASYNC S:ASYNC R:reviewer i R:reviewer i+1 <p> O:consol C:group C:indiv,group C:group C:individual C:group S:STSP S:ASYNC S:ASYNC S:ASYNC S:SYNC R:producer R:reviewer,producer R:moderator R:moderator R:moderator T:presentation T:question-answer, T:Delphi T:tools T:voting free exam E:unresolved E:source readiness E:source availability E:source nodes E:commentary issues remained reviewed reviewed X:no more issue 33 3.2.1 Peer Rating The Peer Rating method of Shneiderman and Myers <ref> [106, 129] </ref> (described in Chapter 2) defines two roles: Administrator and Reviewer, and two phases: Review and Summary. Prior to entering the review phase, the Administrator collects the programs to be reviewed from the programmers and distributes them to the reviewers (i.e., other programmers).
Reference: [130] <author> John B. Smith and Stephen F. Weiss. </author> <title> Hypertext. </title> <journal> Communications of the ACM, </journal> <volume> 31(7) </volume> <pages> 816-819, </pages> <year> 1988. </year>
Reference-contexts: Relationships among nodes are represented by links. Links as well as nodes can have types, so that different artifacts and their relationships can be represented by different types of nodes and links. These typed nodes and links implement a hypertext network <ref> [130] </ref>. In addition to providing fields, a node may also have attributes, such as node name, creation-date, creator, etc. Unlike fields, attributes may be shared by all nodes regardless of their type. Finally, a node may also have state/status. <p> Hypertext/hypermedia systems Hypertext is an approach to information management in which data is stored in a network of nodes connected by links. Nodes can contain text, graphics, audio, video, as well as source code or other forms of data <ref> [130] </ref>. Some people prefer to use the term hypermedia when the nodes may include 77 multimedia data, such as audio, video, animation, film clips, etc. Hypertext systems provide facilities for displaying, creating, modifying, processing and navigating of such nodes and links.
Reference: [131] <institution> IEEE Standard. IEEE Standard Classification for Software Anomalies (IEEE Std 1044-1993). The Institute of Electrical and Electroinc Engineers,Inc., </institution> <year> 1993. </year>
Reference-contexts: Initially, there were 20 errors seeded in each set of the programs, but at the end of the review, there were 23 errors in program 1 and 25 errors in program 2 (i.e., new errors were discovered during the review). The errors were mostly logic and data handling problems <ref> [131] </ref>, such as missing or incorrect condition tests, forgotten cases or steps, incorrect access of data, etc. Some of these errors were specific to C++ programming constructs, such as memory leaks. None of the errors involved an incorrect specification.
Reference: [132] <author> M. Stefik, G. Foster, D. Bobrow, K. Kahn, S. Lanning, and L. Suchman. </author> <title> Beyond the chalkboard: Computer support for collaboration and problem solving in meetings. </title> <journal> Communications of the ACM, </journal> <volume> 30(1) </volume> <pages> 32-47, </pages> <year> 1987. </year>
Reference-contexts: The goal of these systems is to facilitate meeting activities with proper computational support so as to make the meeting more productive. 75 Some examples of these systems include Electronic Meeting System or GroupSystem [109], Colab <ref> [132] </ref>, and Capture Lab [49]. <p> The major facilities provided by these systems include a shared electronic whiteboard (e.g., Colab), or a large screen with video projector (e.g., Capture Lab and EMS), and a set of software tools for supporting meeting activities, such as the brainstorming, organizing, evaluating, proposing, and arguing activities <ref> [132, 109] </ref>. The basic design of such meeting support systems is the provision of public workspace that implements WYSIWIS (What You See Is What I See), or all meeting participants see exactly the same thing [132]. <p> The basic design of such meeting support systems is the provision of public workspace that implements WYSIWIS (What You See Is What I See), or all meeting participants see exactly the same thing <ref> [132] </ref>. However, most systems also provide a private workspace, where the meeting participants may work individually and privately.
Reference: [133] <author> Susan Strauss and Robert G. Ebenau. </author> <title> Software Inspection Process. </title> <publisher> McGraw Hill, </publisher> <year> 1993. </year>
Reference-contexts: Prior to using inspections, AT&T had very large maintenance costs for one of its products. By applying inspections, maintenance of the subsequent releases of the product was reduced by an order of magnitude <ref> [133] </ref>. 2 When carefully instituted in the development process, FTR can provide a significant increase in the development productivity, and/or a significant reduction in the development cost. Basically, it serves as an early detection mechanism for potential errors before reaching the later stages. <p> Thus, one should no longer depend on downstream testing to achieve quality. Software inspection is an early, pro-active defect detection process; it is held very soon after interim work products (e.g., requirements, design, code) are created. Some people use the term in-process software inspection to emphasize this concept <ref> [133] </ref>. Kenett described that the key for ceasing dependence on inspection is to break down the process into subprocesses, identify their inputs and outputs, their internal suppliers and internal customers, and then to construct feedback loops to improve the subprocesses [90]. <p> Thus for example, if the pace of an inspection is well above the standard guideline 150 LOC/hour, the materials need to be reinspected. Control charts are also commonly used to evaluate the performance of an inspection process statistically <ref> [22, 74, 133] </ref>. Others use mathematical models to predict remaining errors after inspections [23, 47]. The principle of continuous process improvement also applies to the inspection process itself. This improvement can be achieved in the same way using statistical process control as discussed above. <p> For example, Inspections focus on the verification objective, while Overviews (another type of review method) focus on the education objective <ref> [133] </ref>. These quality objectives are somewhat similar to the objective component of the FTR framework. For example, Education is similar to comprehension/education in the FTR framework, Evaluation is similar to discussion of alternatives, 43 and Verification, Validation and Assurance are similar to examination (finding all types of defects).
Reference: [134] <author> Norbert A. Streitz, Jorg Geibler, Jorg M. Haake, </author> <title> and Jeroen Hol. DOLPHIN: Integrated meeting support across local and remote desktop environments and liveboards. </title> <booktitle> Proceedings of the 1994 ACM Conference on Computer Supported Cooperative Work, </booktitle> <pages> pages 345-358, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Conceptually, they provide virtual meeting rooms [51], where participants from dispersed locations can discuss and manipulate the materials/applications displayed on shared screens. Examples of such systems include Rapport [51], Mermaid [145], TeamWorkstation [78], and Dolphin <ref> [134] </ref>. Some systems, for example, Dolphin, also include same-time same-place meetings. The basic facilities of conferencing systems usually include real-time voice and video channels, which are implemented through separate communication networks (e.g., a wide area network). <p> These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor [67], Virtual Notebook [77], SEPIA [66], and Clare [144]. Still another systems provide unique support for real-time conferencing and/or face-to-face meetings, for example, Dolphin <ref> [134] </ref>, as well as, CoAuthor and SEPIA. As explained in Chapter 3, the CSRS data modeling language is built around the concept of hypertext. One may design one's own network of typed nodes and links using the language definitions.
Reference: [135] <author> W. Stroebe, M. Diehl, and Abakoumkin. </author> <title> The illusion of group effectivity. </title> <journal> Personality & Social Psychology Bulletin, </journal> <volume> 18, </volume> <month> October </month> <year> 1992. </year>
Reference-contexts: The choice of (2) is used as a metric to indicate the presence of group synergy. This synergy metric is adopted from Stroebe's study <ref> [135] </ref>. However, as discussed later in Chapter 7, the non-existence of (1) and (2) can also be used as a synergy metric. In general, the quantitative data (1) and (2) were used to analyze the main hypothesis, and the research questions concerning the detection rates.
Reference: [136] <author> Ilkka Tervonen. </author> <title> Support for quality-based design and inspection. </title> <journal> IEEE Software, </journal> <pages> pages 44-54, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Another difference between Management and Technical reviews is that in the former the attendance typically includes management, whereas in the latter it consists primarily of technical personnel/peers. 4. Manual versus Computer-supported reviews In recent years, several computer supported review systems have been developed <ref> [17, 84, 61, 98, 99, 92, 136] </ref>. The type of support varies from simple augmentation of the manual practices [17, 61] to totally new review methods [84, 92]. Chapter 4 will discuss these review systems in detail. <p> First, it discusses existing computer supported review systems that implement specific FTR methods. These include ICICLE [17], CIA/Scrutiny [61], CSI [98], CAIS [99], InspeQ [92], and QDA/Tammi <ref> [136] </ref>. The discussion highlights those facilities that are either similar to or different from CSRS. The following subsection compares CSRS with existing computer-supported cooperative work systems. Although, these systems do not support the FTR process directly, they are included here because their unique features may exhibit some similarities with CSRS. <p> However, FTArm provides more extensive asynchronous meeting support, such as automated issue consolidations and explicit supports for roles. CAIS provides no support for roles at all. The FTArm system will be discussed in the next chapter. 4.4.6 QDA/Tammi QDA is a quality-driven assessment tool <ref> [136] </ref>. It is used during the design and inspection phase to assess the quality of source artifacts.
Reference: [137] <author> Danu Tjahjono. </author> <title> Comparing the cost effectiveness of group synchronous review method and individual asynchronous review method using CSRS: Results of pilot study. </title> <type> Technical Report ICS-TR-95-07, </type> <institution> Department of Information and Computer Sciences, 2565 The Mall, University of Hawaii, </institution> <address> Honolulu, Hawaii 96822, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Several bugs related to system operation and system features were encountered during the pilot study. These problems were then fixed in the subsequent release of CSRS for the main study. * Questionnaires. The questionnaires were further revised for the main study. The pilot study is described in detail in <ref> [137] </ref>. 119 Chapter 7 Experimental Results This chapter presents the results of the experiment comparing the review performance of real group reviews using EGSM and nominal group reviews using EIAM. Section 7.1 describes the summary of the experimental procedures.
Reference: [138] <author> Danu Tjahjono. </author> <title> Results of CSRS experiments. </title> <type> Technical Report ICS-TR-95-13, </type> <institution> Department of Information and Computer Sciences, 2565 The Mall, University of Hawaii, </institution> <address> Honolulu, Hawaii 96822, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: This includes preferences in the review process, the review system, and the perceived quality of the review outcomes. Subsequent sections will look at the detail of this data analysis. The raw data of the experiment can be found in <ref> [138] </ref>. Table 7.3: Summary of the experimental results (EGSM vs.
Reference: [139] <author> Danu Tjahjono. </author> <title> CSRS design reference 3.5.0. </title> <type> Technical Report ICS-TR-96-01, </type> <institution> University of Hawaii, Department of Information and Computer Sciences, </institution> <month> February </month> <year> 1996. </year> <month> 231 </month>
Reference-contexts: The actual code at this level is quite large and complex. The reader interested in its detailed implementation should consult the CSRS design manual <ref> [139] </ref>. This section only provides an overview of the major subsystems implemented by the internal level. In general, the internal level comprises the following major subsystems: * Entity . This subsystem provides the operations to manipulate node, field and link entities. * Interface.
Reference: [140] <author> Danu Tjahjono and Philip M. Johnson. </author> <title> FTArm demonstration guide (version 1.2.0). </title> <type> Technical Report ICS-TR-95-19, </type> <institution> Department of Information and Computer Sciences, 2565 The Mall, University of Hawaii, </institution> <address> Honolulu, Hawaii 96822, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: However, it does not describe the detailed implementation of the system, nor the actual commands or keystrokes to run the system. The former has been discussed in detail in the previous chapter, while the latter can be found in the FTArm User's Guide [141] and FTArm/CSRS Demonstration Guide <ref> [140] </ref>. Some FTArm commands, however, will be described here for clarity. Section 5.1 first describes the motivation for FTArm. Section 5.2 describes the review method of FTArm. Section 5.3 details the actual review procedures using FTArm. Section 5.4 discusses some issues and limitations of FTArm.
Reference: [141] <author> Danu Tjahjono and Philip M. Johnson. </author> <title> FTArm user's guide (version 1.2.0). </title> <type> Technical Report ICS-TR-95-18, </type> <institution> Department of Information and Computer Sciences, 2565 The Mall, University of Hawaii, </institution> <address> Honolulu, Hawaii 96822, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: This led to the development of Version 3 with support for the generic modeling of review methods described in previous sections. This version was completed in the Fall 1994, and was released for public use in the Fall 1995 <ref> [141] </ref>. 4.3.2 Experiences with CSRS process modeling Since its completion in 1994, the system has been used to model several different review methods/review systems. The first review method implemented was called Hello-World (i.e., a well-known term for describing simplicity). <p> However, it does not describe the detailed implementation of the system, nor the actual commands or keystrokes to run the system. The former has been discussed in detail in the previous chapter, while the latter can be found in the FTArm User's Guide <ref> [141] </ref> and FTArm/CSRS Demonstration Guide [140]. Some FTArm commands, however, will be described here for clarity. Section 5.1 first describes the motivation for FTArm. Section 5.2 describes the review method of FTArm. Section 5.3 details the actual review procedures using FTArm. Section 5.4 discusses some issues and limitations of FTArm.
Reference: [142] <author> Lawrence G. Votta. </author> <booktitle> Does every inspection need a meeting? In Proceedings of the ACM SIGSOFT 1993 Symposium on Foundations of Software Engineering, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Still others advocate free review (i.e., no specific guideline, based on reviewers' intuition and experiences) [98]. Fagan's method also focuses on error detection during the inspection meeting, while other methods use the meeting for error collection <ref> [74, 98, 142] </ref>. Regarding the participants' interaction, some state the importance of having group meetings [52], while others intentionally eliminate the need for group discussion [127, 110]. <p> Myers once stated that software review is an effective means of finding software faults, however, no one knows why the technique works [106]. Only a few studies have attempted to investigate the role of specific review factors. For example, Votta found that the inspection meeting is not necessarily cost-effective <ref> [142] </ref>. Porter found that Scenario based FTR method can have a higher defect detection rate than either Ad Hoc or Checklist-based FTR methods [116]. Many other studies, however, provide conflicting and/or anecdotal explanation of the causal factors underlying review effectiveness. <p> Unfortunately, the studies did not make a distinction between the errors found for the first time during the meeting and the ones already found from the previous phase (i.e., preparation). As pointed out by Votta and Porter, the former errors are negligible compared to the latter errors <ref> [142, 116] </ref>. Understanding the impact of review factors on review outcomes is essential for the improvement of FTR methods. One example is the question of whether having a face-to-face group meeting is indeed effective. <p> Many FTR methods tend to include this meeting mode in their review process at any cost, because many people believe that this mode will increase group synergy in finding errors. Only few studies have attempted to address this issue as described earlier <ref> [142, 116] </ref>. The experiment described in this thesis also investigates this question, although from a different perspective. 1.3.3 Variations and inflexibility in review systems implementation Recently, many computer-supported review systems have emerged, such as, ICICLE [17], Scrutiny [61], CSI [98], InspeQ [92], etc. <p> Some FTR experiences have indicated that real group settings (i.e., face-to-face meeting) would outperform nominal group settings (no interaction among members) due to the presence of group synergy. However, recent empirical studies found that the meeting phase in Inspection method is not cost effective <ref> [142, 116] </ref>. The experimental study in this research is intended to follow up on this issue. 1.5 Contributions There are three major contributions of this research: 1. Theoretically, this research contributes a novel framework for FTR. This framework provides a new way of analyzing similarities and differences among FTR methods. <p> In Fagan's method [52, 53], the comprehension activity is known as preparation, whereas the examination activity is known as defect finding. Consolidation can be found, for example, in FTR methods suggested by Humphrey [74], Gilb [60], Votta and Porter <ref> [142, 116] </ref>. Gilb refers to consolidation as defects logging, and Votta and Porter refer to it as collection. Correlation can be found in Humphrey's method [74]. Discussion can be found in the third hour meeting phase of some inspection methods [21, 108]. <p> Although consolidation does not involve examining new materials, some studies show that new errors are often discovered during this activity as a result of group interaction [41]. However, other studies find that consolidation (i.e., collection meeting phase) are not effective in discovering new errors <ref> [142, 116, 115] </ref>. Finally, group discussion to resolve issues or to find solutions is often considered detrimental to the review process and thus, should be forbidden [52, 124, 60]. This happens, for example, during the inspection meeting phase. <p> Many researchers believe that maximum collaboration (group process) is more effective than no collaboration (individual process) because of the presence of group synergy [43, 114, 2]. However, some review experiences/studies found that more errors are caught in individual process than group process <ref> [74, 142, 115, 47] </ref>. Many other review methods (namely, Fagan's code inspection and its variations) usually include some combination of individual and group processes. The Selective subgroup process has also been shown to provide unique benefits, such as a significant increase in review performance in N-Fold inspection [97]. <p> Another common technique is to let the meeting leader talk through review materials, while participants interrupt with the issues found during the private phase (sometimes called the document pacing technique) <ref> [60, 142, 115] </ref>. Still another technique is simply asking each participant in turn to report his/her issues [101], or to report and revise the issues as necessary following several rounds (i.e., Delphi technique [71]). 3.1.4 Entry/exit criteria The entry/exit criteria defines the starting or completion condition of a review phase. <p> This study found that code walkthrough is equally effective to testing. However, it is not clear whether this can be attributed to the technique used or the group interaction involved. Votta's study Votta's study investigates the effectiveness of the meeting phase in a two-phase inspection method <ref> [142] </ref>. The first phase involves preparation by individual reviewers, and the second phase involves a group meeting to collect faults identified in the first phase. The objective of the first phase is to find faults, and the objective of the second phase is to consolidate faults. <p> individual activities (i.e., during the individual preparation phase) [52]: Sometimes flagrant errors are found during this operation (i.e., preparation phase), but in general, the number of errors found is not nearly as high as in the inspection operation (i.e., meeting phase) In contrast, recent empirical studies by Votta and Porter <ref> [142, 116, 115] </ref> found that group meetings may not be cost effective compared to the preparation phase in which individual participants work alone. They found that meeting gains offset meeting losses; there was only 4% increase in faults found at the meeting [142]. <p> They found that meeting gains offset meeting losses; there was only 4% increase in faults found at the meeting <ref> [142] </ref>. They recommend that group meetings should be down-sized to include only the moderator and the producer in order to increase the cost effectiveness of FTR. In other words, the meeting should not involve all members of the group. Similar findings were also obtained by Eick et al. [47]. <p> This second phase can also increase participants' confidence and facilitate learning. Actually, this two-phase review method is commonly practiced <ref> [74, 142, 115] </ref> (see also Chapter 3). However, as pointed out by Votta [142], this second phase could be inefficient if not carefully administered. For example, instead of having the entire group members participate in the second phase, one might select only a subset of the participants. <p> This second phase can also increase participants' confidence and facilitate learning. Actually, this two-phase review method is commonly practiced [74, 142, 115] (see also Chapter 3). However, as pointed out by Votta <ref> [142] </ref>, this second phase could be inefficient if not carefully administered. For example, instead of having the entire group members participate in the second phase, one might select only a subset of the participants. <p> has the potential to improve overall review performance, since the individual 152 participants/groups tend to discover different errors (see also section 7.6.2 ). 7.6.3 Related studies As described in Chapter 6, one objective of this study was to follow up the empirical study by Votta, Porter and Eick et al. <ref> [142, 116, 47] </ref>. These studies found that group meetings (collection meeting) are not as beneficial as claimed by Fagan [52]. However, as described in Chapter 6, the review methods used in these studies are slightly different from Fagan's method. <p> This study also found that the performance would start to level off after N=6, as seen in Figure 7.12. Finally, Votta's study also found that group meetings (which are similar to EGSM in this research) minimize false positives <ref> [142] </ref>. 7.6.4 Lessons learned about review experiments The following presents the lessons learned from administering the experiment, observing the review process and implementing the review system used in the experiment. 1. Stick with the experimental schedules. <p> Experimental study The third goal of this research was to study the differences in review performance between real groups and nominal groups using CSRS. This study was intended to follow up on Votta's study <ref> [142] </ref>, which found that the meeting phase of Inspection method was not cost-effective; there were only 4% of the errors discovered during the meeting phase compared to 96% discovered during the individual phase. <p> Both groups in EGSM and EIAM used a one-phase review method with the objective of finding errors, and the technique of paraphrasing (the paraphrasing in EGSM was performed by a reader). In general, this finding adds new knowledge to Votta's study <ref> [142] </ref> by discovering that when the group meeting phase and the preparation phase were tested separately, no significant differences in performance were observed. 159 * With respect to error type C4 (mistyped statement), nominal groups were significantly more effective in catching it than EGSM. <p> Unfortunately, most of these arguments have not been tested empirically. Only a few researchers have conducted an experimental study to investigate these factors. One such a study is Votta's study, which found that the collection meeting phase is not cost effective <ref> [142] </ref>. In view of the FTR framework, Votta's study basically questioned whether the group meeting phase is cost-effective in finding defects compared to the individual preparation phase preceding the meeting phase. The study found it is not cost-effective.
Reference: [143] <author> Dolores R. Wallace and Roger U. Fujii. </author> <title> Software verification and validation: An overview. </title> <journal> IEEE Software, </journal> <pages> pages 10-17, </pages> <year> 1989. </year>
Reference-contexts: Some people generally associate V&V activities with defect detection and removal. There are many techniques and tools used to perform V&V activities. For example, Wallace and Fujii identified 41 V&V techniques and their respective use to address specific quality concerns <ref> [143] </ref>. Four of the techniques are related to FTR/software reviews, which include Formal review, Fagan's inspections, Peer review and Walkthrough. However, their detailed review processes are not described in the literature. Boehm identified a set of V&V techniques that are effective in performing software requirements and design V&V [12].
Reference: [144] <author> Dadong Wan. CLARE: </author> <title> A Computer-Supported Collaborative Learning Environment Based on the Thematic Structure of Scientific Text. </title> <type> PhD thesis, </type> <institution> University of Hawaii, Department of Information and Computer Sciences, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Some examples of hypertext systems include NLS/Augment [50] NoteCards [68], gIBIS [30], and Neptune [36]. These hypertext systems provide mostly generic support for information structuring and browsing. Other systems provide specific support for collaborative writing tasks, for example, CoAuthor [67], Virtual Notebook [77], SEPIA [66], and Clare <ref> [144] </ref>. Still another systems provide unique support for real-time conferencing and/or face-to-face meetings, for example, Dolphin [134], as well as, CoAuthor and SEPIA. As explained in Chapter 3, the CSRS data modeling language is built around the concept of hypertext.
Reference: [145] <author> K. Watabe, S. Sakata, K. Maeno, H. Fukuoka, and T. Ohmori. </author> <title> Distributed multiparty desktop conferencing system: </title> <booktitle> MERMAID. Proceedings of the Conference on Computer-Supported Cooperative Work (CSCW'90), </booktitle> <pages> pages 27-38, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Conferencing systems Conferencing systems provide support for interactive, real-time, and distributed meeting activities, or same-time different-place meetings. Conceptually, they provide virtual meeting rooms [51], where participants from dispersed locations can discuss and manipulate the materials/applications displayed on shared screens. Examples of such systems include Rapport [51], Mermaid <ref> [145] </ref>, TeamWorkstation [78], and Dolphin [134]. Some systems, for example, Dolphin, also include same-time same-place meetings. The basic facilities of conferencing systems usually include real-time voice and video channels, which are implemented through separate communication networks (e.g., a wide area network).
Reference: [146] <author> Gerald M. Weinberg. </author> <title> The Psychology of Computer Programming. </title> <publisher> Van Nostrand Reinhold Company, </publisher> <year> 1971. </year>
Reference-contexts: For this reason, this dissertation presents a FTR framework that enables one to distinguish different types of reviews. This framework will be described in the next chapter. 12 2.2 Psychological perspectives on FTR Psychological issues surrounding review were first elucidated in Weinberg's concept of egoless programming <ref> [146] </ref>. According to Weinberg, programmers are often reluctant to allow their programs to be read by other programmers. The programs are often considered as an extension of one's self. Errors discovered in the programs are considered to be a challenge to one's self-image.
Reference: [147] <author> Edward F. Weller. </author> <title> Lessons learned from three years of inspection data. </title> <journal> IEEE Software, </journal> <pages> pages 38-45, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The study also found that inspections can improve the quality of maintenance fixes (i.e., repair of bad fixes). Finally, this study also reported one case of project failure even though it was under inspection control; however, further analysis indicated that the team had not inspected the design documents <ref> [147] </ref>. 20 * Buck and Dobbins found their code inspection data at IBM facility showed a remarkably consistency over different types of software: 8-12 defects per KLOC for new code and 3-5 staff hours per major defects detected. With such consistency, the data was also used for statistical quality control. <p> Inspect Test Fagan76 [52] Aetna 82% 18% Fagan Fagan86 [53] IBM UK 93% Fagan Standard Bank &gt;50% Fagan AMEX &gt;50% Fagan Ackerman89 [1] Soft developer 2.2 4.5 Fagan OS developer 1.4 8.5 Fagan Hart82 Sperry Univac Walkthrough & Round-robin McKissick84 [101] GE McKissick Russell91 [124] Bell-Northern 0.8-1 2-4 Fagan Weller93 <ref> [147] </ref> Bull HN 80% Fagan Buck84 [20] IBM 3-5 Fagan Bush90 [21] JPL $100 $10K 75% Bush Freedman82 [56] Client Code review Myers88 [107] IBM 85% Fagan Doolan92 [41] Shell Research Doolan Kaplan95 [88] IBM 3.5 15-25 Kaplan Again some of the data in Table 2.3 may be misleading, since the <p> This observation is also consistent with Weller's study that found four-person teams were twice as effective, and more than twice as efficient as three- person teams <ref> [147] </ref>. It is also consistent with the result of the empirical study to be discussed in Chapter 7, although its performance does not double as in Weller's study (the net increase is around 10% from 3 to 5 persons, but it is still well below 75%). <p> It refers to the number of defects found per man-hour invested. However, the term detection rate is not uniformly used. Russell referred to this term as detection efficiency [124]. Weller used detection rate to refer to the number of defects per KLOC <ref> [147] </ref>. Porter and Votta used detection rate to refer to the percentage of defects found per total defects in the source [115]. Only Selby used detection rate as number of faults detected per hour similar to the one used in this study [127]. <p> This study also agrees with Weller's study that found four-person teams are twice as effective, and more than twice as efficient as three-person teams <ref> [147] </ref>. However, unlike Weller, who attributed the increase in performance to increase in team's level of expertise, this study found that it was caused by the increase in the number of unique errors caught.
Reference: [148] <author> Terry Winograd and Fernando Flores. </author> <title> Understanding Computers and Cognition. </title> <address> AddisonWesley, </address> <year> 1987. </year>
Reference-contexts: Conversation Builder (CB) provides facilities to implement a data model using hypertext typed nodes and links, and a process model using the conversation theory of language and action of Winograd and Flores <ref> [148] </ref>. The process model appears to allow the implementation of roles and phases, although support for roles was only partially implemented in Scrutiny. In general, Suite and CB differ from CSRS in several ways.
Reference: [149] <author> Edward Yourdon. </author> <title> Structured Walkthroughs. </title> <publisher> Prentice-Hall, </publisher> <year> 1979. </year>
Reference-contexts: Over the past several years, many different forms of inspections or formal reviews have emerged. Some of these include: Structured Walkthrough <ref> [149] </ref>, Active Design Reviews [110], Humphrey's inspection [74], Verification-Based Inspection [45], and so forth. Throughout this dissertation, the term formal technical review (FTR) is used to refer to a general class of reviews, which includes the specific review techniques described above. <p> Most researchers also agree that reviews must supplement testing; reviews tend to discover different types of errors than the those discovered by testing [105, 63]. In some cases, reviews can find errors that cannot be possibly found in testing <ref> [149] </ref>. Table 2.1: Defect detection efficiency for code [from Jones86] Removal step Lowest Med Highest eff. eff. eff.
Reference: [150] <author> Edward Yourdon. </author> <title> Structured Walkthrough. </title> <publisher> Prentice-Hall, </publisher> <address> 4 edition, </address> <year> 1989. </year>
Reference-contexts: The term walkthrough itself has been used in the literature in a variety of ways. Some refer it as structured walkthroughs <ref> [150] </ref>. Generally, the literature describes walkthrough as an undisciplined process without advanced preparation on the part of reviewers, and the meeting focuses on education purposes [27, 52]. However, many authors consider it a disciplined and formal review process [106, 150, 3]. 3. Round-robin reviews. <p> Some refer it as structured walkthroughs [150]. Generally, the literature describes walkthrough as an undisciplined process without advanced preparation on the part of reviewers, and the meeting focuses on education purposes [27, 52]. However, many authors consider it a disciplined and formal review process <ref> [106, 150, 3] </ref>. 3. Round-robin reviews. <p> some FTR practices advocate the active participation of the producer during group meetings (i.e., have the producer play the role of presenter or reader) for self-debugging mechanism, others discourage the producer's participation to prevent them from brainwashing the reviewers into making the same erroneous assumptions about the product being reviewed <ref> [106, 1, 150] </ref>. Active Design Review [110] and Scenario based review method [115] advocate the use of specialists for an effective review [110] Synchronicity Synchronicity describes the physical interaction mode among participants. <p> component is not described in the literature. 31 Table 3.2: Models of major FTR methods Method Phase 1 Phase 2 Phase 3 Phase 4 Peer Rating Review Summary [106, 129] O:examination O:consolidate C:individual C:individual S:ASYNC S:ASYNC R:reviewer R:administrator T:evaluation form T:? E:completed form E:? Structured Preparation: Walkthrough: Walkthrough O:comprehension O:examination <ref> [150] </ref> C:individual, C:group selective subgroup S:ASYNC,SYNC S:SYNC R:reviewer,producer R:producer T:reading,discussion T:paraphrasing E/X:? E/X:? Round-robin Phase i Phase i+1 [69] O:examination O:examination C:subgroup C:subgroup S:ASYNC S:ASYNC R:reviewer i R:reviewer i+1 T:? T:? Fagan's Overview: Preparation: Inspection: Inspection O:comprehension O:comp (edu) O:examination [52] C:group C:individual C:group S:STSP S:ASYNC S:STSP R:producer R:reviewer R:producer/reader T:presentation <p> The objective is to consolidate the findings from all participants. Thus, the objective of this phase is consolidation performed by the Administrator. The technique is not specified in the literature, as well as the exit criteria. 3.2.2 Yourdon's Structured Walkthrough Yourdon's Structured Walkthrough <ref> [150] </ref> begins with a Preparation or Pre-walkthrough phase where the reviewers learn the review materials individually or talk to the producer directly. Thus, the objective is comprehension with individual or selective-subgroup interaction. No particular technique is used for individual comprehension, other than reading the work product.
Reference: [151] <author> M. V. Zelkowitz, R. Yeh, R. G. Hamlet, J. D. Gannon, and V. R. Basili. </author> <title> Software engineering practices in the U.S. </title> <journal> and Japan. Computer, </journal> <volume> 17(6) </volume> <pages> 57-66, </pages> <year> 1984. </year> <month> 232 </month>
Reference-contexts: In fact, an earlier study by Zelkowitz found that 84% of the companies they surveyed agreed that reviews worked, but the ways reviews were conducted differed greatly <ref> [151, 10] </ref>. 4 1.3.2 Attribution of review outcomes to different review factors One reason why review methods are defined in so many ways is because the contribution of various review factors on review outcomes is not currently understood. <p> As described in the earlier chapters, the current state of FTR is characterized by ambiguities in its practice. Many practitioners believe that all FTR methods are basically the same <ref> [10, 151] </ref>. However, others do not agree. They usually argue that non-Fagan methods are ineffective, because they do 160 not focus on defect finding, do not use paraphrasing, do not have a trained moderator, do not have checklists, and so forth.
References-found: 151

