URL: http://www.mli.gmu.edu/~iimam/papers/ISMIS94.ps
Refering-URL: http://www.mli.gmu.edu/~iimam/pap_slct.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: michalski iimam- @aic.gmu.edu  
Title: Learning Problem-Oriented Decision Structures from Decision Rules: The AQDT-2 System  
Author: Ryszard S. Michalski and Ibrahim F. Imam 
Date: October, 16 19, 1994)  
Note: Published in Lecture Notes in Artificial Intelligence: Methodology for Intelligent Systems, No. 869, pp. 416-426, Spring Verlager, (The Proceeding of the 8 th International Symposium on Methodologies for Intelligent Systems, ISMIS, Charlotte, North Carolina,  11 pages, almost 5x6 inches. six figures and one table.  
Address: Fairfax, VA. 22030  
Affiliation: Center for Artificial Intelligence George Mason University  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Arciszewski, T, Bloedorn, E., Michalski, R., Mustafa, M., and Wnek, J., </author> <title> "Constructive Induction in Structural Design", Report of Machine Learning and Inference Laboratory, </title> <institution> MLI-92-7, Center for AI, George Mason University., </institution> <year> 1992. </year>
Reference-contexts: New features of AQDT-2 are demonstrated in an experiment on determining a decision structure for choosing wind bracings for tall buildings <ref> [1] </ref>. The results briefly illustrate how the system tailors decision structures to different decision making situations. 2 The AQDT-2 Method This section describes the AQDT-2 method for building a decision structure from decision rules.
Reference: 2. <author> Bergadano, F., Matwin, S., Michalski R. S. and Zhang, J., </author> <title> "Learning Two-tiered Descriptions of Flexible Concepts: The POSEIDON System," </title> <journal> Machine Learning , Vol. </journal> <volume> 8, No. 1, </volume> <pages> pp. 5-43, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: This paper presents a new version of the system, called AQDT-2. The new system generates a goal-oriented decision structure from decision rules learned by either rule learning system AQ15 [11] or system AQ17, which has extensive constructive induction capabilities <ref> [2] </ref>. <p> This is done based on ideas of rule truncation described in <ref> [2, 7] </ref>. When noise is expected in the training data, the decision rules with t-weight below a certain threshold (reflecting the expected noise level) are removed.
Reference: 3. <author> Bloedorn, E., Wnek, J., Michalski, R.S., and Kaufman, K. , "AQ17: </author> <title> A Multistrategy Learning System: The Method and Users Guide, Report of Machine Learning and Inference Laboratory, </title> <institution> MLI-93-12, Center for AI, George Mason University. </institution> <year> 1993. </year>
Reference: 4. <author> Bohanec, M. and Bratko, I., </author> <title> Trading Accuracy for Simplicity in Decision Trees, </title> <journal> Machine Learning Journal, </journal> <volume> Vol. 15, No. 3, </volume> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference: 5. <author> Breiman, L., Friedman, J.H., Olshen, R.A. & Stone, C.J. </author> , <title> Classification and Regression Structures, </title> <type> Belmont, </type> <institution> California: Wadsworth Int. Group, </institution> <year> 1984. </year>
Reference-contexts: The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction [13, 14], the gini index of diversity <ref> [5] </ref>, and others [6, 7, 12]. A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making situations it was designed for remain constant. Problems arise when these assumptions do not hold.
Reference: 6. <author> Cestnik, B. & Bratko, I. </author> , <title> On Estimating Probabilities in Structure Pruning, </title> <booktitle> Proceeding of EWSL 91, </booktitle> <pages> (pp. 138-150) Porto, </pages> <address> Portugal, March 6-8, </address> <year> 1991. </year>
Reference-contexts: The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction [13, 14], the gini index of diversity [5], and others <ref> [6, 7, 12] </ref>. A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making situations it was designed for remain constant. Problems arise when these assumptions do not hold.
Reference: 7. <author> Cestnik, B. & Karalic, A., </author> <title> The Estimation of Probabilities in Attribute Selection Measures for Decision Structure Induction in Proceeding of the European Summer School on Machine Learning , July 22-31, </title> <address> Priory Corsendonk, Belgium, </address> <year> 1991. </year>
Reference-contexts: The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction [13, 14], the gini index of diversity [5], and others <ref> [6, 7, 12] </ref>. A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making situations it was designed for remain constant. Problems arise when these assumptions do not hold. <p> This is done based on ideas of rule truncation described in <ref> [2, 7] </ref>. When noise is expected in the training data, the decision rules with t-weight below a certain threshold (reflecting the expected noise level) are removed.
Reference: 8. <author> Imam, I.F. and Michalski, </author> <title> R.S. , "Learning Decision Structures from Decision Rules: A method and initial results from a comparative study", </title> <journal> in Journal of Intelligent Information Systems JIIS, </journal> <volume> Vol. 2, No. 3, </volume> <pages> pp. 279-304, </pages> <editor> Kerschberg, L., Ras, Z., & Zemankova, M. (Eds.), </editor> <publisher> Kluwer Academic Pub., </publisher> <address> MA, </address> <year> 1993. </year>
Reference-contexts: Such virtual decision structures are easy to tailor to any given decision making situation. This approach allows one to generate a decision structure that avoids or delays evaluating an attribute that is difficult to measure. Initial research on this approach, and the first system implementation, AQDT-1, are described in <ref> [8] </ref>. This paper presents a new version of the system, called AQDT-2. The new system generates a goal-oriented decision structure from decision rules learned by either rule learning system AQ15 [11] or system AQ17, which has extensive constructive induction capabilities [2]. <p> Branches stemming from the root are marked by values x6 (in general, it could be groups of values), according to the way they occur in the decision rules (groups subsumed by other groups are removed <ref> [8] </ref>. The branches are assigned subsets of the rules containing these values. The process repeats for a branch until all rules assigned to each branch are of the same class. That class is then assigned to the leaf.
Reference: 9. <editor> Imam, I.F., Michalski, R.S. and Kerschberg, L., </editor> <title> Discovering Attribute Dependence in Databases by Integrating Symbolic Learning and Statistical Analysis Techniques", </title> <booktitle> Proceeding of the First International Workshop on Knowledge Discovery in Database , Washington, </booktitle> <address> D.C., </address> <month> July, </month> <pages> 11-12, </pages> <year> 1993. </year>
Reference-contexts: Importance. The second elementary criterion, the importance of a test, is based on the importance score (IS), introduced in <ref> [9] </ref>. In the obtained rules, each test is assigned a score that represents the total number of training examples, which are covered by the rules involving this test. Decision rules learned by an AQ learning program are accompanied with information on their strength.
Reference: 10. <author> Michalski, </author> <title> R.S. , AQVAL/1-Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition, </title> <booktitle> Proceeding of the First International Joint Conference on Pattern Recognition , (pp. </booktitle> <pages> 3-17), </pages> <address> Washington, DC, </address> <month> October 30- November 1, </month> <year> 1973. </year>
Reference-contexts: For example, the condition part [x3=1 v 3]&[x4=1] is multiplied out to two rules with condition parts [x3=1]&[x4=1] and [x3=3]&[x4=1]. The above criteria are combined into one general test measure using the lexicographic evaluation functional with tolerances (LEF) <ref> [10] </ref>. LEF is a list of some or all of the above elementary criteria, each associated with a "tolerance threshold" in percentage. The criteria are applied to tests in the order defined by LEF.
Reference: 11. <author> Michalski, R.S., Mozetic, I., Hong, J. & Lavrac, N., </author> <title> The MultiPurpose Incremental Learning System AQ15 and Its Testing Application to Three Medical Domains, </title> <booktitle> Proceedings of AAAI-86 , (pp. </booktitle> <pages> 1041-1045), </pages> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: Initial research on this approach, and the first system implementation, AQDT-1, are described in [8]. This paper presents a new version of the system, called AQDT-2. The new system generates a goal-oriented decision structure from decision rules learned by either rule learning system AQ15 <ref> [11] </ref> or system AQ17, which has extensive constructive induction capabilities [2].
Reference: 12. <author> Mingers, J., </author> <title> An Empirical Comparison of selection Measures for Decision-Structure Induction, </title> <journal> Machine Learning, </journal> <volume> Vol. 3, No. 3, </volume> <pages> (pp. 319-342), </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989a. </year>
Reference-contexts: The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction [13, 14], the gini index of diversity [5], and others <ref> [6, 7, 12] </ref>. A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making situations it was designed for remain constant. Problems arise when these assumptions do not hold.
Reference: 13. <author> Quinlan, J.R., </author> <title> Discovering Rules By Induction from Large Collections of Examples, </title> <editor> in D. Michie (Edr), </editor> <booktitle> Expert Systems in the Microelectronic Age , Edinburgh University Press, </booktitle> <year> 1979. </year>
Reference-contexts: Decision trees are typically generated from a set of examples of decisions. The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction <ref> [13, 14] </ref>, the gini index of diversity [5], and others [6, 7, 12]. A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making situations it was designed for remain constant.
Reference: 14. <author> Quinlan, J.R., </author> <title> Learning efficient classification procedures and their application to chess end games in R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach . Los Altos: </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: Decision trees are typically generated from a set of examples of decisions. The essential characteristic of any such method is the attribute selection criterion used for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include the entropy reduction <ref> [13, 14] </ref>, the gini index of diversity [5], and others [6, 7, 12]. A decision tree/decision structure can be an effective tool for describing a decision process, as long as all the required tests can be measured, and the decision making situations it was designed for remain constant.
Reference: 15. <author> Quinlan, J. R. </author> <title> Probabilistic decision structures, </title> <editor> in Y. Kodratoff and R.S. Michalski (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. </booktitle> <address> III , San Mateo, CA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> (pp. 63-111), </pages> <month> June, </month> <year> 1990. </year>
Reference-contexts: That class is then assigned to the leaf. The prediction accuracy was 88.7% (102 testing examples were classified correctly and 13 miss-classified). For comparison, the program C4.5 for learning decision trees from examples was also applied to this same problem <ref> [15] </ref>. The experiment was done with C4.5 using the default window setting (maximum of 20% the number of examples and twice the square root the number of examples), and set the number of trials set to one. <p> = .23, P (C3)=0 and P (C4) =.11. 1 2..4 2 3 1 Complexity No. of nodes: 5 No. of leaves: 7 1 2 C2 x7 C2 C2 C2 .23 C2 .63 C1 .53 A related method for handling the problem of unavailability of an attribute is described by Quinlan <ref> [15] </ref>. His method, however, assigns probabilities to the given decision tree, without first trying to restructure it appropriately to a given decision making situation (in this case, to avoid measuring x1).
Reference: 16. <author> Wnek, J., </author> <title> "A Fast ReImplementation of the AQ-based Inductive Learning Program for Large Datasets: AQ15c," Reports of Machine Learning and Inference Laboratory , MLI 94-3, </title> <institution> Center for Artificial Intelligence, George Mason University, </institution> <note> 1994 (to appear). </note>
Reference-contexts: In the method proposed, this first phase is done by an AQ algorithm-based rule learning method. While past implementations of AQ-based methods were computationally complex, the most recent implementation is very fast <ref> [16] </ref>; thus the generating decision rules phase can be done quite efficiently. The current method has a number of limitations, and several issues need to be investigated further. First of all, there is need for more testing of the method.
References-found: 16

