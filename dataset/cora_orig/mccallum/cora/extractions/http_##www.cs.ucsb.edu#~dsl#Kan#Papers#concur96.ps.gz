URL: http://www.cs.ucsb.edu/~dsl/Kan/Papers/concur96.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~dsl/Kan/Papers/
Root-URL: http://www.cs.ucsb.edu
Title: The Impact of Hardware Models on Shared Memory Consistency Conditions  
Author: Jerry James and Ambuj Singh 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California at Santa Barbara  
Abstract: Shared memory systems provide a contract to the programmer in the form of a consistency condition. The conditions of atomic memory and sequential consistency provide the illusion of a single memory module, as in the uniprocessor case. Weaker conditions improve performance by sacrificing the simple programming model. Consistency conditions are formulated without reference to details of the hardware on which programs execute. We define the notion of a hardware model, a set of limitations on the communication network (e.g., message delay assumptions) and processing nodes (e.g., amount of available memory). We examine the effects of several models on a representative set of consistency conditions. In each model, we show how the conditions are related, and show that some are not appropriate for that model. Our study is carried out through relatively complete implementations, state machines which exactly capture the possible behaviors of all implementations in a given model. In addition to elucidating properties of the consistency conditions, these state machines can be used in proofs of correctness, when a particular hardware model is assumed.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Agrawal, D., Choy, M., Leong, H. V., and Singh, A. K. </author> <title> Mixed consistency: A model for parallel programming. </title> <address> In PODC '94 (Los Angeles, CA, USA, 14-17 Aug. </address> <year> 1994), </year> <pages> pp. 101-10. </pages>
Reference-contexts: Weak consistency conditions introduce a new problem, however; they are frequently unable to support common concurrent programming methods [5]. A class of hybrid consistency conditions (e.g., <ref> [1, 4] </ref>) have been developed which combine two or more conditions in an attempt at reaping the benefits of both. All shared memory consistency conditions have been formulated without regard to the capabilities of any particular distributed system, for full generality.
Reference: 2. <author> Ahamad, M., Bazzi, R., John, R., Kohli, P., and Neiger, G. </author> <title> The power of processor consistency. </title> <booktitle> In SPAA '93 (Velen, </booktitle> <address> Germany, </address> <month> June </month> <year> 1993), </year> <pages> pp. 251-60. </pages>
Reference-contexts: The simple programming model of the strong conditions was sacrificed in the development of weak consistency conditions (e.g., causal memory [3] and processor consistency <ref> [2, 12] </ref>), in an attempt to achieve greater efficiency. Weak consistency conditions introduce a new problem, however; they are frequently unable to support common concurrent programming methods [5]. <p> Processor consistency Processor consistency was first described by Good-man [12], but later researchers put varying interpretations on his definition (see, e.g. <ref> [2, 9, 10, 22] </ref>). We use the interpretation of [2], in which the set of processor consistent histories is the intersection of the coherent and the PRAM histories. <p> Processor consistency Processor consistency was first described by Good-man [12], but later researchers put varying interpretations on his definition (see, e.g. [2, 9, 10, 22]). We use the interpretation of <ref> [2] </ref>, in which the set of processor consistent histories is the intersection of the coherent and the PRAM histories.
Reference: 3. <author> Ahamad, M., Neiger, G., Burns, J. E., Kohli, P., and Hutto, P. W. </author> <title> Causal memory: Definitions, implementation and programming. </title> <journal> Dist. Comput. </journal> <volume> 9, </volume> <month> 1 (Aug. </month> <year> 1995), </year> <pages> 37-49. </pages>
Reference-contexts: The simple programming model of the strong conditions was sacrificed in the development of weak consistency conditions (e.g., causal memory <ref> [3] </ref> and processor consistency [2, 12]), in an attempt to achieve greater efficiency. Weak consistency conditions introduce a new problem, however; they are frequently unable to support common concurrent programming methods [5]. <p> It is cache consistent since the following serializations exist: S x : R 2 (x)? W 1 (x)1 However, it is not sequentially consistent; otherwise, at least one of the read operations would return 1. P 1 : W (x)1 R (y)? Causal Memory Causal memory <ref> [3] </ref> preserves the causal order. Once a write w becomes visible at p i , all writes which may have influenced w are also visible at p i .
Reference: 4. <author> Attiya, H., and Friedman, R. </author> <title> A correctness condition for high-performance multiprocessors. </title> <booktitle> In STOC '92 (Victoria, </booktitle> <address> British Columbia, Canada, </address> <month> 4-6 May </month> <year> 1992), </year> <pages> pp. 679-90. </pages>
Reference-contexts: Weak consistency conditions introduce a new problem, however; they are frequently unable to support common concurrent programming methods [5]. A class of hybrid consistency conditions (e.g., <ref> [1, 4] </ref>) have been developed which combine two or more conditions in an attempt at reaping the benefits of both. All shared memory consistency conditions have been formulated without regard to the capabilities of any particular distributed system, for full generality. <p> Hybrid consistency Hybrid consistency <ref> [4, 8] </ref> divides all accesses into strong and weak varieties. The strong accesses are sequentially consistent. The weak accesses are not ordered, except by the strong accesses they lie between.
Reference: 5. <author> Attiya, H., and Friedman, R. </author> <title> Limitations of fast consistency conditions for distributed shared memories. </title> <journal> Inf. Process. Lett. </journal> <volume> 57, </volume> <month> 5 (Mar. </month> <year> 1996), </year> <pages> 243-8. </pages>
Reference-contexts: Weak consistency conditions introduce a new problem, however; they are frequently unable to support common concurrent programming methods <ref> [5] </ref>. A class of hybrid consistency conditions (e.g., [1, 4]) have been developed which combine two or more conditions in an attempt at reaping the benefits of both. All shared memory consistency conditions have been formulated without regard to the capabilities of any particular distributed system, for full generality.
Reference: 6. <author> Attiya, H., and Welch, J. L. </author> <title> Sequential consistency versus linearizability. </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 12, </volume> <month> 2 (May </month> <year> 1994), </year> <pages> 91-122. </pages>
Reference-contexts: That is, shared memory accesses appear to occur atomically, as in the uniprocessor case. Such systems ease the programmer's burden, but efficiency is a major concern. For example, it is known that neither atomic memory nor sequential consistency can be implemented without some blocking <ref> [6] </ref>. ? This research was supported in part by NSF grants CCR-9223094 and CCR-9505807. The simple programming model of the strong conditions was sacrificed in the development of weak consistency conditions (e.g., causal memory [3] and processor consistency [2, 12]), in an attempt to achieve greater efficiency.
Reference: 7. <author> Eskicioglu, M. R. </author> <title> A comprehensive bibliography of distributed shared memory. Op. </title> <journal> Sys. Review 30, </journal> <month> 1 (Jan. </month> <year> 1996), </year> <pages> 71-96. </pages>
Reference-contexts: An attractive alternative is to implement a shared memory abstraction in software on top of a message passing layer (i.e., distributed shared memory, or DSM <ref> [7] </ref>). A DSM system provides a contract to the programmer in the form of a consistency condition. Strong consistency conditions, such as atomic memory [18] and sequential consistency [17], make the shared memory behave like a single memory module.
Reference: 8. <author> Friedman, R. </author> <title> Implementing hybrid consistency with high-level synchronization operations. </title> <journal> Dist. Comput. </journal> <volume> 9, </volume> <month> 3 (Dec. </month> <year> 1995), </year> <pages> 119-29. </pages>
Reference-contexts: Hybrid consistency Hybrid consistency <ref> [4, 8] </ref> divides all accesses into strong and weak varieties. The strong accesses are sequentially consistent. The weak accesses are not ordered, except by the strong accesses they lie between.
Reference: 9. <author> Gharachorloo, K., Adve, S. V., Gupta, A., Hennessy, J. L., and Hill, M. D. </author> <title> Specifying system requirements for memory consistency models. </title> <type> Tech. Rep. </type> <institution> CSL-TR-93-594, Computer System Laboratory, Stanford University, </institution> <year> 1993. </year> <note> Also University of Wisconsin-Madison Computer Sciences Technical Report #1199. </note>
Reference-contexts: Processor consistency Processor consistency was first described by Good-man [12], but later researchers put varying interpretations on his definition (see, e.g. <ref> [2, 9, 10, 22] </ref>). We use the interpretation of [2], in which the set of processor consistent histories is the intersection of the coherent and the PRAM histories.
Reference: 10. <author> Gharachorloo, K., Lenoski, D., Laudon, J., Gibbons, P., Gupta, A., and Hennessy, J. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In ISCA '90 (Seattle, </booktitle> <address> WA, USA, </address> <month> 28-31 May </month> <year> 1990), </year> <pages> pp. 15-26. </pages> <note> See revision in Stanford University tech. report CSL-TR-93-568. </note>
Reference-contexts: In related work, Gibbons and Merritt [11] use I/O automata to define a base memory system supporting per-variable consistency, or cache consistency. The actions of this automaton are later restricted to support release consistency <ref> [10] </ref>. They establish that release consistency behaves as sequential consistency in the absence of data races. The implementations that we develop are similar in spirit, but based on a different set of primitives closer to the machine architecture. <p> Processor consistency Processor consistency was first described by Good-man [12], but later researchers put varying interpretations on his definition (see, e.g. <ref> [2, 9, 10, 22] </ref>). We use the interpretation of [2], in which the set of processor consistent histories is the intersection of the coherent and the PRAM histories.
Reference: 11. <author> Gibbons, P. B., and Merritt, M. </author> <title> Specifying nonblocking shared memories. </title> <booktitle> In SPAA '92 (San Diego, </booktitle> <address> CA, USA, </address> <month> 29 June-1 July </month> <year> 1992), </year> <pages> pp. 306-15. </pages>
Reference-contexts: Behavior inclusion can be proved using the theory of trace inclusion [21] for I/O automata [20], which develops refinements and forward and backward simulations as proof tools. In related work, Gibbons and Merritt <ref> [11] </ref> use I/O automata to define a base memory system supporting per-variable consistency, or cache consistency. The actions of this automaton are later restricted to support release consistency [10]. They establish that release consistency behaves as sequential consistency in the absence of data races.
Reference: 12. <author> Goodman, J. R. </author> <title> Cache consistency and sequential consistency. </title> <type> Tech. Rep. 1006, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: The simple programming model of the strong conditions was sacrificed in the development of weak consistency conditions (e.g., causal memory [3] and processor consistency <ref> [2, 12] </ref>), in an attempt to achieve greater efficiency. Weak consistency conditions introduce a new problem, however; they are frequently unable to support common concurrent programming methods [5]. <p> SC: There is a serialization S of H such that (9i :: o 1 i S Cache consistency (coherence) Cache consistency, or memory coherence, guarantees that accesses to any given memory location appear in the same order everywhere <ref> [12] </ref>. Processes may disagree on the order of two accesses to two different locations, however. <p> Processor consistency Processor consistency was first described by Good-man <ref> [12] </ref>, but later researchers put varying interpretations on his definition (see, e.g. [2, 9, 10, 22]). We use the interpretation of [2], in which the set of processor consistent histories is the intersection of the coherent and the PRAM histories.
Reference: 13. <author> Gupta, V. </author> <title> Chu Spaces: A Model of Concurrency. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: The implementations that we develop are similar in spirit, but based on a different set of primitives closer to the machine architecture. Pratt and Gupta have developed a theory of concurrency based on the algebra of Chu spaces <ref> [13, 14, 23] </ref>. In this theory, a duality exists between schedules (sequences of events) and automata (state-based structures). This is similar in spirit to our construction of relatively complete implementations, since the possible behaviors of an automaton are captured by the schedule which is its dual, and conversely.
Reference: 14. <author> Gupta, V., and Pratt, V. </author> <title> Gates accept concurrent behavior. </title> <booktitle> In FOCS '93 (Palo Alto, </booktitle> <address> CA, USA, 3-5 Nov. </address> <year> 1993), </year> <pages> pp. 62-71. </pages>
Reference-contexts: The implementations that we develop are similar in spirit, but based on a different set of primitives closer to the machine architecture. Pratt and Gupta have developed a theory of concurrency based on the algebra of Chu spaces <ref> [13, 14, 23] </ref>. In this theory, a duality exists between schedules (sequences of events) and automata (state-based structures). This is similar in spirit to our construction of relatively complete implementations, since the possible behaviors of an automaton are captured by the schedule which is its dual, and conversely.
Reference: 15. <author> Herlihy, M. P., and Wing, J. M. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Trans. Program. Lang. Syst. </journal> <volume> 12, </volume> <month> 3 (July </month> <year> 1990), </year> <pages> 463-92. </pages>
Reference-contexts: This condition, as we explain below, is the intersection of PRAM and CC, or cache consistency. A brief description of each condition follows. Atomic memory (linearizability) Linearizability, or atomic consistency, was first described for atomic variables in [18] and extended to arbitrary objects in <ref> [15] </ref>. The condition requires that all processes agree on the order of all operations, and that order must be consistent with the global time order gt LIN: There is a linearization L of H. Sequential consistency Sequential consistency was first defined in [17].
Reference: 16. <author> James, J., and Singh, A. K. </author> <title> The impact of hardware models on shared memory consistency conditions. </title> <type> Tech. Rep. </type> <institution> TRCS96-12, Computer Science Department, University of California at Santa Barbara, </institution> <year> 1996. </year>
Reference-contexts: In some cases, it may be impossible to produce a relatively complete implementation. Due to space limitations, we give only some of the results of our study. The full paper <ref> [16] </ref> contains the proofs and relatively complete implementation constructions for each model. Here we simply show the existence or non-existence of relatively complete implementations for a few models, although we also give one relatively complete implementation in the appendix. We note that the ability to make random choices seems critical. <p> In the full paper <ref> [16] </ref>, we prove several such theorems. For example, a relatively complete implementation I is relatively complete for any model with a smaller set of implementations, so long as I is still in the set. It is also possible to find transformations from one implementation to another.
Reference: 17. <author> Lamport, L. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Trans. Comput. </journal> <volume> 28, </volume> <month> 9 (Sept. </month> <year> 1979), </year> <pages> 690-1. </pages>
Reference-contexts: A DSM system provides a contract to the programmer in the form of a consistency condition. Strong consistency conditions, such as atomic memory [18] and sequential consistency <ref> [17] </ref>, make the shared memory behave like a single memory module. That is, shared memory accesses appear to occur atomically, as in the uniprocessor case. Such systems ease the programmer's burden, but efficiency is a major concern. <p> The condition requires that all processes agree on the order of all operations, and that order must be consistent with the global time order gt LIN: There is a linearization L of H. Sequential consistency Sequential consistency was first defined in <ref> [17] </ref>. It requires that some serialization S of H exists which preserves each local program order i !. Another way of stating this requirement is that all processes agree on the order of all operations.
Reference: 18. <author> Lamport, L. </author> <title> On interprocess communication, parts I and II. </title> <journal> Dist. Comput. </journal> <volume> 1, </volume> <month> 2 (Apr. </month> <year> 1986), </year> <pages> 77-101. </pages>
Reference-contexts: An attractive alternative is to implement a shared memory abstraction in software on top of a message passing layer (i.e., distributed shared memory, or DSM [7]). A DSM system provides a contract to the programmer in the form of a consistency condition. Strong consistency conditions, such as atomic memory <ref> [18] </ref> and sequential consistency [17], make the shared memory behave like a single memory module. That is, shared memory accesses appear to occur atomically, as in the uniprocessor case. Such systems ease the programmer's burden, but efficiency is a major concern. <p> A history is said to be complete if every Request event has a matching Reply event; i.e., there are no pending operations. In the sequel, we only consider complete histories. We assume that there is a global time model <ref> [18] </ref> for the system, so that runs, executions, and histories are labeled with a time for each event. <p> This condition, as we explain below, is the intersection of PRAM and CC, or cache consistency. A brief description of each condition follows. Atomic memory (linearizability) Linearizability, or atomic consistency, was first described for atomic variables in <ref> [18] </ref> and extended to arbitrary objects in [15]. The condition requires that all processes agree on the order of all operations, and that order must be consistent with the global time order gt LIN: There is a linearization L of H. Sequential consistency Sequential consistency was first defined in [17].
Reference: 19. <author> Lipton, R. J., and Sandberg, J. S. </author> <title> PRAM: A scalable shared memory. </title> <type> Tech. Rep. </type> <institution> CS-TR-180-88, Department of Computer Science, Princeton University, </institution> <month> Sept. </month> <year> 1988. </year>
Reference-contexts: CM: For each process p i , there is a serialization S i of H i+w such that (8o 1 ; o 2 2 H i+w :: o 1 ! o 2 ) o 1 ! o 2 ). Pipelined RAM Pipelined RAM, or PRAM <ref> [19] </ref>, is a very weak consistency condition. It does not provide any guarantees about the relative order of operations between pairs of process histories, but only that write operations are seen by other processes in the order in which they are invoked.
Reference: 20. <author> Lynch, N. A., and Tuttle, M. R. </author> <title> Hierarchical correctness proofs for distributed algorithms. </title> <booktitle> In PODC '87 (Vancouver, </booktitle> <address> British Columbia, Canada, 10-12 Aug. </address> <year> 1987), </year> <pages> pp. 137-51. </pages>
Reference-contexts: A proof of behavior inclusion, coupled with a proof of correctness for the relatively complete implementation, yields the desired correctness proof. Behavior inclusion can be proved using the theory of trace inclusion [21] for I/O automata <ref> [20] </ref>, which develops refinements and forward and backward simulations as proof tools. In related work, Gibbons and Merritt [11] use I/O automata to define a base memory system supporting per-variable consistency, or cache consistency. The actions of this automaton are later restricted to support release consistency [10]. <p> They are related as shown in Fig. 1. Each has possibly infinite state, and special input and output actions for communicating with its environment (i.e., they are I/O automata <ref> [20] </ref>).
Reference: 21. <author> Lynch, N. A., and Vaandrager, F. </author> <title> Forward and backward simulations. </title> <journal> Inf. Comput. </journal> <volume> 121, </volume> <month> 2 (Sept. </month> <year> 1995), </year> <pages> 214-233. </pages>
Reference-contexts: A proof of behavior inclusion, coupled with a proof of correctness for the relatively complete implementation, yields the desired correctness proof. Behavior inclusion can be proved using the theory of trace inclusion <ref> [21] </ref> for I/O automata [20], which develops refinements and forward and backward simulations as proof tools. In related work, Gibbons and Merritt [11] use I/O automata to define a base memory system supporting per-variable consistency, or cache consistency.
Reference: 22. <author> Mosberger, D. </author> <title> Memory consistency models. Op. </title> <journal> Sys. Review 27, </journal> <month> 1 (Jan. </month> <year> 1993), </year> <pages> 18-26. </pages>
Reference-contexts: Processor consistency Processor consistency was first described by Good-man [12], but later researchers put varying interpretations on his definition (see, e.g. <ref> [2, 9, 10, 22] </ref>). We use the interpretation of [2], in which the set of processor consistent histories is the intersection of the coherent and the PRAM histories.
Reference: 23. <author> Pratt, V. </author> <title> The second calculus of binary relations. </title> <booktitle> In MFCS '93 (Gdansk, </booktitle> <address> Poland, 30 Aug.-3 Sept. </address> <year> 1993), </year> <editor> A. M. Borzyszkowski and S. Sokolowski, Eds., </editor> <publisher> Springer-Verlag, </publisher> <pages> pp. 142-155. </pages>
Reference-contexts: The implementations that we develop are similar in spirit, but based on a different set of primitives closer to the machine architecture. Pratt and Gupta have developed a theory of concurrency based on the algebra of Chu spaces <ref> [13, 14, 23] </ref>. In this theory, a duality exists between schedules (sequences of events) and automata (state-based structures). This is similar in spirit to our construction of relatively complete implementations, since the possible behaviors of an automaton are captured by the schedule which is its dual, and conversely.
References-found: 23

