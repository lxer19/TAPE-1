URL: http://www.cs.tu-berlin.de/techreports/kit/documents/Quantz/ijcai93.ps.gz
Refering-URL: http://www.cs.tu-berlin.de/techreports/
Root-URL: 
Email: email: jjq@cs.tu-berlin.de  
Title: Interpretation as Exception Minimization how contextual information comprising grammatical as well as conceptual knowledge can
Author: J. Joachim Quantz 
Note: I show  
Address: Projekt KIT-BACK, FR 5-12, Franklinstr. 28/29, W-1000 Berlin 10, Germany  
Affiliation: Technische Universitat Berlin,  
Abstract: Ambiguity is a notorious problem for Natural Language Processing. According to results obtained by Schmitz and Quantz I see disambiguation as a process in which contextual defaults are used to derive the most preferred interpretation of an expression. 
Abstract-found: 1
Intro-found: 1
Reference: [Allgayer et al., 89] <author> J. Allgayer, R. Jansen-Winkeln, C. Red-dig, N. Reithinger, </author> <title> Bidirectional Use of Knowledge in The Multi-Modal NL Access System XTRA, </title> <booktitle> in IJCAI`89, </booktitle> <pages> 1492-1497 </pages>
Reference-contexts: A fundamental assumption of this approach towards disambiguation is that contextual information of quite heterogenous nature, such as syntactic, semantic, conceptual, or domain specific information is used in the disambiguation process. Consequently, the strictly sequential architecture of interpretation used in most existing NL systems, e.g. <ref> [Allgayer et al., 89] </ref>, [Stock, 91], [Schmitz et al, 91], in which syntactic, semantic, and conceptual information is represented on different levels and in different formalisms, seems to be inadequate. <p> I will give no examples for the modeling of semantic, conceptual, or encyclopedic information in TL, since this has been already exemplified in a number of existing systems, e.g. <ref> [Allgayer et al., 89] </ref>, [Stock, 91], [Schmitz et al, 91], [Bateman, 92]. In the following I will therefore assume that G contains information of quite heterogeneous nature modeled homogeneously as TL formulae.
Reference: [Backofen et al, 90] <author> R. Backofen, H. Trost, H. Uszkoreit, </author> <title> Linking Typed Feature Formalisms and Terminological Knowledge Representation Languages in Natural Language Front-Ends, </title> <note> DFKI Research-Report 91-28, </note> <year> 1991 </year>
Reference-contexts: only for NLP but for diagnosis tasks in general. 2 Terminological Logics and Typed Feature Structures The similarity between Terminological Logics (TL) and Typed Feature Structures (TFS), which provide the formal paradigm of Unification Grammars as HPSG, has been mentioned in a number of papers (e.g., [Nebel and Smolka, 87], <ref> [Backofen et al, 90] </ref>, and [Carpenter, 92]). The basic constructs of TFS are feature structures, i.e. sets of feature value pairs, where features are partial functions from feature structures into values. The corresponding entities in TL are concepts.
Reference: [Bateman, 92] <author> J. Bateman, </author> <title> The Use and Design of Linguistically Motivated Ontologies for Controlling and Deployment of Linguistic Resources, </title> <editor> in [Preuss and Schmitz, </editor> <volume> 92], </volume> <pages> 50-99 </pages>
Reference-contexts: In this paper I try to combine the ideas of HPSG and Terminological Logics (TL), which are widely used in NL systems for the representation of conceptual and domain specific knowledge (e.g., the systems cited above and <ref> [Bateman, 92] </ref>). <p> I will give no examples for the modeling of semantic, conceptual, or encyclopedic information in TL, since this has been already exemplified in a number of existing systems, e.g. [Allgayer et al., 89], [Stock, 91], [Schmitz et al, 91], <ref> [Bateman, 92] </ref>. In the following I will therefore assume that G contains information of quite heterogeneous nature modeled homogeneously as TL formulae.
Reference: [Blasius et al, 87] <editor> K.H. Blasius, U. Hedtstuck, C.-R. Rollin-ger (eds), </editor> <booktitle> Sorts and Types in Artificial Intelligence, </booktitle> <address> Berlin: </address> <publisher> Springer, </publisher> <year> 1987 </year>
Reference: [Buchheit et al, 93] <author> M. Buchheit, R. Klein, W. Nutt, </author> <title> Constructive Problem SolvingA Model Construction Approach Towards Configuration, </title> <note> Preliminary Draft </note>
Reference-contexts: This should probably be done outside the TL system. Related to this issue is the general topic of integrating abductive reasoning into TL. There is currently some research aiming at this integration for configuration tasks <ref> [Buchheit et al, 93] </ref>. Another issue concerns the representation of ambiguous information. Variable depth of analysis means that not all ambiguities have to be resolved but only the ones relevant for the specific purpose of interpretation (e.g., to select the appropriate transfer rules in Machine Translation).
Reference: [Carpenter, 92] <author> B. Carpenter, </author> <title> The Logic of Typed Feature Structures, </title> <publisher> Cambridge: Cambridge University Press, </publisher> <year> 1992 </year>
Reference-contexts: diagnosis tasks in general. 2 Terminological Logics and Typed Feature Structures The similarity between Terminological Logics (TL) and Typed Feature Structures (TFS), which provide the formal paradigm of Unification Grammars as HPSG, has been mentioned in a number of papers (e.g., [Nebel and Smolka, 87], [Backofen et al, 90], and <ref> [Carpenter, 92] </ref>). The basic constructs of TFS are feature structures, i.e. sets of feature value pairs, where features are partial functions from feature structures into values. The corresponding entities in TL are concepts.
Reference: [Donini et al, 91] <author> F.M. Donini, M. Lenzerini, D. Nardi, W. Nutt, </author> <title> The Complexity of Concept Languages, </title> <booktitle> KR'91, </booktitle> <pages> 151-162 </pages>
Reference-contexts: I will not address the issue of decidability or tractability for the resulting TL. Even without these extensions, the core TL needed for representing linguistic information is probably undecidable (see <ref> [Donini et al, 91] </ref> for complexity results of TL). The test/compute construct allows the attachement of external procedures into a TL system. The formal integration of the compute construct into the TL system BACK is described in [Kortum, 93].
Reference: [Donini et al, 92] <author> F.M. Donini, M. Lenzerini, D. Nardi, A. Schaerf, W. Nutt, </author> <title> Adding Epistemic Operators to Concept Languages, </title> <editor> in [Nebel et al, </editor> <volume> 92], </volume> <pages> 342-353 </pages>
Reference-contexts: These arguments will typically be the values of other roles. The integration of epistemic operators into TL has been proposed in <ref> [Donini et al, 92] </ref>. The K operator can be used to distinguish cases in which an object is known to be a filler for role r, from cases in which just the type of the role-filler is known. <p> It thus seems that further restrictions on interpretations are required. The general idea is that the values of certain features have to be determined and that disjunctive information about them is not enough. This can be formalized by using epistemic operators as proposed in <ref> [Donini et al, 92] </ref>. The requirement that a value for a feature f must be known is expressible as the concept 1 Kf. We say that this concept is explicit wrt f and give the following general definition of explicitness.
Reference: [Eberle et al, 92] <author> K. Eberle, W. Kasper, C. Rohrer, </author> <title> Contextual Constraints for MT, </title> <booktitle> Proceedings of the TMI'92, </booktitle> <pages> 213-224 </pages>
Reference-contexts: Schmitz and Quantz have shown that the same general strategy can be used for the resolution of referential, lexical, and structural ambiguities [Schmitz and Quantz, 93]. The basic idea is that contextual information is used to resolve the local ambiguities (see also [Kameyama et al, 91] and <ref> [Eberle et al, 92] </ref>). In most cases, this information cannot be formalized as hard constraints but only as defaults, i.e. as rules which allow for exceptions. Each possible interpretation of an ambiguous expression satisfies some of the defaults while violating others.
Reference: [Hobbs et al, 93] <author> J.R. Hobbs, M. Stickel, D. Appelt, P. Martin, </author> <title> Interpretation as Abduction, </title> <note> to appear in Artificial Intelligence Journal </note>
Reference-contexts: Thus, the depth of analysis can be controled via the set of relevant roles. 2 Given the notion of explicitness we can formalize an ab-ductive approach to interpretation (see <ref> [Hobbs et al, 93] </ref>). In general, abduction is the inference of a formula A from the formulae A ! B and B. <p> Note that multiple abductions result from the existence of several explanations, i.e. formulae A i ! B. The idea of weighted abduction, as proposed in <ref> [Hobbs et al, 93] </ref>, is to assign costs and factors to the A i . Then the preferred abduction is the cheapest one, i.e. the one containing the least expensive explanations. Instead of pursuing the approach of weighted abduction, however, I will use defaults to model preferences between interpretations. <p> This set of relevant roles can be used to control the granularity of interpretation and thus provides the basis for a variable depth of analysis. The resulting approach to interpretation is very similar to the approach of weighted abduction described in <ref> [Hobbs et al, 93] </ref>. A detailed evaluation of both approaches is needed to compare their respective advantages and disadvantages. I see the logical framework presented here as a formal foundation for developing NL systems. To obtain efficient performance, however, heuristics have to be integrated to choose the preferred interpretation.
Reference: [Hoppe et al, 93] <author> T. Hoppe, C. Kindermann, J.J. Quantz, A. Schmiedel, M. Fischer, </author> <title> BACK V5 Tutorial & Manual, </title> <type> KIT Report 100, </type> <institution> Technische Universitat Berlin, </institution> <year> 1993 </year>
Reference-contexts: The following syntax contains only the TL operators used in this paper. For a formal description of the language implemented in the TL system BACK see <ref> [Hoppe et al, 93] </ref>, which also contains illustrating examples for domain modeling with TL.
Reference: [Jackendoff, 83] <author> R. Jackendoff, </author> <title> Semantics and Cognition, </title> <publisher> Cambridge: MIT Press, </publisher> <year> 1983 </year>
Reference-contexts: so is that the approach of weighted abduction cannot deal with the default information we need in order to model the contextual constraints. 3 In [Schmitz and Quantz, 93] it is explained in detail how defaults can be seen as a means to model preference rule systems as proposed in <ref> [Jackendoff, 83] </ref>.
Reference: [Kameyama et al, 91] <author> M. Kameyama, R. Ochitani, S. Peters, </author> <title> Resolving Translation Mismatches With Information Flow, </title> <booktitle> ACL'91, </booktitle> <pages> 193-200 </pages>
Reference-contexts: Schmitz and Quantz have shown that the same general strategy can be used for the resolution of referential, lexical, and structural ambiguities [Schmitz and Quantz, 93]. The basic idea is that contextual information is used to resolve the local ambiguities (see also <ref> [Kameyama et al, 91] </ref> and [Eberle et al, 92]). In most cases, this information cannot be formalized as hard constraints but only as defaults, i.e. as rules which allow for exceptions. Each possible interpretation of an ambiguous expression satisfies some of the defaults while violating others.
Reference: [Kasper, 87] <author> R.T. Kasper, </author> <title> A Unification Method for Disjunctive Feature Descriptions, </title> <booktitle> ACL'87, </booktitle> <pages> 235-242 </pages>
Reference-contexts: Thus an efficient representation of ambiguities is needed to avoid exponential computation of disjunctive normal forms. This topic is currently discussed in the literature on feature formalisms <ref> [Kasper, 87] </ref>, [Trost and Backofen, 92]. Acknowledgements I would like to thank Bob Carpenter, Christa Hauenschild, Jerry Hobbs, Bob Kasper, Werner Nutt, Susanne Preu, Birte Schmitz, and Carla Umbach for providing valuable information and/or thoroughly discussing the ideas underlying this paper.
Reference: [Kortum, 93] <author> G. Kortum, </author> <title> How To Compute 1+1? A Proposal for the Integration of External Functions and Computed Roles into BACK, </title> <type> KIT Report 103, </type> <year> 1993 </year>
Reference-contexts: The test/compute construct allows the attachement of external procedures into a TL system. The formal integration of the compute construct into the TL system BACK is described in <ref> [Kortum, 93] </ref>. The basic idea is that compute (r,pred (arg 1 ,...,arg n )) can be used to determine the role-filler (s) for a role r by calling the external predicate pred with the arguments arg i . These arguments will typically be the values of other roles.
Reference: [Krieger and Nerbonne, 91] <author> H.-U. Krieger, J. Nerbonne, </author> <title> Feature-Based Inheritance Networks for Computational Lexicons, </title> <note> DFKI Research Report 91-31, </note> <year> 1991 </year>
Reference-contexts: Second, there are general rules governing the derivation of wordforms from the stem of a lexeme. These general rules allow for exceptions, however, which are known as irregular forms (see <ref> [Krieger and Nerbonne, 91] </ref>). Lexical information can be used to illustrate the difference between the deductive and the abductive approach to interpretation. In the previous section I took the phonological information associated with an expression as the starting point for interpretation.
Reference: [Nebel and Smolka, 87] <author> B. Nebel, G. Smolka, </author> <title> Representation and Reasoning with Attributive Descriptions, </title> <editor> in [Blasius et al, </editor> <volume> 87], </volume> <pages> 112-139 </pages>
Reference-contexts: therefore of relevance not only for NLP but for diagnosis tasks in general. 2 Terminological Logics and Typed Feature Structures The similarity between Terminological Logics (TL) and Typed Feature Structures (TFS), which provide the formal paradigm of Unification Grammars as HPSG, has been mentioned in a number of papers (e.g., <ref> [Nebel and Smolka, 87] </ref>, [Backofen et al, 90], and [Carpenter, 92]). The basic constructs of TFS are feature structures, i.e. sets of feature value pairs, where features are partial functions from feature structures into values. The corresponding entities in TL are concepts.
Reference: [Nebel et al, 92] <editor> B. Nebel, C. Rich, W. Swartout, </editor> <booktitle> Principles of Knowledge Representation and Reasoning: Proceedings of the Third International Conference (KR'92), </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992 </year>
Reference: [Pereira and Warren, 80] <author> F.C.N. Pereira, D.H.D. Warren, </author> <title> Definite Clause Grammars for Language AnalysisA Survey of the Formalism and a Comparison with Augmented Transition Networks, </title> <booktitle> Artificial Intelligence 13, </booktitle> <pages> 231-278, </pages> <year> 1980 </year>
Reference-contexts: The task of interpretation consists in computing the information implicitly given in (5). Let us assume that another set of TL formulae G contains grammatical information (examples will be given in the next section). The deductive approach to interpretation (see <ref> [Pereira and Warren, 80] </ref>) can then be formalized as follows: 1 Definition 1 Let e be any NL expression. If G [ P e is inconsistent, e has no interpretation.
Reference: [Pollard and Sag, 87] <author> C. Pollard, I.A. Sag, </author> <title> An Information Based Syntax and Semantics, Vol. I Fundamentals, </title> <booktitle> Stan-ford: CSLI Lecture Notes 13, </booktitle> <year> 1987 </year>
Reference-contexts: I see Head-Driven Phrase Structure Grammar (HPSG) as an attempt to provide a homogenous formalism for integrating syntactic and semantic information <ref> [Pollard and Sag, 87] </ref>. In this paper I try to combine the ideas of HPSG and Terminological Logics (TL), which are widely used in NL systems for the representation of conceptual and domain specific knowledge (e.g., the systems cited above and [Bateman, 92]).
Reference: [Preuss and Schmitz, 92] <editor> S. Preu, B. Schmitz (eds.) </editor> , <booktitle> Workshop on Text Representation and Domain Modelling, </booktitle> <address> KIT-Report 97, Technische Universitat Berlin, </address> <year> 1992 </year>
Reference: [Quantz and Royer, 92] <author> J.J. Quantz, V. Royer, </author> <title> A Preference Semantics for Defaults in Terminological Logics, </title> <editor> in [Nebel et al, </editor> <volume> 92], </volume> <pages> 294-305 </pages>
Reference-contexts: Thus 1 r describes all objects which are known to have exactly one filler for r, but 1 Kr describes only those objects for which this filler is explicitly known. Finally, I need a default extension of TL as the one developed in <ref> [Quantz and Royer, 92] </ref>. The idea is to model defaults as rules c 1 ; c 2 that allow for exceptions. Each object which is a c 1 is assumed to be a c 2 unless this leads to inconsistencies. <p> Quantz and Royer have developed a default extension for TL in which a priority ordering between defaults can be specified <ref> [Quantz and Royer, 92] </ref>. The basic idea of their preference semantics is to prefer models with fewer exceptions. If two defaults are in conflict and stand in a priority relation, then the exception to the less prior default is tolerated. <p> As shown in [Schmitz et al, 91] and [Schmitz and Quantz, 93] most interpretations will generally violate more than one default. In order to choose between such interpretations, we thus need a priority ordering not only between single defaults but rather between sets of defaults. Following <ref> [Quantz and Royer, 92] </ref> we can define the exceptions to defaults in a model M as: def = fhffi; di : d 2 [[ffi p ]] I ^ d 62 [[ffi c ]] I g i.e. as pairs of defaults and domain elements which are members of their premise (ffi p <p> Let us now assign a natural number p ffi to each default representing the price of violating it: the more relevant a default, the higher its price. Given these prices we can redefine exception preference <ref> [Quantz and Royer, 92, Def. 5] </ref> as follows: Definition 4 Let G be any set of TL-formulae, M 1 and M 2 any models of G, and D any set of defaults. <p> a default has to be added for each occurrence in E i , i.e., if hffi; di 2 E i and hffi; ei 2 E i the price of ffi will be added twice.) Having thus modified exception preference, we obtain a D-entailment (G j= D o :: c, see <ref> [Quantz and Royer, 92, Def. 8] </ref>), which takes into account a relevance ordering between mul-tisets of defaults. Based on this D-entailment we can define preferred deductive interpretations: Definition 5 Let e be any NL expression. If G [ P e is inconsistent, e has no interpretation. <p> The basic idea is to choose the interpretation yielding a set of exceptions to defaults which is minimal wrt a relevance ordering on the defaults. To formalize this, I slightly modified the default extension of TL as proposed in <ref> [Quantz and Royer, 92] </ref>. Instead of a precedence relation between defaults, a precedence relation between multisets of defaults is needed. Moreover, epistemic operators can be used to define the notion of interpretations being explicit wrt a given set of roles.
Reference: [Schmitz and Quantz, 93] <author> B. Schmitz, J.J. Quantz, </author> <title> Defaults in Machine Translation, </title> <type> KIT Report 106, </type> <institution> Technische Uni-versitat Berlin, </institution> <year> 1993 </year>
Reference-contexts: 1 Introduction Ambiguity is a notorious problem for Natural Language Processing (NLP). Schmitz and Quantz have shown that the same general strategy can be used for the resolution of referential, lexical, and structural ambiguities <ref> [Schmitz and Quantz, 93] </ref>. The basic idea is that contextual information is used to resolve the local ambiguities (see also [Kameyama et al, 91] and [Eberle et al, 92]). <p> This paper is a formal elaboration of the technical aspects sketched in <ref> [Schmitz and Quantz, 93] </ref>. I will therefore reduce the linguistic motivation to a minimum and refer the interested reader to the examples presented in that paper. <p> will now show how interpretation can be defined formally in terms of TL entailment, how syntactic principles from HPSG can be modeled as TL formulae, and how defaults can be used to guide disambiguation by realizing interpretation as exception minimization. 3 Interpretation and Disambiguation Consider the following example taken from <ref> [Schmitz and Quantz, 93] </ref>: (3) Die Hersteller produzieren Schaltkreise. (4) (The manufacturers produce integrated circuits.) (5) e :: phon:Die Hersteller produzieren Schaltkreise u phrase u 4 atom u atom: (w 1 u w 2 u w 3 u w 4 ) w 1 :: word u pos:1 u phon:Die w 2 <p> Instead of pursuing the approach of weighted abduction, however, I will use defaults to model preferences between interpretations. The main reason for doing so is that the approach of weighted abduction cannot deal with the default information we need in order to model the contextual constraints. 3 In <ref> [Schmitz and Quantz, 93] </ref> it is explained in detail how defaults can be seen as a means to model preference rule systems as proposed in [Jackendoff, 83]. <p> The basic idea of their preference semantics is to prefer models with fewer exceptions. If two defaults are in conflict and stand in a priority relation, then the exception to the less prior default is tolerated. Consider again the examples from <ref> [Schmitz and Quantz, 93] </ref>: (6) Die Hersteller produzieren Schaltkreise. (7) Schaltkreise produzieren die Hersteller. <p> In example (6) the interpretation in which Die Hersteller is subject satisfies both defaults, whereas in example (7) it violates the first one. This might be taken as evidence for assigning ffi 2 more relevance than ffi 1 . As shown in [Schmitz et al, 91] and <ref> [Schmitz and Quantz, 93] </ref> most interpretations will generally violate more than one default. In order to choose between such interpretations, we thus need a priority ordering not only between single defaults but rather between sets of defaults.
Reference: [Schmitz et al, 91] <editor> B. Schmitz, S. Preu, C. Hauenschild, </editor> <title> Textrepr asentation und Hintergrundwissen in KIT-FAST, </title> <type> KIT Report 93, </type> <institution> Technische Universitat Berlin, </institution> <year> 1991 </year>
Reference-contexts: Consequently, the strictly sequential architecture of interpretation used in most existing NL systems, e.g. [Allgayer et al., 89], [Stock, 91], <ref> [Schmitz et al, 91] </ref>, in which syntactic, semantic, and conceptual information is represented on different levels and in different formalisms, seems to be inadequate. I see Head-Driven Phrase Structure Grammar (HPSG) as an attempt to provide a homogenous formalism for integrating syntactic and semantic information [Pollard and Sag, 87]. <p> I will give no examples for the modeling of semantic, conceptual, or encyclopedic information in TL, since this has been already exemplified in a number of existing systems, e.g. [Allgayer et al., 89], [Stock, 91], <ref> [Schmitz et al, 91] </ref>, [Bateman, 92]. In the following I will therefore assume that G contains information of quite heterogeneous nature modeled homogeneously as TL formulae. <p> In example (6) the interpretation in which Die Hersteller is subject satisfies both defaults, whereas in example (7) it violates the first one. This might be taken as evidence for assigning ffi 2 more relevance than ffi 1 . As shown in <ref> [Schmitz et al, 91] </ref> and [Schmitz and Quantz, 93] most interpretations will generally violate more than one default. In order to choose between such interpretations, we thus need a priority ordering not only between single defaults but rather between sets of defaults.
Reference: [Stock, 91] <author> O. </author> <title> Stock, Natural Language and Exploration of an Information Space: the ALFresco Interactive System, </title> <booktitle> in IJCAI'91, </booktitle> <pages> 972-978 </pages>
Reference-contexts: A fundamental assumption of this approach towards disambiguation is that contextual information of quite heterogenous nature, such as syntactic, semantic, conceptual, or domain specific information is used in the disambiguation process. Consequently, the strictly sequential architecture of interpretation used in most existing NL systems, e.g. [Allgayer et al., 89], <ref> [Stock, 91] </ref>, [Schmitz et al, 91], in which syntactic, semantic, and conceptual information is represented on different levels and in different formalisms, seems to be inadequate. <p> I will give no examples for the modeling of semantic, conceptual, or encyclopedic information in TL, since this has been already exemplified in a number of existing systems, e.g. [Allgayer et al., 89], <ref> [Stock, 91] </ref>, [Schmitz et al, 91], [Bateman, 92]. In the following I will therefore assume that G contains information of quite heterogeneous nature modeled homogeneously as TL formulae.
Reference: [Trost and Backofen, 92] <editor> H. Trost, R. Backofen (eds), </editor> <title> Coping with Linguistic Ambiguity in Typed Feature Formalisms, </title> <booktitle> Proceedings of a Workshop held at ECAI'92 </booktitle>
Reference-contexts: Thus an efficient representation of ambiguities is needed to avoid exponential computation of disjunctive normal forms. This topic is currently discussed in the literature on feature formalisms [Kasper, 87], <ref> [Trost and Backofen, 92] </ref>. Acknowledgements I would like to thank Bob Carpenter, Christa Hauenschild, Jerry Hobbs, Bob Kasper, Werner Nutt, Susanne Preu, Birte Schmitz, and Carla Umbach for providing valuable information and/or thoroughly discussing the ideas underlying this paper.
References-found: 26

