URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1998/tr-98-038.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1998.html
Root-URL: http://www.icsi.berkeley.edu
Title: A survey of fuzzy clustering algorithms for pattern recognition  
Author: A. Baraldi P. Blonda 
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  1947 Center Street, Suite 600, Berkeley, CA 94704-1198, Ph.:  IESI-CNR, Via Amendola 166/5, Bari 70126, Italy, Ph.: +39+80+5481612, Fx: +39+80+5484311,  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  International Computer Science Institute,  
Pubnum: TR-98-038  
Email: baraldi@icsi.berkeley.edu  blonda@iesi.ba.cnr.it  
Phone: (510) 643-9153 FAX (510) 643-7684  +1+510+643-9153, Fx.: +1+510+643-7684,  
Date: October 1998  
Abstract: Clustering algorithms aim at modelling fuzzy (i.e., ambiguous) unlabeled patterns efficiently. Our goal is to propose a theoretical framework where clustering systems can be compared on the basis of their learning strategies. In the first part of this work, the following issues are reviewed: relative (probabilistic) and absolute (possi-bilistic) fuzzy membership functions and their relationships to the Bayes rule, batch and on-line learning, growing and pruning networks, modular network architectures, topologically perfect mapping, ecological nets and neuro-fuzziness. From this discussion an equivalence between the concepts of fuzzy clustering and soft competitive learning in clustering algorithms is proposed as a unifying framework in the comparison of clustering systems. Moreover, a set of functional attributes is selected for use as dictionary entries in our comparison. In the second part of this paper, five clustering algorithms taken from the literature are reviewed and compared on the basis of the selected properties of interest. These networks clustering models are: i) Self-Organizing Map (SOM); ii) Fuzzy Learning Vector Quantization (FLVQ); iii) Fuzzy Adaptive Resonance Theory (Fuzzy ART); iv) Growing Neural Gas (GNG); and v) Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART). Although our theoretical comparison is fairly simple, it yields observations that may appear 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Baraldi and F. Parmiggiani, </author> <title> "Fuzzy clustering: critical analysis of the contextual mechanisms employed by three neural network models," </title> <booktitle> in SPIE Proc. on Applications of Fuzzy Logic Technology III, </booktitle> <editor> B. Bosacchi, J. Bezdek, Eds., </editor> <booktitle> SPIE vol. </booktitle> <volume> 2761, </volume> <pages> pp. 261-270, </pages> <year> 1996. </year>
Reference-contexts: The goal of this paper is to review and compare self-organization strategies of clustering algorithms that have been called fuzzy, or can be considered fuzzy, according to some definitions found in the existing literature. The best of our knowledge, few such comparative studies have been attempted (e.g., <ref> [1] </ref>). This may be due to the objective difficulty of comparing the great variety of clustering approaches based on a meaningful set of common functional features. <p> Equation (6) becomes p (C i jX k ) = P c ; k = 1; :::; n; i = 1; :::; c: (7) The following relationships hold true: (i) p (C i jX k ), p (X k jC i ) and p (C i ) belong to range <ref> [0; 1] </ref>; (ii) h=1 p (C h jX k ) = 1; k = 1; :::; n, i.e., mixture components C i , i = 1; :::; c; provide a complete partition of the input space; and (iii) h=1 p (C h ) = 1. <p> The decreasing expression of m (e) is <ref> [1] </ref>: m (e) = m 0 (e 4m) where 4m = (m 0 m f )=e max (13) * FLVQ employs metrical neighbors in the input space (analogously to GLVQ, GLVQ-F and FALVQ). <p> This asymptotic behavior is acknowledged in <ref> [1] </ref>, where it is recommended to keep m (e) away from infinity to prevent numerical instability of FLVQ. Our analysis suggests that rather than numerical instability, limiting condition m (e) ! 1 causes prototype collapse in both FLVQ and FCM. <p> A typical ff value is 0.001 [20]. * It requires vigilance threshold 2 <ref> [0; 1] </ref> as a relative number representing external expectations. In detail, controls neuron proliferation (see Equation (23)), such that coarser grouping of input patterns is obtained when the vigilance parameter is lowered.
Reference: [2] <author> Anonymous referee, </author> <booktitle> IEEE Trans. on Neural Networks, </booktitle> <year> 1997. </year>
Reference-contexts: This conceptual equivalence is employed as a unifying framework in the comparison of clustering algorithms. This approach has been considered "interesting" and "quite reasonable" by some researchers <ref> [2] </ref>. <p> the input, output, learning or neural levels" (see [3], where each input feature is expressed in terms of fuzzy membership values indicating a degree of belonging to each of the linguistic properties low, medium and high), our "claim of calling certain networks to be fuzzy/nonfuzzy seems improper and not acceptable" <ref> [2] </ref>. In both cases, comments pertaining to the proposed terminology "fuzzy clustering algorithm" do not affect the core of this work, which is centered on the comparison of learning mechanisms adopted by several clustering algorithms to model fuzzy (ambiguous) patterns. <p> Rather, SOM attempts to find topological structures in the input data and display them in one or two dimensions" [54], i.e., SOM can be employed in data visualization tasks because "SOM simply attempts to achieve a consistent spatial mapping of the training vectors to (usually) two dimensions" <ref> [2] </ref>.
Reference: [3] <author> S. Mitra and S. K. Pal, </author> <title> "Self-organizing neural network as a fuzzy classifier," </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> vol. 24, no. 3, </volume> <pages> pp. 385-399, </pages> <year> 1994. </year>
Reference-contexts: This approach has been considered "interesting" and "quite reasonable" by some researchers [2]. However, other authors believe that since "fuzziness can be incorporated at various levels to generate a fuzzy neural network, i.e., it can be at the input, output, learning or neural levels" (see <ref> [3] </ref>, where each input feature is expressed in terms of fuzzy membership values indicating a degree of belonging to each of the linguistic properties low, medium and high), our "claim of calling certain networks to be fuzzy/nonfuzzy seems improper and not acceptable" [2].
Reference: [4] <author> T. </author> <title> Kohonen,"The self-organizing map," </title> <booktitle> Proceedings of the IEEE vol. </booktitle> <volume> 78, no. 9, </volume> <pages> pp. 1464-1480, </pages> <year> 1990. </year>
Reference-contexts: The clustering models investigated are: i) on-line learning, static-sizing, static-linking Self-Organizing Map (SOM) <ref> [4] </ref>, [5]; ii) off-line learning, static-sizing, no-linking Fuzzy Learning Vector Quantization, FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, <p> With regard to the exploitation of distributed parameters, it can be observed that, on the one hand, most of the on-line clustering algorithms presented in the existing literature, e.g., SOM <ref> [4] </ref>, exploit a global (network-based) time counter, i.e., a time variable that is not specialized on a localized basis, rather than distributed (localized: neuron-based, synapse-based) time variables. <p> It is the presence of this feed-back interaction with the environment that characterizes all natural systems featuring cognitive capabilities [44]. Some artificial neural systems feature none of the biological properties listed above. For example, SOM <ref> [4] </ref> and the Hopfield network [52] are homogeneous systems; they feature no structured architecture and no supervision or reinforcement by, or feedback from, an external environment (also termed supervisor). In reinforcement learning, the neural system is allowed to react to each training case. <p> * Our own interpretation of a fuzzy clustering algorithm, intended as a clustering system exploiting soft competitive (i.e., cooperative and competitive) versus hard (crisp, purely) competitive adaptation of system parameters (see Section 7). 9 SOM Starting from Kohonen's on-line Vector Quantization model (VQ, an acronym used herein in line with <ref> [4] </ref>), which employs a WTA learning strategy, on-line SOM develops a soft competitive learning technique based on two constraints, derived from neurophysiological studies and providing an annealing schedule [9], [64], [65]. They consist of two empirical functions of time, which are user-defined and which obey two heuristic rules [4], [5], the <p> line with <ref> [4] </ref>), which employs a WTA learning strategy, on-line SOM develops a soft competitive learning technique based on two constraints, derived from neurophysiological studies and providing an annealing schedule [9], [64], [65]. They consist of two empirical functions of time, which are user-defined and which obey two heuristic rules [4], [5], the first of which requires learning rates to decrease monotonically with time according to a cooling scheme, i.e., as the number of input pattern presentations increases, all learning rates (winner as well as non-winner) must decrease towards zero (in line with the Robbins-Monro theorem, see Section 3). <p> In greater detail, PEs belonging to the update neighborhood centered on winner unit P E w1 (t) are affected by a lateral excitatory signal generated by the winner, which is shaped like a "bell curve" (bubble strategy), such that <ref> [4] </ref>: ff i (t) = *(t) e (kr i r w1 (t) k 2 =(t) 2 ) (11) where ff i (t) is the learning rate of processing unit P E i at time t, r i and r w1 (t) denote the spatial coordinates of the output unit P E
Reference: [5] <author> T. Kohonen, </author> <title> Self-organizing Maps. 2nd Edition, </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1997. </year>
Reference-contexts: The clustering models investigated are: i) on-line learning, static-sizing, static-linking Self-Organizing Map (SOM) [4], <ref> [5] </ref>; ii) off-line learning, static-sizing, no-linking Fuzzy Learning Vector Quantization, FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, <p> In parallel, the study of artificial neural nets as stand-alone systems should evolve to become the science of ecological nets (econets), where neural systems as well as their external environments are modeled [50]. For example, unlike Kohonen's networks <ref> [5] </ref>, an ART system employs a structured architecture to self-adjust the network dimension to problem-specific conditions. In particular, the ART orienting subsystem models the responses of the external environment to the learning activities of the attentional subsystem [19], [20], [21], [22]. <p> They consist of two empirical functions of time, which are user-defined and which obey two heuristic rules [4], <ref> [5] </ref>, the first of which requires learning rates to decrease monotonically with time according to a cooling scheme, i.e., as the number of input pattern presentations increases, all learning rates (winner as well as non-winner) must decrease towards zero (in line with the Robbins-Monro theorem, see Section 3). <p> Finally, it is important to point out that a batch and iterative SOM version has also been presented <ref> [5] </ref>, [34]. 9.1 Input parameters To simplify the discussion, our approach is to deal with a finite training data set fXg, consisting of n input patterns X k , k = 1; :::; n, where X k 2 R D , such that D is the dimensionality of the input space. <p> This cost function was introduced in a nonneural context to design an optimal vector quantizer codebook for encoding data for transmission along a noisy channel [17]. 9.3 Limitations Despite its many successes in practical applications, SOM contains some major deficiencies (many of which are acknowledged in <ref> [5] </ref>). 14 * Since SOM does not minimize any known objective function, termination is not based on optimizing any model of the process or its data [7]. * On-line SOM is order dependent (due to on-line learning), i.e., the final weight vectors are affected by the order of the input sequence <p> It can also be observed that Equation (15) belongs to the same class of weight adaptation rules employed in the batch form of the SOM algorithm and in the batch and statistically firm EM optimization of Gaussian centers in a mixture model <ref> [5] </ref>, [33], [34]. Equation (15) shows that when 17 m (e) = m is fixed, then FLVQ is equivalent to FCM [6]. <p> in a topologically correct way onto submaps of an output lattice pursuing dimensionality reduction. 12.5 Architectural features The main features of GNG are summarized in Table 2. 13 FOSART Created to overcome the GNG deficiencies listed in Section 12.3, FOSART is an on-line learning algorithm which combines properties of SOM <ref> [5] </ref>, GNG [27], FCM [6], and Fuzzy SART [24] algorithms. FOSART employs a relative fuzzy membership function, derived from the one employed by FCM and Fuzzy SART, to compute activation values, and a Gaussian Basis Function (GBF) to compute absolute (possibilistic) fuzzy membership values (see Section 3).
Reference: [6] <author> J. C. Bezdek and N. R. Pal, </author> <title> "Two soft relative of learning vector quantization," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 5, </volume> <pages> pp. 729-743, </pages> <year> 1995. </year>
Reference-contexts: The clustering models investigated are: i) on-line learning, static-sizing, static-linking Self-Organizing Map (SOM) [4], [5]; ii) off-line learning, static-sizing, no-linking Fuzzy Learning Vector Quantization, FLVQ <ref> [6] </ref> (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], [29], based <p> To summarize, on-line clustering algorithms found in the literature depend to different degrees on distributed parameters whose competitive adaptation is consistent with the development of structured systems (i.e., specialized subsystems). With regard to batch and iterative algorithms, such as Fuzzy cmeans (FCM) and FLVQ <ref> [6] </ref>, [7], global time rather than distributed time variables are employed, i.e., at a given processing time, plasticity is the same for every PE in the net. This is justified by considering that when fuzzy clustering mechanisms are employed, all PEs acquire the same input pattern simultaneously. <p> functions are traditionally applied to clustering algorithms in the intuitive belief that "vector quantizers based on both winner and non-winner information about the relationship of an input pattern to the prototypes will be better representatives of the overall structure of the input data than those based on local information alone" <ref> [6] </ref>. Thus, several clustering algorithms, such as the Fuzzy cmeans (FCM) and the Fuzzy Learning Vector Quantization (FLVQ) algorithms, combine local and global information in the computation of a relative fuzzy membership function [6], [7]. <p> better representatives of the overall structure of the input data than those based on local information alone" <ref> [6] </ref>. Thus, several clustering algorithms, such as the Fuzzy cmeans (FCM) and the Fuzzy Learning Vector Quantization (FLVQ) algorithms, combine local and global information in the computation of a relative fuzzy membership function [6], [7]. <p> More precisely, it can be stated that SOM can be employed in topology-preserving mapping iff the dimension of the input space is not larger than the dimension of the output lattice [49]. 9.5 Architectural features The main features of SOM are summarized in Table 2. 10 FLVQ FLVQ <ref> [6] </ref> (which was first called FKCN [7]) has quickly gained popularity as a fairly successful batch clustering algorithm. <p> FLVQ design aims to improve performance and usability of Kohonen's on-line VQ and SOM algorithms by combining the on-line Kohonen weight adaptation rule (12) with the 15 fuzzy set membership function proposed by the batch FCM algorithm <ref> [6] </ref>, [7]. This allows FLVQ to compute the learning rate and the "size" of the update neighborhood directly from the data. Thus, FLVQ employs a smaller set of user defined parameters than SOM. <p> In <ref> [6] </ref>, the heuristic constraint 7 &gt; m 0 &gt; m f &gt; 1:1 is recommended. * Parameter e max is defined as the maximum number of epochs. * A convergence error * is employed in the termination strategy. 16 10.2 Description of the algorithm * Descending FLVQ employs a weighting exponent <p> Equation (15) shows that when 17 m (e) = m is fixed, then FLVQ is equivalent to FCM <ref> [6] </ref>. It can be proved that by decreasing its weighting exponent m (e), descending FLVQ tends to reduce the width of the learning rate distribution by means of model transitions. <p> This analysis is also consistent with the heuristic choice 7 &gt; m 0 &gt; m f &gt; 1:1 recommended in <ref> [6] </ref>. On the other hand, learning rate values do not necessarily decrease with time, i.e., FLVQ does not behave consistently with the first Kohonen constraint, its update mechanism being roughly opposite to that of other clustering algorithms, like SOM. <p> Intuitively, this learning policy is not desirable because it does not provide prototypes with the large initial plasticity values required to pursue fast (rough) initial learning. * In <ref> [6] </ref>, it is clarified that FLVQ, like SOM, does not optimize any known objective function, and that it is expected to reach termination when the FCM objective function is approximately minimized [7]. <p> way onto submaps of an output lattice pursuing dimensionality reduction. 12.5 Architectural features The main features of GNG are summarized in Table 2. 13 FOSART Created to overcome the GNG deficiencies listed in Section 12.3, FOSART is an on-line learning algorithm which combines properties of SOM [5], GNG [27], FCM <ref> [6] </ref>, and Fuzzy SART [24] algorithms. FOSART employs a relative fuzzy membership function, derived from the one employed by FCM and Fuzzy SART, to compute activation values, and a Gaussian Basis Function (GBF) to compute absolute (possibilistic) fuzzy membership values (see Section 3). <p> For example, typical error rates for unsupervised categorization of the IRIS data set are 10-16 mistakes <ref> [6] </ref>.
Reference: [7] <author> E. C. Tsao, J. C. Bezdek and N. R. </author> <title> Pal.,"Fuzzy Kohonen clustering network," </title> <journal> Pattern Recognition, </journal> <volume> vol. 27, no. 5, </volume> <pages> pp. 757-764, </pages> <year> 1994. </year>
Reference-contexts: The clustering models investigated are: i) on-line learning, static-sizing, static-linking Self-Organizing Map (SOM) [4], [5]; ii) off-line learning, static-sizing, no-linking Fuzzy Learning Vector Quantization, FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN <ref> [7] </ref>); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], [29], based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model <p> Absolute and relative mem bership types are related by the following equation: R i;k = P c ; i = 1; :::; c; k = 1; :::; n: (1) Relative typicality values, R i;k , must satisfy the following three conditions <ref> [7] </ref>, [30]: (i) R i;k 2 [0,1], i = 1; :::; c, k = 1; :::; n; P c (iii) 0 &lt; k=1 R i;k &lt; n, i = 1; :::; c. <p> To summarize, on-line clustering algorithms found in the literature depend to different degrees on distributed parameters whose competitive adaptation is consistent with the development of structured systems (i.e., specialized subsystems). With regard to batch and iterative algorithms, such as Fuzzy cmeans (FCM) and FLVQ [6], <ref> [7] </ref>, global time rather than distributed time variables are employed, i.e., at a given processing time, plasticity is the same for every PE in the net. This is justified by considering that when fuzzy clustering mechanisms are employed, all PEs acquire the same input pattern simultaneously. <p> Thus, several clustering algorithms, such as the Fuzzy cmeans (FCM) and the Fuzzy Learning Vector Quantization (FLVQ) algorithms, combine local and global information in the computation of a relative fuzzy membership function [6], <ref> [7] </ref>. <p> a noisy channel [17]. 9.3 Limitations Despite its many successes in practical applications, SOM contains some major deficiencies (many of which are acknowledged in [5]). 14 * Since SOM does not minimize any known objective function, termination is not based on optimizing any model of the process or its data <ref> [7] </ref>. * On-line SOM is order dependent (due to on-line learning), i.e., the final weight vectors are affected by the order of the input sequence [7]. * Prototype parameter estimates may be severely affected by noise points and outliers. <p> 14 * Since SOM does not minimize any known objective function, termination is not based on optimizing any model of the process or its data <ref> [7] </ref>. * On-line SOM is order dependent (due to on-line learning), i.e., the final weight vectors are affected by the order of the input sequence [7]. * Prototype parameter estimates may be severely affected by noise points and outliers. <p> in the grid, while they are independent of the actual distance separating the input pattern from the cluster template. * The size of the output lattice, the step size and the size of the resonance neighborhood must be varied empirically from one data set to another to achieve useful results <ref> [7] </ref>. * Probability density function (pdf) estimation is not achieved [34]. <p> be stated that SOM can be employed in topology-preserving mapping iff the dimension of the input space is not larger than the dimension of the output lattice [49]. 9.5 Architectural features The main features of SOM are summarized in Table 2. 10 FLVQ FLVQ [6] (which was first called FKCN <ref> [7] </ref>) has quickly gained popularity as a fairly successful batch clustering algorithm. FLVQ design aims to improve performance and usability of Kohonen's on-line VQ and SOM algorithms by combining the on-line Kohonen weight adaptation rule (12) with the 15 fuzzy set membership function proposed by the batch FCM algorithm [6], [7]. <p> <ref> [7] </ref>) has quickly gained popularity as a fairly successful batch clustering algorithm. FLVQ design aims to improve performance and usability of Kohonen's on-line VQ and SOM algorithms by combining the on-line Kohonen weight adaptation rule (12) with the 15 fuzzy set membership function proposed by the batch FCM algorithm [6], [7]. This allows FLVQ to compute the learning rate and the "size" of the update neighborhood directly from the data. Thus, FLVQ employs a smaller set of user defined parameters than SOM. <p> does not provide prototypes with the large initial plasticity values required to pursue fast (rough) initial learning. * In [6], it is clarified that FLVQ, like SOM, does not optimize any known objective function, and that it is expected to reach termination when the FCM objective function is approximately minimized <ref> [7] </ref>. In [55], [56], [57], EFLVQ-F learning schemes are formally derived to minimize a given functional when m is constant. It is also shown that FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent. <p> recent advances in the field, the objective function minimized by FLVQ is still unknown, just as the one minimized by SOM is unknown [60]. 10.3 Limitations * Since FLVQ does not minimize any known objective function, termination is not based on optimizing any model of the process or its data <ref> [7] </ref>. * FLVQ is affected by the relative membership problem (high sensitivity to noise, i.e., low robustness [62]). 19 * It does not provide prototypes with large initial plasticity values required to pursue fast (rough) initial learning. * FLVQ features instability when its traditional termination criterion is employed, such that if <p> Nonetheless, since it applies no soft competitive learning strategy, Fuzzy ART cannot be termed fuzzy as defined in Section 7. 11.3 Limitations * Since Fuzzy ART does not minimize any known objective function, its termination is not based on optimizing any model of the process or its data <ref> [7] </ref>. * Owing to its hard competitive implementation, Fuzzy ART is more likely to be trapped in local minima and to generate dead units than soft competitive alternatives [9], [27], [62]. * Fuzzy ART is order dependent due to on-line learning and example-driven neuron generation. <p> units are decreased by a fraction fi; and ii) the termination test is checked (possibly involving parameter c max or the mean accumulated error). 12.3 Limitations * Since GNG does not minimize any known objective function, termination is not based on optimizing any model of the process or its data <ref> [7] </ref>. * Since its neurons feature no "cooling schedule" (i.e., the learning rate does not satisfy Kohonen's first learning constraint), GNG lacks stability because of excessive plasticity, i.e., the processing of a new data set can radically alter maps detected by the system during the previous processing stage (try simulations on <p> If PE-based local counter e i (t) e min , i = 1; :::; c, then stop. Otherwise, goto Step 1. 13.3 Limitations * Since FOSART does not minimize any known objective function, its termination is not based on optimizing any model of the process or its data <ref> [7] </ref>. * FOSART is order-dependent due to on-line learning and example-driven neuron generation. * It combines mini-batch learning techniques (for neuron generation and removal, and for synapse removal) with example-driven generation of synapses. * FOSART employs one up to three parameters defined by the human designer rather than by the user.
Reference: [8] <author> P. K. </author> <title> Simpson,"Fuzzy Min-Max neural network-Part 2: Clustering," </title> <journal> IEEE Trans. on Fuzzy Systems, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 32-45, </pages> <year> 1993. </year>
Reference-contexts: Thus, an ART system belongs to the class of ecological nets. 7 On fuzzy clustering algorithms A clustering algorithm performs unsupervised detection of statistical regularities in a random sequence of input patterns. Our attention is focused on fuzzification of clustering learning schemes. In the definition presented in <ref> [8] </ref>, it is stated that an artificial Neural Network (NN) model performs fuzzy clustering when it allows a pattern to belong to multiple categories to different degrees depending on the neurons' ability to recognize the input pattern. <p> In general, soft competitive learning decreases dependency on initialization and reduces the presence of dead units [27]. We observe that: 1. The fuzzy clustering definition provided in <ref> [8] </ref> is equivalent to the definition of the soft competitive adaptation rule traditionally employed in the field of data compression [9], i.e., a clustering algorithm is termed fuzzy clustering algorithm iff it employs a soft competitive (non-crisp) parameter adaptation strategy. 2. <p> As observed by Simpson <ref> [8] </ref>, to be correctly interpreted as fuzzy operations, these operations would have to be applied to fuzzy set membership values, rather than to the parameters (pattern and template vectors) of absolute fuzzy set membership functions. 3 To summarize, Fuzzy ART employs two absolute fuzzy membership functions, i.e., it employs fuzzy set-theoretic <p> [23]) or not functionally equivalent to Fuzzy ART (e.g., the Fuzzy SART model [24]), reduce the number of searches to one , whatever the input pattern may be. 3 It is curious to observe that the same criticism can be applied to the Fuzzy Min-Max clustering algorithm presented by Simpson <ref> [8] </ref> 23 * It requires input data preprocessing (e.g., normalization or complement coding; in the first case data vector-length information is lost, while in the second case the number of network connections doubles) to prevent category proliferation [19]. * It does not provide pdf estimation. * It cannot be employed in <p> This choice reduces the risk of dead unit formation and may reduce computation time with respect to random initialization. * In the tests provided involving the 2-D Simpson data set (24 patterns) <ref> [8] </ref>, the 2-D two-spirals data set (194 patterns, 2 classes) [73], a 3-D digitized human face (9743 patterns) [18], and the 4-D IRIS data set (150 patterns, 3 classes), FOSART performances are competitive with or better than those of other clustering models found in the literature (VQ, Fuzzy Min-Max, FCM, FLVQ, <p> In this case FOSART, scoring 11 mismatches, performs better than: i) the Fuzzy Min-Max neural network model, where the smallest number of misclassified patterns is 18 when the number of clusters is 3 <ref> [8] </ref>; ii) the off-line Fuzzy c-means algorithm affected by 15 misclassifications [74]; iii) the on-line Kohonen VQ algorithm affected by 17 misclassifications [74]; iv) the class of on-line FALVQ algorithms, affected by 16 misclassifications [54]; and v) the on-line GLVQ-F algorithms, affected by 16 misclassifications [59]. * FOSART is computationally efficient
Reference: [9] <author> T. M. Martinetz, S. G. Berkovich and K. J. Schulten," </author> <title> Neural-gas network for vector quantization and its application to time-series prediction," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 558-569, </pages> <year> 1993. </year>
Reference-contexts: [62]). 2.2 Fuzzy memberships and mixture probabilities To investigate the relationship between (objective) probability density functions and (useful) fuzzy membership functions, note that absolute membership function (3) relates probabilistic membership (1) to Gaussian mixture models, which are widely employed in the framework of optimization problems featuring a firm statistical foundation <ref> [9] </ref>, [31], [32], [43]. <p> Since the traditional k-means clustering algorithms feature a cost function (discretization error) characterized by many local minima, data compression techniques modify c-means algorithms by replacing their Winner-Takes-All strategy (WTA), also termed crisp or hard competitive, with a "soft-max" adaptation rule <ref> [9] </ref>, hereafter referred to as soft competitive learning. 2 2 Note that terms soft-max and soft competitive learning rule adopted in this paper are not to be confused 10 A WTA parameter adaptation strategy is purely competitive and allows no cooperative (soft competitive) learning. <p> Unlike WTA learning, a soft competitive learning scheme is defined as a learning strategy that not only adjusts the winning cluster but also affects all cluster centers depending on their proximity to the input pattern <ref> [9] </ref>. In general, soft competitive learning decreases dependency on initialization and reduces the presence of dead units [27]. We observe that: 1. The fuzzy clustering definition provided in [8] is equivalent to the definition of the soft competitive adaptation rule traditionally employed in the field of data compression [9], i.e., a <p> input pattern <ref> [9] </ref>. In general, soft competitive learning decreases dependency on initialization and reduces the presence of dead units [27]. We observe that: 1. The fuzzy clustering definition provided in [8] is equivalent to the definition of the soft competitive adaptation rule traditionally employed in the field of data compression [9], i.e., a clustering algorithm is termed fuzzy clustering algorithm iff it employs a soft competitive (non-crisp) parameter adaptation strategy. 2. <p> system parameters (see Section 7). 9 SOM Starting from Kohonen's on-line Vector Quantization model (VQ, an acronym used herein in line with [4]), which employs a WTA learning strategy, on-line SOM develops a soft competitive learning technique based on two constraints, derived from neurophysiological studies and providing an annealing schedule <ref> [9] </ref>, [64], [65]. <p> Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in <ref> [9] </ref>, [14], [15], [32], [35], [43]. From a general perspective, it is to be remembered that compared to hard competitive learning soft competitive learning not only decreases dependency on initialization, but also reduces the presence of dead units (see Section 7). <p> Note that in Equation (11), distance kr i r w1 (t) k is equivalent to a neighborhood-ranking of the processing units within the external lattice, i.e., SOM differs from: a) the Neural Gas (NG) algorithm, which employs the neighborhood-ranking of the reference vectors within the input space <ref> [9] </ref>; and b) other clustering algorithms (e.g., "maximum-entropy" clustering [9], FLVQ and FOSART, see Sections 10 and 13) where the adaptation step is a function of the absolute distance of the reference vectors from the current input pattern within the input space. <p> kr i r w1 (t) k is equivalent to a neighborhood-ranking of the processing units within the external lattice, i.e., SOM differs from: a) the Neural Gas (NG) algorithm, which employs the neighborhood-ranking of the reference vectors within the input space <ref> [9] </ref>; and b) other clustering algorithms (e.g., "maximum-entropy" clustering [9], FLVQ and FOSART, see Sections 10 and 13) where the adaptation step is a function of the absolute distance of the reference vectors from the current input pattern within the input space. <p> Analysis of Equation (11) reveals that: i) when (t) = 0, then nodes of SOM become purely competitive and SOM becomes equivalent to a hard (crisp) c-means clustering procedure <ref> [9] </ref>; and ii) when *(t) = 0, then SOM reaches termination (no template vector is attracted by any input pattern). * SOM employs the Kohonen weight adaptation rule to position optimally cluster prototypes that belong to the resonance domain (update neighborhood) centered on the winner PE, such that: T i (t <p> Nonetheless, as long as Equation (11) features (t) &gt; 0 (i.e., as long as the SOM soft learning strategy is not equivalent to a WTA strategy), one cannot specify a cost function that is minimized by Equation (12), i.e., there exists no cost function yielding Equation (12) as its gradient <ref> [9] </ref>, [34], [65]. SOM instead features a set of potential functions, one for each node, to be independently minimized following a stochastic (on-line) gradient descent [65]. <p> (adjacency) structure of the preset graph G has to match the topological structure of the unknown manifold X [49]. 9.4 Advantages * Owing to its soft competitive implementation, SOM is expected to be less likely trapped in local minima and less likely to generate dead units than hard competitive alternatives <ref> [9] </ref>, [27], [62]. * Batch SOM is order independent, i.e., the final weight vectors are not affected by the order of the input sequence. * SOM can be employed as a vector requantization system. For some authors, "SOM was not intended for pattern classification. <p> 10.1) [60]. * It does not provide pdf estimation. * It cannot be employed in topology-preserving mapping. 10.4 Advantages * Owing to its soft competitive implementation, FLVQ is expected to be less likely trapped in local minima and less likely to generate dead units than hard competitive alternatives (e.g., FCM) <ref> [9] </ref>, [27], [62]. * In FLVQ, due to batch learning, the final weight vectors are not affected by the order of the input sequence when its traditional termination criterion is removed. * With respect to SOM, FLVQ requires a smaller set of input parameters (its learning rate and the size of <p> does not minimize any known objective function, its termination is not based on optimizing any model of the process or its data [7]. * Owing to its hard competitive implementation, Fuzzy ART is more likely to be trapped in local minima and to generate dead units than soft competitive alternatives <ref> [9] </ref>, [27], [62]. * Fuzzy ART is order dependent due to on-line learning and example-driven neuron generation. <p> is reached. * It combines mini-batch learning techniques (for neuron generation and removal, and for synapse removal) with example-driven generation of synapses. 12.4 Advantages * Owing to its soft competitive implementation, GNG is less likely to be trapped in local minima and to generate dead units than hard competitive alternatives <ref> [9] </ref>, [27], [62]. <p> Inspired by the Neural-Gas algorithm <ref> [9] </ref>, FOSART computes * i (t) as * i (t) = * i (t)(e i (t); R i (t)) = * ini (* fin =* ini ) e i (t)=e min ; (39) where e i (t) is the PE-based epoch counter at presentation time t, e min is the user-defined <p> This dependency between the learning step size and the set of distances between the input pattern and template vectors relates FOSART to the "maximum-entropy" clustering algorithm <ref> [9] </ref>. In Equation (37), term h i (t) reduces the overlap between receptive fields according to the following expression, inspired by the Neural-Gas algorithm [9] h i (t) = h i (t)[r i (t); fl i (e i (t))] = e r i (t)=fl i (e i (t)) ; (41) where <p> This dependency between the learning step size and the set of distances between the input pattern and template vectors relates FOSART to the "maximum-entropy" clustering algorithm <ref> [9] </ref>. In Equation (37), term h i (t) reduces the overlap between receptive fields according to the following expression, inspired by the Neural-Gas algorithm [9] h i (t) = h i (t)[r i (t); fl i (e i (t))] = e r i (t)=fl i (e i (t)) ; (41) where r i (t) is the neighborhood ranking of node P E i and fl i (e i (t)) is a spread value computed as <p> Widely employed settings for these parameters are fl ini = 5, and fl fin = 0:01 <ref> [9] </ref>, [64]. Thus, learning coefficient h i (t) is monotonically decreasing with neighborhood ranking r i (t) and PE-based epoch counter e i (t) if i 6= w1 (t). Step 4 (b). Non-resonance condition: new processing element allocation. <p> that are in progress, FOSART features Mean Square Error values larger than those obtained with SOM [72]. 13.4 Advantages * Owing to its soft competitive implementation, FOSART is expected to be less prone to being trapped in local minima and less likely to generate dead units than hard competitive alternatives <ref> [9] </ref>, [27], [62].
Reference: [10] <author> H. Ritter, T. M. Martinetz, and K. J. Schulten, </author> <title> Neural computation and self-organizing maps. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Important properties of this cooling schedule have been analyzed in <ref> [10] </ref>, [27], [33], [35], [58], [59].
Reference: [11] <author> M. Porat and Y. Zeevi, </author> <title> "The generalized Gabor scheme of image representation in biological and machine vision," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. 452-467, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: This model transition is equivalent to stating that the initial overlap (oversampling <ref> [11] </ref>) between nodes' receptive fields must decrease monotonically with time until it is reduced to zero, as hard competitive learning renders receptive fields equivalent to Voronoi polyhedra [27].
Reference: [12] <author> K. Rose, F. Guerewitz, and G. Fox, </author> <title> "Statistical mechanics and phase transitions in clustering," </title> <journal> Physical Rev. Letters, </journal> <volume> vol. 65, no. 8, </volume> <pages> pp. 945-948, </pages> <year> 1990. </year>
Reference-contexts: Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 <ref> [12] </ref>, deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in [9], [14], [15], [32], [35], [43].
Reference: [13] <author> K. Rose, F. Guerewitz, and G. Fox, </author> <title> "A deterministic approach to clustering," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 11, no. 11, </volume> <pages> pp. 589-594, </pages> <year> 1990. </year>
Reference-contexts: Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing <ref> [13] </ref>, and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in [9], [14], [15], [32], [35], [43].
Reference: [14] <author> S. P. Luttrell, </author> <title> "Derivation of a class of training algorithms," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 1, </volume> <pages> pp. 229-232, </pages> <year> 1990. </year>
Reference-contexts: Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in [9], <ref> [14] </ref>, [15], [32], [35], [43]. From a general perspective, it is to be remembered that compared to hard competitive learning soft competitive learning not only decreases dependency on initialization, but also reduces the presence of dead units (see Section 7).
Reference: [15] <author> S. P. Luttrell, </author> <title> "A Bayesian analysis of self-organizing maps", </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 767-794, </pages> <year> 1994. </year>
Reference-contexts: Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in [9], [14], <ref> [15] </ref>, [32], [35], [43]. From a general perspective, it is to be remembered that compared to hard competitive learning soft competitive learning not only decreases dependency on initialization, but also reduces the presence of dead units (see Section 7). <p> SOM instead features a set of potential functions, one for each node, to be independently minimized following a stochastic (on-line) gradient descent [65]. In <ref> [15] </ref>, a cost function that leads to an update strategy that is similar to, but not precisely the same as, Equations (11) and (12) is discussed.
Reference: [16] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for clustering data. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1988. </year> <month> 36 </month>
Reference-contexts: It is important to observe that Equation (12) is related to on-line (McQueen's) k-means <ref> [16] </ref>, [33], [43], whose batch (Lloyd's or Forgy's) version [16] is a special case of the EM optimization of a Gaussian mixture [33]. <p> It is important to observe that Equation (12) is related to on-line (McQueen's) k-means <ref> [16] </ref>, [33], [43], whose batch (Lloyd's or Forgy's) version [16] is a special case of the EM optimization of a Gaussian mixture [33]. <p> Equation (21) shows that if m (e) ! 1 + , then both FCM and FLVQ become similar but not equivalent to a batch (Forgy's) c-Means algorithm <ref> [16] </ref>. Forgy's 18 c-Means features a singularity condition when its hard competitive prototype updating, P X k (e)2fX i (e)g X k (e)=n i , deals with a dead unit (n i = 0). In this case, the prototype update cannot be calculated.
Reference: [17] <author> H. Kumazava, M. Kasahara, and T. Namekawa, </author> <title> "A construction of vector quantisers for noisy channels," Elect. </title> <journal> Eng. Jpn., </journal> <volume> vol. 67B, </volume> <pages> pp. 39-47, </pages> <year> 1984. </year>
Reference-contexts: This cost function was introduced in a nonneural context to design an optimal vector quantizer codebook for encoding data for transmission along a noisy channel <ref> [17] </ref>. 9.3 Limitations Despite its many successes in practical applications, SOM contains some major deficiencies (many of which are acknowledged in [5]). 14 * Since SOM does not minimize any known objective function, termination is not based on optimizing any model of the process or its data [7]. * On-line SOM
Reference: [18] <author> M. Fontana, N. A., Borghese, and S. Ferrari, </author> <title> "Image reconstruction using improved Neural-Gas," </title> <booktitle> in Proc. Italian Workshop on Neural Networks '95, </booktitle> <editor> M. Marinaro and R. Tagliaferri Eds. </editor> <publisher> Singapore: World Scientific, </publisher> <pages> pp. 260-265, </pages> <year> 1995. </year>
Reference-contexts: This choice reduces the risk of dead unit formation and may reduce computation time with respect to random initialization. * In the tests provided involving the 2-D Simpson data set (24 patterns) [8], the 2-D two-spirals data set (194 patterns, 2 classes) [73], a 3-D digitized human face (9743 patterns) <ref> [18] </ref>, and the 4-D IRIS data set (150 patterns, 3 classes), FOSART performances are competitive with or better than those of other clustering models found in the literature (VQ, Fuzzy Min-Max, FCM, FLVQ, FALVQ, GLVQ-F, ING).
Reference: [19] <author> G. Carpenter, S. Grossberg and D.B. Rosen, </author> <title> "Fuzzy ART: fast stable learning and categorization of analog patterns by an adaptive resonance system," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 4, </volume> <pages> pp. 759-771, </pages> <year> 1991. </year>
Reference-contexts: The clustering models investigated are: i) on-line learning, static-sizing, static-linking Self-Organizing Map (SOM) [4], [5]; ii) off-line learning, static-sizing, no-linking Fuzzy Learning Vector Quantization, FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) <ref> [19] </ref>, [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], [29], based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model [24], [25]. 1 2 Fuzzy membership and probability density functions This section <p> For example, unlike Kohonen's networks [5], an ART system employs a structured architecture to self-adjust the network dimension to problem-specific conditions. In particular, the ART orienting subsystem models the responses of the external environment to the learning activities of the attentional subsystem <ref> [19] </ref>, [20], [21], [22]. Thus, an ART system belongs to the class of ecological nets. 7 On fuzzy clustering algorithms A clustering algorithm performs unsupervised detection of statistical regularities in a random sequence of input patterns. Our attention is focused on fuzzification of clustering learning schemes. <p> ART 2, designed to detect regularities in analog random sequences, employs a computationally expensive architecture which presents difficulties in parameter selection [22]. To overcome these difficulties, the Fuzzy ART system was developed as a generalization of ART 1 <ref> [19] </ref>, [20]. This means, however, that ART 1-based structural problems may also affect Fuzzy ART. The structured organization of ART systems is made up of two subsystems, termed attentional and orienting subsystem. <p> Fuzzy ART requires a preprocessing stage where either input pattern normalization or complement coding is used to prevent category proliferation. Input data normalization loses vector-length information. Complement coding normalizes input vectors while preserving their amplitude information, but it doubles the number of network connections <ref> [19] </ref>, [20]. <p> Otherwise, the mismatch reset condition and search process are activated (see above). * Fuzzy ART employs a WTA strategy. The weight transformation law applied to the 22 winner template is <ref> [19] </ref>: T w1 (t) (t + 1) = (1 fi) T w1 (t) (t) + fi (X k (t) T w1 (t) (t)): (24) Note that unlike Kohonen's clustering networks (see Section 9), Fuzzy ART employs no cooling schedule because the learning rate is constant in time. * When no existing <p> Nonetheless, since the length of every template can only decrease with learning, templates cannot "cycle", i.e., Fuzzy ART does have a sort of stability [24]. * Fuzzy ART is time-consuming with respect to functionally equivalent implementations [24]. In its traditional implementation consisting of c nodes (see <ref> [19] </ref>), for each input pattern the Fuzzy ART search process requires a minimum of one up to c iterations to detect the winner unit that features the largest activation value among the units capable of satisfying the vigilance test. <p> criticism can be applied to the Fuzzy Min-Max clustering algorithm presented by Simpson [8] 23 * It requires input data preprocessing (e.g., normalization or complement coding; in the first case data vector-length information is lost, while in the second case the number of network connections doubles) to prevent category proliferation <ref> [19] </ref>. * It does not provide pdf estimation. * It cannot be employed in topology preserving mapping. 11.4 Advantages * Feed-back interaction between attentional and orienting subsystems allows Fuzzy ART to self-adjust its size depending on the complexity of the clustering task. * Distinct sample vectors are employed to initialize reference <p> In line with Fuzzy ART, FOSART applies an ART-based, example-driven vigilance test to control neuron generation <ref> [19] </ref>, [24]. Similarly to GNG, FOSART employs a new version of CHR, termed Constrained Competitive Hebbian Rule (CCHR) to generate synaptic links on an example-driven basis [49]. In line with GNG, FOSART removes synaptic links as well as neurons according to a mini-batch parameter adaptation scheme.
Reference: [20] <author> G. Carpenter, S. Grossberg, N. Maukuzon, J. Reynolds and D. B. Rosen, </author> <title> "Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 698-713, </pages> <year> 1992. </year>
Reference-contexts: The clustering models investigated are: i) on-line learning, static-sizing, static-linking Self-Organizing Map (SOM) [4], [5]; ii) off-line learning, static-sizing, no-linking Fuzzy Learning Vector Quantization, FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], <ref> [20] </ref>; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], [29], based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model [24], [25]. 1 2 Fuzzy membership and probability density functions This section proposes <p> For example, unlike Kohonen's networks [5], an ART system employs a structured architecture to self-adjust the network dimension to problem-specific conditions. In particular, the ART orienting subsystem models the responses of the external environment to the learning activities of the attentional subsystem [19], <ref> [20] </ref>, [21], [22]. Thus, an ART system belongs to the class of ecological nets. 7 On fuzzy clustering algorithms A clustering algorithm performs unsupervised detection of statistical regularities in a random sequence of input patterns. Our attention is focused on fuzzification of clustering learning schemes. <p> ART 2, designed to detect regularities in analog random sequences, employs a computationally expensive architecture which presents difficulties in parameter selection [22]. To overcome these difficulties, the Fuzzy ART system was developed as a generalization of ART 1 [19], <ref> [20] </ref>. This means, however, that ART 1-based structural problems may also affect Fuzzy ART. The structured organization of ART systems is made up of two subsystems, termed attentional and orienting subsystem. <p> Fuzzy ART requires a preprocessing stage where either input pattern normalization or complement coding is used to prevent category proliferation. Input data normalization loses vector-length information. Complement coding normalizes input vectors while preserving their amplitude information, but it doubles the number of network connections [19], <ref> [20] </ref>. <p> The input data set is repeatedly presented to the network until a termination criterion is satisfied. * Fuzzy ART employs parameter ff &gt; 0 (e.g., ff 2 [0:001; 1), <ref> [20] </ref>) to break ties, i.e., to bias the function in favor of the longer of two template vectors (see Equation (22)). A typical ff value is 0.001 [20]. * It requires vigilance threshold 2 [0; 1] as a relative number representing external expectations. <p> presented to the network until a termination criterion is satisfied. * Fuzzy ART employs parameter ff &gt; 0 (e.g., ff 2 [0:001; 1), <ref> [20] </ref>) to break ties, i.e., to bias the function in favor of the longer of two template vectors (see Equation (22)). A typical ff value is 0.001 [20]. * It requires vigilance threshold 2 [0; 1] as a relative number representing external expectations. In detail, controls neuron proliferation (see Equation (23)), such that coarser grouping of input patterns is obtained when the vigilance parameter is lowered.
Reference: [21] <author> G. A. Carpenter and S. Grossberg, </author> <title> "A massively parallel architecture for a self-organizing neural pattern recognition machine," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 37, </volume> <pages> pp. 54-115, </pages> <year> 1987. </year>
Reference-contexts: For example, unlike Kohonen's networks [5], an ART system employs a structured architecture to self-adjust the network dimension to problem-specific conditions. In particular, the ART orienting subsystem models the responses of the external environment to the learning activities of the attentional subsystem [19], [20], <ref> [21] </ref>, [22]. Thus, an ART system belongs to the class of ecological nets. 7 On fuzzy clustering algorithms A clustering algorithm performs unsupervised detection of statistical regularities in a random sequence of input patterns. Our attention is focused on fuzzification of clustering learning schemes. <p> ART 1 categorizes binary patterns but features sensitivity to the order of presentation of the random sequence <ref> [21] </ref>. This finding led to the development of the Improved ART 1 system (IART 1), which is less dependent than ART 1 on the order of presentation of the input sequence [69].
Reference: [22] <author> G. A. Carpenter and S. Grossberg, "ART2: </author> <title> Self-organization of stable category recognition codes for analog input patterns," </title> <journal> Applied Optics, </journal> <volume> vol. 26, no. 23, </volume> <pages> pp. 4919-4930, </pages> <year> 1987. </year>
Reference-contexts: For example, unlike Kohonen's networks [5], an ART system employs a structured architecture to self-adjust the network dimension to problem-specific conditions. In particular, the ART orienting subsystem models the responses of the external environment to the learning activities of the attentional subsystem [19], [20], [21], <ref> [22] </ref>. Thus, an ART system belongs to the class of ecological nets. 7 On fuzzy clustering algorithms A clustering algorithm performs unsupervised detection of statistical regularities in a random sequence of input patterns. Our attention is focused on fuzzification of clustering learning schemes. <p> The Adaptive Hamming Net (AHN), which is functionally equivalent to ART 1, optimizes ART 1 both in terms of computation time and storage requirement [23]. ART 2, designed to detect regularities in analog random sequences, employs a computationally expensive architecture which presents difficulties in parameter selection <ref> [22] </ref>. To overcome these difficulties, the Fuzzy ART system was developed as a generalization of ART 1 [19], [20]. This means, however, that ART 1-based structural problems may also affect Fuzzy ART. The structured organization of ART systems is made up of two subsystems, termed attentional and orienting subsystem.
Reference: [23] <author> C. Hung and S. Lin, </author> <title> "Adaptive Hamming Net: a fast-learning ART 1 model without searching," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 4, </volume> <pages> pp. 605-618, </pages> <year> 1995. </year>
Reference-contexts: The Adaptive Hamming Net (AHN), which is functionally equivalent to ART 1, optimizes ART 1 both in terms of computation time and storage requirement <ref> [23] </ref>. ART 2, designed to detect regularities in analog random sequences, employs a computationally expensive architecture which presents difficulties in parameter selection [22]. To overcome these difficulties, the Fuzzy ART system was developed as a generalization of ART 1 [19], [20]. <p> For example, AHN is a feed-forward network functionally equivalent to ART 1 <ref> [23] </ref>. This simplification yields, as a major consequence, a change in the meaning of the term "resonance" as traditionally applied to ART 1-based systems. <p> Several ART-1 based models, either functionally equivalent to Fuzzy ART (e.g, AHN <ref> [23] </ref>) or not functionally equivalent to Fuzzy ART (e.g., the Fuzzy SART model [24]), reduce the number of searches to one , whatever the input pattern may be. 3 It is curious to observe that the same criticism can be applied to the Fuzzy Min-Max clustering algorithm presented by Simpson [8]
Reference: [24] <author> A. Baraldi and E. Alpaydn, </author> <title> "Simplified ART: A new class of ART algorithms," </title> <booktitle> International Computer Science Institute, </booktitle> <address> Berkeley, CA, TR-98-004. </address>
Reference-contexts: iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], [29], based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model <ref> [24] </ref>, [25]. 1 2 Fuzzy membership and probability density functions This section proposes a brief review of probabilistic and possibilistic fuzzy membership concepts, to be compared with Bayes' view of posterior probability and likelihood. 2.1 Absolute and relative fuzzy memberships Let X k be instance k of the input manifold X, <p> On the other hand, in recent years, several on-line Kohonen-based network models exploiting distributed time variables have been presented <ref> [24] </ref>, [26], [27], [28]. For example, in GNG and FOSART a connection-based time variable is equal to the number of times one synapse is selected at adaptation steps. In Fuzzy SART and FOSART a neuron-based time variable is equal to the number of epochs that PE has survived. <p> Although in its original form the ART 1 attentional subsystem employs bottom-up (feed-forward) and top-down (feed-backward) connections, it is easy to prove that this module is mathematically equivalent to an attentional subsystem where feed-forward connections are adopted exclusively <ref> [24] </ref>. For example, AHN is a feed-forward network functionally equivalent to ART 1 [23]. This simplification yields, as a major consequence, a change in the meaning of the term "resonance" as traditionally applied to ART 1-based systems. <p> It can be proved that ART 1-based systems are all affected by the same design inconsistency: they all employ an inherently asymmetrical architecture to perform an inherently symmetrical task the assessment of an interpattern degree of match <ref> [24] </ref>. 11.1 Input parameters To simplify the discussion, our approach is to deal with a finite training data set fXg, consisting of n input patterns X k , k = 1; :::; n, where X k 2 R D , such that D is the dimensionality of the input space. <p> Experimental evidence [69], as well as theoretical analysis <ref> [24] </ref>, reveal that Fuzzy ART sensitivity to the order of presentation of the input sequence is also due to some inconsistencies detected in the system's implementation [24], [69]. * Fuzzy ART may be severely affected by noise points and outliers, i.e., it may fit the noise and not just the data. <p> Experimental evidence [69], as well as theoretical analysis <ref> [24] </ref>, reveal that Fuzzy ART sensitivity to the order of presentation of the input sequence is also due to some inconsistencies detected in the system's implementation [24], [69]. * Fuzzy ART may be severely affected by noise points and outliers, i.e., it may fit the noise and not just the data. <p> Nonetheless, since the length of every template can only decrease with learning, templates cannot "cycle", i.e., Fuzzy ART does have a sort of stability <ref> [24] </ref>. * Fuzzy ART is time-consuming with respect to functionally equivalent implementations [24]. <p> Nonetheless, since the length of every template can only decrease with learning, templates cannot "cycle", i.e., Fuzzy ART does have a sort of stability <ref> [24] </ref>. * Fuzzy ART is time-consuming with respect to functionally equivalent implementations [24]. In its traditional implementation consisting of c nodes (see [19]), for each input pattern the Fuzzy ART search process requires a minimum of one up to c iterations to detect the winner unit that features the largest activation value among the units capable of satisfying the vigilance test. <p> Several ART-1 based models, either functionally equivalent to Fuzzy ART (e.g, AHN [23]) or not functionally equivalent to Fuzzy ART (e.g., the Fuzzy SART model <ref> [24] </ref>), reduce the number of searches to one , whatever the input pattern may be. 3 It is curious to observe that the same criticism can be applied to the Fuzzy Min-Max clustering algorithm presented by Simpson [8] 23 * It requires input data preprocessing (e.g., normalization or complement coding; in <p> an output lattice pursuing dimensionality reduction. 12.5 Architectural features The main features of GNG are summarized in Table 2. 13 FOSART Created to overcome the GNG deficiencies listed in Section 12.3, FOSART is an on-line learning algorithm which combines properties of SOM [5], GNG [27], FCM [6], and Fuzzy SART <ref> [24] </ref> algorithms. FOSART employs a relative fuzzy membership function, derived from the one employed by FCM and Fuzzy SART, to compute activation values, and a Gaussian Basis Function (GBF) to compute absolute (possibilistic) fuzzy membership values (see Section 3). <p> In line with Fuzzy ART, FOSART applies an ART-based, example-driven vigilance test to control neuron generation [19], <ref> [24] </ref>. Similarly to GNG, FOSART employs a new version of CHR, termed Constrained Competitive Hebbian Rule (CCHR) to generate synaptic links on an example-driven basis [49]. In line with GNG, FOSART removes synaptic links as well as neurons according to a mini-batch parameter adaptation scheme.
Reference: [25] <author> A. Baraldi and F. Parmiggiani, </author> <title> "Fuzzy combination of the Kohonen and ART neural network models to detect statistical regularities in a random sequence of multi-valued input patterns," </title> <booktitle> in Proc. Int. Conf. on Neural Networks '97, Houston, TX, </booktitle> <volume> vol. 1, </volume> <pages> pp. 281-286, </pages> <year> 1997. </year>
Reference-contexts: on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], [29], based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model [24], <ref> [25] </ref>. 1 2 Fuzzy membership and probability density functions This section proposes a brief review of probabilistic and possibilistic fuzzy membership concepts, to be compared with Bayes' view of posterior probability and likelihood. 2.1 Absolute and relative fuzzy memberships Let X k be instance k of the input manifold X, where <p> These include the following: A i;k = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 1 i;k = i Gaussian i;k = e i;k i 2 (0; 1] (Gaussian mixtures; [66], [67]); (3) 1 i;k ) p i 2 (0; 1) [58]; (4) (1Gaussian i;k ) 2 2 (1; 1) <ref> [25] </ref>; (5) where d i;k = d (X k ; T i ) is assumed to be the Euclidean distance between input pattern X k and prototype (receptive field center) T i of the i-th category.
Reference: [26] <author> B. Fritzke, </author> <title> "A growing neural gas network learns topologies," </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. S. Touretzky, and T. K. Leen, Eds. </editor> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <pages> pp. 625-632, </pages> <year> 1995. </year>
Reference-contexts: static-sizing, static-linking Self-Organizing Map (SOM) [4], [5]; ii) off-line learning, static-sizing, no-linking Fuzzy Learning Vector Quantization, FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) <ref> [26] </ref>, [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], [29], based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model [24], [25]. 1 2 Fuzzy membership and probability density functions This section proposes a brief review of probabilistic and possibilistic fuzzy membership concepts, <p> On the other hand, in recent years, several on-line Kohonen-based network models exploiting distributed time variables have been presented [24], <ref> [26] </ref>, [27], [28]. For example, in GNG and FOSART a connection-based time variable is equal to the number of times one synapse is selected at adaptation steps. In Fuzzy SART and FOSART a neuron-based time variable is equal to the number of epochs that PE has survived. <p> Note that only few clustering networks employ intra-layer connections explicitly, i.e., by means of specific data structures and parameter adaptation strategies <ref> [26] </ref>, [27], [28], [29]. with the so-called soft-max function or normalized exponential employed in mixture models [33], and mixture-of-experts [38]. 11 8 Dictionary entries in the comparison of clustering algo- rithms From the general discussion developed in Sections 2 to 7 we derive the following set of basis features for use
Reference: [27] <author> B. Fritzke, </author> <title> "Some competitive learning methods", Draft document, </title> <address> http : ==www: neuroinf ormatik:ruhrunibochum:de=ini=V DM= research=gsn=DemoGN G, </address> <year> 1998. </year>
Reference-contexts: static-linking Self-Organizing Map (SOM) [4], [5]; ii) off-line learning, static-sizing, no-linking Fuzzy Learning Vector Quantization, FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], <ref> [27] </ref>; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], [29], based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model [24], [25]. 1 2 Fuzzy membership and probability density functions This section proposes a brief review of probabilistic and possibilistic fuzzy membership concepts, to <p> For example, in an on-line update procedure, if the learning rate remains fixed, then the algorithm converges only in a stochastic sense [39], i.e., model parameters drift from their initial positions to quasi-stationary positions where they start to wander around in dynamic equilibrium <ref> [27] </ref>. When the learning rate decreases monotonically under Robbins-Monro conditions (e.g., ff (t) = 1=t, see [33], p. 96, and [27]), on-line learning algorithms can be shown to converge to a point in the parameter space [39]. As a brief review of batch update and on-line update techniques, refer to [27]. <p> then the algorithm converges only in a stochastic sense [39], i.e., model parameters drift from their initial positions to quasi-stationary positions where they start to wander around in dynamic equilibrium <ref> [27] </ref>. When the learning rate decreases monotonically under Robbins-Monro conditions (e.g., ff (t) = 1=t, see [33], p. 96, and [27]), on-line learning algorithms can be shown to converge to a point in the parameter space [39]. As a brief review of batch update and on-line update techniques, refer to [27]. 4 Beyond error gradient descent: advanced techniques for learning from data In recent years the neural network community has made <p> <ref> [27] </ref>. When the learning rate decreases monotonically under Robbins-Monro conditions (e.g., ff (t) = 1=t, see [33], p. 96, and [27]), on-line learning algorithms can be shown to converge to a point in the parameter space [39]. As a brief review of batch update and on-line update techniques, refer to [27]. 4 Beyond error gradient descent: advanced techniques for learning from data In recent years the neural network community has made a considerable effort in the search for learning techniques that are more effective in dealing with local minima and generalization to new examples than the traditional approaches based on simple <p> Pruning algorithms begin training a network expected to be large with respect to the problem at hand, and then continue by pruning nodes that do not affect the learning process significantly [40], [42]. Vice versa, growing networks start from small networks that grow gradually until convergence is reached <ref> [27] </ref>, [40]. As a result, the complexity of the network is expected to be tuned to the problem at hand, i.e., generalization capability is expected to increase. 4.3 Modular architectures: prior structures and experience-based fine tuning Self-organizing neurological systems consist of highly structured, hierarchical architectures provided with feed-back mechanisms [45]. <p> On the other hand, in recent years, several on-line Kohonen-based network models exploiting distributed time variables have been presented [24], [26], <ref> [27] </ref>, [28]. For example, in GNG and FOSART a connection-based time variable is equal to the number of times one synapse is selected at adaptation steps. In Fuzzy SART and FOSART a neuron-based time variable is equal to the number of epochs that PE has survived. <p> TPM is defined as a mapping S from X to G such that S , together with inverse mapping 1 S from G to X, are neighborhood (adjacency) preserving [49]. 9 Note that exploitation of CHR in FSONNs allows generation of networks consisting of mutually disjointed (specialized and independent) maps <ref> [27] </ref>, [49]. <p> It is sensitive to initialization of templates, i.e., different ini-tializations may lead to very different minimization results. In fact, WTA adaptations may not be able to get the system out of a poor local minimum when this lies in the proximity of the status where the system was started <ref> [27] </ref>. Unlike WTA learning, a soft competitive learning scheme is defined as a learning strategy that not only adjusts the winning cluster but also affects all cluster centers depending on their proximity to the input pattern [9]. <p> In general, soft competitive learning decreases dependency on initialization and reduces the presence of dead units <ref> [27] </ref>. We observe that: 1. <p> Note that only few clustering networks employ intra-layer connections explicitly, i.e., by means of specific data structures and parameter adaptation strategies [26], <ref> [27] </ref>, [28], [29]. with the so-called soft-max function or normalized exponential employed in mixture models [33], and mixture-of-experts [38]. 11 8 Dictionary entries in the comparison of clustering algo- rithms From the general discussion developed in Sections 2 to 7 we derive the following set of basis features for use as <p> Important properties of this cooling schedule have been analyzed in [10], <ref> [27] </ref>, [33], [35], [58], [59]. The second heuristic rule applies to the output lattice of processing units and requires the size of the update (resonance) neighborhood centered on the winner node to decrease monotonically with time, such that a soft competitive learning strategy changes into a hard competitive (WTA) one. <p> This model transition is equivalent to stating that the initial overlap (oversampling [11]) between nodes' receptive fields must decrease monotonically with time until it is reduced to zero, as hard competitive learning renders receptive fields equivalent to Voronoi polyhedra <ref> [27] </ref>. Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in [9], [14], [15], [32], [35], [43]. <p> These initial positions can be randomly assigned or may be driven by sample vectors extracted from the input data set. The latter solution is likely to reduce the formation of dead units <ref> [27] </ref>. * It employs two user-defined monotone decreasing functions of time to compute: i) step size *(t) (cooling schedule, see Section 3); and ii) size (t) of the resonance domain centered on the winner PE in the output lattice. <p> structure of the preset graph G has to match the topological structure of the unknown manifold X [49]. 9.4 Advantages * Owing to its soft competitive implementation, SOM is expected to be less likely trapped in local minima and less likely to generate dead units than hard competitive alternatives [9], <ref> [27] </ref>, [62]. * Batch SOM is order independent, i.e., the final weight vectors are not affected by the order of the input sequence. * SOM can be employed as a vector requantization system. For some authors, "SOM was not intended for pattern classification. <p> [60]. * It does not provide pdf estimation. * It cannot be employed in topology-preserving mapping. 10.4 Advantages * Owing to its soft competitive implementation, FLVQ is expected to be less likely trapped in local minima and less likely to generate dead units than hard competitive alternatives (e.g., FCM) [9], <ref> [27] </ref>, [62]. * In FLVQ, due to batch learning, the final weight vectors are not affected by the order of the input sequence when its traditional termination criterion is removed. * With respect to SOM, FLVQ requires a smaller set of input parameters (its learning rate and the size of the <p> not minimize any known objective function, its termination is not based on optimizing any model of the process or its data [7]. * Owing to its hard competitive implementation, Fuzzy ART is more likely to be trapped in local minima and to generate dead units than soft competitive alternatives [9], <ref> [27] </ref>, [62]. * Fuzzy ART is order dependent due to on-line learning and example-driven neuron generation. <p> An additional mini-batch learning policy is employed to remove synaptic links whose utility has progressively faded away while the presentation of the input sequence goes on <ref> [27] </ref>. <p> Typical values are: * 1 = 0:05, * n = 0:0006 <ref> [27] </ref>. * Parameter 2 N + controls neuron generation at adaptation steps (see above). * Parameters ff and fi are used to decrease the error variables, so that more recent signals are weighted more heavily than previous ones [73]. Typical values are: ff = 0:5, fi = 0:0005 [27]. * Parameter <p> = 0:0006 <ref> [27] </ref>. * Parameter 2 N + controls neuron generation at adaptation steps (see above). * Parameters ff and fi are used to decrease the error variables, so that more recent signals are weighted more heavily than previous ones [73]. Typical values are: ff = 0:5, fi = 0:0005 [27]. * Parameter a max 2 N + is the maximum age of a synaptic link. A typical value is: a max = 88 [27]. * GNG employs a termination parameter, such as the maximum number of neurons c max [27]. 12.2 Description of the algorithm * In line with SOM, <p> Typical values are: ff = 0:5, fi = 0:0005 <ref> [27] </ref>. * Parameter a max 2 N + is the maximum age of a synaptic link. A typical value is: a max = 88 [27]. * GNG employs a termination parameter, such as the maximum number of neurons c max [27]. 12.2 Description of the algorithm * In line with SOM, GNG employs geometrical (distance) relationships in the input (measurement) space to enforce competitive learning mechanisms. <p> Typical values are: ff = 0:5, fi = 0:0005 <ref> [27] </ref>. * Parameter a max 2 N + is the maximum age of a synaptic link. A typical value is: a max = 88 [27]. * GNG employs a termination parameter, such as the maximum number of neurons c max [27]. 12.2 Description of the algorithm * In line with SOM, GNG employs geometrical (distance) relationships in the input (measurement) space to enforce competitive learning mechanisms. <p> schedule" (i.e., the learning rate does not satisfy Kohonen's first learning constraint), GNG lacks stability because of excessive plasticity, i.e., the processing of a new data set can radically alter maps detected by the system during the previous processing stage (try simulations on the web at the address reported in <ref> [27] </ref>). * It is not very easy to use, since it requires seven user-defined parameters whose meaning is not always straightforward. * Sample vectors are not employed to initialize reference vectors. <p> reached. * It combines mini-batch learning techniques (for neuron generation and removal, and for synapse removal) with example-driven generation of synapses. 12.4 Advantages * Owing to its soft competitive implementation, GNG is less likely to be trapped in local minima and to generate dead units than hard competitive alternatives [9], <ref> [27] </ref>, [62]. <p> The excellent adaptivity of GNG can be directly tested by simulations on the web <ref> [27] </ref>. * GNG is computationally efficient because its computation time increases linearly as (c+ No. of links). * GNG can be employed in: a) vector quantization, b) density function estimation, and c) structure detection in input data to be mapped in a topologically correct way onto submaps of an output lattice <p> topologically correct way onto submaps of an output lattice pursuing dimensionality reduction. 12.5 Architectural features The main features of GNG are summarized in Table 2. 13 FOSART Created to overcome the GNG deficiencies listed in Section 12.3, FOSART is an on-line learning algorithm which combines properties of SOM [5], GNG <ref> [27] </ref>, FCM [6], and Fuzzy SART [24] algorithms. FOSART employs a relative fuzzy membership function, derived from the one employed by FCM and Fuzzy SART, to compute activation values, and a Gaussian Basis Function (GBF) to compute absolute (possibilistic) fuzzy membership values (see Section 3). <p> Note that since Gaussian i (t), i = 1; :::; c feature the same spread parameter , then enforcing competitive learning among these GRBFs is equivalent to considering neuron receptive fields as Voronoi polyhedra <ref> [27] </ref>. Step 3. Resonance domain detection. From among processing units candidated by the attentional subsystem as being resonant the orienting subsystem selects those that match external requirements. Only these units are said to belong to the resonance domain. To select these units, the orienting subsystem employs an ART-based vigilance constraint. <p> are in progress, FOSART features Mean Square Error values larger than those obtained with SOM [72]. 13.4 Advantages * Owing to its soft competitive implementation, FOSART is expected to be less prone to being trapped in local minima and less likely to generate dead units than hard competitive alternatives [9], <ref> [27] </ref>, [62].
Reference: [28] <author> A. Baraldi and F. Parmiggiani, </author> <title> "A fuzzy neural network model capable of generating/removing neurons and synaptic links dynamically," </title> <booktitle> in Proc. of the II Italian Workshop on Fuzzy Logic, </booktitle> <address> Bari, Italy, </address> <month> March </month> <year> 1997, </year> <editor> P. Blonda, M. Castellano and A. Petrosino, Eds. </editor> <publisher> Singapore: World Scientific, </publisher> <year> 1998. </year>
Reference-contexts: Quantization, FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) <ref> [28] </ref>, [29], based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model [24], [25]. 1 2 Fuzzy membership and probability density functions This section proposes a brief review of probabilistic and possibilistic fuzzy membership concepts, to be compared with Bayes' view of posterior probability and likelihood. 2.1 Absolute and relative <p> On the other hand, in recent years, several on-line Kohonen-based network models exploiting distributed time variables have been presented [24], [26], [27], <ref> [28] </ref>. For example, in GNG and FOSART a connection-based time variable is equal to the number of times one synapse is selected at adaptation steps. In Fuzzy SART and FOSART a neuron-based time variable is equal to the number of epochs that PE has survived. <p> Note that only few clustering networks employ intra-layer connections explicitly, i.e., by means of specific data structures and parameter adaptation strategies [26], [27], <ref> [28] </ref>, [29]. with the so-called soft-max function or normalized exponential employed in mixture models [33], and mixture-of-experts [38]. 11 8 Dictionary entries in the comparison of clustering algo- rithms From the general discussion developed in Sections 2 to 7 we derive the following set of basis features for use as dictionary <p> this parameter affecting the overall number of training epochs required by the algorithm to reach termination (consider that, in FOSART, units are generated and removed dynamically as the number of input pattern presentations, t, increases). 13.2 Description of the algorithm Since FOSART has never been extensively presented in the literature <ref> [28] </ref>, [29], and has been subjected to continuous refinements, its current version is provided hereafter. Step 0. Initialization. Pattern counter t is set to 0. <p> According to <ref> [28] </ref>, these parameters can be considered as significant prior structures (see Section 4.3), i.e., an important property of the model that must be "hard-wired or built-in, perhaps to be tuned later by experience, but not learned in any statistically meaningful way" [71]. * In several tests regarding satellite image clustering that <p> In the tests provided, the system is stable with respect to small changes in input parameters and in the order of the presentation sequence <ref> [28] </ref>, [29]. * Due to its neuron removal strategy, it is robust against noise, i.e., it avoids overfitting. * Feed-back interaction between attentional and orienting subsystems allows FOSART to self-adjust its size depending on the complexity of the clustering task. 32 * Distinct sample vectors are employed to initialize reference vectors.
Reference: [29] <author> A. Baraldi and F. Parmiggiani, </author> <title> "Novel neural network model combining radial basis function, competitive Hebbian learning rule, and fuzzy simplified adaptive resonance 37 theory", </title> <booktitle> in Proc. SPIE's Optical Science, Engineering and Instrumentation '97: Appli--cations of Fuzzy Logic Technology IV, </booktitle> <address> San Diego, CA, </address> <booktitle> vol. </booktitle> <volume> 3165, </volume> <pages> pp. 98-112, </pages> <year> 1997. </year>
Reference-contexts: FLVQ [6] (which was first called Fuzzy Kohonen Clustering Network, FKCN [7]); iii) on-line learning, dynamic-sizing, no-linking Fuzzy Adaptive Resonance Theory (Fuzzy ART) [19], [20]; iv) on-line learning, dynamic-sizing, dynamic-linking Growing Neural Gas (GNG) [26], [27]; and v) on-line learning, dynamic-sizing, dynamic-linking Fully self-Organizing Simplified Adaptive Resonance Theory (FOSART) [28], <ref> [29] </ref>, based on the Fuzzy Simplified Adaptive Resonance Theory (Fuzzy SART) model [24], [25]. 1 2 Fuzzy membership and probability density functions This section proposes a brief review of probabilistic and possibilistic fuzzy membership concepts, to be compared with Bayes' view of posterior probability and likelihood. 2.1 Absolute and relative fuzzy <p> Note that only few clustering networks employ intra-layer connections explicitly, i.e., by means of specific data structures and parameter adaptation strategies [26], [27], [28], <ref> [29] </ref>. with the so-called soft-max function or normalized exponential employed in mixture models [33], and mixture-of-experts [38]. 11 8 Dictionary entries in the comparison of clustering algo- rithms From the general discussion developed in Sections 2 to 7 we derive the following set of basis features for use as dictionary entries <p> parameter affecting the overall number of training epochs required by the algorithm to reach termination (consider that, in FOSART, units are generated and removed dynamically as the number of input pattern presentations, t, increases). 13.2 Description of the algorithm Since FOSART has never been extensively presented in the literature [28], <ref> [29] </ref>, and has been subjected to continuous refinements, its current version is provided hereafter. Step 0. Initialization. Pattern counter t is set to 0. <p> In the tests provided, the system is stable with respect to small changes in input parameters and in the order of the presentation sequence [28], <ref> [29] </ref>. * Due to its neuron removal strategy, it is robust against noise, i.e., it avoids overfitting. * Feed-back interaction between attentional and orienting subsystems allows FOSART to self-adjust its size depending on the complexity of the clustering task. 32 * Distinct sample vectors are employed to initialize reference vectors.
Reference: [30] <author> Y. Pao, </author> <title> Adaptive pattern recognition and neural networks. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The extent to which X k is compatible with a vague (fuzzy) concept associated with generic state C i can be interpreted "more in terms of a possibility (compatibility) distribution rather than in terms of a probability distribution" <ref> [30] </ref>, p. 58. This legitimizes some possibility distributions, called fuzzy membership functions, that "we believe are useful, but might find difficult to justify on the basis of objective probabilities" [30], p. 57. <p> C i can be interpreted "more in terms of a possibility (compatibility) distribution rather than in terms of a probability distribution" <ref> [30] </ref>, p. 58. This legitimizes some possibility distributions, called fuzzy membership functions, that "we believe are useful, but might find difficult to justify on the basis of objective probabilities" [30], p. 57. <p> Absolute and relative mem bership types are related by the following equation: R i;k = P c ; i = 1; :::; c; k = 1; :::; n: (1) Relative typicality values, R i;k , must satisfy the following three conditions [7], <ref> [30] </ref>: (i) R i;k 2 [0,1], i = 1; :::; c, k = 1; :::; n; P c (iii) 0 &lt; k=1 R i;k &lt; n, i = 1; :::; c. <p> Although possibilistic membership functions, A i;k , i = 1; :::; c, k = 1; :::; n, may satisfy conditions (i) and (iii) listed above (in this case, the upper bound of the membership function is one and the fuzzy set is termed normal <ref> [30] </ref>), they always differ from probabilistic memberships in condition (ii), which is relaxed as follows [61] (iv) max i fA i;k g &gt; 0, k = 1; :::; n.
Reference: [31] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> "Maximum likelihood from incomplete data via the EM algorithm," </title> <journal> J. Royal Statist. Soc. Ser. B, </journal> <volume> vol. 39, </volume> <pages> pp. 1-38, </pages> <year> 1977. </year>
Reference-contexts: 2.2 Fuzzy memberships and mixture probabilities To investigate the relationship between (objective) probability density functions and (useful) fuzzy membership functions, note that absolute membership function (3) relates probabilistic membership (1) to Gaussian mixture models, which are widely employed in the framework of optimization problems featuring a firm statistical foundation [9], <ref> [31] </ref>, [32], [43]. <p> For example, SOM pursues soft competitive learning by means of biologically plausible update rules that employ no fuzzy set-theoretic concept. Another interesting example is the one provided by the Expectation-Maximization (EM) algorithm applied to optimize parameters of a Gaussian mixture <ref> [31] </ref>, [33]. Although it features a firm statistic foundation and employs no fuzzy set-theoretic concept, EM applied to Gaussian mixtures can be termed fuzzy according to definition 1. presented above. <p> Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm <ref> [31] </ref>, are discussed in [9], [14], [15], [32], [35], [43]. From a general perspective, it is to be remembered that compared to hard competitive learning soft competitive learning not only decreases dependency on initialization, but also reduces the presence of dead units (see Section 7).
Reference: [32] <author> E. Alpaydn, </author> <title> "Soft vector quantization and the EM algorithm," </title> <booktitle> Neural Networks, </booktitle> <year> 1998, </year> <note> in press. </note>
Reference-contexts: Fuzzy memberships and mixture probabilities To investigate the relationship between (objective) probability density functions and (useful) fuzzy membership functions, note that absolute membership function (3) relates probabilistic membership (1) to Gaussian mixture models, which are widely employed in the framework of optimization problems featuring a firm statistical foundation [9], [31], <ref> [32] </ref>, [43]. <p> Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in [9], [14], [15], <ref> [32] </ref>, [35], [43]. From a general perspective, it is to be remembered that compared to hard competitive learning soft competitive learning not only decreases dependency on initialization, but also reduces the presence of dead units (see Section 7).
Reference: [33] <author> C. M. Bishop, </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford, UK: Clarendon Press, </publisher> <year> 1995. </year>
Reference-contexts: The goal of on-line learning methods is to avoid storage of a complete data set by discarding each data point once it has been used <ref> [33] </ref>. <p> b) the input data set may be so huge that batch methods become impractical, because of their numerical properties (see below), or 4 computation time, or memory requirement; and c) the input data comes as a continuous stream of unlimited length which makes it totally impossible to apply batch methods <ref> [33] </ref>, [39], [43]. On-line learning typically results in systems that become order-dependent during training (in line with biological complex systems [44]). <p> In batch learning problems (e.g., the simple case of linear model regression, see <ref> [33] </ref>, p. 92, and [39]), exact closed form solutions can lead to numerical difficulties for very large data sets. In these cases the only computationally feasible alternative is provided by iterative batch methods, such as the gradient descent of the cost function [33], [42], that sweep repeatedly through the data set. <p> the simple case of linear model regression, see <ref> [33] </ref>, p. 92, and [39]), exact closed form solutions can lead to numerical difficulties for very large data sets. In these cases the only computationally feasible alternative is provided by iterative batch methods, such as the gradient descent of the cost function [33], [42], that sweep repeatedly through the data set. <p> In iterative batch learning algorithms, the learning rate parameter must be small and its choice is fairly critical: if the learning rate is too small the reduction in error will be very small; if it is too large instabilities (divergent oscillations) may result <ref> [33] </ref>. Analytically, when convergence of iterative batch algorithms to exact closed form solutions is analyzed, then useful hints on constraint of the learning rate value are gathered [39]. <p> By taking intermediate steps in the parameter space, iterative mini-batch algorithms may converge faster than their iterative full batch counterparts. By stretching the same idea further, another alternative solution to iterative full batch algorithms is to develop their on-line (stochastic) 1 approximations <ref> [33] </ref>. On-line learning algorithms are simple and intuitive because they are based on the following heuristic ([33], p. 46; [40]): the sum over the training samples, which is found in the iterative and batch solution of the cost function minimization task, is dropped by separating the contribution from the last data <p> In general, the learning rate ff (t) of the on-line update rule must satisfy the three conditions applied to the coefficients of the Robbins-Monro algorithm for finding the roots of a function iteratively, which are (see <ref> [33] </ref>, pp. 47, 96): i) lim t!1 ff (t) = 0; P 1 and iii) t=1 ff 2 (t) &lt; 1. <p> When the learning rate decreases monotonically under Robbins-Monro conditions (e.g., ff (t) = 1=t, see <ref> [33] </ref>, p. 96, and [27]), on-line learning algorithms can be shown to converge to a point in the parameter space [39]. <p> As an example of the curse of dimensionality, consider that any function estimator increases its number of adjustable parameters with the dimensionality of input space. As a consequence, the size of training data required to compute a reliable estimate of adaptive parameters may become huge in practical problems <ref> [33] </ref>, [35]. The complexity of a learning system increases with the number of independent and adjustable parameters, also termed degrees of freedom, to be adapted during the learning process. <p> According to the qualitative principle of Occam's razor, a sound basis for generalizing beyond a given set of examples is to prefer the simplest hypothesis that fits observed data <ref> [33] </ref>, [42]. This principle states that to be effective, the cost function minimized by an inductive learning system should provide a trade-off between how well the model fits the training data and model complexity. <p> For example, SOM pursues soft competitive learning by means of biologically plausible update rules that employ no fuzzy set-theoretic concept. Another interesting example is the one provided by the Expectation-Maximization (EM) algorithm applied to optimize parameters of a Gaussian mixture [31], <ref> [33] </ref>. Although it features a firm statistic foundation and employs no fuzzy set-theoretic concept, EM applied to Gaussian mixtures can be termed fuzzy according to definition 1. presented above. <p> Note that only few clustering networks employ intra-layer connections explicitly, i.e., by means of specific data structures and parameter adaptation strategies [26], [27], [28], [29]. with the so-called soft-max function or normalized exponential employed in mixture models <ref> [33] </ref>, and mixture-of-experts [38]. 11 8 Dictionary entries in the comparison of clustering algo- rithms From the general discussion developed in Sections 2 to 7 we derive the following set of basis features for use as dictionary entries in a theoretical comparison of data clustering algorithms: * fuzzy set-theoretical concepts, such <p> Important properties of this cooling schedule have been analyzed in [10], [27], <ref> [33] </ref>, [35], [58], [59]. The second heuristic rule applies to the output lattice of processing units and requires the size of the update (resonance) neighborhood centered on the winner node to decrease monotonically with time, such that a soft competitive learning strategy changes into a hard competitive (WTA) one. <p> It is important to observe that Equation (12) is related to on-line (McQueen's) k-means [16], <ref> [33] </ref>, [43], whose batch (Lloyd's or Forgy's) version [16] is a special case of the EM optimization of a Gaussian mixture [33]. <p> It is important to observe that Equation (12) is related to on-line (McQueen's) k-means [16], <ref> [33] </ref>, [43], whose batch (Lloyd's or Forgy's) version [16] is a special case of the EM optimization of a Gaussian mixture [33]. <p> It can also be observed that Equation (15) belongs to the same class of weight adaptation rules employed in the batch form of the SOM algorithm and in the batch and statistically firm EM optimization of Gaussian centers in a mixture model [5], <ref> [33] </ref>, [34]. Equation (15) shows that when 17 m (e) = m is fixed, then FLVQ is equivalent to FCM [6]. It can be proved that by decreasing its weighting exponent m (e), descending FLVQ tends to reduce the width of the learning rate distribution by means of model transitions. <p> A stochastic (on-line, sequential) weight adaptation law, derived from stochastic optimization of the parameters of a Gaussian mixture model <ref> [33] </ref>, is applied to all processing units belonging to the resonance domain.
Reference: [34] <author> C. M. Bishop, M. Svensen, and C. Williams, "GTM: </author> <title> a principled alternative to the self-organizing map," </title> <booktitle> in Proc. Int. Conf. on Artificial Neural Networks, </booktitle> <address> ICANN'96. </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 164-170, </pages> <year> 1996. </year>
Reference-contexts: Finally, it is important to point out that a batch and iterative SOM version has also been presented [5], <ref> [34] </ref>. 9.1 Input parameters To simplify the discussion, our approach is to deal with a finite training data set fXg, consisting of n input patterns X k , k = 1; :::; n, where X k 2 R D , such that D is the dimensionality of the input space. <p> as long as Equation (11) features (t) &gt; 0 (i.e., as long as the SOM soft learning strategy is not equivalent to a WTA strategy), one cannot specify a cost function that is minimized by Equation (12), i.e., there exists no cost function yielding Equation (12) as its gradient [9], <ref> [34] </ref>, [65]. SOM instead features a set of potential functions, one for each node, to be independently minimized following a stochastic (on-line) gradient descent [65]. <p> distance separating the input pattern from the cluster template. * The size of the output lattice, the step size and the size of the resonance neighborhood must be varied empirically from one data set to another to achieve useful results [7]. * Probability density function (pdf) estimation is not achieved <ref> [34] </ref>. Attempts have been made to interpret the density of codebook vectors as a model of the input data distribution but with limited success [26],[27], [34], [51]. * It should not be employed in topology-preserving mapping when the dimension of the input space is larger than three. <p> neighborhood must be varied empirically from one data set to another to achieve useful results [7]. * Probability density function (pdf) estimation is not achieved <ref> [34] </ref>. Attempts have been made to interpret the density of codebook vectors as a model of the input data distribution but with limited success [26],[27], [34], [51]. * It should not be employed in topology-preserving mapping when the dimension of the input space is larger than three. <p> It can also be observed that Equation (15) belongs to the same class of weight adaptation rules employed in the batch form of the SOM algorithm and in the batch and statistically firm EM optimization of Gaussian centers in a mixture model [5], [33], <ref> [34] </ref>. Equation (15) shows that when 17 m (e) = m is fixed, then FLVQ is equivalent to FCM [6]. It can be proved that by decreasing its weighting exponent m (e), descending FLVQ tends to reduce the width of the learning rate distribution by means of model transitions.
Reference: [35] <author> V. Cherkassky and F. Mulier, </author> <title> Learning From Data: Concepts, Theory, and Methods. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1998. </year>
Reference-contexts: unobserved samples belonging to a so-called test set, or about unknown properties hidden in the training data; and ii) the goal of the learning process is to minimize a risk functional (theoretically computed over an infinite data set) by adapting system parameters on the basis of the finite training set <ref> [35] </ref>, i.e., the learning problem is turned into an optimization problem [39]. <p> As an example of the curse of dimensionality, consider that any function estimator increases its number of adjustable parameters with the dimensionality of input space. As a consequence, the size of training data required to compute a reliable estimate of adaptive parameters may become huge in practical problems [33], <ref> [35] </ref>. The complexity of a learning system increases with the number of independent and adjustable parameters, also termed degrees of freedom, to be adapted during the learning process. <p> This also means that model complexity must be controlled by a priori (background) knowledge, i.e., subjective knowledge available before any evidence (e.g., empirical risk) provided by the training data is observed. Different in 6 ductive principles provide cost functions considered as different quantitative formulations of Occam's qualitative principle <ref> [35] </ref>. A rough taxonomy of advanced techniques for optimal learning, originally proposed in [40], is presented hereafter. 4.1 Global optimization Instead of local algorithms like gradient descent, one may explore techniques that guarantee global optimization while effectively facing the curse of dimensionality. <p> Among the most significant developments in this area, Support Vector Machines (SVMs), based on the Vapnik-Chervonenkis (VC) statistical learning theory and capable of detecting the global minimum of a cost function for classification problems, are becoming increasingly popular in finding solutions to both classification and function regression tasks <ref> [35] </ref>, [36], [37]. 4.2 Growing networks and pruning Human designers typically have the opportunity to embed task-specific prior knowledge in an inductive learning algorithm, e.g., by setting the topology and the complexity of a multilayer perceptron when backpropagation weight adaptation is applied. <p> Important properties of this cooling schedule have been analyzed in [10], [27], [33], <ref> [35] </ref>, [58], [59]. The second heuristic rule applies to the output lattice of processing units and requires the size of the update (resonance) neighborhood centered on the winner node to decrease monotonically with time, such that a soft competitive learning strategy changes into a hard competitive (WTA) one. <p> Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in [9], [14], [15], [32], <ref> [35] </ref>, [43]. From a general perspective, it is to be remembered that compared to hard competitive learning soft competitive learning not only decreases dependency on initialization, but also reduces the presence of dead units (see Section 7).
Reference: [36] <author> V. Vapnik, </author> <title> The Nature of Statistical Learning Theory. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Among the most significant developments in this area, Support Vector Machines (SVMs), based on the Vapnik-Chervonenkis (VC) statistical learning theory and capable of detecting the global minimum of a cost function for classification problems, are becoming increasingly popular in finding solutions to both classification and function regression tasks [35], <ref> [36] </ref>, [37]. 4.2 Growing networks and pruning Human designers typically have the opportunity to embed task-specific prior knowledge in an inductive learning algorithm, e.g., by setting the topology and the complexity of a multilayer perceptron when backpropagation weight adaptation is applied.
Reference: [37] <author> C. Burges, </author> <title> "A tutorial on Support Vector Machines for pattern recognition," submitted to Data Mining and Knowledge Discovery, </title> <year> 1998. </year>
Reference-contexts: Among the most significant developments in this area, Support Vector Machines (SVMs), based on the Vapnik-Chervonenkis (VC) statistical learning theory and capable of detecting the global minimum of a cost function for classification problems, are becoming increasingly popular in finding solutions to both classification and function regression tasks [35], [36], <ref> [37] </ref>. 4.2 Growing networks and pruning Human designers typically have the opportunity to embed task-specific prior knowledge in an inductive learning algorithm, e.g., by setting the topology and the complexity of a multilayer perceptron when backpropagation weight adaptation is applied.
Reference: [38] <author> M. J. Jordan and R. A. Jacobs, </author> <title> "Hierarchical mixture of experts and the EM algorithm," </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 181-214, </pages> <year> 1994. </year>
Reference-contexts: In applied mathematics, the principle of tackling a problem by dividing it into simpler subproblems whose solutions can be combined to yield a solution to the complex problem is termed divide and conquer <ref> [38] </ref>. In supervised learning, an interesting modular proposal that addresses the major problem of providing effective integration of the system modules is presented in [38]. <p> a problem by dividing it into simpler subproblems whose solutions can be combined to yield a solution to the complex problem is termed divide and conquer <ref> [38] </ref>. In supervised learning, an interesting modular proposal that addresses the major problem of providing effective integration of the system modules is presented in [38]. <p> Note that only few clustering networks employ intra-layer connections explicitly, i.e., by means of specific data structures and parameter adaptation strategies [26], [27], [28], [29]. with the so-called soft-max function or normalized exponential employed in mixture models [33], and mixture-of-experts <ref> [38] </ref>. 11 8 Dictionary entries in the comparison of clustering algo- rithms From the general discussion developed in Sections 2 to 7 we derive the following set of basis features for use as dictionary entries in a theoretical comparison of data clustering algorithms: * fuzzy set-theoretical concepts, such as absolute and
Reference: [39] <author> M. J. Jordan and C. M. Bishop, </author> <title> "An introduction to graphical models and machine learning," </title> <type> draft document, </type> <year> 1998. </year>
Reference-contexts: properties hidden in the training data; and ii) the goal of the learning process is to minimize a risk functional (theoretically computed over an infinite data set) by adapting system parameters on the basis of the finite training set [35], i.e., the learning problem is turned into an optimization problem <ref> [39] </ref>. When system parameters are learned from training data, there are two classes of learning situations, depending on how data are presented to the learner: the "batch" setting in which data are available as a block, and the "on-line" setting in which data arrive sequentially [39]. <p> turned into an optimization problem <ref> [39] </ref>. When system parameters are learned from training data, there are two classes of learning situations, depending on how data are presented to the learner: the "batch" setting in which data are available as a block, and the "on-line" setting in which data arrive sequentially [39]. In many practical problems, when a sequential data stream can be stored for analysis as a block, or a block of data can be analyzed sequentially, the user is free to take either the batch or the on-line point of view [39]. <p> the "on-line" setting in which data arrive sequentially <ref> [39] </ref>. In many practical problems, when a sequential data stream can be stored for analysis as a block, or a block of data can be analyzed sequentially, the user is free to take either the batch or the on-line point of view [39]. The goal of on-line learning methods is to avoid storage of a complete data set by discarding each data point once it has been used [33]. <p> the input data set may be so huge that batch methods become impractical, because of their numerical properties (see below), or 4 computation time, or memory requirement; and c) the input data comes as a continuous stream of unlimited length which makes it totally impossible to apply batch methods [33], <ref> [39] </ref>, [43]. On-line learning typically results in systems that become order-dependent during training (in line with biological complex systems [44]). <p> Batch methods are preferred when our only interest is in a final answer, i.e., the best answer that we can obtain from a finite training data set as the exact closed form solution to a minimization problem <ref> [39] </ref>. In batch learning problems (e.g., the simple case of linear model regression, see [33], p. 92, and [39]), exact closed form solutions can lead to numerical difficulties for very large data sets. <p> our only interest is in a final answer, i.e., the best answer that we can obtain from a finite training data set as the exact closed form solution to a minimization problem <ref> [39] </ref>. In batch learning problems (e.g., the simple case of linear model regression, see [33], p. 92, and [39]), exact closed form solutions can lead to numerical difficulties for very large data sets. In these cases the only computationally feasible alternative is provided by iterative batch methods, such as the gradient descent of the cost function [33], [42], that sweep repeatedly through the data set. <p> Analytically, when convergence of iterative batch algorithms to exact closed form solutions is analyzed, then useful hints on constraint of the learning rate value are gathered <ref> [39] </ref>. <p> If this is the case (e.g., for gradient descent algorithms), this functional feature is clearly incompatible with the original motivation that justifies the study of such iterative batch learning schemes. In such situations, an alternative solution, called "mini-batch" <ref> [39] </ref>, is to average parameter update values over subsets of the entire training data set. By taking intermediate steps in the parameter space, iterative mini-batch algorithms may converge faster than their iterative full batch counterparts. <p> Although this heuristic seems reasonable, there should be some analytical proof that the on-line proce 1 "Stochastic" refers to the assumption that the single data point being presented to the learning system is chosen on the basis of a stochastic process <ref> [39] </ref>. 5 dure converges to a solution. It is known that the difference between exact batch-mode and heuristic on-line updating is arbitrarily reduced by the adoption of "small" learning rates [42], [48]. <p> According to the view that on-line procedures are approximations of iterative batch algorithms, learning rate constraints capable of guaranteeing convergence of the iterative batch mode may be applied to the on-line problem under an appropriate definition of convergence (for the linear regression case, see <ref> [39] </ref>). If these conditions hold, it has been observed that on-line learning systems, by requiring significantly less computation time per parameter update, can be significantly faster in converging than iterative batch algorithms [39]. <p> be applied to the on-line problem under an appropriate definition of convergence (for the linear regression case, see <ref> [39] </ref>). If these conditions hold, it has been observed that on-line learning systems, by requiring significantly less computation time per parameter update, can be significantly faster in converging than iterative batch algorithms [39]. <p> For example, in an on-line update procedure, if the learning rate remains fixed, then the algorithm converges only in a stochastic sense <ref> [39] </ref>, i.e., model parameters drift from their initial positions to quasi-stationary positions where they start to wander around in dynamic equilibrium [27]. <p> When the learning rate decreases monotonically under Robbins-Monro conditions (e.g., ff (t) = 1=t, see [33], p. 96, and [27]), on-line learning algorithms can be shown to converge to a point in the parameter space <ref> [39] </ref>.
Reference: [40] <author> M. Bianchini and M. Gori, </author> <title> "Optimal learning in artificial neural networks: a review of theoretical results," </title> <journal> Neurocomputing, </journal> <volume> vol. 13, no. </volume> <pages> 2-4, pp. 313-346, </pages> <year> 1996. </year>
Reference-contexts: By stretching the same idea further, another alternative solution to iterative full batch algorithms is to develop their on-line (stochastic) 1 approximations [33]. On-line learning algorithms are simple and intuitive because they are based on the following heuristic ([33], p. 46; <ref> [40] </ref>): the sum over the training samples, which is found in the iterative and batch solution of the cost function minimization task, is dropped by separating the contribution from the last data point to provide a sequential update formula, i.e., to allow one parameter update for every new data point presentation. <p> Different in 6 ductive principles provide cost functions considered as different quantitative formulations of Occam's qualitative principle [35]. A rough taxonomy of advanced techniques for optimal learning, originally proposed in <ref> [40] </ref>, is presented hereafter. 4.1 Global optimization Instead of local algorithms like gradient descent, one may explore techniques that guarantee global optimization while effectively facing the curse of dimensionality. <p> Pruning algorithms begin training a network expected to be large with respect to the problem at hand, and then continue by pruning nodes that do not affect the learning process significantly <ref> [40] </ref>, [42]. Vice versa, growing networks start from small networks that grow gradually until convergence is reached [27], [40]. <p> Pruning algorithms begin training a network expected to be large with respect to the problem at hand, and then continue by pruning nodes that do not affect the learning process significantly <ref> [40] </ref>, [42]. Vice versa, growing networks start from small networks that grow gradually until convergence is reached [27], [40]. As a result, the complexity of the network is expected to be tuned to the problem at hand, i.e., generalization capability is expected to increase. 4.3 Modular architectures: prior structures and experience-based fine tuning Self-organizing neurological systems consist of highly structured, hierarchical architectures provided with feed-back mechanisms [45]. <p> In line with biological learning systems, a classical engineering paradigm consists of partitioning the solution to a problem between several modules specialized in learning a single task, i.e., modular architectures are the natural solution to most significant practical problems <ref> [40] </ref>, [44]. In applied mathematics, the principle of tackling a problem by dividing it into simpler subproblems whose solutions can be combined to yield a solution to the complex problem is termed divide and conquer [38]. <p> In supervised learning, an interesting modular proposal that addresses the major problem of providing effective integration of the system modules is presented in [38]. Analytically, the importance of developing modular architectures has been stressed in <ref> [40] </ref>, [41], where sufficient (but not necessary) conditions capable of guaranteeing local minima free cost functions are detected, such that a simple gradient descent algorithm can always reach the absolute minimum of the error surface. 7 Contiguous to the problem of fine-tuning modular learning systems on the basis of training experiences <p> functions are detected, such that a simple gradient descent algorithm can always reach the absolute minimum of the error surface. 7 Contiguous to the problem of fine-tuning modular learning systems on the basis of training experiences is the problem of prior structures, i.e., the problem of learning from tabula rasa <ref> [40] </ref>. Minsky claims that a"significant learning at significant rate presupposes some significant prior structure" [47]. In other words, important properties of the model must be "hard-wired or built-in, perhaps to be tuned later by experience, but not learned in any statistical meaningful way" [71].
Reference: [41] <author> M. Bianchini, P. Frasconi, M. Gori, </author> <title> "Learning without local minima in radial basis function networks," </title> <journal> IEEE Trans. Neural Networks, vo. </journal> <volume> 6, no. 3, </volume> <pages> pp. 749-756, </pages> <year> 1995. </year>
Reference-contexts: In supervised learning, an interesting modular proposal that addresses the major problem of providing effective integration of the system modules is presented in [38]. Analytically, the importance of developing modular architectures has been stressed in [40], <ref> [41] </ref>, where sufficient (but not necessary) conditions capable of guaranteeing local minima free cost functions are detected, such that a simple gradient descent algorithm can always reach the absolute minimum of the error surface. 7 Contiguous to the problem of fine-tuning modular learning systems on the basis of training experiences is
Reference: [42] <author> T. Mitchell, </author> <title> Machine Learning. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1997. </year>
Reference-contexts: On-line learning typically results in systems that become order-dependent during training (in line with biological complex systems [44]). Moreover, on-line systems, where parameter adaptation is example-driven <ref> [42] </ref>, are more sensitive to the presence of noise as they do not average over the noise on the data, i.e., they tend to provide highly oscillatory (non-smooth) output functions that perform poorly in terms of generalization ability. <p> In these cases the only computationally feasible alternative is provided by iterative batch methods, such as the gradient descent of the cost function [33], <ref> [42] </ref>, that sweep repeatedly through the data set. <p> It is known that the difference between exact batch-mode and heuristic on-line updating is arbitrarily reduced by the adoption of "small" learning rates <ref> [42] </ref>, [48]. According to the view that on-line procedures are approximations of iterative batch algorithms, learning rate constraints capable of guaranteeing convergence of the iterative batch mode may be applied to the on-line problem under an appropriate definition of convergence (for the linear regression case, see [39]). <p> According to the qualitative principle of Occam's razor, a sound basis for generalizing beyond a given set of examples is to prefer the simplest hypothesis that fits observed data [33], <ref> [42] </ref>. This principle states that to be effective, the cost function minimized by an inductive learning system should provide a trade-off between how well the model fits the training data and model complexity. <p> Pruning algorithms begin training a network expected to be large with respect to the problem at hand, and then continue by pruning nodes that do not affect the learning process significantly [40], <ref> [42] </ref>. Vice versa, growing networks start from small networks that grow gradually until convergence is reached [27], [40].
Reference: [43] <author> J. Buhmann, </author> <title> "Learning and data clustering," in Handbook of Brain Theory and Neural Networks, </title> <editor> M. Arbib, Ed. </editor> <publisher> Bradford Books / MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: memberships and mixture probabilities To investigate the relationship between (objective) probability density functions and (useful) fuzzy membership functions, note that absolute membership function (3) relates probabilistic membership (1) to Gaussian mixture models, which are widely employed in the framework of optimization problems featuring a firm statistical foundation [9], [31], [32], <ref> [43] </ref>. <p> input data set may be so huge that batch methods become impractical, because of their numerical properties (see below), or 4 computation time, or memory requirement; and c) the input data comes as a continuous stream of unlimited length which makes it totally impossible to apply batch methods [33], [39], <ref> [43] </ref>. On-line learning typically results in systems that become order-dependent during training (in line with biological complex systems [44]). <p> Interpretations of this second heuristic rule, and relationships between SOM and other optimization techniques such as "maximum-entropy" clustering 12 [12], deterministic annealing [13], and the Expectation-Maximization (EM) optimization algorithm [31], are discussed in [9], [14], [15], [32], [35], <ref> [43] </ref>. From a general perspective, it is to be remembered that compared to hard competitive learning soft competitive learning not only decreases dependency on initialization, but also reduces the presence of dead units (see Section 7). <p> It is important to observe that Equation (12) is related to on-line (McQueen's) k-means [16], [33], <ref> [43] </ref>, whose batch (Lloyd's or Forgy's) version [16] is a special case of the EM optimization of a Gaussian mixture [33].
Reference: [44] <author> R. Serra and G. </author> <title> Zanarini, </title> <booktitle> Complex systems and cognitive processes. </booktitle> <address> Berlin, Germany: </address> <publisher> Springer-Verlag 1990. </publisher>
Reference-contexts: On-line learning typically results in systems that become order-dependent during training (in line with biological complex systems <ref> [44] </ref>). Moreover, on-line systems, where parameter adaptation is example-driven [42], are more sensitive to the presence of noise as they do not average over the noise on the data, i.e., they tend to provide highly oscillatory (non-smooth) output functions that perform poorly in terms of generalization ability. <p> In line with biological learning systems, a classical engineering paradigm consists of partitioning the solution to a problem between several modules specialized in learning a single task, i.e., modular architectures are the natural solution to most significant practical problems [40], <ref> [44] </ref>. In applied mathematics, the principle of tackling a problem by dividing it into simpler subproblems whose solutions can be combined to yield a solution to the complex problem is termed divide and conquer [38]. <p> It is the presence of this feed-back interaction with the environment that characterizes all natural systems featuring cognitive capabilities <ref> [44] </ref>. Some artificial neural systems feature none of the biological properties listed above. For example, SOM [4] and the Hopfield network [52] are homogeneous systems; they feature no structured architecture and no supervision or reinforcement by, or feedback from, an external environment (also termed supervisor). <p> While intra-layer connections are found in every biological cognitive system, their explicit adaptation has been introduced in artificial neural networks only recently [49]. 33 3) Within the framework of the multi-disciplinary science of artificial complex systems <ref> [44] </ref>, fuzzy set-theoretic concepts may be adopted to model soft competitive learning mechanisms that are considered useful by the human developer, although they may be difficult to justify on the basis of objective probabilities.
Reference: [45] <author> R. Llinas and P. Churchland, </author> <title> The Mind-Brained Continuum. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year> <month> 38 </month>
Reference-contexts: As a result, the complexity of the network is expected to be tuned to the problem at hand, i.e., generalization capability is expected to increase. 4.3 Modular architectures: prior structures and experience-based fine tuning Self-organizing neurological systems consist of highly structured, hierarchical architectures provided with feed-back mechanisms <ref> [45] </ref>. In these systems, the combination of an initial architecture produced by evolution and experience-based additional fine-tuning prepares the whole system to function in an entire domain by generalizing its learned behaviour to instances not previously encountered [46].
Reference: [46] <author> B. Happel and J. Murre, </author> <title> "Design and evolution of modular neural network architec-ture," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 6/7, </volume> <pages> pp. 985-1004, </pages> <year> 1994. </year>
Reference-contexts: In these systems, the combination of an initial architecture produced by evolution and experience-based additional fine-tuning prepares the whole system to function in an entire domain by generalizing its learned behaviour to instances not previously encountered <ref> [46] </ref>. In line with biological learning systems, a classical engineering paradigm consists of partitioning the solution to a problem between several modules specialized in learning a single task, i.e., modular architectures are the natural solution to most significant practical problems [40], [44].
Reference: [47] <author> M. L. Minsky and S. A. Papert, </author> <title> Perceptrons Expanded Edition. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Minsky claims that a"significant learning at significant rate presupposes some significant prior structure" <ref> [47] </ref>. In other words, important properties of the model must be "hard-wired or built-in, perhaps to be tuned later by experience, but not learned in any statistical meaningful way" [71].
Reference: [48] <author> D. E. Rumelhart, G. E. Hinton and R. J. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing, </title> <editor> D. E. Rumelhart and J. L. McClelland, Eds. </editor> <address> Cambridge MA: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: It is known that the difference between exact batch-mode and heuristic on-line updating is arbitrarily reduced by the adoption of "small" learning rates [42], <ref> [48] </ref>. According to the view that on-line procedures are approximations of iterative batch algorithms, learning rate constraints capable of guaranteeing convergence of the iterative batch mode may be applied to the on-line problem under an appropriate definition of convergence (for the linear regression case, see [39]).
Reference: [49] <author> T. M. Martinetz and K. J. Schulten, </author> <title> "Topology representing networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 3, </volume> <pages> pp. 507-522, </pages> <year> 1994. </year>
Reference-contexts: To summarize, in batch clustering algorithms found in the literature, exploitation of distributed rather than global parameters may deserve further investigation in the framework of developing self-organizing networks consisting of specialized subsystems. 5 Topologically correct mapping The presentation of the Competitive Hebbian Rule (CHR) <ref> [49] </ref>, introducing competition among synaptic links, represented a fundamental breakthrough in the evolution of Fully 8 Self-Organizing artificial Neural Network models (FSONN). <p> The exploitation of the Euclidean inter-pattern distance in competitive learning shapes neuron receptive fields as Voronoi polyhedra <ref> [49] </ref>. According to CHR, if connection between P E w1 and P E w2 does not exist, it is generated. It has been proved that CHR forms Topology Preserving Maps (TPMs) [49]. <p> The exploitation of the Euclidean inter-pattern distance in competitive learning shapes neuron receptive fields as Voronoi polyhedra <ref> [49] </ref>. According to CHR, if connection between P E w1 and P E w2 does not exist, it is generated. It has been proved that CHR forms Topology Preserving Maps (TPMs) [49]. <p> Thus, a TPM is defined as a mapping S from X to G such that S , together with inverse mapping 1 S from G to X, are neighborhood (adjacency) preserving <ref> [49] </ref>. 9 Note that exploitation of CHR in FSONNs allows generation of networks consisting of mutually disjointed (specialized and independent) maps [27], [49]. <p> is defined as a mapping S from X to G such that S , together with inverse mapping 1 S from G to X, are neighborhood (adjacency) preserving <ref> [49] </ref>. 9 Note that exploitation of CHR in FSONNs allows generation of networks consisting of mutually disjointed (specialized and independent) maps [27], [49]. <p> In fact, SOM tries to form a neighborhood-preserving inverse mapping 1 S from lattice G to input manifold X, but not necessarily a neighborhood preserving mapping S from X to G <ref> [49] </ref>. To obtain a topologically correct map by running the SOM algorithm, the topological (adjacency) structure of the preset graph G has to match the topological structure of the unknown manifold X [49]. 9.4 Advantages * Owing to its soft competitive implementation, SOM is expected to be less likely trapped in <p> lattice G to input manifold X, but not necessarily a neighborhood preserving mapping S from X to G <ref> [49] </ref>. To obtain a topologically correct map by running the SOM algorithm, the topological (adjacency) structure of the preset graph G has to match the topological structure of the unknown manifold X [49]. 9.4 Advantages * Owing to its soft competitive implementation, SOM is expected to be less likely trapped in local minima and less likely to generate dead units than hard competitive alternatives [9], [27], [62]. * Batch SOM is order independent, i.e., the final weight vectors are not affected by the <p> More precisely, it can be stated that SOM can be employed in topology-preserving mapping iff the dimension of the input space is not larger than the dimension of the output lattice <ref> [49] </ref>. 9.5 Architectural features The main features of SOM are summarized in Table 2. 10 FLVQ FLVQ [6] (which was first called FKCN [7]) has quickly gained popularity as a fairly successful batch clustering algorithm. <p> to random initialization. * Fuzzy ART can be employed as a vector requantization system. 11.5 Architectural features The main features of Fuzzy ART are summarized in Table 2. 12 GNG GNG combines the growth mechanism inherited from the earlier proposed Growing Cell Structures [73] with the synapse geration rule CHR <ref> [49] </ref>. GNG is capable of generating and removing both synaptic links and PEs, i.e, GNG belongs to the class of FSONN models (see Section 5). <p> In line with Fuzzy ART, FOSART applies an ART-based, example-driven vigilance test to control neuron generation [19], [24]. Similarly to GNG, FOSART employs a new version of CHR, termed Constrained Competitive Hebbian Rule (CCHR) to generate synaptic links on an example-driven basis <ref> [49] </ref>. In line with GNG, FOSART removes synaptic links as well as neurons according to a mini-batch parameter adaptation scheme. <p> While intra-layer connections are found in every biological cognitive system, their explicit adaptation has been introduced in artificial neural networks only recently <ref> [49] </ref>. 33 3) Within the framework of the multi-disciplinary science of artificial complex systems [44], fuzzy set-theoretic concepts may be adopted to model soft competitive learning mechanisms that are considered useful by the human developer, although they may be difficult to justify on the basis of objective probabilities.
Reference: [50] <editor> D. Parisi, "La scienza cognitiva tra intelligenza artificiale e vita artificiale," in Neu-roscienze e Scienze dell'Artificiale: dal Neurone all'Intelligenza E. Biondi et al., Eds. </editor> <address> Bologna, Italy: </address> <publisher> Patron, </publisher> <pages> pp. 321-341, </pages> <year> 1991. </year>
Reference-contexts: nets To perform cognitive tasks, biological neural systems exploit: i) dishomogeneous nets, where several types of PEs are combined; ii) structured architectures, consisting of hierarchies of subnets; and iii) feed-back mechanisms, where feed-back information is provided by the external environment to the natural system in response to the system's actions <ref> [50] </ref>. It is the presence of this feed-back interaction with the environment that characterizes all natural systems featuring cognitive capabilities [44]. Some artificial neural systems feature none of the biological properties listed above. <p> In parallel, the study of artificial neural nets as stand-alone systems should evolve to become the science of ecological nets (econets), where neural systems as well as their external environments are modeled <ref> [50] </ref>. For example, unlike Kohonen's networks [5], an ART system employs a structured architecture to self-adjust the network dimension to problem-specific conditions. In particular, the ART orienting subsystem models the responses of the external environment to the learning activities of the attentional subsystem [19], [20], [21], [22].
Reference: [51] <author> Y. Zheng and J. F. Greenleaf, </author> <title> "The effect of concave and convex weight adjustments on self-organizing maps," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 7, no. 1, </volume> <pages> pp. 87-96, </pages> <year> 1996. </year>
Reference-contexts: Attempts have been made to interpret the density of codebook vectors as a model of the input data distribution but with limited success [26],[27], [34], <ref> [51] </ref>. * It should not be employed in topology-preserving mapping when the dimension of the input space is larger than three.
Reference: [52] <author> J. J. </author> <title> Hopfield, "Neural networks and physical systems with emergent collective computational abilities," </title> <journal> Proc. National Academy of Sciences, </journal> <volume> vol. 74, </volume> <pages> pp. 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: It is the presence of this feed-back interaction with the environment that characterizes all natural systems featuring cognitive capabilities [44]. Some artificial neural systems feature none of the biological properties listed above. For example, SOM [4] and the Hopfield network <ref> [52] </ref> are homogeneous systems; they feature no structured architecture and no supervision or reinforcement by, or feedback from, an external environment (also termed supervisor). In reinforcement learning, the neural system is allowed to react to each training case. It is then told whether its reaction was effective or not [53].
Reference: [53] <author> T. </author> <title> Masters, Signal and Image Processing With Neural Networks. A c++ Sourcebook. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1994. </year>
Reference-contexts: In reinforcement learning, the neural system is allowed to react to each training case. It is then told whether its reaction was effective or not <ref> [53] </ref>. To increase their biological plausibility, artificial neural models should employ differentiated structures provided with dishomoge-neous layers, specialized subnets, hierarchies of maps, etc.
Reference: [54] <author> N. B. Karayiannis and P. Pai, </author> <title> "Fuzzy algorithms for learning vector quantization," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 7, no. 5, </volume> <pages> pp. 1196-1211, </pages> <year> 1996. </year>
Reference-contexts: For some authors, "SOM was not intended for pattern classification. Rather, SOM attempts to find topological structures in the input data and display them in one or two dimensions" <ref> [54] </ref>, i.e., SOM can be employed in data visualization tasks because "SOM simply attempts to achieve a consistent spatial mapping of the training vectors to (usually) two dimensions" [2]. <p> FLVQ is also related to several on-line fuzzy clustering algorithms such as the sequential Generalized LVQ (GLVQ) [58] and GLVQ Family algorithms (GLVQ-F) [59], and the class of on-line Fuzzy Algorithms for Learning Vector Quantization (FALVQ, whose proposed instances are termed FALVQ 1, FALVQ 2 and FALVQ 3) <ref> [54] </ref>, [57]. Table 1 summarizes functional comparisons between these learning vector quantization algorithms. For a detailed analysis of these models, refer to [60]. <p> where the smallest number of misclassified patterns is 18 when the number of clusters is 3 [8]; ii) the off-line Fuzzy c-means algorithm affected by 15 misclassifications [74]; iii) the on-line Kohonen VQ algorithm affected by 17 misclassifications [74]; iv) the class of on-line FALVQ algorithms, affected by 16 misclassifications <ref> [54] </ref>; and v) the on-line GLVQ-F algorithms, affected by 16 misclassifications [59]. * FOSART is computationally efficient because its computation time increases linearly as (c+ No. of links). * FOSART can be employed in: a) vector quantization, b) density function estimation, and c) structure detection in input data to be mapped
Reference: [55] <author> N. B. Karayiannis, and M. Ravuri, </author> <title> "An integrated approach to fuzzy learning vector quantization and fuzzy c-means clustering", in Intelligent Engineering Systems Through Artificial Neural Networks, </title> <editor> C. H. Dagli et al., Eds., </editor> <volume> vol. </volume> <pages> 5, </pages> <address> New Yoork, NY: </address> <publisher> ASME Press, pp.247-252,1995. </publisher>
Reference-contexts: Recent advances in the field have presented a broad family of batch FLVQ algorithms formally defined as a class of cost function minimization schemes. Hereafter, this class of batch vector quantizers will be referred to as the Extended FLVQ Family (EFLVQ-F) <ref> [55] </ref>, [56], [57]. FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent. <p> Width of the learning rate GLVQ 3 , GLVQ-F 2 distrib. is constant FCM, EFLVQ-F 1 FALVQ Width of the learning rate distrib. decreases with time FLVQ - Cooling schedule VQ, FALVQ, (learning rate decreases with time) EFLVQ-F 4 GLVQ, GLVQ-F 5 No cooling schedule FCM, FLVQ - 1 see <ref> [55] </ref>, p. 252 (m = 2). 2 see [59], p. 1068 (m = 2). 3 extended to the entire net. 4 see [55], p. 251. 5 see [57], p. 33. 10.1 Input parameters * FLVQ requires the user to define number c of natural groups to be detected. * It requires <p> distrib. decreases with time FLVQ - Cooling schedule VQ, FALVQ, (learning rate decreases with time) EFLVQ-F 4 GLVQ, GLVQ-F 5 No cooling schedule FCM, FLVQ - 1 see <ref> [55] </ref>, p. 252 (m = 2). 2 see [59], p. 1068 (m = 2). 3 extended to the entire net. 4 see [55], p. 251. 5 see [57], p. 33. 10.1 Input parameters * FLVQ requires the user to define number c of natural groups to be detected. * It requires the initial and final weighting exponent m 0 and m f , controlling the "amount of fuzziness" of the algorithm. <p> These theoretical conclusions about transitions of the FLVQ learning strategy as a function of decreasing m (e) (increasing epoch time e) are consistent with those regarding FCM, EFLVQ-F and GLVQ-F <ref> [55] </ref>, [56], [57], [59], all systems employing Equation (14) as their relative membership function (see Table 1). This analysis is also consistent with the heuristic choice 7 &gt; m 0 &gt; m f &gt; 1:1 recommended in [6]. <p> In <ref> [55] </ref>, [56], [57], EFLVQ-F learning schemes are formally derived to minimize a given functional when m is constant. It is also shown that FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent.
Reference: [56] <author> N. B. Karayiannis, J. C. Bezdek, </author> <title> "An integrated approach to fuzzy learning vector quantization and fuzzy c-means clustering", </title> <journal> IEEE Trans. on Fuzzy Systems, </journal> <volume> vol. 5, no. 4, </volume> <pages> pp. 622-628, </pages> <year> 1997. </year>
Reference-contexts: Recent advances in the field have presented a broad family of batch FLVQ algorithms formally defined as a class of cost function minimization schemes. Hereafter, this class of batch vector quantizers will be referred to as the Extended FLVQ Family (EFLVQ-F) [55], <ref> [56] </ref>, [57]. FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent. <p> These theoretical conclusions about transitions of the FLVQ learning strategy as a function of decreasing m (e) (increasing epoch time e) are consistent with those regarding FCM, EFLVQ-F and GLVQ-F [55], <ref> [56] </ref>, [57], [59], all systems employing Equation (14) as their relative membership function (see Table 1). This analysis is also consistent with the heuristic choice 7 &gt; m 0 &gt; m f &gt; 1:1 recommended in [6]. <p> In [55], <ref> [56] </ref>, [57], EFLVQ-F learning schemes are formally derived to minimize a given functional when m is constant. It is also shown that FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent.
Reference: [57] <author> N. B. Karayiannis, </author> " <title> Learning vector quantization: A review," </title> <journal> Int. Journal of Smart Engineering System Design, </journal> <volume> vol. 1, </volume> <pages> pp. 33-58, </pages> <year> 1997. </year>
Reference-contexts: Recent advances in the field have presented a broad family of batch FLVQ algorithms formally defined as a class of cost function minimization schemes. Hereafter, this class of batch vector quantizers will be referred to as the Extended FLVQ Family (EFLVQ-F) [55], [56], <ref> [57] </ref>. FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent. <p> FLVQ is also related to several on-line fuzzy clustering algorithms such as the sequential Generalized LVQ (GLVQ) [58] and GLVQ Family algorithms (GLVQ-F) [59], and the class of on-line Fuzzy Algorithms for Learning Vector Quantization (FALVQ, whose proposed instances are termed FALVQ 1, FALVQ 2 and FALVQ 3) [54], <ref> [57] </ref>. Table 1 summarizes functional comparisons between these learning vector quantization algorithms. For a detailed analysis of these models, refer to [60]. <p> - Cooling schedule VQ, FALVQ, (learning rate decreases with time) EFLVQ-F 4 GLVQ, GLVQ-F 5 No cooling schedule FCM, FLVQ - 1 see [55], p. 252 (m = 2). 2 see [59], p. 1068 (m = 2). 3 extended to the entire net. 4 see [55], p. 251. 5 see <ref> [57] </ref>, p. 33. 10.1 Input parameters * FLVQ requires the user to define number c of natural groups to be detected. * It requires the initial and final weighting exponent m 0 and m f , controlling the "amount of fuzziness" of the algorithm. <p> These theoretical conclusions about transitions of the FLVQ learning strategy as a function of decreasing m (e) (increasing epoch time e) are consistent with those regarding FCM, EFLVQ-F and GLVQ-F [55], [56], <ref> [57] </ref>, [59], all systems employing Equation (14) as their relative membership function (see Table 1). This analysis is also consistent with the heuristic choice 7 &gt; m 0 &gt; m f &gt; 1:1 recommended in [6]. <p> In [55], [56], <ref> [57] </ref>, EFLVQ-F learning schemes are formally derived to minimize a given functional when m is constant. It is also shown that FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent.
Reference: [58] <author> J. C. Bezdek, and N. R. Pal," </author> <title> Generalized clustering networks and Kohonen's self-organizing scheme," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 549-557, </pages> <year> 1993. </year>
Reference-contexts: These include the following: A i;k = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 1 i;k = i Gaussian i;k = e i;k i 2 (0; 1] (Gaussian mixtures; [66], [67]); (3) 1 i;k ) p i 2 (0; 1) <ref> [58] </ref>; (4) (1Gaussian i;k ) 2 2 (1; 1) [25]; (5) where d i;k = d (X k ; T i ) is assumed to be the Euclidean distance between input pattern X k and prototype (receptive field center) T i of the i-th category. <p> Important properties of this cooling schedule have been analyzed in [10], [27], [33], [35], <ref> [58] </ref>, [59]. The second heuristic rule applies to the output lattice of processing units and requires the size of the update (resonance) neighborhood centered on the winner node to decrease monotonically with time, such that a soft competitive learning strategy changes into a hard competitive (WTA) one. <p> FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent. FLVQ is also related to several on-line fuzzy clustering algorithms such as the sequential Generalized LVQ (GLVQ) <ref> [58] </ref> and GLVQ Family algorithms (GLVQ-F) [59], and the class of on-line Fuzzy Algorithms for Learning Vector Quantization (FALVQ, whose proposed instances are termed FALVQ 1, FALVQ 2 and FALVQ 3) [54], [57]. Table 1 summarizes functional comparisons between these learning vector quantization algorithms.
Reference: [59] <author> N. B. Karayiannis, J. C. Bezdek, N. R. Pal, R. J. Hathaway and P. Pai, </author> <title> "Repair to GLVQ: A new family of competitive learning schemes, </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 7, no. 5, </volume> <pages> pp. 1062-1071, </pages> <year> 1996. </year>
Reference-contexts: Important properties of this cooling schedule have been analyzed in [10], [27], [33], [35], [58], <ref> [59] </ref>. The second heuristic rule applies to the output lattice of processing units and requires the size of the update (resonance) neighborhood centered on the winner node to decrease monotonically with time, such that a soft competitive learning strategy changes into a hard competitive (WTA) one. <p> FLVQ updating can be seen as a special case of EFLVQ-F learning schemes for a restricted range of the weighting exponent. FLVQ is also related to several on-line fuzzy clustering algorithms such as the sequential Generalized LVQ (GLVQ) [58] and GLVQ Family algorithms (GLVQ-F) <ref> [59] </ref>, and the class of on-line Fuzzy Algorithms for Learning Vector Quantization (FALVQ, whose proposed instances are termed FALVQ 1, FALVQ 2 and FALVQ 3) [54], [57]. Table 1 summarizes functional comparisons between these learning vector quantization algorithms. For a detailed analysis of these models, refer to [60]. <p> GLVQ-F 2 distrib. is constant FCM, EFLVQ-F 1 FALVQ Width of the learning rate distrib. decreases with time FLVQ - Cooling schedule VQ, FALVQ, (learning rate decreases with time) EFLVQ-F 4 GLVQ, GLVQ-F 5 No cooling schedule FCM, FLVQ - 1 see [55], p. 252 (m = 2). 2 see <ref> [59] </ref>, p. 1068 (m = 2). 3 extended to the entire net. 4 see [55], p. 251. 5 see [57], p. 33. 10.1 Input parameters * FLVQ requires the user to define number c of natural groups to be detected. * It requires the initial and final weighting exponent m 0 <p> These theoretical conclusions about transitions of the FLVQ learning strategy as a function of decreasing m (e) (increasing epoch time e) are consistent with those regarding FCM, EFLVQ-F and GLVQ-F [55], [56], [57], <ref> [59] </ref>, all systems employing Equation (14) as their relative membership function (see Table 1). This analysis is also consistent with the heuristic choice 7 &gt; m 0 &gt; m f &gt; 1:1 recommended in [6]. <p> number of clusters is 3 [8]; ii) the off-line Fuzzy c-means algorithm affected by 15 misclassifications [74]; iii) the on-line Kohonen VQ algorithm affected by 17 misclassifications [74]; iv) the class of on-line FALVQ algorithms, affected by 16 misclassifications [54]; and v) the on-line GLVQ-F algorithms, affected by 16 misclassifications <ref> [59] </ref>. * FOSART is computationally efficient because its computation time increases linearly as (c+ No. of links). * FOSART can be employed in: a) vector quantization, b) density function estimation, and c) structure detection in input data to be mapped in a topologically correct way onto submaps of an output lattice
Reference: [60] <author> A. Baraldi, P. Blonda, F. Parmiggiani, G. Pasquariello and G. Satalino, </author> <title> "Model transitions in descending FLVQ," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 9, no. 5, </volume> <year> 1998. </year> <month> 39 </month>
Reference-contexts: Table 1 summarizes functional comparisons between these learning vector quantization algorithms. For a detailed analysis of these models, refer to <ref> [60] </ref>. <p> This is demonstrated by the following analysis of FLVQ asymptotic behaviors. Asymptotic case A: m (e) ! 1 causes trivial vector quantization. It is easy to verify that <ref> [60] </ref> lim T i (e + 1) = T i (e) + n k=1 1 n X X k (e); i = 1; 2; :::; c: (19) Equation (19) shows that when m (e) ! 1, all input patterns are weighted equally whatever category i may be. <p> Therefore, all FLVQ cluster centers collapse into the same point, i.e., all input patterns are mapped into the same prototype which is the center of gravity of the input data set. FLVQ shares with FCM this asymptotic behavior leading to trivial vector quantization <ref> [60] </ref>. Unfortunately, Equations (14) to (18) applied iteratively are unable to make identical centroids move away from each other, i.e., whenever centroids converge to the center of gravity (grand mean) of the input data set then separate processing units can no longer be adapted to different degrees. <p> We conclude that despite recent advances in the field, the objective function minimized by FLVQ is still unknown, just as the one minimized by SOM is unknown <ref> [60] </ref>. 10.3 Limitations * Since FLVQ does not minimize any known objective function, termination is not based on optimizing any model of the process or its data [7]. * FLVQ is affected by the relative membership problem (high sensitivity to noise, i.e., low robustness [62]). 19 * It does not provide <p> Experimental tests reveal that clustering results improve when convergence is reached at m f values close to 1, while termination parameter * is ignored (see Section 10.1) <ref> [60] </ref>. * It does not provide pdf estimation. * It cannot be employed in topology-preserving mapping. 10.4 Advantages * Owing to its soft competitive implementation, FLVQ is expected to be less likely trapped in local minima and less likely to generate dead units than hard competitive alternatives (e.g., FCM) [9], [27],
Reference: [61] <author> R. Krishnapuram and J. M. Keller, </author> <title> "A possibilistic approach to clustering," </title> <journal> IEEE Trans. on Fuzzy Systems, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 98-110, </pages> <year> 1993. </year>
Reference-contexts: Depending on the conditions required to state that c fuzzy states C i , i = 1; :::; c; are a fuzzy c-partition of the input data set, membership functions can be divided into two categories <ref> [61] </ref>, [62]: 1. relative or probabilistic or constrained fuzzy membership values R i;k ; and 2. absolute or possibilistic fuzzy membership (typicality) values A i;k , where index k ranges over patterns and index i over concepts. <p> Constraint (ii) is an inherently probabilistic constraint <ref> [61] </ref>, relating R i;k values to posterior probability estimates in a Bayesian framework. Because of condition (ii), R i;k values are relative numbers dependent on the absolute membership of the pattern in all other classes, thus indirectly on the total number of classes. <p> 1; :::; c, k = 1; :::; n, may satisfy conditions (i) and (iii) listed above (in this case, the upper bound of the membership function is one and the fuzzy set is termed normal [30]), they always differ from probabilistic memberships in condition (ii), which is relaxed as follows <ref> [61] </ref> (iv) max i fA i;k g &gt; 0, k = 1; :::; n. Owing to condition (iv), the sum of absolute memberships of a noise point in all the "good" categories need not be equal to one. <p> This causes "the relative membership problem of FCM" <ref> [61] </ref>. It means that since Equation (14) provides membership values that are relative numbers, then noise points and outliers may have significantly high membership values and may severely affect the prototype parameter estimate (15).
Reference: [62] <author> R. N. Dave and R. Krishnapuram, </author> <title> "Robust clustering method: a unified view," </title> <journal> IEEE Transactions on Fuzzy Systems, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 270-293, </pages> <year> 1997. </year>
Reference-contexts: Depending on the conditions required to state that c fuzzy states C i , i = 1; :::; c; are a fuzzy c-partition of the input data set, membership functions can be divided into two categories [61], <ref> [62] </ref>: 1. relative or probabilistic or constrained fuzzy membership values R i;k ; and 2. absolute or possibilistic fuzzy membership (typicality) values A i;k , where index k ranges over patterns and index i over concepts. <p> On one hand in probabilistic fuzzy clustering, owing to condition (ii), noise points and outliers, featuring low possibilistic typicalities with respect to all templates (codebooks), may have significantly high probabilistic membership values and may severely affect the prototype parameter estimate (e.g., refer to <ref> [62] </ref>). On the other hand, in possi-bilistic fuzzy clustering, learning rates computed from absolute typicalities tend to produce coincident clusters [62], [63]. <p> and outliers, featuring low possibilistic typicalities with respect to all templates (codebooks), may have significantly high probabilistic membership values and may severely affect the prototype parameter estimate (e.g., refer to <ref> [62] </ref>). On the other hand, in possi-bilistic fuzzy clustering, learning rates computed from absolute typicalities tend to produce coincident clusters [62], [63]. This poor behavior can be explained by the fact that cluster prototypes are uncoupled in possibilistic clustering, i.e., possibilistic clustering algorithms try to minimize an objective function by operating on each cluster independently. This leads to an increase in the number of local minima. <p> Variables i , i and p i are all resolution parameters belonging to range (0; 1) (see <ref> [62] </ref>). 2.2 Fuzzy memberships and mixture probabilities To investigate the relationship between (objective) probability density functions and (useful) fuzzy membership functions, note that absolute membership function (3) relates probabilistic membership (1) to Gaussian mixture models, which are widely employed in the framework of optimization problems featuring a firm statistical foundation [9], <p> of the preset graph G has to match the topological structure of the unknown manifold X [49]. 9.4 Advantages * Owing to its soft competitive implementation, SOM is expected to be less likely trapped in local minima and less likely to generate dead units than hard competitive alternatives [9], [27], <ref> [62] </ref>. * Batch SOM is order independent, i.e., the final weight vectors are not affected by the order of the input sequence. * SOM can be employed as a vector requantization system. For some authors, "SOM was not intended for pattern classification. <p> one minimized by SOM is unknown [60]. 10.3 Limitations * Since FLVQ does not minimize any known objective function, termination is not based on optimizing any model of the process or its data [7]. * FLVQ is affected by the relative membership problem (high sensitivity to noise, i.e., low robustness <ref> [62] </ref>). 19 * It does not provide prototypes with large initial plasticity values required to pursue fast (rough) initial learning. * FLVQ features instability when its traditional termination criterion is employed, such that if P c i=1 d (T i (e); T i (e 1)) 2 *, then FLVQ is terminated. <p> * It does not provide pdf estimation. * It cannot be employed in topology-preserving mapping. 10.4 Advantages * Owing to its soft competitive implementation, FLVQ is expected to be less likely trapped in local minima and less likely to generate dead units than hard competitive alternatives (e.g., FCM) [9], [27], <ref> [62] </ref>. * In FLVQ, due to batch learning, the final weight vectors are not affected by the order of the input sequence when its traditional termination criterion is removed. * With respect to SOM, FLVQ requires a smaller set of input parameters (its learning rate and the size of the update <p> minimize any known objective function, its termination is not based on optimizing any model of the process or its data [7]. * Owing to its hard competitive implementation, Fuzzy ART is more likely to be trapped in local minima and to generate dead units than soft competitive alternatives [9], [27], <ref> [62] </ref>. * Fuzzy ART is order dependent due to on-line learning and example-driven neuron generation. <p> * It combines mini-batch learning techniques (for neuron generation and removal, and for synapse removal) with example-driven generation of synapses. 12.4 Advantages * Owing to its soft competitive implementation, GNG is less likely to be trapped in local minima and to generate dead units than hard competitive alternatives [9], [27], <ref> [62] </ref>. <p> in progress, FOSART features Mean Square Error values larger than those obtained with SOM [72]. 13.4 Advantages * Owing to its soft competitive implementation, FOSART is expected to be less prone to being trapped in local minima and less likely to generate dead units than hard competitive alternatives [9], [27], <ref> [62] </ref>.
Reference: [63] <author> M. Barni, V. Cappellini, and A. Mecocci, </author> <title> "Comments on a possibilistic approach to clustering," </title> <journal> IEEE Trans. Fuzzy Systems, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 393-396, </pages> <year> 1996. </year>
Reference-contexts: On the other hand, in possi-bilistic fuzzy clustering, learning rates computed from absolute typicalities tend to produce coincident clusters [62], <ref> [63] </ref>. This poor behavior can be explained by the fact that cluster prototypes are uncoupled in possibilistic clustering, i.e., possibilistic clustering algorithms try to minimize an objective function by operating on each cluster independently. This leads to an increase in the number of local minima.
Reference: [64] <author> F. Ancona, S. Ridella, S., Rovetta, and R. Zunino, </author> <title> "On the importance of sorting in Neural Gas training of vector quantizers," </title> <booktitle> in Proc. International Conference on Neural Networks '97, </booktitle> <address> Houston, TX, </address> <month> June </month> <year> 1997, </year> <journal> vol. </journal> <volume> 3, </volume> <pages> pp. 1804-1808, </pages> <year> 1997. </year>
Reference-contexts: This also means that Processing Elements (PEs) exploiting a relative membership function as their activation function are context-sensitive, i.e., R i;k provides a tool for modeling network-wide internode communication by assuming that PEs are coupled through feed-sideways (lateral) connections <ref> [64] </ref>. <p> parameters (see Section 7). 9 SOM Starting from Kohonen's on-line Vector Quantization model (VQ, an acronym used herein in line with [4]), which employs a WTA learning strategy, on-line SOM develops a soft competitive learning technique based on two constraints, derived from neurophysiological studies and providing an annealing schedule [9], <ref> [64] </ref>, [65]. <p> Widely employed settings for these parameters are fl ini = 5, and fl fin = 0:01 [9], <ref> [64] </ref>. Thus, learning coefficient h i (t) is monotonically decreasing with neighborhood ranking r i (t) and PE-based epoch counter e i (t) if i 6= w1 (t). Step 4 (b). Non-resonance condition: new processing element allocation.
Reference: [65] <author> E. Erwin, K., Obermayer, and K. Schulten, </author> <title> "Self-organizing maps: ordering, convergence properties and energy functions," </title> <journal> Biol. Cybernetics, </journal> <volume> vol. 67, </volume> <pages> pp. 47-55, </pages> <year> 1992. </year>
Reference-contexts: (see Section 7). 9 SOM Starting from Kohonen's on-line Vector Quantization model (VQ, an acronym used herein in line with [4]), which employs a WTA learning strategy, on-line SOM develops a soft competitive learning technique based on two constraints, derived from neurophysiological studies and providing an annealing schedule [9], [64], <ref> [65] </ref>. <p> long as Equation (11) features (t) &gt; 0 (i.e., as long as the SOM soft learning strategy is not equivalent to a WTA strategy), one cannot specify a cost function that is minimized by Equation (12), i.e., there exists no cost function yielding Equation (12) as its gradient [9], [34], <ref> [65] </ref>. SOM instead features a set of potential functions, one for each node, to be independently minimized following a stochastic (on-line) gradient descent [65]. <p> strategy), one cannot specify a cost function that is minimized by Equation (12), i.e., there exists no cost function yielding Equation (12) as its gradient [9], [34], <ref> [65] </ref>. SOM instead features a set of potential functions, one for each node, to be independently minimized following a stochastic (on-line) gradient descent [65]. In [15], a cost function that leads to an update strategy that is similar to, but not precisely the same as, Equations (11) and (12) is discussed.
Reference: [66] <author> J. R. Williamson, </author> <title> "Gaussian ARTMAP: a neural network for fast incremental learning of noisy multidimensional maps," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 9, no. 5, </volume> <pages> pp. 881-897, </pages> <year> 1996. </year>
Reference-contexts: These include the following: A i;k = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 1 i;k = i Gaussian i;k = e i;k i 2 (0; 1] (Gaussian mixtures; <ref> [66] </ref>, [67]); (3) 1 i;k ) p i 2 (0; 1) [58]; (4) (1Gaussian i;k ) 2 2 (1; 1) [25]; (5) where d i;k = d (X k ; T i ) is assumed to be the Euclidean distance between input pattern X k and prototype (receptive field center) T
Reference: [67] <author> J. R. Williamson, </author> <title> "A constructive, incremental-learning network for mixture model ing and classification," </title> <journal> Neural Computation, </journal> <volume> vol. 9, </volume> <pages> pp. 1517-1543, </pages> <year> 1997. </year> <note> [68] ftp://ftp.sas.com/pub/neural/FAQ. </note>
Reference-contexts: These include the following: A i;k = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 1 i;k = i Gaussian i;k = e i;k i 2 (0; 1] (Gaussian mixtures; [66], <ref> [67] </ref>); (3) 1 i;k ) p i 2 (0; 1) [58]; (4) (1Gaussian i;k ) 2 2 (1; 1) [25]; (5) where d i;k = d (X k ; T i ) is assumed to be the Euclidean distance between input pattern X k and prototype (receptive field center) T i
Reference: [69] <author> F. Y. Shih, J. Moh and F. Chang, </author> " <title> A new ART-based neural architecture for pattern classification and image enhancement without prior knowledge," </title> <journal> Pattern Recognition, </journal> <volume> vol. 25, no. 5, </volume> <pages> pp. 533-542, </pages> <year> 1992. </year>
Reference-contexts: ART 1 categorizes binary patterns but features sensitivity to the order of presentation of the random sequence [21]. This finding led to the development of the Improved ART 1 system (IART 1), which is less dependent than ART 1 on the order of presentation of the input sequence <ref> [69] </ref>. The Adaptive Hamming Net (AHN), which is functionally equivalent to ART 1, optimizes ART 1 both in terms of computation time and storage requirement [23]. ART 2, designed to detect regularities in analog random sequences, employs a computationally expensive architecture which presents difficulties in parameter selection [22]. <p> Experimental evidence <ref> [69] </ref>, as well as theoretical analysis [24], reveal that Fuzzy ART sensitivity to the order of presentation of the input sequence is also due to some inconsistencies detected in the system's implementation [24], [69]. * Fuzzy ART may be severely affected by noise points and outliers, i.e., it may fit the <p> Experimental evidence <ref> [69] </ref>, as well as theoretical analysis [24], reveal that Fuzzy ART sensitivity to the order of presentation of the input sequence is also due to some inconsistencies detected in the system's implementation [24], [69]. * Fuzzy ART may be severely affected by noise points and outliers, i.e., it may fit the noise and not just the data.
Reference: [70] <author> J. Huang, M. Georgiopoulos and G. L. Heileman, </author> <title> "Fuzzy ART properties," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 2, </volume> <pages> pp. 203-213, </pages> <year> 1995. </year>
Reference-contexts: In detail, controls neuron proliferation (see Equation (23)), such that coarser grouping of input patterns is obtained when the vigilance parameter is lowered. Parameters and ff are interrelated as illustrated in <ref> [70] </ref> (when decreases, ff must also decrease). * Learning rate fi is independent of time and set within range (0; 1) (see Equation (24)). * It requires a termination threshold, e.g., the maximum number of epochs e max , i.e., the number of times the finite training data set is repeatedly
Reference: [71] <author> S. Geman, E. Bienenstock and R. Dourstat, </author> <title> "Neural networks and the bias/variance dilemma," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 1-58, </pages> <year> 1992. </year>
Reference-contexts: Minsky claims that a"significant learning at significant rate presupposes some significant prior structure" [47]. In other words, important properties of the model must be "hard-wired or built-in, perhaps to be tuned later by experience, but not learned in any statistical meaningful way" <ref> [71] </ref>. Intuitively, starting from some prior learning structure, experience-based (inductive) fine-tuning of network parameters should allow a structured organization of a distributed system to emerge naturally from elementary interactions of PEs. <p> According to [28], these parameters can be considered as significant prior structures (see Section 4.3), i.e., an important property of the model that must be "hard-wired or built-in, perhaps to be tuned later by experience, but not learned in any statistically meaningful way" <ref> [71] </ref>. * In several tests regarding satellite image clustering that are in progress, FOSART features Mean Square Error values larger than those obtained with SOM [72]. 13.4 Advantages * Owing to its soft competitive implementation, FOSART is expected to be less prone to being trapped in local minima and less likely
Reference: [72] <author> P. Blonda, G. Satalino, A. Baraldi and A. Bognani, </author> <title> "Neuro-fuzzy analysis of remote sensed antarctic data," </title> <booktitle> in Proc. of the II Italian Workshop on Fuzzy Logic, </booktitle> <address> Bari, Italy, </address> <month> March </month> <year> 1997, </year> <editor> P. Blonda, M. Castellano, A. Petrosino, Eds. </editor> <publisher> Singapore: World Scientific, </publisher> <year> 1998. </year>
Reference-contexts: property of the model that must be "hard-wired or built-in, perhaps to be tuned later by experience, but not learned in any statistically meaningful way" [71]. * In several tests regarding satellite image clustering that are in progress, FOSART features Mean Square Error values larger than those obtained with SOM <ref> [72] </ref>. 13.4 Advantages * Owing to its soft competitive implementation, FOSART is expected to be less prone to being trapped in local minima and less likely to generate dead units than hard competitive alternatives [9], [27], [62].
Reference: [73] <author> B. Fritzke, </author> <title> "Growing cell structures A self-organizing network for unsupervised and supervised learning," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 9, </volume> <pages> pp. 1441-1460, </pages> <year> 1994. </year>
Reference-contexts: and may reduce computation time with respect to random initialization. * Fuzzy ART can be employed as a vector requantization system. 11.5 Architectural features The main features of Fuzzy ART are summarized in Table 2. 12 GNG GNG combines the growth mechanism inherited from the earlier proposed Growing Cell Structures <ref> [73] </ref> with the synapse geration rule CHR [49]. GNG is capable of generating and removing both synaptic links and PEs, i.e, GNG belongs to the class of FSONN models (see Section 5). <p> Typical values are: * 1 = 0:05, * n = 0:0006 [27]. * Parameter 2 N + controls neuron generation at adaptation steps (see above). * Parameters ff and fi are used to decrease the error variables, so that more recent signals are weighted more heavily than previous ones <ref> [73] </ref>. Typical values are: ff = 0:5, fi = 0:0005 [27]. * Parameter a max 2 N + is the maximum age of a synaptic link. <p> accumulated errors, it is robust against noise, and this avoids overfitting. * In the tests provided involving the 2-D two-spiral data set (194 patterns, 2 classes), a two-stage hybrid classifier (supervised + unsupervised) exploiting GNG as its first layer performs better classification than other distributed systems found in the literature <ref> [73] </ref>. <p> This choice reduces the risk of dead unit formation and may reduce computation time with respect to random initialization. * In the tests provided involving the 2-D Simpson data set (24 patterns) [8], the 2-D two-spirals data set (194 patterns, 2 classes) <ref> [73] </ref>, a 3-D digitized human face (9743 patterns) [18], and the 4-D IRIS data set (150 patterns, 3 classes), FOSART performances are competitive with or better than those of other clustering models found in the literature (VQ, Fuzzy Min-Max, FCM, FLVQ, FALVQ, GLVQ-F, ING).
Reference: [74] <author> S. Kim and and S. Mitra, </author> <title> "Integrated Adaptive Fuzzy Clustering (IAFC) algorithm," </title> <booktitle> in Proc. of the Second IEEE International Conference on Fuzzy Systems, </booktitle> <volume> vol. 2, </volume> <pages> pp. 1264-1268, </pages> <year> 1993. </year>
Reference-contexts: In this case FOSART, scoring 11 mismatches, performs better than: i) the Fuzzy Min-Max neural network model, where the smallest number of misclassified patterns is 18 when the number of clusters is 3 [8]; ii) the off-line Fuzzy c-means algorithm affected by 15 misclassifications <ref> [74] </ref>; iii) the on-line Kohonen VQ algorithm affected by 17 misclassifications [74]; iv) the class of on-line FALVQ algorithms, affected by 16 misclassifications [54]; and v) the on-line GLVQ-F algorithms, affected by 16 misclassifications [59]. * FOSART is computationally efficient because its computation time increases linearly as (c+ No. of links). <p> 11 mismatches, performs better than: i) the Fuzzy Min-Max neural network model, where the smallest number of misclassified patterns is 18 when the number of clusters is 3 [8]; ii) the off-line Fuzzy c-means algorithm affected by 15 misclassifications <ref> [74] </ref>; iii) the on-line Kohonen VQ algorithm affected by 17 misclassifications [74]; iv) the class of on-line FALVQ algorithms, affected by 16 misclassifications [54]; and v) the on-line GLVQ-F algorithms, affected by 16 misclassifications [59]. * FOSART is computationally efficient because its computation time increases linearly as (c+ No. of links). * FOSART can be employed in: a) vector quantization, b) density
Reference: [75] <author> B. Fritzke. </author> <type> Personal communication, </type> <year> 1997. </year>
Reference-contexts: In <ref> [75] </ref>, it is anticipated that the future development of GNG will employ, besides the neuron insertion criterion described above, the following rule for neuron removal: every adaptation steps, the unit featuring lowest utility for error reduction is removed.
Reference: [76] <author> B. Fritzke., </author> <title> "The LBG-U method for vector quantization An improvement over LBG inspired from neural networks," </title> <journal> Neural Processing Letters, </journal> <volume> vol. 5, no. 1, </volume> <year> 1997. </year> <month> 40 </month>
Reference-contexts: The utility U (T w1 (t) (t)) of template vector T w1 (t) (t), w1 (t) 2 f1; cg, where c is the total number of neurons and t is the number of input pattern presentations is defined as follows <ref> [76] </ref>: U (T w1 (t) (t)) = X (t fl )2fM w1 (t) g where d (X (t fl ); T w1 (t fl ) (t fl )) is the Euclidean inter-pattern distance between vectors X (t fl ) and T w1 (t fl ) (t fl ) at presentation time
References-found: 75

