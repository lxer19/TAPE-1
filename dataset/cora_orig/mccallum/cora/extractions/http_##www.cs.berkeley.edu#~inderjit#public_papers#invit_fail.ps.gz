URL: http://www.cs.berkeley.edu/~inderjit/public_papers/invit_fail.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~inderjit/
Root-URL: 
Title: Current inverse iteration software can fail  
Author: Inderjit S. Dhillon 
Keyword: Inverse iteration, symmetric, tridiagonal matrix, eigenvalues, eigenvectors.  
Note: AMS subject classification 15A18, 65F15, 65F25.  
Date: February 15, 1998  
Address: San Jose, California  
Affiliation: IBM Almaden Research Center  
Abstract: Inverse Iteration is widely used to compute the eigenvectors of a matrix once accurate eigen-values are known. We discuss various issues involved in any implementation of inverse iteration for real, symmetric matrices. Current implementations resort to reorthogonalization when eigenvalues agree to more than three digits relative to the norm. Such reorthogonalization can have unexpected consequences. Indeed, as we show in this paper, the implementations in EIS-PACK [18] and LAPACK [1] may fail. We illustrate with both theoretical and empirical failures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <note> LAPACK Users' Guide (second edition). SIAM, Philadelphia, </note> <year> 1995. </year> <pages> 324 pages. </pages>
Reference-contexts: In this paper, we list the various issues involved in implementing inverse iteration. We discuss the plausible choices that may be made at each step, illustrating with the choices made in EISPACK [18] and LAPACK <ref> [1] </ref>. The task of computing eigenvectors has been extensively studied since the 1960s and there is considerable inverse iteration software in the public domain that is available for use. One would expect that software for this well-defined and seemingly "simple" task would come with proofs of correctness. <p> We now look in detail at two existing implementations of inverse iteration and see how they address the issues discussed in the previous section. EISPACK [18] and LAPACK <ref> [1] </ref> are linear algebra software libraries that contain routines to solve various eigenvalue problems. <p> error, where there is no gradual underflow, occurs on (1=")T where T is as in (4.10). tu In contrast, xStein chooses its scale factor to be t = kbk 1 The only significant difference from the choice in Tinvit is the term max ("; jU nn j) instead of " <ref> [1, 12] </ref>. However this difference introduces a serious error not present in Tinvit, which we now explain. Suppose approximates 1 in (4.7). When = 1 , it can be proved that U nn must be zero in exact arithmetic. <p> It checks to see if overflow would occur, and if so, perturbs tiny entries on the diagonal of U <ref> [1, 12] </ref>. This check is in the inner loop when solving U y = x where x = t L 1 P 1 b. Coupled with the extra iterations done after convergence, this results in xStein being slower than Tinvit.
Reference: [2] <author> S. Chandrasekaran. </author> <title> When is a Linear System Ill-Conditioned? PhD thesis, </title> <institution> Departement of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <year> 1994. </year>
Reference-contexts: Even when a very accurate eigenvalue approximation is available, the following may influence the choice of the shift when more than one eigenvector is desired. * The pairing problem. In <ref> [2] </ref>, Chandrasekaran gives a surprising example showing how inverse iteration can fail to give small residuals in exact arithmetic if the eigenvalues and eigenvectors are not paired up properly. We reproduce the example in Section 4. <p> In particular, approximations to small eigenvalues are often incorrect in all their digits (eigenvalues found by the QR algorithm and the Divide and Conquer method can have an error as large as O ("kT k)). In response to such problems, Chandrasekaran proposed a new version of inverse iteration in <ref> [2] </ref> that is considerably different from the EISPACK and LAPACK implementations. The differences include an alternate convergence criterion. <p> Surprisingly, as the following example shows, this implementation can fail to give small residual norms even in exact arithmetic by incorrectly pairing up the eigenvalues and eigenvectors. Example 4.1 [The Pairing Error.] (Chandrasekaran <ref> [2] </ref>) Let 1 be an arbitrary real number, and where " is the machine precision. Explicitly, i+1 = 1 + 2 i1 ". <p> To cure this problem, Chandrasekaran proposed that ^ i O (n"kAk) be used as the shifts for inverse iteration so that each shift is guaranteed to lie to the left of its true eigenvalue <ref> [2] </ref>. As we shall see later, neither EISPACK nor LAPACK perform this `artificial' perturbation. The discerning reader will realize that the above problem is not the failure of the basic inverse iteration process. <p> However, as we saw in earlier sections, the goal of orthogonality (2.2) can lead to a myriad of problems. Most of the "difficult" errors in the EISPACK and LAPACK implementations arise due to the explicit orthogonalization of iterates when eigenvalues are close. In <ref> [2] </ref>, Chandrasekaran gives an explanation of these failures, and proposes a more robust version of inverse iteration. However, this new version is more involved and potentially requires much more orthogonalization than existing implementations.
Reference: [3] <author> S. Chandrasekaran, </author> <year> 1996. </year> <title> private communication. </title>
Reference-contexts: However, this new version is more involved and potentially requires much more orthogonalization than existing implementations. There is no current plan to widely distribute software based on this new version of inverse iteration <ref> [3] </ref>. Recently, there has been much work on another approach for computing numerically orthogonal approximations to eigenvectors. However this work is still ongoing and beyond the scope of this paper. The interested reader is requested to see [5, 15] and await [6]. 17 Acknowledgements.
Reference: [4] <author> J. Demmel and A. McKenney. </author> <title> A test matrix generation suite. </title> <institution> Computer science dept. </institution> <type> technical report, </type> <institution> Courant Institute, </institution> <address> New York, NY, </address> <month> July </month> <year> 1989. </year> <note> (LAPACK Working Note #9). </note>
Reference-contexts: We now give an example where this perturbation is too big. As a result, the shifts used to compute the eigenvectors are quite different from the true eigenvalues and prevent the convergence criterion from being attained. Example 4.2 [Excessive Perturbation.] Using LAPACK's test matrix generator <ref> [4] </ref>, we generated a 200 fi 200 tridiagonal matrix such that ^ 1 ^ 100 "; ^ 101 ^ 199 "; ^ n = 1 Tinvit (T , ^ ) /* Tinvit computes all the eigenvectors of T given the computed eigenvalues ^ 1 ; ^ 2 ; : : :
Reference: [5] <author> I. S. Dhillon. </author> <title> A New O(n 2 ) Algorithm for the Symmetric Tridiagonal Eigenvalue/Eigenvector Problem. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, California, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: Knowing the position of a large component of v also enables us to avoid the possibility of overflow in the eigenvector computation. See <ref> [5, 15, 8, 10, 9] </ref> for more details. IV. & V. Convergence criterion and Orthogonality. It is easy to find a criterion that guarantees small residual norms, see goal (2.1). However, as we saw in earlier sections, the goal of orthogonality (2.2) can lead to a myriad of problems. <p> Recently, there has been much work on another approach for computing numerically orthogonal approximations to eigenvectors. However this work is still ongoing and beyond the scope of this paper. The interested reader is requested to see <ref> [5, 15] </ref> and await [6]. 17 Acknowledgements. I would like to thank Professors B.N.Parlett, J.W.Demmel, Axel Ruhe and an anonymous referee for a careful reading of the manuscript and for many useful suggestions.
Reference: [6] <author> I.S. Dhillon and B.N. Parlett. </author> <title> Computing the eigenvectors of a symmetric tridiagonal matrix. </title> <note> in preparation. </note>
Reference-contexts: Recently, there has been much work on another approach for computing numerically orthogonal approximations to eigenvectors. However this work is still ongoing and beyond the scope of this paper. The interested reader is requested to see [5, 15] and await <ref> [6] </ref>. 17 Acknowledgements. I would like to thank Professors B.N.Parlett, J.W.Demmel, Axel Ruhe and an anonymous referee for a careful reading of the manuscript and for many useful suggestions.
Reference: [7] <author> J. DuCroz, </author> <month> December </month> <year> 1994. </year> <title> private communication. </title>
Reference-contexts: A similar overflow occurrence (in IEEE double precision arithmetic) on an 8fi8 matrix, with a largest element of magnitude 2 484 10 145 , was reported to us by Jeremy DuCroz <ref> [7] </ref>. The problems reported in the above two examples can be cured by reverting back to the choice of scale factor in EISPACK's Tinvit. IV. Convergence Criterion. Both Tinvit and xStein judge the quality of an approximate eigenvector by the norm growth obtained in solving the linear system (4.7).
Reference: [8] <author> K. V. Fernando. </author> <title> On computing an eigenvector of a tridiagonal matrix part I: Basic results. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 18(4) </volume> <pages> 1013-1034, </pages> <month> October </month> <year> 1997. </year>
Reference-contexts: Knowing the position of a large component of v also enables us to avoid the possibility of overflow in the eigenvector computation. See <ref> [5, 15, 8, 10, 9] </ref> for more details. IV. & V. Convergence criterion and Orthogonality. It is easy to find a criterion that guarantees small residual norms, see goal (2.1). However, as we saw in earlier sections, the goal of orthogonality (2.2) can lead to a myriad of problems.
Reference: [9] <author> S. K. Godunov, A. G. Antonov, O. P. Kiriljuk, and V. I. Kostin. </author> <title> Guaranted Accuracy in Numerical Linear Algebra. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1993. </year> <note> A revised translation of a Russian text first published in 1988 in Novosibirsk. </note>
Reference-contexts: Knowing the position of a large component of v also enables us to avoid the possibility of overflow in the eigenvector computation. See <ref> [5, 15, 8, 10, 9] </ref> for more details. IV. & V. Convergence criterion and Orthogonality. It is easy to find a criterion that guarantees small residual norms, see goal (2.1). However, as we saw in earlier sections, the goal of orthogonality (2.2) can lead to a myriad of problems.
Reference: [10] <author> S. K. Godunov, V. I. Kostin, and A. D. Mitchenko. </author> <title> Computation of an eigenvector of symmetric tridiagonal matrices. </title> <journal> Siberian Math. J., </journal> <volume> 26 </volume> <pages> 71-85, </pages> <year> 1985. </year>
Reference-contexts: Knowing the position of a large component of v also enables us to avoid the possibility of overflow in the eigenvector computation. See <ref> [5, 15, 8, 10, 9] </ref> for more details. IV. & V. Convergence criterion and Orthogonality. It is easy to find a criterion that guarantees small residual norms, see goal (2.1). However, as we saw in earlier sections, the goal of orthogonality (2.2) can lead to a myriad of problems.
Reference: [11] <author> Alston S. </author> <title> Householder. Unitary triangularization of a nonsymmetric matrix. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 5 </volume> <pages> 339-342, </pages> <year> 1958. </year>
Reference-contexts: However, as we will see in Section 4.1, xStein also suffers from some of the same problems as Tinvit in addition to introducing a new serious error. Both EISPACK and LAPACK solve the dense symmetric eigenproblem by reducing the dense matrix to tridiagonal form by Householder transformations <ref> [11] </ref>, and then finding the eigenvalues and eigenvectors of the tridiagonal matrix. Both Tinvit and xStein operate on a symmetric tridiagonal matrix.
Reference: [12] <author> I. C. F. Ipsen. </author> <title> Computing an eigenvector with inverse iteration. </title> <journal> SIAM Review, </journal> <volume> 39(2) </volume> <pages> 254-291, </pages> <year> 1997. </year>
Reference-contexts: error, where there is no gradual underflow, occurs on (1=")T where T is as in (4.10). tu In contrast, xStein chooses its scale factor to be t = kbk 1 The only significant difference from the choice in Tinvit is the term max ("; jU nn j) instead of " <ref> [1, 12] </ref>. However this difference introduces a serious error not present in Tinvit, which we now explain. Suppose approximates 1 in (4.7). When = 1 , it can be proved that U nn must be zero in exact arithmetic. <p> It checks to see if overflow would occur, and if so, perturbs tiny entries on the diagonal of U <ref> [1, 12] </ref>. This check is in the inner loop when solving U y = x where x = t L 1 P 1 b. Coupled with the extra iterations done after convergence, this results in xStein being slower than Tinvit.
Reference: [13] <author> E. Jessup and I. Ipsen. </author> <title> Improving the accuracy of inverse iteration. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13(2) </volume> <pages> 550-572, </pages> <year> 1992. </year>
Reference-contexts: A random starting vector is a popular choice since the probability that it has a negligible component in the desired direction is extremely low, see <ref> [13] </ref> for a detailed study. III. Scaling of right hand side. Equation (3.4) implies that kv (1) k j~ 1 t (1) j=j 1 j where t (1) is the scale factor in the first iteration of (3.3). <p> In order to achieve accuracy comparable to that of the Divide and Conquer and QR/QL methods, the search for a better implementation of inverse iteration led to xStein <ref> [13] </ref>. However, as we will see in Section 4.1, xStein also suffers from some of the same problems as Tinvit in addition to introducing a new serious error. <p> On the other hand, xStein chooses a random starting vector, each of whose elements comes from a uniform (1; 1) distribution. Neither choice of starting vectors is likely to be pathologically deficient in the desired eigenvector. The random starting vectors are designed to be superior to Tinvit's choice <ref> [13] </ref>. III. Scaling of right hand side. Both Tinvit and xStein solve the linear system (T I)y = t b as P LU y = t b (4.7) at each iteration. Suppose that is an approximation to 1 . <p> The hope is to get greater linear independence of the iterates before the MGS step <ref> [13] </ref>. However, as we now show, the Tinvit error as reported above persists due to inaccuracies in the small eigenvalues. Example 4.7 [Linear Dependence Persists.] Consider again the matrix T given in (4.13). <p> I. Choice of shift. Of the various issues discussed in this paper, the choice of starting vector and convergence criterion have been extensively studied <ref> [20, 16, 17, 13] </ref>. Surprisingly, the choice of shift for inverse iteration seems to have drawn little attention. We feel that the shift is probably the most important variable in inverse iteration. Examples 4.6 and 4.7 highlight the importance of shifts that are as accurate as possible. II. & III.
Reference: [14] <author> B. Parlett. </author> <title> The Symmetric Eigenvalue Problem. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <note> second edition, </note> <year> 1997. </year>
Reference-contexts: It is common practice nowadays to compute eigenvalues first, and then invoke inverse iteration with very accurate . Due to the fundamental limitations of finite precision arithmetic, eigenvalues of symmetric matrixes can, in general, only be computed to a guaranteed accuracy of O (n"kAk) <ref> [14] </ref>. Even when a very accurate eigenvalue approximation is available, the following may influence the choice of the shift when more than one eigenvector is desired. * The pairing problem. <p> However when eigenvalues are close, goal (2.2) is not automatic. As we now discuss, the methods used to compute numerically orthogonal vectors can impact the choice of the convergence criterion. V. Orthogonality. Standard perturbation theory <ref> [14, Section 11-7] </ref> says that if ^v is a unit vector, is the eigenvalue closest to and v is 's eigenvector then j sin 6 (v; ^v)j gap () where gap () = min i 6= j i j.
Reference: [15] <author> B.N. Parlett and I.S. Dhillon. </author> <title> Fernando's solution to Wilkinson's problem: An application of double factorization. </title> <journal> Lin. Alg. Appl., </journal> <volume> 267 </volume> <pages> 247-279, </pages> <year> 1997. </year> <month> 18 </month>
Reference-contexts: Knowing the position of a large component of v also enables us to avoid the possibility of overflow in the eigenvector computation. See <ref> [5, 15, 8, 10, 9] </ref> for more details. IV. & V. Convergence criterion and Orthogonality. It is easy to find a criterion that guarantees small residual norms, see goal (2.1). However, as we saw in earlier sections, the goal of orthogonality (2.2) can lead to a myriad of problems. <p> Recently, there has been much work on another approach for computing numerically orthogonal approximations to eigenvectors. However this work is still ongoing and beyond the scope of this paper. The interested reader is requested to see <ref> [5, 15] </ref> and await [6]. 17 Acknowledgements. I would like to thank Professors B.N.Parlett, J.W.Demmel, Axel Ruhe and an anonymous referee for a careful reading of the manuscript and for many useful suggestions.
Reference: [16] <author> G. Peters and J.H. Wilkinson. </author> <title> The calculation of specified eigenvectors by inverse iteration, contribution II/18, </title> <booktitle> volume II of Handbook of Automatic Computation, </booktitle> <pages> pages 418-439. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, Heidelberg, Berlin, </address> <year> 1971. </year>
Reference-contexts: I. Choice of shift. Of the various issues discussed in this paper, the choice of starting vector and convergence criterion have been extensively studied <ref> [20, 16, 17, 13] </ref>. Surprisingly, the choice of shift for inverse iteration seems to have drawn little attention. We feel that the shift is probably the most important variable in inverse iteration. Examples 4.6 and 4.7 highlight the importance of shifts that are as accurate as possible. II. & III.
Reference: [17] <author> G. Peters and J.H. Wilkinson. </author> <title> Inverse iteration, ill-conditioned equations and Newton's method. </title> <journal> SIAM Review, </journal> <volume> 21 </volume> <pages> 339-360, </pages> <year> 1979. </year>
Reference-contexts: The inverse iteration process is widely used to find the eigenvectors of a symmetric tridiagonal matrix T . Earlier fears about loss of accuracy due to the near singularity of T I were allayed in <ref> [17] </ref>. Inverse iteration can easily deliver a vector ^v that has a small residual, i.e. small k (T I )^vk, whenever is close to . However a small residual does not guarantee orthogonality when eigenvalues are close together. <p> However Wilkinson showed that the errors made in computing v (i+1) , although large, are almost entirely in the direction of v 1 when 1 is isolated. Since we are interested only in computing the direction of v 1 these errors pose no danger, see <ref> [17] </ref>. Thus to compute the eigenvector of an isolated eigenvalue, the more accurate the shift is, the better is the approximate eigenvector. It is common practice nowadays to compute eigenvalues first, and then invoke inverse iteration with very accurate . <p> I. Choice of shift. Of the various issues discussed in this paper, the choice of starting vector and convergence criterion have been extensively studied <ref> [20, 16, 17, 13] </ref>. Surprisingly, the choice of shift for inverse iteration seems to have drawn little attention. We feel that the shift is probably the most important variable in inverse iteration. Examples 4.6 and 4.7 highlight the importance of shifts that are as accurate as possible. II. & III.
Reference: [18] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix Eigensystem Routines - EISPACK Guide, </title> <booktitle> volume 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1976. </year>
Reference-contexts: In this paper, we list the various issues involved in implementing inverse iteration. We discuss the plausible choices that may be made at each step, illustrating with the choices made in EISPACK <ref> [18] </ref> and LAPACK [1]. The task of computing eigenvectors has been extensively studied since the 1960s and there is considerable inverse iteration software in the public domain that is available for use. One would expect that software for this well-defined and seemingly "simple" task would come with proofs of correctness. <p> We now look in detail at two existing implementations of inverse iteration and see how they address the issues discussed in the previous section. EISPACK <ref> [18] </ref> and LAPACK [1] are linear algebra software libraries that contain routines to solve various eigenvalue problems.
Reference: [19] <author> J. H. Wilkinson. </author> <title> The calculation of the eigenvectors of codiagonal matrices. </title> <journal> Computer J., </journal> <volume> 1 </volume> <pages> 90-96, </pages> <year> 1958. </year>
Reference-contexts: an efficient procedure to find such a k, Wilkinson proposed choosing P Le as the starting vector, i.e., solving U v (1) = e in the first iteration, where T I = P LU is the LU decomposition obtained by partial pivoting and e is the vector of all 1's <ref> [19, 20] </ref>. A random starting vector is a popular choice since the probability that it has a negligible component in the desired direction is extremely low, see [13] for a detailed study. III. Scaling of right hand side.
Reference: [20] <author> J. H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1965. </year> <month> 19 </month>
Reference-contexts: The information presented here does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. 1 already in the cluster. Unfortunately, this strategy is not foolproof as recognized by Wilkinson <ref> [20, p.344] </ref> : `Inverse Iteration gives a very satisfactory solution to the problem as far as reasonably well-separated eigenvalues are concerned. The problem of determining reliably full digital information in the subspace spanned by eigenvectors corresponding to coincident or pathologically close eigenvalues has never been satisfactorily solved'. <p> The solution v (i+1) in (3.3) is very sensitive to small changes in when there is more than one eigenvalue near . In <ref> [20, p.329] </ref>, Wilkinson notes that `The extreme sensitivity of the computed eigenvector to very small changes in [ in our notation] may be turned to practical advantage and used to obtain independent eigenvectors corresponding to coincident or pathologically close eigenvalues'. <p> From (3.4), assuming that j 1 j t j i j for i 6= 1, v (1) is a good approximation to v 1 provided that ~ 1 is not "negligible", i.e., the starting vector b must have a non-negligible component in the direction of the desired eigenvector. In <ref> [20, pp.315-321] </ref>, Wilkinson investigates and rejects the choice of e 1 or e n as a starting vector 4 (where e i is the ith column of the n fi n identity matrix). <p> an efficient procedure to find such a k, Wilkinson proposed choosing P Le as the starting vector, i.e., solving U v (1) = e in the first iteration, where T I = P LU is the LU decomposition obtained by partial pivoting and e is the vector of all 1's <ref> [19, 20] </ref>. A random starting vector is a popular choice since the probability that it has a negligible component in the desired direction is extremely low, see [13] for a detailed study. III. Scaling of right hand side. <p> To guarantee (2.1), v (i+1) is usually accepted when the norm growth is O (1=n"kAk), see <ref> [20, p.324] </ref> for details. For the basic iteration of (3.3) this convergence criterion can always be met in a few iterations, provided the starting vector is not pathologically deficient in the desired eigenvector and is within O (n"kAk) of the true eigenvalue. <p> We now compare and contrast how these implementations handle the various issues discussed in Section 3. I. Choice of shift. Even though in exact arithmetic all the eigenvalues of an unreduced tridiagonal matrix are distinct, some of the computed eigenvalues may be identical to working accuracy. In <ref> [20, p.329] </ref>, Wilkinson recommends that pathologically close eigenvalues be perturbed by a small amount in order to get an orthogonal basis of the desired subspace. <p> of an eigenvector of a tridiagonal matrix can 2 In the summer of 1996, a core dump on the main computer aboard the Ariane 5 rocket was interpreted as flight data, causing a violent trajectory correction that led to the disintegration of the rocket 13 frequently be very small, see <ref> [20, pp.317-321] </ref>. In such a case, xStein can choose an unnec-essary large scale factor which can occasionally lead to overflow, as shown by the following example. Example 4.5 [Undeserved overflow.] Consider the matrix given above in (4.13). <p> I. Choice of shift. Of the various issues discussed in this paper, the choice of starting vector and convergence criterion have been extensively studied <ref> [20, 16, 17, 13] </ref>. Surprisingly, the choice of shift for inverse iteration seems to have drawn little attention. We feel that the shift is probably the most important variable in inverse iteration. Examples 4.6 and 4.7 highlight the importance of shifts that are as accurate as possible. II. & III.
References-found: 20

