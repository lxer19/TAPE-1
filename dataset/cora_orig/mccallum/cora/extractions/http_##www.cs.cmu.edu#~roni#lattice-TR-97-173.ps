URL: http://www.cs.cmu.edu/~roni/lattice-TR-97-173.ps
Refering-URL: 
Root-URL: 
Title: Lattice based language models  
Author: Pierre Dupont and Ronald Rosenfeld 
Note: Project sponsored by the National Security Agency under Grant No. MDA904-97-1-0006. The United States Government is authorized to reproduce and distribute reprints notwithstanding any copyright notation hereon. Current address:  
Address: Pittsburgh, PA 15213  Universite Jean Monnet, 23, rue P. Michelon, 42023 Saint-Etienne cedex, France,  
Affiliation: School of Computer Science Carnegie Mellon University  Dept. Math.,  
Pubnum: CMU-CS-97-173  
Email: E-mail: pdupont@univ-st-etienne.fr  
Date: September 1997  
Abstract: This paper introduces lattice based language models, a new language modeling paradigm. These models construct multi-dimensional hierarchies of partitions and select the most promising partitions to generate the estimated distributions. We discussed a specific two dimensional lattice and propose two primary features to measure the usefulness of each node: the training-set history count and the smoothed entropy of its prediction. Smoothing techniques are reviewed and a generalization of the conventional backoff strategy to multiple dimensions is proposed. Preliminary experimental results are obtained on the SWITCHBOARD corpus which lead to a 6.5 % perplexity reduction over a word trigram model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Jelinek and R. Mercer. </author> <title> Interpolated estimation of markov source parameters from sparse data. </title> <editor> In E. Gelsema and L. Kanal, editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 381-397. </pages> <publisher> North-Holland, </publisher> <address> Ams-terdam, </address> <year> 1980. </year>
Reference-contexts: makes the simplifying assumption: Pr (wjh) = Pr (w i jw 1 ; w 2 ; : : : ; w i1 ) Pr (w i jw iN+1 ; : : : ; w i1 ) N-gram models have dominated statistical language modeling ever since their introduction in the 1970's <ref> [1] </ref>. In spite of their apparent limitations, N-gram models proved simple, robust, and surprisingly hard to improve on ([2]). Within the N-gram paradigm, much work was done on smoothing, word clustering and adaptation.
Reference: [2] <author> F. Jelinek. </author> <title> Up from trigrams, the struggle for improved language models. </title> <booktitle> In European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1037-1040, </pages> <address> Berlin, </address> <year> 1993. </year>
Reference: [3] <author> I.J.Good. </author> <title> The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40 </volume> <pages> 237-264, </pages> <year> 1953. </year>
Reference: [4] <author> H. Ney, U. Essen, and R. Kneser. </author> <title> On structuring probabilistic dependences in stochastic language modelling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 1-38, </pages> <year> 1994. </year>
Reference: [5] <author> S.M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustic, Speech and Signal Processing, </journal> <volume> 35(3) </volume> <pages> 400-401, </pages> <year> 1987. </year>
Reference: [6] <author> P. Brown, V. Della Pietra, P. de Souza, J. Lai, and R. Mercer. </author> <title> Class-based N-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <year> 1992. </year>
Reference: [7] <author> R. Kneser and H. Ney. </author> <title> Forming word classes by statistical clustering for statistical language modeling. </title> <booktitle> In Proceedings of the 1st QUALICO Conference, </booktitle> <address> Trier, Germany, </address> <month> September </month> <year> 1991. </year>
Reference: [8] <author> R. Kuhn. </author> <title> Speech recognition and the frequency of recently used words: A modified markov model for natural language. </title> <booktitle> In COLING 88, 12th International Conference on Computational Linguistics, </booktitle> <pages> pages 348-350, </pages> <address> Budapest, </address> <month> August </month> <year> 1988. </year> <month> 24 </month>
Reference: [9] <author> R. Kuhn and R. De Mori. </author> <title> A cache-based natural language model for speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(6) </volume> <pages> 570-583, </pages> <year> 1990. </year>
Reference: [10] <author> R. Kuhn and R. De Mori. </author> <title> Correction to a cache-based natural language model for speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(6) </volume> <pages> 691-692, </pages> <month> June </month> <year> 1992. </year>
Reference: [11] <author> F. Jelinek, B. Merialdo, S. Roukos, and M. Strauss. </author> <title> A dynamic language model for speech recognition. </title> <booktitle> In Proceedings of the DARPA Workshop on Speech and Natural Language, </booktitle> <pages> pages 293-295, </pages> <month> February </month> <year> 1991. </year>
Reference: [12] <author> L. Bahl, P. Brown, P. de Souza, and R. Mercer. </author> <title> A tree-based statistical language model for natural language speech recognition. </title> <journal> IEEE Transactions on Acoustic, Speech and Signal Processing, </journal> <volume> 37(7) </volume> <pages> 1001-1008, </pages> <year> 1989. </year>
Reference: [13] <author> S. Della Pietra, V. Della Pietra, R. Mercer, and S. Roukos. </author> <title> Adaptive language modeling using minimum discriminant estimation. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 633-636, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year>
Reference: [14] <author> R. Lau, R. Rosenfeld, and S. Roukos. </author> <title> Trigger-based language models: a maximum entropy approach. </title> <booktitle> In International Conference on Acoustic, Speech and Signal Processing, </booktitle> <volume> volume II, </volume> <pages> pages 45-48, </pages> <year> 1993. </year>
Reference: [15] <author> R. Rosenfeld. </author> <title> Adaptive Statistical Language Modeling: a Maximum Entropy Approach. </title> <editor> Ph. D. </editor> <title> dissertation, </title> <type> TR CMU-CS-94-138, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference: [16] <author> A. Berger, S. Della Pietra, and V. Della Pietra. </author> <title> A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1), </volume> <month> March </month> <year> 1996. </year>
Reference: [17] <author> R. Rosenfeld. </author> <title> A maximum entropy approach to adaptive statistical language modeling. </title> <booktitle> Computer, Speech and Language, </booktitle> <volume> 10 </volume> <pages> 187-228, </pages> <year> 1996. </year>
Reference: [18] <author> K. Seymore and R. Rosenfeld. </author> <title> Scalable backoff language models. </title> <booktitle> In International Conference on Spoken Language Processing, </booktitle> <pages> pages 232-235, </pages> <year> 1996. </year>
Reference-contexts: i2 w i1 )g f (w i3 w i2 w i1 )g : : : fw i1 g or similarly f (g i4 g i3 g i2 g i1 )g f (g i3 g i2 g i1 )g : : : fg i1 g 2 Even backoff models with cutoffs <ref> [18] </ref> are static as long as the cutoff values are fixed for all histories. 3 Each node of the lattice represents a particular predictor which, for a given history h and a given predicted word w, is associated with two counts C (h; w) and C (h).
Reference: [19] <author> R. Kneser and H. Ney. </author> <title> Improved clustering techniques for class-based language modelling. </title> <booktitle> In European Conference on Speech Communication and Technology, </booktitle> <pages> pages 973-976, </pages> <address> Berlin, </address> <year> 1993. </year> <month> 25 </month>
Reference-contexts: This mapping can be automatically constructed by a clustering algorithm such as the one proposed by Kneser and Ney <ref> [19] </ref>. Its objective is to find a mapping such that an associated class bigram model 7 has a locally minimal perplexity on the training data. This criterion can be shown to be equivalent to the local minimization of the loss of mutual information between words.
Reference: [20] <author> J.J. Godfrey, E.C. Holliman, and J. McDaniel. </author> <title> Switchboard: Telephone speech corpus for research and development. </title> <booktitle> In International Conference on Acoustic, Speech and Signal Processing, </booktitle> <pages> pages 517-520, </pages> <address> Detroit, </address> <year> 1995. </year>
Reference-contexts: )) 8 Notice that the optimal number of classes for a single class bigram model can be estimated on a held-out set or by leaving-one-out. 8 4 Data Analysis 4.1 Switchboard data The Switchboard data used in the present work consists of about 2.5 million words of transcribed conversational speech <ref> [20] </ref>. We chose a vocabulary of 9802 words corresponding to a coverage of 98.5 %. This vocabulary is closed as it contains the special token UNK to which any out of vocabulary word is mapped. For the current experiments the data were randomly split into 3 sets.
Reference: [21] <author> I.J. </author> <title> Good. The population frequencies of species and the estimation of population parmaters. </title> <journal> Biometrika, </journal> <volume> 40 </volume> <pages> 237-264, </pages> <year> 1953. </year>
Reference-contexts: This discounted value may depend on the count C (h; w) as in Turing-Good discounting <ref> [21] </ref> or may be constant as in the case of absolute discounting [22]. The normalized discounted probability mass is distributed to unseen events in proportion to their backoff estimates (p back (wjh)).
Reference: [22] <author> H. Ney, U. Essen, and R. Kneser. </author> <title> On structuring probabilistic dependences in stochastic language modelling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 1-38, </pages> <year> 1994. </year>
Reference-contexts: This discounted value may depend on the count C (h; w) as in Turing-Good discounting [21] or may be constant as in the case of absolute discounting <ref> [22] </ref>. The normalized discounted probability mass is distributed to unseen events in proportion to their backoff estimates (p back (wjh)). Here the backoff distribution p back (wjh) is only used if the higher order estimate cannot be used, that is when C (h; w) = 0. <p> In summary there are at least four possible methods of smoothing available. We can decide whether or not shadowing is used and whether or not 12 Ney et al. uses the term non linear interpolation <ref> [22] </ref>. 14 Baseline +KN backoff distributions +non-shadowing +both C50 2g 126 125 127 123 C300 2g 102 100 102 99 C1600 2g 98 96 98 94 Word 2g 97 95 97 94 3g 84 83 84 77 Table 2: Comparison of held-out perplexities for various backoff schemes the Kneser-Ney (KN) backoff
Reference: [23] <author> R. Kneser and H. Ney. </author> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In International Conference on Acoustic, Speech and Signal Processing, </booktitle> <pages> pages 181-184, </pages> <address> Detroit, </address> <year> 1995. </year>
Reference-contexts: Kneser and Ney proposed an alternative backoff distribution which performs better <ref> [23] </ref>: p back (wjh) = C (:; ^ h; w) w 0 C (:; ^ h; w 0 ) where C (:; ^ h; w) = g:^g= ^ h;C (g;w)&gt;0 Here ^ h denotes a coarser history that is typically a 2-gram history if h denotes a 3-gram history.
Reference: [24] <author> S. Besling and H.-G. Meier. </author> <title> Language model speaker adaptation. </title> <booktitle> In Eu-ropean Conference on Speech Communication and Technology, </booktitle> <address> Madrid, </address> <year> 1995. </year>
Reference-contexts: Considering two backoff distributions was already proposed in a different context <ref> [24] </ref> where a speaker specific language model was combined with a non-specific language model. In that case however a hierarchy between the backoff distributions was defined a priori. To the contrary in the model proposed here both backoff distributions are combined by linear interpolation.
References-found: 24

