URL: http://www.cs.princeton.edu/~liv/papers/survey.ps
Refering-URL: http://www.cs.princeton.edu/~liv/papers/papers.html
Root-URL: http://www.cs.princeton.edu
Email: iftode@cs.rutgers.edu  jps@cs.princeton.edu  
Title: Shared Virtual Memory: Progress and Challenges  
Author: Liviu Iftode Jaswinder Pal Singh 
Address: Piscataway, NJ 08855  Princeton, NJ 08544  
Affiliation: Computer Science Department Rutgers University  Computer Science Department Princeton University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. V. Adve, A. L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> A comparison of entry consistency and lazy release consistency implementation. </title> <booktitle> In Proceedings of the 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: This requires instrumenting memory operations in the program, incurring runtime overhead. It has been shown to perform slightly better than diffs for migratory sharing patterns, but the only available comparison shows that (for a non-adaptive no-home LRC protocol) diffs perform better overall due to the cost of instrumentation <ref> [1] </ref>. 4 Architectural Support Simple architectural support can accelerate SVM protocols.
Reference: [2] <author> C. Amza, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Software dsm protocols that adapt between single writer and multiple writer. </title> <booktitle> In Proceedings of the 3rd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher <ref> [4, 21, 39, 38, 47, 2, 19, 30] </ref>. <p> Of the three layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic <ref> [4, 21, 47, 2] </ref>, and additional hardware support in the communication architecture to reduce communication costs [39, 11, 35, 19, 18, 7, 30, 29]. <p> A subsequent page fault fetches the full page only from this last releaser, just as home-based protocols fetch it only from the home. 3.3.3 Adaptive Protocols Much of the machinery developed to support multiple writers is not well suited to single-writer patterns. Recent papers <ref> [2, 24] </ref> propose adaptive no-home LRC protocols. If write-write sharing is not detected within a synchronization interval at run time, the protocol switches from multiple-writer to single-writer for that page, transferring the whole page when necessary and eliminating diffs and extra messages. <p> Protocol changes should now mostly be driven by such bottlenecks discovered by running real applications. Protocol optimizations can be directed by compilers [14], users [3], or the runtime system <ref> [2, 24, 42, 33] </ref>. Another major area enabled by SVM progress is re-search in the application layer itself for these systems. Structuring applications or data structures more appropriately for page-grained SVM can improve performance dramatically [22].
Reference: [3] <author> A.R. Lebeck B. Falsafi, S.K. Reinhart, I. Schoinas, M.D. Hill, J.R. Larus, A. Rogers, and D.A. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: For example, expensive page faults cause dilation of critical sections which, in turn, amplifies lock contention and the waiting time for synchronization. Protocol changes should now mostly be driven by such bottlenecks discovered by running real applications. Protocol optimizations can be directed by compilers [14], users <ref> [3] </ref>, or the runtime system [2, 24, 42, 33]. Another major area enabled by SVM progress is re-search in the application layer itself for these systems. Structuring applications or data structures more appropriately for page-grained SVM can improve performance dramatically [22].
Reference: [4] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher <ref> [4, 21, 39, 38, 47, 2, 19, 30] </ref>. <p> Of the three layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic <ref> [4, 21, 47, 2] </ref>, and additional hardware support in the communication architecture to reduce communication costs [39, 11, 35, 19, 18, 7, 30, 29]. <p> LRC is discussed further in Section 3. Relaxing the consistency model beyond LRC is much more difficult due to both protocol and especially programming complexity. However there is some potential for performance improvement, especially in applications with a lot of lock synchronization. Entry Consistency <ref> [4] </ref> proposed the idea of binding data to synchronization variables, and making only the data bound to that variable consistent at a synchronization event. That is, the consistency model governs not only when data are made coherent but also which data should be made coherent at a synchronization point. <p> The tradeoff between these approaches needs to be better understood. 3.3.4 Alternative All-Software Propagation An alternative all-software method to track and merge modifications in multiple-writer protocols is to maintain per-word dirty bits in software <ref> [4] </ref>. This requires instrumenting memory operations in the program, incurring runtime overhead.
Reference: [5] <author> Angelos Bilas, Cheng Liao, and Jaswinder Pal Singh. </author> <title> Network interface support for shared virtual memory on clusters. </title> <type> Technical Report TR-579-98, </type> <institution> Computer Science Department, Princeton University, Princeton, NJ-08544, </institution> <month> March </month> <year> 1998. </year>
Reference-contexts: Such a system has been built by programming simple remote deposit and remote fetch mechanisms into the nonintrusive Myrinet network interface, as well as a mechanism to provide mutual exclusion for locks, and by modifying the protocol propagation and application methods to exploit these mechanisms <ref> [5] </ref>. It has demonstrated substantial performance improvement across a range of applications, and shown that all three simple types of support are important for different applications. Further integration of the mechanisms into the memory system may help more. <p> The gap between hardware cache coherence and software shared memory is still quite large for several types of applications (e.g. <ref> [20, 22, 5] </ref>), and software shared memory performance has not yet been proven to scale except for a limited class of applications. Some key bottlenecks have been shown to generate cascading effects in SVM. <p> Simultaneous research in applications and the systems layers, rather than treating either as fixed, is important to understand and exploit the synergies among the layers <ref> [40, 5] </ref>, truly understand the potential of SVM, and develop guidelines for performance-portable shared memory programming across hardware-coherent systems and clusters, the two major emerging multiprocessor platforms. <p> Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform. <p> The result of work so far in all the layers together has demonstrated substantial performance improvements, with end performance quite close to hardware-coherent systems for small-scale SMP-based systems for several applications <ref> [5] </ref>, but even this has required application restructuring and there is still a long way to go.
Reference: [6] <author> Angelos Bilas and Jaswinder Pal Singh. </author> <title> The effects of communication parameters on end performance of shared virtual memory clusters. </title> <booktitle> In Proceedings of Supercomputing'97, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: The interrupt overhead is the most significant parameter of the communication architecture in determining the performance of an SVM protocol <ref> [6] </ref>. Using polling as an alternative to interrupts may or may not improve performance, depending on the interrupt versus polling and instrumentation overhead on the platform. These message handling costs can also affect the performance tradeoffs in protocol laziness, for both propagation and application.
Reference: [7] <author> M.A. Blumrich, R.D. Alpert, A. Bilas, Y. Chen, D.W. Clark, S. Damianakis, C. Dubnicki, E.W. Felten, L. Iftode, K. Li, M. Martonosi, and R.A. Shillner. </author> <title> Design choices in the shrimp system: An empirical study. </title> <booktitle> In Proceedings of the 25th Annual Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic [4, 21, 47, 2], and additional hardware support in the communication architecture to reduce communication costs <ref> [39, 11, 35, 19, 18, 7, 30, 29] </ref>. With the relative maturity of protocols, in the last couple of years SVM research has moved to greater emphasis on the application layer and the synergies available across layers. <p> Recent real implementations include the AURC home-based protocol on the SHRIMP multicom-puter, which uses the automatic update hardware mechanism <ref> [7] </ref> to snoop writes off the memory bus and propagate them to the home of the page if it is remote. This eliminates diffs, but can generate more traffic on the memory bus and network and relies on the ability to map pages write-through in the caches. <p> The fine-grain remote writes can be used not only for application data but also for protocol meta-data. Results from the implementation show that AURC usually outperforms the all-software home-based HLRC <ref> [18, 7] </ref>, but not dramatically. The Cashmere system [29] does not rely on snooping but instruments shared writes to generate explicit remote writes as well, using the remote write support in the Memory Channel [17] (together with a multiple-writer DC protocol and the broadcast support).
Reference: [8] <author> L. Borrmann and M. Herdieckerhoff. </author> <title> A coherency model for virtually shared memory. </title> <booktitle> In 1990 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: If the processor is involved, it may as well apply the data right away. We therefore treat data propagation and application together. Managing data propagation is quite simple for single-writer protocols <ref> [32, 8, 24] </ref>. Multiple-writer schemes propagate data modifications more lazily, but they require a mechanism to keep track of the writes by different processes to a page and to merge them before another process needs to see them according to the consistency model.
Reference: [9] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Finally, protocol processing incurs large overheads because it is usually performed in software on the main processor, so the main computation must be interrupted to service remote protocol requests. Breakthrough protocols from Rice University in the early 1990s <ref> [9, 27] </ref> used the recently introduced release consistency model [16] to breathe new life into the the SVM approach. <p> Weak consistency [12] was the first model based on this idea. However, release consistency (RC) [16], which separates synchronization operations into acquires and releases, was the first to inspire a major breakthrough in the performance of SVM <ref> [9] </ref>. RC guarantees that coherence operations due to writes are performed before the releases that follow the writes. Lazy release consistency or LRC [27] further delays the deadline for coherence operations to be performed to the time of acquires rather than the releases that precede them. <p> With lazy propagation, as used in protocols implementing LRC [27], the invalidation corresponding to a write by processor A is conveyed to a processor B only on demand upon B's next acquire operation, if that acquire is causally "after" the release by the writing processor A. Munin <ref> [9] </ref> and TreadMarks [25] were the first SVM systems with eager and lazy software release consistent protocols, respectively. Laziness has implications for the amount and lifetime of coherence state that must be maintained. <p> This is like an update-based protocol but with less useless traffic since updates are propagated only at releases. Hybrid update/invalidate protocols further reduce useless update traffic <ref> [9, 42] </ref>. The laziest form of data propagation is to fetch the diffs when they are actually needed by the next sharer, i.e. on a page fault. The faulting processor obtains the diffs from the relevant writers and merges them into its copy of the page.
Reference: [10] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform.
Reference: [11] <author> A.L. Cox and R.J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic [4, 21, 47, 2], and additional hardware support in the communication architecture to reduce communication costs <ref> [39, 11, 35, 19, 18, 7, 30, 29] </ref>. With the relative maturity of protocols, in the last couple of years SVM research has moved to greater emphasis on the application layer and the synergies available across layers. <p> The Brazos system uses multicast support in a mostly-update protocol, with protocol mechanisms for reducing the drawbacks of updates [42]. 4.2 Fine-grain Remote Writes Several papers have suggested hardware support for fine-grain remote operations in the network interface <ref> [11, 35, 30, 29] </ref>. Recent real implementations include the AURC home-based protocol on the SHRIMP multicom-puter, which uses the automatic update hardware mechanism [7] to snoop writes off the memory bus and propagate them to the home of the page if it is remote.
Reference: [12] <author> M. Dubois, C. Scheurich, and F. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Weak consistency <ref> [12] </ref> was the first model based on this idea. However, release consistency (RC) [16], which separates synchronization operations into acquires and releases, was the first to inspire a major breakthrough in the performance of SVM [9].
Reference: [13] <author> M. Dubois, J.C. Wang, L.A. Barroso, K. Lee, and Y-S Chen. </author> <title> Delayed consistency and its effects on the miss rate of parallel programs. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 197-206, </pages> <year> 1991. </year>
Reference-contexts: We will call eager release consistency or ERC protocols those that use eager propagation and also apply (and acknowledge) write notices eagerly as soon as they are received. Delayed consistency (DC) <ref> [13] </ref> protocols use eager propagation but lazy application. Incoming invalidations are not applied right away but are queued until the destination node performs its next acquire operation (whether causally related to that release or not, since it doesn't know).
Reference: [14] <author> S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory systems. </title> <booktitle> In ASPLOS-VII, </booktitle> <year> 1996. </year>
Reference-contexts: For example, expensive page faults cause dilation of critical sections which, in turn, amplifies lock contention and the waiting time for synchronization. Protocol changes should now mostly be driven by such bottlenecks discovered by running real applications. Protocol optimizations can be directed by compilers <ref> [14] </ref>, users [3], or the runtime system [2, 24, 42, 33]. Another major area enabled by SVM progress is re-search in the application layer itself for these systems. Structuring applications or data structures more appropriately for page-grained SVM can improve performance dramatically [22].
Reference: [15] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. Softflash: </author> <title> Analyzing the performance of clustered distributed virtual shared memory. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform.
Reference: [16] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Finally, protocol processing incurs large overheads because it is usually performed in software on the main processor, so the main computation must be interrupted to service remote protocol requests. Breakthrough protocols from Rice University in the early 1990s [9, 27] used the recently introduced release consistency model <ref> [16] </ref> to breathe new life into the the SVM approach. Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher [4, 21, 39, 38, 47, 2, 19, 30]. <p> The intuition behind these models is that a user cannot distinguish the behavior of a system running under such relaxation from one that uses a sequentially consistent model, as long as programs are "properly labeled" <ref> [16] </ref>, which means that they are free of data races and all synchronization is identified as such. Weak consistency [12] was the first model based on this idea. However, release consistency (RC) [16], which separates synchronization operations into acquires and releases, was the first to inspire a major breakthrough in the <p> under such relaxation from one that uses a sequentially consistent model, as long as programs are "properly labeled" <ref> [16] </ref>, which means that they are free of data races and all synchronization is identified as such. Weak consistency [12] was the first model based on this idea. However, release consistency (RC) [16], which separates synchronization operations into acquires and releases, was the first to inspire a major breakthrough in the performance of SVM [9]. RC guarantees that coherence operations due to writes are performed before the releases that follow the writes.
Reference: [17] <author> Richard Gillett. </author> <title> Memory channel network for pci. </title> <booktitle> In Proceedings of Hot Interconnects '95 Symposium, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: Results from the implementation show that AURC usually outperforms the all-software home-based HLRC [18, 7], but not dramatically. The Cashmere system [29] does not rely on snooping but instruments shared writes to generate explicit remote writes as well, using the remote write support in the Memory Channel <ref> [17] </ref> (together with a multiple-writer DC protocol and the broadcast support).
Reference: [18] <author> L. Iftode, M. Blumrich, C. Dubnicki, D.L. Oppenheimer, J.P. Singh, and K. Li. </author> <title> Shared virtual memory with automatic update support. </title> <type> Technical Report TR-575-98, </type> <institution> Princeton, NJ, </institution> <month> February </month> <year> 1998. </year>
Reference-contexts: layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic [4, 21, 47, 2], and additional hardware support in the communication architecture to reduce communication costs <ref> [39, 11, 35, 19, 18, 7, 30, 29] </ref>. With the relative maturity of protocols, in the last couple of years SVM research has moved to greater emphasis on the application layer and the synergies available across layers. <p> The fine-grain remote writes can be used not only for application data but also for protocol meta-data. Results from the implementation show that AURC usually outperforms the all-software home-based HLRC <ref> [18, 7] </ref>, but not dramatically. The Cashmere system [29] does not rely on snooping but instruments shared writes to generate explicit remote writes as well, using the remote write support in the Memory Channel [17] (together with a multiple-writer DC protocol and the broadcast support).
Reference: [19] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In Proceedings of the 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher <ref> [4, 21, 39, 38, 47, 2, 19, 30] </ref>. <p> layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic [4, 21, 47, 2], and additional hardware support in the communication architecture to reduce communication costs <ref> [39, 11, 35, 19, 18, 7, 30, 29] </ref>. With the relative maturity of protocols, in the last couple of years SVM research has moved to greater emphasis on the application layer and the synergies available across layers. <p> In fact, the history of SVM research so far is largely a history of protocol laziness. This research also led to the distinction between single-writer and multiple writer protocols [24], as well as between home-based <ref> [19, 21, 47] </ref> and no-home or distributed [27, 42] data collection. Greater laziness typically implies less frequent communication and protocol operations, but also greater programming difficulty and protocol state. <p> A home node is selected for each shared page and modifications are propagated eagerly (at or before a release) to the home only <ref> [19] </ref>. This can be done either using diffs [21, 47] or architectural support as discussed later [19, 30]. Unlike the home copy, the non-home copies of a page are kept coherent through invalidations as usual. <p> A home node is selected for each shared page and modifications are propagated eagerly (at or before a release) to the home only [19]. This can be done either using diffs [21, 47] or architectural support as discussed later <ref> [19, 30] </ref>. Unlike the home copy, the non-home copies of a page are kept coherent through invalidations as usual. On a page fault, a non-home processor obtains the whole page from the home rather than obtaining diffs from previous writers. <p> The former is found to perform substantially better on this platform, with the performance gap increasing with the number of processors [47]. Similar results are seen in earlier, simulation-based comparisons between a home-based protocol with some hardware support and a no-home LRC for a wider range of applications <ref> [19, 20] </ref>. An alternative multiple-writer scheme that shares some properties of home-based write collection was used in [26] to implement an ERC protocol (the one in Table 1).
Reference: [20] <author> L. Iftode, J. P. Singh, and Kai Li. </author> <title> Understanding application performance on shared virtual memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: In addition to their direct costs, these operations substantially pollute the primary cache and hurt application performance further <ref> [20] </ref>. 3.3.2 Home-Based Protocols: An Intermediate Approach An intermediate form of laziness in data propagation is to propagate the modifications in two stages. A home node is selected for each shared page and modifications are propagated eagerly (at or before a release) to the home only [19]. <p> The former is found to perform substantially better on this platform, with the performance gap increasing with the number of processors [47]. Similar results are seen in earlier, simulation-based comparisons between a home-based protocol with some hardware support and a no-home LRC for a wider range of applications <ref> [19, 20] </ref>. An alternative multiple-writer scheme that shares some properties of home-based write collection was used in [26] to implement an ERC protocol (the one in Table 1). <p> It is very important that performance and tradeoffs evaluations in SVM use a wider range of applications, classified according to both inherent sharing patterns and the patterns induced by interaction with system granularities <ref> [20] </ref>. The gap between hardware cache coherence and software shared memory is still quite large for several types of applications (e.g. [20, 22, 5]), and software shared memory performance has not yet been proven to scale except for a limited class of applications. <p> The gap between hardware cache coherence and software shared memory is still quite large for several types of applications (e.g. <ref> [20, 22, 5] </ref>), and software shared memory performance has not yet been proven to scale except for a limited class of applications. Some key bottlenecks have been shown to generate cascading effects in SVM.
Reference: [21] <author> L. Iftode, J.P. Singh, and K. Li. </author> <title> Scope consistency: a bridge between release consistency and entry consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year> <note> To appear in Theory of Computing Systems Journal, </note> <year> 1998. </year>
Reference-contexts: Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher <ref> [4, 21, 39, 38, 47, 2, 19, 30] </ref>. <p> Of the three layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic <ref> [4, 21, 47, 2] </ref>, and additional hardware support in the communication architecture to reduce communication costs [39, 11, 35, 19, 18, 7, 30, 29]. <p> However, the programmer has to provide the binding, which is a major burden. The granularity in entry consistency is that of user-defined objects or regions, not transparent pages. Scope consistency (ScC) <ref> [21] </ref> tries to achieve the benefits of entry consistency without the burden of explicit binding. Instead, synchronization variables define scopes through which memory is viewed, and an implicit association of data (pages) with scopes is achieved dynamically when a write access occurs inside a scope. <p> In fact, the history of SVM research so far is largely a history of protocol laziness. This research also led to the distinction between single-writer and multiple writer protocols [24], as well as between home-based <ref> [19, 21, 47] </ref> and no-home or distributed [27, 42] data collection. Greater laziness typically implies less frequent communication and protocol operations, but also greater programming difficulty and protocol state. <p> A home node is selected for each shared page and modifications are propagated eagerly (at or before a release) to the home only [19]. This can be done either using diffs <ref> [21, 47] </ref> or architectural support as discussed later [19, 30]. Unlike the home copy, the non-home copies of a page are kept coherent through invalidations as usual. On a page fault, a non-home processor obtains the whole page from the home rather than obtaining diffs from previous writers.
Reference: [22] <author> Dongming Jiang, Hongzhang Shan, and Jaswinder Pal Singh. </author> <title> Application restructuring and performance portability across shared virtual memory and hardware-coherent multiprocessors. </title> <booktitle> In Proceedings of the 6th ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The gap between hardware cache coherence and software shared memory is still quite large for several types of applications (e.g. <ref> [20, 22, 5] </ref>), and software shared memory performance has not yet been proven to scale except for a limited class of applications. Some key bottlenecks have been shown to generate cascading effects in SVM. <p> Protocol optimizations can be directed by compilers [14], users [3], or the runtime system [2, 24, 42, 33]. Another major area enabled by SVM progress is re-search in the application layer itself for these systems. Structuring applications or data structures more appropriately for page-grained SVM can improve performance dramatically <ref> [22] </ref>. While the restructurings are often algorithmic (e.g. reducing synchronization, even at the cost of imbalance or communication, and making programs algorithmically single-writer), the good news is that they are mostly performance-portable to hardware-coherent systems as well [22]. <p> applications or data structures more appropriately for page-grained SVM can improve performance dramatically <ref> [22] </ref>. While the restructurings are often algorithmic (e.g. reducing synchronization, even at the cost of imbalance or communication, and making programs algorithmically single-writer), the good news is that they are mostly performance-portable to hardware-coherent systems as well [22].
Reference: [23] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance evaluation of cluster-based multiprocessor built from atm switches and bus-based multiprocessor servers. </title> <booktitle> In Proceedings of the 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform.
Reference: [24] <author> P. Keleher. </author> <title> On the importance of being lazy. </title> <type> Technical Report UMIACS-TR-98-06, </type> <institution> University of Maryland, College Park, </institution> <year> 1998. </year>
Reference-contexts: In fact, the history of SVM research so far is largely a history of protocol laziness. This research also led to the distinction between single-writer and multiple writer protocols <ref> [24] </ref>, as well as between home-based [19, 21, 47] and no-home or distributed [27, 42] data collection. Greater laziness typically implies less frequent communication and protocol operations, but also greater programming difficulty and protocol state. <p> If the processor is involved, it may as well apply the data right away. We therefore treat data propagation and application together. Managing data propagation is quite simple for single-writer protocols <ref> [32, 8, 24] </ref>. Multiple-writer schemes propagate data modifications more lazily, but they require a mechanism to keep track of the writes by different processes to a page and to merge them before another process needs to see them according to the consistency model. <p> A subsequent page fault fetches the full page only from this last releaser, just as home-based protocols fetch it only from the home. 3.3.3 Adaptive Protocols Much of the machinery developed to support multiple writers is not well suited to single-writer patterns. Recent papers <ref> [2, 24] </ref> propose adaptive no-home LRC protocols. If write-write sharing is not detected within a synchronization interval at run time, the protocol switches from multiple-writer to single-writer for that page, transferring the whole page when necessary and eliminating diffs and extra messages. <p> However, home-based protocols usually handle multiple-writer situations more efficiently than adaptive no-home protocols. A recent study <ref> [24] </ref> shows that an adaptive no-home LRC can outperform an HLRC protocol, but the comparison is not conclusive since few real applications are used, the number of processors is small, and the HLRC implementation is slightly different from that described in [47]. <p> Protocol changes should now mostly be driven by such bottlenecks discovered by running real applications. Protocol optimizations can be directed by compilers [14], users [3], or the runtime system <ref> [2, 24, 42, 33] </ref>. Another major area enabled by SVM progress is re-search in the application layer itself for these systems. Structuring applications or data structures more appropriately for page-grained SVM can improve performance dramatically [22].
Reference: [25] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Munin [9] and TreadMarks <ref> [25] </ref> were the first SVM systems with eager and lazy software release consistent protocols, respectively. Laziness has implications for the amount and lifetime of coherence state that must be maintained. <p> Orchestrating the transitive propagation of coherence information requires an LRC protocol to store more complex state for much longer, using vector timestamps to keep track of the causal partial ordering of synchronization events <ref> [25] </ref>.
Reference: [26] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> An evaluation of software-based release consistent protocols. </title> <journal> In The Journal of Parallel and Distributed Computing, </journal> <volume> volume 29, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: We will discuss this issue further in Section 3.3. Table 1 reports the results of a comparison of a particular ERC protocol similar to Munin and the TreadMarks LRC protocol <ref> [26] </ref>. Both are multiple-writer protocols. However, the comparison is not purely about laziness. This ERC protocol uses a hybrid update-invalidation approach (see Section 3.3.2) while the LRC protocol is invalidation-based. <p> Benchmarks ERC LRC SOR 7.2 7.4 Water 3.8 4.6 Barnes 2.6 3.2 FFT 4.2 3.6 ILINK 5.9 5.8 MIP 4.2 5.6 Table 1: Speedups for Multiple-writer ERC and LRC, 8 processors <ref> [26] </ref> Focusing only on laziness, a single-writer DC protocol, a single-writer LRC protocol (similar to that in [28]), a multiple-writer LRC protocol (similar to that in Section 3.3.2) and a sequentially consistent protocol (all invalidation-based) have been compared on a number of SPLASH-2 applications that cover most of the key sharing <p> Similar results are seen in earlier, simulation-based comparisons between a home-based protocol with some hardware support and a no-home LRC for a wider range of applications [19, 20]. An alternative multiple-writer scheme that shares some properties of home-based write collection was used in <ref> [26] </ref> to implement an ERC protocol (the one in Table 1).
Reference: [27] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Finally, protocol processing incurs large overheads because it is usually performed in software on the main processor, so the main computation must be interrupted to service remote protocol requests. Breakthrough protocols from Rice University in the early 1990s <ref> [9, 27] </ref> used the recently introduced release consistency model [16] to breathe new life into the the SVM approach. <p> However, release consistency (RC) [16], which separates synchronization operations into acquires and releases, was the first to inspire a major breakthrough in the performance of SVM [9]. RC guarantees that coherence operations due to writes are performed before the releases that follow the writes. Lazy release consistency or LRC <ref> [27] </ref> further delays the deadline for coherence operations to be performed to the time of acquires rather than the releases that precede them. It is a slightly lazier model than RC, although this cannot be distinguished by a "properly labeled" program. LRC is discussed further in Section 3. <p> In fact, the history of SVM research so far is largely a history of protocol laziness. This research also led to the distinction between single-writer and multiple writer protocols [24], as well as between home-based [19, 21, 47] and no-home or distributed <ref> [27, 42] </ref> data collection. Greater laziness typically implies less frequent communication and protocol operations, but also greater programming difficulty and protocol state. For example, while release consistency was first defined for hardware-coherent systems, those systems tend to propagate and apply operations as soon as possible to keep hardware simple. <p> With lazy propagation, as used in protocols implementing LRC <ref> [27] </ref>, the invalidation corresponding to a write by processor A is conveyed to a processor B only on demand upon B's next acquire operation, if that acquire is causally "after" the release by the writing processor A.
Reference: [28] <author> P.J. Keleher. </author> <title> The relative importance of concurrent writers and weak consistency models. </title> <booktitle> In Proceedings of the 16th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Benchmarks ERC LRC SOR 7.2 7.4 Water 3.8 4.6 Barnes 2.6 3.2 FFT 4.2 3.6 ILINK 5.9 5.8 MIP 4.2 5.6 Table 1: Speedups for Multiple-writer ERC and LRC, 8 processors [26] Focusing only on laziness, a single-writer DC protocol, a single-writer LRC protocol (similar to that in <ref> [28] </ref>), a multiple-writer LRC protocol (similar to that in Section 3.3.2) and a sequentially consistent protocol (all invalidation-based) have been compared on a number of SPLASH-2 applications that cover most of the key sharing patterns [48]. <p> Also, the multiple-writer protocol performs much better than the single-writer for such irregular applications (though sometimes multiple-writer protocols can perform worse for applications whose patterns of page access are mostly single-writer <ref> [28] </ref>). The study is limited by the fact that the platform provides hardware rather than VM-based access control for pages.
Reference: [29] <author> L. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cier-nak, S. Parthasarathy, W. Meira Jr., S. Dwarkadas, and M. Scott. </author> <title> Vm-based shared memory on low-latency, remote-memory-access networks. </title> <booktitle> In Proceedings of the 24th Annual Symposium on Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic [4, 21, 47, 2], and additional hardware support in the communication architecture to reduce communication costs <ref> [39, 11, 35, 19, 18, 7, 30, 29] </ref>. With the relative maturity of protocols, in the last couple of years SVM research has moved to greater emphasis on the application layer and the synergies available across layers. <p> The trend today is toward lazy, multiple-writer protocols. However, the performance/storage/complexity tradeoffs in laziness are not yet clear for emerging plat forms and real applications and bear further research (see also Section 4). A multiple-writer DC protocol is implemented in the Cashmere system <ref> [30, 29] </ref>. 3.3 Laziness in Data Propagation Like coherence information, data modifications can be propagated and applied with varying degrees of laziness. We define laziness of data propagation with respect to when coherence operations are propagated rather than when the corresponding local writes or releases are performed. <p> Lazy protocols are inherently point to point. For example, the Cashmere DC protocol uses the broadcast support in the DEC Memory Channel interconnect to perform directory updates and synchronization <ref> [29] </ref>, as well as to to ensure a total ordering on operations. This clever exploitation eliminates the need for acknowledgments, and allows directories and eager coherence approaches to be used efficiently, eliminating time-stamps as well. <p> The Brazos system uses multicast support in a mostly-update protocol, with protocol mechanisms for reducing the drawbacks of updates [42]. 4.2 Fine-grain Remote Writes Several papers have suggested hardware support for fine-grain remote operations in the network interface <ref> [11, 35, 30, 29] </ref>. Recent real implementations include the AURC home-based protocol on the SHRIMP multicom-puter, which uses the automatic update hardware mechanism [7] to snoop writes off the memory bus and propagate them to the home of the page if it is remote. <p> The fine-grain remote writes can be used not only for application data but also for protocol meta-data. Results from the implementation show that AURC usually outperforms the all-software home-based HLRC [18, 7], but not dramatically. The Cashmere system <ref> [29] </ref> does not rely on snooping but instruments shared writes to generate explicit remote writes as well, using the remote write support in the Memory Channel [17] (together with a multiple-writer DC protocol and the broadcast support).
Reference: [30] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using memory-mapped network interfaces to improve the performance of distributed shared memory. </title> <booktitle> In Proceedings of the 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher <ref> [4, 21, 39, 38, 47, 2, 19, 30] </ref>. <p> layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic [4, 21, 47, 2], and additional hardware support in the communication architecture to reduce communication costs <ref> [39, 11, 35, 19, 18, 7, 30, 29] </ref>. With the relative maturity of protocols, in the last couple of years SVM research has moved to greater emphasis on the application layer and the synergies available across layers. <p> The trend today is toward lazy, multiple-writer protocols. However, the performance/storage/complexity tradeoffs in laziness are not yet clear for emerging plat forms and real applications and bear further research (see also Section 4). A multiple-writer DC protocol is implemented in the Cashmere system <ref> [30, 29] </ref>. 3.3 Laziness in Data Propagation Like coherence information, data modifications can be propagated and applied with varying degrees of laziness. We define laziness of data propagation with respect to when coherence operations are propagated rather than when the corresponding local writes or releases are performed. <p> A home node is selected for each shared page and modifications are propagated eagerly (at or before a release) to the home only [19]. This can be done either using diffs [21, 47] or architectural support as discussed later <ref> [19, 30] </ref>. Unlike the home copy, the non-home copies of a page are kept coherent through invalidations as usual. On a page fault, a non-home processor obtains the whole page from the home rather than obtaining diffs from previous writers. <p> The Brazos system uses multicast support in a mostly-update protocol, with protocol mechanisms for reducing the drawbacks of updates [42]. 4.2 Fine-grain Remote Writes Several papers have suggested hardware support for fine-grain remote operations in the network interface <ref> [11, 35, 30, 29] </ref>. Recent real implementations include the AURC home-based protocol on the SHRIMP multicom-puter, which uses the automatic update hardware mechanism [7] to snoop writes off the memory bus and propagate them to the home of the page if it is remote.
Reference: [31] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocessor programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: Several factors limit the performance of a shared virtual memory implementation. First, the large granularities of coherence and communication cause unnecessary or artifactual communication, especially when a strict consistency model like sequential consistency <ref> [31] </ref> is used. The large granularity of coherence (a page) can cause false sharing when multiple processors access different variables co-located on the same page and at least one access is a write.
Reference: [32] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely-coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> October </month> <year> 1986. </year> <note> Tech Report YALEU-RR-492. </note>
Reference-contexts: 1 Introduction The idea of implementing a shared address space in software across a network of computers using the virtual memory mechanism|the shared virtual memory (SVM) approach|was proposed more than a decade ago <ref> [32] </ref>. An SVM protocol provides a coherent shared virtual address space at page granularity, using the local page table for access control and message-passing for inter-node communication. Several factors limit the performance of a shared virtual memory implementation. <p> If the processor is involved, it may as well apply the data right away. We therefore treat data propagation and application together. Managing data propagation is quite simple for single-writer protocols <ref> [32, 8, 24] </ref>. Multiple-writer schemes propagate data modifications more lazily, but they require a mechanism to keep track of the writes by different processes to a page and to merge them before another process needs to see them according to the consistency model.
Reference: [33] <author> R. Monnerat and R Bianchini. </author> <title> Efficiently adapting to sharing patterns in software dsm. </title> <booktitle> In Proceedings of the 4th IEEE Symposium on High-Performance Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: Protocol changes should now mostly be driven by such bottlenecks discovered by running real applications. Protocol optimizations can be directed by compilers [14], users [3], or the runtime system <ref> [2, 24, 42, 33] </ref>. Another major area enabled by SVM progress is re-search in the application layer itself for these systems. Structuring applications or data structures more appropriately for page-grained SVM can improve performance dramatically [22].
Reference: [34] <author> T. Mowry, C. Chan, and A. Lo. </author> <title> Comparative evaluation of latency tolerance techniques for software distributed shared memory. </title> <booktitle> In Proceedings of the 4th IEEE Symposium on High-Performance Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: Relaxed memory consistency models are used by virtually all SVM systems. Recent studies on systems with large interconnect latencies such as ATM indicate that prefetching and multithreading can be quite successful as well <ref> [34, 41] </ref>.
Reference: [35] <author> K. Petersen and K. Li. </author> <title> Cache coherence for shared memory multiprocessors based on virtual memory support. </title> <booktitle> In Proceedings of the IEEE 7th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic [4, 21, 47, 2], and additional hardware support in the communication architecture to reduce communication costs <ref> [39, 11, 35, 19, 18, 7, 30, 29] </ref>. With the relative maturity of protocols, in the last couple of years SVM research has moved to greater emphasis on the application layer and the synergies available across layers. <p> The Brazos system uses multicast support in a mostly-update protocol, with protocol mechanisms for reducing the drawbacks of updates [42]. 4.2 Fine-grain Remote Writes Several papers have suggested hardware support for fine-grain remote operations in the network interface <ref> [11, 35, 30, 29] </ref>. Recent real implementations include the AURC home-based protocol on the SHRIMP multicom-puter, which uses the automatic update hardware mechanism [7] to snoop writes off the memory bus and propagate them to the home of the page if it is remote.
Reference: [36] <author> R. Samanta, A. Bilas, L. Iftode, and J.P Singh. </author> <title> Home-based svm protocols for smp clusters: Design and performance. </title> <booktitle> In Proceedings of the 4th IEEE Symposium on High-Performance Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform. <p> the scalability of performance that matters, but also the scalability of memory needed by the protocol for its data structures, so eager and home-based protocols may have an advantage as the number of nodes increases [47], and protocols should scale their memory needs with the number of nodes, not processors <ref> [36] </ref>. Further protocol innovations may be needed, and the area bears further investigation. 7 Conclusions There has been a lot of progress in shared virtual memory over the last several years.
Reference: [37] <author> D. Scales and K. Gharachorloo. </author> <title> Towards transparent and efficient software distributed shared memory. </title> <booktitle> In Proceedings of the Sixteenth Symposium on Operating Systems Principles, </booktitle> <year> 1997. </year>
Reference-contexts: Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform. <p> Also, because the local coherence and synchronization within a multiprocessor node is performed in hardware, the overall performance is expected to be better than that on clusters of uniprocessors. Fine-grained software shared memory systems have also been built across SMP nodes <ref> [37] </ref>. Using multiprocessor nodes has been found to indeed improve performance, with the extent of the improvement depending on the localization of communication and synchronization in the application.
Reference: [38] <author> D.J. Scales, K. Gharachorloo, and C.A. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher <ref> [4, 21, 39, 38, 47, 2, 19, 30] </ref>. <p> However, code instrumentation is not always portable, it adds overhead on loads and stores and, since fine-grained communication may be more frequent, the approach depends critically on low-latency messaging. Recent optimizations that reduce instrumentation overhead have revived the fine-grained approach <ref> [38] </ref>. Several of the key optimizations so far are specific to RISC or even DEC Alpha architectures. Among protocol-level optimizations, support for multiple coherence granularities on a per data structure basis is particularly useful, though it does rely on support from the programmer.
Reference: [39] <author> I. Schoinas, B. Falsafi, A.R. Lebeck, S.K. Reinhardt, J.R. Larus, and D.A. Wood. </author> <title> Fine-grain access for distributed shared memory. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher <ref> [4, 21, 39, 38, 47, 2, 19, 30] </ref>. <p> layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic [4, 21, 47, 2], and additional hardware support in the communication architecture to reduce communication costs <ref> [39, 11, 35, 19, 18, 7, 30, 29] </ref>. With the relative maturity of protocols, in the last couple of years SVM research has moved to greater emphasis on the application layer and the synergies available across layers. <p> Table 2 shows some of the results as measured on the Wisconsin Typhoon-zero cluster <ref> [39] </ref>. The lazier application and propagation of the LRC protocol have significant advantages over the DC protocol, especially in complex irregular applications that use substantial lock synchronization. <p> nodes to reduce the frequency of software involvement, and (iii) scalability. 6.1 Fine-grain Software DSM For software coherence at a fine or variable grain on clusters, access control is usually provided by instrumenting memory operations in the code rather than through the virtual memory mechanism which only deals with pages <ref> [39] </ref>. An advantage of fine granularity is simplicity, since a sequential consistency model can be used without suffering much false sharing and communication fragmentation. The programming model is exactly the same as in the more mainstream hardware shared memory.
Reference: [40] <author> Jaswinder Pal Singh, Angelos Bilas, Dongming Jiang, and Yuanyuan Zhou. </author> <title> Limits to the performance of software shared memory: A layered approach. </title> <type> Technical Report TR-576-98, </type> <institution> Computer Science Department, Princeton University, Prince-ton, NJ-08544, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: Simultaneous research in applications and the systems layers, rather than treating either as fixed, is important to understand and exploit the synergies among the layers <ref> [40, 5] </ref>, truly understand the potential of SVM, and develop guidelines for performance-portable shared memory programming across hardware-coherent systems and clusters, the two major emerging multiprocessor platforms. <p> Application restructuring aids SVM further. More research is needed to determine which approach is clearly superior to the other given instrumentation costs and future trends in communication performance <ref> [40] </ref> (generally, higher bandwidth and message handling costs favor SVM while lower latency favors a fine-grained approach).
Reference: [41] <author> E. Speight and J. Bennett. </author> <title> Using multicast and multithread-ing to reduce communication in software dsm systems. </title> <booktitle> In Proceedings of the 4th IEEE Symposium on High-Performance Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: Relaxed memory consistency models are used by virtually all SVM systems. Recent studies on systems with large interconnect latencies such as ATM indicate that prefetching and multithreading can be quite successful as well <ref> [34, 41] </ref>.
Reference: [42] <author> E. Speight and J.K. Bennett. Brazos: </author> <title> A third generation dsm systems. </title> <booktitle> In USENIX Workshop on Windows-NT, </booktitle> <year> 1997. </year>
Reference-contexts: Unfortunately, there are still programs where additional programming effort is needed beyond programming for LRC. ScC implementations are page-grained, unlike EC, so they can obtain the prefetching effect of pages while keeping false sharing low. ScC has been implemented in the Brazos prototype <ref> [42] </ref>, showing performance improvements. EC may fit well into object-oriented languages, and ScC's small programming burden may be alleviated by tools that check if a properly labeled program for RC also satisfies ScC. <p> In fact, the history of SVM research so far is largely a history of protocol laziness. This research also led to the distinction between single-writer and multiple writer protocols [24], as well as between home-based [19, 21, 47] and no-home or distributed <ref> [27, 42] </ref> data collection. Greater laziness typically implies less frequent communication and protocol operations, but also greater programming difficulty and protocol state. For example, while release consistency was first defined for hardware-coherent systems, those systems tend to propagate and apply operations as soon as possible to keep hardware simple. <p> This is like an update-based protocol but with less useless traffic since updates are propagated only at releases. Hybrid update/invalidate protocols further reduce useless update traffic <ref> [9, 42] </ref>. The laziest form of data propagation is to fetch the diffs when they are actually needed by the next sharer, i.e. on a page fault. The faulting processor obtains the diffs from the relevant writers and merges them into its copy of the page. <p> However, the techniques rely deeply on the specialized support and it is unclear how well they will scale. The Brazos system uses multicast support in a mostly-update protocol, with protocol mechanisms for reducing the drawbacks of updates <ref> [42] </ref>. 4.2 Fine-grain Remote Writes Several papers have suggested hardware support for fine-grain remote operations in the network interface [11, 35, 30, 29]. <p> Protocol changes should now mostly be driven by such bottlenecks discovered by running real applications. Protocol optimizations can be directed by compilers [14], users [3], or the runtime system <ref> [2, 24, 42, 33] </ref>. Another major area enabled by SVM progress is re-search in the application layer itself for these systems. Structuring applications or data structures more appropriately for page-grained SVM can improve performance dramatically [22].
Reference: [43] <author> R. Stets, S. Dwarkadas, N. Hardavellas, G. Hunt, L. Kon-tothanassis, S. Parthasarathy, and M. Scott. Cashmere-2l: </author> <title> Software coherent shared memory on a clustered remote-write network. </title> <booktitle> In Proceedings of the Sixteenth Symposium on Operating Systems Principles, </booktitle> <year> 1997. </year>
Reference-contexts: Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform.
Reference: [44] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: techniques are less likely to be effective except when the overhead is o*oaded to a separate communication assist. 5 Application-driven Evaluation and Perfor mance Portability A significant limitation of most of the studies performed in the past to evaluate protocol tradeoffs has been in the applications they use. "Standard" applications <ref> [44] </ref> that are used for hardware-coherent shared memory have only recently begun to be used, and most programs that have been used have been kernels with mostly very simple behavior.
Reference: [45] <author> D. Yeung, J. Kubiatowicz, and A. Agarwal. Mgs: </author> <title> A multigrain shared memory system. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform. <p> For example, an SVM protocol may be implemented across clusters of distributed-memory, hardware-coherent machines rather than SMPs, with performance hopefully increasing as node size increases relative to the number of nodes. A start in the latter area has been made <ref> [45] </ref>, but with a relatively simple protocol.
Reference: [46] <author> S. Zhou, M. Stumm, K. Li, and D. Wortman. </author> <title> Heterogeneous distributed shared memory: An experimental study. </title> <journal> IEEE Transactions on Parallel and Distributed Computing, </journal> <volume> 3(5) </volume> <pages> 540-554, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors [48] 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes <ref> [46, 10, 23, 15, 45, 36, 5, 43, 37] </ref>. A software shared memory layer provides a uniform, coherent shared memory programming model rather than a hybrid message-passing/shared-memory interface for this increasingly important platform.
Reference: [47] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance evaluation of two home-based lazy release consistency protocols for shared virtual memory systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Since then, SVM research has witnessed a very active and fruitful decade (see Figure 1), with many research groups building on one anothers ideas to push performance higher <ref> [4, 21, 39, 38, 47, 2, 19, 30] </ref>. <p> Of the three layers that affect end performance (application, protocol/model, and communication architecture), most of the efforts so far have been in the lower two: relaxed consistency models and protocol implementations to reduce communication frequency and traffic <ref> [4, 21, 47, 2] </ref>, and additional hardware support in the communication architecture to reduce communication costs [39, 11, 35, 19, 18, 7, 30, 29]. <p> In fact, the history of SVM research so far is largely a history of protocol laziness. This research also led to the distinction between single-writer and multiple writer protocols [24], as well as between home-based <ref> [19, 21, 47] </ref> and no-home or distributed [27, 42] data collection. Greater laziness typically implies less frequent communication and protocol operations, but also greater programming difficulty and protocol state. <p> Second, the need to retain diffs at the writers (like write notices) can generate very large memory consumption, even larger than the application data set size itself <ref> [47] </ref>, greatly limiting scalability. Periodic garbage collection is needed, which adds overhead. Third, the diffs to be transferred can accumulate rapidly and increase communication traffic. For example, under a single-writer, migratory sharing pattern, each process that acquires a lock fetches the diffs, applies them and generates new diffs. <p> A home node is selected for each shared page and modifications are propagated eagerly (at or before a release) to the home only [19]. This can be done either using diffs <ref> [21, 47] </ref> or architectural support as discussed later [19, 30]. Unlike the home copy, the non-home copies of a page are kept coherent through invalidations as usual. On a page fault, a non-home processor obtains the whole page from the home rather than obtaining diffs from previous writers. <p> Table 3 compares the all-software home-based multiple-writer LRC (HLRC) protocol with a traditional no-home, multiple-writer LRC protocol. The former is found to perform substantially better on this platform, with the performance gap increasing with the number of processors <ref> [47] </ref>. Similar results are seen in earlier, simulation-based comparisons between a home-based protocol with some hardware support and a no-home LRC for a wider range of applications [19, 20]. <p> On a release, invalidations are sent eagerly to sharers (ERC); Benchmarks LRC HLRC LU 11.5 13.9 Water-Nsq 11.7 18.9 Water-Spatial 14 20 Raytrace 10.6 26.8 Table 3: Speedups for traditional no-home LRC and HLRC on a 32-processor Intel Paragon <ref> [47] </ref>. on receiving invalidations, those sharers that hold locally modified copies of the page compute and return their diffs to the releasing processor, bringing it up to date. <p> A recent study [24] shows that an adaptive no-home LRC can outperform an HLRC protocol, but the comparison is not conclusive since few real applications are used, the number of processors is small, and the HLRC implementation is slightly different from that described in <ref> [47] </ref>. The tradeoff between these approaches needs to be better understood. 3.3.4 Alternative All-Software Propagation An alternative all-software method to track and merge modifications in multiple-writer protocols is to maintain per-word dirty bits in software [4]. This requires instrumenting memory operations in the program, incurring runtime overhead. <p> Note that it is not only the scalability of performance that matters, but also the scalability of memory needed by the protocol for its data structures, so eager and home-based protocols may have an advantage as the number of nodes increases <ref> [47] </ref>, and protocols should scale their memory needs with the number of nodes, not processors [36]. Further protocol innovations may be needed, and the area bears further investigation. 7 Conclusions There has been a lot of progress in shared virtual memory over the last several years.
Reference: [48] <author> Y. Zhou, L. Iftode, J.P. Singh, K.Li, B.R. Toonen, I.Schoinas, M.D. Hill, and D.A. Wood. </author> <title> Relaxed consistency and coherence granularity in dsm systems: A performance evaluation. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1997. </year>
Reference-contexts: only on laziness, a single-writer DC protocol, a single-writer LRC protocol (similar to that in [28]), a multiple-writer LRC protocol (similar to that in Section 3.3.2) and a sequentially consistent protocol (all invalidation-based) have been compared on a number of SPLASH-2 applications that cover most of the key sharing patterns <ref> [48] </ref>. Table 2 shows some of the results as measured on the Wisconsin Typhoon-zero cluster [39]. The lazier application and propagation of the LRC protocol have significant advantages over the DC protocol, especially in complex irregular applications that use substantial lock synchronization. <p> LU 8.6 8.6 8.4 8.3 Ocean 2.7 3.8 5.7 8.3 Water-Nsquared 11.2 11.3 11.3 11.2 Volrend 0.8 1.7 2.9 9 Water-Spatial 4.9 5.8 7.3 12 Raytrace 6.6 8 9 13 Barnes 0.9 1.9 2.2 6 Table 2: Speedups for SC, Single-writer DC and LRC, and Multiple-writer LRC Protocols, 16 processors <ref> [48] </ref>. The multiple-writer solution used is the software home-based one described in Section 3.3.2. The trend today is toward lazy, multiple-writer protocols. However, the performance/storage/complexity tradeoffs in laziness are not yet clear for emerging plat forms and real applications and bear further research (see also Section 4). <p> Among protocol-level optimizations, support for multiple coherence granularities on a per data structure basis is particularly useful, though it does rely on support from the programmer. A recent study compared the fine-grained and page-grained approaches using four different consistency protocols on a fairly large and varied set of applications <ref> [48] </ref>. The platform provided a uniform hardware access control mechanism for both approaches but ran the protocols in software. <p> Benchmarks FG- SC HLRC LU 6.3 8.3 Ocean 6.1 8.3 Water-Nsquared 11.4 11.1 Volrend 6 9 Water-Spatial 11.6 11.1 Raytrace 12.2 13 Barnes 7 6 Table 4: Fine-grain SC and HLRC, 16 processors <ref> [48] </ref> 6.2 SVM across Multiprocessor Clusters Another recent development is motivated by the increasing popularity of small-scale hardware-coherent multiprocessors and the ease of constructing "systems of systems" or clusters that use these multiprocessors as their nodes [46, 10, 23, 15, 45, 36, 5, 43, 37].
References-found: 48

