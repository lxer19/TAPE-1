URL: http://www.bioele.nuee.nagoya-u.ac.jp/wsc1/papers/files/backer.ps.gz
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Keyword: Genetic Programming, Machine learning, Missing data, Strongly Typed Genetic Programming  
Abstract: Learning with imprecise or missing data has been a major challenge for machine learning. A new approach using Strongly Typed Genetic Programming is proposed here, which uses extra computations based on other input data to approximate the missing values. It eliminates the need for preprocessing and makes use of correlations between the input data. The decision process itself and the handling of unknown data can be extracted from the resulting program for an analysis afterwards. Comparing it to an alternative approach on a simple example shows the usefulness of this approach. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andre, D. </author> <year> (1994): </year> <title> Automatically Defined Features: The Simultaneous Evolution of 2-Dimensional Feature Detectors and an Algorithm for Using Them. </title> <editor> In (Kinnear, </editor> <booktitle> 1994), </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Even worse, every method will have the same problems with missing values again, as there can be missing values among the other input data. Extra computations for the missing values This is where the new approach begins. Like coevolving classifiers and detectors <ref> (e.g. Andre, 1994) </ref> it is possible to coevolve approximators and a main program that makes use of the approximations in one GP-run. There are only some minor changes to be made on the normal preparation of a GP-run, when using Strongly Typed Genetic Programming as defined in (Montana, 1994).
Reference: <author> Backer, G. </author> <year> (1995): </year> <title> Lernen mittels Genetischer Pro-grammierung. </title> <type> Master Thesis, </type> <institution> Technische Universi-tt Braunschweig. (german) Forsstrm, J. </institution> <year> (1992): </year> <title> Machine Learning in Clinical Medicine by Knowledge Acquisition from Patient Databases. </title> <publisher> Academic Dissertation, University of Turku. </publisher>
Reference-contexts: More relevant for learning a good classification is the performance of resulting programs on examples that are not learned, the quality of generalization. In Genetic Programming this seems to have been forgotten quite often <ref> (Backer, 1995) </ref>. Therefore besides looking at the fitness, the resulting programs are also tested on 60 non-fitness examples, so that the generalizing capabilities can be examined. In the original version without missing values, the best programs (on the learned data) classified the unknown examples with 3,3 - 6,6 % errors. <p> % extra computations 12.2 % 5.0 % plausible values 20.8 % 10 % Table 2: error rates of resulting programs on new examples (60 non-fitness cases) Extra computations on unknown values have also been applied to a more complex problem the diagnosis of the disease Nephropatia Epidemica (Forsstrm, 1992) in <ref> (Backer, 1995) </ref>. They have been shown to be useful there too, but there are a lot of other factors to be considered, making the role of this approach less separable from other aspects. <p> It allows for insight into the solution and the handling of missing values. Examples with missing values can be used without further preprocessing. Notes 1 The experiments were done as part of my master thesis <ref> (Backer, 1995) </ref>. 2 To get a more detailed explanation, take a look at my ho-mepage, which includes a HTML-Version of this article with some additional information: http://www.psych.nat.tu-bs.de/gb/gb.htm
Reference: <editor> Kinnear, K.E. Jr., Editor (1994): </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Koza, J.R. </author> <year> (1992): </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Introduction Noisy, vague and incomplete data are common in real-world learning problems. Only recently the different areas of soft computing have adressed this problem. In this article the possibility of learning from examples with incomplete or missing data through genetic programming <ref> (Koza, 1992) </ref> will be shown. Because of the expensiveness of data acquisition for examples to learn from or because of other reasons, the examples that can be used for learning are often not complete. <p> There are only some minor changes to be made on the normal preparation of a GP-run, when using Strongly Typed Genetic Programming as defined in (Montana, 1994). In short, Strongly Typed Genetic Programming (STGP) introduces data-types into genetic programming. GP as defined in <ref> (Koza, 1992) </ref> makes no use of a type-concept. In nowadays programming languages, it is very common to assign data-types to variables, constants and functions, thus seperating for example real numbers from characters. <p> It may be convenient to use an automatic simplifier on the program or to simplify the program "by hand" first, because some occurences of the statement may not be used at all <ref> (see Koza, 1992) </ref>. All this can be seen in Prog. 1, which is a result of the approach with extra computations applied to the Iris-data with 30 % missing values.
Reference: <author> Montana, D.J. </author> <year> (1994): </year> <title> Strongly Typed Genetic Programming. </title> <type> BBN Technical Report #7866, </type> <institution> Bolt Be-ranek and Newman, Inc. </institution>
Reference-contexts: Andre, 1994) it is possible to coevolve approximators and a main program that makes use of the approximations in one GP-run. There are only some minor changes to be made on the normal preparation of a GP-run, when using Strongly Typed Genetic Programming as defined in <ref> (Montana, 1994) </ref>. In short, Strongly Typed Genetic Programming (STGP) introduces data-types into genetic programming. GP as defined in (Koza, 1992) makes no use of a type-concept. In nowadays programming languages, it is very common to assign data-types to variables, constants and functions, thus seperating for example real numbers from characters.
Reference: <author> Murphy, P.M., Aha, D.W. </author> <year> (1994): </year> <title> UCI Repository of Machine Learning Darabases [machine-readable data repository]. </title> <institution> University of California: Dept. of Information and Computer Science, </institution> <address> Irvine, CA. </address>
Reference-contexts: Experiment A simple and well-known machine learning task should be used for demonstrating some of the features of using extra computations for missing values. It is the classification of iris-plants from the Machine learning-database <ref> (Murphy et al., 1994) </ref>, which con-tains 150 examples of three types of Iris-plants, namely iris-versicolour, iris-setosa and iris-virginica with length and width of petal and sepal in mm (reals ranging from 1 to 79)1.
References-found: 6

