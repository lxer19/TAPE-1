URL: ftp://ftp.rpi.edu/pub/math/tech/RPImr217.ps
Refering-URL: http://www.rpi.edu/~bennek/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Parametric Optimization Method for Machine Learning  
Author: Kristin P. Bennett Erin J. Bredensteiner 
Date: January 31, 1995  
Affiliation: R.P.I. Math  
Pubnum: Report No. 217  
Abstract: The classification problem of constructing a plane to separate the members of two sets can be formulated as a parametric bilinear program. This approach was originally created to minimize the number of points misclassified. However, a novel interpretation of the algorithm is that the subproblems represent alternative error functions of the misclassified points. Each subproblem identifies a specified number of outliers and minimizes the magnitude of the errors on the remaining points. A tuning set is used to select the best result amoung the subproblems. A parametric Frank-Wolfe method was used to solve the bilinear subproblems. Computational results on a number of datasets indicate that the results compare very favorably with linear programming and simulated annealing approaches. The algorithm can be used as part of a decision tree algorithm to create nonlinear classifiers.
Abstract-found: 1
Intro-found: 1
Reference: [Ben92] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: When the decisions are multivariate linear functions of the input attributes, exhaustive search is no longer feasible. Linear programming and perceptron algorithms have been used to construct decisions that minimize the distances of the misclassified points from the separating plane. Linear programming approaches <ref> [BM92, Ben92, Glo90] </ref> find optimal decisions by this criterion in polynomial time. Decision tree methods based on heuristic variants of perceptron algorithms [Utg89, BU92] have worked well in practice, but the algorithms may fail to converge and may not find optimal solutions.
Reference: [BFOS84] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International, </booktitle> <address> California, </address> <year> 1984. </year>
Reference-contexts: This parametric approach includes the linear program that minimizes the average misclassification error as a subproblem [BM92]. This research investigates mathematical programming methods for constructing decisions in decision trees. Classical decision tree algorithms such as CART <ref> [BFOS84] </ref> and ID3 [Qui84] use exhaustive search to find decisions based on a single input attribute. When the decisions are multivariate linear functions of the input attributes, exhaustive search is no longer feasible.
Reference: [BM92] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: Email bennek@rpi.edu, bredee@rpi.edu. This material is based on research supported by National Science Foundation Grant 949427. 1 can be such that it minimizes the distances of the misclassified points from the separating plane <ref> [BM92] </ref>. In misclassification minimization the problem is to minimize the number of misclassified points. For a given problem, different error functions may result in better (or worse) separators in terms of generalization. <p> In practice, the choice of error functions is not always clear. Thus we propose a hybrid approach (parametric misclassification minimization) that identifies the outliers and minimizes the distances of the remaining misclassified points. This parametric approach includes the linear program that minimizes the average misclassification error as a subproblem <ref> [BM92] </ref>. This research investigates mathematical programming methods for constructing decisions in decision trees. Classical decision tree algorithms such as CART [BFOS84] and ID3 [Qui84] use exhaustive search to find decisions based on a single input attribute. <p> When the decisions are multivariate linear functions of the input attributes, exhaustive search is no longer feasible. Linear programming and perceptron algorithms have been used to construct decisions that minimize the distances of the misclassified points from the separating plane. Linear programming approaches <ref> [BM92, Ben92, Glo90] </ref> find optimal decisions by this criterion in polynomial time. Decision tree methods based on heuristic variants of perceptron algorithms [Utg89, BU92] have worked well in practice, but the algorithms may fail to converge and may not find optimal solutions. <p> When ffi = 0 the subproblem (7) is precisely the following linear program <ref> [BM92] </ref> which minimizes the average magnitude of the misclassification errors, i.e. min 1 k ev subject to u + Aw efl e 0 v Bw + efl e 0 (8) The parametric bilinear problem requires the solution of a series of subproblems for different values of ffi.
Reference: [BM93] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Bilinear separation of two sets in n-space. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 207-227, </pages> <year> 1993. </year>
Reference-contexts: In Section 2, we investigate the parametric bilinear programming formulation of the misclassi-fication minimization program, first proposed by Mangasarian [Man94]. We discuss a novel interpretation of the suboptimal solutions as an alternative error criterion. In Section 3, we propose an algorithm based on the Frank-Wolfe method discussed in <ref> [BM93] </ref> for solving the parametric bilinear programming problem. This algorithm is attractive because half of the subproblems have closed form solutions. Computational results on a number of practical problems are given in Section 4. The following notation is used. <p> We begin by describing the common parts of both approaches. First subproblem (7) is solved for ffi = 0. This corresponds to solving the LP (8) to find the plane that minimizes the average distance of misclassified points to the plane. For ffi &gt; 0, a Frank-Wolfe type algorithm <ref> [BM93] </ref> is used to solve bilinear subproblem (7). This algorithm has the beneficial property that it decomposes the problem into two linear programs one of which has a closed form integer solution. Note that for ffi &gt; 0 the bilinear subproblem is nonconvex and may have local minima. <p> Details of all the algorithms are given below. 3.1 Bilinear Subproblems The parametric bilinear programming formulation (7) is an uncoupled bilinear program. The Frank-Wolfe algorithm applied to an uncoupled bilinear program will converge to a global solution or a stationary point <ref> [BM93] </ref>.
Reference: [BU92] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Multivariate decision trees. </title> <type> COINS Technical Report 92-83, </type> <institution> University of Massachusetts, Amherst, Massachusetts, </institution> <year> 1992. </year> <note> To appear in Machine Learning. </note>
Reference-contexts: Linear programming and perceptron algorithms have been used to construct decisions that minimize the distances of the misclassified points from the separating plane. Linear programming approaches [BM92, Ben92, Glo90] find optimal decisions by this criterion in polynomial time. Decision tree methods based on heuristic variants of perceptron algorithms <ref> [Utg89, BU92] </ref> have worked well in practice, but the algorithms may fail to converge and may not find optimal solutions. The problem of creating a linear function that minimizes the number of points misclassified is NP-complete [Hea92].
Reference: [CPL94] <institution> CPLEX Optimization Incorporated, </institution> <address> Incline Village, Nevada. Using the CPLEX Callable Library, </address> <year> 1994. </year>
Reference-contexts: MISMIN was implemented in AMPL [FGK93], a mathematical programming software package, utilizing the CPLEX 3.0 <ref> [CPL94] </ref> solver. We present results for LP (8), MISMIN, and MISMIN-P. Computational results were tabulated for each of these choices. For MISMIN-P we used the testing set for the tuning set in order to see what the best answer would be.
Reference: [DJS + 89] <author> R. Detrano, A. Janosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, and V. Froelicher. </author> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease. </title> <journal> American Journal of Cardiology, </journal> <volume> 64 </volume> <pages> 304-310, </pages> <year> 1989. </year>
Reference-contexts: So MISMIN-P should do at least as well as MISMIN and LP (8) alone. 4 Computational Results In this section, we present results of computational experiments performed using MISMIN-P on four real world data sets: Cleveland Heart Disease Database <ref> [DJS + 89] </ref>, Wisconsin Breast Cancer Database [WM90] and Star/Galaxy Dim and Bright data sets [OSP + 92]. MISMIN was implemented in AMPL [FGK93], a mathematical programming software package, utilizing the CPLEX 3.0 [CPL94] solver. We present results for LP (8), MISMIN, and MISMIN-P.
Reference: [FGK93] <author> R. Fourer, D. Gay, and B. Kernighan. </author> <title> AMPL A Modeling Language for Mathematical Programming. </title> <publisher> Boyd and Frazer, </publisher> <address> Danvers, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: MISMIN was implemented in AMPL <ref> [FGK93] </ref>, a mathematical programming software package, utilizing the CPLEX 3.0 [CPL94] solver. We present results for LP (8), MISMIN, and MISMIN-P. Computational results were tabulated for each of these choices.
Reference: [Glo90] <author> F. Glover. </author> <title> Improved linear programming models for discriminant analysis. </title> <journal> Decision Sciences, </journal> <volume> 21 </volume> <pages> 771-785, </pages> <year> 1990. </year>
Reference-contexts: When the decisions are multivariate linear functions of the input attributes, exhaustive search is no longer feasible. Linear programming and perceptron algorithms have been used to construct decisions that minimize the distances of the misclassified points from the separating plane. Linear programming approaches <ref> [BM92, Ben92, Glo90] </ref> find optimal decisions by this criterion in polynomial time. Decision tree methods based on heuristic variants of perceptron algorithms [Utg89, BU92] have worked well in practice, but the algorithms may fail to converge and may not find optimal solutions.
Reference: [Hea92] <author> David Heath. </author> <title> A geometric framework for machine learning. </title> <type> PhD thesis, </type> <institution> The John Hopkins University, </institution> <year> 1992. </year>
Reference-contexts: Decision tree methods based on heuristic variants of perceptron algorithms [Utg89, BU92] have worked well in practice, but the algorithms may fail to converge and may not find optimal solutions. The problem of creating a linear function that minimizes the number of points misclassified is NP-complete <ref> [Hea92] </ref>. The algorithms CSADT [HKS93] and OC1 [MKSB93, MKS94] use simulated annealing to minimize functions of the number of misclassified points. Using mathematical programming, we have developed an algorithm that combines the two error criteria and exploits the mathematical structure of the underlying problem in order to find better solutions. <p> The primary limitations are that the problem is NP-complete <ref> [Hea92] </ref> and has infinitely many local minima [Man94]. Thus the problem may require extensive computational time. The parametric bilinear 2 ..... Plane 1 |- Plane 2 minimizing the distance of the misclassified points from the plane.
Reference: [HKS93] <author> D. Heath, S. Kasif, and S. Salzberg. </author> <title> Learning oblique decision trees. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Artificial Intelligence, </booktitle> <pages> pages 1002-1007, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The problem of creating a linear function that minimizes the number of points misclassified is NP-complete [Hea92]. The algorithms CSADT <ref> [HKS93] </ref> and OC1 [MKSB93, MKS94] use simulated annealing to minimize functions of the number of misclassified points. Using mathematical programming, we have developed an algorithm that combines the two error criteria and exploits the mathematical structure of the underlying problem in order to find better solutions.
Reference: [Lue84] <author> D. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1984. </year>
Reference-contexts: Then u = (A w + efl + e) + (9) Proof. Let l i 0; i = 1 : : : 9 be the optimal dual variables for problem (7). The optimal w; fl; r; u; s; v are feasible and satisfy the remaining optimality conditions <ref> [Lue84] </ref>: l 1 (u + Aw efl e) = 0 l 3 (u) = 0 l 4 (v) = 0 l 7 (r) = 0 l 8 (s) = 0 1 k (sB) l 1 A + l 2 B = 0 m (re) + 1 1 1 1 1 (10) <p> Consider the optimality conditions <ref> [Lue84] </ref> of the bilinear program (7) and LP (12). Clearly the primal constraints of LP (12) are satisfied since they are a subset of the constraints of (7). The dual variables l i 0; i = 1; : : : ; m satisfy the optimality conditions given in (10).
Reference: [MA92] <author> P.M. Murphy and D.W. Aha. </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, Irvine, California, </institution> <year> 1992. </year>
Reference-contexts: Each patient is classified as to whether there is presence or absence of heart disease. There are 137 patients who have a presence of heart disease. This data set is available via anonymous file transfer protocol (ftp) from the University of California Irvine UCI Repository Of Machine Learning Databases <ref> [MA92] </ref>. Wisconsin Breast Cancer Database This data set is used to classify a set of 682 patients with breast cancer. Each patient is represented by nine integral attributes ranging in value from 1 to 10. <p> The two classes represented are benign and malignant: 442 of the patients are benign while 240 are malignant. This data set is also available via anonymous ftp from the University of California Irvine UCI Repository Of Machine Learning Databases <ref> [MA92] </ref>. Star/Galaxy Databases The Star/Galaxy Database consists of two data sets: dim and bright. The dim data set has 4192 examples and the bright data set has 2462 examples. Each example represents a star or a galaxy and is described by 14 numeric attributes.
Reference: [Man94] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-232, </pages> <year> 1994. </year>
Reference-contexts: Using mathematical programming, we have developed an algorithm that combines the two error criteria and exploits the mathematical structure of the underlying problem in order to find better solutions. In Section 2, we investigate the parametric bilinear programming formulation of the misclassi-fication minimization program, first proposed by Mangasarian <ref> [Man94] </ref>. We discuss a novel interpretation of the suboptimal solutions as an alternative error criterion. In Section 3, we propose an algorithm based on the Frank-Wolfe method discussed in [BM93] for solving the parametric bilinear programming problem. This algorithm is attractive because half of the subproblems have closed form solutions. <p> 1] n with components (x fl ) i := 0 if (x fl ) i 0 and (x fl ) i := 1 if (x fl ) i &gt; 0; i = 1; : : : ; n. 2 Misclassification Minimization In this section, we investigate the misclassification minimization problem <ref> [Man94] </ref> which minimizes the number of points misclassified by a plane and discuss its benefits and limitations. The primary limitations are that the problem is NP-complete [Hea92] and has infinitely many local minima [Man94]. Thus the problem may require extensive computational time. The parametric bilinear 2 ..... <p> : : : ; n. 2 Misclassification Minimization In this section, we investigate the misclassification minimization problem <ref> [Man94] </ref> which minimizes the number of points misclassified by a plane and discuss its benefits and limitations. The primary limitations are that the problem is NP-complete [Hea92] and has infinitely many local minima [Man94]. Thus the problem may require extensive computational time. The parametric bilinear 2 ..... Plane 1 |- Plane 2 minimizing the distance of the misclassified points from the plane. <p> As shown by Mangasarian <ref> [Man94] </ref> this problem can be reformulated into a linear program with equilibrium constraints: min er + es r 0 s 0 r + e 0 s + e 0 u (r + e) = 0 v (s + e) = 0 It has been demonstrated [Man94] that this linear program has <p> As shown by Mangasarian <ref> [Man94] </ref> this problem can be reformulated into a linear program with equilibrium constraints: min er + es r 0 s 0 r + e 0 s + e 0 u (r + e) = 0 v (s + e) = 0 It has been demonstrated [Man94] that this linear program has a stationary point for almost every (w; fl). <p> By careful choice of the sequence of ffi and by using the solution of one subproblem as the starting point for the next subproblem, we found better solutions and reduced the computation time. The secant method proposed but not implemented in <ref> [Man94] </ref> was used to select the values of ffi: MISMIN-P is a parametric misclassification minimization algorithm that selects the best bilinear subproblem (7) solved within the MISMIN algorithm by using a tuning set. The plane that performs best on a reserved set of points is selected as the final plane.
Reference: [MKS94] <author> S. Murthy, S. Kasif, and S. Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-32, </pages> <year> 1994. </year> <month> 12 </month>
Reference-contexts: The problem of creating a linear function that minimizes the number of points misclassified is NP-complete [Hea92]. The algorithms CSADT [HKS93] and OC1 <ref> [MKSB93, MKS94] </ref> use simulated annealing to minimize functions of the number of misclassified points. Using mathematical programming, we have developed an algorithm that combines the two error criteria and exploits the mathematical structure of the underlying problem in order to find better solutions. <p> The very promising results in Table 1 are for a single plane. OC1 is designed to construct decision trees with many decisions. In <ref> [MKS94, MKSB93] </ref> OC1 was found to construct simpler trees that generalized better than those of other univariate decison tree approaches. In Table 2, we compare the best results reported in [MKSB93] for OC1 with those for MISMIN-P.
Reference: [MKSB93] <author> S. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 322-327, </pages> <address> Boston, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The problem of creating a linear function that minimizes the number of points misclassified is NP-complete [Hea92]. The algorithms CSADT [HKS93] and OC1 <ref> [MKSB93, MKS94] </ref> use simulated annealing to minimize functions of the number of misclassified points. Using mathematical programming, we have developed an algorithm that combines the two error criteria and exploits the mathematical structure of the underlying problem in order to find better solutions. <p> For MISMIN-P we used the testing set for the tuning set in order to see what the best answer would be. In practice the tuning set must not include the testing data, so this should be regarded as an optimistic estimate. OC1 <ref> [MKSB93] </ref>, a simulated annealing algorithm, was applied to make comparisons on the dependability of the MISMIN results. OC1 is an algorithm that generates multivariate decision trees based on deterministic and randomized procedures. <p> The results obtained for OC1 represent the accuracy of the root hyperplane of the decision tree constructed by this algorithm. The defaults were chosen for all parameters in OC1 except no pruning portion was used and 50 iterations were chosen (the value used in <ref> [MKSB93] </ref>). In correspondence with misclassification minimization the sum minority impurity measure was applied. Training and testing set accuracies were measured for each dataset. <p> The very promising results in Table 1 are for a single plane. OC1 is designed to construct decision trees with many decisions. In <ref> [MKS94, MKSB93] </ref> OC1 was found to construct simpler trees that generalized better than those of other univariate decison tree approaches. In Table 2, we compare the best results reported in [MKSB93] for OC1 with those for MISMIN-P. <p> OC1 is designed to construct decision trees with many decisions. In [MKS94, MKSB93] OC1 was found to construct simpler trees that generalized better than those of other univariate decison tree approaches. In Table 2, we compare the best results reported in <ref> [MKSB93] </ref> for OC1 with those for MISMIN-P. Once again 10-fold cross validation was used to estimate the the average testing set accuracy and number of decisions used. There is no significant difference in the testing set accuracies for the two methods.
Reference: [OSP + 92] <author> S. Odewahn, E. Stockwell, R. Pennington, R Humphreys, and W Zumach. </author> <title> Automated star/galaxy discrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103(1) </volume> <pages> 318-331, </pages> <year> 1992. </year>
Reference-contexts: at least as well as MISMIN and LP (8) alone. 4 Computational Results In this section, we present results of computational experiments performed using MISMIN-P on four real world data sets: Cleveland Heart Disease Database [DJS + 89], Wisconsin Breast Cancer Database [WM90] and Star/Galaxy Dim and Bright data sets <ref> [OSP + 92] </ref>. MISMIN was implemented in AMPL [FGK93], a mathematical programming software package, utilizing the CPLEX 3.0 [CPL94] solver. We present results for LP (8), MISMIN, and MISMIN-P. Computational results were tabulated for each of these choices. <p> The bright data set is nearly linearly separable, while the dim data set is significantly more difficult. These two data sets are generated from a large set of star and galaxy images collected by Odewahn <ref> [OSP + 92] </ref> at the University of Minnesota. Table 1 shows that MISMIN performs substantially better than OC1. Not all the differences are statistically significant, but the trends are clear. Both this version of the OC1 algorithm and MISMIN are optimizing the same error function: the number of misclassified points.
Reference: [Qui84] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1984. </year>
Reference-contexts: This parametric approach includes the linear program that minimizes the average misclassification error as a subproblem [BM92]. This research investigates mathematical programming methods for constructing decisions in decision trees. Classical decision tree algorithms such as CART [BFOS84] and ID3 <ref> [Qui84] </ref> use exhaustive search to find decisions based on a single input attribute. When the decisions are multivariate linear functions of the input attributes, exhaustive search is no longer feasible.
Reference: [Utg89] <author> P. E. Utgoff. </author> <title> Perceptron trees: A case study in hybrid concept representations. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 377-391, </pages> <year> 1989. </year>
Reference-contexts: Linear programming and perceptron algorithms have been used to construct decisions that minimize the distances of the misclassified points from the separating plane. Linear programming approaches [BM92, Ben92, Glo90] find optimal decisions by this criterion in polynomial time. Decision tree methods based on heuristic variants of perceptron algorithms <ref> [Utg89, BU92] </ref> have worked well in practice, but the algorithms may fail to converge and may not find optimal solutions. The problem of creating a linear function that minimizes the number of points misclassified is NP-complete [Hea92].
Reference: [WM90] <author> W. H. Wolberg and O.L. Mangasarian. </author> <title> Multisurface method of pattern separation for medical diagnosis applied to breast cytology. </title> <booktitle> Proceedings of the National Academy of Sciences,U.S.A., </booktitle> <volume> 87 </volume> <pages> 9193-9196, </pages> <year> 1990. </year> <month> 13 </month>
Reference-contexts: So MISMIN-P should do at least as well as MISMIN and LP (8) alone. 4 Computational Results In this section, we present results of computational experiments performed using MISMIN-P on four real world data sets: Cleveland Heart Disease Database [DJS + 89], Wisconsin Breast Cancer Database <ref> [WM90] </ref> and Star/Galaxy Dim and Bright data sets [OSP + 92]. MISMIN was implemented in AMPL [FGK93], a mathematical programming software package, utilizing the CPLEX 3.0 [CPL94] solver. We present results for LP (8), MISMIN, and MISMIN-P. Computational results were tabulated for each of these choices.
References-found: 20

