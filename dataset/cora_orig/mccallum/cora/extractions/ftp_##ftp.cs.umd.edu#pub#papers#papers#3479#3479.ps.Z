URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3479/3479.ps.Z
Refering-URL: http://www.ics.uci.edu/~mlearn/MLlist/v7/12.html
Root-URL: 
Email: lawrence@elec.uq.oz.au, fgiles,sandiwayg@research.nj.nec.com  
Title: On the Applicability of Neural Network and Machine Learning Methodologies to Natural Language Processing  
Author: Steve Lawrence C. Lee Giles Sandiway Fong 
Note: Also with  Also with the  
Address: 4 Independence Way Princeton, NJ 08540  College Park, MD 20742  4072, Australia.  College Park, MD 20742.  
Affiliation: NEC Research Institute  Institute for Advanced Computer Studies University of Maryland  Electrical and Computer Engineering, University of Queensland, St. Lucia Qld  Institute for Advanced Computer Studies, University of Maryland,  
Pubnum: Technical Report UMIACS-TR-95-64 and CS-TR-3479  
Abstract: We examine the inductive inference of a complex grammar specifically, we consider the task of training a model to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. We investigate the following models: feed-forward neural networks, Fransconi-Gori-Soda and Back-Tsoi locally recurrent networks, Elman, Narendra & Parthasarathy, and Williams & Zipser recurrent networks, Euclidean and edit-distance nearest-neighbors, simulated annealing, and decision trees. The feed-forward neural networks and non-neural network machine learning models are included primarily for comparison. We address the question: How can a neural network, with its distributed nature and gradient descent based iterative calculations, possess linguistic capability which is traditionally handled with symbolic computation and recursive processes? Initial simulations with all models were only partially successful by using a large temporal window as input. Models trained in this fashion did not learn the grammar to a significant degree. Attempts at training recurrent networks with small temporal input windows failed until we implemented several techniques aimed at improving the convergence of the gradient descent training algorithms. We discuss the theory and present an empirical study of a variety of models and learning algorithms which highlights behaviour not present when attempting to learn a simpler grammar. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert B. Allen. </author> <title> Sequential connectionist networks for answering simple questions about a microworld. </title> <booktitle> In 5th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 489-495, </pages> <year> 1983. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [7] [20] [17] [18] [19]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] <ref> [1] </ref> [11] [23] [32]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32].
Reference: [2] <author> A.D. Back and A.C. Tsoi. </author> <title> FIR and IIR synapses, a new neural network architecture for time series modelling. </title> <journal> Neural Computation, </journal> <volume> 3(3) </volume> <pages> 337-350, </pages> <year> 1991. </year>
Reference-contexts: Fransconi-Gori-Soda define local-output and local-activation versions of the architecture where the feedback is taken from the respective points. We have used the local-output version where the output of a node, y (t) = f (wy (t 1) + P n 3. Back-Tsoi FIR <ref> [2] </ref>. An FIR filter and gain term is included in every synapse. 6 Sequences of length zero up to the actual sequence length are considered.
Reference: [3] <author> E.B. Baum and F. Wilczek. </author> <title> Supervised learning of probability distributions by neural networks. </title> <editor> In D.Z. An-derson, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pages 52-61, </pages> <address> New York, 1988. (Denver 1987), </address> <publisher> American Institute of Physics. </publisher>
Reference: [4] <author> N. Chomsky. </author> <title> Three models for the description of language. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> IT-2:113-124, </volume> <year> 1956. </year>
Reference-contexts: PropN ! John j Mary V ! chase j feed j see... In the Chomsky hierarchy of phrase structured grammars, the simplest grammar and its associated automata are regular grammars and finite-state-automata (FSA). However, it has been firmly established <ref> [4] </ref> that the syntactic structures of natural language cannot be parsimoniously described by regular languages.
Reference: [5] <author> N.A. Chomsky. </author> <title> Lectures on Government and Binding. </title> <publisher> Foris Publications, </publisher> <year> 1981. </year>
Reference-contexts: In the light of such examples and the fact that such contrasts crop up not just in English but in other languages (for example, the stubborn contrast also holds in Dutch), some linguists (chiefly Chomsky <ref> [5] </ref>) have hypothesized that it is only reasonable that such knowledge is only partially acquired: the lack of variation found across speakers, and indeed, languages for certain classes of data suggests that there exists a fixed component of the language system.
Reference: [6] <author> N.A. Chomsky. </author> <title> Knowledge of Language: Its Nature, Origin, and Use. </title> <type> Prager, </type> <year> 1986. </year>
Reference-contexts: and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately describe in a formal system <ref> [6] </ref>? Here, we will provide a couple of examples of the kind of knowledge native speakers often take for granted.
Reference: [7] <author> A. Cleeremans, D. Servan-Schreiber, and J. McClelland. </author> <title> Finite state automata and simple recurrent recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 372-381, </pages> <year> 1989. </year>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [7] </ref> [20] [17] [18] [19]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32].
Reference: [8] <author> J. P. Crutchfield and K. Young. </author> <title> Computation at the onset of chaos. </title> <editor> In W. Zurek, editor, </editor> <title> Complexity, Entropy and the Physics of Information. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: The algorithm is currently impractical for anything except relatively small grammars [40]. 2 constitute complex, dynamical systems. Pollack [42] points out that Crutchfield and Young <ref> [8] </ref> have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars.
Reference: [9] <author> Christian Darken and John Moody. </author> <title> Note on learning rate schedules for stochastic optimization. </title> <booktitle> In Neural Information Processing Systems 3, </booktitle> <pages> pages 832-838. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Moody and Darkin have proposed "search then converge" learning rate schedules of the form <ref> [9] </ref> [10]: (t) = 1 + t (4) where (t) is the learning rate at time t, 0 is the initial learning rate, and t is a constant.
Reference: [10] <author> Christian Darken and John Moody. </author> <title> Towards faster stochastic gradient search. </title> <booktitle> In Neural Information Processing Systems 4, </booktitle> <pages> pages 1009-1016. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Moody and Darkin have proposed "search then converge" learning rate schedules of the form [9] <ref> [10] </ref>: (t) = 1 + t (4) where (t) is the learning rate at time t, 0 is the initial learning rate, and t is a constant.
Reference: [11] <author> Jeffrey L. Elman. </author> <title> Structured representations and connectionist models. </title> <booktitle> In 6th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 17-25, </pages> <year> 1984. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [7] [20] [17] [18] [19]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] <ref> [11] </ref> [23] [32]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51].
Reference: [12] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: Narendra and Parthasarathy. A feed-forward network augmented with feedback connections from the output nodes to the hidden nodes. As described in [39]. 5. Elman. A simple recurrent network with feedback from each hidden node to all hidden nodes as described in <ref> [12] </ref>, [13]. 6. Williams and Zipser. A fully recurrent network where all non-input nodes are connected to all other nodes as described in [55].
Reference: [13] <author> J.L. Elman. </author> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <journal> Machine Learning, </journal> 7(2/3):195-226, 1991. 
Reference-contexts: In this paper we consider replacing the inference algorithm with a neural network or a machine learning methodology. Our grammar is that of the English language. The simple grammar used by Elman <ref> [13] </ref> shown in figure 1 contains some of the structures in the complete English grammar: eg. agreement, verb argument structure, interactions with relative clauses, and recursion. S ! NP VP "." NP ! PropN j N j N RC VP ! V (NP) N ! boy j girl j cat... <p> Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions <ref> [13] </ref>, and that they are Turing equivalent [47]. The recurrent neural networks investigated in this paper 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. <p> Narendra and Parthasarathy. A feed-forward network augmented with feedback connections from the output nodes to the hidden nodes. As described in [39]. 5. Elman. A simple recurrent network with feedback from each hidden node to all hidden nodes as described in [12], <ref> [13] </ref>. 6. Williams and Zipser. A fully recurrent network where all non-input nodes are connected to all other nodes as described in [55]. <p> Dev. NMSE Std. Dev. Elman 0.387 0.023 0.405 0.14 W & Z 0.650 0.022 0.835 0.13 Table 5: Training set NMSE comparison for logistic and tanh sigmoid activation functions. that the initial training constrains later training in a useful way <ref> [13] </ref>. Results of four simulations per case comparing the use of sectioning with standard training on our problem are shown in table 6. The use of sectioning has consistently decreased performance.
Reference: [14] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda. Unified integration of explicit rules and learning by example in recurrent networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> 1992. To appear. </note>
Reference-contexts: Multi-layer perceptron. The output of a neuron is computed using 8 y l 0 N l1 X w l i A (1) 2. Frasconi-Gori-Soda locally recurrent networks. The Fransconi-Gori-Soda network has a locally recurrent globally feedforward architecture which includes a feedback connection around each hidden layer node <ref> [14] </ref>. Fransconi-Gori-Soda define local-output and local-activation versions of the architecture where the feedback is taken from the respective points. We have used the local-output version where the output of a node, y (t) = f (wy (t 1) + P n 3. Back-Tsoi FIR [2].
Reference: [15] <author> K.S. Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J, </address> <year> 1982. </year>
Reference-contexts: introduction we recommend Harrison [24] and Fu <ref> [15] </ref>. Briefly, a grammar G is a four tuple fN; T; P; Sg, where N and T are sets of terminals and nonterminals comprising the alphabet of the grammar, P is a set of production rules, and S is the start symbol. <p> with the procedures that can be used to infer the syntactic or production rules of an unknown grammar G based on a finite set of strings I from L (G), the language generated by G, and possibly also on a finite set of strings from the complement of L (G) <ref> [15] </ref>. In this paper we consider replacing the inference algorithm with a neural network or a machine learning methodology. Our grammar is that of the English language.
Reference: [16] <author> M. Gasser and C. Lee. </author> <title> Networks that learn phonology. </title> <type> Technical report, </type> <institution> Computer Science Department, Indiana University, </institution> <year> 1990. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32]. Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> [16] </ref> [21] [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex.
Reference: [17] <author> C.L. Giles, D. Chen, C.B. Miller, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Second-order recurrent neural networks for grammatical inference. </title> <booktitle> In 1991 IEEE INNS International Joint Conference on Neural Networks Seattle, </booktitle> <volume> volume II, </volume> <pages> pages 273-281, </pages> <address> Piscataway, NJ, 1991. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [7] [20] <ref> [17] </ref> [18] [19]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32].
Reference: [18] <author> C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [7] [20] [17] <ref> [18] </ref> [19]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32]. <p> Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] <ref> [18] </ref> on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [13], and that they are Turing equivalent [47].
Reference: [19] <author> C.L. Giles, C.B. Miller, D. Chen, G.Z. Sun, H.H. Chen, and Y.C. Lee. </author> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 317-324, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [7] [20] [17] [18] <ref> [19] </ref>. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32].
Reference: [20] <author> C.L. Giles, G.Z. Sun, H.H. Chen, Y.C. Lee, and D. Chen. </author> <title> Higher order recurrent networks & grammatical inference. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 380-387, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [7] <ref> [20] </ref> [17] [18] [19]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32].
Reference: [21] <author> M. Hare. </author> <title> The role of similarity in hungarian vowel harmony: A connectionist account. </title> <type> Technical Report CRL Tech report 9004, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] <ref> [21] </ref> [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex.
Reference: [22] <author> M. Hare, D. Corina, and G. Cottrell. </author> <title> Connectionist perspective on prosodic structure. </title> <type> Technical Report CRL Newsletter Volume 3 Number 2, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1989. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology <ref> [22] </ref> [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex.
Reference: [23] <author> Catherine L. Harris and Jeffrey L. Elman. </author> <title> Representing variable information with simple recurrent networks. </title> <booktitle> In 6th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 635-642, </pages> <year> 1984. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] <ref> [23] </ref> [32]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51].
Reference: [24] <author> M.H. Harrison. </author> <title> Introduction to Formal Language Theory. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Reading, MA, </address> <year> 1978. </year>
Reference-contexts: introduction we recommend Harrison <ref> [24] </ref> and Fu [15]. Briefly, a grammar G is a four tuple fN; T; P; Sg, where N and T are sets of terminals and nonterminals comprising the alphabet of the grammar, P is a set of production rules, and S is the start symbol.
Reference: [25] <author> S. Haykin. </author> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: We were unable to obtain significant convergence. 4. Weight initialisation. Random weights are initialised with the goal of ensuring that the sigmoids do not start out in saturation but are not very small (corresponding to a flat part of the error surface) <ref> [25] </ref>). In addition, several sets of random weights are tested and the set which provides the best performance on the training data is chosen 14 . <p> No Sectioning Sectioning NMSE Std. Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6: Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ([3] [27] [49] <ref> [25] </ref> [26]) and has a natural interpretation in terms of learning probabilities [35]. <p> All inputs were within the range zero to one. All target outputs were either 0.1 or 0.9. Bias inputs were used. The best of 20 random weight sets was chosen based on training set performance. Weights were initialised as shown in Haykin <ref> [25] </ref>. The logistic output activation function was used. The quadratic cost function was used. The search then converge learning rate schedule used was = 0 N=2 + c 1 (1c 2 )N where = learning rate, 0 = initial learning rate, N = total training 13 training.
Reference: [26] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6: Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ([3] [27] [49] [25] <ref> [26] </ref>) and has a natural interpretation in terms of learning probabilities [35].
Reference: [27] <author> J.J. </author> <title> Hopfield. Learning algorithms and probability distributions in feed-forward and feed-back networks. </title> <booktitle> Proceedings of the National Academy of Sciences, USA, </booktitle> <volume> 84 </volume> <pages> 8429-8433, </pages> <year> 1987. </year>
Reference-contexts: No Sectioning Sectioning NMSE Std. Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6: Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ([3] <ref> [27] </ref> [49] [25] [26]) and has a natural interpretation in terms of learning probabilities [35].
Reference: [28] <author> B. G. Horne and C. Lee Giles. </author> <title> An experimental comparison of recurrent neural networks. In Advances in Neural Information Processing Systems 7, </title> <note> page to appear, </note> <year> 1995. </year>
Reference-contexts: Additionally, analysis of the data suggests that 100% correct classification on the training data with only two word inputs would not be possible without learning significant aspects of the grammar. Another comparison of recurrent neural network architectures, that of Giles and Horne <ref> [28] </ref>, compared various networks on randomly generated 6 and 64-state finite memory machines.
Reference: [29] <author> E.B. Hunt, J.Marin, and P.T.Stone. </author> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1966. </year>
Reference-contexts: CLS <ref> [29] </ref> used a heuristic lookahead method to construct decision trees. ID3 [44] extended CLS by using information content in the heuristic function. We tested the C4.5 algorithm by Ross Quinlan [45], which is an industrial strength version of ID3 designed to handle noise.
Reference: [30] <author> L. Ingber. </author> <title> Very fast simulated re-annealing. </title> <journal> Mathl. Comput. Modelling, </journal> <volume> 12 </volume> <pages> 967-973, </pages> <year> 1989. </year>
Reference-contexts: The error during training for a sample of each network architecture is shown in figure 4. The errors shown in the graph are the 16 We have used the adaptive simulated annealing code by Lester Ingber <ref> [30] </ref> [31]. 17 The package used for simulated annealing by Lester Ingber includes speedups to the basic algorithm but even when we used these the algorithm did not converge. 11 TRAIN large small window window MLP 100 55 BT-FIR 100 56 Elman 100 100 W&Z 94 92 TEST large small window
Reference: [31] <author> L. Ingber. </author> <title> Adaptive simulated annealing (asa). </title> <type> Technical report, </type> <institution> Lester Ingber Research, </institution> <address> McLean, VA, ftp.caltech.edu: /pub/ingber/asa.Z, </address> <year> 1993. </year>
Reference-contexts: The error during training for a sample of each network architecture is shown in figure 4. The errors shown in the graph are the 16 We have used the adaptive simulated annealing code by Lester Ingber [30] <ref> [31] </ref>. 17 The package used for simulated annealing by Lester Ingber includes speedups to the basic algorithm but even when we used these the algorithm did not converge. 11 TRAIN large small window window MLP 100 55 BT-FIR 100 56 Elman 100 100 W&Z 94 92 TEST large small window window
Reference: [32] <author> M. F. St. John and J. L. McLelland. </author> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 5-46, </pages> <year> 1990. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] <ref> [32] </ref>. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. <p> several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] <ref> [32] </ref>. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex.
Reference: [33] <author> A. K. Joshi. </author> <title> Tree adjoining grammars: how much context-sensitivity is required to provide reasonable structural descriptions? In L. </title> <editor> Karttunen D. R. Dowty and A. M. Zwicky, editors, </editor> <booktitle> Natural Language Parsing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars. Several modern computational linguistic grammatical theories fall in this class <ref> [33] </ref> [43]. 1.3 Language and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately describe in
Reference: [34] <author> Joseph B. Kruskal. </author> <title> An overview of sequence comparison. </title> <editor> In David Sankoff and Joseph B. Kruskal, editors, </editor> <title> Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: The following equations are used iteratively to calculate the distances ending in the distance between the two complete sequences. i and j range from 0 to the length of the respective sequences and the superscripts denote sequences of the corresponding length. For more details see <ref> [34] </ref>. d (a i ; b j ) = min d (a i1 ; b j + w (a i ; 0) deletion of a i d (a i1 ; b j1 ) + w (a i ; b j ) b j replaces a i d (a i ; b
Reference: [35] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: Cost function. The relative entropy cost function has received particular attention ([3] [27] [49] [25] [26]) and has a natural interpretation in terms of learning probabilities <ref> [35] </ref>.
Reference: [36] <author> H. Lasnik and J. Uriagereka. </author> <title> A Course in GB Syntax: Lectures on Binding and Empty Categories. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: vs. innate components assumed by Chomsky, to produce the same judgements as native speakers on the sharply grammatical/ungrammatical pairs of the sort discussed in the next section. 2 Data Our primary data consists of 552 English positive and negative examples taken from an introductory GB-linguistics textbook by Lasnik and Uriagereka <ref> [36] </ref>. Most of these examples are organized into minimal pairs like the example I am eager for John to win/*I am eager John to win that we have seen above.
Reference: [37] <author> B. MacWhinney, J. Leinbach, R. Taraban, and J. McDonald. </author> <title> Language learning: cues or rules? Journal of Memory and Language, </title> <booktitle> 28 </booktitle> <pages> 255-277, </pages> <year> 1989. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] <ref> [37] </ref> and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex.
Reference: [38] <author> R. Miikkulainen and M. Dyer. </author> <title> Encoding input/output representations in connectionist cognitive systems. </title> <editor> In G. E. Hinton D. S. Touretzky and T. J. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 188-195, </pages> <address> Los Altos, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment <ref> [38] </ref> [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex.
Reference: [39] <author> K.S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1(1):4, </volume> <year> 1990. </year>
Reference-contexts: Narendra and Parthasarathy. A feed-forward network augmented with feedback connections from the output nodes to the hidden nodes. As described in <ref> [39] </ref>. 5. Elman. A simple recurrent network with feedback from each hidden node to all hidden nodes as described in [12], [13]. 6. Williams and Zipser. A fully recurrent network where all non-input nodes are connected to all other nodes as described in [55].
Reference: [40] <author> Fernando Pereira. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In ACL 92, </booktitle> <year> 1992. </year>
Reference-contexts: The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models. However, finite-state models cannot represent hierarchical structures as found in natural language 1 <ref> [40] </ref>. In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [7] [20] [17] [18] [19]. <p> The recurrent neural networks investigated in this paper 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. The algorithm is currently impractical for anything except relatively small grammars <ref> [40] </ref>. 2 constitute complex, dynamical systems. Pollack [42] points out that Crutchfield and Young [8] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars.
Reference: [41] <author> D. M. Pesetsky. </author> <title> Paths and Categories. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1982. </year>
Reference: [42] <author> J.B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252, </pages> <year> 1991. </year>
Reference-contexts: Certain phenomena (eg. center embedding) are more compactly described by context-free grammars which are recognised by push-down automata, while others (eg. crossed-serial dependencies and agreement) are better described by context-sensitive grammars which are recognised by linear bounded automata <ref> [42] </ref>. 1.2 Representational Power Natural language has traditionally been handled using symbolic computation and recursive processes. The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models. However, finite-state models cannot represent hierarchical structures as found in natural language 1 [40]. <p> The recurrent neural networks investigated in this paper 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. The algorithm is currently impractical for anything except relatively small grammars [40]. 2 constitute complex, dynamical systems. Pollack <ref> [42] </ref> points out that Crutchfield and Young [8] have studied the computational complexity of dynamical systems reaching the onset of chaos via period-doubling. They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars.
Reference: [43] <author> C. Pollard. </author> <title> Generalised context-free grammars, head grammars and natural language. </title> <type> PhD thesis, </type> <institution> Department of Linguistics, Stanford University, </institution> <address> Palo Alto, CA, </address> <year> 1984. </year>
Reference-contexts: They have shown that these systems are not regular, but are finitely described by Indexed Context-Free-Grammars. Several modern computational linguistic grammatical theories fall in this class [33] <ref> [43] </ref>. 1.3 Language and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately describe in a
Reference: [44] <author> J.R. Quinlan. </author> <title> Discovering rules from large collections of examples: a case study. </title> <editor> In D. Michie, editor, </editor> <booktitle> Expert Systems in the Microelectronic Age. </booktitle> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1979. </year>
Reference-contexts: CLS [29] used a heuristic lookahead method to construct decision trees. ID3 <ref> [44] </ref> extended CLS by using information content in the heuristic function. We tested the C4.5 algorithm by Ross Quinlan [45], which is an industrial strength version of ID3 designed to handle noise.
Reference: [45] <author> Ross Quinlan. </author> <title> C4.5 Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: CLS [29] used a heuristic lookahead method to construct decision trees. ID3 [44] extended CLS by using information content in the heuristic function. We tested the C4.5 algorithm by Ross Quinlan <ref> [45] </ref>, which is an industrial strength version of ID3 designed to handle noise. C4.5 only deals with strings of constant length and we used an input space corresponding to the longest string we do not expect C4.5 to be highly suitable to the problem. The default C4.5 parameters were used.
Reference: [46] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: Theoretically, the Williams & Zipser network is the most powerful in terms of representational ability, yet the Elman network provides better performance. Investigation shows that this is due to the more complex error surface of the Williams & Zipser architecture. This result is supported by the parsimony principle <ref> [46] </ref>. Results indicate that a global minimum is never found for the task and algorithms described here, however, we note that the local minima which are found consistently possess performance which is similar within each architecture.
Reference: [47] <editor> H.T. Siegelmann and E.D. Sontag. </editor> <booktitle> On the computational power of neural nets. In Proceedings of the Fifth ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 440-449, </pages> <address> New York, N.Y., 1992. </address> <publisher> ACM. </publisher>
Reference-contexts: Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [13], and that they are Turing equivalent <ref> [47] </ref>. The recurrent neural networks investigated in this paper 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. The algorithm is currently impractical for anything except relatively small grammars [40]. 2 constitute complex, dynamical systems.
Reference: [48] <author> P.Y. Simard, M.B. Ottaway, and D.H. Ballard. </author> <title> Analysis of recurrent backpropagation. </title> <editor> In D. Touretzky, G. Hin-ton, and T. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 103-112, </pages> <address> San Mateo, 1989. (Pittsburg 1988), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Elman 0.470 0.078 0.651 0.0046 W & Z 0.657 0.019 0.760 0.023 Table 7: Training set NMSE comparison for logistic and tanh sigmoid activation functions. 10 8 Simulated Annealing Previous work has shown the use of simulated annealing for finding the parameters of a recurrent network model to improve performance <ref> [48] </ref>. For comparison with the gradient descent based algorithms we have investigated using simulated annealing to train exactly the same Elman network as has been successfully trained to 100% correct training set classification using backpropagation through time (details in a later section). <p> In comparison, the successful Elman models obtain an NMSE of approximately 0.1. We have not found the use of simulated annealing to improve performance, as Simard et. al. <ref> [48] </ref> have. Their problem was the parity problem with only four hidden units. 9 Results Our results are based on multiple training/test set partitions and multiple random seeds. We have also used a set of Japanese control data.
Reference: [49] <author> S.A. Solla, E. Levin, and M. Fleisher. </author> <title> Accelerated learning in layered neural networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 625-639, </pages> <year> 1988. </year>
Reference-contexts: No Sectioning Sectioning NMSE Std. Dev. NMSE Std. Dev. Elman 0.367 0.011 0.573 0.051 W & Z 0.594 0.084 0.617 0.026 Table 6: Training set NMSE comparison for the use of training data sectioning. 8. Cost function. The relative entropy cost function has received particular attention ([3] [27] <ref> [49] </ref> [25] [26]) and has a natural interpretation in terms of learning probabilities [35].
Reference: [50] <author> Andreas Stolcke. </author> <title> Learning feature-based semantics with simple recurrent networks. </title> <type> Technical Report TR-90-015, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference [7] [20] [17] [18] [19]. Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: <ref> [50] </ref> [1] [11] [23] [32]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32].
Reference: [51] <author> M. Tomita. </author> <title> Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <pages> pages 105-108, </pages> <address> Ann Arbor, Mi, </address> <year> 1982. </year> <month> 20 </month>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages <ref> [51] </ref>. Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [13], and that they are Turing equivalent [47].
Reference: [52] <author> D. S. Touretzky. </author> <title> Rules and maps in connectionist symbol processing. </title> <type> Technical Report Technical Report CMU--CS-89-158, </type> <institution> Carnegie Mellon University: Department of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1989. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] <ref> [52] </ref>, morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex.
Reference: [53] <author> D. S. Touretzky. </author> <title> Towards a connectioninst phonology: The 'many maps' approach to sequence manipulation. </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 188-195, </pages> <year> 1989. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, eg. papers using the Elman network for natural language tasks include: [50] [1] [11] [23] [32]. Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] <ref> [53] </ref> [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. [54] [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex.
Reference: [54] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite-state languages using second-order recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(3):406, </address> <year> 1992. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [16] [21] [53] [52], morphology [22] [37] and role assignment [38] [32]. Induction of simpler grammars has been addressed often - eg. <ref> [54] </ref> [18] on learning Tomita languages [51]. Our task differs from these in that the grammar is considerably more complex. It has been shown that recurrent networks have the representational power required for hierarchical solutions [13], and that they are Turing equivalent [47].
Reference: [55] <author> R.J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 270-280, </pages> <year> 1989. </year>
Reference-contexts: As described in [39]. 5. Elman. A simple recurrent network with feedback from each hidden node to all hidden nodes as described in [12], [13]. 6. Williams and Zipser. A fully recurrent network where all non-input nodes are connected to all other nodes as described in <ref> [55] </ref>. <p> Normalised mean squared error results are defined as 9 Backpropagation through time extends backpropagation to include temporal aspects and arbitrary connection topologies by considering an equivalent feedforward network created by unfolding the recurrent network in time. 10 Real-time <ref> [55] </ref> recurrent learning was also tested but did not show any significant convergence for our problem. 11 Without modifying the standard gradient descent algorithms we were only able to train networks which operated on a large temporal input window.
Reference: [56] <author> R.J. Williams and D. Zipser. </author> <title> Gradient-based learning algorithms for recurrent connectionist networks. </title> <editor> In Y. Chauvin and D.E. Rumelhart, editors, </editor> <title> Backpropagation: Theory, Architectures, and Applications. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1990. </year> <month> 21 </month>
Reference-contexts: We expect the feedforward and locally recurrent architectures to encounter difficulty performing the task and include them primarily as control cases. 7 Gradient Descent Learning We have used backpropagation through time 9 <ref> [56] </ref> to train the globally recurrent networks 10 , standard backpropagation for the multi-layer perceptron, and the gradient descent algorithms described by the authors for the locally recurrent networks. The error surface of a multilayer network is non-convex, non-quadratic, and often has large dimensionality.
References-found: 56

