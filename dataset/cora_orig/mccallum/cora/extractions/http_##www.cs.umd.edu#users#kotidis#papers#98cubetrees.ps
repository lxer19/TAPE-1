URL: http://www.cs.umd.edu/users/kotidis/papers/98cubetrees.ps
Refering-URL: http://www.cs.umd.edu/users/kotidis/publications.html
Root-URL: 
Email: kotidis@cs.umd.edu  nick@cs.umd.edu  
Title: An Alternative Storage Organization for ROLAP Aggregate Views Based on Cubetrees  
Author: Yannis Kotidis Nick Roussopoulos 
Affiliation: Department of Computer Science University of Maryland  Department of Computer Science Institute of Advanced Computer Studies University of Maryland  
Abstract: The Relational On-Line Analytical Processing (ROLAP) is emerging as the dominant approach in data warehousing with decision support applications. In order to enhance query performance, the RO-LAP approach relies on selecting and materializing in summary tables appropriate subsets of aggregate views which are then engaged in speeding up OLAP queries. However, a straight forward relational storage implementation of materialized ROLAP views is immensely wasteful on storage and incredibly inadequate on query performance and incremental update speed. In this paper we propose the use of Cubetrees, a collection of packed and compressed R-trees, as an alternative storage and index organization for ROLAP views and provide an efficient algorithm for mapping an arbitrary set of OLAP views to a collection of Cubetrees that achieve excellent performance. Compared to a conventional (relational) storage organization of materialized OLAP views, Cubetrees offer at least a 2-1 storage reduction, a 10-1 better OLAP query performance, and a 100-1 faster updates. We compare the two alternative approaches with data generated from the TPC-D benchmark and stored in the Informix Universal Server (IUS). The straight forward implementation materializes the ROLAP views using IUS tables and conventional B-tree indexing. The Cubetree implementation materializes the same ROLAP views using a Cubetree Datablade developed for IUS. The experiments demonstrate that the Cubetree storage organization is superior in storage, query performance and update speed. 
Abstract-found: 1
Intro-found: 1
Reference: [AAD + 96] <author> S. Agrawal, R. Agrawal, P. Deshpande, A. Gupta, J. Naughton, R. Ramakrishnan, and S. Sarawagi. </author> <title> On the Computation of Multidimensional Aggregates. </title> <booktitle> In Proc. of VLDB, </booktitle> <pages> pages 506-521, </pages> <address> Bombay, India, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: On the other hand, view V fpartkey;suppkey;custkeyg can be derived only from the fact table. The materialization of set V through typical relational tables was done by computing each view from the smallest parent <ref> [AAD + 96] </ref>, as shown in Figure 10. For speeding up the computation we issued transactions that requested exclusive locks on the tables, since con-currency is not an issue when loading the warehouse. <p> However this step can be hardly considered as an overhead, since sorting is at the same time used for computing the views in V. In our implementation we used a variation of the sort-based algorithms that are discussed in <ref> [AAD + 96] </ref> for computing the lattice. The general idea of the algorithm is to minimize the processing requirements by computing an element of the cube-lattice from one of its parents as we already saw in Figure 10. <p> The mere size of them does not permit frequent re-computation. Having a set of views materialized in the warehouse adds an extra overhead to the update phase. Many optimization techniques <ref> [AAD + 96, HRU96, ZDN97] </ref> deal only with the initial computation of the aggregate data, while others [GMS93, GL95, JMS95, MQM97] focus on incrementally updating the materialized views.
Reference: [ACT97] <institution> ACT Inc. The Cubetree Datablade. </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: The straight forward implementation materializes the ROLAP views using IUS tables which are then indexed with B-trees. The Cubetree implementation materializes the same ROLAP views using a Cubetree Datablade <ref> [ACT97] </ref> developed for IUS. The experiments demonstrate that the Cubetree storage organization is superior in storage, query performance and update speed. Section 2 defines the Cubetree storage organization for materializing ROLAP aggregate views and the mapping of SQL queries to the underlying Datablade supporting the Cubetrees. <p> This explains the reason why the combined indexing and materializing storage organization of the Cube trees is more economical by a factor of more than 2-1. 3 Experiments In order to validate our performance expectations, we used a Cu-betree Datablade <ref> [ACT97] </ref> that implements the Cubetrees, the Se-lectMapping algorithm and the supporting routines on the Informix Universal Server. This Datablade defines a Cubetree access method as an alternative primary storage organization for materialized views and provides the end-user with a clean and transparent SQL interface.
Reference: [BPT97] <author> E. Baralis, S. Paraboschi, and E. Teniente. </author> <title> Materialized View Selection in a Multidimensional Database. </title> <booktitle> In Proc. of the 23th International Conference on VLDB, </booktitle> <pages> pages 156-165, </pages> <address> Athens, Greece, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: In this case, the ROLAP approach relies on selecting and materializing in summary tables the right subsets of aggregate views along with their secondary indexing that improves overall aggregate query processing <ref> [Rou82, BPT97, GHRU97, Gup97] </ref>. Like the MOLAP case, controlled redundancy is introduced to improve performance. A discriminating and fundamental difference remains however. <p> The computation and materialization of all possible aggregate views, over all interesting attributes, with respect to the given hierarchies, is often unrealistic both because of the mere size of the data and of the incredibly high update cost when new data is shipped to the warehouse. Several techniques <ref> [Rou82, BPT97, GHRU97, Gup97] </ref> have been proposed to select appropriate subsets of aggregate views of the Data Cube to materialize through summary tables. 1 Because these views are typically very large, indexing, which adds to their redundancy, is necessary to speed up queries on then.
Reference: [FR89] <author> C. Faloutsos and S. Roseman. </author> <title> Fractals for Secondary Key Retrieval. </title> <booktitle> Eighth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS), </booktitle> <pages> pages 247-252, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Thus, there is no interleaving between the points of different views. This is true because of the sorting and is one of the reasons for considering only sorts based on lowY, lowX and not space filling curves <ref> [FR89] </ref> when packing the trees. Clearly the same sort order is used for computing the views at creation time and during updates, as will be shown in the experiments section. One can prove that the SelectMapping algorithm picks a minimal set R of Cubetrees with such organization to store V.
Reference: [GBLP96] <author> J. Gray, A. Bosworth, A. Layman, and H. Piramish. </author> <title> Data Cube: A Relational Aggregation Operator Generalizing Group-By, </title> <booktitle> Cross-Tab, and Sub-Totals. In Proc. of the 12th Int. Conference on Data Engineering, </booktitle> <pages> pages 152-159, </pages> <address> New Orleans, </address> <month> February </month> <year> 1996. </year> <note> IEEE. </note>
Reference-contexts: Each dimension table contains information specific to the dimension itself. The fact table correlates all dimensions through a set of foreign keys. A typical OLAP query might involve aggregation among different dimensions. The Data Cube <ref> [GBLP96] </ref> represents the computation of interesting aggregate functions over all combinations of dimension tables. Thus, the size of the cube itself is exponential in the number of dimensions in the warehouse.
Reference: [GHRU97] <author> H. Gupta, V. Harinarayan, A. Rajaraman, and J. Ull-man. </author> <title> Index Selection for OLAP. </title> <booktitle> In Proceedings of the Intl. Conf. on Data Engineering, </booktitle> <pages> pages 208-219, </pages> <address> Bur-mingham, UK, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: In this case, the ROLAP approach relies on selecting and materializing in summary tables the right subsets of aggregate views along with their secondary indexing that improves overall aggregate query processing <ref> [Rou82, BPT97, GHRU97, Gup97] </ref>. Like the MOLAP case, controlled redundancy is introduced to improve performance. A discriminating and fundamental difference remains however. <p> The computation and materialization of all possible aggregate views, over all interesting attributes, with respect to the given hierarchies, is often unrealistic both because of the mere size of the data and of the incredibly high update cost when new data is shipped to the warehouse. Several techniques <ref> [Rou82, BPT97, GHRU97, Gup97] </ref> have been proposed to select appropriate subsets of aggregate views of the Data Cube to materialize through summary tables. 1 Because these views are typically very large, indexing, which adds to their redundancy, is necessary to speed up queries on then. <p> The trade-off in this selection is between speed up of the queries and time to reorganize these views. Several techniques have been proposed to deal with this problem. For the purposes of our work we have used the 1-greedy algorithm presented in <ref> [GHRU97] </ref>, for the view selection. This algorithm computes the cost of answering a query q, as the total number of tuples that have to be accessed on every table and index that is used to answer q. <p> and update cost, of the two storage alternatives. 5 The notation I a;b;c refers to an index on view V fa;b;cg that uses as the search key the concatenation of a,b and c attributes. 3.1 Queries Description The query model that is used by the TPC-D benchmark, involves slice queries <ref> [GHRU97] </ref> on the lattice hyper-space. This type of queries consist of a list of simple selection predicates between a dimension attribute and a constant value, while aggregating the measure quantity among another disjoint set of group-by attributes. For our experiments, we only considered selection predicates that use the equal operator. <p> This dataset was then used for loading the set of views with the appropriate tuples. The total number of rows in the generated fact table of Figure 1 was 6,001,215. We used the lattice framework to define a derives-from relation <ref> [MQM97, GHRU97] </ref> between the views shown in Figure 9. For example view V fpartkeyg can be derived from view V fpartkey;suppkeyg and also from view V fpartkey;suppkey;custkeyg . On the other hand, view V fpartkey;suppkey;custkeyg can be derived only from the fact table.
Reference: [GL95] <author> T. Griffin and L. Libkin. </author> <title> Incremental Maintenance of Views with Duplicates. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 328-339, </pages> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: The mere size of them does not permit frequent re-computation. Having a set of views materialized in the warehouse adds an extra overhead to the update phase. Many optimization techniques [AAD + 96, HRU96, ZDN97] deal only with the initial computation of the aggregate data, while others <ref> [GMS93, GL95, JMS95, MQM97] </ref> focus on incrementally updating the materialized views. However traditional database systems, will most probably expose their limitations when dealing with updates rates of several MBs or GBs per time unit (hour, date etc) of incoming data, in the context of a data warehouse.
Reference: [GMS93] <author> A. Gupta, </author> <title> I.S. Mumick, and V.S. Subrahmanian. Maintaining Views Incrementally. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 157-166, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The mere size of them does not permit frequent re-computation. Having a set of views materialized in the warehouse adds an extra overhead to the update phase. Many optimization techniques [AAD + 96, HRU96, ZDN97] deal only with the initial computation of the aggregate data, while others <ref> [GMS93, GL95, JMS95, MQM97] </ref> focus on incrementally updating the materialized views. However traditional database systems, will most probably expose their limitations when dealing with updates rates of several MBs or GBs per time unit (hour, date etc) of incoming data, in the context of a data warehouse.
Reference: [Gup97] <author> H. Gupta. </author> <title> Selections of Views to Materialize in a Data Warehouse. </title> <booktitle> In Proceedings of ICDT, </booktitle> <pages> pages 98-112, Delphi, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: In this case, the ROLAP approach relies on selecting and materializing in summary tables the right subsets of aggregate views along with their secondary indexing that improves overall aggregate query processing <ref> [Rou82, BPT97, GHRU97, Gup97] </ref>. Like the MOLAP case, controlled redundancy is introduced to improve performance. A discriminating and fundamental difference remains however. <p> The computation and materialization of all possible aggregate views, over all interesting attributes, with respect to the given hierarchies, is often unrealistic both because of the mere size of the data and of the incredibly high update cost when new data is shipped to the warehouse. Several techniques <ref> [Rou82, BPT97, GHRU97, Gup97] </ref> have been proposed to select appropriate subsets of aggregate views of the Data Cube to materialize through summary tables. 1 Because these views are typically very large, indexing, which adds to their redundancy, is necessary to speed up queries on then.
Reference: [Gut84] <author> A. Guttman. R-Trees: </author> <title> A Dynamic Index Structure for Spatial Searching. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 47-57, </pages> <address> Boston, MA, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: Another major drawback is that in the relational storage data is typically stored unsorted which prohibits efficient merge operations during the updates. In [RKR97] we introduced Cubetrees, a collection of packed R-trees <ref> [Gut84, RL85] </ref>, as a multidimensional indexing scheme for the Data Cube. Cubetrees best features include their efficiency during incremental bulk update and their high query throughput.
Reference: [HRU96] <author> V. Harinarayan, A. Rajaraman, and J. Ullman. </author> <title> Implementing Data Cubes Efficiently. </title> <booktitle> In Proc. of ACM SIG-MOD, </booktitle> <pages> pages 205-216, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: The measure attribute in every case is the quantity of the parts that is involved in each transaction. Figure 1 shows a simplified model of the warehouse and Figure 9 the Data Cube operator as a lattice <ref> [HRU96] </ref> in the fpartkey, suppkey, custkeyg dimensions. In SQL terms, each node in the lattice represents a view that aggregates data by the denoted attributes. <p> The mere size of them does not permit frequent re-computation. Having a set of views materialized in the warehouse adds an extra overhead to the update phase. Many optimization techniques <ref> [AAD + 96, HRU96, ZDN97] </ref> deal only with the initial computation of the aggregate data, while others [GMS93, GL95, JMS95, MQM97] focus on incrementally updating the materialized views.
Reference: [JMS95] <author> H. Jagadish, I. Mumick, and A. Silberschatz. </author> <title> View Maintenance Issues in the Chronicle Data Model. </title> <booktitle> In Proceedings of PODS, </booktitle> <pages> pages 113-124, </pages> <address> San Jose, CA, </address> <year> 1995. </year>
Reference-contexts: The mere size of them does not permit frequent re-computation. Having a set of views materialized in the warehouse adds an extra overhead to the update phase. Many optimization techniques [AAD + 96, HRU96, ZDN97] deal only with the initial computation of the aggregate data, while others <ref> [GMS93, GL95, JMS95, MQM97] </ref> focus on incrementally updating the materialized views. However traditional database systems, will most probably expose their limitations when dealing with updates rates of several MBs or GBs per time unit (hour, date etc) of incoming data, in the context of a data warehouse.
Reference: [Kim96] <author> R. Kimball. </author> <title> The Data Warehouse Toolkit. </title> <publisher> John Wiley & Sons, </publisher> <year> 1996. </year>
Reference-contexts: The Relational OLAP approach starts off with the premise that OLAP queries can generate the multidimensional projections on the fly without having to store and maintain them in foreign storage. This approach is exemplified by a star schema <ref> [Kim96] </ref> linking the dimensions with a fact table storing the data. Join and bit-map indices [Val87, OQ97, OG95] are used for speeding up the joins between the dimension and the fact tables.
Reference: [KR97] <author> Y. Kotidis and N. Roussopoulos. </author> <title> A Generalized Framework for Indexing OLAP Aggregates. </title> <type> Technical Report CS-TR-3841, </type> <institution> University of Maryland, </institution> <month> Oct </month> <year> 1997. </year>
Reference-contexts: Cubetrees best features include their efficiency during incremental bulk update and their high query throughput. The bulk incremental update relies on their internal organization which is maintained sorted at all times and permits both an efficient merge pack algorithm and sequential writes on the disk. An expanded ex-periment <ref> [KR97] </ref> showed that Cubetrees can easily achieve a packing rate of 6GB/hour on an 2100A/275MHz Alphaserver with a single CPU and a single disk, a fairly low-end hardware platform compared with todays warehousing standards. <p> In general, one may choose to map each view to a different Cubetree or, in the other extreme, to put as much information as possible to each Cubetree. In <ref> [KR97] </ref> we present comparisons using views from a Data Warehouse with 10 dimension tables. These experiments indicate that the Se-lectMapping algorithm achieves the best compromise with respect to the following criteria: * Degree of clustering inside each Cubetree * Storage requirements * Efficiency of bulk-loading the Cubetrees * Query performance. <p> This is because R-trees in general behave faster in bounded range queries <ref> [Sar97, KR97] </ref>. Thus, in a more general experiment where arbitrary range queries are allowed we expect that the Cubetrees would be even faster. For any view V , there are 2 jV j different types of slice queries.
Reference: [MQM97] <author> I. S. Mumick, D. Quass, and B. S. Mumick. </author> <title> Maintenance of Data Cubes and Summary Tables in a Warehouse. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 100-111, </pages> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: This dataset was then used for loading the set of views with the appropriate tuples. The total number of rows in the generated fact table of Figure 1 was 6,001,215. We used the lattice framework to define a derives-from relation <ref> [MQM97, GHRU97] </ref> between the views shown in Figure 9. For example view V fpartkeyg can be derived from view V fpartkey;suppkeyg and also from view V fpartkey;suppkey;custkeyg . On the other hand, view V fpartkey;suppkey;custkeyg can be derived only from the fact table. <p> The mere size of them does not permit frequent re-computation. Having a set of views materialized in the warehouse adds an extra overhead to the update phase. Many optimization techniques [AAD + 96, HRU96, ZDN97] deal only with the initial computation of the aggregate data, while others <ref> [GMS93, GL95, JMS95, MQM97] </ref> focus on incrementally updating the materialized views. However traditional database systems, will most probably expose their limitations when dealing with updates rates of several MBs or GBs per time unit (hour, date etc) of incoming data, in the context of a data warehouse.
Reference: [OG95] <author> P. O'Neil and G. Graefe. </author> <title> Multi-Table Joins Through Bitmapped Join Indices. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 24(3) </volume> <pages> 8-11, </pages> <month> Sept </month> <year> 1995. </year>
Reference-contexts: This approach is exemplified by a star schema [Kim96] linking the dimensions with a fact table storing the data. Join and bit-map indices <ref> [Val87, OQ97, OG95] </ref> are used for speeding up the joins between the dimension and the fact tables. Since data is generated on the fly, the maintenance cost of the MOLAP structures is avoided at the cost of index maintenance. <p> Given the hierarchy part-type ! part, if view V 2 were not materialized, queries would normally require a join between the fact table F and the part table. Special purpose indices <ref> [Val87, OQ97, OG95] </ref> can be used to compute these joins faster. However, such indices add to the redundancy of the warehouse and, in most cases, a materialized view, accompanied with a conventional B-tree index will perform better.
Reference: [OQ97] <author> P. O'Neil and D. Quass. </author> <title> Improved Query Performance with Variant Indexes. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 38-49, </pages> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: This approach is exemplified by a star schema [Kim96] linking the dimensions with a fact table storing the data. Join and bit-map indices <ref> [Val87, OQ97, OG95] </ref> are used for speeding up the joins between the dimension and the fact tables. Since data is generated on the fly, the maintenance cost of the MOLAP structures is avoided at the cost of index maintenance. <p> Given the hierarchy part-type ! part, if view V 2 were not materialized, queries would normally require a join between the fact table F and the part table. Special purpose indices <ref> [Val87, OQ97, OG95] </ref> can be used to compute these joins faster. However, such indices add to the redundancy of the warehouse and, in most cases, a materialized view, accompanied with a conventional B-tree index will perform better.
Reference: [RKR97] <author> N. Roussopoulos, Y. Kotidis, and M. Roussopoulos. Cubetree: </author> <title> Organization of and Bulk Incremental Updates on the Data Cube. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 89-99, </pages> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: This causes the keys to be replicated several times in addition to the values stored in the relational storage too. Another major drawback is that in the relational storage data is typically stored unsorted which prohibits efficient merge operations during the updates. In <ref> [RKR97] </ref> we introduced Cubetrees, a collection of packed R-trees [Gut84, RL85], as a multidimensional indexing scheme for the Data Cube. Cubetrees best features include their efficiency during incremental bulk update and their high query throughput.
Reference: [RL85] <author> N. Roussopoulos and D. Leifker. </author> <title> Direct Spatial Search on Pictorial Databases Using Packed R-trees. </title> <booktitle> In Procs. of 1985 ACM SIGMOD Intl. Conf. on Management of Data, </booktitle> <address> Austin, </address> <year> 1985. </year>
Reference-contexts: Another major drawback is that in the relational storage data is typically stored unsorted which prohibits efficient merge operations during the updates. In [RKR97] we introduced Cubetrees, a collection of packed R-trees <ref> [Gut84, RL85] </ref>, as a multidimensional indexing scheme for the Data Cube. Cubetrees best features include their efficiency during incremental bulk update and their high query throughput.
Reference: [Rou82] <author> N. Roussopoulos. </author> <title> View Indexing in Relational Databases. </title> <journal> ACM TODS, </journal> <volume> 7(2), </volume> <month> June </month> <year> 1982. </year>
Reference-contexts: In this case, the ROLAP approach relies on selecting and materializing in summary tables the right subsets of aggregate views along with their secondary indexing that improves overall aggregate query processing <ref> [Rou82, BPT97, GHRU97, Gup97] </ref>. Like the MOLAP case, controlled redundancy is introduced to improve performance. A discriminating and fundamental difference remains however. <p> The computation and materialization of all possible aggregate views, over all interesting attributes, with respect to the given hierarchies, is often unrealistic both because of the mere size of the data and of the incredibly high update cost when new data is shipped to the warehouse. Several techniques <ref> [Rou82, BPT97, GHRU97, Gup97] </ref> have been proposed to select appropriate subsets of aggregate views of the Data Cube to materialize through summary tables. 1 Because these views are typically very large, indexing, which adds to their redundancy, is necessary to speed up queries on then.
Reference: [Sar97] <author> S. Sarawagi. </author> <title> Indexing OLAP Data. </title> <journal> IEEE Bulletin on Data Engineering, </journal> <volume> 20(1) </volume> <pages> 36-43, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: This is because R-trees in general behave faster in bounded range queries <ref> [Sar97, KR97] </ref>. Thus, in a more general experiment where arbitrary range queries are allowed we expect that the Cubetrees would be even faster. For any view V , there are 2 jV j different types of slice queries.
Reference: [Val87] <author> P. Valduriez. </author> <title> Join indices. </title> <journal> ACM TODS, </journal> <volume> 12(2) </volume> <pages> 218-246, </pages> <year> 1987. </year>
Reference-contexts: This approach is exemplified by a star schema [Kim96] linking the dimensions with a fact table storing the data. Join and bit-map indices <ref> [Val87, OQ97, OG95] </ref> are used for speeding up the joins between the dimension and the fact tables. Since data is generated on the fly, the maintenance cost of the MOLAP structures is avoided at the cost of index maintenance. <p> Given the hierarchy part-type ! part, if view V 2 were not materialized, queries would normally require a join between the fact table F and the part table. Special purpose indices <ref> [Val87, OQ97, OG95] </ref> can be used to compute these joins faster. However, such indices add to the redundancy of the warehouse and, in most cases, a materialized view, accompanied with a conventional B-tree index will perform better.
Reference: [ZDN97] <author> Y. Zhao, P. M. Deshpande, and J. F. Naughton. </author> <title> An Array-Based Algorithm for Simultaneous Multidimensional Aggregates. </title> <booktitle> In Proceedings of the ACM SIG-MOD International Conference on Management of Data, </booktitle> <pages> pages 159-170, </pages> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: The mere size of them does not permit frequent re-computation. Having a set of views materialized in the warehouse adds an extra overhead to the update phase. Many optimization techniques <ref> [AAD + 96, HRU96, ZDN97] </ref> deal only with the initial computation of the aggregate data, while others [GMS93, GL95, JMS95, MQM97] focus on incrementally updating the materialized views.
References-found: 23

