URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/ecml97po.ps
Refering-URL: http://ilk.kub.nl/~ilk/papers/abstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: 2  
Title: A Feature-Relevance Heuristic for Indexing and Compressing Large Case Bases  
Author: Walter Daelemans Antal van den Bosch and Jakub Zavrel 
Address: Netherlands  Netherlands  
Affiliation: 1 Computational Linguistics, Tilburg University, The  Dept. of Computer Science, Universiteit Maastricht, The  
Abstract: This paper reports results with igtree, a formalism for indexing and compressing large case bases in Instance-Based Learning (ibl) and other lazy-learning techniques. The concept of information gain (entropy minimisation) is used as a heuristic feature-relevance function for performing the compression of the case base into a tree. igtree reduces storage requirements and the time required to compute classifications considerably for problems where current ibl approaches fail for complexity reasons. Moreover, generalisation accuracy is often similar, for the tasks studied, to that obtained with information-gain-weighted variants of lazy learning, and alternative approaches such as c4.5. Although igtree was designed for a specific class of problems -linguistic disambiguation problems with symbolic (nominal) features, huge case bases, and a complex interaction between (sub)regularities and exceptions- we show in this paper that the approach has a wider applicability when generalising it to tribl, a hybrid combination of igtree and ibl.
Abstract-found: 1
Intro-found: 1
Reference: [Aha91] <author> Aha, D. W., Kibler, D., & Albert, M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 37-66. </pages>
Reference-contexts: Because of their computational complexity, standard inductive machine-learning algorithms can often not be applied to datasets of more than a fraction of this size. In lazy learning (e.g., the ib1 instance-based learning algorithm described in <ref> [Aha91] </ref>), similarity of a new case to stored cases is used to find the nearest neighbours of the new case. The class of the new case is predicted on the basis of the classes associated with the nearest-neighbour cases and the frequency of their occurrences. <p> During experimentation with the linguistic problems, we also found that accuracy (generalisation performance) decreased considerably when the case base is pruned in some way (e.g., using ib2 <ref> [Aha91] </ref>, or by eliminating non-typical cases). Keeping available all potentially relevant case information turns out to be essential for good accuracy on our linguistic problems, because they often exhibit a lot of sub-regularity and pockets of exceptions that have potential relevance in generalisation.
Reference: [Car93] <author> Cardie, C. </author> <year> (1996). </year> <title> Embedded Machine Learning Systems for Natural Language Processing: A General Framework. </title> <editor> In: Wermter, S., Riloff, E., and Scheler, G. (Eds.), </editor> <title> Symbolic, connectionist, and statistical approaches to learning for natural language processing, </title> <publisher> Springer LNAI Series, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction The formalism presented here originated in research on the application of lazy-learning techniques (such as case-based learning, instance-based learning, and memory-based reasoning) to real-world problems in language technology, cf. [Dae95] and <ref> [Car93] </ref> for overviews of the approach and results. The main difference between learning linguistic problems and learning the typical "benchmark" problems employed in machine-learning research, is that for the linguistic problems huge datasets are available. <p> Removing the exceptions decreases accuracy significantly. A feature-relevance ordering technique is available which assigns sufficiently differing relevance to individual features to allow a fixed ordering. These properties apply to a large class of real-world problems, including almost all disambiguation tasks in language technology <ref> [Dae95, Car93] </ref>. For this type of task, igtree attains a generalisation accuracy similar to alternative lazy-learning techniques and other inductive machine-learning techniques, with modest memory space and processing time requirements. Retrieval is especially fast because its complexity is independent from the number of cases.
Reference: [Dae95] <author> Daelemans, W. </author> <year> (1995). </year> <title> Memory-based lexical acquisition and processing. </title> <editor> In Steffens, P. (Ed.), </editor> <booktitle> Machine Translation and the Lexicon, Lecture Notes in Artificial Intelligence 898. </booktitle> <address> Berlin: </address> <publisher> Springer. </publisher>
Reference-contexts: 1 Introduction The formalism presented here originated in research on the application of lazy-learning techniques (such as case-based learning, instance-based learning, and memory-based reasoning) to real-world problems in language technology, cf. <ref> [Dae95] </ref> and [Car93] for overviews of the approach and results. The main difference between learning linguistic problems and learning the typical "benchmark" problems employed in machine-learning research, is that for the linguistic problems huge datasets are available. <p> Removing the exceptions decreases accuracy significantly. A feature-relevance ordering technique is available which assigns sufficiently differing relevance to individual features to allow a fixed ordering. These properties apply to a large class of real-world problems, including almost all disambiguation tasks in language technology <ref> [Dae95, Car93] </ref>. For this type of task, igtree attains a generalisation accuracy similar to alternative lazy-learning techniques and other inductive machine-learning techniques, with modest memory space and processing time requirements. Retrieval is especially fast because its complexity is independent from the number of cases.
Reference: [DV92] <author> Daelemans, W. and Van den Bosch, A. </author> <year> (1992). </year> <title> Generalisation performance of backpropagation learning on a syllabification task. </title> <editor> In M. Drossaers and A. Nijholt (Eds.), TWLT3: </editor> <booktitle> Connectionism and Natural Language Processing. </booktitle> <institution> Enschede: Twente University. </institution>
Reference-contexts: We noticed that ib1, when extended with a simple feature-weighting similarity function, outperforms ib1, and sometimes also outperforms both connectionist approaches and knowledge-based "linguistic-engineering" approaches [VD93]. The similarity function we introduced in lazy learning <ref> [DV92] </ref> consisted simply of multiplying, when comparing two feature vectors, the similarity between the values for each feature with the corresponding information gain, or in case of features with different numbers of values the gain ratio, for that feature.We call this version of lazy learning ib1-ig.
Reference: [DVW97] <author> Daelemans, W., Van den Bosch, A., and Weijters, A. </author> <year> (1997). </year> <title> IGTree: Using trees for classification in lazy learning algorithms. </title> <note> To appear in Artificial Intelligence Review, special issue on lazy learning. </note>
Reference-contexts: Based on these findings with ib1-ig, we designed a variant of ib1 in which the case base is compressed into a tree-based data structure in such a way that access to relevant cases is faster, and no relevant information about the cases is lost. This simple algorithm, igtree <ref> [VD93, DVW97] </ref>, uses a feature-relevance metric such as information gain to restructure the case base into a decision tree. In Section 2, we describe the igtree model and its properties. <p> We discuss related research in Section 5, and present our conclusions in Section 6. 2 IGTree In this Section, we provide both an intuitive and algorithmic description of igtree, and provide some analyses on complexity issues. A more detailed discussion can be found in <ref> [DVW97] </ref>. igtree compresses a case base into a decision tree by recursively partitioning the case base on the basis of the most relevant features.
Reference: [DZBG96] <author> Daelemans, W., J. Zavrel, P. Berck, S. Gillis. </author> <year> (1996) </year> <month> MBT: </month> <title> A Memory-Based Part of Speech Tagger-Generator. </title> <editor> In: E. Ejerhed and I. Dagan (eds.) </editor> <booktitle> Proceedings of the Fourth Workshop on Very Large Corpora, Copenhagen, Denmark, </booktitle> <pages> 14-27. </pages>
Reference-contexts: The correct category of a word depends on both its lexical probability P r (catjword), and its contextual probability P r (catjcontext). Tagging is a typical instance of the type of disambiguation tasks found in Natural Language Processing. The architecture of our Instance-Based Learning tagger (see <ref> [DZBG96] </ref> for a full description) takes the form of a tagger generator: given a corpus tagged with the desired tag set, a tagger is generated which maps the words of new text to tags in this tag set according to the same systematicity. <p> We provide word form information to the tagger by encoding the first letter and the three last letters of the word as separate features in the case representation. Context information is added to the case representation in a similar way as with known words. In an evaluation reported in <ref> [DZBG96] </ref> it is shown that the igtree tagger, trained on 2 million cases, has a performance on new text that is competitive with alternative hand-crafted and statistical approaches (96.7% on known words, 90.6% on unknown words, 96.4% overall generalisation accuracy).
Reference: [KL95] <author> Kohavi, R. and Li, C-H. </author> <year> (1995). </year> <title> Oblivious decision trees, graphs, and top-down pruning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1071-1077. </pages> <address> Montreal: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Procedures for building igtrees (`BUILD-IG-TREE') and searching igtrees (`SEARCH-IG-TREE'). and the mushroom and soybean uci benchmark datasets. We compare the per formance of igtree in terms of generalisation accuracy and storage to ib1 (im plementing 1-nearest-neighbour), and ib1-ig. When available, results for c4.5, and eodg <ref> [KL95] </ref> are provided as well. eodg is an implementation of Oblivious Decision Graphs, which is an extension to Oblivious Decision Trees (see Section 5 for a discussion on the relation between igtree and Oblivious Decision Trees). <p> For both igtree and ib1-ig, the gain ratio criterion [Qui93] rather than the information gain criterion was employed, as the dataset has differing numbers of feature values. Also displayed in Table 2 are the results reported by Kohavi and Li <ref> [KL95] </ref>, who performed 10-fold CV experiments on the same datasets, with c4.5, c4.5rules, and eodg [KL95]. <p> Also displayed in Table 2 are the results reported by Kohavi and Li <ref> [KL95] </ref>, who performed 10-fold CV experiments on the same datasets, with c4.5, c4.5rules, and eodg [KL95]. Table 2 displays the average generalisation performance for these two datasets on test cases of igtree, ib1, and ib1-ig, and lists the average sizes of the decision trees and graphs generated by igtree as found in our experiments, and by c4.5 and eodg as reported in [KL95]. <p> c4.5rules, and eodg <ref> [KL95] </ref>. Table 2 displays the average generalisation performance for these two datasets on test cases of igtree, ib1, and ib1-ig, and lists the average sizes of the decision trees and graphs generated by igtree as found in our experiments, and by c4.5 and eodg as reported in [KL95]. The results of Table 2 indicate that the generalisation accuracy of igtree is similar and sometimes better than that of the algorithms tested by Kohavi and Table 2. <p> accuracy Size mushroom (%) (# nodes) soybean (%) (# nodes) IGTree 100.00 0.00 20.0 91.61 2.84 207.1 IB1 100.00 0.00 - 91.30 3.10 - Kohavi & Li (1995): EODG 100.00 0.00 31.6 81.13 1.87 80.9 C4.5 100.00 0.00 30.5 92.54 1.43 66.5 C4.5rules 100.00 0.00 - 91.28 1.00 - Li <ref> [KL95] </ref>, ib1, and ib1-ig. <p> In sum, tribl performs well on these five datasets, attaining roughly the same performance as the better of algorithms, except with the Letter dataset. These results are also in the same range as the results of c4.5, c4.5rules, and eodg reported in <ref> [KL95] </ref> (except for the Chess database). Due to the fact that a partial igtree is still built in tribl, the speed advantage of igtree largely remains in tribl. <p> This makes igtree induction considerably faster than c4.5. The price paid for this advantage, however, is that the influence of feature interaction on the relevance order is ignored. The igtree approach differs in one essential aspect from other oblivious-decision-tree [LS94] and oblivious-decision-graph <ref> [KL95] </ref> approaches: in trees generated by igtree, leaves are not necessarily stored at the same level. During tree building, expansion of the tree is stopped when all cases in the subset indexed by a node are of the same class. <p> Similarly, igtree classifies a new case by investigating a variable and often limited number of features, rather than a fixed number of (relevant) features, as in <ref> [KL95] </ref>. 6 Conclusions We have shown that igtree, a case-base compression and indexing formalism based on oblivious decision trees and a feature-relevance heuristic, is extremely well suited for a class of problems which can be characterised by the following properties: A large case base is available (on the order of 100-1000
Reference: [LS94] <author> Langley, P. and Sage, S. </author> <year> (1994). </year> <title> Oblivious decision trees and abstract cases. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This makes igtree induction considerably faster than c4.5. The price paid for this advantage, however, is that the influence of feature interaction on the relevance order is ignored. The igtree approach differs in one essential aspect from other oblivious-decision-tree <ref> [LS94] </ref> and oblivious-decision-graph [KL95] approaches: in trees generated by igtree, leaves are not necessarily stored at the same level. During tree building, expansion of the tree is stopped when all cases in the subset indexed by a node are of the same class.
Reference: [Mur95] <author> Murphy, P. </author> <year> (1995). </year> <title> UCI repository of machine learning databases a machine-readable repository. </title> <institution> Maintained at the Department of Information and Computer Science, University of California, Irvine. </institution> <note> Anonymous ftp from ics.uci.edu in the directory pub/machine-learning/databases. </note>
Reference: [Qui93] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The soybean (large) dataset contains 631 cases with 35 attributes, all of which were discretised. Each case (representing the symptoms of a plant) is labelled with one of 19 diagnoses. For both igtree and ib1-ig, the gain ratio criterion <ref> [Qui93] </ref> rather than the information gain criterion was employed, as the dataset has differing numbers of feature values. Also displayed in Table 2 are the results reported by Kohavi and Li [KL95], who performed 10-fold CV experiments on the same datasets, with c4.5, c4.5rules, and eodg [KL95]. <p> Top-Down Induction of Decision Trees (tdidt), and Oblivious Decision Trees or Graphs. A fundamental difference with tdidt concerns the purpose of igtrees. The goal of tdidt, as in the state-of-the-art program c4.5 <ref> [Qui93] </ref>, is to abstract from the training examples. In contrast, we use decision trees for lossless compression of the training examples. Pruning of the resulting tree in order to derive understandable decision trees or rule sets is therefore not an issue in our approach.
Reference: [VD93] <author> Van den Bosch, A. and Daelemans, W. </author> <year> (1993). </year> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL, </booktitle> <pages> 45-53. </pages> <address> Utrecht: </address> <month> OTS. </month> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: In ib1, all features are assigned the same relevance, which is undesirable for our linguistic problems. We noticed that ib1, when extended with a simple feature-weighting similarity function, outperforms ib1, and sometimes also outperforms both connectionist approaches and knowledge-based "linguistic-engineering" approaches <ref> [VD93] </ref>. <p> Based on these findings with ib1-ig, we designed a variant of ib1 in which the case base is compressed into a tree-based data structure in such a way that access to relevant cases is faster, and no relevant information about the cases is lost. This simple algorithm, igtree <ref> [VD93, DVW97] </ref>, uses a feature-relevance metric such as information gain to restructure the case base into a decision tree. In Section 2, we describe the igtree model and its properties.
References-found: 11

