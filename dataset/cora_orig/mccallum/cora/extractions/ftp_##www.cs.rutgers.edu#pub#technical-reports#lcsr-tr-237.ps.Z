URL: ftp://www.cs.rutgers.edu/pub/technical-reports/lcsr-tr-237.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: AUTOMATING SOFTWARE DESIGN FOR RESOURCE ASSIGNMENT PROBLEMS  Written under the direction of  
Author: BY KERSTIN VOIGT Prof. Christoper Tong 
Degree: A dissertation submitted to the Graduate School|New Brunswick  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  and approved by  
Date: January, 1995  
Note: Graduate Program in Computer Science  
Address: New Jersey  Brunswick, New Jersey  
Affiliation: Rutgers, The State University of  New  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Amarel. </author> <title> Expert Behavior and Problem Representations. </title> <type> Technical Report CBM-TR-126, </type> <institution> Computer Science Department, Rutgers University, </institution> <month> March </month> <year> 1982. </year>
Reference-contexts: The design proceeds in roughly the four phases listed below: Phases of Classification-Based Design of Algorithms: INPUT: problem specification P rob <ref> [1] </ref> CLASSIFY problem P rob as member of some problem class P class. [2] RETRIEVE the generic algorithm schema P sch associated with problem class P class. [3] INSTANTIATE the generic algorithm schema P sch with information specific to the problem P rob. <p> If RAP is RAP3 (RAP4), in the current state X, collect all multiply assigned consumers (resources). <ref> [1] </ref> Determine the existence of blocking operators: [1.1] For each parameter of X, determine the set of most improving operators M ostImprov among the total set of improving operators. [1.2] Hypothetically apply all operators in M ostImprov to the current state X. [1.3] For RAP2 (RAP5): Label each consumer (resource) in
Reference: [2] <author> S. Amarel. </author> <title> Program Synthesis as a Theory Formation Task Problem Rrepresen-tations and Solution Methods. </title> <type> Technical Report CBM-tr-135, </type> <institution> Computer Science Department, Rutgers University, </institution> <year> 1983. </year>
Reference-contexts: The design proceeds in roughly the four phases listed below: Phases of Classification-Based Design of Algorithms: INPUT: problem specification P rob [1] CLASSIFY problem P rob as member of some problem class P class. <ref> [2] </ref> RETRIEVE the generic algorithm schema P sch associated with problem class P class. [3] INSTANTIATE the generic algorithm schema P sch with information specific to the problem P rob. <p> Continue with the regular patching cycle ... Otherwise, go to step <ref> [2] </ref> [2] Determine and apply block-preventing moves: [2.1] Collect all operators that in step [1.2] lead to assignments that where marked "critical" in step [1.5]. <p> Continue with the regular patching cycle ... Otherwise, go to step <ref> [2] </ref> [2] Determine and apply block-preventing moves: [2.1] Collect all operators that in step [1.2] lead to assignments that where marked "critical" in step [1.5].
Reference: [3] <author> D. Barstow. </author> <title> An experiment in knowledge-based automatic programming. </title> <journal> In Artificial Intelligence, </journal> <volume> volume 12:2, </volume> <pages> pages 73-119. </pages> <publisher> North Holland, </publisher> <month> August </month> <year> 1979. </year>
Reference-contexts: The design proceeds in roughly the four phases listed below: Phases of Classification-Based Design of Algorithms: INPUT: problem specification P rob [1] CLASSIFY problem P rob as member of some problem class P class. [2] RETRIEVE the generic algorithm schema P sch associated with problem class P class. <ref> [3] </ref> INSTANTIATE the generic algorithm schema P sch with information specific to the problem P rob. Produce the specialized algorithm P rob Solv. [4] OPTIMIZE the specialized algorithm P rob Solv. (optional) 15 OUTPUT: algorithm P rob Solv Classification-based approaches have been used successfully in algorithm design before.
Reference: [4] <author> D. Barstow. </author> <title> Domain-Specific Automatic Programming. </title> <journal> IEEE Transactions of Software Engineering, </journal> <volume> SE-11(11), </volume> <month> 11 </month> <year> 1985. </year>
Reference-contexts: Produce the specialized algorithm P rob Solv. <ref> [4] </ref> OPTIMIZE the specialized algorithm P rob Solv. (optional) 15 OUTPUT: algorithm P rob Solv Classification-based approaches have been used successfully in algorithm design before. Two examples are the systems KIDS by Smith [127] and STRATA by Lowry [74]. <p> rules are applied at design run-time, the coarser-grained schemas can be viewed as the result of precompiling and caching the effects of a series of transformations (schema as a "macro-transformation"). 7.1.2 Domain/problem specific systems On the (literally) opposite side of the just discussed approaches are systems like NIX by Barstow <ref> [4] </ref>, SINAPSE by Kant and coworkers [76], or ELF by Setliff [118]. These are software synthesis systems which have been designed to synthesize code that solves specific types of problems in specific domains.
Reference: [5] <author> F.L. Bauer and H. Wossner. </author> <title> Algorithmic Language and Program Development. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: Our representation of data types is modeled after the one suggested in <ref> [5] </ref>. Homogeneous and heterogeneous composite data structures. We distinguish between homogeneous and heterogeneous composites. Heterogeneous composites are data structures where the subcomponents may be different data structures. Homogeneous structures are composite data structures whose components of the same type; ie., the are represented by the same data structure.
Reference: [6] <author> R.E. Bellman and S.E. Dreyfus. </author> <title> Applied Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1962. </year>
Reference: [7] <author> J. Bennett and T. Dietterich. </author> <title> The Test Incorporation Hypothesis and the Weak Methods. </title> <type> Technical Report TR 86-30-4, </type> <institution> Department of Computer Science, Ore-gon State University, Corvallis, Oregon, </institution> <year> 1986. </year>
Reference-contexts: The constraint is still tested for, however, the testing and potential discovery of inconsistencies (deadends) takes place in the earlier stages of generation. Constraint incorporation goes one step further in that constraint information is assimilated into the generation process in a fashion that removes constraint testing altogether (see <ref> [30, 7, 15, 14] </ref>).
Reference: [8] <author> B. Bernhardsson. </author> <title> Explicit Solutions to the N-Queens Problems for all N. </title> <journal> SIGART Bulletin, </journal> <volume> 2(2), </volume> <year> 1991. </year>
Reference: [9] <author> N. Bhatnagar. </author> <title> On-Line Learning From Search Failures. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1992. </year>
Reference-contexts: The 221 first type of approach extracts heuristic measures of the distance to a solution from relaxed models of the original search space [106, 110]. The second type of approach derives heuristics through explaining and specializing general search control rules for the specific domain and problem instance (e.g. <ref> [142, 59, 86, 9] </ref>). Deriving heuristics through model relaxation Pearl [104, 106] has shown how admissible search heuristics can be constructed systematically through relaxation of the domain model. Given a state-space search model, the model can be relaxed by deleting constraints from the list of operator preconditions. <p> Negative heuristics are search control rules that help prune the search space of subspaces that are void of solution states. Such heuristics have been automatically derived by Minton's PRODIGY, and the systems FAILSAFE and FS2, both developed by Bhatnagar <ref> [96, 9] </ref>. MENDER: deriving heuristics by schema instantiation The evaluation functions compiled by MENDER fall into the category of positive heuristics since they are employed to guide the search along the improving paths in the search space.
Reference: [10] <author> A. Biermann. </author> <title> Approaches to Automatic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference: [11] <author> E. Bonomi and J.-L. Lutton. </author> <title> The N -City Travelling Salesman Problem: Statistical Mechanics and the Metropolis Algorithm. </title> <journal> SIAM Review, </journal> <volume> 26(4), </volume> <month> October </month> <year> 1984. </year>
Reference-contexts: Optimization problems in resource assignment are, for instance: * Minimize the sum of all consumed resources. * Maximize the participation of consumers in the consumption of the resources. * Minimize (or, maximize) the sharing of resources by multiple consumers. In the recent literature (e.g., <ref> [40, 154, 119, 113, 153, 144, 11, 112] </ref>), we find reports on good performances of a simulated annealing problem solvers for a wide range of practical optimization problems (e.g., gate matrix layout, channel routing, cell placement, design of "good" codes, traveling salesman).
Reference: [12] <editor> J. Bortz. Lehrbuch der Statistik Fuer Sozialwissentschaftler. </editor> <publisher> Springer Verlag, </publisher> <year> 1977. </year>
Reference-contexts: For nonspecifc hypotheses like the ones above, however, the distribution for 1 and, therefore, fi is not known (also see <ref> [12] </ref>). Despite our inability to determine fi, we nevertheless want to factor in any evidence even if statistically weaker in favor or against the equality hypotheses that frequently appear as instances of H3.
Reference: [13] <author> W. Braudaway and C. Tong. </author> <title> Automated synthesis of constrained generators. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 583-589, </pages> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year>
Reference: [14] <author> W.K. Braudaway. </author> <title> Automates Synthesis of Constrained Generators. In M.R. </title> <editor> Lowry and R.D. McCartney, editors, </editor> <booktitle> Automating Software Design. </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press, </publisher> <year> 1991. </year> <month> 330 </month>
Reference-contexts: Such systems are RICK by Braudaway <ref> [15, 14] </ref>, and the here presented system MENDER. On a larger scale, similar goals are pursued by the ongoing research at Rutgers on the KBSDE environment [139, 136]. RICK is a system that automatically compiles efficient constrained generators for a declarative problem specification. <p> The constraint is still tested for, however, the testing and potential discovery of inconsistencies (deadends) takes place in the earlier stages of generation. Constraint incorporation goes one step further in that constraint information is assimilated into the generation process in a fashion that removes constraint testing altogether (see <ref> [30, 7, 15, 14] </ref>). <p> Frequently, the structure of the generator does not directly align with the terms in which the constraint is expressed. This problem is called the "structure mismatch problem" (see, e.g., <ref> [14] </ref>), and presents a major obstacle in constraint incorporation. In RICK, Braudaway has developed a method by which to overcome the structure mismatch problem. <p> MENDER is one of the subsystems in KBSDE; MENDER is designed to specifically construct patching problem solvers for global resource assignment problems. Other subsystems of KBSDE are RICK <ref> [14, 15] </ref>, and HiT [89].
Reference: [15] <author> W.K. Braudaway. </author> <title> Knowledge Compilation for Incorporating Constraints. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1991. </year>
Reference-contexts: At generation run-time, constraint propagation can sometimes ensure that the domain of the currently to be instantiated parameter is pruned of all values that would violate these object constraints given the values of the previously assigned parameters. On how to automatically construct such generators see, for example, <ref> [15] </ref>. floorplans such that rooms do not overlap. 1 Assume that room R1 has already been generated with its reference corner located at X equal to 2 and Y equal to 4, and with a length and width of 2, respectively. Next room R2 is to be generated. <p> Next a value for R2.L is selected. Again, 1 For historic reasons that have to do with the fact that this dissertation makes use of some of the dissertation work by Braudaway <ref> [15] </ref>, we also assume that all rooms are generated such that at least one room wall is aligned with some house wall. 138 0,0 PRUNE! of overlap Illegal because OKOKOK R2 R2R2R2 R2.L = 3R2.L = 2 R2.X &lt;- 0 R2.Y &lt;- Y R1.Y = 4 R1.W = 2 ing of <p> An example of a piece of code that accomplishes the constraint propagation and that also preserves the satisfaction of the context constraint "rooms do not overlap" is shown in Figure 5.9. (This code has been automatically compiled by Braudaway's RICK system <ref> [15] </ref>.) 147 prune values 3 and4 ok 4.3.2.1. 1 LYX 1 3 3 1 2 4 3 domain: R2: X Y L 4 2 4 2 1 33 1 3 R2.L &lt;- 4 3 Conflict! W W order of generation and interdependency propagate and prune ... operator 'R2.L &lt;- 4'. 148 <p> evaluation of all applicable operators (abbre viated: 'gp-gr-fe'); * generate-and-semi-greedy (abbreviated: 'gp-sm-gr') * generate-and-random-patch (abbreviated: 'gp-df'); 6.2.3 Comparing generate-and-test and generate-and-patch problem solvers To ensure comparable implementations of generate-and-test and generate-and-patch problem solvers, we model the patcher component after the implementation of the generation component (as provided by Braudaway in <ref> [15] </ref>). Thus, one move in the generation space consists of instantiating a parameter with some value for the first time, whereas, in the patching space (space of complete artifacts) one move consists of replacing an old parameter value with a new value. Both operations are comparable in computational effort. <p> Such systems are RICK by Braudaway <ref> [15, 14] </ref>, and the here presented system MENDER. On a larger scale, similar goals are pursued by the ongoing research at Rutgers on the KBSDE environment [139, 136]. RICK is a system that automatically compiles efficient constrained generators for a declarative problem specification. <p> The constraint is still tested for, however, the testing and potential discovery of inconsistencies (deadends) takes place in the earlier stages of generation. Constraint incorporation goes one step further in that constraint information is assimilated into the generation process in a fashion that removes constraint testing altogether (see <ref> [30, 7, 15, 14] </ref>). <p> MENDER is one of the subsystems in KBSDE; MENDER is designed to specifically construct patching problem solvers for global resource assignment problems. Other subsystems of KBSDE are RICK <ref> [14, 15] </ref>, and HiT [89]. <p> These generators precede the MENDER-compiled patcher in the generate-and-patch problem solvers. The Common Lisp code in which the generators are implemented, is either directed adopted from Braudaway's RICK system output (see <ref> [15] </ref>, or it has been modeled after his style of representing generators. Note for each (type of) generator below, two pieces of code are listed namely DEFPROBLEM-SOLVER-INITIALIZE, and DEFOBJECT-GENERATOR Both are defined as macros which, when called, will built up an actual generator instance.
Reference: [16] <author> D.C. Brown. </author> <title> Compilation: The Hidden Dimension of Design Systems. </title> <editor> In H. Yoshikawa and F. Arbab, editors, </editor> <booktitle> Intelligent CAD III, </booktitle> <address> Amsterdam, 1991. </address> <publisher> North Holland. </publisher>
Reference-contexts: Knowledge compilation converts declarative problem specification at the knowledge level to function level algorithms. The latter are composed from functional components (e.g., generators, testers, patchers). Ultimately the function-level construct is translated into a program in some standard programming language at the program level. According to Brown <ref> [16] </ref>, among the effects typical of knowledge compilation are an increase in efficiency or usability of the knowledge and a certain amount of "short-circuiting" or caching of reasoning.
Reference: [17] <author> D.C. Brown. </author> <title> Compilation: The Hidden Dimension of Design Systems. </title> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference: [18] <author> T. Bylander. </author> <title> A Simple Model of Knowledge Compilation. </title> <journal> IEEE Expert, </journal> <volume> 6(2), </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Knowlege compilation methods can be understood as transformations, which may be either content-preserving or content-modifying (see [58]). Among content-modifying knowledge compilation we can further distinguish methods that add knowledge from methods that remove knowledge (e.g., see <ref> [18, 19] </ref>). 220 More narrowly, software-design oriented understandings of knowledge compilation have been expressed by, for example, Dietterich [31] and Tong [140, 141].
Reference: [19] <author> B. Chandrasekaran. </author> <title> Models versus Rules, Deep versus Compiled, Content versus Form. </title> <journal> IEEE Expert, </journal> <volume> 6(2), </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Knowlege compilation methods can be understood as transformations, which may be either content-preserving or content-modifying (see [58]). Among content-modifying knowledge compilation we can further distinguish methods that add knowledge from methods that remove knowledge (e.g., see <ref> [18, 19] </ref>). 220 More narrowly, software-design oriented understandings of knowledge compilation have been expressed by, for example, Dietterich [31] and Tong [140, 141].
Reference: [20] <author> W. Clancey. </author> <title> Classification Problem-Solving. </title> <booktitle> In Proceedings of AAAI-84, </booktitle> <month> August </month> <year> 1984. </year>
Reference: [21] <author> D. Cohen. </author> <title> Automatic Compilation of Logical Specifications into Efficient Programs. </title> <booktitle> In Proceedings of AAAI-86, </booktitle> <year> 1986. </year>
Reference: [22] <author> J.M. Crawford and L.D. Auton. </author> <title> Experimental Results on the Crossover Point in Satisfiability Problems. </title> <booktitle> In Proceedings of AAAI-93, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1993. </year>
Reference: [23] <author> G. Dantzig. </author> <title> Programming in a Linear Structure. </title> <journal> Econometrics, </journal> <volume> 17, </volume> <year> 1949. </year>
Reference: [24] <author> J. de Kleer. </author> <title> Choices Without Backtracking. </title> <booktitle> In </booktitle> ???, ??? 
Reference: [25] <author> M. Zweben, E. Davis, B. Daun, E. Drasher, M. Deale, and M. Eskey. </author> <title> Learning to improve constraint-based scheduling. </title> <journal> Artificial Intelligence, </journal> <volume> 58, </volume> <year> 1992. </year>
Reference-contexts: On the other hand, the newly learned expression is also a specialization of the original definition of concept P. This view has been expressed by researchers in explanation-based specialization (EBS) such as Minton [86, 83] or Zweben <ref> [25] </ref>. (A slight technical distinction between EBG and EBS techniques does exist: EBS works strictly top-down from the general concept, driven by the specific training example, and does not necessitate the additional bottom-up processing of the proof-tree in EBG.) In principle, given the appropriate ingredients, any general concept can be specialized,
Reference: [26] <author> R. Dechter. </author> <title> Enhancement Schemes for Constraint Processing: Backjumping, Learning, and Cutset Decomposition. </title> <journal> Artificial Intelligence, </journal> <volume> 41(3), </volume> <year> 1990. </year>
Reference-contexts: 227 F = future variables C = current variable P = past variables P C F P C F Backtrack: Forward-Checking: Partial Lookahead: Full Lookahead: order of value-variable assignments There exist a variety of other techniques that take advantage of the structure of the constraint graph to reduce search (e.g. <ref> [28, 26] </ref>. Alternatively to processing the underlying constraint-graph of a CSP, other techniques have been suggested that do not alter the search space, but strive to search the original space more efficiently.
Reference: [27] <author> R. Dechter. </author> <title> From local to global consistency. </title> <journal> Artificial Intelligence, </journal> <volume> 55, </volume> <year> 1992. </year>
Reference: [28] <author> R. Dechter and J. Pearl. </author> <title> Network-Based Heuristics for Constraint-Satisfaction Problems. </title> <journal> Artificial Intelligence, </journal> <volume> 34(1), </volume> <year> 1987. </year>
Reference-contexts: 227 F = future variables C = current variable P = past variables P C F P C F Backtrack: Forward-Checking: Partial Lookahead: Full Lookahead: order of value-variable assignments There exist a variety of other techniques that take advantage of the structure of the constraint graph to reduce search (e.g. <ref> [28, 26] </ref>. Alternatively to processing the underlying constraint-graph of a CSP, other techniques have been suggested that do not alter the search space, but strive to search the original space more efficiently. <p> Alternatively to processing the underlying constraint-graph of a CSP, other techniques have been suggested that do not alter the search space, but strive to search the original space more efficiently. Such techniques are look-ahead techniques [100], dependency-directed backtracking schemes [131], and heuristics for variable and value ordering (e.g., <ref> [109, 28] </ref>). Nadel has examined backtracking search algorithms for solving CSPs with varying levels of look-ahead. Look-ahead is a mechanism which to determine whether or not consistent value assignments to yet unassigned variables still exist given the set of past and current assignments.
Reference: [29] <author> T. Dietterich, </author> <title> editor. </title> <booktitle> Proceedings of the Workshop on Knowledge Compilation. AAAI, </booktitle> <institution> Oregon State Univ., </institution> <month> September </month> <year> 1986. </year>
Reference: [30] <author> T. Dietterich and J. Bennett. </author> <title> The Test Incorporation Theory of Problem Solving (Preliminary Report). </title> <editor> In T. Dietterich, editor, </editor> <booktitle> Proceedings of the Workshop on Knowledge Compilation, </booktitle> <month> September </month> <year> 1986. </year>
Reference-contexts: The constraint is still tested for, however, the testing and potential discovery of inconsistencies (deadends) takes place in the earlier stages of generation. Constraint incorporation goes one step further in that constraint information is assimilated into the generation process in a fashion that removes constraint testing altogether (see <ref> [30, 7, 15, 14] </ref>).
Reference: [31] <author> T. G. Dietterich. </author> <title> Bridging the Gap betweem Specification and Implementation. </title> <journal> IEEE Expert, </journal> <volume> 6(2), </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Among content-modifying knowledge compilation we can further distinguish methods that add knowledge from methods that remove knowledge (e.g., see [18, 19]). 220 More narrowly, software-design oriented understandings of knowledge compilation have been expressed by, for example, Dietterich <ref> [31] </ref> and Tong [140, 141]. Dietterich sees knowledge compilation techniques as the means by which to close the gap between declarative problem specifications and run-time problem solvers that are capable of exploiting the domain and problem knowledge in a more direct and task-oriented fashion.
Reference: [32] <editor> W.H. Press et. al. </editor> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Mass., </address> <year> 1988. </year>
Reference-contexts: The method is tableau-based, and each iterative step modifies the tableau around a selected "pivot element" such that the objective function is improved. For detailed descriptions of the above three methods (and others) see, for example, <ref> [32] </ref>. It is interesting to note that many numerical analysis methods tend to resort to iterative improvement when problems become less manageable due to increased size (e.g., larger numbers of variables or equations), or increasingly complex properties of their internal structures.
Reference: [33] <author> O. Etzioni. </author> <title> Something on: sign test, censored data ... ???, ??? 331 </title>
Reference-contexts: Statistical sign-tests are useful when dealing with data sets that might include "censored" data. Censored data are data that correspond to some upper resource bound after problem solving failed to produce a solution within the given bound. For further details on these statistical tests see, for example, <ref> [48, 33] </ref>. 2 sets of sample data, namely DA and DB. DA and DB stand for performance data collected for two problem solvers A and B, respectively.
Reference: [34] <author> E. Freuder and M. Quinn. </author> <title> Taking Advantage of Stable Sets of Variables in Constraint-Satisfaction Problems. </title> <booktitle> In Proceedings of IJCAI-85, </booktitle> <address> Menlo Park, CA, </address> <year> 1985. </year>
Reference-contexts: In addition, the focus of dependency-backtracking will remain narrow and, thus, useful only as long as the number of interacting variables is significantly less than the number of all variables in the CSP. In the literature (e.g., <ref> [34, 109] </ref>), one finds a variety of heuristics for the run-time or dering of variables and values.
Reference: [35] <author> E.C. Freuder. </author> <title> A sufficient condition for backtrack-free search. </title> <journal> Journal of the ACM, </journal> <volume> 29(1), </volume> <year> 1982. </year>
Reference: [36] <editor> E.C. Freuder. Backtrack-Free and Backtrack-Bounded Search. In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer, </publisher> <year> 1988. </year>
Reference-contexts: Thus, k-consistency implies that the domains of k mutually constraining variables have been pruned of values such that all remaining combinations of variable assignments are guaranteed to satisfy all constraints between the k variables. It has been proven <ref> [36] </ref>, that after preprocessing a constraint-graph with width 3 w to establish k-consistency for k &gt; w the constraint-graph can be solved by backtrack-free search.
Reference: [37] <author> R. Mooney G. Dejong. </author> <title> Explanation-Based Learning: An Alternative View. </title> <journal> Machine Learning, </journal> <volume> 2(???), </volume> ??? 
Reference-contexts: Deriving heuristics through explanation-based learning The term explanation-based learning (EBL) summarizes variants of a concept learning technique developed and extended by several researchers in machine learning (e.g., <ref> [59, 37] </ref>). The basic declarative ingredients for EBL are (1) a general concept description, (2) a domain theory that underlies the definition of the general concept, and (3) a training example, which is a specific instance (e.g., object) in the domain.
Reference: [38] <author> M.R. Garey and D.S. Johnston. </author> <title> Computers and Intractability. A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: that Satisfy Constraints in Timetable Design 5.3.1 Timetable design Problems in the domain of timetable design involve for example the assignment of workers, tasks and hours such that certain requirements (e.g., the avoidance of worker or task conflicts) are met (for a version of this in general NP-complete problem see <ref> [38] </ref>). We chose a variant of this timetable design problem which phrases the problem as one of designing timetables that assign lecturers, seminars, days, locations, and salaries such that various requirements are met. <p> Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem [109, 130], (2) propositional satisfiability (SAT) <ref> [46, 69, 115, 38] </ref>, (3) scheduling [109], and (4) graph-coloring [115, 38]. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing. <p> Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem [109, 130], (2) propositional satisfiability (SAT) [46, 69, 115, 38], (3) scheduling [109], and (4) graph-coloring <ref> [115, 38] </ref>. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing.
Reference: [39] <author> J. Gaschnig. </author> <title> A Problem Similarity Approach to Devising Heuristics: First Results. </title> <booktitle> In Proceedings of IJCAI-79, </booktitle> <year> 1979. </year>
Reference: [40] <author> S. Kirkpatrick, </author> <title> C.D. Gelatt, and M.P. Vecchi. Optimization by Simulated Annealing. </title> <journal> Science, </journal> <volume> 220(4598), </volume> <month> May </month> <year> 1983. </year>
Reference-contexts: In addition, simulated annealing which is a type of probabilistic heuristic repair method has been successful in solving numerous global problems in domains such as, for example, VLSI circuit circuit placement and routing, the traveling salesman problem, the design of "good" binary codes (e.g., <ref> [40, 119] </ref>). <p> Smaller, more fine-tuning changes to the configuration happen at the lower temperatures. Yet, since occasional relapses to a higher objective function are tolerated probabilistically, this form of local search will always retain is ability escape from local maxima at nonzero temperatures (e.g., see <ref> [40] </ref>). Kirkpatrick [40] reports on successful applications of simulated annealing to a variety of problems in the physical design of computers. <p> Smaller, more fine-tuning changes to the configuration happen at the lower temperatures. Yet, since occasional relapses to a higher objective function are tolerated probabilistically, this form of local search will always retain is ability escape from local maxima at nonzero temperatures (e.g., see <ref> [40] </ref>). Kirkpatrick [40] reports on successful applications of simulated annealing to a variety of problems in the physical design of computers. <p> Optimization problems in resource assignment are, for instance: * Minimize the sum of all consumed resources. * Maximize the participation of consumers in the consumption of the resources. * Minimize (or, maximize) the sharing of resources by multiple consumers. In the recent literature (e.g., <ref> [40, 154, 119, 113, 153, 144, 11, 112] </ref>), we find reports on good performances of a simulated annealing problem solvers for a wide range of practical optimization problems (e.g., gate matrix layout, channel routing, cell placement, design of "good" codes, traveling salesman).
Reference: [41] <author> M.R. Genesereth and N.J. Nilsson. </author> <booktitle> Logical Foundations of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference: [42] <author> A.K. Goel. </author> <title> Knowledge Compilation. </title> <journal> IEEE Expert, </journal> <volume> 6(2), </volume> <month> April </month> <year> 1991. </year>
Reference: [43] <author> D.E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1989. </year>
Reference-contexts: We see potentially interesting parallels between this latter suggestion and genetic algorithms and evolutionary computing. Genetic algorithms (e.g., see <ref> [43] </ref>) produce increasingly better solutions through repeated "crossovers" of pairs of highly ranked structures from the set ("population") of current solutions. Simulated annealing patchers The type of local search discussed so far, assumed that the problems to be solved were decision problems.
Reference: [44] <author> C. Green. </author> <title> Application of Theorem Proving to Problem Solving. </title> <booktitle> In Proceedings of IJCAI-1, </booktitle> <year> 1969. </year>
Reference-contexts: Next we will discuss the systems in each group in greater detail. 7.1.1 Domain-independent and interactive software design systems Among early attempts at domain-independent software design are the constructive theorem proving systems by Green <ref> [44] </ref> and Manna and Waldinger [78]. Such systems derive programs from formal specifications constructively from resolution theorem proofs. Given a problem specification in first order logic (FOL), the existence of a solution is first proved formally.
Reference: [45] <author> C. Green. </author> <title> Foreword. In Automating Software Design. </title> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference: [46] <author> J. Gu. </author> <title> Efficient Local Search for Very Large-Scale Satisfiability Problems. </title> <journal> SIGART Bulletin, </journal> <volume> 3(1), </volume> <year> 1992. </year>
Reference-contexts: Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem [109, 130], (2) propositional satisfiability (SAT) <ref> [46, 69, 115, 38] </ref>, (3) scheduling [109], and (4) graph-coloring [115, 38]. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing.
Reference: [47] <author> R.M. Haralick and G.L. Elliot. </author> <title> Increasing Tree Search Efficiency for Constraint Satisfaction Problems. </title> <journal> Artificial Intelligence, </journal> <volume> 14, </volume> <year> 1980. </year>
Reference-contexts: Backtracking performs zero-degree look-ahead. When forward-checking is added, it is also verified that the value selection for the current variable will not result in inconsistencies between as yet unassigned (F for 'future') variables and the current and past variables (see also <ref> [47] </ref>). In a further refinement to forward-checking , partial look-ahead also makes sure that no inconsistencies will arise between more future variables 228 and less future variables (in the given order of assignment). Finally, full look-ahead adds bi-directional consistency checks between all future variables.
Reference: [48] <author> Hays. Statistics. Holt, Rinehart and Winston, Inc., </author> <year> 1973. </year>
Reference-contexts: Statistical sign-tests are useful when dealing with data sets that might include "censored" data. Censored data are data that correspond to some upper resource bound after problem solving failed to produce a solution within the given bound. For further details on these statistical tests see, for example, <ref> [48, 33] </ref>. 2 sets of sample data, namely DA and DB. DA and DB stand for performance data collected for two problem solvers A and B, respectively.
Reference: [49] <author> G.E. Hinton and T.J. Sejnowski. </author> <title> Learning and Relearning in Boltzmann Machines. </title> <editor> In J.L. McClelland D.E. Rumelhart and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing Volume 1, chapter 7. </booktitle> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference: [50] <author> R. Holte. </author> <title> Efficient candidate elimination through test incorporation. </title> <editor> In D. Ben-jamin, editor, </editor> <booktitle> Change of Representation and Inductive Bias, </booktitle> <pages> pages 223-230. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference: [51] <author> J.E. Hopcroft and J.D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year> <month> 332 </month>
Reference: [52] <author> G. Hunter. </author> <title> Metalogic An Introduction to the Metatheory of Standard First Order Logic. </title> <institution> University of California Press, </institution> <year> 1973. </year>
Reference-contexts: depending on data d 2 D is semi-decidable if and only if there exists an algorithm which when given data d always delivers an answer in a finite amount of time when the answer to the problem is positive, but may not terminate for negative answers (e.g., see [67] or <ref> [52] </ref> on decidability). 68 3.2.1 Initial question and namings An initial, very generally phrased question determines whether MENDER might be at all qualified to handle the type of problem on the user's mind.
Reference: [53] <author> T. Ibaraki. </author> <title> Branch-and-bound procedure and state space representation of combinatorial optimization problems. </title> <journal> Information and Control, </journal> <volume> 36(1), </volume> <year> 1978. </year>
Reference: [54] <author> Gent I.P and T. Walsh. </author> <title> Towards an Understanding of Hill-climbing Procedures for SAT. </title> <booktitle> In Proceedings of AAAI-93, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Researchers generally agree that the ability to make lateral moves (preferred over down-hill moves) is crucial in escaping the local maxima which are the inherent drawback of greedy local search (e.g., see <ref> [109, 69, 54, 91] </ref>). The opinions differ on the importance of a starting state that is "close" to some solution. Minton [109], Musick and Russell [98] are among those who are in favor of good starting states. Our own experimentations with MENDER's compiled patchers support this view.
Reference: [55] <author> E. Kant. </author> <title> A knowledge-based approach to using efficiency estimation in program synthesis. </title> <booktitle> In IJCAI-6, </booktitle> <pages> pages 457-462, </pages> <address> Tokyo, Japan, </address> <year> 1979. </year>
Reference: [56] <author> E. Kant. </author> <title> Understanding and automating algorithm design. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(11):1361-1374, </volume> <month> November </month> <year> 1985. </year>
Reference: [57] <author> J. Keane. </author> <title> A Generalized Lookahead Heuristic for Patchers Produced by the MENDER Patcher-Compiler in the KBSDE Knowledge-Compilation System. </title> <institution> Computer Science Department, Rutgers University, </institution> <month> September </month> <year> 1991. </year>
Reference: [58] <author> R.M. Keller. </author> <title> Applying Knowledge Compilation Techniques to Model-Based Reasoning. </title> <journal> IEEE Expert, </journal> <volume> 6(2), </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Such wide definitions of knowledge compilation have been given, for example, by Keller <ref> [58] </ref> or Tong [140]. Knowlege compilation methods can be understood as transformations, which may be either content-preserving or content-modifying (see [58]). <p> Such wide definitions of knowledge compilation have been given, for example, by Keller <ref> [58] </ref> or Tong [140]. Knowlege compilation methods can be understood as transformations, which may be either content-preserving or content-modifying (see [58]). Among content-modifying knowledge compilation we can further distinguish methods that add knowledge from methods that remove knowledge (e.g., see [18, 19]). 220 More narrowly, software-design oriented understandings of knowledge compilation have been expressed by, for example, Dietterich [31] and Tong [140, 141].
Reference: [59] <author> T.M. Mitchell, R.M. Keller, and S.T. Kedar-Cabelli. </author> <title> Explanation-Based Generalization: A Unifying View. </title> <journal> Machine Learning, </journal> <volume> 1(???), </volume> ??? 
Reference-contexts: The 221 first type of approach extracts heuristic measures of the distance to a solution from relaxed models of the original search space [106, 110]. The second type of approach derives heuristics through explaining and specializing general search control rules for the specific domain and problem instance (e.g. <ref> [142, 59, 86, 9] </ref>). Deriving heuristics through model relaxation Pearl [104, 106] has shown how admissible search heuristics can be constructed systematically through relaxation of the domain model. Given a state-space search model, the model can be relaxed by deleting constraints from the list of operator preconditions. <p> Deriving heuristics through explanation-based learning The term explanation-based learning (EBL) summarizes variants of a concept learning technique developed and extended by several researchers in machine learning (e.g., <ref> [59, 37] </ref>). The basic declarative ingredients for EBL are (1) a general concept description, (2) a domain theory that underlies the definition of the general concept, and (3) a training example, which is a specific instance (e.g., object) in the domain.
Reference: [60] <author> K.M. Kelly and L.I. Steinberg. </author> <title> Constraint Propagation in Design: Reducing the Cost. </title> <type> Technical report, </type> <institution> Computer Science Department, Rutgers University, </institution> <year> 1988. </year> <note> Rutgers AI/Design Project Working Paper #82. </note>
Reference: [61] <author> N. Keng and D.Y.Y. Yun. </author> <title> A Planning/Scheduling Methodology for the Constrained Resource Problem. </title> <booktitle> In Proceedings of IJCAI-89, </booktitle> <year> 1989. </year>
Reference: [62] <author> D. Kibler and P. Morris. </author> <title> Don't Be Stupid. </title> <booktitle> In Proceedings of IJCAI-81, </booktitle> <year> 1981. </year>
Reference: [63] <author> D. Kirsh. </author> <title> Foundations of AI: the big issues. </title> <journal> Artificial Intelligence, </journal> <volume> 47, </volume> <year> 1991. </year>
Reference: [64] <author> R. Kowalski. </author> <title> Logic for Problem Solving. </title> <publisher> Elsevier, </publisher> <year> 1983. </year>
Reference-contexts: Here, it should be ensured that the user is presented with a set of meaningful choices only. 82 3.3.1 Representation of predicates and functions We adopt a clausal representation of predicates in the style of Horn clauses 3 (e.g., <ref> [64] </ref>). <p> RAP-PA is capable of propagating polarities through the definitions of nonprimitives at compile-time. By adopting a specification scheme similar to Prolog (e.g., <ref> [64] </ref>), polarities of nonprimitives can be propagated through the bodies of the definitions in a systematic bottom-up, right-to-left fashion. The basic principle of this technique is demonstrated in the following small example of a nonprimitive function called plus1 times5. plus1 times5 (x,y) :- plus (x,1,x1), times (x1,5,y).
Reference: [65] <author> V. Kumar. </author> <title> Algorithms for Constraint-Satisfaction Problems: A Survey. </title> <journal> The AI Magazine, </journal> <volume> 13(1), </volume> <month> Spring </month> <year> 1992. </year>
Reference-contexts: Each constraint is defined over a subset of the set of variables. 6 The task is to determine an assignment of values from domain D i to the variables X i (i = 1:::n) such that the assignment satisfies all constraints C j (j = 1:::m) (e.g., see <ref> [65] </ref>). Resource assignment problems (RAPs) We explain our notion of resource assignment problems, and then show how it describes a subclass of CSPs. Consumers and resources. A resource assignment problem is a problem of relating two structures. <p> Therefore, research on on efficient constraint satisfaction methods is of interest to developers of software design systems. We will give a brief overview over the most important types of techniques for constraint satisfaction (see also <ref> [65] </ref>). The most basic problem solving method for CSPs is generate-and-test with backtracking. In order to alleviate some, or, ideally all, of the "thrashing" that backtracking schemas typically suffer from, various constraint propagation techniques applicable at either compile-time, or run-time have been suggested.
Reference: [66] <author> J. Lam and J.-M. Delosme. </author> <title> Logic Minimization Using Simulated Annealing. </title> <booktitle> In Proceedings of the 23rd Design Automation Conference, </booktitle> <year> 1986. </year>
Reference-contexts: Simulated annealing has also performed well on the classical traveling salesman problem. We add a by no means exhaustive list of applications of simulated annealing in other areas: gate-matrix layout [68], design of "good codes" [119], logic minization <ref> [66] </ref>, and floorplanning [153]. For more examples see also [113, 144]. In retrospect, it is interesting to note, that MENDER would have been able to automatically compile local search components that conduct simulated annealing, had we given it the knowledge of an "simulated annealing shell".
Reference: [67] <editor> J. Van Leeuwen, editor. </editor> <booktitle> Handbook of Theoretical Computer Science: Formal Models and Semantics, volume B. </booktitle> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: problem P depending on data d 2 D is semi-decidable if and only if there exists an algorithm which when given data d always delivers an answer in a finite amount of time when the answer to the problem is positive, but may not terminate for negative answers (e.g., see <ref> [67] </ref> or [52] on decidability). 68 3.2.1 Initial question and namings An initial, very generally phrased question determines whether MENDER might be at all qualified to handle the type of problem on the user's mind.
Reference: [68] <author> H.W. Leong. </author> <title> A New Algorithm for Gate Matrix Layout. </title> <booktitle> In </booktitle> ???, ??? 
Reference-contexts: Simulated annealing has also performed well on the classical traveling salesman problem. We add a by no means exhaustive list of applications of simulated annealing in other areas: gate-matrix layout <ref> [68] </ref>, design of "good codes" [119], logic minization [66], and floorplanning [153]. For more examples see also [113, 144].
Reference: [69] <author> B. Selman, H. Levesque, and D. Mitchell. </author> <title> A New Method for Solving Hard Satisfiability Problems. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem [109, 130], (2) propositional satisfiability (SAT) <ref> [46, 69, 115, 38] </ref>, (3) scheduling [109], and (4) graph-coloring [115, 38]. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing. <p> Researchers generally agree that the ability to make lateral moves (preferred over down-hill moves) is crucial in escaping the local maxima which are the inherent drawback of greedy local search (e.g., see <ref> [109, 69, 54, 91] </ref>). The opinions differ on the importance of a starting state that is "close" to some solution. Minton [109], Musick and Russell [98] are among those who are in favor of good starting states. Our own experimentations with MENDER's compiled patchers support this view. <p> Furthermore, the performance data collected for our automatically constructed patchers agree with the observations of astonishingly 239 efficient performances of manually coded patchers for difficult, large-scale instances of problems in the domains of n-queens, scheduling, and satisfiability (SAT) (e.g., see <ref> [109, 130, 69] </ref>). 8.3.2 Explaining the benefits of patching We can identify roughly three features of MENDER-constructed patchers that can account for their good performances: 1. Patchers search spaces of complete states, and they do so in a constraint-oriented fashion. 2.
Reference: [70] <author> C. Liew and C. Tong. </author> <title> Knowledge compilation: A prototype system and a conceptual framework. AI/VLSI Project Working Paper No. </title> <type> 47, </type> <institution> Dept. of Computer Science, Rutgers Univ., </institution> <month> February </month> <year> 1987. </year> <month> 333 </month>
Reference: [71] <author> S. Lin and B.W. Kernighan. </author> <title> An Effective Heuristic Algorithm for the Traveling Salesman Problem. </title> <journal> Operations Research, </journal> <volume> 21, </volume> <year> 1973. </year>
Reference: [72] <author> M. Lowry. </author> <title> Algorithm synthesis through problem reformulation. </title> <booktitle> In Proceedings of AAAI87, </booktitle> <pages> pages 432-436, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: Here the type of a problem, not necessarily its very specifics, determine the ultimate form of the synthesized software. Examples of software design systems that are interactive and make use of problem class knowledge are the systems KIDS by Smith [121, 122, 127] and STRATA by Lowry <ref> [72, 74] </ref>. KIDS constructs search algorithms, e.g, global search algorithms, for specific applications by retrieving from a library of abstract global search theories a theory that applies to the datatypes mentioned in the problem specification.
Reference: [73] <author> M. Lowry and R. Duran. </author> <title> Knowledge-based Software Engineering. </title> <editor> In P.R. Cohen A. Barr and E. A. Feigenbaum, editors, </editor> <booktitle> The Handbook of Artificial Intelligence, Volume IV, chapter 20. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: the context of local search we will mention research on simulated annealing algorithms, and we will briefly list uses of iterative repair in standard numerical analysis techniques. 7.1 Automated Software Design Good comprehensive overviews of the current state of research on automated software design can be found in [82] and <ref> [73] </ref>. All efforts in automating software design share the goal of converting a problem description in some higher-level language into an operational problem solver or, program, which solves the specified problem efficiently. Yet, the approaches that have been taken in pursing this goal are quite varied.
Reference: [74] <author> M.R. Lowry. </author> <title> Automating the Design of Local Search Algorithms. In M.R. </title> <editor> Lowry and R.D. McCartney, editors, </editor> <booktitle> Automating Software Design. </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: Produce the specialized algorithm P rob Solv. [4] OPTIMIZE the specialized algorithm P rob Solv. (optional) 15 OUTPUT: algorithm P rob Solv Classification-based approaches have been used successfully in algorithm design before. Two examples are the systems KIDS by Smith [127] and STRATA by Lowry <ref> [74] </ref>. These systems assist the human user in the design of various types of search algorithms (e.g., local search, global search, divide-and-conquer, etc.). Problem specifications are matched up with generic algorithms schemas, and various pieces of information that can help improve algorithm performance are inferred by deductive reasoning components. <p> Each step of classification-based design has associated with it some predetermined semantics. When the design history is stored in parallel with the design process, the potentially necessary future reengineering of the resulting problem solver is greatly facilitated. The above mentioned systems KIDS and STRATA <ref> [127, 74] </ref> realize a combination of classification-based design and deductive reasoning from first principle. The combinatoric complexity of the deductive reasoning processes made it necessary for both systems to interact with user in order to obtain guidance in searching the vast space of logical antecedents and consequents. <p> Here the type of a problem, not necessarily its very specifics, determine the ultimate form of the synthesized software. Examples of software design systems that are interactive and make use of problem class knowledge are the systems KIDS by Smith [121, 122, 127] and STRATA by Lowry <ref> [72, 74] </ref>. KIDS constructs search algorithms, e.g, global search algorithms, for specific applications by retrieving from a library of abstract global search theories a theory that applies to the datatypes mentioned in the problem specification.
Reference: [75] <author> M.R. Lowry and R.D. </author> <title> McCartney. Preface of Section Two: Knowledge-Based Specification Acquisition. In M.R. </title> <editor> Lowry and R.D. McCartney, editors, </editor> <booktitle> Automating Software Design. </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: Even those that have sufficient training have difficulty ensuring that their specifications are internally consistent, complete, and externally consistent with the domains constraints." [cited from <ref> [75] </ref>] Consequently, more work is passed on to the design system. Unfortunately, some typical work (such as consistency verifications) can be tasks that are semi-decidable. 1 As a result, in the worst case, running a design system that needs to solve semi-decidable problems may be unacceptably expensive.
Reference: [76] <author> E. Kant, F. Daube, W. MacGregor, and J. Wald. </author> <title> Scientific Programming by automated synthesis. In M.R. </title> <editor> Lowry and R.D. McCartney, editors, </editor> <booktitle> Automating Software Design. </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: the coarser-grained schemas can be viewed as the result of precompiling and caching the effects of a series of transformations (schema as a "macro-transformation"). 7.1.2 Domain/problem specific systems On the (literally) opposite side of the just discussed approaches are systems like NIX by Barstow [4], SINAPSE by Kant and coworkers <ref> [76] </ref>, or ELF by Setliff [118]. These are software synthesis systems which have been designed to synthesize code that solves specific types of problems in specific domains. Domain and problem specificity has the advantage that special features of the domains can be exploited to design efficient and effective programs.
Reference: [77] <author> A.K. Mackworth. </author> <title> Consistency in Networks of Relations. </title> <journal> Artificial Intelligence, </journal> <volume> 8(1), </volume> <year> 1977. </year>
Reference-contexts: The most basic problem solving method for CSPs is generate-and-test with backtracking. In order to alleviate some, or, ideally all, of the "thrashing" that backtracking schemas typically suffer from, various constraint propagation techniques applicable at either compile-time, or run-time have been suggested. Mackworth <ref> [77] </ref> has presented (compile-time) procedures that preprocess the constraint-graph of a CSP, thereby establishing node- and arc-consistency for individual variables ("nodes") and constrained pairs of variables ("pairs of nodes connected by arcs"). The notion of arc-consistency can be extended from two constrained variables to k (k &gt; 2) variables.
Reference: [78] <author> Z. Manna and R. Waldinger. </author> <title> Knowledge and Reasoning in Program Synthesis. </title> <journal> Artificial Intelligence, </journal> <volume> 6(2), </volume> <year> 1975. </year>
Reference-contexts: By virtue of our abductive polarity analysis, this subset is a priori known to contain the overall best operator). The method of abductive polarity analysis has been developed as part of this dissertation research. It is modeled after a known technique called 'polarity analysis' <ref> [78, 124] </ref>) over which is has the advantage of handling functions that are nonmonotonic (details in Chapter 4). 20 1.2.2 The key ideas of this dissertation The work of this dissertation is based on roughly four ideas and insights. We briefly summarize each one. Problem classes RAP. <p> Next we will discuss the systems in each group in greater detail. 7.1.1 Domain-independent and interactive software design systems Among early attempts at domain-independent software design are the constructive theorem proving systems by Green [44] and Manna and Waldinger <ref> [78] </ref>. Such systems derive programs from formal specifications constructively from resolution theorem proofs. Given a problem specification in first order logic (FOL), the existence of a solution is first proved formally. From such proofs, the terms to compute correct solutions are extracted and composed into a program.
Reference: [79] <author> Z. Manna and R. Waldinger. </author> <title> A deductive approach to program synthesis. </title> <booktitle> In ijcai-6, </booktitle> <pages> pages 542-551, </pages> <year> 1979. </year>
Reference: [80] <author> Z. Manna and R. Waldinger. </author> <title> Special Relations in Automated Deduction. </title> <journal> JACM, </journal> <volume> 33(1), </volume> <month> January </month> <year> 1986. </year>
Reference-contexts: We will now present this method in detail. 4.3.1 Polarity analysis Polarity analysis is a technique for tracing qualitative changes in function values (e.g., increase or decrease) back to qualitative changes in the function's arguments (see <ref> [80, 126] </ref>). <p> In fact, it has been used for purposes as varied as the construction of specification morphisms [124], speed up of search in resolution theorem proving <ref> [80] </ref>, and qualitative algebraic reasoning for the design and verification of physical systems [152]. Smith employs polarity analysis in the automated design of algorithms. <p> Algorithm design is viewed as the construction of a specification morphism, that is, a translation of a formal problem specification into a corresponding specification in some algorithm or program language. Polarity analysis ensures that the translation adheres to mono-tonicity laws and preserves correctness. Manna and Waldinger <ref> [80] </ref> employ polarity analysis to speed up search in resolution theorem proving. By replacing sentences with positive polarity with false and sentences with negative polarity with true, the combined sentence becomes more false, whereby speedier progress is made towards the resolution goal false.
Reference: [81] <author> R. </author> <title> McCartney. Synthesizing algorithms with performance constraints. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 149-154. </pages> <publisher> AAAI, </publisher> <month> August </month> <year> 1987. </year>
Reference: [82] <author> R. </author> <title> McCartney. Knowledge-Based Software Engineering: Where We Are and Where W Are Going. In Automating Software Design, chapter 1. </title> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: In the context of local search we will mention research on simulated annealing algorithms, and we will briefly list uses of iterative repair in standard numerical analysis techniques. 7.1 Automated Software Design Good comprehensive overviews of the current state of research on automated software design can be found in <ref> [82] </ref> and [73]. All efforts in automating software design share the goal of converting a problem description in some higher-level language into an operational problem solver or, program, which solves the specified problem efficiently. Yet, the approaches that have been taken in pursing this goal are quite varied.
Reference: [83] <author> S. Minton. </author> <title> Learning effictive search control knowledge: an explanation-based approach. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1988. </year>
Reference-contexts: On the other hand, the newly learned expression is also a specialization of the original definition of concept P. This view has been expressed by researchers in explanation-based specialization (EBS) such as Minton <ref> [86, 83] </ref> or Zweben [25]. (A slight technical distinction between EBG and EBS techniques does exist: EBS works strictly top-down from the general concept, driven by the specific training example, and does not necessitate the additional bottom-up processing of the proof-tree in EBG.) In principle, given the appropriate ingredients, any general <p> MULTI-TAC by Minton 2 We adopt Horn clause notation; ',' indicates conjunction, ';' indicates disjunction. 223 [84] and, before that, PRODIGY <ref> [83] </ref> have successfully applied EBL to general rules of search control such as, for example, "operator is the sole alternative" [83], or "in stantiate the most constraining variable first" [84]. The outputs of EBL were effective search control rules specialized to the specific domain and problem at hand. <p> MULTI-TAC by Minton 2 We adopt Horn clause notation; ',' indicates conjunction, ';' indicates disjunction. 223 [84] and, before that, PRODIGY <ref> [83] </ref> have successfully applied EBL to general rules of search control such as, for example, "operator is the sole alternative" [83], or "in stantiate the most constraining variable first" [84]. The outputs of EBL were effective search control rules specialized to the specific domain and problem at hand.
Reference: [84] <author> S. Minton. </author> <title> An Analytic Learining System for Specializing Heuristics. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chambery, </address> <year> 1993. </year>
Reference-contexts: MULTI-TAC by Minton 2 We adopt Horn clause notation; ',' indicates conjunction, ';' indicates disjunction. 223 <ref> [84] </ref> and, before that, PRODIGY [83] have successfully applied EBL to general rules of search control such as, for example, "operator is the sole alternative" [83], or "in stantiate the most constraining variable first" [84]. <p> MULTI-TAC by Minton 2 We adopt Horn clause notation; ',' indicates conjunction, ';' indicates disjunction. 223 <ref> [84] </ref> and, before that, PRODIGY [83] have successfully applied EBL to general rules of search control such as, for example, "operator is the sole alternative" [83], or "in stantiate the most constraining variable first" [84]. The outputs of EBL were effective search control rules specialized to the specific domain and problem at hand. In addition to automatically deriving such positive heuristics (heuristics that guide the search along promising paths in the search space) EBL is equally apt to derive negative heuristics.
Reference: [85] <author> S. Minton. </author> <title> Integrating Heuristics for Constraint Satisfaction Problems: A Case Study. </title> <booktitle> In Proceedings of AAAI-93, </booktitle> <address> Washington DC, </address> <month> July </month> <year> 1993. </year>
Reference: [86] <author> S. Minton and J. G. Carbonell. </author> <title> Strategies for learning search control rules: an explanation-based approach. </title> <booktitle> In Proceedings IJCAI-87, </booktitle> <pages> pages 228-235, </pages> <address> Milan, Italy, </address> <month> August </month> <year> 1987. </year> <month> 334 </month>
Reference-contexts: The 221 first type of approach extracts heuristic measures of the distance to a solution from relaxed models of the original search space [106, 110]. The second type of approach derives heuristics through explaining and specializing general search control rules for the specific domain and problem instance (e.g. <ref> [142, 59, 86, 9] </ref>). Deriving heuristics through model relaxation Pearl [104, 106] has shown how admissible search heuristics can be constructed systematically through relaxation of the domain model. Given a state-space search model, the model can be relaxed by deleting constraints from the list of operator preconditions. <p> On the other hand, the newly learned expression is also a specialization of the original definition of concept P. This view has been expressed by researchers in explanation-based specialization (EBS) such as Minton <ref> [86, 83] </ref> or Zweben [25]. (A slight technical distinction between EBG and EBS techniques does exist: EBS works strictly top-down from the general concept, driven by the specific training example, and does not necessitate the additional bottom-up processing of the proof-tree in EBG.) In principle, given the appropriate ingredients, any general
Reference: [87] <author> S. Mittal and A. Araya. </author> <title> A Knowledge-Based Framework for Design. </title> <booktitle> In Proceedings of AAAI-86, </booktitle> <year> 1986. </year>
Reference: [88] <author> S. Mittal and F. Frayman. </author> <title> Making Partial Choices in Constraint Reasoning Problems. </title> <booktitle> In Proceedings of AAAI-87, </booktitle> <year> 1987. </year>
Reference: [89] <author> S. Mohan and C. Tong. </author> <title> Automatic Construction of a Hierarchical Generate-and-Test Algorithm. </title> <booktitle> In Proceedings of the Sixth International Machine Learning Workshop, </booktitle> <address> Ithaca, NY, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: MENDER is one of the subsystems in KBSDE; MENDER is designed to specifically construct patching problem solvers for global resource assignment problems. Other subsystems of KBSDE are RICK [14, 15], and HiT <ref> [89] </ref>.
Reference: [90] <author> S. Mohan and C. Tong. </author> <title> Automatic construction of hierarchical generate-and-test algorithms. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <month> June </month> <year> 1989. </year>
Reference: [91] <author> P. Morris. </author> <title> On the Density of Solutions in Equilibrium Points for the Queens Problem. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Researchers generally agree that the ability to make lateral moves (preferred over down-hill moves) is crucial in escaping the local maxima which are the inherent drawback of greedy local search (e.g., see <ref> [109, 69, 54, 91] </ref>). The opinions differ on the importance of a starting state that is "close" to some solution. Minton [109], Musick and Russell [98] are among those who are in favor of good starting states. Our own experimentations with MENDER's compiled patchers support this view.
Reference: [92] <author> P. Morris. </author> <title> The Breakout Method For Escaping From Local Minima. </title> <booktitle> In Proceedings of AAAI-93, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1993. </year>
Reference: [93] <author> J. Mostow. </author> <title> Learning by being told: machine transformation of advice into a heuristic search procedure. </title> <booktitle> In Machine Learning, </booktitle> <pages> pages 367-403. </pages> <address> Palo Alto, CA: </address> <publisher> Tioga, </publisher> <year> 1983. </year>
Reference-contexts: rm) + In English, this finding states that increased coverage may also be achieved by increasing parameters W (room width). "Monotonically necessary" and "monotonically sufficient" conditions We find an interesting close relationship between the recommended argument changes that are computed by RAP-PA, and conditions on problem solving paths which in <ref> [93] </ref> have been given the names "monotonically necessary conditions" and "monotonically sufficient conditions. We review these notions briefly. Consider a path s to a solution, and all partial paths s 0 from the starting state of s to some intermediate state in s.
Reference: [94] <author> J. Mostow. </author> <title> A preliminary report on DIOGENES: Progress towards Semiautomatic Design of Specialized Heuristic Search Algorithms. </title> <booktitle> In Proceedings of the Workshop on Automated Software Design, </booktitle> <address> St. Paul, MN., </address> <month> August </month> <year> 1988. </year> <note> AAAI. </note>
Reference-contexts: Due to the combinatorically explosive search space typical of resolution theorem proving, the burden of making the control decisions is placed on the user. The system's task is to carry out the requested resolution step. DIOGENES developed by Mostow <ref> [94] </ref> is an interactive system that applies a series of speedup transformations to an initial problem specification and, thus, converts it into an efficient algorithm. At each decision point, the system computes a catalogue of applicable transformation rules.
Reference: [95] <author> J. Mostow. </author> <title> Design by derivational analogy: Issues in the automated replay of design plans. </title> <journal> Artificial Intelligence, </journal> <volume> 40(1-3):119-184, </volume> <month> September </month> <year> 1989. </year> <editor> In J. Car-bonell (editor), </editor> <title> Special Volume on Machine Learning, </title> <note> reprinted by MIT Press as Machine Learning: Paradigms and Methods, </note> <year> 1990. </year>
Reference: [96] <author> J. Mostow and N. Bhatnagar. </author> <title> Failsafe A Floor Planner that Uses EBG to Learn from its Failures. </title> <booktitle> In Proceedings of IJCAI-87, </booktitle> <year> 1987. </year>
Reference-contexts: Negative heuristics are search control rules that help prune the search space of subspaces that are void of solution states. Such heuristics have been automatically derived by Minton's PRODIGY, and the systems FAILSAFE and FS2, both developed by Bhatnagar <ref> [96, 9] </ref>. MENDER: deriving heuristics by schema instantiation The evaluation functions compiled by MENDER fall into the category of positive heuristics since they are employed to guide the search along the improving paths in the search space.
Reference: [97] <author> J. Mostow and K. Voigt. </author> <title> Explicit integration of multiple goals in heuristic algorithm design. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI87), </booktitle> <volume> volume 2, </volume> <pages> pages 1090-1096, </pages> <address> Milan, Italy, August 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [98] <author> R. Musick and S. Russell. </author> <title> How Long Will It Take? In Proceedings of AAAI-92, </title> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The opinions differ on the importance of a starting state that is "close" to some solution. Minton [109], Musick and Russell <ref> [98] </ref> are among those who are in favor of good starting states. Our own experimentations with MENDER's compiled patchers support this view. In a very recent paper, Selman and Kautz [114] report on two improvements to the 230 GSAT local search problem solver.
Reference: [99] <author> B. A. </author> <title> Nadel. Representation Selection for Constraint Satisfaction: A Case Study Using n-Queens. </title> <journal> IEEE Expert, </journal> <month> June </month> <year> 1990. </year>
Reference: [100] <author> B.A. </author> <title> Nadel. Tree Search and Arc Consistency in Constraint Satisfaction Algorithms. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer, </publisher> <year> 1988. </year>
Reference-contexts: Alternatively to processing the underlying constraint-graph of a CSP, other techniques have been suggested that do not alter the search space, but strive to search the original space more efficiently. Such techniques are look-ahead techniques <ref> [100] </ref>, dependency-directed backtracking schemes [131], and heuristics for variable and value ordering (e.g., [109, 28]). Nadel has examined backtracking search algorithms for solving CSPs with varying levels of look-ahead.
Reference: [101] <author> A. Newell. </author> <title> The Knowledge Level. </title> <journal> Artificial Intelligence, </journal> <volume> 18(1), </volume> <year> 1982. </year> <month> 335 </month>
Reference-contexts: Thus, the general task of automating algorithm design consists of constructing methods and systems of the type depicted in In this dissertation, we utilize Newell's distinction of of three levels of program design <ref> [101] </ref>. Program design can be viewed as mapping a declarative problem specifi cation, which is said to reside on the knowledge level, to a configuration of functional components at the next level, called the function level.
Reference: [102] <author> N. Nilsson. </author> <booktitle> Principles of artificial intelligence (second edition). </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1984. </year>
Reference: [103] <author> D.S. Johnson, C.H. Papadimitriou, and M. Yannakakis. </author> <title> How Easy is Local Search. </title> <booktitle> In 26 th Symp. Found. Comp. </booktitle> <address> Sc., </address> <year> 1985. </year>
Reference: [104] <author> J. Pearl. </author> <title> On the discovery and generation of certain heuristics. </title> <journal> The AI Magazine, </journal> <volume> IV(1):23-33, </volume> <month> Winter/Spring </month> <year> 1983. </year>
Reference-contexts: The second type of approach derives heuristics through explaining and specializing general search control rules for the specific domain and problem instance (e.g. [142, 59, 86, 9]). Deriving heuristics through model relaxation Pearl <ref> [104, 106] </ref> has shown how admissible search heuristics can be constructed systematically through relaxation of the domain model. Given a state-space search model, the model can be relaxed by deleting constraints from the list of operator preconditions.
Reference: [105] <author> J. Pearl. </author> <title> Basic Heuristic Search Procedures. In Heuristics, chapter 2. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: The effectiveness of heuristics is proportional to the accuracy by which they measure or predict the "landscape" of the underlying search space (see, e.g., <ref> [107, 105] </ref>). Many automatically synthesized programs fall into the category of search algorithms (see, for example KIDS, STRATA, or RICK above). Therefore, automating the derivation of search heuristics is of crucial relevance to the successful design. Among existing methods to automatically derive heuristics we distinguish two types of approaches.
Reference: [106] <author> J. Pearl. </author> <title> Formal Properties of Heuristic Methods. In Heuristics, chapter 3. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: Among existing methods to automatically derive heuristics we distinguish two types of approaches. The 221 first type of approach extracts heuristic measures of the distance to a solution from relaxed models of the original search space <ref> [106, 110] </ref>. The second type of approach derives heuristics through explaining and specializing general search control rules for the specific domain and problem instance (e.g. [142, 59, 86, 9]). <p> The second type of approach derives heuristics through explaining and specializing general search control rules for the specific domain and problem instance (e.g. [142, 59, 86, 9]). Deriving heuristics through model relaxation Pearl <ref> [104, 106] </ref> has shown how admissible search heuristics can be constructed systematically through relaxation of the domain model. Given a state-space search model, the model can be relaxed by deleting constraints from the list of operator preconditions.
Reference: [107] <author> J. Pearl. </author> <title> Heuristics and Problem Representations. In Heuristics, chapter 1. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: The effectiveness of heuristics is proportional to the accuracy by which they measure or predict the "landscape" of the underlying search space (see, e.g., <ref> [107, 105] </ref>). Many automatically synthesized programs fall into the category of search algorithms (see, for example KIDS, STRATA, or RICK above). Therefore, automating the derivation of search heuristics is of crucial relevance to the successful design. Among existing methods to automatically derive heuristics we distinguish two types of approaches.
Reference: [108] <author> J. Pearl. </author> <title> Heuristics Viewed as Information Provided by Simplified Models. In Heuristics, chapter 4. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference: [109] <author> S. Minton, M.D. Johnston, A.B. Philips, and P. Laird. </author> <title> Minimizing Conflicts: A Heuristic Repair Method for Constraint Satisfaction and Scheduling Problems. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 58(1-3), </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Evidence in favor of this argument can be found in the literature. Recently, large-scale instances of n-queens and scheduling problems have been solved with surprising efficiency by heuristic repair problem solvers (e.g., <ref> [109] </ref>). In addition, simulated annealing which is a type of probabilistic heuristic repair method has been successful in solving numerous global problems in domains such as, for example, VLSI circuit circuit placement and routing, the traveling salesman problem, the design of "good" binary codes (e.g., [40, 119]). <p> Alternatively to processing the underlying constraint-graph of a CSP, other techniques have been suggested that do not alter the search space, but strive to search the original space more efficiently. Such techniques are look-ahead techniques [100], dependency-directed backtracking schemes [131], and heuristics for variable and value ordering (e.g., <ref> [109, 28] </ref>). Nadel has examined backtracking search algorithms for solving CSPs with varying levels of look-ahead. Look-ahead is a mechanism which to determine whether or not consistent value assignments to yet unassigned variables still exist given the set of past and current assignments. <p> In addition, the focus of dependency-backtracking will remain narrow and, thus, useful only as long as the number of interacting variables is significantly less than the number of all variables in the CSP. In the literature (e.g., <ref> [34, 109] </ref>), one finds a variety of heuristics for the run-time or dering of variables and values. <p> Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem <ref> [109, 130] </ref>, (2) propositional satisfiability (SAT) [46, 69, 115, 38], (3) scheduling [109], and (4) graph-coloring [115, 38]. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing. <p> Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem [109, 130], (2) propositional satisfiability (SAT) [46, 69, 115, 38], (3) scheduling <ref> [109] </ref>, and (4) graph-coloring [115, 38]. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing. <p> Researchers generally agree that the ability to make lateral moves (preferred over down-hill moves) is crucial in escaping the local maxima which are the inherent drawback of greedy local search (e.g., see <ref> [109, 69, 54, 91] </ref>). The opinions differ on the importance of a starting state that is "close" to some solution. Minton [109], Musick and Russell [98] are among those who are in favor of good starting states. Our own experimentations with MENDER's compiled patchers support this view. <p> The opinions differ on the importance of a starting state that is "close" to some solution. Minton <ref> [109] </ref>, Musick and Russell [98] are among those who are in favor of good starting states. Our own experimentations with MENDER's compiled patchers support this view. In a very recent paper, Selman and Kautz [114] report on two improvements to the 230 GSAT local search problem solver. <p> Furthermore, the performance data collected for our automatically constructed patchers agree with the observations of astonishingly 239 efficient performances of manually coded patchers for difficult, large-scale instances of problems in the domains of n-queens, scheduling, and satisfiability (SAT) (e.g., see <ref> [109, 130, 69] </ref>). 8.3.2 Explaining the benefits of patching We can identify roughly three features of MENDER-constructed patchers that can account for their good performances: 1. Patchers search spaces of complete states, and they do so in a constraint-oriented fashion. 2.
Reference: [110] <author> A.E. </author> <title> Prieditis. Discovering Effective Admissible Heuristics by Abstraction and Speedup: A Transformational Approach. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1990. </year>
Reference-contexts: Among existing methods to automatically derive heuristics we distinguish two types of approaches. The 221 first type of approach extracts heuristic measures of the distance to a solution from relaxed models of the original search space <ref> [106, 110] </ref>. The second type of approach derives heuristics through explaining and specializing general search control rules for the specific domain and problem instance (e.g. [142, 59, 86, 9]). <p> This count is then used as an estimate of the lower bound 1 of the effort needed to reach a solution state in the original space. With the system ABSOLVER, Prieditis <ref> [110] </ref> has automated and extended the model relaxation approach to deriving search heuristics for state-space search. Note, in successful state space search the emphasis is on achieving short, ideally optimal, paths to a solution state. This is not the typical objective in automated software design.
Reference: [111] <author> C. Rich and R.C. Waters. </author> <title> The Programmer's Apprentice: A Research Overview. </title> <journal> IEEE Computer, </journal> <volume> 21(11), </volume> <year> 1988. </year>
Reference-contexts: Yet, since transformation rules tend to capture knowledge at a grain size that is coarser than the first principle knowledge typically expressed in FOL, DIOGENES is somewhat less general than constructive theorem approaches. The Programmer's Apprentice by Rich and Waters <ref> [111, 150] </ref>, predates DIOGENES. This system includes KBEmacs (Knowledge-Based Editor in Emacs), which serves as an assistant to the human programmer. Essentially, the system provides the user with a libary of program schemas ("cliches") which capture generic computings tasks, and assists in the specialization of the schemas for specific tasks.
Reference: [112] <author> M.D. Huang, F. Romeo, and A. Sangiovanni-Vincentelli. </author> <title> An Efficient General Cooling Schedule for Simulated Annealing. </title> <booktitle> In </booktitle> ???, ??? 
Reference-contexts: Optimization problems in resource assignment are, for instance: * Minimize the sum of all consumed resources. * Maximize the participation of consumers in the consumption of the resources. * Minimize (or, maximize) the sharing of resources by multiple consumers. In the recent literature (e.g., <ref> [40, 154, 119, 113, 153, 144, 11, 112] </ref>), we find reports on good performances of a simulated annealing problem solvers for a wide range of practical optimization problems (e.g., gate matrix layout, channel routing, cell placement, design of "good" codes, traveling salesman).
Reference: [113] <author> C. Sechen and A. Sangiovanni-Vincentelli. </author> <title> The TimberWolf Placement and Routing Package. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> SC-20(2), </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: Simulated annealing has also performed well on the classical traveling salesman problem. We add a by no means exhaustive list of applications of simulated annealing in other areas: gate-matrix layout [68], design of "good codes" [119], logic minization [66], and floorplanning [153]. For more examples see also <ref> [113, 144] </ref>. In retrospect, it is interesting to note, that MENDER would have been able to automatically compile local search components that conduct simulated annealing, had we given it the knowledge of an "simulated annealing shell". <p> Optimization problems in resource assignment are, for instance: * Minimize the sum of all consumed resources. * Maximize the participation of consumers in the consumption of the resources. * Minimize (or, maximize) the sharing of resources by multiple consumers. In the recent literature (e.g., <ref> [40, 154, 119, 113, 153, 144, 11, 112] </ref>), we find reports on good performances of a simulated annealing problem solvers for a wide range of practical optimization problems (e.g., gate matrix layout, channel routing, cell placement, design of "good" codes, traveling salesman).
Reference: [114] <author> B. Selman and H.A. Kautz. </author> <title> Domain-Independent Extensions to GSAT: Solving Large Structured Satisfiability Problems. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chambery, </address> <year> 1993. </year>
Reference-contexts: Minton [109], Musick and Russell [98] are among those who are in favor of good starting states. Our own experimentations with MENDER's compiled patchers support this view. In a very recent paper, Selman and Kautz <ref> [114] </ref> report on two improvements to the 230 GSAT local search problem solver. One improvement is somewhat particular to SAT problems, or problems that are structurally similar. It consists in dynamically increas ing the weights of clauses that have remained false after a repair has been applied.
Reference: [115] <author> B. Selman and H.A. Kautz. </author> <title> An Empirical Study of Greedy Local Search for Satisfiability Testing. </title> <booktitle> In Proceedings of AAAI-93, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem [109, 130], (2) propositional satisfiability (SAT) <ref> [46, 69, 115, 38] </ref>, (3) scheduling [109], and (4) graph-coloring [115, 38]. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing. <p> Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem [109, 130], (2) propositional satisfiability (SAT) [46, 69, 115, 38], (3) scheduling [109], and (4) graph-coloring <ref> [115, 38] </ref>. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing.
Reference: [116] <author> D. Mitchell, B. Selman, and H. Levesque. </author> <title> Hard and Easy Distributions of SAT Problems. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem [109, 130], (2) propositional satisfiability (SAT) [46, 69, 115, 38], (3) scheduling [109], and (4) graph-coloring [115, 38]. Mitchell et.al. <ref> [116] </ref> have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing.
Reference: [117] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference: [118] <author> D. Setliff. </author> <title> On the Automatic Selection of Data Structure and Algorithms. In M.R. </title> <editor> Lowry and R.D. McCartney, editors, </editor> <booktitle> Automating Software Design. </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press, </publisher> <year> 1991. </year> <month> 336 </month>
Reference-contexts: viewed as the result of precompiling and caching the effects of a series of transformations (schema as a "macro-transformation"). 7.1.2 Domain/problem specific systems On the (literally) opposite side of the just discussed approaches are systems like NIX by Barstow [4], SINAPSE by Kant and coworkers [76], or ELF by Setliff <ref> [118] </ref>. These are software synthesis systems which have been designed to synthesize code that solves specific types of problems in specific domains. Domain and problem specificity has the advantage that special features of the domains can be exploited to design efficient and effective programs.
Reference: [119] <author> A.A. El Gamal, L.A. Hemachandra, I. Shperling, and V.K. Wei. </author> <title> Using Simulated Annealing to Design Good Codes. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-33(1), </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: In addition, simulated annealing which is a type of probabilistic heuristic repair method has been successful in solving numerous global problems in domains such as, for example, VLSI circuit circuit placement and routing, the traveling salesman problem, the design of "good" binary codes (e.g., <ref> [40, 119] </ref>). <p> Simulated annealing has also performed well on the classical traveling salesman problem. We add a by no means exhaustive list of applications of simulated annealing in other areas: gate-matrix layout [68], design of "good codes" <ref> [119] </ref>, logic minization [66], and floorplanning [153]. For more examples see also [113, 144]. In retrospect, it is interesting to note, that MENDER would have been able to automatically compile local search components that conduct simulated annealing, had we given it the knowledge of an "simulated annealing shell". <p> Optimization problems in resource assignment are, for instance: * Minimize the sum of all consumed resources. * Maximize the participation of consumers in the consumption of the resources. * Minimize (or, maximize) the sharing of resources by multiple consumers. In the recent literature (e.g., <ref> [40, 154, 119, 113, 153, 144, 11, 112] </ref>), we find reports on good performances of a simulated annealing problem solvers for a wide range of practical optimization problems (e.g., gate matrix layout, channel routing, cell placement, design of "good" codes, traveling salesman).
Reference: [120] <author> D. Smith. </author> <title> Derived preconditions and their use in program synthesis. </title> <editor> In D. Love-land, editor, </editor> <booktitle> Proceedings of the Sixth Conference on Automated Deduction, </booktitle> <pages> pages 172-193. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year> <note> Lecture Notes in Computer Science 138. </note>
Reference: [121] <author> D. Smith. </author> <title> Top-down synthesis of divide-and-conquer algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 27(1) </volume> <pages> 43-96, </pages> <month> September </month> <year> 1985. </year> <note> Reprinted in Readings in Artificial Intelligence and Software Engineering, </note> <editor> C. Rich and R. Waters, Eds., </editor> <address> Los Altos, CA, </address> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Here the type of a problem, not necessarily its very specifics, determine the ultimate form of the synthesized software. Examples of software design systems that are interactive and make use of problem class knowledge are the systems KIDS by Smith <ref> [121, 122, 127] </ref> and STRATA by Lowry [72, 74]. KIDS constructs search algorithms, e.g, global search algorithms, for specific applications by retrieving from a library of abstract global search theories a theory that applies to the datatypes mentioned in the problem specification.
Reference: [122] <author> D. Smith. KIDS: </author> <title> A knowledge-based software development system. </title> <booktitle> In Proceedings of the Workshop on Automated Software Design, </booktitle> <address> St. Paul, MN., </address> <month> August </month> <year> 1988. </year> <note> AAAI. </note>
Reference-contexts: Here the type of a problem, not necessarily its very specifics, determine the ultimate form of the synthesized software. Examples of software design systems that are interactive and make use of problem class knowledge are the systems KIDS by Smith <ref> [121, 122, 127] </ref> and STRATA by Lowry [72, 74]. KIDS constructs search algorithms, e.g, global search algorithms, for specific applications by retrieving from a library of abstract global search theories a theory that applies to the datatypes mentioned in the problem specification.
Reference: [123] <author> D. R. Smith. </author> <title> Structure and design of global search algorithms. </title> <type> Technical report kes.u.87.12, </type> <institution> Kestrel Institute, 1801 Page Mill Road, </institution> <address> Palo Alto, CA 94304, </address> <month> July </month> <year> 1988. </year>
Reference: [124] <author> D. R. Smith. </author> <title> Constructing Specification Morphisms. </title> <type> Technical Report KES.U.92.1, </type> <institution> Kestrel Institute, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: By virtue of our abductive polarity analysis, this subset is a priori known to contain the overall best operator). The method of abductive polarity analysis has been developed as part of this dissertation research. It is modeled after a known technique called 'polarity analysis' <ref> [78, 124] </ref>) over which is has the advantage of handling functions that are nonmonotonic (details in Chapter 4). 20 1.2.2 The key ideas of this dissertation The work of this dissertation is based on roughly four ideas and insights. We briefly summarize each one. Problem classes RAP. <p> In fact, it has been used for purposes as varied as the construction of specification morphisms <ref> [124] </ref>, speed up of search in resolution theorem proving [80], and qualitative algebraic reasoning for the design and verification of physical systems [152]. Smith employs polarity analysis in the automated design of algorithms.
Reference: [125] <author> Donald Smith. </author> <title> On a topology-enumerating floorplanner. </title> <type> Personal communication. </type>
Reference-contexts: Polarity analysis rules associated with all primitive structures, functions and pred icates. 5. Knowledge of "context constraints", and possibly the knowledge of constraint propagation routines that ensure their a priori satisfaction by all candidate prior 1 An example is the floorplanner suggested by Smith <ref> [125] </ref>. The design of this problem solver exploits various topological and geometric features of rectangular structures.
Reference: [126] <author> Douglas R. Smith. </author> <title> Constructing specification morphisms. </title> <journal> Journal of Symbolic Computation, Special Issue on Automatic Programming, </journal> <volume> 15(5-6):571-606, </volume> <month> May-June </month> <year> 1993. </year>
Reference-contexts: We will now present this method in detail. 4.3.1 Polarity analysis Polarity analysis is a technique for tracing qualitative changes in function values (e.g., increase or decrease) back to qualitative changes in the function's arguments (see <ref> [80, 126] </ref>).
Reference: [127] <author> D.R. Smith. </author> <title> KIDS A Knowledge-Based Software Development System. In M.R. </title> <editor> Lowry and R.D. McCartney, editors, </editor> <booktitle> Automating Software Design. </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: Produce the specialized algorithm P rob Solv. [4] OPTIMIZE the specialized algorithm P rob Solv. (optional) 15 OUTPUT: algorithm P rob Solv Classification-based approaches have been used successfully in algorithm design before. Two examples are the systems KIDS by Smith <ref> [127] </ref> and STRATA by Lowry [74]. These systems assist the human user in the design of various types of search algorithms (e.g., local search, global search, divide-and-conquer, etc.). <p> Each step of classification-based design has associated with it some predetermined semantics. When the design history is stored in parallel with the design process, the potentially necessary future reengineering of the resulting problem solver is greatly facilitated. The above mentioned systems KIDS and STRATA <ref> [127, 74] </ref> realize a combination of classification-based design and deductive reasoning from first principle. The combinatoric complexity of the deductive reasoning processes made it necessary for both systems to interact with user in order to obtain guidance in searching the vast space of logical antecedents and consequents. <p> Here the type of a problem, not necessarily its very specifics, determine the ultimate form of the synthesized software. Examples of software design systems that are interactive and make use of problem class knowledge are the systems KIDS by Smith <ref> [121, 122, 127] </ref> and STRATA by Lowry [72, 74]. KIDS constructs search algorithms, e.g, global search algorithms, for specific applications by retrieving from a library of abstract global search theories a theory that applies to the datatypes mentioned in the problem specification.
Reference: [128] <author> P. Smolensky. </author> <title> Information Processing in Dynamical Systems: Foundations of Harmony Theory. </title> <editor> In J.L. McClelland D.E. Rumelhart and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing Volume 1, chapter 6. </booktitle> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference: [129] <author> E. Sontag. </author> <title> On designing cooling functions for simulated annealing. </title> <type> Personal communication. </type>
Reference-contexts: The cooling function critically determines the problem solver's ability to escape from local maxima and converge on the optimum. Yet, in practice, the choice of a cooling function is not well-understood <ref> [129] </ref>.
Reference: [130] <author> R. Sosic and J. Gu. </author> <title> A Polynomial-Time Algorithm for the N-Queens Problem. </title> <journal> SIGART Bulletin, </journal> <volume> 1(3), </volume> <year> 1990. </year>
Reference-contexts: Within the past two years, very impressive performances of local search problem solvers have been reported for large-scale problem instances in the domains of (1) the n-queens problem <ref> [109, 130] </ref>, (2) propositional satisfiability (SAT) [46, 69, 115, 38], (3) scheduling [109], and (4) graph-coloring [115, 38]. Mitchell et.al. [116] have verified that the local search method GSAT is capable of efficiently solving instances of SAT that are known to be among the hardest problems for satisfiability testing. <p> Furthermore, the performance data collected for our automatically constructed patchers agree with the observations of astonishingly 239 efficient performances of manually coded patchers for difficult, large-scale instances of problems in the domains of n-queens, scheduling, and satisfiability (SAT) (e.g., see <ref> [109, 130, 69] </ref>). 8.3.2 Explaining the benefits of patching We can identify roughly three features of MENDER-constructed patchers that can account for their good performances: 1. Patchers search spaces of complete states, and they do so in a constraint-oriented fashion. 2.
Reference: [131] <author> R.M. Stallman and G.J. Sussman. </author> <title> Forward Reasoning and Dependency-Directed Backtracking in a System for Computer-Aided Circuit Analysis. </title> <journal> Artificial Intelligence, </journal> <volume> 9, </volume> <year> 1977. </year> <month> 337 </month>
Reference-contexts: Alternatively to processing the underlying constraint-graph of a CSP, other techniques have been suggested that do not alter the search space, but strive to search the original space more efficiently. Such techniques are look-ahead techniques [100], dependency-directed backtracking schemes <ref> [131] </ref>, and heuristics for variable and value ordering (e.g., [109, 28]). Nadel has examined backtracking search algorithms for solving CSPs with varying levels of look-ahead.
Reference: [132] <author> L. Steinberg and R. Ling. </author> <title> A priori knowledge of structure vs. constraint propagation: One fragment of a science of design. </title> <institution> Rutgers AI/Design Group Working Paper 164., </institution> <month> March </month> <year> 1990. </year>
Reference: [133] <author> L. Sterling and E. Shapiro. </author> <title> The Art of Prolog. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference: [134] <author> S. Tappel. </author> <title> Some algorithm design methods. </title> <booktitle> In Proceedings of AAAI80, </booktitle> <year> 1980. </year>
Reference-contexts: The purpose of test/constraint incorporation procedures is the step-wise refinement of a static problem specification to a generative algorithm that produces solutions efficiently. Increase of efficiency is due to the migration of test/constraint knowledge is from the tester component into the generator component of candidate solution structures. Tappel <ref> [134] </ref> was the first to sketch out such an incorporation method: (1) the constraints (or, tests) are propagated such that they are positioned adjacent to the generator of candidate solutions structures; (2) the constraints (or, tests) are incorporated into the generator; (3) new constraints may be deduced, and the incorporation processes
Reference: [135] <author> N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. </author> <title> Equation of state calculations for fast computing machines. </title> <journal> J. Chem. Phys., </journal> <volume> 21, </volume> <year> 1953. </year>
Reference-contexts: Simulated annealing is a form of local search that has been shown to effectively and efficiently solve multivariate combinatorial optimization problems. Solutions to optimization problems are states for which some objective criterion is optimized. Metropolis et. al. <ref> [135] </ref> have introduced simulated annealing as an algorithm that efficiently simulates processes in statistical mechanics. In particular, it simulates a col lection of atoms in equilibrium at a given temperature. Random displacements of atoms are permitted when they result in lower energy levels.
Reference: [136] <author> C. Tong. KBSDE: </author> <title> An Environment For Developing Knowledge-Based Design Tools. </title> <editor> In T. Dietterich, editor, </editor> <booktitle> Proceedings of the Workshop on Knowledge Compilation, </booktitle> <pages> pages 127-138, </pages> <institution> Oregon State Univ., </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: Such systems are RICK by Braudaway [15, 14], and the here presented system MENDER. On a larger scale, similar goals are pursued by the ongoing research at Rutgers on the KBSDE environment <ref> [139, 136] </ref>. RICK is a system that automatically compiles efficient constrained generators for a declarative problem specification.
Reference: [137] <author> C. Tong. </author> <title> Toward Knowledge Compilation as A Classification Process. </title> <booktitle> In Proceedings of the Workshop on Automating Software Design (IJCAI89), </booktitle> <address> Detroit, </address> <month> August </month> <year> 1989. </year>
Reference: [138] <author> C. Tong. </author> <title> A Divide-and-Conquer Approach to Knowledge Compilation. In M.R. </title> <editor> Lowry and R.D. McCartney, editors, </editor> <booktitle> Automating Software Design. </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference: [139] <author> C. Tong. </author> <title> A divide-and-conquer approach to knowledge compilation (chapter 11). In Automating Software Design. </title> <publisher> AAAI Press, </publisher> <year> 1991. </year> <note> Also available as AI/Design Project Working Paper #174. </note>
Reference-contexts: Such systems are RICK by Braudaway [15, 14], and the here presented system MENDER. On a larger scale, similar goals are pursued by the ongoing research at Rutgers on the KBSDE environment <ref> [139, 136] </ref>. RICK is a system that automatically compiles efficient constrained generators for a declarative problem specification. <p> As such, MENDER's initial interaction with user replaces a problem reformulation step that would be required within the context of a larger algorithm design environment. An example of such an environment (in the spirit of the KBSDE project <ref> [139] </ref>) is sketched in Figure 9.1. Here constraints may initially appear in a form that does not match any RAP definition. They must be reformulated as RAP before they can be handled by MENDER. <p> MENDER to a system that is additionally capable of compiling constraint-specific simulating annealing problem solvers for resource assignment optimization problems. 2 9.6.2 Embedding MENDER within larger knowledge-based software design environment Our work on MENDER was conducted in the context of the development of the KB-SDE system as Rutgers University (see <ref> [139] </ref>). The KBSDE system is intended to be a knowledge-based software design environment which adopts a divide-and-conquer approach towards the problem-driven compilation of efficient software.
Reference: [140] <author> C. Tong. </author> <title> The Nature and Significance of Knowledge Compilation. </title> <journal> IEEE Expert, </journal> <volume> 6(2), </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Such wide definitions of knowledge compilation have been given, for example, by Keller [58] or Tong <ref> [140] </ref>. Knowlege compilation methods can be understood as transformations, which may be either content-preserving or content-modifying (see [58]). <p> Among content-modifying knowledge compilation we can further distinguish methods that add knowledge from methods that remove knowledge (e.g., see [18, 19]). 220 More narrowly, software-design oriented understandings of knowledge compilation have been expressed by, for example, Dietterich [31] and Tong <ref> [140, 141] </ref>. Dietterich sees knowledge compilation techniques as the means by which to close the gap between declarative problem specifications and run-time problem solvers that are capable of exploiting the domain and problem knowledge in a more direct and task-oriented fashion.
Reference: [141] <author> C. Tong and D. Sriram. </author> <title> Chapter i. </title> <editor> In C. Tong and D. Sriram, editors, </editor> <booktitle> Artificial Intelligence in Engineering Design, volume I. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Among content-modifying knowledge compilation we can further distinguish methods that add knowledge from methods that remove knowledge (e.g., see [18, 19]). 220 More narrowly, software-design oriented understandings of knowledge compilation have been expressed by, for example, Dietterich [31] and Tong <ref> [140, 141] </ref>. Dietterich sees knowledge compilation techniques as the means by which to close the gap between declarative problem specifications and run-time problem solvers that are capable of exploiting the domain and problem knowledge in a more direct and task-oriented fashion. <p> MENDER: incorporating constraints into patchers Generators need not be the only functional components that are targets for constraint incorporation. In fact, when adopting the broader view expressed by Tong and Sriram in <ref> [141] </ref>, namely that one "effective way to use design knowledge is to compile it into the active components of the search algorithm (e.g., creating customized routines for efficiently performing special tasks...)", then MENDER's construction of patchers to satisfy global resource assignment problems is also an example of knowledge compilation. <p> In this fashion, the constraint itself helps to determine the course of problem solving as opposed to merely providing the problem solver's test or termination criterion (also see <ref> [141] </ref>). Customarily, constraint incorporation has been associated with generation-oriented problem solving processes. Test (or, constraint) knowledge is moved inside the generator of the target artifact by placing either compile-time, or run-time restrictions on the domains of the artifact's parameters.
Reference: [142] <author> T.M. Mitchell, P.E. Utgoff, and R.B. Banerji. </author> <title> Learning by experimentation: Acquiring and refining problem-solving heuristics. </title> <editor> In J.G. Carbonell R.S. Michalski and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning. </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: The 221 first type of approach extracts heuristic measures of the distance to a solution from relaxed models of the original search space [106, 110]. The second type of approach derives heuristics through explaining and specializing general search control rules for the specific domain and problem instance (e.g. <ref> [142, 59, 86, 9] </ref>). Deriving heuristics through model relaxation Pearl [104, 106] has shown how admissible search heuristics can be constructed systematically through relaxation of the domain model. Given a state-space search model, the model can be relaxed by deleting constraints from the list of operator preconditions.
Reference: [143] <author> M. Valtorta. </author> <title> Response to: Explicit Solutions to the N-Queens Problems for all N. </title> <journal> SIGART Bulletin, </journal> <volume> 2(4), </volume> <year> 1991. </year>
Reference: [144] <author> M.P. Vecchi and S. Kirkpatrick. </author> <title> Global Wiring by Simulated Annealing. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <volume> CAD-2(4), </volume> <month> October </month> <year> 1983. </year>
Reference-contexts: Simulated annealing has also performed well on the classical traveling salesman problem. We add a by no means exhaustive list of applications of simulated annealing in other areas: gate-matrix layout [68], design of "good codes" [119], logic minization [66], and floorplanning [153]. For more examples see also <ref> [113, 144] </ref>. In retrospect, it is interesting to note, that MENDER would have been able to automatically compile local search components that conduct simulated annealing, had we given it the knowledge of an "simulated annealing shell". <p> Optimization problems in resource assignment are, for instance: * Minimize the sum of all consumed resources. * Maximize the participation of consumers in the consumption of the resources. * Minimize (or, maximize) the sharing of resources by multiple consumers. In the recent literature (e.g., <ref> [40, 154, 119, 113, 153, 144, 11, 112] </ref>), we find reports on good performances of a simulated annealing problem solvers for a wide range of practical optimization problems (e.g., gate matrix layout, channel routing, cell placement, design of "good" codes, traveling salesman).
Reference: [145] <author> K. Voigt. </author> <title> Knowledge-Based Design of Generate-and-Patch Problem Solvers that Solve Global Resource Assignment Problems. </title> <booktitle> In Proceedings of the AAAI-92 Workshop on Automating Software Design, </booktitle> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference: [146] <author> K. Voigt. </author> <title> More Efficient Heuristic Repair through Compile-Time Reduction of "Cost per Node". </title> <booktitle> In Proceedings of the Sixth Australian Joint Conference on Artificial Intelligence (AI'93), </booktitle> <address> Melbourne, </address> <month> November </month> <year> 1993. </year> <month> 338 </month>
Reference: [147] <author> K. Voigt. </author> <title> Polarity Analysis to Determine "Promising Operators" for Heuristic Repair Algorithms that Solve Resource Assignment Problems. </title> <booktitle> In Proceedings of the Sixth Australian Joint Conference on Artificial Intelligence (AI'93), </booktitle> <address> Melbourne, </address> <month> November </month> <year> 1993. </year>
Reference: [148] <author> K. Voigt and C. Tong. </author> <title> Automating the Construction of Patchers that Satisfy Global Constraints. </title> <booktitle> In Proceedings of IJCAI-89, </booktitle> <address> Detroit, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Both types of operators are members of the set of improving operators, yet blocking operators should be avoided since they are guaranteed to lead to local maxima, whereas block-preventing operators, when applied in time, can counteract this effect (see <ref> [148] </ref>). The typical effects of both types of operators are visualized in the floorplanning example depicted in Figure 9.3. Suppose that the objective is to patch a 6-room floorplan (see (a)) such that the house area is fully filled (context: "rooms do not overlap").
Reference: [149] <author> R. Waldinger. </author> <title> Achieving Several Goals Simultaneously. </title> <booktitle> In Readings In Artificial Intelligence. </booktitle> <publisher> Tioga, </publisher> <year> 1981. </year>
Reference: [150] <author> R.C. Waters. </author> <title> The Programmer's Apprentice: A Session with KBEmacs. </title> <editor> In C. Rich and R.C. Waters, editors, </editor> <booktitle> Readings in Artificial Intelligence and Software Engineering. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Yet, since transformation rules tend to capture knowledge at a grain size that is coarser than the first principle knowledge typically expressed in FOL, DIOGENES is somewhat less general than constructive theorem approaches. The Programmer's Apprentice by Rich and Waters <ref> [111, 150] </ref>, predates DIOGENES. This system includes KBEmacs (Knowledge-Based Editor in Emacs), which serves as an assistant to the human programmer. Essentially, the system provides the user with a libary of program schemas ("cliches") which capture generic computings tasks, and assists in the specialization of the schemas for specific tasks.
Reference: [151] <editor> R.C. Waters and E.J. Chikofsky, Guest Editors. </editor> <booktitle> Reverse engineering. Communications of the ACM, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The three levels are depicted abstractly in Figure 1.2. Note that the reverse mappings, i.e. the mapping of program level constructs to the function level, or further-on to the knowledge level, roughly correspond to processes that have been termed "reverse engineering" <ref> [151] </ref>. This dissertation focuses on the design processes that concern mappings from the knowledge level to the function level.
Reference: [152] <author> B.C. Williams. </author> <title> A Symbolic Approach to Qualitative Algebraic Reasoning. </title> <booktitle> In Proceedings of AAAI-88, </booktitle> <address> Saint Paul, Minnesota, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: In fact, it has been used for purposes as varied as the construction of specification morphisms [124], speed up of search in resolution theorem proving [80], and qualitative algebraic reasoning for the design and verification of physical systems <ref> [152] </ref>. Smith employs polarity analysis in the automated design of algorithms. Algorithm design is viewed as the construction of a specification morphism, that is, a translation of a formal problem specification into a corresponding specification in some algorithm or program language. <p> Manna and Waldinger [80] employ polarity analysis to speed up search in resolution theorem proving. By replacing sentences with positive polarity with false and sentences with negative polarity with true, the combined sentence becomes more false, whereby speedier progress is made towards the resolution goal false. Williams <ref> [152] </ref> has presented a qualitative algebra of signs that share similarities with the our polarity constants. William's algebra supports reasoning about the design and verification of physical systems in the absence of accurate values for physical quantities.
Reference: [153] <author> D.F. Wong and C.L. Liu. </author> <title> A New Algorithm for Floorplan Design. </title> <booktitle> In Proceedings of the 23rd Design Automation Conference, </booktitle> <year> 1986. </year>
Reference-contexts: Simulated annealing has also performed well on the classical traveling salesman problem. We add a by no means exhaustive list of applications of simulated annealing in other areas: gate-matrix layout [68], design of "good codes" [119], logic minization [66], and floorplanning <ref> [153] </ref>. For more examples see also [113, 144]. In retrospect, it is interesting to note, that MENDER would have been able to automatically compile local search components that conduct simulated annealing, had we given it the knowledge of an "simulated annealing shell". <p> Optimization problems in resource assignment are, for instance: * Minimize the sum of all consumed resources. * Maximize the participation of consumers in the consumption of the resources. * Minimize (or, maximize) the sharing of resources by multiple consumers. In the recent literature (e.g., <ref> [40, 154, 119, 113, 153, 144, 11, 112] </ref>), we find reports on good performances of a simulated annealing problem solvers for a wide range of practical optimization problems (e.g., gate matrix layout, channel routing, cell placement, design of "good" codes, traveling salesman).
Reference: [154] <author> H.W. Leong, D.F. Wong, and C.L. Liu. </author> <title> A Simulated-Annealing Channel Router. </title> <booktitle> In Proceedings of the IEEE Int. Conf. Comp. Aided Design, </booktitle> <year> 1985. </year> <month> 339 </month>
Reference-contexts: Optimization problems in resource assignment are, for instance: * Minimize the sum of all consumed resources. * Maximize the participation of consumers in the consumption of the resources. * Minimize (or, maximize) the sharing of resources by multiple consumers. In the recent literature (e.g., <ref> [40, 154, 119, 113, 153, 144, 11, 112] </ref>), we find reports on good performances of a simulated annealing problem solvers for a wide range of practical optimization problems (e.g., gate matrix layout, channel routing, cell placement, design of "good" codes, traveling salesman).
References-found: 154

