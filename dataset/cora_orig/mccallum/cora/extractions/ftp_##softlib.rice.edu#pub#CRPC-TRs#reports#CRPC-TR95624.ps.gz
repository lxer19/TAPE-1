URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95624.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: fchialin, als, saltzg@cs.umd.edu  
Title: CHAOS++: A Runtime Library for Supporting Distributed Dynamic Data Structures  
Author: Chialin Chang Alan Sussman Joel Saltz 
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies and Department of Computer Science University of Maryland,  
Abstract: Traditionally, applications executed on distributed memory architectures in single-program multiple-data (SPMD) mode use distributed (multi-dimensional) data arrays. Good performance has been achieved by applying runtime techniques to such applications executing in a loosely synchronous manner. However, many applications utilize language constructs such as pointers to synthesize dynamic complex data structures, such as linked lists, trees and graphs, with elements consisting of complex composite data types. Existing runtime systems that solely rely on global indices cannot be used for these applications, as no global names or indices are imposed upon the elements of these data structures. CHAOS++ is a portable object-oriented runtime library that supports applications using dynamic distributed data structures, including both arrays and pointer-based data structures. In particular, CHAOS++ deals with complex data types and pointer-based data structures by providing mobile objects and globally addressable objects. Preprocessing techniques are used to analyze communication patterns, and data exchange primitives are provided to carry out efficient data transfer. Performance results for four applications are also included to demonstrate the wide applicability of the runtime library.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agrawal, A. Sussman, and J. Saltz. </author> <title> Efficient Runtime Support for Parallelizing Block Structured Applications. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 158-67. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year> <month> 29 </month>
Reference-contexts: For regularly distributed data, the data distribution across the processors (e.g. block, cyclic), and its associated descriptor, can be represented compactly, for example by using regular section descriptors [19]. This form of descriptor has been used in the Multiblock Parti library <ref> [1, 2, 42] </ref>, which CHAOS++ uses to partition data and generate communication schedules for regularly distributed arrays. Multiblock Parti has been designed to support the parallelization of applications that work on multiple structured regular grids, such as those that arise in multiblock and/or multigrid codes.
Reference: [2] <author> G. Agrawal, A. Sussman, and J. Saltz. </author> <title> An Integrated Runtime and Compile-Time Approach for Paral--lelizing Structured and Block Structured Applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(7) </volume> <pages> 747-54, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: For regularly distributed data, the data distribution across the processors (e.g. block, cyclic), and its associated descriptor, can be represented compactly, for example by using regular section descriptors [19]. This form of descriptor has been used in the Multiblock Parti library <ref> [1, 2, 42] </ref>, which CHAOS++ uses to partition data and generate communication schedules for regularly distributed arrays. Multiblock Parti has been designed to support the parallelization of applications that work on multiple structured regular grids, such as those that arise in multiblock and/or multigrid codes.
Reference: [3] <author> R. Bennett, K. Bryant, A. Sussman, R. Das, and J. Saltz. Jovian: </author> <title> A Framework for Optimizing Parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: For every specified sensor image, the algorithm first computes a sub-image that spatially contains the whole query region, and regularly partitions the sub-image across the processors by blocks of columns of data vectors to obtain good load balance. Jovian I/O library <ref> [3] </ref> routines are then invoked to read the computed sub-image from the disks and distribute the data vectors as specified. The Jovian library has been designed to optimize the I/O performance of multiprocessor architectures with multiple disks or disk arrays.
Reference: [4] <author> R. Bennett, K. Bryant, A. Sussman, R. Das, and J. Saltz. </author> <title> Collective I/O: Models and Implementation. </title> <institution> Technical Report CS-TR-3429 and UMIACS-TR-95-29, University of Maryland, Department of Computer Science and University of Maryland Institute for Advanced Computer Studies, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: A detailed description of the I/O performance of the VIM application using the Jovian library is given in Bennett et al. <ref> [4] </ref>. The results from Figure 11 show that good speedup is obtained. 5.4 Image Processing Image Segmentation Another application under development is image segmentation. This application segments a given image into a hierarchy of components based on the border contrast between adjacent components.
Reference: [5] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF Compiler for Distributed Memory MIMD Computers: Design, Implementation and Performance Results. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 351-60. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: While Multiblock Parti produces distributed array descriptors at runtime, because it allows the sizes of arrays to be determined at runtime, compilers for data parallel languages such as Fortran 90D <ref> [5] </ref> and High Performance Fortran (HPF) [22], can produce such descriptors at compile time.
Reference: [6] <author> M.C. Carroll and L. Pollock. </author> <title> Composites: Trees for Data Parallel Programming. </title> <booktitle> In Proceedings of the 1994 International Conference on Computer Languages, </booktitle> <pages> pages 43-54. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: For example, new elements can only be added to the data structure one at a time, not in parallel. This is a serious restriction for efficient parallel implementations, because it could require many synchronization operations, which are usually expensive on distributed memory machines. Carroll et al. <ref> [6] </ref> propose a composite tree model, which describes a data-parallel computation as a tree structure. The computation starts at the root of the structure, which, usually in parallel, invokes methods from its child nodes. The child nodes, in turn, invoke methods from their child nodes, and so on.
Reference: [7] <author> K. Mani Chandy and Carl Kesselman. </author> <title> CC++: A declarative concurrent object oriented programming notation. </title> <type> Technical Report CS-TR-92-01, </type> <institution> Department of Computer Science, California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Furthermore, partitioning a pointer-based data structure may assign two elements connected via pointers to two different processors, raising the need for global pointers. A global pointer, as supported by several languages, including Split-C [11], CC++ <ref> [7] </ref>, and pC++ [26], may point to an object owned by another processor, and effectively consists of a processor identifier and a local pointer that is only valid on the named processor. <p> Roughly speaking, there are two types of systems that are relevant. The first type of system augments an existing language with parallel constructs. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of system, based on C++, include Mentat [17], CC++ <ref> [7] </ref>, ICC++ [10], C** [24], CHARM [21], and pC++ [26]. Mentat, CC++ and CHARM consider program execution as completely unstructured interactions among a set of objects and support only asynchronous communication, and this approach suffers from overhead incurred from either polling or interrupts.
Reference: [8] <author> C. Chang, A. Sussman, and J. Saltz. </author> <title> Object-oriented runtime support for complex distributed data structures. </title> <institution> Technical Report CS-TR-3438 and UMIACS-TR-95-35, University of Maryland, Department of Computer Science and University of Maryland Institute for Advanced Computer Studies, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: of building the schedule can be amortized. 5 Applications and Performance Results To provide examples of using CHAOS++, and to evaluate the performance of the CHAOS++ library, we will present results for the polygon overlay problem discussed in [9], and also for three complete applications selected from three distinct classes <ref> [8] </ref>: computational aerodynamics (scientific computation), geographic information systems (spatial database processing), and image processing. The performance results for the parallel polygon overlay algorithms were obtained on the SP-2 at the University of Maryland Institute for Advanced Computer Studies (UMIACS), using the IBM C++ compiler, mpCC, and the MPL communication library.
Reference: [9] <author> C. Chang, A. Sussman, and J. Saltz. </author> <title> Chaos++: A runtime library for supporting distributed dynamic data structures. </title> <editor> In G. V. Wilson and P. Lu, editors, </editor> <title> Parallel Programming Using C++. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: communication phases, the same communication schedule can be reused, and the cost of building the schedule can be amortized. 5 Applications and Performance Results To provide examples of using CHAOS++, and to evaluate the performance of the CHAOS++ library, we will present results for the polygon overlay problem discussed in <ref> [9] </ref>, and also for three complete applications selected from three distinct classes [8]: computational aerodynamics (scientific computation), geographic information systems (spatial database processing), and image processing. <p> experiments were conducted on the Intel iPSC/860 at the National Institutes of Health (NIH), compiled with the GNU C++ compiler, gcc version 2.5.8, and the IBM SP-1s at both Argonne National Laboratory and the Cornell Theory Center, using mpCC and MPL. 5.1 Polygon Overlay The polygon overlay problem presented in <ref> [9] </ref> is a simplified version of the general polygon overlay problem, which arises in geographical information systems. The inputs are two maps A and B, each of which consists of a set of non-overlapping polygons.
Reference: [10] <author> A.A. Chien and Julian Dolby. </author> <title> The Illinois Concert System: A Problem-solving Environment for Irregular Applications. </title> <booktitle> In Proceedings of DAGS'94, The Symposium on Parallel Computation and Problem Solving Environments, </booktitle> <year> 1994. </year>
Reference-contexts: The first type of system augments an existing language with parallel constructs. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of system, based on C++, include Mentat [17], CC++ [7], ICC++ <ref> [10] </ref>, C** [24], CHARM [21], and pC++ [26]. Mentat, CC++ and CHARM consider program execution as completely unstructured interactions among a set of objects and support only asynchronous communication, and this approach suffers from overhead incurred from either polling or interrupts.
Reference: [11] <author> D.E. Culler, A. Dusseau, S.C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proc. Supercomputing '93, </booktitle> <year> 1993. </year>
Reference-contexts: Furthermore, partitioning a pointer-based data structure may assign two elements connected via pointers to two different processors, raising the need for global pointers. A global pointer, as supported by several languages, including Split-C <ref> [11] </ref>, CC++ [7], and pC++ [26], may point to an object owned by another processor, and effectively consists of a processor identifier and a local pointer that is only valid on the named processor.
Reference: [12] <author> R. Das, D.J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The Design and Implementation of a Parallel Unstructured Euler Solver Using Software Primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-96, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Good performance has been achieved by applying such runtime techniques to various problems with unstructured data access patterns, such as molecular dynamics for computational chemistry [20], particle-in-cell (PIC) codes for computational aerodynamics [28], and computational fluid dynamics <ref> [12] </ref>. Unfortunately, many existing runtime systems for parallelizing applications with complex data access patterns on distributed memory parallel machines fail to handle pointers.
Reference: [13] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed Memory Compiler Methods for Irregular Problems Data Copy Reuse and Runtime Partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Languages, Compilers and Runtime Environments for Distributed Memory Machines, </booktitle> <pages> pages 185-220. </pages> <publisher> Elsevier, </publisher> <year> 1992. </year>
Reference-contexts: Both inspectors produce the same form of communication schedule, which can be used by the Multiblock Parti collective data movement routines. 2.3 Data Movement The preprocessing, or inspector, phase of a program parallelized using CHAOS results in a data structure called a communication schedule <ref> [13] </ref>, which stores the send/receive patterns of off-processor elements. The computation, or executor, phase uses the schedules to carry out communication. A communication schedule is used to fetch off-processor elements into a local buffer and to scatter these elements back to their home processors after the computational phase is over.
Reference: [14] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-79, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Most of these runtime systems also rely on the existence of global indices, which makes them applicable only to distributed arrays. CHAOS++ is a runtime library targeted at object-oriented applications with dynamic communication patterns. It subsumes CHAOS <ref> [14] </ref>, which is a runtime library developed to efficiently support applications with irregular patterns of data accesses to distributed arrays. <p> In this scheme, the translation table is divided into pages, and pages are distributed across processors. Processors that refer to a page frequently receive a copy of the page, making subsequent references local. A more detailed description of this scheme is presented in Das et al. <ref> [14] </ref>. For applications with more regular data access patterns, the translation tables employed by the CHAOS library are not required. Instead, distributed array descriptors that contain complete information about the portions of the arrays residing on each processor can be generated. <p> To minimize memory usage, a translation table can be distributed among the processors, in which case interprocessor communication is required for dereference. CHAOS++ uses the paged translation table <ref> [14] </ref> described 14 in Section 2.2. The advantage of a paged translation table is that each processor is allowed to choose which pages to replicate based on the access patterns to its distributed arrays, so that the communication cost for dereference can be minimized. Das et al. [14] discuss the performance <p> paged translation table <ref> [14] </ref> described 14 in Section 2.2. The advantage of a paged translation table is that each processor is allowed to choose which pages to replicate based on the access patterns to its distributed arrays, so that the communication cost for dereference can be minimized. Das et al. [14] discuss the performance behavior of two applications using paged translation tables, and to no surprise, performance improves as more of the table entries are cached locally. 4.1.2 Mapping Structures Partitioning a pointer-based data structure among processors, as described in Section 3.1, generates global pointers that point from one processor to <p> Both types of communication schedules have the same structure, with the only difference being the way information is represented in the schedules. CHAOS++ uses CHAOS communication schedules for distributed arrays <ref> [14] </ref>, which refer to local objects and buffer space by local indices. The starting addresses of the distributed arrays involved are needed only when the actual data transfer occurs.
Reference: [15] <editor> G.C. Fox, R.D. Williams, and P.C. Messina. </editor> <booktitle> Parallel Computing Works. </booktitle> <publisher> Morgan Kaufman, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction A large class of applications execute on distributed memory parallel computers in single-program multiple-data (SPMD) mode in a loosely synchronous manner <ref> [15] </ref>. That is, collections of data objects are partitioned among processors, and the program executes a sequence of concurrent computational phases.
Reference: [16] <author> M. Gerndt. </author> <title> Updating Distributed Variables in Local Computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-93, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Intra-array communication is handled by allocation of extra space at the beginning and end of each array dimension on each processor. These extra elements are called overlap, or ghost, cells <ref> [16] </ref>.
Reference: [17] <author> A.S. Grimshaw. </author> <title> Easy-to-Use Object-Oriented Parallel Processing with Mentat. </title> <journal> IEEE Computer, </journal> <volume> 26(5) </volume> <pages> 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Roughly speaking, there are two types of systems that are relevant. The first type of system augments an existing language with parallel constructs. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of system, based on C++, include Mentat <ref> [17] </ref>, CC++ [7], ICC++ [10], C** [24], CHARM [21], and pC++ [26]. Mentat, CC++ and CHARM consider program execution as completely unstructured interactions among a set of objects and support only asynchronous communication, and this approach suffers from overhead incurred from either polling or interrupts.
Reference: [18] <author> R. Gupta. </author> <title> SPMD Execution of Programs with Pointer-Based Data Structures on Distributed-Memory Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 92-107, </pages> <year> 1992. </year>
Reference-contexts: These libraries both provide efficient management of array-based parallel constructs distributed across processors. CHAOS++, however, performs optimization through preprocessing techniques, and provides efficient support for dynamic distributed data structures, including pointer-based data structures. Some recent work focuses on support for distributed pointer-based data structures. Gupta <ref> [18] </ref> suggests an approach that assigns to every element of a distributed pointer-based data structure a name known to all processors. The name is based on the position of the element in the data structure, which is registered with all processors as the element is inserted into the data structure.
Reference: [19] <author> P. Havlak and K. Kennedy. </author> <title> An Implementation of Interprocedural Bounded Regular Section Analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-60, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Instead, distributed array descriptors that contain complete information about the portions of the arrays residing on each processor can be generated. For regularly distributed data, the data distribution across the processors (e.g. block, cyclic), and its associated descriptor, can be represented compactly, for example by using regular section descriptors <ref> [19] </ref>. This form of descriptor has been used in the Multiblock Parti library [1, 2, 42], which CHAOS++ uses to partition data and generate communication schedules for regularly distributed arrays.
Reference: [20] <author> Y.-S. Hwang, R. Das, J.H. Saltz, M. Hodoscek, and B.R. Brooks. </author> <title> Parallelizing Molecular Dynamics Programs for Distributed Memory Machines. </title> <journal> IEEE Computational Science & Engineering, </journal> <volume> 2(2) </volume> <pages> 18-29, </pages> <month> Summer </month> <year> 1995. </year> <month> 30 </month>
Reference-contexts: Optimizations that can be carried out by compilers are thus limited, and runtime analysis is required [38]. Good performance has been achieved by applying such runtime techniques to various problems with unstructured data access patterns, such as molecular dynamics for computational chemistry <ref> [20] </ref>, particle-in-cell (PIC) codes for computational aerodynamics [28], and computational fluid dynamics [12]. Unfortunately, many existing runtime systems for parallelizing applications with complex data access patterns on distributed memory parallel machines fail to handle pointers.
Reference: [21] <author> L. Kale and S. Krishnan. CHARM++: </author> <title> A Portable Concurrent Object Oriented System Based on C++. </title> <editor> In Andreas Paepcke, editor, </editor> <booktitle> Proceedings of OOPSLA'93, </booktitle> <volume> volume 28, </volume> <pages> pages 91-108. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: The first type of system augments an existing language with parallel constructs. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of system, based on C++, include Mentat [17], CC++ [7], ICC++ [10], C** [24], CHARM <ref> [21] </ref>, and pC++ [26]. Mentat, CC++ and CHARM consider program execution as completely unstructured interactions among a set of objects and support only asynchronous communication, and this approach suffers from overhead incurred from either polling or interrupts.
Reference: [22] <author> C.H. Koelbel, D.B. Loveman, R.S. Schreiber, G.L. Steele Jr., and M.E. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: While Multiblock Parti produces distributed array descriptors at runtime, because it allows the sizes of arrays to be determined at runtime, compilers for data parallel languages such as Fortran 90D [5] and High Performance Fortran (HPF) <ref> [22] </ref>, can produce such descriptors at compile time. In either case, this compact representation allows the descriptors to be replicated across all processors, so that communication schedules can be built without additional communication being performed, as is required when using a CHAOS distributed translation table.
Reference: [23] <author> S.R. Kohn and S.B. Baden. </author> <title> A Robust Parallel Programming Model for Dynamic Non-Uniform Scientific Computations. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 509-517. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Communication can thus be performed efficiently for loosely synchronous applications. The second type of system is a user-level class library that assumes no special support from the compiler, just like CHAOS++. Examples include P++ [30] and LPARX <ref> [23] </ref>. These libraries both provide efficient management of array-based parallel constructs distributed across processors. CHAOS++, however, performs optimization through preprocessing techniques, and provides efficient support for dynamic distributed data structures, including pointer-based data structures. Some recent work focuses on support for distributed pointer-based data structures.
Reference: [24] <author> J.R. Larus. </author> <title> C**: a Large-Grain, Object-Oriented, Data-Parallel Programming Language. </title> <editor> In U. Baner-jee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages And Compilers for Parallel Computing (5th International Workshop), </booktitle> <pages> pages 326-41. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: The first type of system augments an existing language with parallel constructs. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of system, based on C++, include Mentat [17], CC++ [7], ICC++ [10], C** <ref> [24] </ref>, CHARM [21], and pC++ [26]. Mentat, CC++ and CHARM consider program execution as completely unstructured interactions among a set of objects and support only asynchronous communication, and this approach suffers from overhead incurred from either polling or interrupts.
Reference: [25] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4), </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: However, the techniques used in the library can also be applied to other environments that provide a standard C++ compiler and a mechanism for global data accesses, including various distributed shared memory architectures <ref> [25, 29, 41] </ref>. The remainder of the chapter is structured as follows. In Section 2 we discuss how CHAOS++ supports complex objects and distributed arrays, and in Section 3, we discuss how distributed pointer-based data structures are supported in CHAOS++. Section 4 describes the implementation of the library.
Reference: [26] <author> A. Malony, B. Mohr, P. Beckman, D. Gannon, S. Yang, F. Bodin, and S. Kesavan. </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 588-597. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Furthermore, partitioning a pointer-based data structure may assign two elements connected via pointers to two different processors, raising the need for global pointers. A global pointer, as supported by several languages, including Split-C [11], CC++ [7], and pC++ <ref> [26] </ref>, may point to an object owned by another processor, and effectively consists of a processor identifier and a local pointer that is only valid on the named processor. <p> The first type of system augments an existing language with parallel constructs. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of system, based on C++, include Mentat [17], CC++ [7], ICC++ [10], C** [24], CHARM [21], and pC++ <ref> [26] </ref>. Mentat, CC++ and CHARM consider program execution as completely unstructured interactions among a set of objects and support only asynchronous communication, and this approach suffers from overhead incurred from either polling or interrupts. ICC++, pC++ and C** provide array-based collections of objects, and parallel constructs to operate on them.
Reference: [27] <author> R. Mirchandaney, J.H. Saltz, R.M. Smith, K. Crowley, and D.M. Nicol. </author> <title> Principles of Runtime Support for Parallel Processors. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 140-52. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1988. </year>
Reference-contexts: For the loosely synchronous applications we have described in the introduction, the data access pattern of a computation phase is usually known before entering the computation phase and is repeated many times. CHAOS thus carries out optimization through two phases, the inspector phase and the executor phase <ref> [27, 38] </ref>. During program execution, the CHAOS inspector routines examine the data references expressed in the indirection arrays, given in global indices, and convert them into host processor numbers and local indices.
Reference: [28] <author> B. Moon and J. Saltz. </author> <title> Adaptive Runtime Support for Direct Simulation Monte Carlo Methods on Distributed Memory Architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 176-83. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Optimizations that can be carried out by compilers are thus limited, and runtime analysis is required [38]. Good performance has been achieved by applying such runtime techniques to various problems with unstructured data access patterns, such as molecular dynamics for computational chemistry [20], particle-in-cell (PIC) codes for computational aerodynamics <ref> [28] </ref>, and computational fluid dynamics [12]. Unfortunately, many existing runtime systems for parallelizing applications with complex data access patterns on distributed memory parallel machines fail to handle pointers. <p> Since particles move between mesh cells, the cells, and thus the particles, are redistributed across the processors occasionally (once every few time steps) to maintain a good load balance. Moon <ref> [28] </ref> describes a parallel implementation of the DSMC application that uses the CHAOS runtime library. <p> The simulated space consists of 9,720 cells, and initially contains about 48,600 particles. 400 time steps are performed, and a chain partitioner <ref> [28] </ref> is used to dynamically partition the mesh cells at runtime when load imbalance is detected. The Fortran code has been shown to be a good implementation, and the C++ version is at most 15% slower.
Reference: [29] <author> W. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: However, the techniques used in the library can also be applied to other environments that provide a standard C++ compiler and a mechanism for global data accesses, including various distributed shared memory architectures <ref> [25, 29, 41] </ref>. The remainder of the chapter is structured as follows. In Section 2 we discuss how CHAOS++ supports complex objects and distributed arrays, and in Section 3, we discuss how distributed pointer-based data structures are supported in CHAOS++. Section 4 describes the implementation of the library.
Reference: [30] <author> R. Parsons and D. Quinlan. </author> <title> Run-time Recognition of Task Parallelism Within the P++ Class Library. </title> <booktitle> In Proceedings of the 1993 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 77-86. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: Communication can thus be performed efficiently for loosely synchronous applications. The second type of system is a user-level class library that assumes no special support from the compiler, just like CHAOS++. Examples include P++ <ref> [30] </ref> and LPARX [23]. These libraries both provide efficient management of array-based parallel constructs distributed across processors. CHAOS++, however, performs optimization through preprocessing techniques, and provides efficient support for dynamic distributed data structures, including pointer-based data structures. Some recent work focuses on support for distributed pointer-based data structures.
Reference: [31] <author> R. Parulekar, L. Davis, R. Chellappa, J. Saltz, A. Sussman, and J. Towhshend. </author> <title> High Performance Computing for Land Cover Dynamics. </title> <booktitle> In Proceedings of the International Joint Conference on Pattern Recognition, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: It has been developed as part of the on-going Grand Challenge project in Land Cover Dynamics at the University of Maryland <ref> [31] </ref>. The overall project involves developing scalable and portable parallel programs for a variety of image and map data processing applications, to be integrated with new methods for parallel I/O of large scale images and maps.
Reference: [32] <author> R. Ponnusamy, Y.-S. Hwang, R. Das, J.H. Saltz, A. Choudhary, and G. Fox. </author> <title> Supporting Irregular Distributions using Data-Parallel Languages. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 3(1) </volume> <pages> 12-24, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: an object transferred to another processor may not retain the same pointer values for its sub-objects, but the new pointers will point at its new sub-objects with the same user data values (and connectivity, through pointers) as in the original object. 2.2 Partitioning Data Arrays The CHAOS runtime support library <ref> [32, 37] </ref> contains procedures that * support static and dynamic distributed array partitioning, * partition loop iterations and indirection arrays, * remap arrays from one distribution to another, and * carry out index translation, buffer allocation and communication schedule generation.
Reference: [33] <author> D.F.G. Rault and M.S. Woronowicz. </author> <title> Spacecraft Contamination Investigation by Direct Simulation Monte Carlo Contamination on UARS/HALOE. </title> <booktitle> In Proceedings AIAA 31th Aerospace Sciences Meeting and Exhibit, </booktitle> <address> Reno, Nevada, </address> <month> January </month> <year> 1993. </year> <institution> American Institute of Aeronautics and Astronautics. </institution>
Reference-contexts: It includes movement and collision handling of simulated particles on a spatial flow field domain overlaid by a 3-dimensional Cartesian mesh <ref> [33] </ref>. Depending upon its current spatial location, each particle is associated with a mesh cell, which typically contains a few particles, and moves from cell to cell as it participates in collisions and various boundary interactions in the simulated physical space.
Reference: [34] <author> P.J. Roache. </author> <title> Computational Fluid Dynamics, volume 1. </title> <publisher> Hermosa Publishers, </publisher> <address> Albuquerque, New Mexico, </address> <year> 1972. </year>
Reference-contexts: What distinguishes the DSMC method from other Particle-in-Cell (PIC) methods is that the movement and collision processes of particles are 22 a ping-pong communication scheme. and a ping-pong communication scheme. 23 completely uncoupled over a time step <ref> [34] </ref>. In the parallel implementation, mesh cells are distributed irregularly among the processors to achieve a good load balance. A CHAOS++ translation table with an entry for each mesh cell is constructed to describe the distribution.
Reference: [35] <author> C. Rodrguez. </author> <title> An Appearance-Based Approach to Object Recognition in Aerial Images. </title> <type> Master's thesis, </type> <institution> Computer Science Department, University of Maryland, College Park, </institution> <year> 1994. </year>
Reference-contexts: This application segments a given image into a hierarchy of components based on the border contrast between adjacent components. The segmentation serves as a preprocessing phase for an appearance-based object recognition system developed at the University of Maryland <ref> [35] </ref>. The hierarchy is used by a high-level vision phase to heuristically combine components from various levels of the hierarchy into possible instances of objects. Further analysis by shape delineation processes select the combinations that correspond to the locally best instances of objects.
Reference: [36] <author> A. Rogers, M.C. Carlisle, J.H. Reppy, and L.J. Hendren. </author> <title> Supporting Dynamic Data Structures on Distributed-Memory Machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2) </volume> <pages> 233-63, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: However, it is not clear how accesses through pointers of an element of a distributed pointer-based data structure can be implemented under the composite tree model, other than with fine-grain blocking sends and receives. Olden <ref> [36] </ref> is a C-based system that supports recursively defined pointer-based data structures. In Olden, an application allocates nodes of its pointer-based data structures on the heaps of different processors, and the runtime system migrates threads of computation to processors that own the heap-allocated nodes being processed.
Reference: [37] <author> J. Saltz, R. Ponnusamy, S.D. Sharma, B. Moon, Y.-S. Hwang, M. Uysal, and R. Das. </author> <title> A Manual for the CHAOS Runtime Library. </title> <institution> Technical Report CS-TR-3437 and UMIACS-TR-95-34, University of Maryland, Department of Computer Science and University of Maryland Institute for Advanced Computer Studies, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Performance results for the polygon overlay problem and three complete applications are presented in Section 5. Section 6 discusses some related work, and we present conclusions in Section 7. 2 Runtime Support for Distributed Arrays CHAOS++ provides runtime support for distributed arrays through use of the Maryland CHAOS <ref> [37] </ref> and Multiblock Parti libraries [42]. The CHAOS library is employed as the underlying support for arrays with irregular distributions, while Multiblock Parti is used to support regular data distributions. <p> an object transferred to another processor may not retain the same pointer values for its sub-objects, but the new pointers will point at its new sub-objects with the same user data values (and connectivity, through pointers) as in the original object. 2.2 Partitioning Data Arrays The CHAOS runtime support library <ref> [32, 37] </ref> contains procedures that * support static and dynamic distributed array partitioning, * partition loop iterations and indirection arrays, * remap arrays from one distribution to another, and * carry out index translation, buffer allocation and communication schedule generation. <p> Multiblock Parti also has data transportation routines similar to those in CHAOS for performing the communication specified by a schedule produced by either the ghost cell fill or regular section move routines. A more detailed description of the CHAOS procedures can be found in Saltz et al. <ref> [37] </ref>. 2.4 An Example We now discuss how to parallelize Program 1.2 using the CHAOS runtime library. The transformed code for Program 1.2 that carries out the inspector and executor is shown in Program 1.3.
Reference: [38] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Optimizations that can be carried out by compilers are thus limited, and runtime analysis is required <ref> [38] </ref>. Good performance has been achieved by applying such runtime techniques to various problems with unstructured data access patterns, such as molecular dynamics for computational chemistry [20], particle-in-cell (PIC) codes for computational aerodynamics [28], and computational fluid dynamics [12]. <p> For the loosely synchronous applications we have described in the introduction, the data access pattern of a computation phase is usually known before entering the computation phase and is repeated many times. CHAOS thus carries out optimization through two phases, the inspector phase and the executor phase <ref> [27, 38] </ref>. During program execution, the CHAOS inspector routines examine the data references expressed in the indirection arrays, given in global indices, and convert them into host processor numbers and local indices.
Reference: [39] <author> S.D. Sharma, R. Ponnusamy, B. Moon, Y.-S. Hwang, R. Das, and J. Saltz. </author> <title> Run-time and Compile-time Support for Adaptive Irregular Problems. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 97-106, </pages> <month> November </month> <year> 1994. </year> <month> 31 </month>
Reference-contexts: CHAOS also provides primitives to redistribute data arrays efficiently at runtime. Special attention has been devoted towards optimizing the inspector for adaptive applications, where communication patterns are not reused many times <ref> [39] </ref>. 2.1 Mobile Objects CHAOS++ defines an abstract data type, called Mobject, for mobile objects. These are objects that may be transferred from one processor to another, so they must know how to pack and unpack themselves to and from a message buffer. <p> Communication schedules determine the number of communication startups and the volume of communication, so it is important to optimize them. CHAOS also supports specialized communication schedules <ref> [39] </ref>. For some adaptive applications, particularly those from the particle-in-cell domain, there is no significance attached to the placement order of incoming array elements. Such application-specific information can be used to build much cheaper light-weight communication schedules.
Reference: [40] <author> C.T. Shock, C. Chang, L. Davis, S. Goward, J. Saltz, and A. Sussman. </author> <title> A high performance image database system for remote sensing. </title> <booktitle> In 24th AIPR Workshop on Tools and Techniques for Modeling and Simulation, </booktitle> <address> Washington, D.C., </address> <month> October </month> <year> 1995. </year> <institution> Society for Photogrammetric and Industrial Engineers. </institution> <note> To appear. </note>
Reference-contexts: For each mesh cell, the algorithm selects from the given images the set of data points that spatially intersect with the mesh cell, using a C++ class library that supports spatial operators <ref> [40] </ref> (currently under development as part of the Grand Challenge project), and computes a vegetation index. CHAOS++ has been linked to this spatial operator class library to implement a parallel version of VIM.
Reference: [41] <author> J.P. Singh, T. Joe, J.L. Hennessy, and A. Gupta. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessors. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 214-25. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: However, the techniques used in the library can also be applied to other environments that provide a standard C++ compiler and a mechanism for global data accesses, including various distributed shared memory architectures <ref> [25, 29, 41] </ref>. The remainder of the chapter is structured as follows. In Section 2 we discuss how CHAOS++ supports complex objects and distributed arrays, and in Section 3, we discuss how distributed pointer-based data structures are supported in CHAOS++. Section 4 describes the implementation of the library.
Reference: [42] <author> A. Sussman, G. Agrawal, and J. Saltz. </author> <title> A Manual for the Multiblock PARTI Runtime Primitives, Revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and University of Maryland Institute for Advanced Computer Studies, </institution> <month> December </month> <year> 1993. </year> <month> 32 </month>
Reference-contexts: Section 6 discusses some related work, and we present conclusions in Section 7. 2 Runtime Support for Distributed Arrays CHAOS++ provides runtime support for distributed arrays through use of the Maryland CHAOS [37] and Multiblock Parti libraries <ref> [42] </ref>. The CHAOS library is employed as the underlying support for arrays with irregular distributions, while Multiblock Parti is used to support regular data distributions. <p> For regularly distributed data, the data distribution across the processors (e.g. block, cyclic), and its associated descriptor, can be represented compactly, for example by using regular section descriptors [19]. This form of descriptor has been used in the Multiblock Parti library <ref> [1, 2, 42] </ref>, which CHAOS++ uses to partition data and generate communication schedules for regularly distributed arrays. Multiblock Parti has been designed to support the parallelization of applications that work on multiple structured regular grids, such as those that arise in multiblock and/or multigrid codes.
References-found: 42

