URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3417/3417.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: frobertb,ksb,als,raja,saltzg@cs.umd.edu  
Title: A Framework for Optimizing Parallel I/O  
Author: Robert Bennett Kelvin Bryant Alan Sussman Raja Das Joel Saltz 
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies and Department of Computer Science University of Maryland  
Abstract: There has been a great deal of recent interest in parallel I/O. This paper discusses issues in the design and implementation of a portable I/O library designed to optimize the performance of multiprocessor architectures that include multiple disks or disk arrays. The major emphasis of the paper is on optimizations that are made possible by the use of collective I/O, so that I/O requests for multiple processors can be combined to improve performance. Performance measurements from benchmarking our implementation of an I/O library that currently performs collective local optimizations, called Jovian, on three application templates are also presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Access to compact global descriptors can allow an I/O library to skip the collective communication stage that is needed when performing collective local I/O optimization. The collective global I/O optimizations in such a library will be similar to the optimizations carried out by the Multiblock Parti library <ref> [1, 24] </ref> for interprocessor communication of regularly accessed data. The interface that is provided by an I/O library that implements these optimizations is called a global view. An example of an interface for a global view is illustrated in Figure 1 (a). <p> In that case, the user program is responsible for translating local, in-core, data structure addresses into the correct global addresses for accessing the entire out-of-core data structure (perhaps through calls to a runtime library such as CHAOS [9] or Multiblock PARTI <ref> [1] </ref>). A distributed view for out-of-core data structures greatly simplifies the design and implementation of a collective I/O optimizing library. <p> The view determines which of the overhead operations are performed within the library. For example, a global view would require that regular section analysis be performed to determine the ownership of the data to be read or written. This operation is currently implemented in Multiblock PARTI <ref> [1] </ref>, and the associated computational overhead is not very high (linear in the number of dimensions of the distributed array). <p> The general model for out-of-core compilation relies on having the compiler generate calls to various runtime libraries to handle both I/O and interprocessor communication. For communication, our techniques have been validated for irregular, block structured and structured data access patterns <ref> [1, 8, 20, 21] </ref>. Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data. The problem then becomes keeping track of the data currently available in memory. <p> Each C/P had access to a local ~ 1GB SCSI disk, with a maximum transfer rate of about 3MB/sec. The structured grid applications templates were parallelized using the Multiblock PARTI runtime library <ref> [1] </ref>. This library provides the functionality to lay out distributed arrays in a user specified way. For two of the application templates, the in-core structured grid was partitioned across processors in both dimensions (in HPF terms, with a (block,block) distribution).
Reference: [2] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> On efficient runtime support for multiblock and multigrid applications: Regular section analysis. </title> <institution> Technical Report CS-TR-3140 and UMIACS-TR-93-92, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Regular section analysis has been studied in detail in the context of compilation for distributed memory machines <ref> [2, 4, 12, 16, 22] </ref>, and such analysis can be applied to accessing out-of-core arrays as well. The analysis is inexpensive for the most common data distributions, so the regular section interface should provide good performance.
Reference: [3] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: In addition, the API's impact on designing optimizing I/O libraries, in terms of memory usage, communication cost, performance and ease of integration into runtime support for parallel compilers is discussed in detail. Specific examples from the Jovian library <ref> [3] </ref> are provided to illustrate the issues for an existing design. There are two complementary views for accessing an out-of-core data structure (residing on secondary storage), such as an array, from each process running a parallel program.
Reference: [4] <author> Siddhartha Chatterjee, John R. Gilbert, Fred J.E. Long, Robert Schreiber, and Shang-Hua Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 149-158, </pages> <month> May </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 28, No. </volume> <pages> 7. </pages>
Reference-contexts: Regular section analysis has been studied in detail in the context of compilation for distributed memory machines <ref> [2, 4, 12, 16, 22] </ref>, and such analysis can be applied to accessing out-of-core arrays as well. The analysis is inexpensive for the most common data distributions, so the regular section interface should provide good performance.
Reference: [5] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krishnaiyer, Ravi Ponnusamy, Tarvinder Singh, and Rajeev Thakur. </author> <title> PASSION: Parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> NPAC, </institution> <month> September </month> <year> 1994. </year> <note> Also available as CRPC Report CRPC-TR94483. </note>
Reference-contexts: Each I/O process can use this information to determine what to read from disk and what application process to send the data to. There are at least two other projects that perform collective local I/O optimizations. Choudhary <ref> [5, 10] </ref> assumes that each processor has access to either a physical or logical I/O device. They partition parallel I/O into two phases.
Reference: [6] <author> R. Das, Y.-S. Hwang, M. Uysal, J. Saltz, and A. Sussman. </author> <title> Applying the CHAOS/PARTI library to irregular problems in computational chemistry and computational aerodynamics. </title> <booktitle> In Proceedings of the 1993 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 45-56. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: Such codes come from research efforts in areas such as computational chemistry [14] and computational fluid dynamics <ref> [6, 7, 23] </ref>. A significant number of parallelized programs make use of a Single Process Multiple Data (SPMD) model of computation. SPMD programs frequently need to move large sets of data from secondary storage to each processor's primary memory.
Reference: [7] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Such codes come from research efforts in areas such as computational chemistry [14] and computational fluid dynamics <ref> [6, 7, 23] </ref>. A significant number of parallelized programs make use of a Single Process Multiple Data (SPMD) model of computation. SPMD programs frequently need to move large sets of data from secondary storage to each processor's primary memory.
Reference: [8] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: The general model for out-of-core compilation relies on having the compiler generate calls to various runtime libraries to handle both I/O and interprocessor communication. For communication, our techniques have been validated for irregular, block structured and structured data access patterns <ref> [1, 8, 20, 21] </ref>. Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data. The problem then becomes keeping track of the data currently available in memory.
Reference: [9] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3163 and UMIACS-TR-93-109. </note>
Reference-contexts: For arrays, such a distributed array descriptor can contain a logical description of the in-core and out-of-core data distributions and pointers to processor memory for the array. For irregularly distributed data, a structure similar to the CHAOS distributed translation table <ref> [9] </ref> can be employed to describe the data transfer. Access to compact global descriptors can allow an I/O library to skip the collective communication stage that is needed when performing collective local I/O optimization. <p> In that case, the user program is responsible for translating local, in-core, data structure addresses into the correct global addresses for accessing the entire out-of-core data structure (perhaps through calls to a runtime library such as CHAOS <ref> [9] </ref> or Multiblock PARTI [1]). A distributed view for out-of-core data structures greatly simplifies the design and implementation of a collective I/O optimizing library. <p> The amount of savings that can be obtained depends on the format used to describe the data transfer. 6 The description of the data transfer can take on several forms in the context of a global view. For example, a structure similar to the CHAOS distributed translation table <ref> [9] </ref> can be used for specifying irregular accesses, and regular accesses can be specified by regular sections, global index ranges or even blocks. We would like to use the format that will provide the the smallest possible overhead. Regular sections are suitable for regular accesses. <p> Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data. The problem then becomes keeping track of the data currently available in memory. For data that is irregularly distributed across processes, a data structure analogous to the CHAOS distributed translation table <ref> [9] </ref>, can serve to store the locations (process and local address) of data currently available in the local memory of some process. We call this data structure an out-of-core data access descriptor.
Reference: [10] <author> Juan Miguel del Rosario and Alok N. Choudhary. </author> <title> High-performance I/O for massively parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Each I/O process can use this information to determine what to read from disk and what application process to send the data to. There are at least two other projects that perform collective local I/O optimizations. Choudhary <ref> [5, 10] </ref> assumes that each processor has access to either a physical or logical I/O device. They partition parallel I/O into two phases.
Reference: [11] <author> Christos Faloutsos, M. Ranganathan, and Yannis Manolopoulos. </author> <title> Fast subsequence matching in time-series databases. </title> <booktitle> In Proceedings of the 1994 ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3190 and UMIACS-TR-93-131. 16 </note>
Reference-contexts: These applications are being developed as part of the Grand Challenge project in Land Cover Dynamics at the University of Maryland [19]. * Data mining applications for categorizing objects in a database, currently being developed at the University of Maryland <ref> [11] </ref>. * Sparse, adaptive, and block-structured scientific codes with irregular data access patterns and noncontiguous accesses to large data sets on disk. Such codes come from research efforts in areas such as computational chemistry [14] and computational fluid dynamics [6, 7, 23].
Reference: [12] <author> S.K.S. Gupta, S.D. Kaushik, S. Mufti, S. Sharma, C.-H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed memory machines. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Regular section analysis has been studied in detail in the context of compilation for distributed memory machines <ref> [2, 4, 12, 16, 22] </ref>, and such analysis can be applied to accessing out-of-core arrays as well. The analysis is inexpensive for the most common data distributions, so the regular section interface should provide good performance.
Reference: [13] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The least general, but most compact, interface allows for accessing a regular section <ref> [13] </ref> of an out-of-core distributed array, with non-unit strides allowed in each dimension. A range with stride (lower bound index, upper bound index, stride) is the one-dimensional case of a regular section.
Reference: [14] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Bernard Brooks, and Milan Hodoscek. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines: An application of the CHAOS runtime support library. </title> <institution> Technical Report CS-TR-3374 and UMIACS-TR-94-125, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> November </month> <year> 1994. </year> <note> Submitted to IEEE Computational Science and Engineering. </note>
Reference-contexts: Such codes come from research efforts in areas such as computational chemistry <ref> [14] </ref> and computational fluid dynamics [6, 7, 23]. A significant number of parallelized programs make use of a Single Process Multiple Data (SPMD) model of computation. SPMD programs frequently need to move large sets of data from secondary storage to each processor's primary memory.
Reference: [15] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The main factors are communication overhead, memory usage and performance. Another factor that must be considered is the ease of integrating the library into runtime support for a parallelizing compiler, for example for data parallel languages such as High Performance Fortran <ref> [15] </ref> or pC++ [18]. 2.3.1 Communication Overhead Communication overheads will be incurred throughout an I/O library. This is to be expected, since the library uses the interprocessor interconnect to collect data from and distribute data to application processes during I/O operations. <p> Regular sections compactly describe Fortran 90 array notations, and also allow for descriptions of local sections of distributed arrays for regular data distributions in languages such as High Performance Fortran (HPF) <ref> [15] </ref>. I/O requests based on regular sections can be very compact, requiring only three values for each array dimension (low, high, stride), so can minimize the cost of making requests to the I/O system.
Reference: [16] <author> Charles Koelbel. </author> <title> Compile-time generation of regular communication patterns. </title> <booktitle> In Proceedings Supercomputing '91. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1991. </year>
Reference-contexts: Regular section analysis has been studied in detail in the context of compilation for distributed memory machines <ref> [2, 4, 12, 16, 22] </ref>, and such analysis can be applied to accessing out-of-core arrays as well. The analysis is inexpensive for the most common data distributions, so the regular section interface should provide good performance.
Reference: [17] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Department of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: The first phase reads data in a layout that corresponds to the logical disk layout; the second phase then performs in-memory permutations to lay out the data as required. The goal is to do the preprocessing needed so that disk requests access data in large contiguous chunks. Kotz <ref> [17] </ref> modifies this approach by having compute processors pass their disk access requests to I/O processors. In Kotz's approach, the I/O processors are responsible for coordinating the collective requests to the file system.
Reference: [18] <author> A. Malony, B. Mohr, P. Beckman, D. Gannon, S. Yang, F. Bodin, and S. Kesavan. </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 588-597. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: The main factors are communication overhead, memory usage and performance. Another factor that must be considered is the ease of integrating the library into runtime support for a parallelizing compiler, for example for data parallel languages such as High Performance Fortran [15] or pC++ <ref> [18] </ref>. 2.3.1 Communication Overhead Communication overheads will be incurred throughout an I/O library. This is to be expected, since the library uses the interprocessor interconnect to collect data from and distribute data to application processes during I/O operations.
Reference: [19] <author> Rahul Parulekar, Larry Davis, Rama Chellappa, Joel Saltz, Alan Sussman, and John Towhshend. </author> <title> High performance computing for land cover dynamics. </title> <booktitle> In Proceedings of the International Joint Conference on Pattern Recognition, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: These applications are being developed as part of the Grand Challenge project in Land Cover Dynamics at the University of Maryland <ref> [19] </ref>. * Data mining applications for categorizing objects in a database, currently being developed at the University of Maryland [11]. * Sparse, adaptive, and block-structured scientific codes with irregular data access patterns and noncontiguous accesses to large data sets on disk.
Reference: [20] <author> Ravi Ponnusamy, Yuan-Shin Hwang, Joel Saltz, Alok Choudhary, and Geoffrey Fox. </author> <title> Supporting irregular distributions in FORTRAN 90D/HPF compilers. </title> <institution> Technical Report CS-TR-3268 and UMIACS-TR-94-57, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> May </month> <year> 1994. </year> <note> To appear in IEEE Parallel and Distributed Technology, Spring 1995. </note>
Reference-contexts: The general model for out-of-core compilation relies on having the compiler generate calls to various runtime libraries to handle both I/O and interprocessor communication. For communication, our techniques have been validated for irregular, block structured and structured data access patterns <ref> [1, 8, 20, 21] </ref>. Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data. The problem then becomes keeping track of the data currently available in memory.
Reference: [21] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choudhary. </author> <title> Runtime-compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 361-370. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3055 and UMIACS-TR-93-32. </note>
Reference-contexts: The general model for out-of-core compilation relies on having the compiler generate calls to various runtime libraries to handle both I/O and interprocessor communication. For communication, our techniques have been validated for irregular, block structured and structured data access patterns <ref> [1, 8, 20, 21] </ref>. Extending these techniques to handle out-of-core data structures requires using the local memories as "caches" for out-of-core data. The problem then becomes keeping track of the data currently available in memory.
Reference: [22] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1994. To appear. </note>
Reference-contexts: Regular section analysis has been studied in detail in the context of compilation for distributed memory machines <ref> [2, 4, 12, 16, 22] </ref>, and such analysis can be applied to accessing out-of-core arrays as well. The analysis is inexpensive for the most common data distributions, so the regular section interface should provide good performance.
Reference: [23] <author> A. Sussman, J. Saltz, R. Das, S. Gupta, D. Mavriplis, R. Ponnusamy, and K. Crowley. </author> <title> PARTI primitives for unstructured and block structured problems. </title> <booktitle> Computing Systems in Engineering, 3(1-4):73-86, 1992. Papers presented at the Symposium on High-Performance Computing for Flight Vehicles, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Such codes come from research efforts in areas such as computational chemistry [14] and computational fluid dynamics <ref> [6, 7, 23] </ref>. A significant number of parallelized programs make use of a Single Process Multiple Data (SPMD) model of computation. SPMD programs frequently need to move large sets of data from secondary storage to each processor's primary memory.
Reference: [24] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year> <month> 17 </month>
Reference-contexts: Access to compact global descriptors can allow an I/O library to skip the collective communication stage that is needed when performing collective local I/O optimization. The collective global I/O optimizations in such a library will be similar to the optimizations carried out by the Multiblock Parti library <ref> [1, 24] </ref> for interprocessor communication of regularly accessed data. The interface that is provided by an I/O library that implements these optimizations is called a global view. An example of an interface for a global view is illustrated in Figure 1 (a).
References-found: 24

