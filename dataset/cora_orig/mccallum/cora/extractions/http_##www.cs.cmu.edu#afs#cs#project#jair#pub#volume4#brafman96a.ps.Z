URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/brafman96a.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/brafman96a.html
Root-URL: 
Email: brafman@cs.ubc.ca  moshet@ie.technion.ac.il  
Title: On Partially Controlled Multi-Agent Systems  
Author: Ronen I. Brafman Moshe Tennenholtz 
Address: Vancouver, B.C., Canada V6L 1Z4  Haifa 32000, Israel  
Affiliation: Computer Science Department University of British Columbia  Industrial Engineering and Management Technion Israel Institute of Technology  
Note: Journal of Artificial Intelligence Research 4 (1996) 477-507 Submitted 9/95; published 6/96  
Abstract: Motivated by the control theoretic distinction between controllable and uncontrollable events, we distinguish between two types of agents within a multi-agent system: controllable agents, which are directly controlled by the system's designer, and uncontrollable agents, which are not under the designer's direct control. We refer to such systems as partially controlled multi-agent systems, and we investigate how one might influence the behavior of the uncontrolled agents through appropriate design of the controlled agents. In particular, we wish to understand which problems are naturally described in these terms, what methods can be applied to influence the uncontrollable agents, the effectiveness of such methods, and whether similar methods work across different domains. Using a game-theoretic framework, this paper studies the design of partially controlled multi-agent systems in two contexts: in one context, the uncontrollable agents are expected utility maximizers, while in the other they are reinforcement learners. We suggest different techniques for controlling agents' behavior in each domain, assess their success, and examine their relationship.
Abstract-found: 1
Intro-found: 1
Reference: <author> Altenberg, L., & Feldman, M. W. </author> <year> (1987). </year> <title> Selection, generalized transmission, and the evolution of modifier genes. i. the reduction principle. </title> <booktitle> Genetics, </booktitle> <pages> 559-572. </pages>
Reference: <author> Bellman, R. </author> <year> (1962). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: In this section we start exploring the question of how a teacher should teach. First, we define what an optimal policy is. Then, we will define Markov decision processes (MDP) <ref> (Bellman, 1962) </ref>, and show that under certain assumptions teaching can be viewed as an MDP. This will allow us to tap into the vast knowledge that has accumulated on solving these problems. In particular, we can use well known methods, such as value iteration (Bellman, 1962), to find the optimal teaching <p> will define Markov decision processes (MDP) <ref> (Bellman, 1962) </ref>, and show that under certain assumptions teaching can be viewed as an MDP. This will allow us to tap into the vast knowledge that has accumulated on solving these problems. In particular, we can use well known methods, such as value iteration (Bellman, 1962), to find the optimal teaching policy. We start by defining an optimal teaching policy. A teaching policy is a function that returns an action at each iteration; possibly, it may depend on a complete history of the past joint actions. <p> In the experiments below we use a method based on value-iteration <ref> (Bellman, 1962) </ref>. Now suppose that the student can be in a set of possible states, that his set of actions is A s , and that the teacher's set of actions is A t .
Reference: <author> Bond, A. H., & Gasser, L. </author> <year> (1988). </year> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Ablex Publishing Corporation. </publisher>
Reference-contexts: 1. Introduction The control of agents is a central research topic in two engineering fields: Artificial Intelligence (AI) and Discrete Events Systems (DES) (Ramadge & Wonham, 1989). One particular area both of these fields have been concerned with is multi-agent environments; examples include work in distributed AI <ref> (Bond & Gasser, 1988) </ref>, and work on decentralized supervisory control (Lin & Wonham, 1988). Each of these fields has developed its own techniques and has incorporated particular assumptions into its models. <p> Despite its somewhat futuristic flavor (although instances of such shared environments are beginning to appear in cyberspace), this scenario is useful in illustrating the vulnerability of some of the most popular coordination mechanism appearing in the multi-agent literature within AI <ref> (e.g., see Bond & Gasser, 1988) </ref> when we assume that the agents involved are fully rational.
Reference: <author> Briggs, W., & Cook, D. </author> <year> (1995). </year> <title> Flexible Social Laws. </title> <booktitle> In Proc. 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 688-693. </pages>
Reference: <author> Dixit, A. K., & Nalebuff, B. J. </author> <year> (1991). </year> <title> Thinking strategically : the competitive edge in business, politics, and everyday life. </title> <publisher> Norton, </publisher> <address> New York. </address>
Reference: <author> Durfee, E. H., Lesser, V. R., & Corkill, D. D. </author> <year> (1987). </year> <title> Coherent Cooperation Among Communicating Problem Solvers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36, </volume> <pages> 1275-1291. </pages>
Reference: <author> Dwork, C., & Moses, Y. </author> <year> (1990). </year> <title> Knowledge and Common Knowledge in a Byzantine Environment: Crash Failures. </title> <journal> Information and Computation, </journal> <volume> 88 (2), </volume> <pages> 156-186. </pages>
Reference-contexts: Several approaches for coordination of agent activity are discussed in the distributed systems and the DAI literature. Some examples are: protocols for reaching consensus <ref> (Dwork & Moses, 1990) </ref>, rational deals and negotiations (Zlotkin & Rosenschein, 1993; Kraus & Wilkenfeld, 1991; Rosenschein & Genesereth, 1985), organizational structures (Durfee, Lesser, & Corkill, 1987; Fox, 1981; Malone, 1987), and social laws (Moses & Tennenholtz, 1995; Shoham & Tennenholtz, 1995; Minsky, 1991; Briggs & Cook, 1995).
Reference: <editor> Eatwell, J., Milgate, M., & Newman, P. (Eds.). </editor> <year> (1989). </year> <title> The New Palgrave: Game Theory. </title>
Reference-contexts: The prisoner's dilemma captures the essence of many important social and economic situations; in particular, it encapsulates the notion of cooperation. It has thus motivated enormous discussion among game-theorists and mathematical economists <ref> (for an overview, see Eatwell, Milgate, & Newman, 1989) </ref>. In the prisoner's dilemma, whatever the choice of one player, the second player can maximize its payoff by playing Defect. It thus seems "rational" for each player to defect. <p> Similarly, the teacher will follow a Defect by the student with a Defect on her 495 Brafman & Tennenholtz scheme of Section 8.1. Teaching strategies shown: approximately optimal strat egy, Q-learning, TFT, and 2TFT. part. This strategy, called Tit-For-Tat (TFT for short), is well known <ref> (Eatwell et al., 1989) </ref>. Our experiments show that it is not very successful in teaching a BQL (see Figure 3). We also experimented with a variant of TFT, which we call 2TFT. <p> Within game theory there is an extensive body of work that tries to understand the evolution of cooperation in the iterated prisoner's dilemma and to find good playing strategies for it <ref> (Eatwell et al., 1989) </ref>. In that work both players have the same knowledge, and teaching is not an issue. Last but not least, our work has important links to work on conditioning and especially operant conditioning in psychology (Mackintosh, 1983).
Reference: <institution> W.W.Norton & Company, Inc. </institution>
Reference: <author> Fox, M. S. </author> <year> (1981). </year> <title> An organizational view of distributed systems. </title> <journal> IEEE Trans. Sys., Man., Cyber., </journal> <volume> 11, </volume> <pages> 70-80. </pages>
Reference: <author> Fudenberg, D., & Tirole, J. </author> <year> (1991). </year> <title> Game Theory. </title> <publisher> MIT Press. </publisher>
Reference: <author> Gilboa, I., & Matsui, A. </author> <year> (1991). </year> <title> Social stability and equilibrium. </title> <journal> Econometrica, </journal> <volume> 59 (3), </volume> <pages> 859-867. </pages>
Reference-contexts: Indeed, these examples constitute the two central models of self-motivated agents in game theory and decision theory, referred to as the educative and evolutive models <ref> (e.g., see Gilboa & Matsui, 1991) </ref>. The special nature of the uncontrollable agents and the special structure of the uncontrollable events they induce is what differentiates PCMAS from corresponding models in the DES literature. This difference raises new questions and suggests a new perspective on the design of multi-agent systems.
Reference: <author> Gold, M. </author> <year> (1978). </year> <title> Complexity of Automaton Identificaion from Given Data. </title> <journal> Information and Control, </journal> <volume> 37, </volume> <pages> 302-320. </pages>
Reference-contexts: An agent's ability to function in an environment is greatly affected by its knowledge of the environment. In some special cases, we can design agents with sufficient knowledge for performing a task <ref> (Gold, 1978) </ref>, but, in general, agents must acquire information on-line in order to optimize their performance, i.e., they must learn. One possible approach to improving the performance of learning algorithms is employing a teacher.
Reference: <author> Huberman, B. A., & Hogg, T. </author> <year> (1988). </year> <title> The Behavior of Computational Ecologies. </title> <editor> In Huberman, B. A. (Ed.), </editor> <booktitle> The Ecology of Computation. </booktitle> <publisher> Elsevier Science. </publisher>
Reference: <author> Kaelbling, L. </author> <year> (1990). </year> <title> Learning in embedded systems. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution> <note> 505 Brafman & Tennenholtz Kandori, </note> <author> M., Mailath, G., & Rob, R. </author> <year> (1991). </year> <title> Learning, Mutation and Long Equilibria in Games. </title> <institution> Mimeo. University of Pennsylvania, </institution> <year> 1991. </year>
Reference: <author> Kinderman, R., & Snell, S. L. </author> <year> (1980). </year> <title> Markov Random Fields and their Applications. </title> <publisher> American Mathematical Society. </publisher>
Reference: <author> Kittock, J. E. </author> <year> (1994). </year> <title> The impact of locality and authority on emergent conventions. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI '94), </booktitle> <pages> pp. 420-425. </pages>
Reference: <author> Kraus, S., & Wilkenfeld, J. </author> <year> (1991). </year> <title> The Function of Time in Cooperative Negotiations. </title> <booktitle> In Proc. of AAAI-91, </booktitle> <pages> pp. 179-184. </pages>
Reference: <author> Lin, F., & Wonham, W. </author> <year> (1988). </year> <title> Decentralized control and coordination of discrete-event systems. </title> <booktitle> In Proceedings of the 27th IEEE Conf. Decision and Control, </booktitle> <pages> pp. 1125-1130. </pages>
Reference-contexts: One particular area both of these fields have been concerned with is multi-agent environments; examples include work in distributed AI (Bond & Gasser, 1988), and work on decentralized supervisory control <ref> (Lin & Wonham, 1988) </ref>. Each of these fields has developed its own techniques and has incorporated particular assumptions into its models. Hence, it is only natural that techniques and assumptions used by one field may be adopted by the other or may lead to new insights for the other field.
Reference: <author> Lin, L. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <booktitle> Machine Learning, </booktitle> <pages> 8 (3-4). </pages>
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proc. of the 11th Int. Conf. on Mach. Learn. </booktitle>
Reference: <author> Luce, R. D., & Raiffa, H. </author> <year> (1957). </year> <title> Games and Decisions- Introduction and Critical Survey. </title> <publisher> John Wiley and Sons. </publisher>
Reference: <author> Mackintosh, N. </author> <year> (1983). </year> <title> Conditioning and Associative Learning. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: In that work both players have the same knowledge, and teaching is not an issue. Last but not least, our work has important links to work on conditioning and especially operant conditioning in psychology <ref> (Mackintosh, 1983) </ref>. In conditioning experiments an experimenter tries to induce changes in its subjects by arranging that certain relationships will hold in their environment, or by explicitly (in operant conditioning) reinforcing the subjects' actions. In our framework the controlled agent plays a similar role to that of the experimenter.
Reference: <author> Malone, T. W. </author> <year> (1987). </year> <title> Modeling Coordination in Organizations and Markets. </title> <journal> Management Science, </journal> <volume> 33 (10), </volume> <pages> 1317-1332. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1995). </year> <title> Reward Functions for Accelerating Learning. </title> <booktitle> In Proceedings of the 11th international conference on Machine Learning, </booktitle> <pages> pp. 181-189. </pages>
Reference: <author> Minsky, N. </author> <year> (1991). </year> <title> The imposition of protocols over open distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17 (2), </volume> <pages> 183-195. </pages>
Reference: <author> Moses, Y., & Tennenholtz, M. </author> <year> (1995). </year> <journal> Artificial social systems. Computers and Artificial Intelligence, </journal> <volume> 14 (6), </volume> <pages> 533-562. </pages>
Reference: <author> Narendra, K., & Thathachar, M. A. L. </author> <year> (1989). </year> <title> Learning Automata: An Introduction. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: We wish to emphasize that although BQL is a bit less sophisticated than "real" reinforcement learners discussed in the AI literature (which is defined below), it is a popular and powerful type of learning rule, which is much discussed and used in the literature <ref> (Narendra & Thathachar, 1989) </ref>. The second student is a Q-learner (QL). He can observe the teacher's actions and has a number of possible states. The QL maintains a Q-value for each state-action pair. His states encode his recent experiences, i.e., the past joint actions.
Reference: <author> Owen, G. </author> <year> (1982). </year> <title> Game Theory (2nd Ed.). </title> <publisher> Academic Press. </publisher>
Reference: <author> Parker, L. E. </author> <year> (1993). </year> <title> Learning in Cooperative Robot Teams. </title> <booktitle> In Proceedings of IJCAI-93 Workshop on Dynamically Interacting Robots. </booktitle>
Reference: <author> Ramadge, P., & Wonham, W. </author> <year> (1989). </year> <title> The Control of Discrete Event Systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77 (1), </volume> <pages> 81-98. </pages>
Reference-contexts: 1. Introduction The control of agents is a central research topic in two engineering fields: Artificial Intelligence (AI) and Discrete Events Systems (DES) <ref> (Ramadge & Wonham, 1989) </ref>. One particular area both of these fields have been concerned with is multi-agent environments; examples include work in distributed AI (Bond & Gasser, 1988), and work on decentralized supervisory control (Lin & Wonham, 1988).
Reference: <author> Rosenschein, J. S., & Genesereth, M. R. </author> <year> (1985). </year> <title> Deals Among Rational Agents. </title> <booktitle> In Proc. 9th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <month> 91-99. </month> <title> 506 On Partially Controlled Multi-Agent Systems Schelling, </title> <editor> T. </editor> <year> (1980). </year> <title> The Strategy of Conflict. </title> <publisher> Harvard University Press. </publisher>
Reference: <author> Sen, S., Sekaran, M., & Hale, J. </author> <year> (1994). </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proc. of AAAI-94, </booktitle> <pages> pp. 426-431. </pages>
Reference: <author> Shoham, Y., & Tennenholtz, M. </author> <year> (1992). </year> <title> Emergent Conventions in Multi-Agent Systems: initial experimental results and observations. </title> <booktitle> In Proc. of the 3rd International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pp. 225-231. </pages>
Reference: <author> Shoham, Y., & Tennenholtz, M. </author> <year> (1995). </year> <title> Social Laws for Artificial Agent Societies: Off-line Design. </title> <journal> Artificial Inteligence, </journal> <volume> 73. </volume>
Reference: <author> Sondik, E. J. </author> <year> (1978). </year> <title> The optimal control of partially observable markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26 (2). </volume>
Reference-contexts: When these assumptions are invalid, we obtain a partially observable Markov decision process (POMDP) <ref> (Sondik, 1978) </ref>. Unfortunately, although POMDPs can be used in principle to obtain the ideal policy for our agents, current techniques for solving POMDPs are limited to very small problems.
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by method of temporal differences. </title> <journal> Mach. Lear., </journal> <volume> 3 (1), </volume> <pages> 9-44. </pages>
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents. In Proceedings of the 10th International Conference on Machine Learning. </booktitle>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning With Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> Cambridge University. </institution>
Reference: <author> Watkins, C., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 279-292. </pages>
Reference-contexts: For a QL with memory one or more, the problem is a fully observable Markov decision process once the teacher plays TFT, because TFT is a deterministic function of the previous joint action. We know that Q-learning converges to the optimal policy under such conditions <ref> (Watkins & Dayan, 1992) </ref>. Adding more memory effectively adds irrelevant attributes, which, in turn, causes a slower learning rate. We have also examined whether 2TFT would be successful when agents have a memory of two.
Reference: <author> Weidlich, W., & Haag, G. </author> <year> (1983). </year> <title> Concepts and Models of a Quantitative Sociology; The Dynamics of Interacting Populations. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Whitehead, S. </author> <year> (1991). </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <pages> pp. 607-613. </pages>
Reference: <author> Yanco, H., & Stein, L. </author> <year> (1993). </year> <title> An Adaptive Communication Protocol for Cooperating Mobile Robots. In From Animal to Animats: </title> <booktitle> Proceedings of the Second International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> pp. 478-485. </pages>
Reference: <author> Zlotkin, G., & Rosenschein, J. S. </author> <year> (1993). </year> <title> A Domain Theory for Task Oriented Negotiation. </title> <booktitle> In Proc. 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 416-422. 507 </pages>
References-found: 44

