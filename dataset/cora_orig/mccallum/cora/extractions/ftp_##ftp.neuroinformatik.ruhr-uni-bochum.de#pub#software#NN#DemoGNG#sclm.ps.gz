URL: ftp://ftp.neuroinformatik.ruhr-uni-bochum.de/pub/software/NN/DemoGNG/sclm.ps.gz
Refering-URL: http://neural-server.aston.ac.uk/NN/software.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Some Competitive Learning Methods  (Some additions and refinements are planned for  
Author: Bernd Fritzke 
Date: April 5, 1997  
Note: Draft from  this document so it will stay in the draft status still for a while.) Comments are welcome.  
Address: Ruhr-Universitat Bochum  
Affiliation: Systems Biophysics Institute for Neural Computation  
Abstract-found: 0
Intro-found: 1
Reference: <author> H.-U. Bauer and K. Pawelzik. </author> <title> Quantifying the neighborhood preservation of self-organizing feature maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(4) </volume> <pages> 570-579, </pages> <year> 1992. </year>
Reference-contexts: A related question is, how topology-preserving is the mapping from the input data space onto the discrete network structure, i.e. how well are similarities preserved? Several quantitative measures have been proposed to evaluate this like the topographic product <ref> (Bauer and Pawelzik, 1992) </ref> or the topographic function (Villmann et al., 1994). 3.4. OTHER GOALS 9 3.4 Other Goals Competitive learning methods can also be used for density estimation, i.e. for the generation of an estimate for the unknown probability density p (~) of the input signals.
Reference: <author> H.-U. Bauer and T. Villmann. </author> <title> Growing a hypercubical output space in a self-organizing feature map. </title> <institution> Tr-95-030, International Computer Science Institute, Berkeley, </institution> <year> 1995. </year>
Reference: <author> J. Blackmore and R. Miikkulainen. </author> <title> Incremental grid growing: encoding high-dimensional structure into a two-dimensional feature map. </title> <type> TR AI92-192, </type> <institution> University of Texas at Austin, Austin, TX, </institution> <year> 1992. </year>
Reference: <author> C. Darken and J. Moody. </author> <title> Fast adaptive k-means clustering: Some empirical results. </title> <booktitle> In Proc. IJCNN, </booktitle> <volume> volume II, </volume> <pages> pages 233-238. </pages> <booktitle> IEEE Neural Networks Council, </booktitle> <year> 1990. </year>
Reference: <author> D. DeSieno. </author> <title> Adding a conscience to competitive learning. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 117-124, </pages> <address> New York, 1988. (San Diego 1988) IEEE. </address>
Reference: <author> E. Forgy. </author> <title> Cluster analysis of multivariate data: efficiency vs. interpretanility of classifications. </title> <journal> Biometrics, </journal> <note> 21:768, 1965. abstract. </note>
Reference: <author> B. Fritzke. </author> <title> Growing cell structures a self-organizing network for unsupervised and supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(9) </volume> <pages> 1441-1460, </pages> <year> 1994a. </year>
Reference-contexts: The network size N was set to 100. 5.4 Growing Neural Gas This method (Fritzke, 1994b, 1995a) is different from the previously described models since the number of units is changed (mostly increased) during the self-organization process. The growth mechanism from the earlier proposed growing cell structures <ref> (Fritzke, 1994a) </ref> and the topology generation of competitive Hebbian learning (Martinetz and Schulten, 1991) are combined to a new model. Starting with very few units new units are inserted successively. To determine where to insert new units, local error measures are gathered during the adaptation process. <p> Figure 6.2 displays the final results after 40000 adaptation steps for three other distribution. The parameters were i = 3:0; f = 0:1; * i = 0:5; * f = 0:005; t max = 10000 and N 1 = N 2 = 10. 6.2 Growing Cell Structures This model <ref> (Fritzke, 1994a) </ref> is rather similar to the growing neural gas model 1 . The main difference is that the network topology is constrained to consist of k-dimensional simplices whereby k is some positive integer chosen in advance.
Reference: <author> B. Fritzke. </author> <title> Fast learning with incremental RBF networks. </title> <journal> Neural Processing Letters, </journal> <volume> 1(1) </volume> <pages> 2-5, </pages> <year> 1994b. </year>
Reference-contexts: The network size N was set to 100. 5.4 Growing Neural Gas This method <ref> (Fritzke, 1994b, 1995a) </ref> is different from the previously described models since the number of units is changed (mostly increased) during the self-organization process.
Reference: <author> B. Fritzke. </author> <title> A growing neural gas network learns topologies. </title> <editor> In G. Tesauro, D. </editor> <publisher> S. </publisher>
Reference: <editor> Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 625-632. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995a. </year>
Reference: <author> B. Fritzke. </author> <title> Incremental learning of local linear mappings. </title> <editor> In F. Fogelman and P. Gallinari, editors, ICANN'95: </editor> <booktitle> International Conference on Artificial Neural Networks, </booktitle> <pages> pages 217-222, </pages> <address> Paris, France, </address> <year> 1995b. </year> <note> EC2 & Cie. </note>
Reference: <author> B. Fritzke. </author> <title> The LBG-U method for vector quantization an improvement over LBG inspired from neural networks. </title> <journal> Neural Processing Letters, </journal> <volume> 5(1), </volume> <year> 1997. </year>
Reference-contexts: LBG is guaranteed to converge in a finite number of Lloyd iterations to a local minimum of the distortion error function (see figure 4.1 for an example). An extension of LBG, called LBG-U <ref> (Fritzke, 1997) </ref>, is often able to improve on the local minima found by LBG. LBG-U performs non-local moves of single reference vectors which do not contribute much to error reduction (and are, therefore, not useful, thus the "U" in LBG-U) to locations where large quantization error does occur.
Reference: <author> R. M. Gray. </author> <title> Vector quantization. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 1 </volume> <pages> 4-29, </pages> <year> 1984. </year>
Reference: <author> R. M. Gray. </author> <title> Vector Quantization and Signal Compression. </title> <publisher> Kluwer Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: The theoretical foundation for this is that it can be shown 10 4.2. ON-LINE UPDATE: BASIC ALGORITHM 11 <ref> (Gray, 1992) </ref> that a necessary condition for a set of reference vectors fw c jc 2 Ag to minimize the distortion error E (D; A) = 1=jDj c2A ~2R c is that each reference vector w c fulfills the centroid condition.
Reference: <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for clustering data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year> <title> 42 BIBLIOGRAPHY 43 S. Jokusch. A neural network which adapts its structure to a given set of patterns. </title>
Reference: <editor> In R. Eckmiller, G. Hartmann, and G. Hauske, editors, </editor> <booktitle> Parallel Processing in Neural Systems and Computers, </booktitle> <pages> pages 169-172. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1990. </year>
Reference: <author> J. A. Kangas, T. Kohonen, and T. Laaksonen. </author> <title> Variants of self-organizing maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(1) </volume> <pages> 93-99, </pages> <year> 1990. </year>
Reference: <author> S. Kirkpatrick, C. D. G. Jr., , and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220, </volume> <year> 1983. </year>
Reference-contexts: In particular at the beginning of the simulation the exponentially decaying learning rate is considerably larger than that dictated by the harmonic series. This can be interpreted as introducing noise to the system which is then gradually removed and, therefore, suggests a relationship to simulated annealing techniques <ref> (Kirkpatrick et al., 1983) </ref>. Simulated annealing gives a system the ability to escape from poor local minima to which it might have been initialized.
Reference: <author> T. Kohonen. </author> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference: <author> Y. Linde, A. Buzo, and R. M. Gray. </author> <title> An algorithm for vector quantizer design. </title> <journal> IEEE Transactions on Communication, </journal> <volume> COM-28:84-95, </volume> <year> 1980. </year>
Reference: <author> S. P. Lloyd. </author> <title> Least squares quantization in pcm. </title> <type> technical note, </type> <institution> Bell Laboratories, </institution> <year> 1957. </year> <note> published in 1982 in IEEE Transactions on Information Theory. </note>
Reference: <author> J. MacQueen. </author> <title> On convergence of k-means and partitions with minimum average variance. </title> <journal> Ann. Math. Statist., </journal> <note> 36:1084, 1965. abstract. </note>
Reference-contexts: In fact, it has been shown that k-means does converge asymptot ically to a configuration where each reference vector w c is positioned such that it coincides with the expectation value E (~j~ 2 V c ) = V c of its Voronoi region V c <ref> (MacQueen, 1965) </ref>. One can note that (4.17) is the continuous variant of the centroid condition (4.2). Figure 4.5 shows some stages of a simulation for a simple ring-shaped data distribution.
Reference: <author> J. MacQueen. </author> <title> Some methods for classification and analysis of multivariate observations. </title> <booktitle> volume 1 of Proceedings of the Fifth Berkeley Symposium on Mathematical statistics and probability, </booktitle> <pages> pages 281-297, </pages> <address> Berkeley, </address> <year> 1967. </year> <institution> University of California Press. </institution>
Reference-contexts: This algorithm is known as k-means <ref> (MacQueen, 1967) </ref>, which is a rather appropriate name, because each reference vector w c (t) is always the exact arithmetic mean of the input signals ~ c 1 ; ~ c t it has been winner for so far.
Reference: <author> T. M. Martinetz. </author> <title> Competitive Hebbian learning rule forms perfectly topology preserving maps. </title> <booktitle> In ICANN'93: International Conference on Artificial Neural Networks, </booktitle> <pages> pages 427-434, </pages> <address> Amsterdam, 1993. </address> <publisher> Springer. </publisher>
Reference: <author> T. M. Martinetz, S. G. Berkovich, and K. J. Schulten. </author> <title> Neural-gas network for vector quantization and its application to time-series prediction. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(4) </volume> <pages> 558-569, </pages> <year> 1993. </year>
Reference: <author> T. M. Martinetz, H. J. Ritter, and K. J. Schulten. </author> <title> 3D-neural-network for learning visuomotor-coordination of a robot arm. </title> <booktitle> In International Joint Conference on Neural Networks, pages II.351-356, </booktitle> <address> Washington DC, </address> <year> 1989. </year>
Reference: <author> T. M. Martinetz and K. J. Schulten. </author> <title> A "neural-gas" network learns topologies. </title> <editor> In T. Kohonen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 397-402. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: In one case there is no topology at all (neural gas). In other cases the dimensionality of the network depends on the local dimensionality of the data and may vary within the input space. 5.1 Neural Gas The neural gas algorithm <ref> (Martinetz and Schulten, 1991) </ref> sorts for each input signal ~ the units of the network according to the distance of their reference vectors to ~. Based on this "rank order" a certain number of units is adapted. <p> Obviously, the method is sensitive to initialization since the initial positions are always equal to the final positions. a) b) c) 4.4). 24 CHAPTER 5. SCL W/O FIXED NETWORK DIMENSIONALITY 5.3 Neural Gas plus Competitive Hebbian Learn ing This method <ref> (Martinetz and Schulten, 1991, 1994) </ref> is a straight-forward superposition of neural gas and competitive Hebbian learning. It is sometimes denoted as "topology-representing networks" (Martinetz and Schulten, 1994). This term, however, is rather general and would apply also to the growing neural gas model described later. <p> The growth mechanism from the earlier proposed growing cell structures (Fritzke, 1994a) and the topology generation of competitive Hebbian learning <ref> (Martinetz and Schulten, 1991) </ref> are combined to a new model. Starting with very few units new units are inserted successively. To determine where to insert new units, local error measures are gathered during the adaptation process. Each new unit is inserted near the unit which has accumulated most error.
Reference: <author> T. M. Martinetz and K. J. Schulten. </author> <title> Topology representing networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(3) </volume> <pages> 507-522, </pages> <year> 1994. </year>
Reference-contexts: SCL W/O FIXED NETWORK DIMENSIONALITY 5.3 Neural Gas plus Competitive Hebbian Learn ing This method (Martinetz and Schulten, 1991, 1994) is a straight-forward superposition of neural gas and competitive Hebbian learning. It is sometimes denoted as "topology-representing networks" <ref> (Martinetz and Schulten, 1994) </ref>. This term, however, is rather general and would apply also to the growing neural gas model described later. At each adaptation step a connection between the winner and the second-nearest unit is created (this is competitive Hebbian learning).
Reference: <author> J. E. Moody and C. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference: <author> S. M. Omohundro. </author> <title> The Delaunay triangulation and function learning. </title> <institution> Tr-90-001, International Computer Science Institute, Berkeley, </institution> <year> 1990. </year>
Reference-contexts: It is, e.g., the only triangulation in which the circumcircle of each triangle contains no other point from the original point set than the vertices of this triangle. Moreover, the Delaunay triangulation has been shown to be optimal for function interpolation <ref> (Omohundro, 1990) </ref>. The competitive Hebbian learning method (see section 5.2) generates a subgraph of the Delaunay triangulation which is limited to those areas of the input space where data is found.
Reference: <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational geometry. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Efficient algorithms to compute it are only known for two-dimensional data sets <ref> (Preparata and Shamos, 1990) </ref>. The concept itself, however, is applicable to spaces of arbitrarily high dimensions. If one connects all pairs of points for which the respective Voronoi regions share an edge (an (n 1)-dimensional hyperface for spaces of dimension n) one gets the Delaunay Triangulation (see figure 2.1 c).
Reference: <author> H. J. Ritter, T. M. Martinetz, and K. J. </author> <title> Schulten. </title> <publisher> Neuronale Netze. Addison-Wesley, </publisher> <address> Munchen, </address> <year> 1991. </year> <note> 44 BIBLIOGRAPHY J. </note> <author> S. Rodrigues and L. B. Almeida. </author> <title> Improving the learning speed in topological maps of patterns. </title> <booktitle> In Proceedings of INNC, </booktitle> <pages> pages 813-816, </pages> <address> Paris, </address> <year> 1990. </year>
Reference: <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In R. D. E. and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Between the units of the network there exists a (possibly empty) set C A fi A (2.3) of neighborhood connections which are unweighted and symmetric: (i; j) 2 C () (j; i) 2 C: (2.4) These connections have nothing to do with the weighted connections found, e.g., in multi-layer perceptrons <ref> (Rumelhart et al., 1986) </ref>. They are used in some methods to extend the adaptation of the winner (see below) to some of its topological neighbors.
Reference: <author> T. Villmann, R. Der, M. Herrmann, and T. Martinetz. </author> <title> Topology presevation in self-organizing feature maps: exact definition and measurement. </title> <journal> IEEE TNN, </journal> <note> 1994. submitted. </note>
Reference-contexts: A related question is, how topology-preserving is the mapping from the input data space onto the discrete network structure, i.e. how well are similarities preserved? Several quantitative measures have been proposed to evaluate this like the topographic product (Bauer and Pawelzik, 1992) or the topographic function <ref> (Villmann et al., 1994) </ref>. 3.4. OTHER GOALS 9 3.4 Other Goals Competitive learning methods can also be used for density estimation, i.e. for the generation of an estimate for the unknown probability density p (~) of the input signals.
Reference: <author> J. Walter, H. J. Ritter, and K. J. Schulten. </author> <title> Non-linear prediction with self-organizing maps. </title> <booktitle> In International Joint Conference on Neural Networks, pages I.589-594, </booktitle> <address> San Diego, </address> <year> 1990. </year>
Reference: <author> D. J. Willshaw and C. von der Malsburg. </author> <title> How patterned neural connections can be set up by self-organization. </title> <journal> In Proceedings of the Royal Society London, </journal> <volume> volume B194, </volume> <pages> pages 431-445, </pages> <year> 1976. </year>
Reference: <author> L. Xu. </author> <title> Adding learning expectation into the learning procedure of self-organizing maps. </title> <journal> Int. Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 269-283, </pages> <year> 1990. </year>
References-found: 37

