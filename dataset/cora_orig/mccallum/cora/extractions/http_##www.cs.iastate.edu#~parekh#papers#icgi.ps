URL: http://www.cs.iastate.edu/~parekh/papers/icgi.ps
Refering-URL: http://www.cs.iastate.edu/~parekh/resume.html
Root-URL: 
Email: fparekh|honavarg@cs.iastate.edu  
Title: An Incremental Interactive Algorithm for Regular Grammar Inference  
Author: Rajesh Parekh Vasant Honavar 
Address: Ames, IA 50011. U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science Iowa State University,  
Abstract: We present provably correct interactive algorithms for learning regular grammars from positive examples and membership queries. A structurally complete set of strings from a language L(G) corresponding to a target regular grammar G implicitly specifies a lattice of finite state automata (FSA) which contains a FSA M G corresponding to G. The lattice is compactly represented as a version-space and M G is identified by searching the version-space using membership queries. We explore the problem of regular grammar inference in a setting where positive examples are provided intermittently. We provide an incremental version of the algorithm along with a set of sufficient conditions for its convergence.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Angluin, D. </author> <title> A Note on the Number of Queries Needed to Identify Regular Languages. </title> <journal> Information and control, </journal> <volume> 51. '81. </volume> <pages> pp 76-87. </pages>
Reference-contexts: The set G in our algorithm, which represents the set of most general FSA of the lattice that do not accept any negative strings identified by the queries during the inference process is analogous to the border set described by Dupont et al [4]. Angluin <ref> [1] </ref> has proposed an algorithm (ID) to infer the target grammar from a live complete set of examples (which can be constructed from a structurally complete set) using a polynomial number of membership queries.
Reference: 2. <author> Angluin, D. </author> <title> Learning Regular Sets from Queries and Counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75. '87. </volume> <pages> pp 87-106. </pages>
Reference-contexts: Our algorithm does not store the previous examples and is guaranteed to converge to the desired target instead of a set of candidate solutions as is the case for VanLehn and Ball's method. Angluin <ref> [2] </ref> has proposed a polynomial time algorithm (L fl ), which allows the learner to infer the target grammar by posing both membership and equivalence queries. The L fl procedure can be adapted to the PAC learning framework to learn from membership queries and examples alone.
Reference: 3. <author> Biermann, A., and Feldman, J. </author> <title> A Survey of Results in Grammatical Inference. </title> <editor> In Watanabe S. (ed), </editor> <booktitle> Frontiers of Pattern Recognition. </booktitle> <publisher> Academic Press. </publisher> <pages> '72. pp. 31-54. </pages>
Reference-contexts: 1 Introduction Regular Grammar Inference <ref> [3, 5, 9, 12] </ref> is an important machine learning problem with applications in pattern recognition and language acquisition.
Reference: 4. <author> Dupont, P., Miclet, L., and Vidal, E. </author> <title> What is the Search Space of the Regular Inference?. </title> <booktitle> In Proceedings of the ICGI-94, </booktitle> <address> Alicante, Spain, </address> <month> Sept. </month> <pages> '94. pp. 25-37. </pages>
Reference-contexts: The teacher provides a structurally complete set S + which implicitly defines a lattice (or version space) of candidate FSA or the initial hypothesis space that is guaranteed to contain a FSA (M G ) corresponding to the target grammar (G) <ref> [14, 15, 4] </ref>. At all times, the learner maintains two sets of lattice elements | S and G | which correspond respectively to the most specific and most general FSA consistent with the data (sample strings, queries) processed so far. <p> It was also independently proven by Miclet (see <ref> [4] </ref>). Suppose a set S + 0 of positive examples that is not necessarily structurally complete is provided at the start. <p> The set G in our algorithm, which represents the set of most general FSA of the lattice that do not accept any negative strings identified by the queries during the inference process is analogous to the border set described by Dupont et al <ref> [4] </ref>. Angluin [1] has proposed an algorithm (ID) to infer the target grammar from a live complete set of examples (which can be constructed from a structurally complete set) using a polynomial number of membership queries.
Reference: 5. <author> Fu, K. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> N.J. </address> <month> '82. </month>
Reference-contexts: 1 Introduction Regular Grammar Inference <ref> [3, 5, 9, 12] </ref> is an important machine learning problem with applications in pattern recognition and language acquisition.
Reference: 6. <author> Giles, C., Chen, D., Miller, H., Sun, G., and Lee, Y. </author> <title> Second-order Recurrent Neural Networks for Grammatical Inference. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks 91, </booktitle> <volume> vol. 2, </volume> <pages> pp. 273-281, </pages> <month> July '91. </month>
Reference-contexts: The L fl procedure can be adapted to the PAC learning framework to learn from membership queries and examples alone. Rivest and Schapire [18] have suggested a diversity based mechanism dealing with homing sequences. Giles et al <ref> [6] </ref> use recurrent neural networks to learn FSA from positive and negative samples. Lankhorst [11] has presented a genetic algorithms based approach for learning context free grammars.
Reference: 7. <author> Harrison, M. </author> <title> Introduction to Switching and Automata Theory. </title> <publisher> McGraw-Hill, </publisher> <pages> '65. </pages>
Reference-contexts: 0 ; Cg where, W = S fi T, w 0 = (s 0 ; t 0 ) , ffi w ((s,t); ) = (ffi s (s; ); ffi t (t; )) for all 2 and C = f (s,t) j s 2 A and t 2 T - Bg <ref> [7] </ref> forms the query "y 2 L (G)"? that is posed to the teacher. Based on the teacher's response fi is pruned and elements of S and G become progressively more general and more specific respectively.
Reference: 8. <author> Hirsh, H. </author> <title> Incremental Version-Space Merging: A General Framework for Concept Learning. </title> <publisher> Kluwer Academic Publishers, </publisher> <pages> '90. </pages>
Reference-contexts: When the structurally complete sample set has been acquired, an example that is not accepted by all FSA in G k can be classified as negative. The idea of incremental lattice update was inspired by Hirsh's work on Incremental Version-Space Merging <ref> [8] </ref>. The set G in our algorithm, which represents the set of most general FSA of the lattice that do not accept any negative strings identified by the queries during the inference process is analogous to the border set described by Dupont et al [4].
Reference: 9. <author> Honavar, V. </author> <title> Toward Learning Systems That Integrate Different Strategies and Representations. In: Artificial Intelligence and Neural Networks: Steps toward Principled Integration. Honavar, </title> <editor> V. & Uhr, L. </editor> <address> (eds) New York: </address> <publisher> Academic Press, </publisher> <pages> '94. </pages>
Reference-contexts: 1 Introduction Regular Grammar Inference <ref> [3, 5, 9, 12] </ref> is an important machine learning problem with applications in pattern recognition and language acquisition.
Reference: 10. <author> Hopcroft, J., and Ullman, J. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <pages> '79. </pages>
Reference-contexts: Clearly, L (A s ) L (A). For a detailed treatment of this subject see <ref> [10] </ref>. Fig. 1. Finite State Automaton Fig. 2. MCA - M S + This paper is organized as follows: Sect. 2 describes a non-incremental version of the grammar inference algorithm and proves its correctness. Sect. 3 explains the incremental version of the algorithm.
Reference: 11. <author> Lankhorst, M. </author> <title> A Genetic Algorithm for Induction of Nondeterministic Pushdown Automata. </title> <institution> University of Groningen, Computer Science Report CS-R 9502, </institution> <address> The Netherlands. </address> <month> '95. </month>
Reference-contexts: Rivest and Schapire [18] have suggested a diversity based mechanism dealing with homing sequences. Giles et al [6] use recurrent neural networks to learn FSA from positive and negative samples. Lankhorst <ref> [11] </ref> has presented a genetic algorithms based approach for learning context free grammars.
Reference: 12. <author> Miclet, L. and Quinqueton J. </author> <title> Learning from Examples in Sequences and Grammatical Inference. </title> <editor> In Ferrate, G., </editor> <title> et al (eds) Syntactic and Structural Pattern Recognition. </title> <booktitle> NATO ASI Series Vol. F45, </booktitle> <volume> '86. </volume> <pages> pp. 153-171. </pages>
Reference-contexts: 1 Introduction Regular Grammar Inference <ref> [3, 5, 9, 12] </ref> is an important machine learning problem with applications in pattern recognition and language acquisition.
Reference: 13. <author> Mitchell, T. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18. '82. </volume> <pages> pp 203-226. </pages>
Reference-contexts: Based on the teacher's response fi is pruned and elements of S and G become progressively more general and more specific respectively. Since the lattice defines a partial order and the MSE (MGE) test can be performed efficiently on the elements of the lattice, the version-space algorithm <ref> [13] </ref> can be adapted for candidate elimination. Algorithm: 1. Set S = fP 0 g and G = fP E m 1 g. 2. <p> The efficiency of our algorithm thus relies on the fact that the size of these sets at any time is not unreasonably large. The proposed algorithm uses an efficient bidirectional search strategy inspired by Mitchell's <ref> [13] </ref> version space algorithm which enables elimination of large parts of the hypothesis space based on a single query and also to make unambiguous inferences even when the algorithm has not converged.
Reference: 14. <author> Pao, T., and Carr, J. </author> <title> A solution of the Syntactic Induction-Inference Problem for Regular Languages. </title> <journal> Computer Languages, </journal> <volume> Vol. 3, '78, </volume> <pages> pp. 53-64. </pages>
Reference-contexts: The teacher provides a structurally complete set S + which implicitly defines a lattice (or version space) of candidate FSA or the initial hypothesis space that is guaranteed to contain a FSA (M G ) corresponding to the target grammar (G) <ref> [14, 15, 4] </ref>. At all times, the learner maintains two sets of lattice elements | S and G | which correspond respectively to the most specific and most general FSA consistent with the data (sample strings, queries) processed so far. <p> the need for incremental learning algorithms that enable the learner to develop suitable hypotheses based on the available data and, when presented with additional training data, update the hypotheses appropriately without having to reprocess the previous data. 1 The proof of theorem 1 is originally due to Pao and Carr <ref> [14] </ref> and has been reworked in [15]. It was also independently proven by Miclet (see [4]). Suppose a set S + 0 of positive examples that is not necessarily structurally complete is provided at the start. <p> We have presented provably correct non-incremental and incremental versions of an algorithm for regular grammar inference. We have adopted the idea of mapping the structurally complete set of examples to an ordered lattice from the grammar inference algorithm proposed by Pao and Carr <ref> [14] </ref>. Their algorithm is not incremental and requires explicit enumeration of the entire lattice. The hypothesis space defined by the set S + is too large to be represented explicitly or to be searched exhaustively.
Reference: 15. <author> Parekh R., and Honavar, V. </author> <title> An Efficient Interactive Algorithm for Regular Language Learning. </title> <institution> Computer Science TR95-02, Iowa State University, </institution> <note> '95. (Preliminary version appeared in Proceedings of the 5th UNB AI Symposium, Fredericton, Canada, '93). </note>
Reference-contexts: The teacher provides a structurally complete set S + which implicitly defines a lattice (or version space) of candidate FSA or the initial hypothesis space that is guaranteed to contain a FSA (M G ) corresponding to the target grammar (G) <ref> [14, 15, 4] </ref>. At all times, the learner maintains two sets of lattice elements | S and G | which correspond respectively to the most specific and most general FSA consistent with the data (sample strings, queries) processed so far. <p> that enable the learner to develop suitable hypotheses based on the available data and, when presented with additional training data, update the hypotheses appropriately without having to reprocess the previous data. 1 The proof of theorem 1 is originally due to Pao and Carr [14] and has been reworked in <ref> [15] </ref>. It was also independently proven by Miclet (see [4]). Suppose a set S + 0 of positive examples that is not necessarily structurally complete is provided at the start.
Reference: 16. <author> Parekh, R., and Honavar, V. </author> <title> An Incremental Interactive Algorithm for Regular Grammar Inference. </title> <institution> Computer Science TR96-03, Iowa State University, </institution> <month> '96. </month>
Reference-contexts: The answer to this question is affirmative provided certain conditions are satisfied: First, the teacher must provide positive examples in non-decreasing order by length (see <ref> [16] </ref> for an explanation). Second, the teacher must provide an upper bound N on the number of states of the target automaton M G (or by some other means indicate when the learner has processed a structurally complete set of samples). <p> Given an upper bound N on the number of states in M G , it can be shown that there exists a structurally complete set S + with respect to M G such that no string in S + has length greater than 2N 1 <ref> [16] </ref>. <p> Return the solution to which the candidate elimination procedure converges. For a description of the correctness of the incremental version see <ref> [16] </ref>. Example We use the incremental algorithm to infer the FSA in Fig. 1.
Reference: 17. <author> Porat S., and Feldman J. </author> <title> Learning Automata from Ordered Examples. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> pp 109-138. '91. </pages>
Reference-contexts: Our approach offers an alternative to the ID procedure when a structurally complete set of samples is available. A direct extension of the ID procedure to the incremental version has not been studied. Porat and Feldman <ref> [17] </ref> have proposed an algorithm that uses a complete ordered sample and membership queries and is guaranteed to converge in the limit. They maintain a single working hypothesis that gets modified after the presentation of each sample.
Reference: 18. <author> Schapire, R., </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> MIT Press, </publisher> <pages> '92. </pages>
Reference-contexts: The L fl procedure can be adapted to the PAC learning framework to learn from membership queries and examples alone. Rivest and Schapire <ref> [18] </ref> have suggested a diversity based mechanism dealing with homing sequences. Giles et al [6] use recurrent neural networks to learn FSA from positive and negative samples. Lankhorst [11] has presented a genetic algorithms based approach for learning context free grammars.
Reference: 19. <author> VanLehn, K. and Ball, W. </author> <title> A Version Space Approach to Learning Context-Free Grammars. Machine Learning 2, '87. pp 39-74. This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: One potential advantage we see in maintaining a space of hypotheses (as is the case in our method) is the ability to make unambiguous inferences even when the algorithm has not converged to the target. VanLehn and Ball <ref> [19] </ref> have proposed a version-space approach to learning context-free grammars from a set of positive and negative examples that returns a set of grammars consistent with the given sample set.
References-found: 19

