URL: ftp://ringer.cs.utsa.edu/pub/rosen/decorrelate.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00161.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Ensemble Learning using Decorrelated Neural Networks  
Author: Bruce E. Rosen 
Address: San Antonio, TX 78230  
Affiliation: Computer Science Division University of Texas at San Antonio  
Note: To appear in Connections Science. 1  
Abstract: We describe a decorrelation network training method for improving the quality of regression learning in "ensemble" neural networks that are composed of linear combinations of individual neural networks. In this method, individual networks are trained by backpropagation to not only reproduce a desired output, but also to have their errors be linearly decorrelated with the other networks. Outputs from the individual networks are then linearly combined to produce the output of the ensemble network. We demonstrate the performances of decorrelated network training on learning the "3 Parity" logic function, a noisy sine function, and a one dimensional nonlinear function, and compare the results with the ensemble networks composed of independently trained individual networks (without decorrelation training). Empirical results show that when individual networks are forced to be decorrelated with one another the resulting ensemble neural networks have lower mean squared errors than the ensemble networks having independently trained individual networks. This method is particularly applicable when there is insufficient data to train each individual network on disjoint subsets of training patterns.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Lapedes and R. Farber. </author> <title> How neural nets learn. </title> <booktitle> In Neural Information Processing Systems. </booktitle> <publisher> AIP Press, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Back-propagation networks are often of particular interest in many applications <ref> [1] </ref> because they can approximate any square integrable function to any desired degree of accuracy based on an arbitrary finite size training set. Most current research has sought to improve network processing speed through fast training algorithms [2], or to reduce generalization error using pruning methods [3] [4].
Reference: [2] <author> R. A. Jacobs. </author> <title> Increased rates of convergence through learning rate adaptation. Neural Networks, </title> <type> 4(1), </type> <year> 1989. </year>
Reference-contexts: 1 Introduction Back-propagation networks are often of particular interest in many applications [1] because they can approximate any square integrable function to any desired degree of accuracy based on an arbitrary finite size training set. Most current research has sought to improve network processing speed through fast training algorithms <ref> [2] </ref>, or to reduce generalization error using pruning methods [3] [4]. Recently there has developed a mounting interest in examining ensemble networks, where each ensemble is composed of several individually trained neural networks. The outputs of the individual networks are combined to produce the output of the ensemble network. <p> Additionally, parameter tuning is not required since there are no gain, momentum or additional terms to adjust. Overall, we have found this method to be faster than back-propagation with momentum or other extended methods such as Jacob's Delta-Bar-Delta <ref> [2] </ref> or Fahlman's Quickprop [17] for training most data sets. We compared the performances of Equation (1) with Equation (7). For the decorrelation networks, different combinations of the indicator functions in Equations (9) and (10) were used with (t) either constant or decaying inversely with each training pass.
Reference: [3] <author> A. S. Weigend, D. E. Rummelhart, and B. A. Huberman. </author> <title> Back-propagation, weight elimination, and time series prediction. </title> <editor> In D.S. Touretzky et al., editor, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Most current research has sought to improve network processing speed through fast training algorithms [2], or to reduce generalization error using pruning methods <ref> [3] </ref> [4]. Recently there has developed a mounting interest in examining ensemble networks, where each ensemble is composed of several individually trained neural networks. The outputs of the individual networks are combined to produce the output of the ensemble network. <p> The first term in Equation (7) is used to minimize the errors between the network's output and its target output. 4 The second term in the equation is a penalty term similar to the weight decay regularization term used in <ref> [3] </ref>. The intent of this term is not to smooth the network output or to reduce the magnitude or the number of weights [15], but to decrease the correlation between the individual networks. function approximations and have errors that are highly correlated.
Reference: [4] <author> A. S. Weigend, D. E. Rummelhart, and B. A. Huberman. </author> <title> Generalization by weight elimination with application to forecasting. </title> <editor> In R. Lippmann et al., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Most current research has sought to improve network processing speed through fast training algorithms [2], or to reduce generalization error using pruning methods [3] <ref> [4] </ref>. Recently there has developed a mounting interest in examining ensemble networks, where each ensemble is composed of several individually trained neural networks. The outputs of the individual networks are combined to produce the output of the ensemble network.
Reference: [5] <author> Kangan Tumer and Joydeep Ghosh. </author> <title> Order statistics combiners for neural classifiers. </title> <booktitle> In Proceedings of the World Congress on Neural Networks, pages I31-34. </booktitle> <publisher> INNS Press, </publisher> <address> Washington, D.C., </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The outputs of the individual networks are combined to produce the output of the ensemble network. The individual networks are usually combined by simple averaging or by linear regression, but other methods such as using the Order Statistics <ref> [5] </ref> can also be used. We focus on ensemble networks that are linear combinations of individually trained networks. Perrone [6] and Hanshem [7] have shown that these ensemble networks often outperform their individual networks. Ensemble neural networks are only useful when the individual networks disagree in their predictions [8].
Reference: [6] <author> Michael P. Perrone. </author> <title> Improving regression estimation: Averaging methods for variance reduction with extensions to general convex measure optimization. </title> <type> Technical Report Ph.D. Dissertation, </type> <institution> Brown University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The individual networks are usually combined by simple averaging or by linear regression, but other methods such as using the Order Statistics [5] can also be used. We focus on ensemble networks that are linear combinations of individually trained networks. Perrone <ref> [6] </ref> and Hanshem [7] have shown that these ensemble networks often outperform their individual networks. Ensemble neural networks are only useful when the individual networks disagree in their predictions [8]. There are several approaches for creating disagreeing networks. <p> A similar analysis can be applied for general ensemble networks [12]. 3 Decorrelation Networks When combining multiple back-propagation networks each individual network is typically trained independently to minimize Equation (1) <ref> [6] </ref>. Colinearity problems, which reduce the effectiveness of Equations (3) and (4), can occur when the individual network errors are correlated. Bunn [13] [14] states that when combining forecasts, no major gains can be expected if there are large positive correlations between the forecasting errors.
Reference: [7] <author> Sherif Hashem. </author> <title> Optimal linear combinations of neural networks. </title> <type> Technical Report Ph.D. Dissertation, </type> <institution> Purdue University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The individual networks are usually combined by simple averaging or by linear regression, but other methods such as using the Order Statistics [5] can also be used. We focus on ensemble networks that are linear combinations of individually trained networks. Perrone [6] and Hanshem <ref> [7] </ref> have shown that these ensemble networks often outperform their individual networks. Ensemble neural networks are only useful when the individual networks disagree in their predictions [8]. There are several approaches for creating disagreeing networks. <p> Colinearity problems, which reduce the effectiveness of Equations (3) and (4), can occur when the individual network errors are correlated. Bunn [13] [14] states that when combining forecasts, no major gains can be expected if there are large positive correlations between the forecasting errors. And Hashem <ref> [7] </ref> shows that correlations between network outputs may make optimal linear combinations of networks prone to colinearity problems. To illustrate the network correlation problem, Figure 1a shows two networks trying to approximate a desired output surface. <p> After learning, both networks approximate the surface in the same manner leading to significant correlations in the network errors. Generally, both networks simultaneously overestimate or underestimate the desired output. The network outputs are highly correlated potentially leading to severe colinearity and reducing the robustness of the ensemble network <ref> [7] </ref>. To mitigate this potential colinearity problem, Equation (1) is modified by adding a decorrelation penalty to it. The individual networks attempt to not only minimize the error between the target and their output, but also to decorrelate their errors with those from previously trained networks.
Reference: [8] <author> Anders Krogh and Jesper Vedelsby. </author> <title> Neural network ensembles, cross validation, and active learning. </title> <booktitle> In Neural Information Processing Systems, </booktitle> <volume> volume 7. </volume> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: We focus on ensemble networks that are linear combinations of individually trained networks. Perrone [6] and Hanshem [7] have shown that these ensemble networks often outperform their individual networks. Ensemble neural networks are only useful when the individual networks disagree in their predictions <ref> [8] </ref>. There are several approaches for creating disagreeing networks. One approach is to train several individual networks separately and hope that their predictions will be some different. A second approach is to use different activation function or architecture in each individual network.
Reference: [9] <author> Bambang Parmanto, Paul W. Munro, and Howard R. Doyle. </author> <title> Improving committee diagnosis with resampling techniques. </title> <editor> In D. Touretzky, Michael Mozer, and Michael Hasselmo, editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 8. </volume> <publisher> MIT Press, </publisher> <year> 1996. </year> <month> 11 </month>
Reference-contexts: A second approach is to use different activation function or architecture in each individual network. A third approach is to train the individual networks on different subsamples from the original training set <ref> [9, 10] </ref>.
Reference: [10] <author> Peter Sollich and Anders Krogh. </author> <title> Learning with ensembles: How over-fitting can be useful. </title> <booktitle> In Neural Information Processing Systems, </booktitle> <volume> volume 7. </volume> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: A second approach is to use different activation function or architecture in each individual network. A third approach is to train the individual networks on different subsamples from the original training set <ref> [9, 10] </ref>.
Reference: [11] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: This error can be restated in terms of the network's bias and variance <ref> [11] </ref>: E f = EfVar f + Bias 2 f g + 2 ; (2) where Efg is the expectation operator, Var f = Ef (f (~x) Eff (~x)g) 2 g is the variance, and Bias f = Eff (~x)g g (~x) is the bias. 2 The output of a basic
Reference: [12] <author> N. Ueda and R. Nakano. </author> <title> Statistical analysis of the generalization error of ensemble estimators. </title> <note> (submitted to) Neural Computation, </note> <year> 1995. </year>
Reference-contexts: Assuming that all networks learn from the same set of training data, then the generalization error from a basic ensemble network is E ens , defined by the following theorem (from Ueda <ref> [12] </ref>): Theorem Let E ens be the generalization error of the basic ensemble composed of M individual networks. <p> The proof of the above theorem can be found in <ref> [12] </ref>. <p> Since the error of the basic ensemble network is affected by the covariance of network errors, negative error correlation contributes to a decrease in the generalization error. A similar analysis can be applied for general ensemble networks <ref> [12] </ref>. 3 Decorrelation Networks When combining multiple back-propagation networks each individual network is typically trained independently to minimize Equation (1) [6]. Colinearity problems, which reduce the effectiveness of Equations (3) and (4), can occur when the individual network errors are correlated.
Reference: [13] <author> D. W. Bunn. </author> <title> Forecasting with more than one model. </title> <journal> Journal of Forecasting, </journal> <volume> 8 </volume> <pages> 161-166, </pages> <year> 1989. </year>
Reference-contexts: Colinearity problems, which reduce the effectiveness of Equations (3) and (4), can occur when the individual network errors are correlated. Bunn <ref> [13] </ref> [14] states that when combining forecasts, no major gains can be expected if there are large positive correlations between the forecasting errors. And Hashem [7] shows that correlations between network outputs may make optimal linear combinations of networks prone to colinearity problems.
Reference: [14] <author> D. W. Bunn. </author> <title> Statistical efficiency in the linear combination of forecasts. </title> <journal> International Journal of Forecasting, </journal> <volume> 1 </volume> <pages> 151-163, </pages> <year> 1985. </year>
Reference-contexts: Colinearity problems, which reduce the effectiveness of Equations (3) and (4), can occur when the individual network errors are correlated. Bunn [13] <ref> [14] </ref> states that when combining forecasts, no major gains can be expected if there are large positive correlations between the forecasting errors. And Hashem [7] shows that correlations between network outputs may make optimal linear combinations of networks prone to colinearity problems.
Reference: [15] <author> W. L. Buntine and A. S. Weigend. </author> <title> Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5, </volume> <year> 1991. </year>
Reference-contexts: The intent of this term is not to smooth the network output or to reduce the magnitude or the number of weights <ref> [15] </ref>, but to decrease the correlation between the individual networks. function approximations and have errors that are highly correlated. In Figure 1b the two networks have different input-output function approximations and have errors that are uncorrelated.
Reference: [16] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: The ensemble network weights were learned using the delta rule to minimize Equation (1). During learning, each network training cycle was terminated after 1000 training epochs, or when the gradient vanished. All back-propagation learning algorithms used the Fletcher-Reeves version for conjugate gradient descent binary line search and parabolic interpolation <ref> [16] </ref>. The line search and parabolic interpolation aspects of the algorithm do not require error gradients to be calculated, so there are fewer backward passes than forward passes. Additionally, parameter tuning is not required since there are no gain, momentum or additional terms to adjust.
Reference: [17] <author> S. E. Fahlman. </author> <title> Faster-learning variations on back-propagation: An empirical study. </title> <booktitle> In Proceedings of the 1988 Connectionists Models Summer School. </booktitle> <publisher> Morgan Kauffman Inc., </publisher> <year> 1988. </year>
Reference-contexts: Additionally, parameter tuning is not required since there are no gain, momentum or additional terms to adjust. Overall, we have found this method to be faster than back-propagation with momentum or other extended methods such as Jacob's Delta-Bar-Delta [2] or Fahlman's Quickprop <ref> [17] </ref> for training most data sets. We compared the performances of Equation (1) with Equation (7). For the decorrelation networks, different combinations of the indicator functions in Equations (9) and (10) were used with (t) either constant or decaying inversely with each training pass.
Reference: [18] <author> S. E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kauffman Inc., </publisher> <year> 1990. </year>
Reference-contexts: Our experimental results have shown that the errors from linear combinations of individual networks can be reduced when the individual networks learn to be decorrelated. In contrast to the Cascade Correlation learning architecture <ref> [18] </ref> which attempts to create new hidden units and tries to maximize the magnitude of the correlation between a new unit's output and the residual error, decorrelation networks attempt to minimize the correlation of errors between the individual networks.

References-found: 18

