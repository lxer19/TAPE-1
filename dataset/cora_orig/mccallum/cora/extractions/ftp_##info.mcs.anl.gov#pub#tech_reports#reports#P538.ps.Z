URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P538.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts95.htm
Root-URL: http://www.mcs.anl.gov
Phone: 2  
Title: Solution of Dense Systems of Linear Equations Arising from Integral Equation Formulations  
Author: Kimmo Forsman, William Gropp, Lauri Kettunen, David Levine, and Jukka Salonen 
Address: Magnetism, P.O. Box. 692, FIN-33101 Tampere, Finland  9700 South Cass Ave., Argonne, IL 60439, U.S.A.  
Affiliation: 1 Tampere University of Technology, Laboratory of Electricity and  Argonne National Laboratory, Mathematics and Computer Science Division,  
Abstract: This paper discusses efficient solution of dense systems of linear equations arising from integral equation formulations. Several preconditioners in connection with Krylov iterative solvers are examined and compared with LU factorization. Results are shown demonstrating practical aspects and issues we have encountered in implementing iterative solvers on both parallel and sequential computers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Kettunen, K. Forsman, D. Levine, and W. Gropp, </author> <title> "Integral equations and nonlinear 3D magnetostatics," </title> <journal> Int. J. Numer. Methods Eng., </journal> <volume> vol. 38, </volume> <pages> pp. 2655-2675, </pages> <year> 1995. </year>
Reference-contexts: Our goal in this paper is to discuss various issues we have encountered in trying to find and implement efficient sequential and parallel solvers for a magnetostatic volume integral formulation. The corresponding code is called GFUNET <ref> [1] </ref>, and the system matrices it generates are asymmetric. We have tested both iterative Krylov methods and LU factorization. The rest of this paper is organized as follows. The Basic Linear Algebra Subprograms are discussed in Section II. The Krylov class of iterative methods are discussed in Section III. <p> However, in our case there are rather large off-diagonal elements in the system matrix due to the spanning tree extraction technique <ref> [1] </ref>, [9] we use. These large elements, far from the diagonal, make it more difficult to find an appropriate preconditioner. <p> The linear systems arise during GFUNET's solution of three-dimensional nonlinear magnetostatics problem, where a related sequence of linear systems is solved. Details of the nonlinear solution method are given in <ref> [1] </ref>. The first test problem is the international electromagnetic force benchmark TEAM (Testing Electromagnetic Analysis Methods) problem 20 [12]. <p> The results show that all of the iterative solvers are more efficient than the LAPACK LU solver. Table IV compares the LAPACK LU solver with GM-RES using five different preconditioners. Here, three different test problems, TEAM problem 13 [13], TEAM problem 20, and a positron accumulator ring dipole magnet <ref> [1] </ref>, were used. The timings, parameters, and starting guess are the same as those used in Table III. The block diagonal and sparse preconditioners both provide consistently good results which are better than LAPACK's LU solver.
Reference: [2] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh, </author> <title> "Basic linear algebra subprograms for fortran usage," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> vol. 5, </volume> <pages> pp. 308-323, </pages> <year> 1979. </year>
Reference-contexts: For optimal performance data movement within the memory hierarchy should be minimized. Standard programming languages such as Fortran or C, do not have tools to explicitly control the data movement within the memory hierarchy. However, many computers provide machine-optimized versions of the Basic Linear Algebra Subprograms (BLAS) <ref> [2] </ref>, low-level linear algebra routines that optimize the use of the memory hierarchy. In the case of dense system of linear equations, use of the BLAS can significantly decrease the total solution time.
Reference: [3] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vet--terling, </author> <title> Numerical Recipes, </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: The advantage of using the BLAS is shown in Table I. Systems of equations generated by GFUNET are solved on a DEC 3000-700 AXP workstation using the LU solver from Numerical Recipes <ref> [3] </ref> and the LU solver from LA-PACK [4], where machine-optimized BLAS routines are used. The LAPACK solver is an order of magnitude faster than the Numerical Recipes LU solver.
Reference: [4] <author> E. Anderson et al., </author> <note> LAPACK Users's Guide. SIAM, </note> <year> 1992. </year>
Reference-contexts: The advantage of using the BLAS is shown in Table I. Systems of equations generated by GFUNET are solved on a DEC 3000-700 AXP workstation using the LU solver from Numerical Recipes [3] and the LU solver from LA-PACK <ref> [4] </ref>, where machine-optimized BLAS routines are used. The LAPACK solver is an order of magnitude faster than the Numerical Recipes LU solver.
Reference: [5] <author> R. Barret et al., </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: Different Krylov methods arise from different choices of the subspaces K s and L s and from the ways in which the system is preconditioned. We have written Fortran code based on the TEMPLATES book <ref> [5] </ref> for the following Krylov methods: generalized minimal residual (GMRES) [6], conjugate gradient squared (CGS) [7], and bi-conjugate gradient stabilized (Bi-CGSTAB) [8]. In our implementation, we have strived to use BLAS routines whenever possible. A.
Reference: [6] <author> Y. Saad and M. Schultz, </author> <title> "GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems," </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> vol. 6, </volume> <pages> pp. 865-881, </pages> <year> 1985. </year>
Reference-contexts: Different Krylov methods arise from different choices of the subspaces K s and L s and from the ways in which the system is preconditioned. We have written Fortran code based on the TEMPLATES book [5] for the following Krylov methods: generalized minimal residual (GMRES) <ref> [6] </ref>, conjugate gradient squared (CGS) [7], and bi-conjugate gradient stabilized (Bi-CGSTAB) [8]. In our implementation, we have strived to use BLAS routines whenever possible. A.
Reference: [7] <author> P. Sonneveld, </author> <title> "CGS: a fast Lanczos-type solver for nonsymmetric linear systems," </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> vol. 10, </volume> <pages> pp. 36-52, </pages> <year> 1989. </year>
Reference-contexts: We have written Fortran code based on the TEMPLATES book [5] for the following Krylov methods: generalized minimal residual (GMRES) [6], conjugate gradient squared (CGS) <ref> [7] </ref>, and bi-conjugate gradient stabilized (Bi-CGSTAB) [8]. In our implementation, we have strived to use BLAS routines whenever possible. A. <p> Here ~ b and ~r 0 are arbitrary vectors. CGS and Bi-CGSTAB are variants of BiCG that do not require the computation of A T x s . It has been shown <ref> [7] </ref> that CGS is faster than BiCG but often has quite irregular convergence behavior. Bi-CGSTAB was developed to have the same convergence rate as CGS at its best, without having the same difficulties.
Reference: [8] <author> H. A. van der Vorst, </author> <title> "Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems," </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> vol. 13, </volume> <pages> pp. 631-644, </pages> <year> 1992. </year>
Reference-contexts: We have written Fortran code based on the TEMPLATES book [5] for the following Krylov methods: generalized minimal residual (GMRES) [6], conjugate gradient squared (CGS) [7], and bi-conjugate gradient stabilized (Bi-CGSTAB) <ref> [8] </ref>. In our implementation, we have strived to use BLAS routines whenever possible. A.
Reference: [9] <author> K. Forsman and L. Kettunen, </author> <title> "Tetrahedral mesh generation in convex primitives by maximizing solid angles," </title> <journal> IEEE Trans. Magn., </journal> <volume> vol. 30, </volume> <pages> pp. 3535-3538, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: However, in our case there are rather large off-diagonal elements in the system matrix due to the spanning tree extraction technique [1], <ref> [9] </ref> we use. These large elements, far from the diagonal, make it more difficult to find an appropriate preconditioner.
Reference: [10] <author> H. A. van der Vorst, </author> <title> "High performance preconditioning," </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> vol. 10, </volume> <pages> pp. 1174-1185, </pages> <year> 1989. </year>
Reference-contexts: Band: M is formed by taking directly from A a band whose bandwidth is bw. 4. Standard Incomplete LU: The SILU preconditioner <ref> [10] </ref> is M = (L s + ~ D) ~ D 1 ( ~ D + U s ), where L s and U s TABLE II ADDITIONAL STORAGE NEEDED FOR PRECONDITIONERS Preconditioner Additional q = 4; bw = n=4 Storage and nz = 0:05n 2 Diagonal 0 0 Block Diagonal
Reference: [11] <author> T. A. Davis, </author> <title> "Users' guide for the unsymmetric-pattern mul-tifrontal package (UMFPACK)," </title> <type> Tech. Rep. </type> <institution> TR-93-020, CIS Dept., Univ. of Florida, </institution> <address> Gainesville, FL, </address> <year> 1993. </year>
Reference-contexts: Sparse: A sparse preconditioner is formed by letting the nonzeros of M be those elements of A satisfying ja ij j o min (ja ii j; ja jj j), where o is the dropping coefficient. The number of nonzeros in M is nz. Equation (2) is solved using UMFPACK <ref> [11] </ref> developed for unsymmetric sparse matrices. Using the block diagonal, band, and sparse precondi-tioners, one must decide how large a portion of the system matrix A to include in the preconditioner M . In our case the choice was based on numerical experiments.
Reference: [12] <author> N. Takahashi, T. Nakata, and H. Morishige, </author> <title> "Summary of results for problem 20 (3-d static force problem)," </title> <booktitle> in Proc. of the Fourth Int. TEAM Workshop, (Florida International University, </booktitle> <address> Miami, U.S.A.), </address> <pages> pp. 85-91, </pages> <year> 1994. </year>
Reference-contexts: The linear systems arise during GFUNET's solution of three-dimensional nonlinear magnetostatics problem, where a related sequence of linear systems is solved. Details of the nonlinear solution method are given in [1]. The first test problem is the international electromagnetic force benchmark TEAM (Testing Electromagnetic Analysis Methods) problem 20 <ref> [12] </ref>.
Reference: [13] <author> T. Nakata, N. Takahashi, and K. Fujiwara, </author> <title> "Summary of results for team workshop problem 13 (3-d nonlinear magneto-static model)," </title> <booktitle> in Proc. of the Fourth Int. TEAM Workshop, (Florida International University, </booktitle> <address> Miami, U.S.A.), </address> <pages> pp. 33-39, </pages> <year> 1994. </year>
Reference-contexts: The results show that all of the iterative solvers are more efficient than the LAPACK LU solver. Table IV compares the LAPACK LU solver with GM-RES using five different preconditioners. Here, three different test problems, TEAM problem 13 <ref> [13] </ref>, TEAM problem 20, and a positron accumulator ring dipole magnet [1], were used. The timings, parameters, and starting guess are the same as those used in Table III. The block diagonal and sparse preconditioners both provide consistently good results which are better than LAPACK's LU solver.
Reference: [14] <author> W. D. Gropp and B. F. Smith, </author> <title> "Scalable, extensible, and portable numerical libraries," </title> <booktitle> in Proceedings of Scalable Parallel Libraries Conference, </booktitle> <pages> pp. 87-93, </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: The parallel results we present were computed on an IBM SP parallel computer with 128 RS/6000 model 370 processors, each with 128 Mbytes of memory and a one Gbyte local disk. The parallel solvers are from PETSc (Portable and Extensible Tools for Scientific Computing) <ref> [14] </ref>, a large toolkit of software for portable, parallel scientific computation. The PETSc solvers are written to make efficient use of the BLAS.
References-found: 14

