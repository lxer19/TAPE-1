URL: http://dna.stanford.edu/~cnevill/publications/ANNES95.ps.gz
Refering-URL: http://dna.stanford.edu/~cnevill/resume.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: cgn,geoff,ihw@cs.waikato.ac.nz  
Title: The Development of Holtes 1R Classifier attribute a, form a rule as follows: For each
Author: Craig G. Nevill-Manning, Geoffrey Holmes and Ian H. Witten 
Note: For each  
Address: Hamilton, New Zealand.  
Affiliation: Department of Computer Science, University of Waikato,  
Abstract: 2 The 1R Algorithm The 1R machine learning scheme is a very simple one that proves surprisingly effective on the standard datasets commonly used for evaluation. This paper describes the method and discusses two aspects of the algorithm that bear further analysis: the way that intervals are formed when discretizing continuously - valued attributes, and the treatment of missing values are treated. We then show how the algorithm can be extended to avoid a problem endemic to most practical machine learning algorithmstheir frequent dismissal of an attribute as irrelevant when in fact it is highly relevant when combined with other attributes. Like other empirical learning methods, 1R takes as input a set of examples, each with several attributes and a class. Its goal is to infer a rule that predicts the class given the values of the attributes. The 1R algorithm chooses the most informative single attribute and bases the rule on this attribute alone. Full details can be found in Holtes paper, but the basic idea is shown in Figure 1. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Dougherty, J., Kohavi, R., and Sahami, M. </author> <year> (1995), </year> <title> Supervised and Unsupervised Discretization of Continuous Features, </title> <booktitle> Proc. Machine Learning 95, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <month> CA. </month> <title> Because 1R is extremely efficient in its evaluation of attributes, it can be used to identify promising attribute combinations. This process, which we call 2R, produces new attributes by concatenating pairs of attributes, then runs 1R on this new dataset. The best 2-rule formed by this process indicates the best pair of attributes in the dataset. </title>
Reference: 2. <author> Holte, R.C., Acker, L., Porter, B.W. </author> <year> (1989). </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> Proc. Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 813-818. </pages>
Reference-contexts: Consider, for example, this rule formed from the protime attribute by 1R from one of Holtes datasets (HE): classifications of the quantization. As the golf dataset is not sufficiently complex, we will use the G2 dataset from Holtes <ref> [2] </ref> corpus as an example. Figure 2 plots the two description lengths against each other for three attributes from the G2 dataset: Na, Ca and RI. Each of the dark points corresponds to a certain number of split points, the number increasing as we move rightwards.
Reference: 3. <author> Holte, R.C. </author> <year> (1993). </year> <title> Very Simple Classification Rules Perform Well on Most Commonly Used Datasets. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <month> 63-90. </month> <title> For example, in the golf dataset, one new attribute would result from the concatenation of outlook and windy. The values of this new attribute would be sunny-false, </title> <editor> sunny-true, overcast-false, overcast-true, rain-false and rain-true. </editor> <title> These new attributes gain accuracy partly as a result of the greater number of unique values that they contain, so comparisons with the individual attributes are meaningless. However comparison with other derived attributes is informative. </title>
Reference-contexts: 1 Introduction In a contentious paper demonstrating the inadequacies of datasets used to benchmark machine learning algorithms, Robert Holte of the University of Ottawa described a very simple learning algorithm, which he called 1R, that competes favorably with state-of-the-art techniques in the field (Holte, <ref> [3] </ref>). Holte did not promote the use of 1R as a rival mainstream learning technique; rather, he used it to show that most of the datasets that researchers were using to test their algorithms did not embody very complex rules. The algorithm assumes that the attributes are discrete. <p> Empirical evidence led him to a value of six for datasets with large numbers of instances and three for smaller datasets (with less than about 50 instances) (Holte et al, <ref> [3] </ref>). until there are no more split points. 4. Choose the best split point on the accuracy vs number of splits curve.
Reference: 4. <author> Holmes, G., and Nevill-Manning, C.G. </author> <year> (1995). </year> <title> Feature Selection via The Discovery of Simple Classification Rules. </title> <booktitle> Proc. International Symposium on Intelligent Data Analysis (IDA-95), </booktitle> <address> Baden-Baden, Germany. </address>
Reference-contexts: We have adopted this philosophy in our own work and have been able to demonstrate the efficacy of 1R as a filter to select relevant subsets of attributes prior to learning (Holmes and Nevill-Manning, <ref> [4] </ref>). 2.1 A Worked Example Table 1 shows the golf data (Quinlan, [7]), a small illustrative dataset that uses weather information to decide whether or not to play golf.
Reference: 5. <author> John, G., Kohavi, R., and Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant Features and the Subset Selection Problem. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pp 121-129. </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The complexity of the 1R algorithm is O ( n) for 1-rules (n attributes) and O (n 2 ) for 1-rule pairs. Our current implementation represents something of a brute-force approach, and we intend to spend time in the future developing a more efficient algorithm. John et al. <ref> [5] </ref> have shown that considering attributes individually when building decision trees can result in larger and less accurate trees than if attributes are considered in combination. They give an example where four attributes predict the class perfectly when taken together, but where C4.5 prefers attributes that were generated randomly.
Reference: 6. <author> Natarajan, B.K. </author> <year> (1993), </year> <title> Occams razor for functions, </title> <booktitle> Proc. IEEE Data Compression Conference, Snowbird, Utah, </booktitle> <pages> pp. </pages> <month> 60 - 69. </month> <title> The best of the resulting 2-rules does not always include the attribute which produces the best 1-rule. When it does not, the 2-rule contains a useful pair of attributes which would have been ignored by greedy schemes like C4.5. </title>
Reference-contexts: However, once the quantization has been achieved, adding more split points only accounts for noise in the data, and the gain in accuracy will be much less. It is the boundary between fitting structure and fitting noise that we are looking for. This approach is inspired by Natarajans work <ref> [6] </ref>. i f protime is missing then live (53/67) e l s e i f protime &lt; 36 then die (8/12) e l s e i f protime 36 then live (66/76) There are 155 instances of which approximately 1/3 are missing.
Reference: 7. <author> Quinlan, J.R. </author> <year> (1993), </year> <title> C4.5: programs for machine learning , San Mateo, </title> <address> CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 18. </pages>
Reference-contexts: We have adopted this philosophy in our own work and have been able to demonstrate the efficacy of 1R as a filter to select relevant subsets of attributes prior to learning (Holmes and Nevill-Manning, [4]). 2.1 A Worked Example Table 1 shows the golf data (Quinlan, <ref> [7] </ref>), a small illustrative dataset that uses weather information to decide whether or not to play golf. The dataset has two nominal attributes, outlook (with values sunny, overcast and rain), and windy (with values true and false), and two continuous-valued ones, temperature and humidity.

References-found: 7

