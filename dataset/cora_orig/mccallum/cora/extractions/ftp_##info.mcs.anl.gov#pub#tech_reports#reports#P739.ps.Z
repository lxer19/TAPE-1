URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P739.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts99.htm
Root-URL: http://www.mcs.anl.gov
Title: Numerical Relativity in a Distributed Environment  
Author: Werner Benger ; Ian Foster Jason Novotny Edward Seidel ;; John Shalf Warren Smith Paul Walker 
Affiliation: 1 Max-Planck-Institut fur Gravitationsphysik, Albert-Einstein-Institut 2 Konrad-Zuse-Zentrum fur Informationstechnik Berlin 3 Mathematics and Computer Science Division, Argonne National Laboratory 4 National Center for Supercomputing Applications 5 Departments of Astronomy and Physics, University of Illinois at Urbana-Champaign  
Abstract: The Cactus parallel simulation framework provides a modular and extensible set of components for solving relativity problems on parallel computers. In recent work, we have investigated techniques that would enable the execution of Cactus applications in wide area "computational grid" environments. In a first study, we investigated the feasibility of distributing a single simulation across multiple supercomputers, while in a second we studied techniques for reducing communication costs associated with remote visualization and steering. Distributed simulation was achieved by using MPICH-G, an implementation of the Message Passing Interface standard that uses mechanisms provided by the Globus grid toolkit to enable wide area execution. Experiments were performed across SGI Origins and Cray T3Es with geographical separations ranging from hundreds to thousands of kilometers. Total execution time when distributed increased by between 48% and 133%, depending on configuration. We view these results as encouraging as they were obtained with essentially no specialized algorithmic structures in the Cactus application. Work on remote visualization focused on the development of a Cactus module that computes isosurfaces inline with numerical relativity calculations. Experiments demonstrated that this technique can reduce network bandwidth requirements by a factor ranging from 2.5 to 114, depending on the nature of the problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> The cactus project. </institution> <note> http://cactus.aei-potsdam.mpg.de. </note>
Reference-contexts: The next section describes the Cactus framework. Section 3 describes the Globus toolkit, Section 4 describes our experiences and results, and Section 5 contains our conclusions. 2 Cactus Cactus <ref> [1] </ref> is a modular framework for creating portable parallel finite-difference simulation codes.
Reference: [2] <author> GR3D. </author> <note> http://wugrav.wustl.edu/Codes/GR3D. </note>
Reference-contexts: The Cactus code's primary application has been solving Einstein's equations of gravity [4], including studies involving black holes [5], self-gravitating fields [16], and relativistic hydrodynamics such as the coalescence of two neutron stars (NASA Neutron Star Grand Challenge Project) <ref> [2] </ref>. The Cactus code was originally developed at the Max-Planck-Institut for Gravitational Physics (Albert-Einstein-Institut) in Potsdam, Germany, by Paul Walker and Joan Masso. Since then, development of Cactus has been taken over by a community of Cactus users, particularly at Washington University in St.
Reference: [3] <editor> Sc'97 conference demonstration. </editor> <address> http://cactus.aei-potsdam.mpg.de/News/SC97.html. </address>
Reference-contexts: For the largest problems, the computational resources needed to visualize the resulting data may be nearly as large as supercomputer that was originally used to create the data. In order to meet these challenges, researchers at NCSA and the Rechenzentrum der MPG at Garching Germany <ref> [3] </ref> created a remote visualization thorn that does the visualization computations in-line with the code and can be steered by using a remote client application. This allows the visualization computations to run with the same degree of parallelism as the simulation codes, resulting in exceptionally fast performance.
Reference: [4] <institution> Les Houches School on Relativistic Astrophysical Source of Gravitational Radiation. </institution> <year> 1996. </year>
Reference-contexts: Section 3 describes the Globus toolkit, Section 4 describes our experiences and results, and Section 5 contains our conclusions. 2 Cactus Cactus [1] is a modular framework for creating portable parallel finite-difference simulation codes. The Cactus code's primary application has been solving Einstein's equations of gravity <ref> [4] </ref>, including studies involving black holes [5], self-gravitating fields [16], and relativistic hydrodynamics such as the coalescence of two neutron stars (NASA Neutron Star Grand Challenge Project) [2]. The Cactus code was originally developed at the Max-Planck-Institut for Gravitational Physics (Albert-Einstein-Institut) in Potsdam, Germany, by Paul Walker and Joan Masso.
Reference: [5] <author> P. Anninos, K. Camarda, J. Mass E. Seidel, W.M. Suen, and J. Towns. </author> <title> Three Dimensional Numerical Relativity: The Evolution of Black Holes, </title> <year> 1995. </year>
Reference-contexts: The Cactus code's primary application has been solving Einstein's equations of gravity [4], including studies involving black holes <ref> [5] </ref>, self-gravitating fields [16], and relativistic hydrodynamics such as the coalescence of two neutron stars (NASA Neutron Star Grand Challenge Project) [2]. The Cactus code was originally developed at the Max-Planck-Institut for Gravitational Physics (Albert-Einstein-Institut) in Potsdam, Germany, by Paul Walker and Joan Masso.
Reference: [6] <author> K. Czajkowski, I. Foster, C. Kesselman, S. Martin, W. Smith, and S. Tuecke. </author> <title> A Resource Management Architecture for Metasystems. </title> <booktitle> Lecture Notes on Computer Science, </booktitle> <year> 1998. </year>
Reference-contexts: Further, the MDS contains dynamic information such as the number of free nodes on computer systems and the latency and bandwidth between systems. This allows real-time decisions to be made about what resources to use. 4 The Globus Resource Allocation Manager <ref> [6] </ref> (GRAM) provides resource management functions to start, monitor, and terminate serial and parallel applications. The purpose of the GRAM is not to replace local scheduling systems but to provide a common interface to the variety of local scheduling systems already in use.
Reference: [7] <author> S. Fitzgerald, I. Foster, C. Kesselman, G. vol Laszewski, W. Smith, and S. Tuecke. </author> <title> A Directory Service for Configuring High-Performance Distributed Computations. </title> <booktitle> In Sixth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <year> 1997. </year>
Reference-contexts: Once a user has been authenticated to Globus, he is then automatically authenticated to all local administrative domains that have installed Globus and that the user has access to. This is accomplished by mapping Globus credentials to local credentials through a security gateway. The Metacomputing Directory Service <ref> [7] </ref> (MDS) provides a logically centralized place to store information about entities. The MDS is accessed using the Lightweight Directory Access Protocol [12] (LDAP) and stores information in a hierarchical directory information tree. The directory service contains information about computer systems, networks, users, and so forth.
Reference: [8] <author> I. Foster, R. Gjersten, J. Mass, M. Nardulli, M. Parashar, T. Roy, E. Seidel, J. Shalf, and D. Weber. </author> <title> Computing and Visualizing Einstein's Gravitational Waves across the Metacenter, </title> <note> 1995. http://www.ncsa.uiuc.edu/General/Training/SC95/GII.Apps.html. </note>
Reference-contexts: Another approach is to perform the isosurfacing on a separate parallel computer. Members of our team used that approach for a distributed computing demo at SC95 on the I-WAY <ref> [8] </ref>. The problem with this approach is that the cost of moving the data from the simulation host can far exceed the benefits of parallelizing the visualization computations.
Reference: [9] <author> I. Foster and N. Karonis. </author> <title> A Grid-Enabled MPI: Message Passing in Heterogeneous Distributed Computing Systems. </title> <booktitle> In Proceedings of the ACM/IEEE SC98 Conference. </booktitle> <publisher> ACM Press, </publisher> <year> 1998. </year>
Reference-contexts: We distribute Cactus simulations 1 2 across multiple supercomputers using the mechanisms provided by the Globus toolkit. In particular, we use Globus mechanisms for authentication, access to remote computer systems, file transfer, and communication. The Cactus code uses MPI for communication and makes use of MPICH-G <ref> [9] </ref>, an MPI implementation layered atop Globus communication mechanisms. These communication mechanisms allow a MPI application to be executed on distributed resources. <p> MPICH is a portable implementation of MPI developed at Argonne National Laboratory and Missouri State University. MPICH is implemented over an abstract communication device, which in turn can be implemented using many different communication mechanisms such as shared memory or a proprietary message-passing protocol. A Globus implementation <ref> [9] </ref> of this abstract communication device that uses the Nexus [10] communication library, and Globus mechanisms for resource allocation is available. The Nexus library is a low-level Globus component that is a communication library that provides asynchronous remote procedure calls and threading mechanisms.
Reference: [10] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus Approach to Integrating Multithreading and Communication. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (37):70-82, 1996. 
Reference-contexts: MPICH is implemented over an abstract communication device, which in turn can be implemented using many different communication mechanisms such as shared memory or a proprietary message-passing protocol. A Globus implementation [9] of this abstract communication device that uses the Nexus <ref> [10] </ref> communication library, and Globus mechanisms for resource allocation is available. The Nexus library is a low-level Globus component that is a communication library that provides asynchronous remote procedure calls and threading mechanisms.
Reference: [11] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The GRAM will use GASS to transfer the remote executable to the local system and then execute it. This functionality along with the GASS file I/O commands enables location-independent applications. One high-level Globus service used by Cactus is an implementation of the MPI <ref> [11] </ref> message-passing standard called MPICH. MPICH is a portable implementation of MPI developed at Argonne National Laboratory and Missouri State University. MPICH is implemented over an abstract communication device, which in turn can be implemented using many different communication mechanisms such as shared memory or a proprietary message-passing protocol.
Reference: [12] <author> Timothy A. Howes and Mark C. Smith. LDAP: </author> <title> Programming Directory-Enabled Applications with Lightweight Directory Access Protocol. </title> <publisher> Macmillan Technical Publishing, </publisher> <year> 1997. </year>
Reference-contexts: This is accomplished by mapping Globus credentials to local credentials through a security gateway. The Metacomputing Directory Service [7] (MDS) provides a logically centralized place to store information about entities. The MDS is accessed using the Lightweight Directory Access Protocol <ref> [12] </ref> (LDAP) and stores information in a hierarchical directory information tree. The directory service contains information about computer systems, networks, users, and so forth. Further, the MDS contains dynamic information such as the number of free nodes on computer systems and the latency and bandwidth between systems.
Reference: [13] <author> David A. Lifka. </author> <title> The ANL/IBM SP Scheduling System. </title> <booktitle> Lecture Notes on Computer Science, </booktitle> <volume> 949 </volume> <pages> 295-303, </pages> <year> 1995. </year>
Reference-contexts: The purpose of the GRAM is not to replace local scheduling systems but to provide a common interface to the variety of local scheduling systems already in use. GRAMs have been layered atop EASY <ref> [13] </ref>, LSF [15], Condor [14], and other local scheduling systems. In addition, a GRAM that simply does a fork () and exec () is available for unscheduled local resources. Globus provides access to remote data using the Global Access to Secondary Storage (GASS) component.
Reference: [14] <author> M. Litzkow and M. Livny. </author> <title> Experience with the condor distributed batch system. </title> <booktitle> In IEEE Workshop on Experimental Distributed Systems, </booktitle> <year> 1990. </year>
Reference-contexts: The purpose of the GRAM is not to replace local scheduling systems but to provide a common interface to the variety of local scheduling systems already in use. GRAMs have been layered atop EASY [13], LSF [15], Condor <ref> [14] </ref>, and other local scheduling systems. In addition, a GRAM that simply does a fork () and exec () is available for unscheduled local resources. Globus provides access to remote data using the Global Access to Secondary Storage (GASS) component.
Reference: [15] <institution> Platform Computing Corporation. </institution> <note> LSF 2.2 User's Guide, Febuary 1996. </note>
Reference-contexts: The purpose of the GRAM is not to replace local scheduling systems but to provide a common interface to the variety of local scheduling systems already in use. GRAMs have been layered atop EASY [13], LSF <ref> [15] </ref>, Condor [14], and other local scheduling systems. In addition, a GRAM that simply does a fork () and exec () is available for unscheduled local resources. Globus provides access to remote data using the Global Access to Secondary Storage (GASS) component.
Reference: [16] <author> Edward Seidel and Wai-Mo Suen. </author> <title> Formation of Solitonic Stars Through Gravitational Cooling, </title> <booktitle> 1994. Proceedings of the Seventh Marcel Grossman Meeting on General Relativity. </booktitle>
Reference-contexts: The Cactus code's primary application has been solving Einstein's equations of gravity [4], including studies involving black holes [5], self-gravitating fields <ref> [16] </ref>, and relativistic hydrodynamics such as the coalescence of two neutron stars (NASA Neutron Star Grand Challenge Project) [2]. The Cactus code was originally developed at the Max-Planck-Institut for Gravitational Physics (Albert-Einstein-Institut) in Potsdam, Germany, by Paul Walker and Joan Masso.
References-found: 16

