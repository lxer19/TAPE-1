URL: http://www.cs.utah.edu/~cs686/Previous/s96/coma_ft_dsm.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/s96/
Root-URL: 
Title: Tolerating Node Failures in Cache Only Memory Architectures  
Author: Michel Ban atre, Alain Gefflaut, Christine Morin 
Note: N 2335 PROGRAMME 1  
Affiliation: INSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B. Lim, G. Ma, and D. Nussbaum. </author> <title> The Mit Alewife Machine : A Large-Scale Distributed Memory Multiprocessor. </title> <type> Technical Report MIT/LCS/TM-454, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Shared memory provides a flexible and powerful computing environment. Two variations of these architectures have emerged: Cache Coherent Non Uniform Memory Access machines (CC NUMA) <ref> [1, 18] </ref>, which statically divide the main memory among the nodes of the architecture, and Cache Only Memory Architectures (COMAs) [14, 10] which convert the per node memory into a large cache of the shared address space, called an Attraction Memory (AM).
Reference: [2] <author> M. Ban^atre, A. Ge*aut, P. Joubert, P.A. Lee, and C. Morin. </author> <title> An Architecture for Tolerating Processor Failures in Shared-Memory Multiprocessors. </title> <type> Technical Report 1965, </type> <institution> INRIA, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: These two properties can be guaranteed by storing recovery points INRIA Tolerating Node Failures in Cache Only Memory Architectures 7 in stable storage [16] for which efficient hardware implementations have already been proposed <ref> [2, 3, 4, 7] </ref>. In SSMMs, it is however unreasonable to develop specific hardware stable storage implementations as the cost of the architecture would drastically grow. <p> Taking a global recovery point simplifies the algorithm as other mechanisms must else be used to deal with the effect of data sharing on backward error recovery <ref> [2] </ref>. The algorithm is depicted in Figure 3. To guarantee atomicity of the establishment, and hence to tolerate any possible failure during its execution, the algorithm uses a traditional two-phase commit protocol [12]. <p> To limit the recovery point establishment overhead, other techniques could be envisaged. Dependency tracking between communicating processors could limit the number of processors included in a recovery point establishment operation <ref> [2] </ref>. Recovery point establishment performed in parallel with the execution of the application [19, 9] could hide the time required by this operation. The extended protocol can be implemented in any COMA. For a real implementation, other aspects of fault tolerance have however to be investigated.
Reference: [3] <author> M. Ban^atre and P. Joubert. </author> <title> Cache Management in a Tightly Coupled Fault Tolerant Multiprocessor. </title> <booktitle> In Proc. of 20th International Symposium on Fault-Tolerant Computing Systems, </booktitle> <pages> pages 89-96, </pages> <institution> Newcastle, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: These two properties can be guaranteed by storing recovery points INRIA Tolerating Node Failures in Cache Only Memory Architectures 7 in stable storage [16] for which efficient hardware implementations have already been proposed <ref> [2, 3, 4, 7] </ref>. In SSMMs, it is however unreasonable to develop specific hardware stable storage implementations as the cost of the architecture would drastically grow.
Reference: [4] <author> M. Ban^atre, G. Muller, B. Rochat, and P. Sanchez. </author> <title> Design Decisions for the FTM : A General Purpose Fault Tolerant Machine. </title> <booktitle> In Proc. of 21st International Symposium on Fault-Tolerant Computing Systems, </booktitle> <pages> pages 71-78, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: These two properties can be guaranteed by storing recovery points INRIA Tolerating Node Failures in Cache Only Memory Architectures 7 in stable storage [16] for which efficient hardware implementations have already been proposed <ref> [2, 3, 4, 7] </ref>. In SSMMs, it is however unreasonable to develop specific hardware stable storage implementations as the cost of the architecture would drastically grow.
Reference: [5] <author> L. A. Barroso and M. Dubois. </author> <title> Cache Coherence on a Slotted Ring. </title> <booktitle> In Proc. of 1991 International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 230-237, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The conflict resolution adopted here for the standard part of the protocol is quite similar to that presented in <ref> [5] </ref>. It introduces some new transient states which cannot be described here due to lack of space. Conceptually, the extended coherence protocol can be implemented exactly as it is described in Figure 2.
Reference: [6] <author> J. Bartlett, J. Gray, and B. Horst. </author> <title> Fault Tolerance in Tandem Computer Systems. </title> - <editor> In A. Avizienis, H. Kopetz, and J.C. Laprie, editors, </editor> <booktitle> The Evolution of Fault-Tolerant Computing, </booktitle> <volume> volume 1, </volume> <pages> pages 55-76. </pages> <publisher> Springer Verlag, </publisher> <year> 1987. </year>
Reference-contexts: This technique has several advantages over other fault tolerance approaches. Active software replication [8] requires a strong synchronization between replica leading, in particular for shared memory, to a significant increase of inter-node communications and to a high performance degradation. Hardware static replication like nMR (n Modular Redundancy) <ref> [6, 15] </ref> with voting requires a full replication of a majority of hardware components and is certainly too expensive to be applied to architectures with a large number of components. In contrast, BER limits the hardware development and allows the use of all the processors for a computation.
Reference: [7] <author> Ph. A. Bernstein. </author> - <title> Sequoia: A Fault-tolerant Tightly Coupled Multiprocessor for Transaction Processing. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 37-45, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: These two properties can be guaranteed by storing recovery points INRIA Tolerating Node Failures in Cache Only Memory Architectures 7 in stable storage [16] for which efficient hardware implementations have already been proposed <ref> [2, 3, 4, 7] </ref>. In SSMMs, it is however unreasonable to develop specific hardware stable storage implementations as the cost of the architecture would drastically grow.
Reference: [8] <author> K.P. Birman. </author> <title> Replication and Fault-tolerance in the ISIS System. </title> <booktitle> In Proc. of 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-86, </pages> <address> Washington, </address> <month> December </month> <year> 1985. </year>
Reference-contexts: This technique has several advantages over other fault tolerance approaches. Active software replication <ref> [8] </ref> requires a strong synchronization between replica leading, in particular for shared memory, to a significant increase of inter-node communications and to a high performance degradation.
Reference: [9] <author> D. B. Johnson E. L. Elnozahy and W. Zwaenepeol. </author> <title> The Performance of Consistent Checkpoint. </title> <booktitle> In Proc. of 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 39-47, </pages> <month> October </month> <year> 1992. </year> <title> INRIA Tolerating Node Failures in Cache Only Memory Architectures 27 </title>
Reference-contexts: All the simulations are sufficiently long so that several recovery point establishments occur. The frequencies range from 400 to 0 recovery points per second. These frequencies may seem quite high in front of other evaluations like in <ref> [9] </ref>. In the absence of real recovery point frequencies, they give, however, the performance degradation for different computing environments. <p> To limit the recovery point establishment overhead, other techniques could be envisaged. Dependency tracking between communicating processors could limit the number of processors included in a recovery point establishment operation [2]. Recovery point establishment performed in parallel with the execution of the application <ref> [19, 9] </ref> could hide the time required by this operation. The extended protocol can be implemented in any COMA. For a real implementation, other aspects of fault tolerance have however to be investigated. In particular error detection and confinement should be included in the nodes to ensure the fail-stop property.
Reference: [10] <author> S. Frank, H. Burkhardt, and J. Rothnie. </author> <title> The KSR1 : Bridging the Gap Between Shared Memory and MPPs. </title> <booktitle> In IEEE Computer Society, editor, Proc. of spring COMPCON'93, </booktitle> <pages> pages 285-294, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Shared memory provides a flexible and powerful computing environment. Two variations of these architectures have emerged: Cache Coherent Non Uniform Memory Access machines (CC NUMA) [1, 18], which statically divide the main memory among the nodes of the architecture, and Cache Only Memory Architectures (COMAs) <ref> [14, 10] </ref> which convert the per node memory into a large cache of the shared address space, called an Attraction Memory (AM). Due to their increasing number of components both CC-NUMA machines and COMAs have, despite an important increase in hardware reliability, a very high probability to experience hardware failures. <p> Performance evaluation results obtained by simulation are given in Section 4 for an implementation of the protocol in a slotted ring COMA similar to a single ring KSR1 <ref> [10] </ref>. Section 5 concludes our presentation. 2 Cache Only Memory Architectures CC-NUMA architectures and COMAs have similar organizations. They both have a distributed main memory and a scalable interconnection network. <p> COMA usually use hierarchical organization to locate memory items on a miss. Directories at each level of the hierarchy maintain information about memory item copies located in a sub-hierarchy. Such an organization is used in the DDM [14] and KSR1 <ref> [10] </ref> architectures. Basically, all COMAs use the same coherence protocol. This protocol can be simplified to four basic item states which change according to requests received from the local processor (Pread/Pwrite), or from remote nodes over the network (Nread/Nwrite). <p> RR n-2335 16 M. Ban^atre, A. Ge*aut & C. Morin 4 Performance Evaluation in a Slotted Ring COMA In this section, we evaluate through simulations, the overheads introduced by the extended coherence protocol in a slotted ring COMA architecture, such as a single ring KSR1 <ref> [10] </ref>, and which uses a snooping coherence protocol. For such an architecture, the protocol can be used to tolerate either hardware or software transient node failures. To take permanent node and network failures into account, some modifications to the existing hardware should be envisaged.
Reference: [11] <author> A. Ge*aut and P. Joubert. SPAM: </author> <title> A Multiprocessor Execution Driven Simulation Kernel. </title> <journal> International Journal in Computer Simulation, </journal> <note> To appear, </note> <year> 1994. </year>
Reference-contexts: Ban^atre, A. Ge*aut & C. Morin [21]. Each node is simulated by a process which interacts with the other components of the architecture. To collect address traces, the simulator uses the SPAM execution-driven simulation kernel <ref> [11] </ref>. The simulated architecture is not exactly similar to a real KSR1 ring. It retains however, all of the most important features of this architecture. The network accepts 16 simultaneous requests though the real architecture uses 13 slots.
Reference: [12] <author> J. Gray. </author> <booktitle> Notes on Database Operating Systems., volume 60 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1978. </year>
Reference-contexts: The algorithm is depicted in Figure 3. To guarantee atomicity of the establishment, and hence to tolerate any possible failure during its execution, the algorithm uses a traditional two-phase commit protocol <ref> [12] </ref>. During the first phase (establish phase), the new version of the recovery point is established by replicating all modified items on two distinct nodes, in state Pre-commit. This new recovery point is made of all non modified recovery item copies (Shared-CK copies) plus all modified item copies (Pre-commit copies).
Reference: [13] <author> A. Gupta and W.D. Weber. </author> <title> Cache Invalidation Patterns in Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: A combination of misses and injections is also a reason for this limited pollution overhead. The only exception is Barnes where T P ollution increases with higher recovery point frequencies. The reason is that this application uses mostly read objects <ref> [13] </ref> replicated on many nodes. The presence of Inv-CK copies generates new readmiss/injections. Even for this application the pollution overhead remain low (less than 5%).
Reference: [14] <author> E. Hagersten, A. Landin, and S. Haridi. </author> - <title> DDM A Cache-Only Memory Architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Shared memory provides a flexible and powerful computing environment. Two variations of these architectures have emerged: Cache Coherent Non Uniform Memory Access machines (CC NUMA) [1, 18], which statically divide the main memory among the nodes of the architecture, and Cache Only Memory Architectures (COMAs) <ref> [14, 10] </ref> which convert the per node memory into a large cache of the shared address space, called an Attraction Memory (AM). Due to their increasing number of components both CC-NUMA machines and COMAs have, despite an important increase in hardware reliability, a very high probability to experience hardware failures. <p> This is the reason why we have chosen to investigate COMAs. COMA usually use hierarchical organization to locate memory items on a miss. Directories at each level of the hierarchy maintain information about memory item copies located in a sub-hierarchy. Such an organization is used in the DDM <ref> [14] </ref> and KSR1 [10] architectures. Basically, all COMAs use the same coherence protocol. This protocol can be simplified to four basic item states which change according to requests received from the local processor (Pread/Pwrite), or from remote nodes over the network (Nread/Nwrite).
Reference: [15] <author> E. S. Harrison and E. Schmitt. </author> <title> The Structure of System/88, a Fault-Tolerant Computer. </title> - <journal> IBM Systems Journal, </journal> <volume> 26(3) </volume> <pages> 293-318, </pages> <year> 1987. </year>
Reference-contexts: This technique has several advantages over other fault tolerance approaches. Active software replication [8] requires a strong synchronization between replica leading, in particular for shared memory, to a significant increase of inter-node communications and to a high performance degradation. Hardware static replication like nMR (n Modular Redundancy) <ref> [6, 15] </ref> with voting requires a full replication of a majority of hardware components and is certainly too expensive to be applied to architectures with a large number of components. In contrast, BER limits the hardware development and allows the use of all the processors for a computation.
Reference: [16] <author> B. Lampson. </author> <title> Atomic Transactions. In Distributed Systems and Architecture and Implementation : an Advanced Course, </title> <booktitle> volume 105 of Lecture Notes in Computer Science, </booktitle> <pages> pages 246-265. </pages> <publisher> Springer Verlag, </publisher> <year> 1981. </year>
Reference-contexts: These two properties can be guaranteed by storing recovery points INRIA Tolerating Node Failures in Cache Only Memory Architectures 7 in stable storage <ref> [16] </ref> for which efficient hardware implementations have already been proposed [2, 3, 4, 7]. In SSMMs, it is however unreasonable to develop specific hardware stable storage implementations as the cost of the architecture would drastically grow.
Reference: [17] <author> P.A. Lee and T. Anderson. </author> <title> Fault Tolerance: </title> <booktitle> Principles and Practice, volume 3 of Dependable Computing and Fault-Tolerant Systems. </booktitle> <address> Springer Verlag, </address> <note> second revised edition, </note> <year> 1990. </year>
Reference-contexts: Fault tolerance becomes then mandatory rather than optional for large scale shared memory multiprocessor architectures. In this paper, we propose a new solution to cope with multiple transient and single permanent node failures in a COMA. Our approach uses a backward error recovery scheme <ref> [17] </ref> where the replication mechanisms of a COMA are used to ensure the conservation and replication of recovery data in the AMs of the architecture. To implement this, an extended coherence protocol manages transparently both current and recovery data. <p> RR n-2335 6 M. Ban^atre, A. Ge*aut & C. Morin 3 Extending the Coherence Protocol for To lerating Node Failures 3.1 Principles Among the techniques that can be used to tolerate node failures in a SSMM, Backward Error Recovery (BER) <ref> [17] </ref> seems to be the most attractive solution. This technique has several advantages over other fault tolerance approaches. Active software replication [8] requires a strong synchronization between replica leading, in particular for shared memory, to a significant increase of inter-node communications and to a high performance degradation. <p> Such an approach requires a coordination of communicating processors during the establishment of a recovery point so that the set of processor recovery point always form a recovery line <ref> [17] </ref>. In this paper, for the sake of simplicity, coordination between processors is ensured by using a global checkpointing scheme (all processors are involved in a recovery point establishment). To tolerate any single failure in a system, recovery data must satisfy two properties.
Reference: [18] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horo-witz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Shared memory provides a flexible and powerful computing environment. Two variations of these architectures have emerged: Cache Coherent Non Uniform Memory Access machines (CC NUMA) <ref> [1, 18] </ref>, which statically divide the main memory among the nodes of the architecture, and Cache Only Memory Architectures (COMAs) [14, 10] which convert the per node memory into a large cache of the shared address space, called an Attraction Memory (AM).
Reference: [19] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> Real-Time Concurrent Checkpoint for Parallel Programs. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice Parallel Programming (PPOPP), SIGPLAN notices, </booktitle> <volume> volume 25, </volume> <pages> pages 79-88, </pages> <year> 1990. </year>
Reference-contexts: To limit the recovery point establishment overhead, other techniques could be envisaged. Dependency tracking between communicating processors could limit the number of processors included in a recovery point establishment operation [2]. Recovery point establishment performed in parallel with the execution of the application <ref> [19, 9] </ref> could hide the time required by this operation. The extended protocol can be implemented in any COMA. For a real implementation, other aspects of fault tolerance have however to be investigated. In particular error detection and confinement should be included in the nodes to ensure the fail-stop property.
Reference: [20] <author> H. Nilsson and P. Stenstrom. </author> <title> Performance Evaluation of Link-Based Cache Coherence Schemes. </title> <booktitle> Proc. of the 26th Annual Hawa i International Conference on System Sciences, </booktitle> <pages> pages 486-495, </pages> <year> 1993. </year>
Reference-contexts: Table 2 describes their characteristics. Four of the applications (Mp3d, Water, Choleskyr and Barnes) come from the SPLASH benchmark suite [22]. The last one (Solve), is a very simple parallel application resolving a system of N equations by iterations <ref> [20] </ref>. The simulator is implemented with a discrete event simulation library providing management, scheduling and synchronization of lightweight processes RR n-2335 20 M. Ban^atre, A. Ge*aut & C. Morin [21]. Each node is simulated by a process which interacts with the other components of the architecture.
Reference: [21] <author> H. Schwetman. </author> - <title> Csim user's guide, </title> <type> rev. 2. Technical Report ACT-126-90, Rev. 2, </type> <institution> MCC, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The last one (Solve), is a very simple parallel application resolving a system of N equations by iterations [20]. The simulator is implemented with a discrete event simulation library providing management, scheduling and synchronization of lightweight processes RR n-2335 20 M. Ban^atre, A. Ge*aut & C. Morin <ref> [21] </ref>. Each node is simulated by a process which interacts with the other components of the architecture. To collect address traces, the simulator uses the SPAM execution-driven simulation kernel [11]. The simulated architecture is not exactly similar to a real KSR1 ring.

References-found: 21

