URL: http://c.gp.cs.cmu.edu:5103/afs/cs/academic/class/11761-s97/WWW/tex/class_circulate.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/project/theo-4/text-learning/www/resources-new.html
Root-URL: http://www.cs.cmu.edu
Title: Class-based n-gram models of natural language  
Author: Peter F. Brown Vincent J. Della Pietra Peter V. deSouza Jenifer C. Lai Robert L. Mercer 
Date: December 17, 1990  
Abstract: We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their cooccurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. 
Abstract-found: 1
Intro-found: 1
Reference: [Averbuch et al., 1987] <author> Averbuch, A., Bahl, L., Bakis, R., Brown, P., Cole, A., Daggett, G., Das, S., Davies, K., Gennaro, S. D., de Souza, P., Epstein, E., Fraleigh, D., Jelinek, F., Moorhead, J., Lewis, B., Mercer, R., Nadas, A., Nahamoo, D., Picheny, M., Shichman, G., Spinelli, P., Compernolle, D. V., and Wilkens, H. </author> <year> (1987). </year> <title> Experiments with the Tangora 20,000 word speech recognizer. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 701-704, </pages> <address> Dallas, Texas. </address>
Reference-contexts: The vocabulary of English is very large and so, even for small values of n, the number of parameters in an n-gram model is enormous. The IBM Tangora speech recognition system has a vocabulary of about 20,000 words and employs a 3-gram language model with over eight trillion parameters <ref> [Averbuch et al., 1987] </ref>. We can illustrate the problems attendant to parameter estimation for a 3-gram language model with the data in Table 1.
Reference: [Bahl et al., 1983] <author> Bahl, L. R., Jelinek, F., and Mercer, R. L. </author> <year> (1983). </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2):179-190. </journal>
Reference: [Baum, 1972] <author> Baum, L. </author> <year> (1972). </year> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8. </pages>
Reference-contexts: w i j w i1 X j (w i1 1 ): (3) Given values for P r (j) (), the j (w i1 1 ) are chosen, with the help of the EM algorithm, so as to maximize the probability of some additional sample of text called the held-out data <ref> [Baum, 1972, Dempster et al., 1977, Jelinek and Mercer, 1980] </ref>. When we use interpolated estimation to combine the estimates from 1-, 2-, and 3-gram models, we choose the 's to depend on the history, w i1 1 , only through the count of the 2-gram, w i2 w i1 .
Reference: [Brown et al., 1990] <author> Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. </author> <year> (1990). </year> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85. </pages>
Reference-contexts: In the first of these, he should focus his attention on conditional probabilities and on Markov chains; in the second, on entropy and mutual information. 2 Language Models and has recently been proposed for machine translation <ref> [Brown et al., 1990] </ref> and for automatic spelling correction [Mays et al., 1990]. In automatic speech recognition, y is an acoustic signal; in machine translation, y is a sequence of words in another language; and in spelling correction, y is a sequence of characters produced by a possibly imperfect typist.
Reference: [Dempster et al., 1977] <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, 39(B):1-38. </journal>
Reference-contexts: w i j w i1 X j (w i1 1 ): (3) Given values for P r (j) (), the j (w i1 1 ) are chosen, with the help of the EM algorithm, so as to maximize the probability of some additional sample of text called the held-out data <ref> [Baum, 1972, Dempster et al., 1977, Jelinek and Mercer, 1980] </ref>. When we use interpolated estimation to combine the estimates from 1-, 2-, and 3-gram models, we choose the 's to depend on the history, w i1 1 , only through the count of the 2-gram, w i2 w i1 .
Reference: [Feller, 1950] <author> Feller, W. </author> <year> (1950). </year> <title> An Introduction to Probability Theory and its Applications, Volume I. </title> <publisher> John Wiley & Sons, Inc. </publisher>
Reference-contexts: The reader who is unfamiliar with this field or who has allowed his facility with some of its concepts to fall into disrepair may profit from a brief perusal of references <ref> [Feller, 1950] </ref> and [Gallager, 1968].
Reference: [Gallager, 1968] <author> Gallager, R. G. </author> <year> (1968). </year> <title> Information Theory and Reliable Communication. </title> <publisher> John Wiley & Sons, Inc. </publisher>
Reference-contexts: The reader who is unfamiliar with this field or who has allowed his facility with some of its concepts to fall into disrepair may profit from a brief perusal of references [Feller, 1950] and <ref> [Gallager, 1968] </ref>.
Reference: [Good, 1953] <author> Good, I. </author> <year> (1953). </year> <title> The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3 and </volume> 4):237-264. 
Reference-contexts: Similarly, of the 1:773 fi 10 16 3-grams that might have occurred, only 75,349,888 actually did occur and of these, 53,737,350 occurred only once each. From these data and Turing's formula <ref> [Good, 1953] </ref>, we can expect that maximum likelihood estimates will be zero for 14.7 percent of the 3-grams and for 2.2 percent of the 2-grams in a new sample of English text.
Reference: [Jelinek and Mercer, 1980] <author> Jelinek, F. and Mercer, R. L. </author> <year> (1980). </year> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop on Pattern Recognition in Practice, </booktitle> <pages> pages 381-397, </pages> <address> Amsterdam, The Netherlands: </address> <publisher> North-Holland. </publisher>
Reference-contexts: As n increases, the accuracy of an n-gram model increases, but the reliability of our parameter estimates, drawn as they must be from a limited training text, decreases. Jelinek and Mercer <ref> [Jelinek and Mercer, 1980] </ref> describe a technique called interpolated estimation that combines the estimates of several language models so as to use the estimates of the more accurate models where they are reliable and, where they are unreliable, to fall back on the more reliable estimates of less accurate models. <p> w i j w i1 X j (w i1 1 ): (3) Given values for P r (j) (), the j (w i1 1 ) are chosen, with the help of the EM algorithm, so as to maximize the probability of some additional sample of text called the held-out data <ref> [Baum, 1972, Dempster et al., 1977, Jelinek and Mercer, 1980] </ref>. When we use interpolated estimation to combine the estimates from 1-, 2-, and 3-gram models, we choose the 's to depend on the history, w i1 1 , only through the count of the 2-gram, w i2 w i1 .
Reference: [Kucera and Francis, 1967] <author> Kucera, H. and Francis, W. </author> <year> (1967). </year> <title> Computational Analysis of Present-Day American English. </title> <publisher> Brown University Press, </publisher> <address> Providence R.I. </address>
Reference-contexts: We estimated these 's from a held-out sample of 4,630,934 million words. We measure the performance of our model on the Brown corpus which contains a variety of English text and is not included in either our training or held-out data <ref> [Kucera and Francis, 1967] </ref>. The Brown corpus contains 1,014,312 words and has a perplexity of 244 with respect to our interpolated model. 3 Word classes Clearly, some words are similar to other words in their meaning and syntactic function.
Reference: [Mays et al., 1990] <author> Mays, E., Damerau, F. J., and Mercer, R. L. </author> <year> (1990). </year> <title> Context based spelling correction. </title> <booktitle> In Proceedings of the IBM Natural Language ITL, </booktitle> <pages> pages 517-522, </pages> <address> Paris, France. </address> <month> 17 </month>
Reference-contexts: In the first of these, he should focus his attention on conditional probabilities and on Markov chains; in the second, on entropy and mutual information. 2 Language Models and has recently been proposed for machine translation [Brown et al., 1990] and for automatic spelling correction <ref> [Mays et al., 1990] </ref>. In automatic speech recognition, y is an acoustic signal; in machine translation, y is a sequence of words in another language; and in spelling correction, y is a sequence of characters produced by a possibly imperfect typist.
References-found: 11

