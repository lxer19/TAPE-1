URL: ftp://ftp.cs.washington.edu/tr/1993/04/UW-CSE-93-04-09.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/ladner/papers.html
Root-URL: 
Title: Building Counting Networks from Larger Balancers  
Author: Edward W. Felten Anthony LaMarca Richard Ladner 
Date: April 30, 1993  
Address: Seattle, WA 98195 U.S.A.  
Affiliation: Dept. of Computer Science and Engineering University of Washington  
Pubnum: Technical Report #93-04-09  
Abstract: We introduce a generalization of the counting networks of Aspnes, Herlihy, and Shavit [AHS91]. Our counting networks are constructed using k-balancers, rather than the 2-balancers of Aspnes et al. For reasonable values of k, k-balancers and 2-balancers can be implemented with equal efficiency on existing computers. Our k-bitonic networks have depths ranging from O(1) to O(log 2 w), where w is the number of inputs to the network. The depth of our networks varies with the choice of k; choosing the optimal k depends on a tradeoff between the desire for a shallow network and the desire for low contention. We present the k-bitonic construction, prove its correctness, and introduce some useful variations to the construction. We then compare the performance of our networks with the networks of Aspnes et al., using simulation of idealized counting networks. Our networks perform at least as well in all cases, and typically have throughput about 25% higher than the networks of Aspnes et al. 
Abstract-found: 1
Intro-found: 1
Reference: [AA92] <author> E. Ahoronson and H. Attiya. </author> <title> Counting Networks with Arbitrary Fan-Out. </title> <booktitle> In 3rd Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 104-113, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: It should be noted that we are not the first to generalize the balancer. Ahoronson and Attiya use the b-balancer with a finite number of inputs and b outputs <ref> [AA92] </ref>. However, they do not use these larger balancers to construct practical networks. They present an impossibility result showing that networks of certain sizes cannot be built with some sizes of balancers.
Reference: [AHS91] <author> J. Aspnes, M. Herlihy, and N. Shavit. </author> <title> Counting Networks and Multi-processor Coordination. </title> <booktitle> In 23st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 348-358, </pages> <year> 1991. </year>
Reference-contexts: The cost of contention has motivated the search for low-contention coordination algorithms, which require relatively few processors to access any one memory location. Counting networks, which were originally defined by Aspnes, Herlihy, and Shavit <ref> [AHS91] </ref>, are one such mechanism. Counting networks are acyclic networks of shared objects called balancers. Counting networks can be used to build shared counters, producer/consumer buffers and distributed queues, none of which require synchronization on a single shared location. <p> A counting network is a balancing network which has the step property. The step property says that in any quiescent state, 0 y i y j 1 for any 0 i j &lt; w. Apnes, Herlihy and Shavit presented the bitonic counting network construction <ref> [AHS91] </ref> based on Batcher's bitonic sorting network [Bat68]. A bitonic network with w input wires has depth O (log 2 w) 1 ; that is, a token must pass through O (log 2 w) balancers to traverse the network. <p> We call our construction the k-bitonic construction since it is a gen 4 eralization of the original bitonic counting network construction <ref> [AHS91] </ref>. The k-bitonic construction attempts to build the network exclusively from balancers of size k. Our construction, however, is recursive and at times the subproblems are too small to be built from balancers of size k. In these situations, the construction uses balancers as close to size k as possible.
Reference: [AKS83] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> An O(n log n) sorting network. </title> <journal> Combi-natorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference-contexts: Their networks have depth O (log 3 w), or alternatively O (log 2 w) with a very large constant if the AKS sorting network <ref> [AKS83] </ref> is used in the construction.
Reference: [And90] <author> T. E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Scholarship. Part of the research was done while Ladner was at Victoria University of Wellington, New Zealand. 1 The usual approach to coordination is to serialize access to shared data by using a lock. While much work has been done to optimize the performance of locking <ref> [And90, MCS91] </ref>, these techniques still suffer from memory contention, since the processors are forced to serialize through a single memory location. The effect of contention grows as the number of processors increases, and as memory latencies become relatively longer.
Reference: [Bat68] <author> K. E. Batcher. </author> <title> Sorting Networks and Their Applications. </title> <booktitle> In Proceedings of AFIPS Joint Computer Conference, </booktitle> <volume> volume 32, </volume> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference-contexts: The step property says that in any quiescent state, 0 y i y j 1 for any 0 i j &lt; w. Apnes, Herlihy and Shavit presented the bitonic counting network construction [AHS91] based on Batcher's bitonic sorting network <ref> [Bat68] </ref>. A bitonic network with w input wires has depth O (log 2 w) 1 ; that is, a token must pass through O (log 2 w) balancers to traverse the network.
Reference: [Klu91] <author> M. R. Klugerman. </author> <title> Lecture 17: Counting Networks. In F.T. Leighton, </title> <editor> C.E. Leiser-son, and N. Kahale, editors, </editor> <booktitle> Research Seminar Series 15: Advanced Parallel and VLSI Computation, </booktitle> <pages> pages 153-161. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Recently Klugerman has presented alternative constructions with depth O (log w log log w) and O (C log fl w log w) <ref> [Klu91, KP92] </ref>. While these new constructions rep resent an asymptotic improvement in counting network depth, the constants are too large 1 All logarithms are base 2 unless otherwise specified 2 B 2 B 2 B 2 B 4 B 4 B 2 B 2 k-balancer. to make these networks practical.
Reference: [KP92] <author> M. R. Klugerman and C. G. Plaxton. </author> <title> Small-Depth Counting Networks. </title> <booktitle> In 24st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 417-428, </pages> <year> 1992. </year>
Reference-contexts: Recently Klugerman has presented alternative constructions with depth O (log w log log w) and O (C log fl w log w) <ref> [Klu91, KP92] </ref>. While these new constructions rep resent an asymptotic improvement in counting network depth, the constants are too large 1 All logarithms are base 2 unless otherwise specified 2 B 2 B 2 B 2 B 4 B 4 B 2 B 2 k-balancer. to make these networks practical.
Reference: [MCS91] <author> John Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year> <month> 16 </month>
Reference-contexts: Scholarship. Part of the research was done while Ladner was at Victoria University of Wellington, New Zealand. 1 The usual approach to coordination is to serialize access to shared data by using a lock. While much work has been done to optimize the performance of locking <ref> [And90, MCS91] </ref>, these techniques still suffer from memory contention, since the processors are forced to serialize through a single memory location. The effect of contention grows as the number of processors increases, and as memory latencies become relatively longer.
References-found: 8

