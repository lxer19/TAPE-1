URL: ftp://gaia.cs.umass.edu/pub/salehi/ca-sigmetrics.ps.gz
Refering-URL: http://www.cs.umass.edu/~salehi/home.html
Root-URL: 
Title: Scheduling for Cache Affinity in Parallelized Communication Protocols  
Author: James D. Salehi, James F. Kurose, and Don Towsley 
Address: Amherst MA 01003, USA  
Affiliation: Dept. of Computer Science, University of Massachusetts,  
Abstract: We explore processor-cache affinity scheduling of parallel network protocol processing in a setting in which protocol processing executes on a shared-memory multiprocessor concurrently with a general workload of non-protocol activity. We find that affinity scheduling can significantly reduce the communication delay associated with protocol processing, enabling the host to support a greater number of concurrent streams and to provide a higher maximum throughput to individual streams. In addition, we compare implementations of two parallelization approaches(Locking and Independent Protocol Stacks) with very different caching behaviors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Salehi, J. Kurose, D. Towsley. </author> <title> Scheduling for Cache Affinity in Parallelized Communication Protocols. </title> <type> TR UM-CS-1994-075, </type> <institution> U. Massachusetts, </institution> <month> Oct. </month> <year> 1994. </year> <note> (available via ftp from gaia.cs.umass.edu in pub/Sale94:Scheduling.ps.Z) </note>
Reference-contexts: Under Wired-Streams processor scheduling, a packet-filter operation indicates which processor is selected when scheduling a new protocol thread, based on the stream ID of the next packet. During packet processing (d), the protocol thread accesses memory resources, which can also be affinity scheduled. In the full paper <ref> [1] </ref> we also consider this alternative. To conduct our study, we implement two approaches to paral-lelizing the receive-side UDP/IP/FDDI protocol stack of our initially unparallelized version of the x-kernel. * Locking enables parallelism by protecting accessto non-read-only protocol data structures with software locks (Figure 2a provides an abstract view). <p> Extensive details of the simulation model, its analytic component, the experimental design and resulting measurements are provided in <ref> [1] </ref>. 3 Experimental measurements The experimental measurements suggest the importance of scheduling for code affinity. When the protocol footprint is cached, packet execution time under IPS (Locking) is 57.4 s (66.6 s). But when executing at a cold processor, the execution time is 232:2s (284:3s). <p> The time between packet arrivals is exponentially distributed; this is appropriate for the type of multiprocessor communication, such as IPC or RPC, which benefits from the per-packet latency reduction enabled by affinity scheduling. See <ref> [1] </ref> for graphs reflecting the overhead of software checksumming, which is not shown here. Locking, illustrating that MRU processor scheduling performs best across the broad range of arrival rates, reducing mean packet delay by as much as 30-50%. <p> Although not shown in Figure 3, Wired-Streams scheduling increases the maximum supportable arrival rate 21% (in comparison to MRU/LRU). We have also shown that thread stack affinity scheduling yields a 10-15% delay reduction across the broad range of packet arrival rates <ref> [1] </ref>. packet delay under IPS is 40-50% lower than under Locking, and the host's maximum supportable arrival rate 70-120% higher. Figure 4 does not reflect that MRU processor scheduling is non-work-conserving under IPS, but work-conserving under Locking. <p> Figure 4 does not reflect that MRU processor scheduling is non-work-conserving under IPS, but work-conserving under Locking. A comparison of Locking with IPS under a burstier arrival process 2 shows that Locking provides lower mean delay for this burstier traffic, although IPS maintains its much higher maximum arrival rate <ref> [1] </ref>. Additional results can be found in the full paper.
Reference: [2] <author> J. Singh, H. Stone and D. Thiebaut. </author> <title> A Model of Workloads and Its Use in Miss-Rate Prediction for Fully Associative Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 811-825, </pages> <month> Jul. </month> <year> 1992. </year>
Reference-contexts: The execution of the non-protocol task displaces the protocol footprint from the processor cache, increasing protocol execution time. To capture this effect, we develop an analytic model of packet execution time, combining well-established analytic results from other researchers <ref> [2, 4] </ref>. The model accounts for the impact of general non-protocol activity on packet execution time as well as the specific cache architecture and organization of our experimental platform.
Reference: [3] <author> M. Squillante and E. Lazowska. </author> <title> Using Processor Cache Affinity Information in Shared-Memory Multiprocessor Scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: 1 Introduction and problem formulation Parallel networking is an area of research which has recently generated considerable interest, and one for which processor-cache affinity scheduling <ref> [3] </ref> has not yet been explored.
Reference: [4] <author> D. Thiebaut and H. Stone. </author> <title> Footprints in the Cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> Nov. </month> <year> 1987. </year> <title> 2 Specifically, for deterministic/geometric-sized batches with exponential inter-batch arrival times. </title> <type> 2 </type>
Reference-contexts: The execution of the non-protocol task displaces the protocol footprint from the processor cache, increasing protocol execution time. To capture this effect, we develop an analytic model of packet execution time, combining well-established analytic results from other researchers <ref> [2, 4] </ref>. The model accounts for the impact of general non-protocol activity on packet execution time as well as the specific cache architecture and organization of our experimental platform.
References-found: 4

