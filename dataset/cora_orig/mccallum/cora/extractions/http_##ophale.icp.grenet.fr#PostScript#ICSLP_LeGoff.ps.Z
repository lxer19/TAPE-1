URL: http://ophale.icp.grenet.fr/PostScript/ICSLP_LeGoff.ps.Z
Refering-URL: http://ophale.icp.grenet.fr/benoit.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A TEXT-TO-AUDIOVISUAL-SPEECH SYNTHESIZER FOR FRENCH  
Author: Bertrand Le Goff, Christian Benot 
Address: la Communication Parle, INPG/ENSERG-Universit Stendhal, BP25X 38040, GRENOBLE Cdex 9, France  
Affiliation: Institut de  
Abstract: An audiovisual speech synthesizer from unlimited French text is here presented. It uses a 3-D parametric model of the face. The facial model is controlled by eight parameters. Target values have been assigned to the parameters, for each French viseme, based upon measurements made on a human speaker. Parameter trajectories are modeled by means of dominance functions associated with each parameter and each viseme. A dominance function is characterized by three coefficients so that coarticulation finally depends on the phonetic context, the speech rate, and an "hypo-hyper articulation" coefficient adjustable by the user. Finally, the visual and audiovisual intelligibility of our visual synthesizer has been evaluated in its first version, and compared to that of the acoustic synthesizer on which it was implemented. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bailly G. & Guerti M. </author> <title> "Synthesis-by-rule for French", </title> <booktitle> Proceedings of the XIIth International Congress of Phonetic Sciences, Aix-en-Provence, France, n2, </booktitle> <pages> 506-511, </pages> <year> 1991. </year>
Reference-contexts: This technique allows parameter trajectories to be smoothed over time. It thus gives a fair account of the coarticulation phenomenon. Our visual synthesizer is now synchronized with the diphone-based speech synthesizer developed at the ICP (Bailly & Guerti, 1991 <ref> [1] </ref>). Finally, a first evaluation of the system performance was run in terms of its visual and audiovisual intelligibility compared to the auditory intelligibility of the acoustic synthesizer. 2. TALKING FACE SYNTHESIS underlying structure (without teeth and tongue) uttering a /i/.
Reference: 2. <author> Benot C., Lallouache M.T., Mohamadi T. & Abry C. </author> <title> "A set of French visemes for visual speech synthesis", Talking Machines: Theories, Models and Designs , G. </title> <editor> Bailly and C. Benot, Eds, </editor> <publisher> Elsevier Science Publishers B.V., North-Holland, Amsterdam, </publisher> <pages> 485-504, </pages> <year> 1992. </year>
Reference-contexts: Target Values Seventeen "visemes" (Fisher, 1968 [9]) are necessary to describe the visual production of French at the symbolic level: [ D], [i], <ref> [y, u, 2, o, ofi] </ref>, [e, (, (fi], [o], [, fi], [a fi], [p, b, m], [t, d, n], [k, g], [f, v], [s, z], [ 6, =], [l], [ @*fl [w], [j].
Reference: 3. <author> Benot C., Mohamadi T. & Kandel S. </author> <title> "Audio-Visual Intelligibility of French speech in noise" , Journal of Speech & Hearing Research, </title> <booktitle> n37, </booktitle> <pages> 1195-1203, </pages> <year> 1994. </year>
Reference-contexts: Visible speech is particularly effective when the auditory speech is degraded, because of noise, bandwidth filtering, or hearing-impairment, as shown for long in English (Sumby & Pollack, 1954 [18]; Summerfield et al., 1989 [19]; Erber, 1969 [7]; Erber, 1975 [8]), and more recently in French (Benot et al., 1994 <ref> [3] </ref>). Synthetic faces also increase the intelligibility of natural speech when the facial gestures and speech sounds are coherent (Le Goff et al., 1994 [12]; Le Goff et al., 1995 [13]). Therefore, we may easily assume that synthetic faces enhance the intelligibility of synthetic speech. <p> It involves the auditory only and audiovisual presentation of speech material to subjects under several conditions of acoustic degradation. This test has already been performed with entirely natural speech (Benot et al., 1994 <ref> [3] </ref>), as well as with natural acoustic speech and a series of facial models, including that here presented, animated from measurements of the speaker's facial gestures (Le Goff et al., 1995 [13]). 3.1. Protocol Eighteen nonsense words were presented to thirty French normal-hearers.
Reference: 4. <author> Benot C., Fuster-Duran A. & Le Goff B. </author> <title> "An investigation of hypo- and hyper-speech in the visual modality", </title> <booktitle> Proceedings of ETRW 96, </booktitle> <address> Autrans, France, </address> <year> 1996. </year>
Reference-contexts: In addition, three other visual conditions have been tested and discussed elsewhere (Benot et al., 1996 <ref> [4] </ref>). Subjects had to give a response to both the vowel and the consonant from a 18-choice response box on a screen. They also had to rank the confidence in their response on a 0-10 scale for each stimulus. <p> Therefore, it may serve in many experiments to investigate visual and bimodal speech perception: It was recently used to evaluate the importance of hypo- and hyper-speech in the visual modality (Benot et al., 1996 <ref> [4] </ref>). In add ition, audiovisual synthesis has many potential applications as a human/machine interface, and in computer-aided learning of a foreign languages and of speechreading. The audiovisual speech synthesizer here presented is a first step towards a more efficient system.
Reference: 5. <author> Cohen M.M. & Massaro D.W. </author> <title> "Synthesis of visible speech", Behaviour Research Methods, </title> <booktitle> Instruments "& Computers , 22(2), </booktitle> <pages> 260-263, </pages> <year> 1990. </year>
Reference: 6. <author> Cohen M.M. & Massaro D.W. </author> <title> "Modeling coarticulation in synthetic visual speech", </title> <booktitle> Proceedings of Computer Animation93, </booktitle> <editor> Magnenat-Thalmann & Thalmann Eds, </editor> <address> Geneve, Suisse, </address> <year> 1993. </year>
Reference-contexts: Target Values Seventeen "visemes" (Fisher, 1968 [9]) are necessary to describe the visual production of French at the symbolic level: [ D], [i], [y, u, 2, o, ofi], [e, (, (fi], [o], [, fi], [a fi], [p, b, m], [t, d, n], [k, g], [f, v], [s, z], <ref> [ 6, =] </ref>, [l], [ @*fl [w], [j]. Two classes were added: a "prephonatory" shape, with lips ajar; and a "rest" position, with closed lips, because these shapes do not match "speech" gestures.
Reference: 7. <author> Erber N.P. </author> <title> "Interaction of audition and vision in the recognition of oral speech stimuli" Journal of Speech & Hearing Research, </title> <booktitle> n12, </booktitle> <pages> 423-425, </pages> <year> 1969. </year>
Reference-contexts: Visible speech is particularly effective when the auditory speech is degraded, because of noise, bandwidth filtering, or hearing-impairment, as shown for long in English (Sumby & Pollack, 1954 [18]; Summerfield et al., 1989 [19]; Erber, 1969 <ref> [7] </ref>; Erber, 1975 [8]), and more recently in French (Benot et al., 1994 [3]). Synthetic faces also increase the intelligibility of natural speech when the facial gestures and speech sounds are coherent (Le Goff et al., 1994 [12]; Le Goff et al., 1995 [13]).
Reference: 8. <author> Erber N.P. </author> <title> "Auditory-visual perception of speech", </title> <journal> Journal of Speech & Hearing Disorders, </journal> <volume> n40, </volume> <pages> 481-492, </pages> <year> 1975. </year>
Reference-contexts: Visible speech is particularly effective when the auditory speech is degraded, because of noise, bandwidth filtering, or hearing-impairment, as shown for long in English (Sumby & Pollack, 1954 [18]; Summerfield et al., 1989 [19]; Erber, 1969 [7]; Erber, 1975 <ref> [8] </ref>), and more recently in French (Benot et al., 1994 [3]). Synthetic faces also increase the intelligibility of natural speech when the facial gestures and speech sounds are coherent (Le Goff et al., 1994 [12]; Le Goff et al., 1995 [13]).
Reference: 9. <author> Fisher C.G. </author> <title> "Confusions among visually perceived consonants", </title> <journal> Journal of Speech & Hearing Research , n15, </journal> <pages> 474-482, </pages> <year> 1968. </year>
Reference-contexts: Target Values Seventeen "visemes" (Fisher, 1968 <ref> [9] </ref>) are necessary to describe the visual production of French at the symbolic level: [ D], [i], [y, u, 2, o, ofi], [e, (, (fi], [o], [, fi], [a fi], [p, b, m], [t, d, n], [k, g], [f, v], [s, z], [ 6, =], [l], [ @*fl [w], [j].
Reference: 10. <author> Guiard-Marigny T., Adjoudani A. & Benot C. </author> <title> "A 3D model of the lips", </title> <booktitle> Proceedings of the 2nd ETRW on Speech Synthesis, </booktitle> <address> New Platz, USA, </address> <year> 1994. </year>
Reference-contexts: In that perspective, a high-resolution model of the lips, controlled by only five parameters, was developed at the ICP (Guiard-Marigny et al., 1994 <ref> [10] </ref>) and it was then implemented by Le Goff et al.(1994)[12] onto Cohen & Massaro (1990)[5]'s face model which we modified so that it is now controlled by only eight articulatory parameters. <p> These data are easily obtained from a string of phonemes and their duration, as predicted and provided by a text-to-speech synthesizer. 2.1. The Face Model Our facial model (see figure 1) was built up upon the 3-D lip model developed at the ICP (Guiard-Marigny et al., 1994 <ref> [10] </ref>). This 3-D model was implemented by Le Goff et al. (1994)[12] on a version of the original Parke (1974)[15]'s model later modified by Cohen & Massaro (1990)[5].
Reference: 11. <author> Henton C. & Litwinowicz P. </author> <title> "Saying and seeing it with feeling: techniques for synthesizing visible, emotional speech", </title> <booktitle> Proceedings of the Second ESCA/IEEE Workshop on Speech Synthesis, </booktitle> <address> New Paltz, New York, USA, 73-76, </address> <year> 1994. </year>
Reference-contexts: visual synthesis of speech, existing systems are essentially based on a limited set of facial images occurring in the natural production of speech that are displayed one after the other, depending on the phoneme simultaneously uttered by the acoustic synthesizer (e.g., Saintourens et al., 1990 [17]; Henton et al., 1994 <ref> [11] </ref>). Actually, coarticulation effects and transition smoothing are much more naturally simulated by means of parametric models specially controlled for speech production, as that developed by Cohen and Massaro (1993)[6] from the original Parke (1974)[15]'s model.
Reference: 12. <author> Le Goff B., Guiard-Marigny T., Cohen M. & Benot C. </author> <title> "Real-Time Analysis-Synthesis and Intelligibility of Talking Faces", </title> <booktitle> Proceedings of the Second ESCA/IEEE Workshop on Speech Synthesis, </booktitle> <address> New Paltz, New York, USA, 53-56, </address> <year> 1994. </year>
Reference-contexts: Synthetic faces also increase the intelligibility of natural speech when the facial gestures and speech sounds are coherent (Le Goff et al., 1994 <ref> [12] </ref>; Le Goff et al., 1995 [13]). Therefore, we may easily assume that synthetic faces enhance the intelligibility of synthetic speech. However, this goal can only be reached if the articulatory parameters of the facial animation signal the same message as the auditory speech.
Reference: 13. <author> Le Goff B., Guiard-Marigny T. & Benot C. </author> <title> "Read my lips ... and my jaw! How intelligible are the components of a speaker's face?" Procedings of Eurospeech'95, </title> <address> Madrid, Spain, </address> <year> 1995. </year>
Reference-contexts: Synthetic faces also increase the intelligibility of natural speech when the facial gestures and speech sounds are coherent (Le Goff et al., 1994 [12]; Le Goff et al., 1995 <ref> [13] </ref>). Therefore, we may easily assume that synthetic faces enhance the intelligibility of synthetic speech. However, this goal can only be reached if the articulatory parameters of the facial animation signal the same message as the auditory speech. <p> This test has already been performed with entirely natural speech (Benot et al., 1994 [3]), as well as with natural acoustic speech and a series of facial models, including that here presented, animated from measurements of the speaker's facial gestures (Le Goff et al., 1995 <ref> [13] </ref>). 3.1. Protocol Eighteen nonsense words were presented to thirty French normal-hearers. The corpus was made of VCVCV sequences, with V = /a/, /i/ or /y/, and C = /b/, / = /, /l/, /R/, /v/ or /z/. <p> This last observation tends to prove that the coarticulation model used is rather robust to hypoarticulation, since identification of vowels and consonants is not significantly decreased when undershoots are generated, that is when targets are not reached. However, synthesizer's scores are far below those previously obtained in <ref> [13] </ref>. The same corpus uttered by the same face (tongue excluded) animated by parameters measured on a human face was identified correctly at 41.3% from vision alone. As with a natural face, subjects could identify 63% of the stimuli.
Reference: 14. <author> McGurk H. & MacDonald J. </author> <title> "Hearing Lips and Seeing Voices", </title> <journal> Nature, </journal> <volume> 264, 746748, </volume> <year> 1976. </year>
Reference-contexts: However, this goal can only be reached if the articulatory parameters of the facial animation signal the same message as the auditory speech. If an ambiguous message or even a contradictory message is given by the visible speech, then intelligibility is decreased (McGurk & MacDonald, 1976 <ref> [14] </ref>). Most of the existing parametric models of the human face have been developed to optimize the visual rendering of facial expressions (Parke, 1974 [15]; Waters, 1987 [20]; Platt & Badler, 1981 [16]; Viaud & Yahia, 1992 [21]).
Reference: 15. <author> Parke F.I. </author> <title> "A parametric model for human faces", </title> <type> PhD Dissertation, </type> <institution> University of Utah, Department of Computer Sciences, </institution> <year> 1974. </year>
Reference-contexts: If an ambiguous message or even a contradictory message is given by the visible speech, then intelligibility is decreased (McGurk & MacDonald, 1976 [14]). Most of the existing parametric models of the human face have been developed to optimize the visual rendering of facial expressions (Parke, 1974 <ref> [15] </ref>; Waters, 1987 [20]; Platt & Badler, 1981 [16]; Viaud & Yahia, 1992 [21]).
Reference: 16. <author> Platt S.M. & Badler N.I. </author> <title> "Animating Facial Expressions", </title> <journal> Computer Graphics, </journal> <volume> 15, </volume> <pages> 245-252, </pages> <year> 1981. </year>
Reference-contexts: Most of the existing parametric models of the human face have been developed to optimize the visual rendering of facial expressions (Parke, 1974 [15]; Waters, 1987 [20]; Platt & Badler, 1981 <ref> [16] </ref>; Viaud & Yahia, 1992 [21]).
Reference: 17. <author> Saintourens M., Tramus M.H., Huitric H. & Nahas M. </author> <title> "Creation of a synthetic face speaking in real time with a synthetic voice", </title> <booktitle> Proceedings of ESCA workshop on speech synthesis, Autrans, France, </booktitle> <pages> 249-252, </pages> <year> 1990. </year>
Reference-contexts: As regards the visual synthesis of speech, existing systems are essentially based on a limited set of facial images occurring in the natural production of speech that are displayed one after the other, depending on the phoneme simultaneously uttered by the acoustic synthesizer (e.g., Saintourens et al., 1990 <ref> [17] </ref>; Henton et al., 1994 [11]). Actually, coarticulation effects and transition smoothing are much more naturally simulated by means of parametric models specially controlled for speech production, as that developed by Cohen and Massaro (1993)[6] from the original Parke (1974)[15]'s model.
Reference: 18. <author> Sumby W.H. & Pollack I. </author> <title> "Visual contribution to speech intelligibility in noise" , Journal of the Acoustical Society of America, </title> <booktitle> n26, </booktitle> <pages> 212-215, </pages> <year> 1954. </year>
Reference-contexts: 1. INTRODUCTION There is valuable and effective information afforded by a view of the speaker's face in speech perception by humans. Visible speech is particularly effective when the auditory speech is degraded, because of noise, bandwidth filtering, or hearing-impairment, as shown for long in English (Sumby & Pollack, 1954 <ref> [18] </ref>; Summerfield et al., 1989 [19]; Erber, 1969 [7]; Erber, 1975 [8]), and more recently in French (Benot et al., 1994 [3]).
Reference: 19. <author> Summerfield Q., MacLeod A., McGrath M. & Brooke M. </author> <title> "Lips, teeth, and the benefits of lipreading", in Handbook of Research on Face Processing , A.W. </title> <editor> Young & H.D. Ellis Editors, </editor> <publisher> Elsevier Science Publishers, </publisher> <pages> 223-233, </pages> <year> 1989. </year>
Reference-contexts: Visible speech is particularly effective when the auditory speech is degraded, because of noise, bandwidth filtering, or hearing-impairment, as shown for long in English (Sumby & Pollack, 1954 [18]; Summerfield et al., 1989 <ref> [19] </ref>; Erber, 1969 [7]; Erber, 1975 [8]), and more recently in French (Benot et al., 1994 [3]). Synthetic faces also increase the intelligibility of natural speech when the facial gestures and speech sounds are coherent (Le Goff et al., 1994 [12]; Le Goff et al., 1995 [13]).
Reference: 20. <author> Waters K. </author> <title> "A Muscle Model for Animating Three-Dimensional Facial Expression", </title> <booktitle> Computer Graphics , 21, </booktitle> <pages> 17-23, </pages> <year> 1987. </year>
Reference-contexts: Most of the existing parametric models of the human face have been developed to optimize the visual rendering of facial expressions (Parke, 1974 [15]; Waters, 1987 <ref> [20] </ref>; Platt & Badler, 1981 [16]; Viaud & Yahia, 1992 [21]).
Reference: 21. <author> Viaud M.L. & Yahia H. </author> <title> "Facial Animation with wrinkles", </title> <address> Eurographics'92, </address> <year> 1992. </year>
Reference-contexts: Most of the existing parametric models of the human face have been developed to optimize the visual rendering of facial expressions (Parke, 1974 [15]; Waters, 1987 [20]; Platt & Badler, 1981 [16]; Viaud & Yahia, 1992 <ref> [21] </ref>).
References-found: 21

