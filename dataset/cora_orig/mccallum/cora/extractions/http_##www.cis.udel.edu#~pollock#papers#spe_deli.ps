URL: http://www.cis.udel.edu/~pollock/papers/spe_deli.ps
Refering-URL: http://www.cis.udel.edu/~jochen/passages/pubs.htm
Root-URL: http://www.cis.udel.edu
Email: ffenwick,pollockg@cis.udel.edu  
Phone: (302) 831-1953 fax: (302) 831-8458  
Title: Issues and Experiences in Implementing a Distributed Tuplespace  
Author: James B. Fenwick Jr. Lori L. Pollock 
Address: 19716  
Affiliation: Department of Computer and Information Sciences University of Delaware Newark, DE  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: However, implementations that select positive broadcast as the method of distributing tuples must also select a coherence protocol as tuples are duplicated on all nodes. The available protocols are essentially the same as those developed by the cache coherency research (for background one can start with <ref> [28, 13, 1, 15] </ref>, and for more current research see [14, 30]). Bjornson [6] noted the advantages of replicating "read-only" tuples, thus implementations which provide this optimization must also decide on an appropriate coherence protocol. <p> The other type of tuple reuses the unique identifier, indicates the process to execute and its arguments, and the position of its return value in the resultant tuple. For example, the eval statement above would translate into code resembling, eid = unique_eval_id (); e_values [0] = 3; e_values <ref> [1] </ref> = i; out ("resultant values", eid, 2, e_values); e_args [0] = i; out ("eval process", eid, 2, FOO, e_args); out ("eval process", eid, 3, BAR, e_args); The eval server repeatedly performs the following steps.
Reference: [2] <author> Sudhir Ahuja, Nicholas Carriero, David Gelernter, and Venkatesh Krishnaswamy. </author> <title> Matching language and hardware for parallel computation in the linda machine. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 921-929, </pages> <month> Aug </month> <year> 1988. </year>
Reference-contexts: This scheme is best visualized by considering a machine organized as a mesh of processors. Tuples are multicast to all processors in a machine's row, and templates are sent to all processors in the machine's column. This scheme was adopted by Krishnaswamy for his Linda machine <ref> [2] </ref>. Methods using broadcast (or multicast) require a type of coherence protocol. Consider the negative broadcast scheme, for example. A template is broadcast to all nodes and more than one node has a matching tuple.
Reference: [3] <author> Mauricio Arango and Donald Berndt. Tsnet: </author> <title> A linda implementation for networks of unix-based computers. </title> <type> Technical Report YALEU/DCS/TR-739, </type> <institution> Yale University, Department of Computer Science, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: Table 1 shows various combinations of tuples and templates and whether or not they match. 1 In response to the high cost of process creation, many Linda implementations only create processes for the function-valued fields of an eval. 4 int i=3, x <ref> [3] </ref>, y [2,3]; float f; Tuple Template Match? Comments ("semaphore") ("semaphore") yes values match (2) (3) no values do not match (3) (i) yes values and types match (2) (i) no types match, values do not (2 (?i) yes types match (i 2) (2, 5.6) (?i, ?f) yes types match (2, <p> One possibility is for tuples to be stored on the node executing the out or eval 2 . Templates, generated by the in, rd, inp, and rdp operators, are sent to all nodes. Bjornson termed this scheme negative broadcast. This is the method utilized in the implementations of <ref> [31, 3] </ref>. Another possibility is positive broadcast which is the inverse of negative broadcast. Tuples are broadcast to all nodes, and the node executing the in, rd, inp, and rdp operator locally handles the templates. Carriero's S-NET implementation [12] used this scheme. <p> A better solution is planned and involves simply adjusting the identifier for the shared memory segment if a previous attempt was denied. 40 IMPLEMENTATION COMPARISONS There have been several other published implementations of the Linda tuplespace targeting distributed memory machines <ref> [6, 44, 3, 16, 7, 31] </ref>. The differences and similarities of these systems are summarized in Table 2. Blank entries in the table are due to inadequate information in the publication relating to that characteristic. <p> The differences and similarities of these systems are summarized in Table 2. Blank entries in the table are due to inadequate information in the publication relating to that characteristic. The Tsnet publication <ref> [3] </ref> is no longer in print and we were unable to obtain a copy of this report. However, some information is available indirectly from Bjornson's thesis [6]. The two-level tuple transfer protocol used by [44] is a variation of the Example Protocol 2 described in the IMPLEMENTATION ISSUES section.
Reference: [4] <author> David E. Bakken and Richard D. Schlichting. </author> <title> Supporting fault-tolerant parallel programming in linda. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(3) </volume> <pages> 287-302, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Interesting motivation and examples of applicability of multiple tuplespaces is also described. <ref> [32, 4] </ref> also utilize flavors of the multiple tuplespace concept. Presently, there seems to be no clear semantic definition of multiple tuplespaces or how they can/should be implemented. Persistent tuplespaces Another interesting extension is having the tuplespace persist beyond the lifetime of an application. <p> See <ref> [4, 26, 50] </ref> for in-depth treatments of fault-tolerant Linda implementations. 22 implementation of tuplespace managers, tuple and tuplespace data structures, tuplespace access functions, and implementation of the eval operation. Providing a thorough treatment of the implementation details requires examining and discussing source code.
Reference: [5] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 2nd ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 168-176, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In this environment, tuplespace provides a shared memory abstraction on top of a message-passing, distributed memory machine. Because tuplespace is a logically shared memory, it encounters implementation issues similar to other distributed shared memories (DSM) <ref> [33, 35, 5, 27] </ref>. Nitzberg and Lo [36] present a survey of the important issues of implementing DSMs and discuss how the developers of several DSMs faced these design choices. However, because of its special nature, a tuplespace implementation must address several additional issues [44].
Reference: [6] <author> Robert D. Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. The experiences of this implementation effort have found existing distributed tuplespace implementations <ref> [6, 12, 16, 31, 44, 49] </ref> to be incomplete in their descriptions of the implementation issues faced and the potential solutions and pitfalls in addressing these issues. <p> The input to the hash function can be a dedicated key field in every tuple and template (hence its partition identifier) [12], or a combination of the tuplespace partition identifier and the values of actual fields in the tuple/template <ref> [6] </ref>. Bjornson [6] provides a significant amount of information regarding the effectiveness of many hash functions. A potential disadvantage with a hash based scheme lies in the observation that the number of distinct partition identifiers for many applications is typically small. <p> The input to the hash function can be a dedicated key field in every tuple and template (hence its partition identifier) [12], or a combination of the tuplespace partition identifier and the values of actual fields in the tuple/template <ref> [6] </ref>. Bjornson [6] provides a significant amount of information regarding the effectiveness of many hash functions. A potential disadvantage with a hash based scheme lies in the observation that the number of distinct partition identifiers for many applications is typically small. <p> This information could be used to "suggest" which nodes should manage which partitions, or on which nodes processes should run, or a combination of these. Another possibility is a dynamically tunable distribution method. In 9 fact, Bjornson <ref> [6] </ref> describes two runtime heuristics (described below as system extensions) that use actual tuplespace access patterns in an attempt to discover and correct "bad" hash-based distribution decisions. <p> The available protocols are essentially the same as those developed by the cache coherency research (for background one can start with [28, 13, 1, 15], and for more current research see [14, 30]). Bjornson <ref> [6] </ref> noted the advantages of replicating "read-only" tuples, thus implementations which provide this optimization must also decide on an appropriate coherence protocol. Tuple Transfer Protocol Tuple transfer is the movement of a tuple among nodes of the machine either to store the tuple or upon a tuple match or both. <p> For example, on a shared memory machine, access to the values of the parent process' global variables is easily achieved. However, in a network of workstations, this is not the case. Leichter [31] and Bjornson <ref> [6] </ref> also discuss this semantic issue. Due to the semantic confusion, it would seem prudent for implementors to select the most restrictive interpretation, allowing the new process to only have access to r-values of explicitly passed parameters. <p> Providing access to these executables for each node via a common file system could eliminate the complication of runtime transmission of large codes 3 . A general, machine-independent approach is to use an eval server <ref> [6, 41] </ref>. The eval servers, which run on every node, monitor tuplespace for active tuples. A particular eval server can evaluate all processes spec 3 In fact, the wide utilization of NFS makes this rather straightforward. 14 ified within the active tuple, or only one such process. <p> Runtime optimizations Bjornson added a number of enhancements to his basic tuplespace implementation <ref> [6] </ref>. Tuple broadcasting sends tuples to all nodes hoping to eliminate the cost of performing a rd operation. An in operation requires additional processing to remove the replicated tuples. This optimization is most effective with some preprocessor/compiler assistance indicating those partitions that contain rd operations. <p> In particular, Deli successfully executes on a network of SUN 4 workstations connected via Ethernet. Deli is implemented in C on a UNIX platform utilizing the Berkeley socket API for network communication. Deli was modeled after Bjornson's distributed Linda implementation <ref> [6] </ref> which has been used in recent versions of production Linda systems [7], and is also based on our experiences with the ntsnet utility from the SCA Inc. distribution of Linda. <p> The data structures described contain enough informaiton to support the inout collapse optimization; however, the access functions have been simplified and thus do not indicate the alternative handling that inout tuples receive. Deli is modeled after Bjornson's tuplespace description <ref> [6] </ref> to allow fairer comparison. Thus Deli utilizes essentially the same data structures as Bjornson to represent tuples and tuplespace. To save the reader from referring back and forth to Bjornson's work, these data structures are described below. <p> Collisions due to tuples and templates having the same key are chained together in the same bucket of the hash table. However, because the hash table is finite, the possibility remains for tuples/templates with different keys to map to the same bucket. Bjornson <ref> [6] </ref> simply adds these differing-key, colliding tuples/templates to the bucket's chain. This approach is justified, in large part, by noting that the hash table will never require expansion (and subsequent reorganization) which can be a time-consuming step. <p> A better solution is planned and involves simply adjusting the identifier for the shared memory segment if a previous attempt was denied. 40 IMPLEMENTATION COMPARISONS There have been several other published implementations of the Linda tuplespace targeting distributed memory machines <ref> [6, 44, 3, 16, 7, 31] </ref>. The differences and similarities of these systems are summarized in Table 2. Blank entries in the table are due to inadequate information in the publication relating to that characteristic. <p> Blank entries in the table are due to inadequate information in the publication relating to that characteristic. The Tsnet publication [3] is no longer in print and we were unable to obtain a copy of this report. However, some information is available indirectly from Bjornson's thesis <ref> [6] </ref>. The two-level tuple transfer protocol used by [44] is a variation of the Example Protocol 2 described in the IMPLEMENTATION ISSUES section. The rendezvous node tuple transfer protocol [6] dictates that all tuples/templates of a tuplespace partition be sent to one node. <p> However, some information is available indirectly from Bjornson's thesis <ref> [6] </ref>. The two-level tuple transfer protocol used by [44] is a variation of the Example Protocol 2 described in the IMPLEMENTATION ISSUES section. The rendezvous node tuple transfer protocol [6] dictates that all tuples/templates of a tuplespace partition be sent to one node. Because this rendezvous node is solely responsible for the tuples/templates of the partition, no coordination with other nodes is required to forward tuples to requesting nodes. <p> Bjornson describes a method of storing a large tuple locally at the generating node and having the rendezvous node inform the generator node of the intended recipient node upon a match. Bjornson's implementation <ref> [6] </ref> performs a number of optimizations which are applied due to dynamic analysis, and also some optimizations due to limited static (preprocessor) analysis. These include rendezvous node remapping and tuple broadcast. The Linda-LAN system supports the instructional footprinting optimization of Landry and Arthur [29].
Reference: [7] <author> Leigh D. Cagan and Andrew H. Sherman. </author> <title> Linda unites network systems. </title> <journal> IEEE Spectrum, </journal> <volume> 30(12) </volume> <pages> 31-35, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Deli is implemented in C on a UNIX platform utilizing the Berkeley socket API for network communication. Deli was modeled after Bjornson's distributed Linda implementation [6] which has been used in recent versions of production Linda systems <ref> [7] </ref>, and is also based on our experiences with the ntsnet utility from the SCA Inc. distribution of Linda. <p> A better solution is planned and involves simply adjusting the identifier for the shared memory segment if a previous attempt was denied. 40 IMPLEMENTATION COMPARISONS There have been several other published implementations of the Linda tuplespace targeting distributed memory machines <ref> [6, 44, 3, 16, 7, 31] </ref>. The differences and similarities of these systems are summarized in Table 2. Blank entries in the table are due to inadequate information in the publication relating to that characteristic.
Reference: [8] <author> N. Carriero, D. Gelernter, and T.G. Mattson. </author> <title> Linda in heterogeneous computing environments. </title> <booktitle> In Workshop on Heterogeneous Processing '92 Proceedings, </booktitle> <pages> pages 43-46, </pages> <year> 1992. </year>
Reference-contexts: A similar alternative would have the Linda compiler pass an intermediate form of the program to a native compiler on a node of each architecture for code generation. The above discussion assumes a tuplespace accessible by operations of processes in a single application image 4 . <ref> [8] </ref> terms this a closed tuplespace. An open tuplespace would permit access by any Linda operation regardless 4 While multiple instances of the image may be active due to the eval operator, it remains the same image. 17 of application. <p> Because the tuplespace model decouples the communicating processes, there is already support for heterogeneity between applications written in different languages. There are no tuplespace calling convention issues to worry about. <ref> [8] </ref> discusses an approach to providing truly heterogeneous support through the use of multiple and persistent tuplespaces, where communication applications utilize an open tuplespace, and a closed tuplespace is restricted to processes of a single application.
Reference: [9] <author> Nicholas Carriero and David Gelernter. </author> <title> A foundation for advanced compile-time analysis of linda programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 389-404. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: While researchers have demonstrated that distributed tuplespace implementations can be efficient for a wide variety of "real" applications encompassing a large scope of parallel algorithm classifications [19, 34, 11], there remains opportunity for improvement through compiler analysis targeting the underlying message passing <ref> [9, 21, 29, 20] </ref>. Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed.
Reference: [10] <author> Nicholas Carriero and David Gelernter. </author> <title> How to Write Parallel Programs, A First Course. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Our implementation of the generative communication model is an implementation of Linda tuplespace, the best-known implementation of the generative communication model <ref> [23, 10] </ref>. We have built a Linda optimizing compiler based on the SUIF compiler infrastructure [45], and a distributed tuplespace runtime system. This paper focuses more directly on the distributed tuplespace runtime system, called Deli (University of Delaware Linda). <p> tuplespace model, discuss a number of implementation issues and the solutions taken by other researchers, the details of our own implementation, and a comparison of prominent implementations. 2 LINDA TUPLESPACE Linda is a coordination language consisting of a small number of primitive operations which are added into existing sequential languages <ref> [23, 10] </ref>. These operations perform the communication and synchronization necessary for parallel programming. The following subsections describe the basic Linda model, and an improvement that addresses the efficiency of the associative tuplespace memory.
Reference: [11] <author> Nicholas Carriero and David Gelernter. </author> <title> Learning from our successes. </title> <editor> In Janusz S. Kowalik and Lucio Grandinetti, editors, </editor> <booktitle> Software for Parallel Computation, volume 106 of NATO ASI Series F: Computer and Systems Sciences, </booktitle> <pages> pages 37-45. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: While researchers have demonstrated that distributed tuplespace implementations can be efficient for a wide variety of "real" applications encompassing a large scope of parallel algorithm classifications <ref> [19, 34, 11] </ref>, there remains opportunity for improvement through compiler analysis targeting the underlying message passing [9, 21, 29, 20]. Such research requires a tuplespace implementation that allows experimentation and modification.
Reference: [12] <author> Nicholas John Carriero, Jr. </author> <title> Implementation of Tuple Space Machines. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. The experiences of this implementation effort have found existing distributed tuplespace implementations <ref> [6, 12, 16, 31, 44, 49] </ref> to be incomplete in their descriptions of the implementation issues faced and the potential solutions and pitfalls in addressing these issues. <p> types and shapes match (1,2,3,4) (?x) no shapes do not match (1,2,3) (?y [0]) yes types and shapes match Table 1: Tuple Matching Examples Tuplespace Partitioning Improvement The earliest work on making Linda more efficient was directed at minimizing the amount of search required for the associative matching of tuplespace <ref> [12] </ref>. The idea is to partition tuplespace at compile time, and implement each partition with a data structure that makes searching the partition fast. While tuples and templates are runtime objects, the operations which produce tuples (out, eval) and templates (in, inp, rd, rdp) are available at compile time. <p> The input to the hash function can be a dedicated key field in every tuple and template (hence its partition identifier) <ref> [12] </ref>, or a combination of the tuplespace partition identifier and the values of actual fields in the tuple/template [6]. Bjornson [6] provides a significant amount of information regarding the effectiveness of many hash functions. <p> This is the method utilized in the implementations of [31, 3]. Another possibility is positive broadcast which is the inverse of negative broadcast. Tuples are broadcast to all nodes, and the node executing the in, rd, inp, and rdp operator locally handles the templates. Carriero's S-NET implementation <ref> [12] </ref> used this scheme. A hybrid 2 Here, we are speaking of the resultant passive tuple generated by an eval. 8 of these two methods is to multicast tuples and templates to a subset of nodes.
Reference: [13] <author> Lucien M. Censier and Paul Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> Dec </month> <year> 1978. </year> <month> 45 </month>
Reference-contexts: However, implementations that select positive broadcast as the method of distributing tuples must also select a coherence protocol as tuples are duplicated on all nodes. The available protocols are essentially the same as those developed by the cache coherency research (for background one can start with <ref> [28, 13, 1, 15] </ref>, and for more current research see [14, 30]). Bjornson [6] noted the advantages of replicating "read-only" tuples, thus implementations which provide this optimization must also decide on an appropriate coherence protocol.
Reference: [14] <author> David Chaiken. </author> <title> Mechanisms and Interfaces for Software-Extended Coherent Shared Memory. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1994. </year> <month> MIT/LCS/TR-644. </month>
Reference-contexts: The available protocols are essentially the same as those developed by the cache coherency research (for background one can start with [28, 13, 1, 15], and for more current research see <ref> [14, 30] </ref>). Bjornson [6] noted the advantages of replicating "read-only" tuples, thus implementations which provide this optimization must also decide on an appropriate coherence protocol.
Reference: [15] <author> Hoichi Cheong and Alexander V. Veidenbaum. </author> <title> Compiler-directed cache management in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 39-48, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: However, implementations that select positive broadcast as the method of distributing tuples must also select a coherence protocol as tuples are duplicated on all nodes. The available protocols are essentially the same as those developed by the cache coherency research (for background one can start with <ref> [28, 13, 1, 15] </ref>, and for more current research see [14, 30]). Bjornson [6] noted the advantages of replicating "read-only" tuples, thus implementations which provide this optimization must also decide on an appropriate coherence protocol.
Reference: [16] <author> George E. Cline and James D. Arthur. Linda-lan: </author> <title> A controlled parallel processing environment. </title> <type> Technical Report TR 92-37, </type> <institution> Virginia Polytechnic Institute and State University, </institution> <year> 1992. </year>
Reference-contexts: Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. The experiences of this implementation effort have found existing distributed tuplespace implementations <ref> [6, 12, 16, 31, 44, 49] </ref> to be incomplete in their descriptions of the implementation issues faced and the potential solutions and pitfalls in addressing these issues. <p> Processor Location of a Tuple For tuplespace (or any DSM) to allow the sharing of data, a process must be able to locate and retrieve the data it requires. This issue is termed tuplespace organization and distribution in [44]. One choice, taken by Cline and Arthur <ref> [16] </ref>, is to centralize tuplespace on a single node of the parallel machine. All tuples are stored on the one processor, and locating a tuple is simplified because all processes simply route the request to this node. <p> A better solution is planned and involves simply adjusting the identifier for the shared memory segment if a previous attempt was denied. 40 IMPLEMENTATION COMPARISONS There have been several other published implementations of the Linda tuplespace targeting distributed memory machines <ref> [6, 44, 3, 16, 7, 31] </ref>. The differences and similarities of these systems are summarized in Table 2. Blank entries in the table are due to inadequate information in the publication relating to that characteristic.
Reference: [17] <author> C. Davidson. </author> <title> Technical correspondence on linda in contex. </title> <journal> Communications of the ACM, </journal> <volume> 32(10) </volume> <pages> 1249-1252, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The two distinguishing characteristics of tuplespace that give rise to its power and flexibility are communication uncoupling and the associative nature of the logically shared tuplespace. 1 Implementing the shared tuplespace on a distributed memory architecture has raised concerns regarding efficiency and performance <ref> [17] </ref>. While researchers have demonstrated that distributed tuplespace implementations can be efficient for a wide variety of "real" applications encompassing a large scope of parallel algorithm classifications [19, 34, 11], there remains opportunity for improvement through compiler analysis targeting the underlying message passing [9, 21, 29, 20].
Reference: [18] <author> Gary S. Delp, David J. Farber, Ronald G. Minnich, Jonathan M. Smith, and Ming-Chit Tan. </author> <title> Memory as a network abstraction. </title> <journal> IEEE Network, </journal> <volume> 5(4) </volume> <pages> 34-41, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: Granularity refers to the size of the unit of sharing. For some DSMs, this unit is the size of an operating system page [33], while others use a smaller block size <ref> [18] </ref>. One DSM uses a combination of a large and small unit of transfer [39]. For tuplespace, the unit of sharing is the tuple. However, because tuple sizes vary, the tuplespace implementor is not afforded the simplicity of predetermined fixed length transmissions.
Reference: [19] <author> Ashish Deshpande and Martin Schultz. </author> <title> Efficient parallel programming with linda. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <pages> pages 238-244, </pages> <address> Minneapolis, Minnesota, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: While researchers have demonstrated that distributed tuplespace implementations can be efficient for a wide variety of "real" applications encompassing a large scope of parallel algorithm classifications <ref> [19, 34, 11] </ref>, there remains opportunity for improvement through compiler analysis targeting the underlying message passing [9, 21, 29, 20]. Such research requires a tuplespace implementation that allows experimentation and modification.
Reference: [20] <author> James B. Fenwick, Jr. and Lori L. Pollock. </author> <title> Global static analysis for optimizing shared tuple space communication on distributed memory systems. </title> <booktitle> In Proceedings of 8th IASTED International Conference on Parallel and Distributed Computing and Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: While researchers have demonstrated that distributed tuplespace implementations can be efficient for a wide variety of "real" applications encompassing a large scope of parallel algorithm classifications [19, 34, 11], there remains opportunity for improvement through compiler analysis targeting the underlying message passing <ref> [9, 21, 29, 20] </ref>. Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. <p> We have built a Linda optimizing compiler based on the SUIF compiler infrastructure [45], and a distributed tuplespace runtime system. This paper focuses more directly on the distributed tuplespace runtime system, called Deli (University of Delaware Linda). More details on the Linda optimizing compiler can be found in <ref> [20, 21, 22] </ref>. <p> Advanced compiler analysis Two of the runtime optimizations just described require compiler support. The others require the runtime system to constantly monitor and gather statistical information regarding tuple access, and only after a period of time of inefficient accesses are the dynamic optimizations performed. Aggressive compiler analyses 19 <ref> [20, 21] </ref> can improve the situation by finding more opportunities for inout collapse and making more informed judgments for tuple broadcast (i.e., estimating number of rd's versus in's). <p> application connect to an existing persistent tuplespace? How is the persistent 20 tuplespace created initially? How can a persistent tuplespace support a separate but simultaneous invocation of an application? DESIGN GOALS Deli was developed for the purpose of validating and experimenting with our compiler analysis and optimization of Linda programs <ref> [21, 20] </ref>. To make this Linda tuplespace implementation a robust research tool, the following goals were established. These goals are presented here as they affect implementation choices described in the following section. Software-based approach. No specialized hardware or specific interconnection topology should be required. <p> These include rendezvous node remapping and tuple broadcast. The Linda-LAN system supports the instructional footprinting optimization of Landry and Arthur [29]. Deli currently supports static communication optimizations similar to Bjornson but using a more informed and comprehensive analysis within the compiler <ref> [21, 20] </ref>. The trans-puter implementation of Shekhar and Srikant [44] allows the application to specify among a set of alternative implementation parameters such as tuple transfer protocol.
Reference: [21] <author> James B. Fenwick, Jr. and Lori L. Pollock. </author> <title> Identifying tuple usage patterns in an optimizing linda compiler. </title> <booktitle> In Proceedings of Mid-Atlantic Workshop on Programming Languages and Systems (MASPLAS '96), </booktitle> <year> 1996. </year>
Reference-contexts: While researchers have demonstrated that distributed tuplespace implementations can be efficient for a wide variety of "real" applications encompassing a large scope of parallel algorithm classifications [19, 34, 11], there remains opportunity for improvement through compiler analysis targeting the underlying message passing <ref> [9, 21, 29, 20] </ref>. Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. <p> We have built a Linda optimizing compiler based on the SUIF compiler infrastructure [45], and a distributed tuplespace runtime system. This paper focuses more directly on the distributed tuplespace runtime system, called Deli (University of Delaware Linda). More details on the Linda optimizing compiler can be found in <ref> [20, 21, 22] </ref>. <p> Advanced compiler analysis Two of the runtime optimizations just described require compiler support. The others require the runtime system to constantly monitor and gather statistical information regarding tuple access, and only after a period of time of inefficient accesses are the dynamic optimizations performed. Aggressive compiler analyses 19 <ref> [20, 21] </ref> can improve the situation by finding more opportunities for inout collapse and making more informed judgments for tuple broadcast (i.e., estimating number of rd's versus in's). <p> application connect to an existing persistent tuplespace? How is the persistent 20 tuplespace created initially? How can a persistent tuplespace support a separate but simultaneous invocation of an application? DESIGN GOALS Deli was developed for the purpose of validating and experimenting with our compiler analysis and optimization of Linda programs <ref> [21, 20] </ref>. To make this Linda tuplespace implementation a robust research tool, the following goals were established. These goals are presented here as they affect implementation choices described in the following section. Software-based approach. No specialized hardware or specific interconnection topology should be required. <p> These include rendezvous node remapping and tuple broadcast. The Linda-LAN system supports the instructional footprinting optimization of Landry and Arthur [29]. Deli currently supports static communication optimizations similar to Bjornson but using a more informed and comprehensive analysis within the compiler <ref> [21, 20] </ref>. The trans-puter implementation of Shekhar and Srikant [44] allows the application to specify among a set of alternative implementation parameters such as tuple transfer protocol.
Reference: [22] <author> James B. Fenwick, Jr. and Lori L. Pollock. </author> <title> Implementing an optimizing linda compiler using suif. </title> <booktitle> In Proceedings of SUIF Compiler Workshop, </booktitle> <address> Stanford, California, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: We have built a Linda optimizing compiler based on the SUIF compiler infrastructure [45], and a distributed tuplespace runtime system. This paper focuses more directly on the distributed tuplespace runtime system, called Deli (University of Delaware Linda). More details on the Linda optimizing compiler can be found in <ref> [20, 21, 22] </ref>.
Reference: [23] <author> David Gelernter. </author> <title> Generative communication in linda. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(1) </volume> <pages> 80-112, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Moreover, distributed memory parallel architectures offer better scalability than shared memory machines. However, it is generally agreed that shared memory parallel programming is easier than explicit message passing programming. The generative communication <ref> [23] </ref> paradigm of parallel programming offers the simplicity of shared memory programming and only a small number of primitives for coordination, while also providing flexibility, power, and the potential to scale like message passing. Rather than sharing variables, processes share a data space. <p> Our implementation of the generative communication model is an implementation of Linda tuplespace, the best-known implementation of the generative communication model <ref> [23, 10] </ref>. We have built a Linda optimizing compiler based on the SUIF compiler infrastructure [45], and a distributed tuplespace runtime system. This paper focuses more directly on the distributed tuplespace runtime system, called Deli (University of Delaware Linda). <p> tuplespace model, discuss a number of implementation issues and the solutions taken by other researchers, the details of our own implementation, and a comparison of prominent implementations. 2 LINDA TUPLESPACE Linda is a coordination language consisting of a small number of primitive operations which are added into existing sequential languages <ref> [23, 10] </ref>. These operations perform the communication and synchronization necessary for parallel programming. The following subsections describe the basic Linda model, and an improvement that addresses the efficiency of the associative tuplespace memory.
Reference: [24] <author> David Gelernter. </author> <title> Multiple tuple spaces in linda. </title> <booktitle> In Proceedings Parallel Architectures and Languages Europe (PARLE 89), </booktitle> <volume> volume 2, </volume> <pages> pages 20-27. </pages> <publisher> Springer Verlag LNCS 366, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: Multiple tuplespaces The basic tuplespace model utilizes a single tuplespace equally accessible by any operation in any process in the application. Some researchers have explored allowing multiple tuplespaces thus treating a tuplespace as a fundamental object of the model. <ref> [24] </ref> describes a new tuplespace data type and an operation to create an object of this data type (i.e., a tuplespace). Interesting motivation and examples of applicability of multiple tuplespaces is also described. [32, 4] also utilize flavors of the multiple tuplespace concept.
Reference: [25] <author> Milind Girkar and Constantine D. Polychronopoulos. </author> <title> The hierarchical task graph as a universal intermediate representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5) </volume> <pages> 519-551, </pages> <year> 1994. </year>
Reference-contexts: Functional, task, or DAG parallelism is the parallelism that can be exploited among different operations in a program by using data and control dependences [38] targeting a parallelism that is more coarsely grained than loop-level parallelism. A Linda compiler may apply other research <ref> [38, 25] </ref> to extract these finer grained kinds of parallelism present in individual, explicitly specified Linda processes. Tuplespace is an associative memory, thus tuples are accessed not by an address but rather by their contents. Accessing a tuple therefore requires a description of the tuple desired.
Reference: [26] <author> Karp Joo Jeong and Dennis Shasha. Plinda 2.0: </author> <title> A transaction/checkpointing approach to fault-tolerant linda. </title> <booktitle> In Proceedings of the 13th IEEE Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 96-105, </pages> <year> 1994. </year>
Reference-contexts: See <ref> [4, 26, 50] </ref> for in-depth treatments of fault-tolerant Linda implementations. 22 implementation of tuplespace managers, tuple and tuplespace data structures, tuplespace access functions, and implementation of the eval operation. Providing a thorough treatment of the implementation details requires examining and discussing source code.
Reference: [27] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: In this environment, tuplespace provides a shared memory abstraction on top of a message-passing, distributed memory machine. Because tuplespace is a logically shared memory, it encounters implementation issues similar to other distributed shared memories (DSM) <ref> [33, 35, 5, 27] </ref>. Nitzberg and Lo [36] present a survey of the important issues of implementing DSMs and discuss how the developers of several DSMs faced these design choices. However, because of its special nature, a tuplespace implementation must address several additional issues [44].
Reference: [28] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <month> Sept </month> <year> 1979. </year>
Reference-contexts: However, implementations that select positive broadcast as the method of distributing tuples must also select a coherence protocol as tuples are duplicated on all nodes. The available protocols are essentially the same as those developed by the cache coherency research (for background one can start with <ref> [28, 13, 1, 15] </ref>, and for more current research see [14, 30]). Bjornson [6] noted the advantages of replicating "read-only" tuples, thus implementations which provide this optimization must also decide on an appropriate coherence protocol.
Reference: [29] <author> Kenneth Landry and John D. Arthur. </author> <title> Achieving asynchronous speedup while preserving synchronous semantics: An implementation of instructional footprinting in linda. </title> <booktitle> In The 1994 International Conference on Computer Languages, </booktitle> <pages> pages 55-63, </pages> <address> Toulouse, France, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: While researchers have demonstrated that distributed tuplespace implementations can be efficient for a wide variety of "real" applications encompassing a large scope of parallel algorithm classifications [19, 34, 11], there remains opportunity for improvement through compiler analysis targeting the underlying message passing <ref> [9, 21, 29, 20] </ref>. Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. <p> Compiler analysis could also suggest to the runtime system which partitions may require rehashing thus reducing (possibly eliminating) the time period of inefficiency. Advanced compiler techniques are also able to move Linda operations so as to achieve a form of latency hiding <ref> [29] </ref>. Multiple tuplespaces The basic tuplespace model utilizes a single tuplespace equally accessible by any operation in any process in the application. <p> Bjornson's implementation [6] performs a number of optimizations which are applied due to dynamic analysis, and also some optimizations due to limited static (preprocessor) analysis. These include rendezvous node remapping and tuple broadcast. The Linda-LAN system supports the instructional footprinting optimization of Landry and Arthur <ref> [29] </ref>. Deli currently supports static communication optimizations similar to Bjornson but using a more informed and comprehensive analysis within the compiler [21, 20]. The trans-puter implementation of Shekhar and Srikant [44] allows the application to specify among a set of alternative implementation parameters such as tuple transfer protocol.
Reference: [30] <author> Alvin R. Lebeck. </author> <title> Tools and Techniques for Memory System Design and Analysis. </title> <type> PhD thesis, </type> <institution> University of Wisconsin at Madison, Computer Sciences Department, </institution> <note> expected 1995. </note>
Reference-contexts: The available protocols are essentially the same as those developed by the cache coherency research (for background one can start with [28, 13, 1, 15], and for more current research see <ref> [14, 30] </ref>). Bjornson [6] noted the advantages of replicating "read-only" tuples, thus implementations which provide this optimization must also decide on an appropriate coherence protocol.
Reference: [31] <author> Jerrold Sol Leichter. </author> <title> Shared Tuple Memories, Shared Memories, Buses and LAN's - Linda Implementations Across the Spectrum of Connectivity. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. The experiences of this implementation effort have found existing distributed tuplespace implementations <ref> [6, 12, 16, 31, 44, 49] </ref> to be incomplete in their descriptions of the implementation issues faced and the potential solutions and pitfalls in addressing these issues. <p> One possibility is for tuples to be stored on the node executing the out or eval 2 . Templates, generated by the in, rd, inp, and rdp operators, are sent to all nodes. Bjornson termed this scheme negative broadcast. This is the method utilized in the implementations of <ref> [31, 3] </ref>. Another possibility is positive broadcast which is the inverse of negative broadcast. Tuples are broadcast to all nodes, and the node executing the in, rd, inp, and rdp operator locally handles the templates. Carriero's S-NET implementation [12] used this scheme. <p> In fact, this form of locality of reference was precisely the reasoning for Leichter's defense of using a negative broadcast distribution <ref> [31] </ref>. Without any empirical evidence of such locality of reference patterns, it would appear that a hash-based decision on where to store tuples has a better chance of succeeding by the first metric than operator-based methods do by the second. <p> For example, on a shared memory machine, access to the values of the parent process' global variables is easily achieved. However, in a network of workstations, this is not the case. Leichter <ref> [31] </ref> and Bjornson [6] also discuss this semantic issue. Due to the semantic confusion, it would seem prudent for implementors to select the most restrictive interpretation, allowing the new process to only have access to r-values of explicitly passed parameters. <p> This separation may make certain communication optimizations infeasible. Saving tuplespace to disk will ensure its continual operability; however, in a distributed setting, it is not clear how this should be done. Leichter <ref> [31] </ref> suggests two approaches but does not recommend one over another. <p> A better solution is planned and involves simply adjusting the identifier for the shared memory segment if a previous attempt was denied. 40 IMPLEMENTATION COMPARISONS There have been several other published implementations of the Linda tuplespace targeting distributed memory machines <ref> [6, 44, 3, 16, 7, 31] </ref>. The differences and similarities of these systems are summarized in Table 2. Blank entries in the table are due to inadequate information in the publication relating to that characteristic. <p> It is claimed that this protocol takes advantage of a type of tuplespace locality of reference <ref> [31] </ref>, but it would seem this becomes less the case when multiple distinct nodes concurrently execute the same process a common occurrence in Linda programs. Leichter does not implement the eval operator but describes an implementation that essentially is an eval server.
Reference: [32] <author> Wm Leler. </author> <title> Linda meets unix. </title> <booktitle> Computer, </booktitle> <pages> pages 43-54, </pages> <month> February </month> <year> 1990. </year> <month> 46 </month>
Reference-contexts: Interesting motivation and examples of applicability of multiple tuplespaces is also described. <ref> [32, 4] </ref> also utilize flavors of the multiple tuplespace concept. Presently, there seems to be no clear semantic definition of multiple tuplespaces or how they can/should be implemented. Persistent tuplespaces Another interesting extension is having the tuplespace persist beyond the lifetime of an application. <p> As such, there are a multitude of additional issues encountered that require altering or extending the tuplespace model as we have described. Leler <ref> [32] </ref> adapts the tuplespace model as the basis for a new distributed operat ing system. His implementation includes a number of interesting extensions including multiple tuplespaces, passing tuplespaces as tuple fields, and language-independent data types. This work also guarantees the ordering of tuples requiring network support or increased implementation complexity.
Reference: [33] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov </month> <year> 1989. </year>
Reference-contexts: In this environment, tuplespace provides a shared memory abstraction on top of a message-passing, distributed memory machine. Because tuplespace is a logically shared memory, it encounters implementation issues similar to other distributed shared memories (DSM) <ref> [33, 35, 5, 27] </ref>. Nitzberg and Lo [36] present a survey of the important issues of implementing DSMs and discuss how the developers of several DSMs faced these design choices. However, because of its special nature, a tuplespace implementation must address several additional issues [44]. <p> Granularity refers to the size of the unit of sharing. For some DSMs, this unit is the size of an operating system page <ref> [33] </ref>, while others use a smaller block size [18]. One DSM uses a combination of a large and small unit of transfer [39]. For tuplespace, the unit of sharing is the tuple. However, because tuple sizes vary, the tuplespace implementor is not afforded the simplicity of predetermined fixed length transmissions.
Reference: [34] <author> Timothy G. Mattson. </author> <title> The efficiency of linda for general purpose scientific programming. </title> <journal> Scientific Programming, </journal> <volume> 3(1) </volume> <pages> 61-71, </pages> <year> 1994. </year>
Reference-contexts: While researchers have demonstrated that distributed tuplespace implementations can be efficient for a wide variety of "real" applications encompassing a large scope of parallel algorithm classifications <ref> [19, 34, 11] </ref>, there remains opportunity for improvement through compiler analysis targeting the underlying message passing [9, 21, 29, 20]. Such research requires a tuplespace implementation that allows experimentation and modification.
Reference: [35] <author> Ronald G. Minnich and Daniel V. Pryor. Mether: </author> <title> Supporting the shared memory model on computing clusters. </title> <booktitle> 38th Annual IEEE Computer Society International Computer Conference (COMPCON SPRING '93), </booktitle> <year> 1993. </year>
Reference-contexts: In this environment, tuplespace provides a shared memory abstraction on top of a message-passing, distributed memory machine. Because tuplespace is a logically shared memory, it encounters implementation issues similar to other distributed shared memories (DSM) <ref> [33, 35, 5, 27] </ref>. Nitzberg and Lo [36] present a survey of the important issues of implementing DSMs and discuss how the developers of several DSMs faced these design choices. However, because of its special nature, a tuplespace implementation must address several additional issues [44].
Reference: [36] <author> Bill Nitzberg and Virginia Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In this environment, tuplespace provides a shared memory abstraction on top of a message-passing, distributed memory machine. Because tuplespace is a logically shared memory, it encounters implementation issues similar to other distributed shared memories (DSM) [33, 35, 5, 27]. Nitzberg and Lo <ref> [36] </ref> present a survey of the important issues of implementing DSMs and discuss how the developers of several DSMs faced these design choices. However, because of its special nature, a tuplespace implementation must address several additional issues [44].
Reference: [37] <institution> Oak Ridge National Laboratory, Oak Ridge, TN 37831. </institution> <note> PVM 3.0 User's Guide and Reference Manual, February 1993. (http://www.netlib.org/pvm3/). </note>
Reference-contexts: Glenda appears to use a centralized tuplespace, requiring the user to explicitly start the tuple server on the host node prior to executing the application program. Glenda does not provide the simplicity of the eval operator for process creation. Reflecting the utilization of PVM <ref> [37] </ref> as its underlying communication layer, Glenda instead provides a spawning capability requiring the programmer to manually break the application into separate executable files and to explicitly specify in the spawn invocation which node executes which executable. To take advantage of optimized PVM communication capabilities, Glenda adds additional tuplespace operations.
Reference: [38] <author> Santosh S. Pande, Dharma P. Agrawal, and Jon Mauney. </author> <title> Compiling functional parallelism on distributed-memory systems. </title> <booktitle> IEEE Parallel & Distributed Technology, </booktitle> <pages> pages 64-76, </pages> <month> Spring </month> <year> 1994. </year>
Reference-contexts: This is how the programmer explicitly creates parallelism. This style of explicit parallelism should be differentiated from other types of parallelism. Functional, task, or DAG parallelism is the parallelism that can be exploited among different operations in a program by using data and control dependences <ref> [38] </ref> targeting a parallelism that is more coarsely grained than loop-level parallelism. A Linda compiler may apply other research [38, 25] to extract these finer grained kinds of parallelism present in individual, explicitly specified Linda processes. <p> Functional, task, or DAG parallelism is the parallelism that can be exploited among different operations in a program by using data and control dependences [38] targeting a parallelism that is more coarsely grained than loop-level parallelism. A Linda compiler may apply other research <ref> [38, 25] </ref> to extract these finer grained kinds of parallelism present in individual, explicitly specified Linda processes. Tuplespace is an associative memory, thus tuples are accessed not by an address but rather by their contents. Accessing a tuple therefore requires a description of the tuple desired.
Reference: [39] <author> Joerg Thomas Pfenning, Achim Bachem, and Ronald Minnich. </author> <title> Virtual shared memory programming on workstation clusters. </title> <booktitle> Future Generation Computer Systems, </booktitle> <address> 11(4-5):387-399, </address> <month> Aug </month> <year> 1995. </year>
Reference-contexts: Granularity refers to the size of the unit of sharing. For some DSMs, this unit is the size of an operating system page [33], while others use a smaller block size [18]. One DSM uses a combination of a large and small unit of transfer <ref> [39] </ref>. For tuplespace, the unit of sharing is the tuple. However, because tuple sizes vary, the tuplespace implementor is not afforded the simplicity of predetermined fixed length transmissions. An interesting benefit to tuplespace's granularity is the elimination of the false sharing problem experienced by most DSMs.
Reference: [40] <author> James Nicholas Pinakis. </author> <title> Using Linda as the basis of an Operating System Microkernel. </title> <type> PhD thesis, </type> <institution> University of Western Australia, </institution> <year> 1993. </year>
Reference-contexts: His implementation includes a number of interesting extensions including multiple tuplespaces, passing tuplespaces as tuple fields, and language-independent data types. This work also guarantees the ordering of tuples requiring network support or increased implementation complexity. Pinakis <ref> [40] </ref> similarly uses Linda as the basis for a distributed operating system microkernel.
Reference: [41] <author> Patrick G. Robinson and James D. Arthur. </author> <title> Distributed process creation within a shared data space framework. </title> <type> Technical Report TR 94-13, </type> <institution> Virginia Polytechnic Institute and State University, </institution> <year> 1994. </year>
Reference-contexts: Providing access to these executables for each node via a common file system could eliminate the complication of runtime transmission of large codes 3 . A general, machine-independent approach is to use an eval server <ref> [6, 41] </ref>. The eval servers, which run on every node, monitor tuplespace for active tuples. A particular eval server can evaluate all processes spec 3 In fact, the wide utilization of NFS makes this rather straightforward. 14 ified within the active tuple, or only one such process.
Reference: [42] <author> G. Schoinas. </author> <title> Issues on the implementation of programming system for distributed applications. (part of POSYBL distribution documentation located at ftp://nic.funet.fi/pub/unix/parallel/POSYBL.TAR.Z). </title>
Reference-contexts: A separate process is utilized to register "tuple types" (tuplespace partitions using our notation) and determine the node responsible for managing a particular tuple type. This seems to add an extra layer 42 of processing, communication, and implementation complexity. Two other systems include POSYBL <ref> [42] </ref> and Glenda [43]. Both of these systems have little or no compiler support, thereby requiring the programmer to indicate information to the runtime system through increased programming complexity. POSYBL also changes the semantics of the eval operator and does not support the inp, rdp operators at all.
Reference: [43] <author> Benjamin R. Seyfarth, Jerry L. Bickham, and Mangaiarkarasi Arumughum. </author> <title> Glenda Installation and Use. </title> <institution> University of Southern Mississippi, </institution> <month> November </month> <year> 1993. </year> <title> (part of Glenda distribution documentation located at http://sushi.st.usm.edu/ seyfarth/research/glenda.html). </title>
Reference-contexts: A separate process is utilized to register "tuple types" (tuplespace partitions using our notation) and determine the node responsible for managing a particular tuple type. This seems to add an extra layer 42 of processing, communication, and implementation complexity. Two other systems include POSYBL [42] and Glenda <ref> [43] </ref>. Both of these systems have little or no compiler support, thereby requiring the programmer to indicate information to the runtime system through increased programming complexity. POSYBL also changes the semantics of the eval operator and does not support the inp, rdp operators at all.
Reference: [44] <author> K.H. Shekhar and Y.N. Srikant. </author> <title> Linda sub system on transputers. </title> <journal> Computer Languages, </journal> <volume> 18(2) </volume> <pages> 125-136, </pages> <year> 1993. </year>
Reference-contexts: Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. The experiences of this implementation effort have found existing distributed tuplespace implementations <ref> [6, 12, 16, 31, 44, 49] </ref> to be incomplete in their descriptions of the implementation issues faced and the potential solutions and pitfalls in addressing these issues. <p> Nitzberg and Lo [36] present a survey of the important issues of implementing DSMs and discuss how the developers of several DSMs faced these design choices. However, because of its special nature, a tuplespace implementation must address several additional issues <ref> [44] </ref>. <p> Processor Location of a Tuple For tuplespace (or any DSM) to allow the sharing of data, a process must be able to locate and retrieve the data it requires. This issue is termed tuplespace organization and distribution in <ref> [44] </ref>. One choice, taken by Cline and Arthur [16], is to centralize tuplespace on a single node of the parallel machine. All tuples are stored on the one processor, and locating a tuple is simplified because all processes simply route the request to this node. <p> However, if the number of tuplespace partitions is greater than the number of available nodes, hashing is one of the best methods for efficient tuple distribution <ref> [44] </ref>. Operator-based Tuple Distribution This method uses the Linda operator itself to indicate the destination of the tuple/template. One possibility is for tuples to be stored on the node executing the out or eval 2 . <p> Again, sophisticated compiler analysis estimating tuplespace access patterns and/or runtime statistics gathering of actual access patterns may be able to allow the selection of alternative protocols. Shekhar and Srikant <ref> [44] </ref> suggest that because each application has its own tuplespace access patterns, the tuplespace implementation itself should be reconfigurable for each application so as to minimize inefficiencies. <p> In addition, the semantics of the eval operator are not well-specified, making this a potential source of non-portability of applications across different implementations. Specifically, the model does not define whether the new process shares global variables with its parent, or if a process can modify parameters passed by reference <ref> [44, 49] </ref>. In the absence of a clear semantic definition, implementations seem to generally let the machine's available process creation primitives dictate the definition of their semantics. For example, on a shared memory machine, access to the values of the parent process' global variables is easily achieved. <p> The availability of a common file system can again remedy this problem. Node selection and communication Another issue to address when implementing the eval operation is deciding on an available node to execute the process. Shekhar and Srikant <ref> [44] </ref> discuss the importance of process creation in the context of system load balance. Combining knowledge of the tuple distribution method, computational requirements of processes, information relating processes, and tuple usage could lead to a more balanced load by placing processes on appropriate nodes. <p> Portability. The implementation should port to new systems in a straightforward manner. Reconfigurability. The implementation should be easily reconfigured in order to examine different approaches to several of the implementation issues, such as tuple transfer protocol, and eval operator implementation. Shekhar and Srikant <ref> [44] </ref> argue that reconfigurability provides individual applications with efficiency improvements over a fixed implementation. IMPLEMENTATION Deli (University of Delaware Linda) is an implementation of Linda tuplespace for a distributed memory parallel machine. In particular, Deli successfully executes on a network of SUN 4 workstations connected via Ethernet. <p> A better solution is planned and involves simply adjusting the identifier for the shared memory segment if a previous attempt was denied. 40 IMPLEMENTATION COMPARISONS There have been several other published implementations of the Linda tuplespace targeting distributed memory machines <ref> [6, 44, 3, 16, 7, 31] </ref>. The differences and similarities of these systems are summarized in Table 2. Blank entries in the table are due to inadequate information in the publication relating to that characteristic. <p> The Tsnet publication [3] is no longer in print and we were unable to obtain a copy of this report. However, some information is available indirectly from Bjornson's thesis [6]. The two-level tuple transfer protocol used by <ref> [44] </ref> is a variation of the Example Protocol 2 described in the IMPLEMENTATION ISSUES section. The rendezvous node tuple transfer protocol [6] dictates that all tuples/templates of a tuplespace partition be sent to one node. <p> The Linda-LAN system supports the instructional footprinting optimization of Landry and Arthur [29]. Deli currently supports static communication optimizations similar to Bjornson but using a more informed and comprehensive analysis within the compiler [21, 20]. The trans-puter implementation of Shekhar and Srikant <ref> [44] </ref> allows the application to specify among a set of alternative implementation parameters such as tuple transfer protocol. However the application does not provide these alternative functionalities directly, but rather indicates a choice from the set of alternatives already present within the system.
Reference: [45] <author> Stanford SUIF Compiler Group. </author> <title> The SUIF Parallelizing Compiler Guide. </title> <institution> Stanford University, </institution> <year> 1994. </year> <note> Version 1.0. </note>
Reference-contexts: Our implementation of the generative communication model is an implementation of Linda tuplespace, the best-known implementation of the generative communication model [23, 10]. We have built a Linda optimizing compiler based on the SUIF compiler infrastructure <ref> [45] </ref>, and a distributed tuplespace runtime system. This paper focuses more directly on the distributed tuplespace runtime system, called Deli (University of Delaware Linda). More details on the Linda optimizing compiler can be found in [20, 21, 22].
Reference: [46] <author> W. Richard Stevens. </author> <title> UNIX Network Programming. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: All Deli communications utilize the TCP reliable transport protocol. This additional overhead to establish the 10 This is terminology borrowed from Stevens' excellent text <ref> [46] </ref>. 39 communication circuit is incurred at the cost of reduced implementation time and complexity. The TCP connection is not maintained after the tuple or template is transmitted. Future versions of Deli plan to improve performance by utilizing the UDP datagram service and explicitly ensuring reliable delivery of data.
Reference: [47] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice Hall, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: The sender of a datagram does not know if the datagram reaches its destination. Ensuring delivery of a datagram requires the implementor to provide this extra processing using timeouts, acknowledgments, retransmissions, etc. (see description of various data link layer protocols in <ref> [47] </ref>). 16 Which to use? Of course, this decision is up to each individual implementor who must consider the tuple distribution method selected and characteristics of the particular interconnection network. The choice seems to come down to the performance advantages of connection-less protocols versus the simplicity of the connection-oriented techniques.
Reference: [48] <author> Andrew P. Wack. </author> <title> Practical communication cost formula for users of message passing systems. </title> <booktitle> In Proceedings of the 6th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 418-425, </pages> <year> 1994. </year>
Reference-contexts: Flexibility. The implementation should easily accommodate changes, for a variety of reasons including, * Optimizations. Some optimizations require alternative handling of tuples and templates. * Architecture simulation. The implementation should support the inclusion of a cost model (such as <ref> [48] </ref>) to simulate architecture parameters such as processor speed and interconnection bandwidth. * Heterogeneity. The implementation should be able to accommodate varying processor architectures. Reliability. The implementation must guarantee reliable message passing. Providing fault tolerance is 21 not a present focus 6 . Portability.
Reference: [49] <author> Gregory V. Wilson. </author> <title> Practical Parallel Programming. Scientific and Engineering Computation Series. </title> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Such research requires a tuplespace implementation that allows experimentation and modification. In order to gain this flexibility in experimentation, an implementation of distributed tuplespace for a network of workstations has been developed. The experiences of this implementation effort have found existing distributed tuplespace implementations <ref> [6, 12, 16, 31, 44, 49] </ref> to be incomplete in their descriptions of the implementation issues faced and the potential solutions and pitfalls in addressing these issues. <p> In addition, the semantics of the eval operator are not well-specified, making this a potential source of non-portability of applications across different implementations. Specifically, the model does not define whether the new process shares global variables with its parent, or if a process can modify parameters passed by reference <ref> [44, 49] </ref>. In the absence of a clear semantic definition, implementations seem to generally let the machine's available process creation primitives dictate the definition of their semantics. For example, on a shared memory machine, access to the values of the parent process' global variables is easily achieved.
Reference: [50] <author> Andrew Xu and Barbara Liskov. </author> <title> Design for a fault-tolerant, distributed implementation of linda. </title> <booktitle> In Digest of Papers - FTCS (Fault-Tolerant Computing Symposium), </booktitle> <pages> pages 199-206, </pages> <year> 1989. </year> <month> 47 </month>
Reference-contexts: See <ref> [4, 26, 50] </ref> for in-depth treatments of fault-tolerant Linda implementations. 22 implementation of tuplespace managers, tuple and tuplespace data structures, tuplespace access functions, and implementation of the eval operation. Providing a thorough treatment of the implementation details requires examining and discussing source code.
References-found: 50

