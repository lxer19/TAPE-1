URL: http://now.cs.berkeley.edu/Papers2/Postscript/pdpta.ps
Refering-URL: http://now.cs.berkeley.edu/Implicit/
Root-URL: 
Email: fdusseau, cullerg@cs.berkeley.edu  
Title: Extending Proportional-Share Scheduling to a Network of Workstations  
Author: Andrea C. Arpaci-Dusseau and David E. Culler 
Keyword: Networks of Workstations, Clusters, Parallel Computing, Coscheduling, Fairness, Proportional-Share  
Address: Berkeley  
Affiliation: Computer Science Division University of California,  
Abstract: As networks of workstations (NOW) emerge as a viable platform for a wide range of workloads, a new scheduling approach is needed to allocate the collection of resources across competing users. In this paper, we show that extensions to a proportional-share scheduler for improving response time can still fairly allocate resources to a mix of sequential, interactive, and parallel jobs in this distributed environment. We find that a proportional-share scheduler, specifically a stride-scheduler, running on each node in the cluster is a good building-block. Simple extensions are implemented and analyzed which provide better response-times for interactive jobs by giving those jobs their share of resources over a longer time-interval. When scheduling jobs across the cluster, we show that fairness can be guaranteed if each local scheduler knows the number of tickets issued to each user and if the tickets are balanced across all workstations. Finally, we show that a proportional-share of resources can be provided to time-shared parallel applications through a combination of stride-scheduling and implicit coscheduling. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, G. Edjlali, and J. Saltz. </author> <title> The Utility of Exploiting Idle Workstations for Parallel Computation. </title> <booktitle> In Proceedings of 1997 ACM Sigmetrics International Conferenceon Measurement and Modeling of Computer Systems, </booktitle> <address> Seattle, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: In the not-too-distant past, networks of workstations (NOWs) were a loose collection of workstations physically scattered across users' desks [2, 11, 23, 33, 41]. One of the main research issues was effectively exploiting idle resources when users were not running jobs on their desktop machines <ref> [1, 4, 34] </ref>. Due to the expense of finding available cycles and of migrating jobs if a workstation's owner returned, such systems performed only batch-processing, optimizing for throughput of long-running computationally-intensive applications.
Reference: [2] <author> R. Agrawal and A. Ezzat. </author> <title> Processor Sharing in Nest: A Network of Computer Workstations. </title> <booktitle> In Proceedings of 1st International Conference on Computer Workstations, </booktitle> <month> Nov. </month> <year> 1985. </year>
Reference-contexts: 1 Introduction The definition of a network of workstations has changed in recent years. In the not-too-distant past, networks of workstations (NOWs) were a loose collection of workstations physically scattered across users' desks <ref> [2, 11, 23, 33, 41] </ref>. One of the main research issues was effectively exploiting idle resources when users were not running jobs on their desktop machines [1, 4, 34].
Reference: [3] <author> T. E. Anderson, D. E. Culler, and D. A. Patterson. </author> <title> A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: By connecting these switches and commodity workstations one can build incrementally-scalable, cost-effective, and highly-available shared servers capable of supporting a large number of users running a variety of workloads <ref> [3, 6, 7, 10, 20] </ref>. Pooling resources has the advantagethat resource-intensive applications can use processors, memory, and disks that may otherwise go unused. However, sharing resources raises new challenges in guaranteeing that each user obtains his or her fair share when demand is heavy. <p> We also expect that the presence of migration and the ability to dynamically readjust ticket allocations will dramatically impact the appropriate policies. The next steps of this work involve incorporating the described policies into the U.C. Berkeley NOW <ref> [3, 10] </ref>. Key components of the implementation work include extending the parallel ticket server so it is robust to node failures and placing jobs and tickets without the current centralized master.
Reference: [4] <author> R. H. Arpaci, A. C. Dusseau, A. M. Vahdat, L. T. Liu, T. E. Anderson, and D. A. Patterson. </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> In Proceedings of ACM SIGMETRICS'95/PERFORMANCE'95 Joint International Conference on Measurementand Modeling of Computer Systems, </booktitle> <pages> pages 267-278, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: In the not-too-distant past, networks of workstations (NOWs) were a loose collection of workstations physically scattered across users' desks [2, 11, 23, 33, 41]. One of the main research issues was effectively exploiting idle resources when users were not running jobs on their desktop machines <ref> [1, 4, 34] </ref>. Due to the expense of finding available cycles and of migrating jobs if a workstation's owner returned, such systems performed only batch-processing, optimizing for throughput of long-running computationally-intensive applications. <p> As when placing sequential jobs, the system should select nodes with the fewest competing tickets. However, placing parallel jobs involves additional considerations. Because fine-grain parallel jobs run at the rate of the process on the slowest machine <ref> [4] </ref>, the job receives the same amount of resources whether all machines have exactly T competing tickets, or one machine has T tickets and all others have fewer.
Reference: [5] <author> A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, D. E. Culler, J. M. Hellerstein, and D. P. Patterson. </author> <title> High-Performance Sorting on Networks of Workstations. </title> <booktitle> In Proceedings of the 1997 ACM SIGMOD Conference, </booktitle> <year> 1997. </year>
Reference-contexts: However, users may also specify for themselves the workstation on which to execute a process. This functionality is required so that users can compute on machines near file data <ref> [5] </ref>, can use special devices, can run diagnostics, or explicitly avoid faulty machines. We now briefly describe the impact on competing clients of this level of control. If users can execute their processes on arbitrary machines, users can temporarily receive more than their proportional-share.
Reference: [6] <author> A. Basu, V. Buch, W. Vogels, and T. von Eicken. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: By connecting these switches and commodity workstations one can build incrementally-scalable, cost-effective, and highly-available shared servers capable of supporting a large number of users running a variety of workloads <ref> [3, 6, 7, 10, 20] </ref>. Pooling resources has the advantagethat resource-intensive applications can use processors, memory, and disks that may otherwise go unused. However, sharing resources raises new challenges in guaranteeing that each user obtains his or her fair share when demand is heavy.
Reference: [7] <author> M. A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: By connecting these switches and commodity workstations one can build incrementally-scalable, cost-effective, and highly-available shared servers capable of supporting a large number of users running a variety of workloads <ref> [3, 6, 7, 10, 20] </ref>. Pooling resources has the advantagethat resource-intensive applications can use processors, memory, and disks that may otherwise go unused. However, sharing resources raises new challenges in guaranteeing that each user obtains his or her fair share when demand is heavy.
Reference: [8] <author> N. Boden, D. Cohen, R. Felderman, A. Kulawik, C. Sietz, J. Seizovic, and W. Su. </author> <title> Myrinet A Gigabit-perSecond Local-Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Furthermore, the shared-medium Ethernet interconnections and high-overhead messaging protocols necessitated that only sequential and very coarse-grain parallel applications were run. However, recent advances in low-latency, high-bandwidth switches <ref> [8] </ref> have enabled networks of workstations to more closely resemble massively parallel processors (MPPs). By connecting these switches and commodity workstations one can build incrementally-scalable, cost-effective, and highly-available shared servers capable of supporting a large number of users running a variety of workloads [3, 6, 7, 10, 20].
Reference: [9] <author> J. Chapin, M. Rosenblum, S. Devine, T. Lahiri, D. Teodosiu, and A. Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 12-25, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Using a user-level parallel program has the attractive feature that if the ticket-server crashes, the behavior of the local schedulers degenerates to the case where there is no global information. This behavior is similar to an approach advocated for distributing load information in large-scale shared-memory multiprocessors <ref> [9] </ref>. Even though our ticket-server uses a fast communication network and a low-overhead messaging protocol [24, 35], it is still important to minimize the amount of communication.
Reference: [10] <author> D. Culler, A. Arpaci-Dusseau, R. Arpaci-Dusseau, B. Chun, S. Lumetta, A. Mainwaring, R. Martin, C. Yoshikawa, and F. Wong. </author> <title> Parallel Computing on the Berkeley NOW. </title> <booktitle> In Ninth Joint Symposium on Parallel Processing,Kobe, </booktitle> <address> Japan, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: By connecting these switches and commodity workstations one can build incrementally-scalable, cost-effective, and highly-available shared servers capable of supporting a large number of users running a variety of workloads <ref> [3, 6, 7, 10, 20] </ref>. Pooling resources has the advantagethat resource-intensive applications can use processors, memory, and disks that may otherwise go unused. However, sharing resources raises new challenges in guaranteeing that each user obtains his or her fair share when demand is heavy. <p> We also expect that the presence of migration and the ability to dynamically readjust ticket allocations will dramatically impact the appropriate policies. The next steps of this work involve incorporating the described policies into the U.C. Berkeley NOW <ref> [3, 10] </ref>. Key components of the implementation work include extending the parallel ticket server so it is robust to node failures and placing jobs and tickets without the current centralized master.
Reference: [11] <author> F. Douglis and J. Ousterhout. </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation. </title> <journal> Software - Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-85, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The definition of a network of workstations has changed in recent years. In the not-too-distant past, networks of workstations (NOWs) were a loose collection of workstations physically scattered across users' desks <ref> [2, 11, 23, 33, 41] </ref>. One of the main research issues was effectively exploiting idle resources when users were not running jobs on their desktop machines [1, 4, 34].
Reference: [12] <author> A. C. Dusseau, R. H. Arpaci, and D. E. Culler. </author> <title> Effective Distributed Scheduling of Parallel Workloads. </title> <booktitle> In Proceedings of 1996 ACM Sigmetrics International Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Finally, we combine these two extensions to fairly schedule time-shared parallel jobs in the cluster. We show that a stride scheduler that gives credit to jobs that relinquish the processor implicitly coschedules <ref> [12] </ref> parallel applications, dynamically coordinating time-slices across machines in a completely distributed fashion. In the next four sections we build towards a NOW which can allocate a proportional-share of resources to a mix of sequential, interactive, and parallel jobs. <p> Future work will investigate these interactions more rigorously. 5 Cluster: Parallel Jobs In the previous section, we showed that a proportional-share of resources can be allocated to clients running sequential jobs in a network of workstations. In this final step, we combine stride-scheduling and implicit coscheduling <ref> [12] </ref> to extend our analysis to parallel jobs. 5.1 Background We begin by discussing how parallel jobs are scheduled and how fairness is provided in other distributed systems. The most relevant example of an implementation is Spawn [36], a computational economy for concurrent applications in a distributed network. <p> to best balance tickets and give more users closer to their proportional-share (i.e., users G, H, and I), the parallel job should still be placed on the three nodes with the least tickets. 5.2 Implicit Coscheduling A promising time-sharing approach for scheduling parallel applications in a cluster is implicit coscheduling <ref> [12] </ref>, or dynamic coscheduling [29, 30]. Both of these approaches appear to have many of the advantages of explicit coscheduling, without the disadvantages. <p> Unlike explicit coscheduling, both are completely distributed, requiring no global coordination; both have potential for working well with a mix of parallel and interactive jobs and parallel jobs performing I/O; finally, neither requires that communicating processes be statically identified. Simulations have shown that implicit coscheduling <ref> [12] </ref> effectively schedules both coarse- and fine-grain bulk-synchronous parallel applications. Implicit coscheduling avoids the problems of traditional coscheduling by using communication and synchronization occurring naturally within the application to coordinate scheduling across workstations. There are two events that must be observed and acted upon for processes to implicitly coschedule themselves. <p> Second, if a process receives a message, it can infer that other processes in this application are currently scheduled; consequently, in some circumstances, this process should also be scheduled. The previous simulations <ref> [12] </ref> found that waking up processes waiting for message responses was sufficient; it was not necessary to schedule processes waiting in the ready queue. An important assumption in previous simulations was that each node was running the Solaris time-sharing scheduler. <p> Thus, as desired, these mechanisms usually schedule a blocked destination process when a message arrives. In contrast, processes are not scheduled with any additional probability after sleeping with the default stride-scheduler. Therefore, basic stride-scheduling does not lead to robust implicit coscheduling: simulations in <ref> [12] </ref> showed that a round-robin scheduler (which is the same as the basic stride sched-uler when all processes have equal tickets) led to substantially reduced performance compared to the time-shared prior ity scheduler. <p> By combining these two extensions, a proportional-share of resources can be allocated to parallel jobs as well. Preliminary simulation results indicate that stride-scheduling can implicitly coschedule <ref> [12] </ref> a proportional-share of resources to competing parallel jobs. We have begun to explore the fundamental trade-offs between achieving fairness across users and optimizing through put in a NOW. The system must often choose between balancing tickets (i.e., fairness) and balancing load (i.e., throughput) when placing jobs.
Reference: [13] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> Adaptive Load Sharing in Homogeneous Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(5) </volume> <pages> 662-675, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: A centralized location is not a realistic design decision when building a scalable, fault-tolerant cluster on the scale of hundreds of workstations. A promising approach for our environment is for each client to randomly query k nodes and pick the one best qualified <ref> [13, 26, 40] </ref>. Future work will involve investigating this distributed approach, as well as the impact of migration. 4.4 Impact of Job and Ticket Placement by Users We have described our approach for placing jobs in the cluster when the system undertakes this role.
Reference: [14] <author> D. Ferguson, Y. Yemini, and C. Nikolaou. </author> <title> Microeconomic Algorithms for Load Balancing in Distributed Computer Systems. </title> <booktitle> In International Conference on Distributed Computer Systems, </booktitle> <pages> pages 491-499, </pages> <year> 1988. </year>
Reference-contexts: Not only would we like a finer-level of control than the Condor algorithm provides, but we also need to support time-sharing of individual workstations. On the more theoretical side, micro-economic systems can provide more precise allocation <ref> [14, 25] </ref>. Their basic idea is that clients have funding which they use to purchase resources; when servers have available capacity, they accept bids and sell their resources to the highest bidder.
Reference: [15] <author> B. Ford and S. Susarla. </author> <title> CPU Inheritance Scheduling. </title> <booktitle> In Usenix Association Second Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 91-105, </pages> <year> 1996. </year>
Reference-contexts: New scheduling and resource allocation policies are therefore needed for our new environment. To time-share a cluster fairly, one must first be able to fairly schedule jobs on a single workstation. Several variations of proportional-share scheduling have been proposed that solve this problem for compute-bound jobs <ref> [15, 16, 31, 37] </ref>. We use our implementation of one of these approaches, stride scheduling [38], as a building-block in our evaluations. However, a tension exists between scheduling users fairly in the cluster and appropriately handling the mixed workload we expect to see in NOWs. <p> Proportional-share scheduling appears to provide an adequate solution for our needs, many variations of which have been recently proposed <ref> [15, 16, 31, 37] </ref>. With proportional-share scheduling, the resource consumption rights of each active process are proportional to the relative shares that it is allocated.
Reference: [16] <author> P. Goyal, X. Guo, and H. M. Vin. </author> <title> A Hierarchical CPU Sched-uler for Multimedia Operating Systems. </title> <booktitle> In Usenix Association Second Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 107-121, </pages> <year> 1996. </year>
Reference-contexts: New scheduling and resource allocation policies are therefore needed for our new environment. To time-share a cluster fairly, one must first be able to fairly schedule jobs on a single workstation. Several variations of proportional-share scheduling have been proposed that solve this problem for compute-bound jobs <ref> [15, 16, 31, 37] </ref>. We use our implementation of one of these approaches, stride scheduling [38], as a building-block in our evaluations. However, a tension exists between scheduling users fairly in the cluster and appropriately handling the mixed workload we expect to see in NOWs. <p> Proportional-share scheduling appears to provide an adequate solution for our needs, many variations of which have been recently proposed <ref> [15, 16, 31, 37] </ref>. With proportional-share scheduling, the resource consumption rights of each active process are proportional to the relative shares that it is allocated.
Reference: [17] <author> M. Harchol-Balter and A. Downey. </author> <title> Exploiting Process Lifetime Distributions for Dynamic Load Balancing. </title> <booktitle> In Proceedings of ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 13-24, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The ability to migrate jobs has been shown to significantly help balance load, and thus improve average job response-time <ref> [17] </ref>. Migration is also expected to help ensure that users receive their proportional-share of resources. Unfortunately, space constraints limit our discussion to systems without migration. We now briefly discuss our proposal for choosing a destination workstation for a process.
Reference: [18] <author> J. L. Hellerstein. </author> <title> Achieving Service Rate Objectives with Decay Usage Scheduling. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(8) </volume> <pages> 813-825, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Fair-share scheduling was one of the first approaches to address the problem of allocating resources to users fairly over time <ref> [18, 19, 21] </ref>. However, because these systems are built on top of the traditional priority-based sched-ulers, they inherit their difficulties and can provide fairness only over relatively long intervals and are difficult to tune.
Reference: [19] <author> G. J. Henry. </author> <title> The Fair Share Scheduler. </title> <journal> AT&T Bell Laboratories Technical Journal, </journal> <volume> 63(8) </volume> <pages> 1845-1857, </pages> <month> Oct. </month> <year> 1984. </year>
Reference-contexts: Fair-share scheduling was one of the first approaches to address the problem of allocating resources to users fairly over time <ref> [18, 19, 21] </ref>. However, because these systems are built on top of the traditional priority-based sched-ulers, they inherit their difficulties and can provide fairness only over relatively long intervals and are difficult to tune.
Reference: [20] <author> M. D. Hill, J. R. Larus, S. K. Reinhardt, and D. A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4) </volume> <pages> 300-18, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: By connecting these switches and commodity workstations one can build incrementally-scalable, cost-effective, and highly-available shared servers capable of supporting a large number of users running a variety of workloads <ref> [3, 6, 7, 10, 20] </ref>. Pooling resources has the advantagethat resource-intensive applications can use processors, memory, and disks that may otherwise go unused. However, sharing resources raises new challenges in guaranteeing that each user obtains his or her fair share when demand is heavy.
Reference: [21] <author> J. Kay and P. Lauder. </author> <title> A Fair Share Scheduler. </title> <journal> Communications of the ACM, </journal> <volume> 31(1) </volume> <pages> 44-55, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Fair-share scheduling was one of the first approaches to address the problem of allocating resources to users fairly over time <ref> [18, 19, 21] </ref>. However, because these systems are built on top of the traditional priority-based sched-ulers, they inherit their difficulties and can provide fairness only over relatively long intervals and are difficult to tune.
Reference: [22] <author> W. Lee, M. Frank, V. Lee, K. Mackenzi, and L. Rudolph. </author> <title> Implications of I/O for Gang Scheduled Workloads. </title> <booktitle> In Proceedings of the IPPS '97 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <year> 1997. </year>
Reference-contexts: Ensuring that context-switches occur simultaneously on all processors increases the cost of each context-switch. Second, explicit coscheduling does not interact well with interactive jobs or parallel jobs performing I/O <ref> [22] </ref>. Finally, explicitly coscheduling requires that communicating processes are identified a priori, and so can not be applied to distributed client/server applications. workstation 3. Therefore, Z receives the same allocation whether it runs on machines with less or equal competing tickets relative to workstation 3.
Reference: [23] <author> M. Litzkow, M. Livny, and M. </author> <title> Mutka. Condor A Hunter of Idle Workstations. </title> <booktitle> In Proceedings of the 8th International Conference of Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The definition of a network of workstations has changed in recent years. In the not-too-distant past, networks of workstations (NOWs) were a loose collection of workstations physically scattered across users' desks <ref> [2, 11, 23, 33, 41] </ref>. One of the main research issues was effectively exploiting idle resources when users were not running jobs on their desktop machines [1, 4, 34].
Reference: [24] <author> A. M. Mainwaring. </author> <title> Active Message Application Programming Interface and Communication Subsystem Organization. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1995. </year>
Reference-contexts: This behavior is similar to an approach advocated for distributing load information in large-scale shared-memory multiprocessors [9]. Even though our ticket-server uses a fast communication network and a low-overhead messaging protocol <ref> [24, 35] </ref>, it is still important to minimize the amount of communication. Because many processes are expected to be short-lived, each stride-scheduler only periodically contacts the local ticket server to update and determine the value of currencies, rather than every time a job begins or ends. <p> Berkeley NOW [3, 10]. Key components of the implementation work include extending the parallel ticket server so it is robust to node failures and placing jobs and tickets without the current centralized master. We will also soon measure our implementation of implicit coscheduling on Active Messages <ref> [24, 35] </ref> with both the Solaris time-sharing scheduler and our modified stride-schedulers. Acknowledgments We would like to thank Tom Anderson, Remzi Arpaci-Dusseau, and Amin Vahdat for their many helpful comments on the presentation of this work. Andrea Arpaci-Dusseau is currently supported by an Intel Foundation Graduate Fellowship.
Reference: [25] <author> T. Malone, R. Fikes, K. Grant, and M. Howard. </author> <title> Enterprise: </title> <booktitle> A market-like task scheduler for distributed computing environments, </booktitle> <pages> pages 177-205. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Not only would we like a finer-level of control than the Condor algorithm provides, but we also need to support time-sharing of individual workstations. On the more theoretical side, micro-economic systems can provide more precise allocation <ref> [14, 25] </ref>. Their basic idea is that clients have funding which they use to purchase resources; when servers have available capacity, they accept bids and sell their resources to the highest bidder.
Reference: [26] <author> M. Mitzenmacher. </author> <title> Load balancing and density dependent jump Markov processes. </title> <booktitle> In Proceedings of 37th Conference on Foundations of Computer Science, </booktitle> <pages> pages 213-222, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: A centralized location is not a realistic design decision when building a scalable, fault-tolerant cluster on the scale of hundreds of workstations. A promising approach for our environment is for each client to randomly query k nodes and pick the one best qualified <ref> [13, 26, 40] </ref>. Future work will involve investigating this distributed approach, as well as the impact of migration. 4.4 Impact of Job and Ticket Placement by Users We have described our approach for placing jobs in the cluster when the system undertakes this role.
Reference: [27] <author> M. Mutka and M. Livny. </author> <title> Scheduling Remote Processing Capacity In A Workstation-Processor Bank Network. </title> <booktitle> In Proceedings of the 7th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 2-9, </pages> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: Long-term fairness was obtained in Condor by allowing users who have executed fewer jobs in the past to preempt users who have run more jobs <ref> [27] </ref>. Not only would we like a finer-level of control than the Condor algorithm provides, but we also need to support time-sharing of individual workstations. On the more theoretical side, micro-economic systems can provide more precise allocation [14, 25].
Reference: [28] <author> J. K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In Third International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: When time-sharing parallel jobs, traditional MPPs use explicit coscheduling, or gang scheduling, to coordinate communicating processes across nodes. By scheduling the processes of one job at the same time on each node, each parallel application is given the impression of a dedicated machine <ref> [28] </ref>. Coscheduling, in contrast to independent time-sharing across nodes, allows programs with fine-grain synchronization and communication to avoid busy-waiting and context-switching. Unfortunately, explicit coscheduling has a number of disadvantages that are accentuated in the NOW environment.
Reference: [29] <author> P. G. Sobalvarro, S. Pakin, W. E. Weihl, and A. A. Chien. </author> <title> Dynamic Coscheduling on Workstation Clusters. </title> <note> Available from http://www.research.digital.com/SRC/scheduling, 1997. </note>
Reference-contexts: and give more users closer to their proportional-share (i.e., users G, H, and I), the parallel job should still be placed on the three nodes with the least tickets. 5.2 Implicit Coscheduling A promising time-sharing approach for scheduling parallel applications in a cluster is implicit coscheduling [12], or dynamic coscheduling <ref> [29, 30] </ref>. Both of these approaches appear to have many of the advantages of explicit coscheduling, without the disadvantages.
Reference: [30] <author> P. G. Sobalvarro and W. E. Weihl. </author> <title> Demand-based Coscheduling of Parallel Jobs on Multiprogrammed Multiprocessors. </title> <booktitle> In Proceedings of the IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 63-75, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: and give more users closer to their proportional-share (i.e., users G, H, and I), the parallel job should still be placed on the three nodes with the least tickets. 5.2 Implicit Coscheduling A promising time-sharing approach for scheduling parallel applications in a cluster is implicit coscheduling [12], or dynamic coscheduling <ref> [29, 30] </ref>. Both of these approaches appear to have many of the advantages of explicit coscheduling, without the disadvantages.
Reference: [31] <author> I. Stoica, H. Abdel-Wahab, K. Jeffay, S. Baruah, J. Gehrke, and C. G. Plaxton. </author> <title> A Proportional Share Resource Allocation Algorithm for Real-Time, Time-Shared Systems. </title> <booktitle> In IEEE Real-Time Systems Symposium, </booktitle> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: New scheduling and resource allocation policies are therefore needed for our new environment. To time-share a cluster fairly, one must first be able to fairly schedule jobs on a single workstation. Several variations of proportional-share scheduling have been proposed that solve this problem for compute-bound jobs <ref> [15, 16, 31, 37] </ref>. We use our implementation of one of these approaches, stride scheduling [38], as a building-block in our evaluations. However, a tension exists between scheduling users fairly in the cluster and appropriately handling the mixed workload we expect to see in NOWs. <p> Proportional-share scheduling appears to provide an adequate solution for our needs, many variations of which have been recently proposed <ref> [15, 16, 31, 37] </ref>. With proportional-share scheduling, the resource consumption rights of each active process are proportional to the relative shares that it is allocated.
Reference: [32] <author> I. Stoica, H. Abdel-Wahab, and A. Pothen. </author> <title> A Microeconomic Scheduler for Parallel Computers. </title> <booktitle> In Proceedings of the IPPS '95 Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 122-135, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Further, their implementation requires processes to have an accurate approximation of their lifetime and does not allow time-sharing of workstations. A more recent simulation study allows applications to purchase time on multiple nodes simultaneously, thus supporting frequently communicating parallel applications that must run concurrently <ref> [32] </ref>. However, they also support only space-sharing and applications must predict their lifetime. An implementation of their system appears to need a centralized auction. In general, resource allocation in a parallel system can be performed by either space-sharing or time-sharing.
Reference: [33] <author> M. Theimer, K. Landtz, and D. Cheriton. </author> <title> Preemptable Remote Execution Facilities for the V System. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 2-12, </pages> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: 1 Introduction The definition of a network of workstations has changed in recent years. In the not-too-distant past, networks of workstations (NOWs) were a loose collection of workstations physically scattered across users' desks <ref> [2, 11, 23, 33, 41] </ref>. One of the main research issues was effectively exploiting idle resources when users were not running jobs on their desktop machines [1, 4, 34].
Reference: [34] <author> M. M. Theimer and K. A. Lantz. </author> <title> Finding Idle Machines in a Workstation-Based Distributed System. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(11) </volume> <pages> 1444-57, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: In the not-too-distant past, networks of workstations (NOWs) were a loose collection of workstations physically scattered across users' desks [2, 11, 23, 33, 41]. One of the main research issues was effectively exploiting idle resources when users were not running jobs on their desktop machines <ref> [1, 4, 34] </ref>. Due to the expense of finding available cycles and of migrating jobs if a workstation's owner returned, such systems performed only batch-processing, optimizing for throughput of long-running computationally-intensive applications.
Reference: [35] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: This behavior is similar to an approach advocated for distributing load information in large-scale shared-memory multiprocessors [9]. Even though our ticket-server uses a fast communication network and a low-overhead messaging protocol <ref> [24, 35] </ref>, it is still important to minimize the amount of communication. Because many processes are expected to be short-lived, each stride-scheduler only periodically contacts the local ticket server to update and determine the value of currencies, rather than every time a job begins or ends. <p> Berkeley NOW [3, 10]. Key components of the implementation work include extending the parallel ticket server so it is robust to node failures and placing jobs and tickets without the current centralized master. We will also soon measure our implementation of implicit coscheduling on Active Messages <ref> [24, 35] </ref> with both the Solaris time-sharing scheduler and our modified stride-schedulers. Acknowledgments We would like to thank Tom Anderson, Remzi Arpaci-Dusseau, and Amin Vahdat for their many helpful comments on the presentation of this work. Andrea Arpaci-Dusseau is currently supported by an Intel Foundation Graduate Fellowship.
Reference: [36] <author> C. Waldspurger, T. Hogg, B. Huberman, J. Kephart, and S. Stor-netta. Spawn: </author> <title> A Distributed Computational Economy. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(2) </volume> <pages> 103-117, </pages> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: In this final step, we combine stride-scheduling and implicit coscheduling [12] to extend our analysis to parallel jobs. 5.1 Background We begin by discussing how parallel jobs are scheduled and how fairness is provided in other distributed systems. The most relevant example of an implementation is Spawn <ref> [36] </ref>, a computational economy for concurrent applications in a distributed network. However, because the authors only consider coarse-grain applications, such as Monte-Carlo simulations, they do not simultaneously allocate multiple nodes to a single client.
Reference: [37] <author> C. A. Waldspurger and W. E. Weihl. </author> <title> Lottery Scheduling: Flexible Proportional-Share Resource Management. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 1-11. </pages> <publisher> USENIX Association, </publisher> <year> 1995. </year>
Reference-contexts: New scheduling and resource allocation policies are therefore needed for our new environment. To time-share a cluster fairly, one must first be able to fairly schedule jobs on a single workstation. Several variations of proportional-share scheduling have been proposed that solve this problem for compute-bound jobs <ref> [15, 16, 31, 37] </ref>. We use our implementation of one of these approaches, stride scheduling [38], as a building-block in our evaluations. However, a tension exists between scheduling users fairly in the cluster and appropriately handling the mixed workload we expect to see in NOWs. <p> Proportional-share scheduling appears to provide an adequate solution for our needs, many variations of which have been recently proposed <ref> [15, 16, 31, 37] </ref>. With proportional-share scheduling, the resource consumption rights of each active process are proportional to the relative shares that it is allocated. <p> We focus on stride scheduling [38] because it is easy to understand and to implement, has been well described and analyzed in the literature, and supports modular resource management. 2.2 Stride Scheduling Stride scheduling [38] is a deterministic allocation algorithm that encapsulates resource rights with tickets. Like lottery scheduling <ref> [37] </ref>, resources are allocated to competing clients in proportion to the number of tickets they hold. For example, if a client has t tickets in a system with T total tickets, the client receives t=T of the resources. <p> In this paper, we have shown that we can extend stride-scheduling [38] to provide a proportional-share of resources to users running a mixed workload in a distributed environment. First, we found that giving credit (i.e., exhaustible tickets <ref> [37] </ref>) to jobs that relinquish the processor, rewards jobs that periodically sleep while not harming other competing clients.
Reference: [38] <author> C. A. Waldspurger and W. E. Weihl. </author> <title> Stride Scheduling: Deterministic Proportional-Share Resource Mangement. </title> <type> Technical Report MIT/LCS/TM-528, </type> <institution> Massachusetts Institute of Technology, MIT Laboratory for Computer Science, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: To time-share a cluster fairly, one must first be able to fairly schedule jobs on a single workstation. Several variations of proportional-share scheduling have been proposed that solve this problem for compute-bound jobs [15, 16, 31, 37]. We use our implementation of one of these approaches, stride scheduling <ref> [38] </ref>, as a building-block in our evaluations. However, a tension exists between scheduling users fairly in the cluster and appropriately handling the mixed workload we expect to see in NOWs. <p> Proportional-share scheduling appears to provide an adequate solution for our needs, many variations of which have been recently proposed [15, 16, 31, 37]. With proportional-share scheduling, the resource consumption rights of each active process are proportional to the relative shares that it is allocated. We focus on stride scheduling <ref> [38] </ref> because it is easy to understand and to implement, has been well described and analyzed in the literature, and supports modular resource management. 2.2 Stride Scheduling Stride scheduling [38] is a deterministic allocation algorithm that encapsulates resource rights with tickets. <p> We focus on stride scheduling <ref> [38] </ref> because it is easy to understand and to implement, has been well described and analyzed in the literature, and supports modular resource management. 2.2 Stride Scheduling Stride scheduling [38] is a deterministic allocation algorithm that encapsulates resource rights with tickets. Like lottery scheduling [37], resources are allocated to competing clients in proportion to the number of tickets they hold. <p> In this paper, we have shown that we can extend stride-scheduling <ref> [38] </ref> to provide a proportional-share of resources to users running a mixed workload in a distributed environment. First, we found that giving credit (i.e., exhaustible tickets [37]) to jobs that relinquish the processor, rewards jobs that periodically sleep while not harming other competing clients.
Reference: [39] <author> C. A. Waldspurger and W. E. Weihl. </author> <title> An Object-Oriented Framework for Modular Resource Management. </title> <booktitle> In 5th Workshop on Object-Orientation in Operating Systems (IWOOOS '96), </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: We describe two frameworks that improve the response time of interactive jobs while not compromising the fairness of other competing jobs. Both policies build upon exhaustible tickets <ref> [39] </ref>, which are simply tickets with an expiration time. In general, when a client's exhaustible tickets expire, the client has caught-up to its desired proportional-share of resources.
Reference: [40] <author> S. Zhou. </author> <title> A Trace-Driven Simulation Study of Dynamic Load Balancing. </title> <type> Technical Report UCB/CSD 87/305, </type> <institution> Computer Science Division, Univerisity of California, Berkeley, </institution> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: A centralized location is not a realistic design decision when building a scalable, fault-tolerant cluster on the scale of hundreds of workstations. A promising approach for our environment is for each client to randomly query k nodes and pick the one best qualified <ref> [13, 26, 40] </ref>. Future work will involve investigating this distributed approach, as well as the impact of migration. 4.4 Impact of Job and Ticket Placement by Users We have described our approach for placing jobs in the cluster when the system undertakes this role.
Reference: [41] <author> S. Zhou, J. Wang, X. Zheng, and P. Delisle. </author> <title> Utopia: A Load Sharing Facility for Large, Heterogeneous Distributed Computing Systems. </title> <type> Technical Report CSRI-257, </type> <institution> University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction The definition of a network of workstations has changed in recent years. In the not-too-distant past, networks of workstations (NOWs) were a loose collection of workstations physically scattered across users' desks <ref> [2, 11, 23, 33, 41] </ref>. One of the main research issues was effectively exploiting idle resources when users were not running jobs on their desktop machines [1, 4, 34].
References-found: 41

