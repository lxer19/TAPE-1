URL: http://www.cs.cmu.edu/~jab/pubs/boyan.stage2.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/jab/web/cv/cv100.html
Root-URL: 
Email: fjab,awmg@cs.cmu.edu  
Title: Learning Evaluation Functions for Global Optimization and Boolean Satisfiability  
Author: Justin A. Boyan and Andrew W. Moore 
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Note: (To appear in AAAI-98)  
Abstract: This paper describes Stage, a learning approach to automatically improving search performance on optimization problems. Stage learns an evaluation function which predicts the outcome of a local search algorithm, such as hillclimbing or Walksat, as a function of state features along its search trajectories. The learned evaluation function is used to bias future search trajectories toward better optima. We present positive results on six large-scale optimization domains. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boese, K. D.; Kahng, A. B.; and Muddu, S. </author> <year> 1994. </year> <title> A new adaptive multi-start technique for combinatorial global optimizations. </title> <journal> Operations Research Letters 16 </journal> <pages> 101-113. </pages>
Reference-contexts: What this paper has shown is that for a wide variety of large-scale problems, with very simple choices of features and models, a useful structure can be identified and exploited. A very relevant investigation by Boese et. al. <ref> (Boese, Kahng, & Muddu 1994) </ref> gives further reasons for optimism. They studied the set of local minima reached by independent runs of hillclimbing on a traveling salesman problem and a graph bisection problem.
Reference: <author> Boyan, J. A., and Moore, A. W. </author> <year> 1997. </year> <title> Using prediction to improve combinatorial optimization search. </title> <booktitle> In Proceedings of AISTATS-6. </booktitle>
Reference-contexts: It then uses these new evaluation functions to guide further search. A preliminary version of Stage, described in <ref> (Boyan & Moore 1997) </ref>, showed promising performance on VLSI layout. This paper describes an improved version of Stage with superior results on VLSI layout and thorough results for five other global optimization domains.
Reference: <author> Boyan, J. A. </author> <year> 1998. </year> <title> Learning Evaluation Functions for Global Optimization. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: This choice of satisfies the Markov property. We will also always use simple linear or quadratic regression to fit ~ V , since training these models incrementally is extremely efficient in time and memory <ref> (Boyan 1998) </ref>. Using the Predictions The learned evaluation function ~ V (F (x)) evaluates how promising x is as a starting point for algorithm . To find the best starting point, we must optimize ~ V over X. <p> Table 1 compares Stage's performance with that of hillclimbing, simulated annealing, and best-fit-randomized (Coffman, Garey, & Johnson 1996), a bin-packing algorithm with good worst-case performance guarantees. Stage significantly outperforms all of these. We obtained similar results for all 20 instances in the u250 suite <ref> (Boyan 1998) </ref>. Channel Routing The problem of "Manhattan channel routing" is an important subtask of VLSI circuit design. Given two rows of labelled pins across a rectangular channel, we must connect like-labelled pins to one another by placing wire segments into vertical and horizontal tracks. <p> Results on four other 32-bit parity benchmark instances were similar. In these experiments, Walksat was run with noise=25 and ffi w =10; full details may be found in <ref> (Boyan 1998) </ref>. Future work will pursue this further. We believe that Stage shows promise for hard satisfiabil-ity problems|perhaps for MAXSAT problems where near-miss solutions are useful. Transfer There is a computational cost to training a function approximator on ~ V .
Reference: <author> Chao, H.-Y., and Harper, M. P. </author> <year> 1996. </year> <title> An efficient lower bound algorithm for channel routing. Integration: </title> <journal> The VLSI Journal. </journal>
Reference-contexts: 0:70 &gt; HYC2 9 9 &lt; 0:71; 0:21; 0:67 &gt; HYC4 20 23 &lt; 0:71; 0:03; 0:71 &gt; HYC6 50 51 &lt; 0:70; 0:05; 0:71 &gt; HYC8 21 25 &lt; 0:71; 0:03; 0:70 &gt; Table 2: Stage results (M = 10 5 ; N = 3) on eight problems from <ref> (Chao & Harper 1996) </ref>. The similarities among the learned evaluation functions are striking.
Reference: <author> Coffman, E. G.; Garey, M. R.; and Johnson, D. S. </author> <year> 1996. </year> <title> Approximation algorithms for bin packing: a survey. </title> <editor> In Hochbaum, D., ed., </editor> <title> Approximation Algorithms for NP-Hard Problems. </title> <publisher> PWS Publishing. </publisher>
Reference-contexts: When this occurs, Stage resets the search to a random starting state. Illustrative Example We will now work through a detailed illustrative example of Stage in operation. Then we will provide results on six large, difficult, global optimization problems. Our example comes from the practical, NP-complete domain of bin-packing <ref> (Coffman, Garey, & Johnson 1996) </ref>. In bin-packing, we are given a bin capacity C and a list L = (a 1 ; a 2 ; :::a n ) of n items, each having a size s (a i ) &gt; 0. <p> Bin-packing The first set of results is from a 250-item benchmark bin-packing instance (u250 13, from (Falkenauer & Delchambre 1992)). Table 1 compares Stage's performance with that of hillclimbing, simulated annealing, and best-fit-randomized <ref> (Coffman, Garey, & Johnson 1996) </ref>, a bin-packing algorithm with good worst-case performance guarantees. Stage significantly outperforms all of these. We obtained similar results for all 20 instances in the u250 suite (Boyan 1998). Channel Routing The problem of "Manhattan channel routing" is an important subtask of VLSI circuit design.
Reference: <author> Dorling, D. </author> <year> 1994. </year> <title> Cartograms for visualizing human geography. </title>
Reference-contexts: Again, all algorithms performed comparably, but Stage's solutions were best on average. electoral vote for U.S. President. Cartogram Design A "cartogram" is a map whose boundaries have been deformed so that population density is uniform over the entire map <ref> (Dorling 1994) </ref>. We considered redraw-ing the map of the United States such that each state's area is proportional to its electoral vote. The goal is to best meet the new area targets while minimally distorting the states' shapes and borders.
Reference: <editor> In Hearnshaw, H. M., and Unwin, D. J., eds., </editor> <title> Visualization in Geographical Information Systems. </title> <publisher> Wiley. </publisher> <pages> 85-102. </pages>
Reference: <author> Falkenauer, E., and Delchambre, A. </author> <year> 1992. </year> <title> A genetic algorithm for bin packing and line balancing. </title> <booktitle> In Proc. of the IEEE 1992 International Conference on Robotics and Automation, </booktitle> <pages> 1186-1192. </pages>
Reference-contexts: The actual objective function, Obj = # of bins used. 2. Var = the variance in fullness of the non-empty bins. This feature is similar to a cost function term intro duced in <ref> (Falkenauer & Delchambre 1992) </ref>. run on the example instance. On the first iteration (4a), Stage hillclimbs from the starting state (Obj = 30; Var = 0:011) to a local optimum (Obj = 13; Var = 0:019). <p> On each instance, all algorithms were held to the same number M of total search moves considered, and run N times. Bin-packing The first set of results is from a 250-item benchmark bin-packing instance (u250 13, from <ref> (Falkenauer & Delchambre 1992) </ref>). Table 1 compares Stage's performance with that of hillclimbing, simulated annealing, and best-fit-randomized (Coffman, Garey, & Johnson 1996), a bin-packing algorithm with good worst-case performance guarantees. Stage significantly outperforms all of these. We obtained similar results for all 20 instances in the u250 suite (Boyan 1998).
Reference: <author> Friedman, N., and Yakhini, Z. </author> <year> 1996. </year> <title> On the sample complexity of learning Bayesian networks. </title> <booktitle> In Proc. 12th Conference on Uncertainty in Artificial Intelligence. </booktitle>
Reference-contexts: Bayes Network Structure-Finding Given a data set, an important data mining task is to identify the Bayes net structure that best matches the data. We search the space of acyclic graph structures on A nodes, where A is the number of attributes in each data record. Following <ref> (Friedman & Yakhini 1996) </ref>, we evaluate a network structure by a minimum description length score which trades off between fit accuracy and low model complexity.
Reference: <author> Martin, O. C., and Otto, S. W. </author> <year> 1994. </year> <title> Combining simulated annealing with local search heuristics. </title> <type> Technical Report CS/E 94-016, </type> <institution> Oregon Graduate Institute Department of Computer Science and Engineering. </institution>
Reference-contexts: This led them to recommend a two-phase "adaptive multi-start" hillclimbing technique similar to Stage. A similar heuristic, Chained Local Optimization <ref> (Martin & Otto 1994) </ref>, also works by alternating between greedy search and a user-defined "kick" that moves the search into a nearby but different attracting basin.
Reference: <author> McAllester, D.; Kautz, H.; and Selman, B. </author> <year> 1997. </year> <title> Evidence for invariants in local search. </title> <booktitle> In Proceedings of AAAI-97. </booktitle>
Reference-contexts: Otherwise, it flips a variable which worsens Obj: with probability (1-noise), a variable which harms Obj the least, and with probability noise, a variable at random from the clause. The best setting of noise is problem-dependent <ref> (McAllester, Kautz, & Selman 1997) </ref>. Walksat is so effective that it has rendered nearly obsolete an archive of several hundred benchmark problems collected for a DIMACS Challenge on satis-fiability (Selman, Kautz, & Cohen 1996).
Reference: <author> Ochotta, E. </author> <year> 1994. </year> <title> Synthesis of High-Performance Analog Cells in ASTRX/OBLX. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University Department of Electrical and Computer Engineering. </institution>
Reference-contexts: The hillclimbing runs accepted equi-cost moves and restarted whenever patience consecutive moves produced no improvement. The simulated annealing runs made use of the successful "modified Lam" adaptive annealing schedule <ref> (Ochotta 1994, x4.5) </ref>; its parameters were hand-tuned to perform well across the whole range of problems but not exhaustively optimized for each individual problem instance. On each instance, all algorithms were held to the same number M of total search moves considered, and run N times.
Reference: <author> Russell, S., and Norvig, P. </author> <year> 1995. </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: All of these work by imposing a neighborhood relation on the states of X and then searching the graph that results, guided by Obj. Local search has been likened to "trying to find the top of Mount Everest in a thick fog while suffering from amnesia" <ref> (Russell & Norvig 1995, p.111) </ref>. The climber considers each step by consulting an altimeter and deciding whether to take the step based on the change in altitude.
Reference: <author> Selman, B.; Kautz, H.; and Cohen, B. </author> <year> 1996. </year> <title> Local search strategies for satisfiability testing. In Cliques, Coloring, and Satisfiability: Second DIMACS Implementation Challenge. </title> <publisher> American Mathematical Society. </publisher>
Reference-contexts: Satisfiability Finding a variable assignment which satisfies a large Boolean expression is a fundamental (indeed, the original) NP-complete problem. In recent years, surprisingly difficult formulas have been solved by Walksat <ref> (Selman, Kautz, & Cohen 1996) </ref>, a simple local search method. Walksat, given a formula expressed in CNF (a conjunction of disjunctive clauses), conducts a random walk in assignment space which is biased toward minimizing Obj (x) = # of clauses unsatisfied by assignment x. <p> The best setting of noise is problem-dependent (McAllester, Kautz, & Selman 1997). Walksat is so effective that it has rendered nearly obsolete an archive of several hundred benchmark problems collected for a DIMACS Challenge on satis-fiability <ref> (Selman, Kautz, & Cohen 1996) </ref>. Within that archive, only the largest "parity function learning" instances (nefariously constructed by Kearns, Schapire, Hirsh and Crawford) are known to be solvable in principle, yet not solvable by Walksat.
Reference: <author> Szykman, S., and Cagan, J. </author> <year> 1995. </year> <title> A simulated annealing-based approach to three-dimensional component packing. </title> <journal> ASME Journal of Mechanical Design 117. </journal>
Reference: <author> Webb, S. </author> <year> 1991. </year> <title> Optimization by simulated annealing of three-dimensional conformal treatment planning for radiation fields defined by a multileaf collimator. </title> <journal> Phys. Med. Biol. </journal> <volume> 36 </volume> <pages> 1201-1226. </pages>
Reference-contexts: The optimization problem, then, is to produce a treatment plan which meets target radiation doses for the tumor while minimizing damage to sensitive nearby structures. The current practice is to use simulated annealing for this problem <ref> (Webb 1991) </ref>. the brainstem, and the rest of the head). Given a treatment plan, the objective function is calculated by summing ten terms: an overdose penalty and an underdose penalty for each of the five structures. These ten sub-components were the features for Stage's learning.
Reference: <author> Wong, D. F.; Leong, H.; and Liu, C. </author> <year> 1988. </year> <title> Simulated Annealing for VLSI Design. </title> <publisher> Kluwer. </publisher>
Reference-contexts: In real optimization domains, such additional state features are generally plentiful. Practitioners of local search algorithms often append additional terms to their objective function, and then spend considerable effort tweaking the coefficients. This excerpt, from a book on VLSI layout by simulated annealing <ref> (Wong, Leong, & Liu 1988) </ref>, is typical: Clearly, the objective function to be minimized is the channel width w. However, w is too crude a measure of the quality of intermediate solutions. <p> Segments may cross but not otherwise overlap. The objective is to minimize the area of the channel's rectangular bounding box|or equivalently, to minimize the number of different horizontal tracks needed. We use the clever local search operators defined by Wong for this problem <ref> (Wong, Leong, & Liu 1988) </ref>, but replace their contrived objective function C (see Equation 1 above) with the natural objective function Obj (x) = the channel width w. <p> Results on YK4, an instance with 140 vertical tracks, are given in Table 1. All methods were allowed to consider 500,000 moves per run. Experiment (A) shows that multi-restart hillclimbing finds quite poor solutions. Experiment (B) shows that simulated annealing, as used with the objective function of <ref> (Wong, Leong, & Liu 1988) </ref>, does considerably better. Surprisingly, the annealer of Experiment (C) does better still. <p> The similarities among the learned evaluation functions are striking. Like the hand-tuned cost function C of <ref> (Wong, Leong, & Liu 1988) </ref> (Equation 1), all but one of the Stage-learned cost functions (HYC1) assigned a relatively large positive weight to feature w and a small positive weight to feature p. Unlike the hand-tuned C, all the Stage runs assigned a negative 7 weight to feature U .
Reference: <author> Zhang, W. </author> <year> 1996. </year> <title> Reinforcement Learning for Job-Shop Scheduling. </title> <type> Ph.D. Dissertation, </type> <institution> Oregon State University. </institution> <month> 8 </month>
Reference-contexts: Zhang and Dietterich have explored another way to use learning to improve combinatorial optimization: they learn a search strategy from scratch using online value iteration <ref> (Zhang 1996) </ref>. By contrast, Stage begins with an already-given search strategy and uses prediction to learn to improve on it. Zhang reported success in transferring learned search control knowledge from simple job-shop scheduling instances to more complex ones. Stage offers many directions for further exploration.
References-found: 18

