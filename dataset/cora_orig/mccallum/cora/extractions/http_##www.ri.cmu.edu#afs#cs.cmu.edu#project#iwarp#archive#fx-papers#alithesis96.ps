URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/iwarp/archive/fx-papers/alithesis96.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/iwarp/member/fx/public/www/papers.html
Root-URL: 
Title: Source-Level Debugging of Globally Optimized Code  
Author: Ali-Reza Adl-Tabatabai Thomas Gross, Chair Bernd Bruegge Peter Lee Susan Graham, 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy. Thesis Committee:  
Note: Copyright c fl1996 Ali-Reza Adl-Tabatabai This research was sponsored in part by the Advanced Research Projects Agency/CSTO monitored by SPAWAR under contract N00039-93-C-0152. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.  
Address: Pittsburgh, PA 15213  Berkeley  
Affiliation: School of Computer Science Carnegie Mellon University  UC  
Date: June 20, 1996  
Pubnum: CMU-CS-96-133  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Adl-Tabatabai and T. Gross. </author> <title> Detection and Recovery of Endangered Variables Caused by Instruction Scheduling. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1325. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: But such modifications or limitations no longer qualify a program as optimized and defeat the goal of debugging optimized code. In this dissertation, I assume that the debugger is non-invasive <ref> [2, 1] </ref>: the code generated by the compiler and debugged by the user is the default code generated with optimizations enabled. <p> and re-using runtime storage locations, optimizations make retrieval (modification) of source-level values from (in) the runtime state difficult (if not impossible) because some values may be either inaccessible in the runtime state [2] or inconsistent with what the user expects with respect to the source statement where execution has halted <ref> [1, 4] </ref>. These problems are called data-value problems [109]. In general, optimizations make it impossible for the debugger to provide the illusion that the source program is executing one source statement at a time, and that all in scope variables are available for inspection and modification. <p> be to extend the functionality of a traditional debugger so that it can provide interactions that will guide the user in debugging optimized code. 2.2.2 Values in optimized code Most research on debugging optimized code concentrates on the data-value problem and much terminology exists for this aspect of the problem <ref> [49, 2, 1, 36, 106] </ref>. In this section, I review this terminology. At a break, the debugger conveys the state of source variables by allowing the user to query variable values. <p> Otherwise, if the actual value is not identical, then the variable is said to be noncurrent [49]. In Figure 2.1, d is noncurrent at break hS 2 ; I 4 i. If it cannot be determined with certainty whether a variable is noncurrent, then this variable is called suspect <ref> [1] </ref>. At break hS 1 ; I 5 i, both f and g are suspect: S 3 has prematurely executed, but the value of p is nonresident, so we cannot determine which memory location has been prematurely modified. <p> For example, the debugger can tell the user at which source assignment (s) an endangered variable's actual value was (or may have been) assigned [3]. Several researchers have concentrated on the problems of detecting nonresident variables [2] and endangered variables <ref> [49, 1, 36, 106, 4] </ref>. In the following paragraphs, I summarize each of these prior works. Hennessy [49] introduced the concept of endangered and noncurrent variables, and 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 41 presented the first algorithms to detect endangered variables. <p> Because of ambiguities due either to multiple paths reaching a breakpoint [3], or to pointer assignments that are executed out of order <ref> [1] </ref>, the debugger may not always be able to determine whether a resident variable V is current or noncurrent; therefore, there are situations where the best the debugger can do is to report V as suspect. <p> This transformation does not cause data value problems. Only after the final object code has been produced are all optimizations exposed; thus, the analyses for detecting endangered variables must be performed on an instruction-level representation of the program <ref> [2, 1] </ref>. Like most compilers, however, cmcc has a two level intermediate representation consisting of a machine-independent IR used for global optimizations (e.g., partial redundancy elimination), and an instruction-level representation used for machine dependent optimizations (e.g., instruction scheduling and register allocation).
Reference: [2] <author> A. Adl-Tabatabai and T. Gross. </author> <title> Evicted Variables and the Interaction of Global Register Allocation and Symbolic Debugging. </title> <booktitle> In Conference Record of the 20th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 371383. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1993. </year>
Reference-contexts: But such modifications or limitations no longer qualify a program as optimized and defeat the goal of debugging optimized code. In this dissertation, I assume that the debugger is non-invasive <ref> [2, 1] </ref>: the code generated by the compiler and debugged by the user is the default code generated with optimizations enabled. <p> BACKGROUND * By reordering the execution of source expressions, and re-using runtime storage locations, optimizations make retrieval (modification) of source-level values from (in) the runtime state difficult (if not impossible) because some values may be either inaccessible in the runtime state <ref> [2] </ref> or inconsistent with what the user expects with respect to the source statement where execution has halted [1, 4]. These problems are called data-value problems [109]. <p> be to extend the functionality of a traditional debugger so that it can provide interactions that will guide the user in debugging optimized code. 2.2.2 Values in optimized code Most research on debugging optimized code concentrates on the data-value problem and much terminology exists for this aspect of the problem <ref> [49, 2, 1, 36, 106] </ref>. In this section, I review this terminology. At a break, the debugger conveys the state of source variables by allowing the user to query variable values. <p> If, at a break B, the storage location assigned to a variable V is holding the value of some other variable, then V is nonresident at B <ref> [2] </ref>. If V is resident, the storage location where V is accessible is called V 's residence and the value in V 's residence is V 's actual value [35]. 2 The fourth column of Table 2.2 lists the nonresident variables at breaks in the code of Figure 2.1. <p> For example, the debugger can tell the user at which source assignment (s) an endangered variable's actual value was (or may have been) assigned [3]. Several researchers have concentrated on the problems of detecting nonresident variables <ref> [2] </ref> and endangered variables [49, 1, 36, 106, 4]. In the following paragraphs, I summarize each of these prior works. Hennessy [49] introduced the concept of endangered and noncurrent variables, and 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 41 presented the first algorithms to detect endangered variables. <p> Thus the question of whether an uninitialized variable V is resident or current is irrelevant, since V has no expected value. Detecting and reporting uninitialized variables reduces the number of variables that are reported as nonresident or endangered, and provides additional information to the user <ref> [2] </ref>. In the absence of support provided by the runtime system (e.g., path determiners [108]) or the architecture (e.g., memory tags), detecting uninitialized variables requires that the debugger perform program flow analysis on the source program. <p> Referring back to Figure 5.7 (b), since I 1 and I 3 assign a source-level value of x to register Rx, these instructions are source definitions of x <ref> [2] </ref>. I 2 also assigns a source-level value of x to Rx and is a source definition of x, but this source definition is one that has been generated for a hoisted expression, and so I 2 is referred to as a hoisted definition of x. 112 CHAPTER 5. <p> E may be a constant, a fetch from a temporary, or some more general computation such as addition. In the case that E is a fetch from a temporary T , the compiler generates the residence <ref> [2] </ref> of V in the storage location assigned to T . If E is a constant, the compiler generates a special constant residence for V , indicating that the value of V is a constant. <p> This transformation does not cause data value problems. Only after the final object code has been produced are all optimizations exposed; thus, the analyses for detecting endangered variables must be performed on an instruction-level representation of the program <ref> [2, 1] </ref>. Like most compilers, however, cmcc has a two level intermediate representation consisting of a machine-independent IR used for global optimizations (e.g., partial redundancy elimination), and an instruction-level representation used for machine dependent optimizations (e.g., instruction scheduling and register allocation).
Reference: [3] <author> A. Adl-Tabatabai and T. Gross. </author> <title> Symbolic Debugging of Globally Optimized Code: Data Value Problems and Their Solutions. </title> <type> Technical Report CMU-CS-94-105, CMU, </type> <month> January </month> <year> 1994. </year>
Reference-contexts: The debugger can provide additional guidance by conveying how optimizations have affected source values. For example, the debugger can tell the user at which source assignment (s) an endangered variable's actual value was (or may have been) assigned <ref> [3] </ref>. Several researchers have concentrated on the problems of detecting nonresident variables [2] and endangered variables [49, 1, 36, 106, 4]. In the following paragraphs, I summarize each of these prior works. Hennessy [49] introduced the concept of endangered and noncurrent variables, and 2.4. <p> Because of ambiguities due either to multiple paths reaching a breakpoint <ref> [3] </ref>, or to pointer assignments that are executed out of order [1], the debugger may not always be able to determine whether a resident variable V is current or noncurrent; therefore, there are situations where the best the debugger can do is to report V as suspect.
Reference: [4] <author> A. Adl-Tabatabai and T. Gross. </author> <title> Source-Level Debugging of Scalar Optimized Code. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 3343. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: and re-using runtime storage locations, optimizations make retrieval (modification) of source-level values from (in) the runtime state difficult (if not impossible) because some values may be either inaccessible in the runtime state [2] or inconsistent with what the user expects with respect to the source statement where execution has halted <ref> [1, 4] </ref>. These problems are called data-value problems [109]. In general, optimizations make it impossible for the debugger to provide the illusion that the source program is executing one source statement at a time, and that all in scope variables are available for inspection and modification. <p> For example, the debugger can tell the user at which source assignment (s) an endangered variable's actual value was (or may have been) assigned [3]. Several researchers have concentrated on the problems of detecting nonresident variables [2] and endangered variables <ref> [49, 1, 36, 106, 4] </ref>. In the following paragraphs, I summarize each of these prior works. Hennessy [49] introduced the concept of endangered and noncurrent variables, and 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 41 presented the first algorithms to detect endangered variables.
Reference: [5] <author> A. Adl-Tabatabai, T. Gross, and G.Y. Lueh. </author> <title> Code Reuse in an Optimizing Compiler. </title> <booktitle> In OOPSLA '96 Conference Proceedings. ACM, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: I have implemented all the algorithms I present in this dissertation in the context of the cmcc compiler (this compiler is described in detail in <ref> [5] </ref>). cmcc is a retargetable optimizing C compiler that I have implemented in collaboration with others as part of my research. <p> optimized code generated by gcc (version 2.3.2) and MIPS cc on a DECstation 5000/200. algorithms used throughout cmcc are very similar in structure and a large amount of code reuse is achieved in cmcc through the use of a data-flow analysis framework; details of this framework can be found in <ref> [5] </ref>. To allow experimentation with profile-based optimizations, cmcc can generate an instrumented version of a program that, when executed, produces basic block and control-flow edge execution frequencies. <p> It is much simpler to implement the bookkeeping necessary for an optimization at the same time as the optimization is implemented. * Frameworks provide a very powerful form of code reuse inside an optimizing compiler <ref> [5] </ref>. A framework captures the control structure of a class of computations but leaves specification of the exact functionality to the client of the framework. The best example of how frameworks can be used effectively inside a compiler is for data 7.2. FUTURE WORK 163 flow analysis.
Reference: [6] <author> A. Adl-Tabatabai, G. Langdale, S. Lucco, and R. Wahbe. </author> <title> Efficient and Language-Independent Mobile Programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 127136. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: By reusing registers that are assigned to source-level variables, register allocation com 1 Systems that generated code dynamically (e.g., [70] or <ref> [6] </ref>) are even more challenging to decipher. 15 plicates the debugger's basic task of retrieving the runtime value of a variable.
Reference: [7] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year> <note> 171 172 BIBLIOGRAPHY </note>
Reference-contexts: In this dissertation, I do not consider functional languages, although many of the techniques also apply to functional languages and their compilers. The compiler model used in this research is based on the traditional compiler organization used in most conventional industry and 17 research compilers for imperative languages <ref> [7, 30, 71, 10] </ref>. The compiler first translates a source program into a machine-independent intermediate representation (IR). Classic machine-independent scalar optimizations for example, code hoisting, dead code elimination, strength reduction, and so on are performed on the IR. <p> Examples of code hoisting optimizations include partial redundancy elimination [76, 30, 39, 64], 1 In fact in cmcc and other compilers, induction variable elimination is implemented as a simple variation on dead assignment elimination [30]. 90 CHAPTER 5. DETECTING ENDANGERED VARIABLES loop invariant code motion <ref> [7] </ref>, and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. Examples of code sinking optimizations include partial dead code elimination [65], unspeculation [41], global instruction scheduling algorithms that sink code past conditional branches [28], superblock dead code elimination [29], and forward propagation [18]. <p> In other words, I assume that code hoisting eliminates an available expression from one point in the program after inserting one or more copies of the expression at other locations. The other code hoisting transformations explicitly relocate either a particular operation (e.g., loop invariant code motion <ref> [7] </ref>) or a particular instruction (e.g., global scheduling [14] or superblock optimization [29]). In these cases, I assume that the moved operation is eliminated from its original location and inserted into its new location. <p> Speculative instruction scheduling is becoming an increasingly important technique for extracting such parallelism from a program. I address speculative code motion in Section 5.6. 5.1.4 Optimizations that do not cause endangerment Many scalar optimizations, such as strength reduction, constant folding, and constant propagation <ref> [7] </ref>, do not affect assignments to source variables but create new opportunities for dead assignment elimination and thus indirectly contribute to creating nonresident 92 CHAPTER 5. DETECTING ENDANGERED VARIABLES and endangered variables. <p> The data-flow analysis used to detect endangered variables due to code hoisting is similar to the reaching definitions analysis <ref> [7] </ref>. In the remainder of this section, I describe this data-flow analysis solution.
Reference: [8] <author> A.W. Appel. </author> <title> Continuation-Passing, Closure-Passing Style. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 293302. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1989. </year>
Reference-contexts: Although optimizations do not affect debugging in their approach, their technique seems motivated mainly by the fact that their compiler transforms the code through multiple representations, lambda-calculus, continuation passing style <ref> [8] </ref> and then assembly code, making the tracking back to the source code very difficult.
Reference: [9] <author> A.W. Appel and D.B. MacQueen. </author> <title> A Standard ML Compiler. </title> <editor> In G. Kahn, editor, </editor> <booktitle> Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 301324, </pages> <address> Portland, OR, </address> <month> September </month> <year> 1987. </year> <title> ACM, </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: No implementation of this proposal exists. Tolmach and Appel [93] present an approach to source-level debugging used in the Standard ML of New Jersey (SML-NJ) compiler <ref> [9] </ref>, an optimizing compiler for Standard ML [75]. SML-NJ instruments the source program so that information is gathered at runtime in support of debugger queries.
Reference: [10] <author> M. Auslander and M. Hopkins. </author> <title> An Overview of the PL8 Compiler. </title> <booktitle> In Proceedings of the ACM SIGPLAN '82 Symposium on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1982. </year>
Reference-contexts: The modern RISC approach to designing processors has also increased the effectiveness and importance of classical (machine-independent) compiler optimizations. The simple loadstore instruction sets of RISC processors not only provide a simple code selection target for the compiler, but also act as suitable compiler intermediate representations <ref> [57, 10, 80] </ref>. By modelling each RISC instruction in its intermediate representation, the compiler exposes most of the final instructions to classical optimizations such as partial redundancy elimination, strength reduction, and so on thus reducing path lengths in the object code. <p> In this dissertation, I do not consider functional languages, although many of the techniques also apply to functional languages and their compilers. The compiler model used in this research is based on the traditional compiler organization used in most conventional industry and 17 research compilers for imperative languages <ref> [7, 30, 71, 10] </ref>. The compiler first translates a source program into a machine-independent intermediate representation (IR). Classic machine-independent scalar optimizations for example, code hoisting, dead code elimination, strength reduction, and so on are performed on the IR.
Reference: [11] <author> T. Ball and J.R. Larus. </author> <title> Branch Prediction For Free. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 300313. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: Global register allocation [25, 32] can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers. Using software branch prediction <ref> [98, 11, 42] </ref>, the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors [54, 82].
Reference: [12] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic Program Paral-lelization. </title> <booktitle> Proc. IEEE, </booktitle> <address> 81(2):211243, </address> <month> Feb </month> <year> 1993. </year>
Reference-contexts: In this dissertation, I consider the effects of transformations performed by both the machine-independent optimization and code generation phases of the compiler. I do not consider loop transformations such as interchange, skewing, reversal, blocking, splitting, unswitching, and so forth <ref> [12] </ref>. Such transformations have traditionally been used for vectorization and parallelization, and have only more recently been used in compilers for uni-processors. The effects of loop transformations on source-level debugging is left for future work; in Section 7.2, I suggest an approach for dealing with loop optimizations.
Reference: [13] <author> D. Bernstein, D. Q. Goldin, M. C. Golumbic, H. Krawczyk, Y. Mansour, I. Nahshon, and R. Y. Pinter. </author> <title> Spill Code Minimization Techniques for Optimizing Compilers. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 258263. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1989. </year>
Reference-contexts: These two techniques split a variable's live range, but require no shuffle code since the segments are disconnected. There are several variations on the graph simplification approach: The RS/6000 compiler <ref> [13] </ref> optimizes spill code placement and refines the heuristic spill cost function.
Reference: [14] <author> D. Bernstein and M. Rodeh. </author> <title> Global Instruction Scheduling for Superscalar Machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 241255. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Such resources include instruction-level parallelism, large register files, instruction fetching mechanisms, and memory hierarchies. The compiler speeds up program execution by performing transformations that exploit the resources exposed by a target processor. For example, instruction scheduling <ref> [43, 51, 67, 14, 101] </ref> can increase the efficiency of processors with instruction-level parallelism by statically scheduling independent operations for concurrent execution. Global register allocation [25, 32] can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers. <p> DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions <ref> [14] </ref>. Examples of code sinking optimizations include partial dead code elimination [65], unspeculation [41], global instruction scheduling algorithms that sink code past conditional branches [28], superblock dead code elimination [29], and forward propagation [18]. <p> The other code hoisting transformations explicitly relocate either a particular operation (e.g., loop invariant code motion [7]) or a particular instruction (e.g., global scheduling <ref> [14] </ref> or superblock optimization [29]). In these cases, I assume that the moved operation is eliminated from its original location and inserted into its new location. <p> In fact, it is these invariants that have allowed me to produce a solution to the problem that is significantly simpler than the approaches described by Wismueller [107] and Copperman [36]. Not all compiler optimizations satisfy these safety constraints. Global instruction scheduling algorithms that schedule instructions for speculative execution <ref> [43, 14, 28] </ref>, can hoist an instruction I from a block B to another block that is not post-dominated by B. Since I is being introduced into a program path where it did not exist before, I should be selected from the execution path with the highest execution probability.
Reference: [15] <author> D.S. Blickstein, P.W. Craig, C.S. Davidson, R.N. Faiman, K.D. Glossop, R.B. Grove, S.O. Hobbs, and W.B. Noyce. </author> <title> The GEM Optimizing Compiler System. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4), </volume> <year> 1992. </year>
Reference-contexts: Strength reduction is integrated with partial redundancy elimination using the algorithm described Knoop et al.[63]. Induction variable simplification is performed during partial redundancy elimination to reduce the number of address temporaries introduced by strength reduction. This optimization is also known as base binding <ref> [15] </ref>. Linear function test replacement and induction variable elimination are performed after partial redundancy elimination. Finally, partial dead code elimination (also known as assignment sinking) is performed using the algorithm described by Knoop et al.[65]. Partial dead code elimination is repeated until no more changes are detected in the program.
Reference: [16] <author> S. Borkar, R. Cohn, G. Cox, S. Gleason, T. Gross, H. T. Kung, M. Lam, B. Moore, C. Peterson, J. Pieper, L. Rankin, P. S. Tseng, J. Sutton, J. Urbanski, and J. Webb. </author> <title> iWarp: An Integrated Solution to High-Speed Parallel Computing. </title> <booktitle> In Proc. Supercomputing '88, </booktitle> <pages> pages 330339, </pages> <address> Orlando, Florida, </address> <month> November </month> <year> 1988. </year> <journal> IEEE Computer Society and ACM SIGARCH. </journal> <volume> BIBLIOGRAPHY 173 </volume>
Reference-contexts: Table 3.1 lists the optimizations performed by cmcc. At this time, cmcc has been targeted to the MIPS [59], SPARC [78], DLX [50], and iWarp <ref> [16] </ref> architectures. I obtained the results that I report in this dissertation using the code generator for the MIPS architecture. cmcc consists of two major phases: (1) machine-independent global optimization, and (2) code generation. Each of these phases in part comprises a series of transformations. <p> i &lt; I j if and only if I i is scheduled before I j (in other words, I assume precise interrupts); therefore, this partial ordering can be easily calculated by sequentially numbering all instructions inside of a basic block schedule. (V)LIW or statically scheduled superscalar machines (e.g., the iWarp <ref> [16] </ref>) can execute multiple instructions concurrently; therefore, I i = I j implies that I i and I j complete execution concurrently. The order in which source definition instructions complete execution determines the order in which source-level assignments actually execute.
Reference: [17] <author> P. Briggs. </author> <title> Register Allocation via Graph Coloring. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Although graph simplification can also be implemented using the basic blocks representation of live ranges, graph simplification has traditionally been implemented using the instruction-level representation <ref> [17] </ref>. Renumbering [26] and web analysis [57] are techniques that isolate disjoint segments of a live range; these techniques further refine the instruction-level representation of live ranges, and result in interference graphs of potentially lower degree. <p> These two techniques split a variable's live range, but require no shuffle code since the segments are disconnected. There are several variations on the graph simplification approach: The RS/6000 compiler [13] optimizes spill code placement and refines the heuristic spill cost function. Some approaches <ref> [17, 66] </ref> have implemented live range splitting within the graph simplification framework by splitting live ranges before graph coloring; register allocation then tries to minimize the runtime cost of shuffle code by eliminating unnecessary splits via coalescing (coalescing is discussed in the next section).
Reference: [18] <author> P. Briggs and K. Cooper. </author> <title> Effective Partial Redundancy Elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 159170. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Examples of code sinking optimizations include partial dead code elimination [65], unspeculation [41], global instruction scheduling algorithms that sink code past conditional branches [28], superblock dead code elimination [29], and forward propagation <ref> [18] </ref>. Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available [76] or dead [65]. <p> S 2 : ...x... S 2 : ...y+z... S 2 : ...tmp... (a) (b) (c) and dead code elimination (c) After common subexpression elimination. In cmcc, this assignment propagation is performed to improve partial redundancy elimination <ref> [30, 18] </ref>, and the situation described above occurs quite often. The final effect of this series of transformations is that the source-level variable x is replaced with tmp.
Reference: [19] <author> P. Briggs, K. D. Cooper, K. Kennedy, and L. Torczon. </author> <title> Coloring Heuristics for Register Allocation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 275284. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1989. </year>
Reference-contexts: There are two common heuristic graph coloring algorithms used in the context of register allocation: priority-based coloring [32] and graph simplification <ref> [25, 19] </ref>. Priority-based coloring assigns registers (colors) to live ranges (vertices) in an order determined by the priority of each live range; the priority of a live range L is an estimation of the execution time saved by keeping L in a register.
Reference: [20] <author> G. Brooks, G. Hansen, and S. Simmons. </author> <title> A New Approach to Debugging Optimized Code. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 111. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: No implementation of this proposal exists. For source-to-source transformations (e.g., loop interchange) this approach may be feasible since such transformations are usually performed on a high-level (almost source-level) representation of a program, and can thus be easily reflected in the source. The CXdb debugger <ref> [20] </ref> is an example of an alternative way to implement the exposed approach. CXdb animates execution by highlighting source expressions as the user single steps at the object level. CXdb only highlights the expression that is about to be executed and does not provide a control reference statement. <p> The range records are passed to the debugger, which uses them to determine whether a stopping instruction lies within a variable's live range. The CXdb debugger <ref> [20] </ref> also uses live ranges to determine residency. Using a variable's live range for determining residency is simplistic and conservative: 64 CHAPTER 4. DETECTING NONRESIDENT VARIABLES (a) (b) Ranges in which x is resident. the debugger uses a simple rule that is always right but misses opportunities. <p> However, the value of the eliminated induction variable can be derived from the new address temporary (i.e., i=((tmp-A)&gt;>2)-1). Such an approach is used in the DOC [38] and CXdb <ref> [20] </ref> debuggers. There are other situations where the overall effect of a series of transformations is the replacement of a source-level variable with a compiler temporary, again allowing the debugger to infer values from compiler temporaries; I address this issue in more detail in Section 5.5.5. <p> Such a fine-grained mapping can be used to accurately pin-point to the user the expression where an asynchronous break occurs. The CXdb debugger <ref> [20] </ref> uses such an expression-level mapping to highlight source expressions that are executed as the user single steps through the object code. In the rest of this chapter, I assume a statement-level mapping to the source.
Reference: [21] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving Register Allocation for Subscripted Variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 5365. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Moves: A move instruction transfers a live range's value from one register to another. 4.1.5 Global variables and aggregates Global variables and aggregates (i.e., array and structure elements) can also be register allocation candidates <ref> [100, 21, 84] </ref>. When included as register allocation candidates, globals and aggregates are usually assigned a home location in memory and are promoted to registers over a limited region of the program (e.g., those regions where they are not aliased and are frequently accessed [53]).
Reference: [22] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 4052, </pages> <address> Santa Clara, </address> <month> April </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: Using software branch prediction [98, 11, 42], the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors [54, 82]. Transformations such as blocking [68] and prefetching <ref> [22, 77] </ref> can improve a program's data cache locality, thus reducing stalls due to cache misses [77]. 11 The importance of optimizations is not limited to low-level machine-specific transformations. The modern RISC approach to designing processors has also increased the effectiveness and importance of classical (machine-independent) compiler optimizations.
Reference: [23] <author> D. Callahan and B. Koblenz. </author> <title> Register Allocation via Hierarchical Graph Coloring. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, pages 192203, </booktitle> <address> Toronto, </address> <month> June </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: Some approaches [17, 66] have implemented live range splitting within the graph simplification framework by splitting live ranges before graph coloring; register allocation then tries to minimize the runtime cost of shuffle code by eliminating unnecessary splits via coalescing (coalescing is discussed in the next section). Other approaches <ref> [23, 45, 79, 72] </ref> have devel 60 CHAPTER 4.
Reference: [24] <author> B. </author> <title> Case. Intel Reveals Pentium Implementation Details. </title> <type> Microprocessor Report, </type> <pages> pages 917, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Even newer implementations of CISC architectures are providing a RISC core set of instructions that execute efficiently and are simple to model in a compiler. For example, Intel's Pentium <ref> [24] </ref> and Pentium Pro [47], AMD's K5 [88] and NexGen's Nx686 [48] processors are superscalar implementations of the x86 architecture, which can concurrently dispatch only RISC-like instructions [56]; complex instructions are dispatched one at a time.
Reference: [25] <author> G. J. Chaitin. </author> <title> Register Allocation and Spilling via Graph Coloring. </title> <booktitle> In Proceedings of the ACM SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <pages> pages 98105, </pages> <month> June </month> <year> 1982. </year> <journal> In SIGPLAN Notices, v. </journal> <volume> 17, </volume> <editor> n. </editor> <volume> 6. </volume> <pages> 174 BIBLIOGRAPHY </pages>
Reference-contexts: The compiler speeds up program execution by performing transformations that exploit the resources exposed by a target processor. For example, instruction scheduling [43, 51, 67, 14, 101] can increase the efficiency of processors with instruction-level parallelism by statically scheduling independent operations for concurrent execution. Global register allocation <ref> [25, 32] </ref> can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers. <p> The code generation phase of the compiler performs global register allocation and local (i.e., intra-basic block) instruction scheduling. The global register allocator (described in detail in [72]) is a unique algorithm that integrates live range splitting with a Chaitin-style graph coloring register allocator <ref> [25] </ref>. The register allocation phase performs register coalescing and incorporates the improved coloring heuristics described by Briggs et al.[19]. The instruction scheduler is a list scheduler that can schedule in either the forward or backward directions. <p> The goal is to assign a register for exclusive use by a variable during the variable's live range, which consists of those basic blocks or instructions that lie between definitions and last uses of the variable <ref> [25, 32] </ref>. Live ranges that overlap in the program conflict (or interfere) and are assigned different physical registers. For each compilation unit, the compiler constructs an interference graph whose vertices represent live ranges, and whose edges connect live ranges that conflict. <p> There are two common heuristic graph coloring algorithms used in the context of register allocation: priority-based coloring [32] and graph simplification <ref> [25, 19] </ref>. Priority-based coloring assigns registers (colors) to live ranges (vertices) in an order determined by the priority of each live range; the priority of a live range L is an estimation of the execution time saved by keeping L in a register. <p> Other approaches [23, 45, 79, 72] have devel 60 CHAPTER 4. DETECTING NONRESIDENT VARIABLES oped spilling and splitting heuristics that are sensitive to program structure and execution probabilities. 4.1.2 Register coalescing Coalescing or subsumption <ref> [25] </ref> is an important register allocation optimization that eliminates copy operations by assigning the same physical register to the source and destination operands of a move instruction.
Reference: [26] <author> G. J. Chaitin, M. A. Auslander, A. K. Chandra, J. Cocke, M. E. Hopkins, and P. W. Markstein. </author> <title> Register Allocation by Coloring. </title> <type> Research Report 8395, </type> <institution> IBM Watson Research Center, </institution> <year> 1981. </year>
Reference-contexts: Although graph simplification can also be implemented using the basic blocks representation of live ranges, graph simplification has traditionally been implemented using the instruction-level representation [17]. Renumbering <ref> [26] </ref> and web analysis [57] are techniques that isolate disjoint segments of a live range; these techniques further refine the instruction-level representation of live ranges, and result in interference graphs of potentially lower degree.
Reference: [27] <author> C. Chambers, D. Ungar, and E. Lee. </author> <title> An Efficient Implementation of SELF, A Dynamically-Typed Object-Oriented Language Based on Prototypes. </title> <booktitle> In OOPSLA '89 Conference Proceedings, </booktitle> <pages> pages 4970. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1989. </year>
Reference-contexts: A program representation is described that keeps track of optimizing transformations and allows the compiler to derive the unoptimized version of the program after optimizations. No implementation of this proposal exists. Holzle, Chambers and Ungar [52] describe the approach to debugging optimized code used in the SELF compiler system <ref> [27] </ref>, an optimizing compiler for the object-oriented language SELF [94]. This approach restricts the program points at which the debugger can take control to discrete interrupt points. Optimizations are constrained so that the complete source-level state can be reconstructed at each interrupt point.
Reference: [28] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu. </author> <title> IMPACT: An Architectural Framework for Multiple-Instruction-Issue Processors. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 266275. </pages> <address> ACM/IEEE, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. Examples of code sinking optimizations include partial dead code elimination [65], unspeculation [41], global instruction scheduling algorithms that sink code past conditional branches <ref> [28] </ref>, superblock dead code elimination [29], and forward propagation [18]. Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available [76] or dead [65]. <p> In fact, it is these invariants that have allowed me to produce a solution to the problem that is significantly simpler than the approaches described by Wismueller [107] and Copperman [36]. Not all compiler optimizations satisfy these safety constraints. Global instruction scheduling algorithms that schedule instructions for speculative execution <ref> [43, 14, 28] </ref>, can hoist an instruction I from a block B to another block that is not post-dominated by B. Since I is being introduced into a program path where it did not exist before, I should be selected from the execution path with the highest execution probability.
Reference: [29] <author> P. P. Chang, S. A. Mahlke, and W. W. Hwu. </author> <title> Using Profile Information to Assist Classic Code Optimizations. </title> <journal> Software Practice and Experience, </journal> <volume> 21(12):13011321, </volume> <month> Dec </month> <year> 1991. </year>
Reference-contexts: Code expansion can have a negative effect on the cache behavior of a program; therefore, it is probably not worth performing code duplication specifically to eliminate those ambiguous cases where a variable is resident but potentially uninitialized. Although global optimizations can benefit from code duplication <ref> [29] </ref>, in the case of eliminating resident but potentially uninitialized variables, code duplication is performed at a very late stage of compilation (during or after register allocation). Unless another global optimization phase is performed after register allocation, the code duplication will not benefit optimization. <p> DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. Examples of code sinking optimizations include partial dead code elimination [65], unspeculation [41], global instruction scheduling algorithms that sink code past conditional branches [28], superblock dead code elimination <ref> [29] </ref>, and forward propagation [18]. Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available [76] or dead [65]. <p> The other code hoisting transformations explicitly relocate either a particular operation (e.g., loop invariant code motion [7]) or a particular instruction (e.g., global scheduling [14] or superblock optimization <ref> [29] </ref>). In these cases, I assume that the moved operation is eliminated from its original location and inserted into its new location.
Reference: [30] <author> F. Chow. </author> <title> A Portable, Machine-Independent Global Optimizer Design and Measurements. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1984. </year>
Reference-contexts: In this dissertation, I do not consider functional languages, although many of the techniques also apply to functional languages and their compilers. The compiler model used in this research is based on the traditional compiler organization used in most conventional industry and 17 research compilers for imperative languages <ref> [7, 30, 71, 10] </ref>. The compiler first translates a source program into a machine-independent intermediate representation (IR). Classic machine-independent scalar optimizations for example, code hoisting, dead code elimination, strength reduction, and so on are performed on the IR. <p> Copy propagation and dead code elimination are performed next. These two algorithms are based on the algorithms described by Chow <ref> [30] </ref> and are repeated until they effect no more changes on the program 1 . After copy propagation and dead code elimination, the optimizer performs partial redundancy elimination using the algorithm described by Knoop et al.[64]. <p> Hoisting of assignments causes endangerment by prematurely updating variable values, while sinking of assignments causes endangerment by delaying variable updates. Examples of code hoisting optimizations include partial redundancy elimination <ref> [76, 30, 39, 64] </ref>, 1 In fact in cmcc and other compilers, induction variable elimination is implemented as a simple variation on dead assignment elimination [30]. 90 CHAPTER 5. DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. <p> Examples of code hoisting optimizations include partial redundancy elimination [76, 30, 39, 64], 1 In fact in cmcc and other compilers, induction variable elimination is implemented as a simple variation on dead assignment elimination <ref> [30] </ref>. 90 CHAPTER 5. DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. <p> Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available [76] or dead [65]. For instance, the Morel and Renvoise partial redundancy algorithm [76] and its variants <ref> [30, 39, 62] </ref> compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant. The net effect of this transformation is a hoisting of E. <p> 1 reaches a break at B3, so in addition to telling the user that x is suspect, the debugger can tell the user that the actual value of x is the value assigned by E 1 . 5.5.2 Terminology The Morel and Renvoise partial redundancy algorithm [76] and its variants <ref> [30, 39, 62] </ref> compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant. <p> S 2 : ...x... S 2 : ...y+z... S 2 : ...tmp... (a) (b) (c) and dead code elimination (c) After common subexpression elimination. In cmcc, this assignment propagation is performed to improve partial redundancy elimination <ref> [30, 18] </ref>, and the situation described above occurs quite often. The final effect of this series of transformations is that the source-level variable x is replaced with tmp.
Reference: [31] <author> F. Chow. </author> <title> Minimizing Register Usage Penalty at Procedure Calls. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 8594. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: The placement of callee-saved shuffle code can be optimized by saving and reloading the value of a callee-saved register R only along those paths where R is actually used <ref> [31, 41] </ref>. The values that are saved and reloaded by callee-saved shuffle code are values from calling functions. Callee-saved values have no relationship to values in the context of the callee function; therefore, callee-saved shuffle code does not play a role when determining the residence of a local variable.
Reference: [32] <author> F. C. Chow and J. L. Hennessy. </author> <title> A Priority-Based Coloring Approach to Register Allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12:501 535, </volume> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: The compiler speeds up program execution by performing transformations that exploit the resources exposed by a target processor. For example, instruction scheduling [43, 51, 67, 14, 101] can increase the efficiency of processors with instruction-level parallelism by statically scheduling independent operations for concurrent execution. Global register allocation <ref> [25, 32] </ref> can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers. <p> The goal is to assign a register for exclusive use by a variable during the variable's live range, which consists of those basic blocks or instructions that lie between definitions and last uses of the variable <ref> [25, 32] </ref>. Live ranges that overlap in the program conflict (or interfere) and are assigned different physical registers. For each compilation unit, the compiler constructs an interference graph whose vertices represent live ranges, and whose edges connect live ranges that conflict. <p> There are two common heuristic graph coloring algorithms used in the context of register allocation: priority-based coloring <ref> [32] </ref> and graph simplification [25, 19]. Priority-based coloring assigns registers (colors) to live ranges (vertices) in an order determined by the priority of each live range; the priority of a live range L is an estimation of the execution time saved by keeping L in a register. <p> The basic block representation of a live range is at a coarser and thus less precise granularity than the instruction-level representation: a register is prohibited from holding more than one live range's value within a basic block <ref> [32] </ref>. In practice, this is overly conservative when basic blocks are long, so implementations that use the basic blocks representation usually split large basic blocks into smaller ones [32, 69]. <p> In practice, this is overly conservative when basic blocks are long, so implementations that use the basic blocks representation usually split large basic blocks into smaller ones <ref> [32, 69] </ref>.
Reference: [33] <author> R. Cohn. </author> <title> Source-level Debugging of Automatically Parallelized Programs. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon, </institution> <month> Oct </month> <year> 1992. </year>
Reference-contexts: The effects of loop transformations on source-level debugging is left for future work; in Section 7.2, I suggest an approach for dealing with loop optimizations. This dissertation does not investigate debugging of parallelized code (debugging of parallelized code is the subject of Cohn's dissertation <ref> [33] </ref>). In this dissertation, I concentrate on the low-level algorithms necessary to implement the core functionality of a source-level debugger. This core functionality includes retrieving and reporting variable values, and setting and reporting of breakpoints.
Reference: [34] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman. </author> <title> A VLIW Architecture for a Trace Scheduling Compiler. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 37(8):967979, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: Therefore, speculative code motion is usually guided by profile information. It is assumed that I will not cause a fault, or if it does, the fault is suppressed until the control dependencies of I are resolved <ref> [34, 89, 74] </ref>. As the amount of instruction-level parallelism offered by processors increases, the compiler must look harder for instructions that can be scheduled for parallel execution. Speculative instruction scheduling is becoming an increasingly important technique for extracting such parallelism from a program.
Reference: [35] <author> M. Copperman. </author> <title> Debugging Optimized Code Without Being Misled. </title> <type> Technical Report UCSC-CRL-93-21, </type> <institution> UC Santa Cruz, </institution> <month> June </month> <year> 1993. </year> <note> BIBLIOGRAPHY 175 </note>
Reference-contexts: Bugs can surface when optimizations are enabled, even when compiler optimizations are correct. Optimizations change the data layout of a program and cause the program to execute faster; therefore, the debuggable translation of a program may mask a bug due to differences in storage layout and timing behavior <ref> [35] </ref>. For example, differences in timing behavior may cause the appearance of bugs due to race conditions. Or, differences in memory layout may cause pointer bugs that are unnoticed in the unoptimized version to cause a runtime error in the optimized version or vice-versa. 2. <p> If V is resident, the storage location where V is accessible is called V 's residence and the value in V 's residence is V 's actual value <ref> [35] </ref>. 2 The fourth column of Table 2.2 lists the nonresident variables at breaks in the code of Figure 2.1. The first three columns of this table again show the correspondences between instructions, source expressions, and statements. <p> For example, consider software 3 Some commercial debuggers that claim to provide support for optimized code, naively display the value in a queried variable's storage location without detecting whether the variable is resident <ref> [35] </ref>. 40 CHAPTER 2. <p> BACKGROUND instruction, by detecting store instructions that were moved across statement boundaries by the code scheduler. This information is passed on to the debugger. Copperman <ref> [35, 36] </ref> and Wismueller [107, 106] have investigated the data-value problem of detecting endangered variables caused by global optimizations. Their efforts concentrate only on detecting if a variable is endangered; they do not consider code location problems in any depth. Copperman's approach to detecting endangered variables [35, 36] is based on <p> Copperman <ref> [35, 36] </ref> and Wismueller [107, 106] have investigated the data-value problem of detecting endangered variables caused by global optimizations. Their efforts concentrate only on detecting if a variable is endangered; they do not consider code location problems in any depth. Copperman's approach to detecting endangered variables [35, 36] is based on a specialized data-flow analysis of an intermediate representation of the program. This representation captures the effects of optimizing transformations but does not consider nonresident variables. Copperman does not consider language issues such as undefined evaluation order. <p> S 1 : S 1 : do f do f S 3 : ...*p... S 3 : ...tmp... S 5 : g while (E) S 5 : g while (E) Source program After code hoisting Some researchers <ref> [109, 35, 106] </ref> have suggested that an alternative strategy exists for setting breakpoints at statements from which code has been hoisted.
Reference: [36] <author> M. Copperman. </author> <title> Debugging Optimized Code Without Being Misled. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(3):387427, </volume> <month> May </month> <year> 1994. </year>
Reference-contexts: be to extend the functionality of a traditional debugger so that it can provide interactions that will guide the user in debugging optimized code. 2.2.2 Values in optimized code Most research on debugging optimized code concentrates on the data-value problem and much terminology exists for this aspect of the problem <ref> [49, 2, 1, 36, 106] </ref>. In this section, I review this terminology. At a break, the debugger conveys the state of source variables by allowing the user to query variable values. <p> For example, the debugger can tell the user at which source assignment (s) an endangered variable's actual value was (or may have been) assigned [3]. Several researchers have concentrated on the problems of detecting nonresident variables [2] and endangered variables <ref> [49, 1, 36, 106, 4] </ref>. In the following paragraphs, I summarize each of these prior works. Hennessy [49] introduced the concept of endangered and noncurrent variables, and 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 41 presented the first algorithms to detect endangered variables. <p> BACKGROUND instruction, by detecting store instructions that were moved across statement boundaries by the code scheduler. This information is passed on to the debugger. Copperman <ref> [35, 36] </ref> and Wismueller [107, 106] have investigated the data-value problem of detecting endangered variables caused by global optimizations. Their efforts concentrate only on detecting if a variable is endangered; they do not consider code location problems in any depth. Copperman's approach to detecting endangered variables [35, 36] is based on <p> Copperman <ref> [35, 36] </ref> and Wismueller [107, 106] have investigated the data-value problem of detecting endangered variables caused by global optimizations. Their efforts concentrate only on detecting if a variable is endangered; they do not consider code location problems in any depth. Copperman's approach to detecting endangered variables [35, 36] is based on a specialized data-flow analysis of an intermediate representation of the program. This representation captures the effects of optimizing transformations but does not consider nonresident variables. Copperman does not consider language issues such as undefined evaluation order. <p> Or, if an assignment is eliminated because of backward redundancy, the value must be available somewhere, and the debugger can provide this value to the user. Both Copperman <ref> [36] </ref> and Wismueller [106] assume arbitrary code movement and elimination, and fail to recognize that movement and elimination are not unconstrained. <p> By taking advantage of these code motion invariants, the algorithms for detecting endangered variables are greatly simplified. In fact, it is these invariants that have allowed me to produce a solution to the problem that is significantly simpler than the approaches described by Wismueller [107] and Copperman <ref> [36] </ref>. Not all compiler optimizations satisfy these safety constraints. Global instruction scheduling algorithms that schedule instructions for speculative execution [43, 14, 28], can hoist an instruction I from a block B to another block that is not post-dominated by B. <p> Moreover, the algorithms work on the final object representation of a program; this is in contrast to other approaches that keep around a copy of the original source program representation (e.g., [106]) or work on an auxiliary intermediate representation (e.g., <ref> [36] </ref>). 5.5.1 Example I illustrate with an example the data-value problems caused by global optimizations and how these problems can be managed by the debugger. five expressions E 1 :::E 5 that assign to a source-level variable x and exactly two uses of x, one after E 4 inside block B7 <p> These annotations and markers are ignored by optimizations and optimizations are not constrained in any way. This method of tracking the correspondence between the optimized code and the source code is in contrast to the approach taken by Wismeuller [106] and Copperman <ref> [36] </ref>. In those approaches, a representation of the original source program is kept as a copy, and links are maintained between the intermediate representation used for optimizations and the original representation (e.g., an abstract syntax tree). <p> The analyses are very similar to other analyses that are done by the compiler and can thus take advantage of an infrastructure that is already present. This is in contrast to other approaches that require specialized data-flow analyses and program representations <ref> [36, 106] </ref>. To gather the information required for the data-flow analyses, the program intermediate representation is annotated during optimizations to mark hoisted and sunk assignments, and additional markers are inserted to indicate points from which source-level assignments are eliminated.
Reference: [37] <author> M. Copperman and J. Thomas. Poor Man's Watchpoints. </author> <type> Technical Report UCSC-CRL-93-12, </type> <institution> UC Santa Cruz, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Data breakpoints specify the breakpoint condition in terms of source data objects (e.g., halt when a variable or memory location is referenced). Data breakpoints can be implemented in hardware, software, or a combination. The efficient implementation of data breakpoints is a topic of active research <ref> [61, 96, 97, 60, 37] </ref> and beyond the scope of this dissertation. A more general form of breakpoint, conditional breakpoints, suspends execution when a user specified predicate on the program's state becomes true.
Reference: [38] <author> D. S. Coutant, S. Meloy, and M. Ruscetta. </author> <title> DOC: A Practical Approach to Source-Level Debugging of Globally Optimized Code. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 125 134. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: Hennessy also presents algorithms to recover the expected values of endangered variables; but these recovery algorithms recover values only from memory and do not consider partially computed results that are available in registers. DOC <ref> [38] </ref> is a prototype debugger developed at HP, designed to demonstrate the feasibility of debugging optimized code. The optimizations that DOC handles are instruction scheduling, register allocation, constant propagation, and induction variable elimination. <p> The advantage of this approach is that live range information is already computed by the compiler's register allocation phase. For example, in the DOC debugger <ref> [38] </ref>, the address ranges of instructions in a variable's live range are recorded in the range record data structure at the same time as the interference graph is built by the register allocator. <p> However, the value of the eliminated induction variable can be derived from the new address temporary (i.e., i=((tmp-A)&gt;>2)-1). Such an approach is used in the DOC <ref> [38] </ref> and CXdb [20] debuggers. There are other situations where the overall effect of a series of transformations is the replacement of a source-level variable with a compiler temporary, again allowing the debugger to infer values from compiler temporaries; I address this issue in more detail in Section 5.5.5.
Reference: [39] <author> D. M. Dhamdhere. </author> <title> Practical Adaptation of the Global Optimization Algorithm of Morel and Renvoise. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2):291294, </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Hoisting of assignments causes endangerment by prematurely updating variable values, while sinking of assignments causes endangerment by delaying variable updates. Examples of code hoisting optimizations include partial redundancy elimination <ref> [76, 30, 39, 64] </ref>, 1 In fact in cmcc and other compilers, induction variable elimination is implemented as a simple variation on dead assignment elimination [30]. 90 CHAPTER 5. DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. <p> Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available [76] or dead [65]. For instance, the Morel and Renvoise partial redundancy algorithm [76] and its variants <ref> [30, 39, 62] </ref> compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant. The net effect of this transformation is a hoisting of E. <p> 1 reaches a break at B3, so in addition to telling the user that x is suspect, the debugger can tell the user that the actual value of x is the value assigned by E 1 . 5.5.2 Terminology The Morel and Renvoise partial redundancy algorithm [76] and its variants <ref> [30, 39, 62] </ref> compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant.
Reference: [40] <editor> DWARF Debugging Information Format. </editor> <title> Industry review draft, </title> <booktitle> UNIX International, Programming Languages SIG, </booktitle> <address> Parsippany, NJ, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: One such issue is the interface between the compiler and debugger. The information collected by the compiler must be passed to the debugger. Traditional object file formats are clearly not powerful enough to support this interface. The newer DWARF 2.0 debug format <ref> [40] </ref> supports the communication of residence information: the compiler can specify the storage location associated with a variable for a given range of instructions. DWARF, however, does not define a mechanism for communicating endangerment information to the debugger.
Reference: [41] <author> D. Ebcioglu, R. Groves, K. Kim, G. Silberman, and I. Ziv. </author> <title> VLIW Compilation Techniques in a Superscalar Environment. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 3648. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: The placement of callee-saved shuffle code can be optimized by saving and reloading the value of a callee-saved register R only along those paths where R is actually used <ref> [31, 41] </ref>. The values that are saved and reloaded by callee-saved shuffle code are values from calling functions. Callee-saved values have no relationship to values in the context of the callee function; therefore, callee-saved shuffle code does not play a role when determining the residence of a local variable. <p> DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. Examples of code sinking optimizations include partial dead code elimination [65], unspeculation <ref> [41] </ref>, global instruction scheduling algorithms that sink code past conditional branches [28], superblock dead code elimination [29], and forward propagation [18].
Reference: [42] <author> J. A. Fisher and S. M. Freudenberger. </author> <title> Predicting Conditional Branch Direction From Previous Runs of a Program. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 8597. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: Global register allocation [25, 32] can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers. Using software branch prediction <ref> [98, 11, 42] </ref>, the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors [54, 82].
Reference: [43] <author> J.A. Fisher. </author> <title> Trace Scheduling: A Technique for Global Microcode Compaction. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-30(7):478490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Such resources include instruction-level parallelism, large register files, instruction fetching mechanisms, and memory hierarchies. The compiler speeds up program execution by performing transformations that exploit the resources exposed by a target processor. For example, instruction scheduling <ref> [43, 51, 67, 14, 101] </ref> can increase the efficiency of processors with instruction-level parallelism by statically scheduling independent operations for concurrent execution. Global register allocation [25, 32] can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers. <p> To allow modification of variable values, a function is converted to its unoptimized form at debug time (the TS compiler also performs incremental re-compilation). 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 37 Gupta [46] proposes a technique for debugging code transformed by trace scheduling compilers <ref> [43, 71] </ref>. During debugging, the user specifies commands for monitoring values and conditions at various points in the program. Code is then added to the program to collect the monitored values and the parts of the program containing the new monitor commands are incrementally recompiled. <p> In fact, it is these invariants that have allowed me to produce a solution to the problem that is significantly simpler than the approaches described by Wismueller [107] and Copperman [36]. Not all compiler optimizations satisfy these safety constraints. Global instruction scheduling algorithms that schedule instructions for speculative execution <ref> [43, 14, 28] </ref>, can hoist an instruction I from a block B to another block that is not post-dominated by B. Since I is being introduced into a program path where it did not exist before, I should be selected from the execution path with the highest execution probability.
Reference: [44] <author> C. Fraser and D. Hanson. </author> <title> A Retargetable C Compiler: Design and Implementation. </title> <address> Benjamin/Cummings, </address> <year> 1995. </year>
Reference-contexts: gather the measurements in this dissertation and the program suite I used in my measurements. 3.1 The cmcc compiler cmcc accepts ANSI C as the source language and produces assembly language for a variety of target machine architectures. cmcc uses the lcc ANSI C compiler developed by Fraser and Hanson <ref> [44] </ref> as a front end. I have modified lcc to generate cmcc's internal representation. To preserve maximum flexibility, the cmcc optimizer and code generator runs as a separate C++ program, reading in the internal representation emitted by the lcc front end. Table 3.1 lists the optimizations performed by cmcc. <p> In addition to the SPEC92 C programs, I have included three (publicly-available) C programs: (1) lcc, version 1.9 of the retargetable ANSI C compiler developed by Fraser and Hanson <ref> [44] </ref>; (2) tcl, version 7.3 of the the Tcl (tool command language) scripting language system developed by John Ousterhout [81]; and (3) triangle, version 1.1 of the two-dimensional quality mesh generator and delaunay triangulator developed by Jonathan Shewchuk at Carnegie Mellon University [87].
Reference: [45] <author> S. Freudenberger and J. Ruttenberg. </author> <title> Phase Ordering of Register Allocation and Instruction Scheduling. </title> <editor> In R. Giegerich and S. L. Graham, editors, </editor> <title> Code Generation Concepts, </title> <booktitle> Tools, Techniques, </booktitle> <pages> pages 146170. </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year> <note> 176 BIBLIOGRAPHY </note>
Reference-contexts: Some approaches [17, 66] have implemented live range splitting within the graph simplification framework by splitting live ranges before graph coloring; register allocation then tries to minimize the runtime cost of shuffle code by eliminating unnecessary splits via coalescing (coalescing is discussed in the next section). Other approaches <ref> [23, 45, 79, 72] </ref> have devel 60 CHAPTER 4.
Reference: [46] <author> R. Gupta. </author> <title> Debugging Code Reorganized By a Trace Scheduling Compiler. Structured Programming, </title> <address> 11(3):141150, </address> <year> 1990. </year>
Reference-contexts: Some approaches are invasive: the debugger requires modifications to the program or limitations on the transformations performed by the compiler. Source-level debugging can be made significantly easier if the compiler or debugger is allowed to insert additional code into the program before or after optimizations <ref> [93, 46] </ref>, or if compiler optimizations are constrained so that problems do not occur [111, 52]. But such modifications or limitations no longer qualify a program as optimized and defeat the goal of debugging optimized code. <p> Therefore, providing expected behavior non-invasively is generally not feasible when debugging the optimized translation of a program. Several approaches have been proposed that provide expected behavior at the expense of limiting the scope of optimizations <ref> [52, 111, 46, 93] </ref>. These approaches sacrifice optimizations for debugging and emphasize interactive debugging over post-mortem debugging. <p> These approaches sacrifice optimizations for debugging and emphasize interactive debugging over post-mortem debugging. These approaches use a combination of techniques: * The program points where the debugger can be invoked are limited and optimizations are constrained so that debugger functionality can be provided at these points <ref> [83, 52, 111, 46] </ref>. The compiler optimizes code between invocation points in a manner that allows the user to query program state when the debugger is invoked; for example, the compiler guarantees that all variables that are in scope at an invocation point are current. <p> Instrumentation code can be placed at a few program points specified by the user <ref> [46] </ref> or at all source statements [93]. Optimizations do not interfere with debugging because the instrumentation code is inserted before optimizations. * In response to a user request, the region of code affected by the request is incrementally recompiled by the debugger. <p> Optimizations do not interfere with debugging because the instrumentation code is inserted before optimizations. * In response to a user request, the region of code affected by the request is incrementally recompiled by the debugger. Either new instrumentation code is inserted to implement the user request <ref> [46] </ref> or a function is deoptimized, allowing the user request to be carried out within that function [83, 52, 111]. In the following paragraphs, I first present each approach, and then discuss the limitations of the techniques. 36 CHAPTER 2. <p> To allow modification of variable values, a function is converted to its unoptimized form at debug time (the TS compiler also performs incremental re-compilation). 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 37 Gupta <ref> [46] </ref> proposes a technique for debugging code transformed by trace scheduling compilers [43, 71]. During debugging, the user specifies commands for monitoring values and conditions at various points in the program.
Reference: [47] <author> L. Gwennap. </author> <title> Intel's P6 Uses Decoupled Superscalar Design. </title> <type> Microprocessor Report, </type> <pages> pages 915, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Even newer implementations of CISC architectures are providing a RISC core set of instructions that execute efficiently and are simple to model in a compiler. For example, Intel's Pentium [24] and Pentium Pro <ref> [47] </ref>, AMD's K5 [88] and NexGen's Nx686 [48] processors are superscalar implementations of the x86 architecture, which can concurrently dispatch only RISC-like instructions [56]; complex instructions are dispatched one at a time.
Reference: [48] <author> L. Gwennap. </author> <title> Nx686 Goes Toe-to-Toe with Pentium Pro. </title> <type> Microprocessor Report, </type> <pages> pages 110, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Even newer implementations of CISC architectures are providing a RISC core set of instructions that execute efficiently and are simple to model in a compiler. For example, Intel's Pentium [24] and Pentium Pro [47], AMD's K5 [88] and NexGen's Nx686 <ref> [48] </ref> processors are superscalar implementations of the x86 architecture, which can concurrently dispatch only RISC-like instructions [56]; complex instructions are dispatched one at a time.
Reference: [49] <author> J. L. Hennessy. </author> <title> Symbolic Debugging of Optimized Code. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3):323344, </volume> <month> July </month> <year> 1982. </year>
Reference-contexts: Optimizations, however, make it difficult to provide source-level debugging functionality. Although researchers and implementors have long acknowledged the need for source-level debugging of optimized code <ref> [102, 49, 86] </ref>, compiler developers have generally avoided supporting source-level debugging of optimized code. Designing a source-level debugger for globally optimized code remains an open problem. Current trends in processor design are towards an increasing reliance on compiler transformations to achieve high performance. <p> be to extend the functionality of a traditional debugger so that it can provide interactions that will guide the user in debugging optimized code. 2.2.2 Values in optimized code Most research on debugging optimized code concentrates on the data-value problem and much terminology exists for this aspect of the problem <ref> [49, 2, 1, 36, 106] </ref>. In this section, I review this terminology. At a break, the debugger conveys the state of source variables by allowing the user to query variable values. <p> If the actual value of a variable V at a break B is identical to the expected value of V relative to the corresponding control reference statement, then V is current at B. Otherwise, if the actual value is not identical, then the variable is said to be noncurrent <ref> [49] </ref>. In Figure 2.1, d is noncurrent at break hS 2 ; I 4 i. If it cannot be determined with certainty whether a variable is noncurrent, then this variable is called suspect [1]. <p> For example, the debugger can tell the user at which source assignment (s) an endangered variable's actual value was (or may have been) assigned [3]. Several researchers have concentrated on the problems of detecting nonresident variables [2] and endangered variables <ref> [49, 1, 36, 106, 4] </ref>. In the following paragraphs, I summarize each of these prior works. Hennessy [49] introduced the concept of endangered and noncurrent variables, and 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 41 presented the first algorithms to detect endangered variables. <p> Several researchers have concentrated on the problems of detecting nonresident variables [2] and endangered variables [49, 1, 36, 106, 4]. In the following paragraphs, I summarize each of these prior works. Hennessy <ref> [49] </ref> introduced the concept of endangered and noncurrent variables, and 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 41 presented the first algorithms to detect endangered variables. <p> Therefore, the algorithms described by Hennessy in <ref> [49] </ref> (and corrected by Wall et al. in [99]) deal with noncurrency due to local dead code elimination, and re-ordering introduced by local common subexpressions that are assigned to variables; the algorithms do not consider effects of code generation optimizations such as instruction scheduling and register allocation. <p> The values of I 5 's source registers (R1 and R2) are computed at instructions I 1 and I 2 , and are thus available at I 4 ; therefore, the debugger can provide the expected value of d by interpreting I 5 . This process is called recovery <ref> [49] </ref>. When attempting recovery, the debugger must be prepared to handle the case where the interpreted instructions cause an exception. Also, to allow execution to be resumed, the debugger must not overwrite runtime values during recovery. <p> ENDANGERED VARIABLES CAUSED BY INSTRUCTION SCHEDULING 93 prematurely overwritten V 's expected value, in which case V is a roll back variable, or the assignment that updates the expected value of V has been delayed until after the breakpoint, in which case V is a roll forward variable <ref> [49] </ref>. To detect noncurrent variables, the debugger must detect which operations have executed out of order and how these operations affect the source-level state (i.e., variables and memory locations). <p> Since the effects of instruction scheduling are localized, I need only model what occurs within basic blocks rather than what arises globally among basic blocks. My approach is similar to Hennessy's <ref> [49] </ref>; but my model also considers values held in the physical registers of the machine and computed by individual instructions, whereas Hennessy's approach operates strictly on the machine-independent intermediate representation of the program. 5.2.1 Source execution order I define the canonical execution order of source expressions to be the order in
Reference: [50] <author> J. L Hennessy and D. A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufman, </publisher> <year> 1990. </year>
Reference-contexts: Superscalar machines may allow instructions to complete out of order during execution of a program but usually provide precise interrupts <ref> [50] </ref> when an exception occurs. Recovering a precise machine state when the target machine's exceptions are imprecise is orthogonal to the topic of this dissertation. <p> To preserve maximum flexibility, the cmcc optimizer and code generator runs as a separate C++ program, reading in the internal representation emitted by the lcc front end. Table 3.1 lists the optimizations performed by cmcc. At this time, cmcc has been targeted to the MIPS [59], SPARC [78], DLX <ref> [50] </ref>, and iWarp [16] architectures. I obtained the results that I report in this dissertation using the code generator for the MIPS architecture. cmcc consists of two major phases: (1) machine-independent global optimization, and (2) code generation. Each of these phases in part comprises a series of transformations.
Reference: [51] <author> J.L. Hennessy and T.R. Gross. </author> <title> Postpass Code Optimization of Pipeline Constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3):422 448, </volume> <month> July </month> <year> 1983. </year>
Reference-contexts: Such resources include instruction-level parallelism, large register files, instruction fetching mechanisms, and memory hierarchies. The compiler speeds up program execution by performing transformations that exploit the resources exposed by a target processor. For example, instruction scheduling <ref> [43, 51, 67, 14, 101] </ref> can increase the efficiency of processors with instruction-level parallelism by statically scheduling independent operations for concurrent execution. Global register allocation [25, 32] can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers.
Reference: [52] <author> U. Holzle, C. Chambers, and D. Ungar. </author> <title> Debugging Optimized Code with Dynamic Deoptimization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 3243. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: Source-level debugging can be made significantly easier if the compiler or debugger is allowed to insert additional code into the program before or after optimizations [93, 46], or if compiler optimizations are constrained so that problems do not occur <ref> [111, 52] </ref>. But such modifications or limitations no longer qualify a program as optimized and defeat the goal of debugging optimized code. <p> Therefore, providing expected behavior non-invasively is generally not feasible when debugging the optimized translation of a program. Several approaches have been proposed that provide expected behavior at the expense of limiting the scope of optimizations <ref> [52, 111, 46, 93] </ref>. These approaches sacrifice optimizations for debugging and emphasize interactive debugging over post-mortem debugging. <p> These approaches sacrifice optimizations for debugging and emphasize interactive debugging over post-mortem debugging. These approaches use a combination of techniques: * The program points where the debugger can be invoked are limited and optimizations are constrained so that debugger functionality can be provided at these points <ref> [83, 52, 111, 46] </ref>. The compiler optimizes code between invocation points in a manner that allows the user to query program state when the debugger is invoked; for example, the compiler guarantees that all variables that are in scope at an invocation point are current. <p> Either new instrumentation code is inserted to implement the user request [46] or a function is deoptimized, allowing the user request to be carried out within that function <ref> [83, 52, 111] </ref>. In the following paragraphs, I first present each approach, and then discuss the limitations of the techniques. 36 CHAPTER 2. BACKGROUND Pollock and Soffa [83] propose an incremental compiler to disable optimizations that prohibit the debugger from satisfying a user request. <p> A program representation is described that keeps track of optimizing transformations and allows the compiler to derive the unoptimized version of the program after optimizations. No implementation of this proposal exists. Holzle, Chambers and Ungar <ref> [52] </ref> describe the approach to debugging optimized code used in the SELF compiler system [27], an optimizing compiler for the object-oriented language SELF [94]. This approach restricts the program points at which the debugger can take control to discrete interrupt points. <p> the debugger is invoked at an interrupt point, the function (i.e., the SELF method) containing the interrupt point is deoptimized by the debugger (if it is not deoptimized already) so that debugger functionality such as single stepping and variable query can be provided; this is referred to as dynamic deoptimization <ref> [52] </ref>. Once a function is deoptimized the debugger may be invoked at any source point within that function. The SELF system generates code incrementally for each function at runtime, thus facilitating the use of dynamic deoptimization.
Reference: [53] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. O. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lavery. </author> <title> The Superblock: An Effective Technique for VLIW and Superscalar Compilation. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1,2):229248, </volume> <month> March </month> <year> 1993. </year>
Reference-contexts: When included as register allocation candidates, globals and aggregates are usually assigned a home location in memory and are promoted to registers over a limited region of the program (e.g., those regions where they are not aliased and are frequently accessed <ref> [53] </ref>). Allocation of globals and aggregates requires alias and dependence analysis, and many compilers consider only local scalar variables (besides temporaries and constants) as register allocation candidates. The type and storage class of the register promoted variable, is orthogonal to residency detection.
Reference: [54] <author> W.W. Hwu and P.P. Chang. </author> <title> Achieving High Instruction Cache Performance with an Optimizing Compiler. </title> <booktitle> In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pages 242251. </pages> <publisher> ACM and IEEE Computer Society, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: Using software branch prediction [98, 11, 42], the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors <ref> [54, 82] </ref>. Transformations such as blocking [68] and prefetching [22, 77] can improve a program's data cache locality, thus reducing stalls due to cache misses [77]. 11 The importance of optimizations is not limited to low-level machine-specific transformations.
Reference: [55] <author> Intel. </author> <title> Optimizations for Intel's 32-Bit Processors. Application Note AP-500, </title> <institution> Intel Corp., </institution> <month> February </month> <year> 1994. </year> <note> BIBLIOGRAPHY 177 </note>
Reference-contexts: An Intel application note describing instruction selection and scheduling for the Pentium processor advises against selecting complex instructions and suggests using a loadstore model of instruction selection <ref> [55] </ref>. The larger register files of RISC processors allow the use of good heuristic global register allocation techniques, alleviating the phase-ordering problems between register allocation and classical optimizations that increase register pressure (e.g., code motion, induction variable strength reduction, function inlining, etc.).
Reference: [56] <author> K. Johnson. </author> <title> RISC-like Design Fares Well for x86 CPUs. </title> <type> Microprocessor Report, </type> <pages> pages 2627, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: For example, Intel's Pentium [24] and Pentium Pro [47], AMD's K5 [88] and NexGen's Nx686 [48] processors are superscalar implementations of the x86 architecture, which can concurrently dispatch only RISC-like instructions <ref> [56] </ref>; complex instructions are dispatched one at a time. An Intel application note describing instruction selection and scheduling for the Pentium processor advises against selecting complex instructions and suggests using a loadstore model of instruction selection [55].
Reference: [57] <author> M. S. Johnson and T. C. Miller. </author> <title> Effectiveness of a Machine-Level Global Optimizer. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 99108. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1986. </year>
Reference-contexts: The modern RISC approach to designing processors has also increased the effectiveness and importance of classical (machine-independent) compiler optimizations. The simple loadstore instruction sets of RISC processors not only provide a simple code selection target for the compiler, but also act as suitable compiler intermediate representations <ref> [57, 10, 80] </ref>. By modelling each RISC instruction in its intermediate representation, the compiler exposes most of the final instructions to classical optimizations such as partial redundancy elimination, strength reduction, and so on thus reducing path lengths in the object code. <p> Although graph simplification can also be implemented using the basic blocks representation of live ranges, graph simplification has traditionally been implemented using the instruction-level representation [17]. Renumbering [26] and web analysis <ref> [57] </ref> are techniques that isolate disjoint segments of a live range; these techniques further refine the instruction-level representation of live ranges, and result in interference graphs of potentially lower degree. These two techniques split a variable's live range, but require no shuffle code since the segments are disconnected.
Reference: [58] <author> R.E. Johnson, J.O. Graver, and L.W. Zurawski. </author> <title> TS: An Optimizing Compiler for Smalltalk. </title> <booktitle> In OOPSLA '88 Conference Proceedings, </booktitle> <pages> pages 1826. </pages> <publisher> ACM, </publisher> <month> September </month> <year> 1988. </year>
Reference-contexts: Once a function is deoptimized the debugger may be invoked at any source point within that function. The SELF system generates code incrementally for each function at runtime, thus facilitating the use of dynamic deoptimization. Zurawski and Johnson [111] describe a similar approach used in the TS compiler <ref> [58] </ref>, an optimizing compiler for Typed Smalltalk. Like the SELF system, the debugger can be invoked only at pre-determined points that in the TS system are called inspection points.
Reference: [59] <author> G. Kane and J Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: To preserve maximum flexibility, the cmcc optimizer and code generator runs as a separate C++ program, reading in the internal representation emitted by the lcc front end. Table 3.1 lists the optimizations performed by cmcc. At this time, cmcc has been targeted to the MIPS <ref> [59] </ref>, SPARC [78], DLX [50], and iWarp [16] architectures. I obtained the results that I report in this dissertation using the code generator for the MIPS architecture. cmcc consists of two major phases: (1) machine-independent global optimization, and (2) code generation.
Reference: [60] <author> D. Keppel. </author> <title> Fast Data Breakpoints. </title> <type> Technical Report UWCSE 93-04-06, </type> <institution> University of Washington, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Data breakpoints specify the breakpoint condition in terms of source data objects (e.g., halt when a variable or memory location is referenced). Data breakpoints can be implemented in hardware, software, or a combination. The efficient implementation of data breakpoints is a topic of active research <ref> [61, 96, 97, 60, 37] </ref> and beyond the scope of this dissertation. A more general form of breakpoint, conditional breakpoints, suspends execution when a user specified predicate on the program's state becomes true.
Reference: [61] <author> P. Kessler. </author> <title> Fast Breakpoints: </title> <booktitle> Design and Implementation. In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 7884. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Data breakpoints specify the breakpoint condition in terms of source data objects (e.g., halt when a variable or memory location is referenced). Data breakpoints can be implemented in hardware, software, or a combination. The efficient implementation of data breakpoints is a topic of active research <ref> [61, 96, 97, 60, 37] </ref> and beyond the scope of this dissertation. A more general form of breakpoint, conditional breakpoints, suspends execution when a user specified predicate on the program's state becomes true. <p> The clauses of the predicate can contain values of program variables, as well as references to either control or data breakpoints. The use of conditional breakpoints, as well as their efficient implementation, are an open research topic <ref> [61] </ref> and beyond the scope of this dissertation. 2. The debugger conveys to the user the control state of the halted debuggee in source-level terms by conveying which expressions have and have not executed. <p> A common approach is to use code patching to implement breakpoints <ref> [61] </ref> for example, to implement a control breakpoint at an instruction I, I is replaced by a jump to a code patch that contains a trap instruction followed by I. <p> Code patching can be performed either by the compiler tool chain before the program is run (e.g., as performed in [97] to implement data breakpoints), or by the debugger when the program is being debugged (e.g., to implement control breakpoints <ref> [61] </ref>). If modifications are done at compile time, the resulting code no longer qualifies as optimized (in the sense that this is the best code that can be generated for a given program). If the debugger is not invoked, then the execution has been unnecessarily slowed down. <p> Code patching techniques <ref> [61] </ref> could be used for a faster implementation of this conditional breakpoint: the branch instruction is replaced with an unconditional branch to an out-of-line code sequence that evaluates whether the debugger should be invoked. Note, that such a conditional breakpoint is invasive, because it changes the timing of the debuggee.
Reference: [62] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy Code Motion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 224234. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available [76] or dead [65]. For instance, the Morel and Renvoise partial redundancy algorithm [76] and its variants <ref> [30, 39, 62] </ref> compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant. The net effect of this transformation is a hoisting of E. <p> 1 reaches a break at B3, so in addition to telling the user that x is suspect, the debugger can tell the user that the actual value of x is the value assigned by E 1 . 5.5.2 Terminology The Morel and Renvoise partial redundancy algorithm [76] and its variants <ref> [30, 39, 62] </ref> compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant.
Reference: [63] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy Strength Reduction. </title> <journal> Journal of Programming Languages, </journal> <volume> 1(1):7191, </volume> <year> 1993. </year>
Reference: [64] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Optimal Code Motion: </title> <journal> Theory and Practice. ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(4):11171155, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Hoisting of assignments causes endangerment by prematurely updating variable values, while sinking of assignments causes endangerment by delaying variable updates. Examples of code hoisting optimizations include partial redundancy elimination <ref> [76, 30, 39, 64] </ref>, 1 In fact in cmcc and other compilers, induction variable elimination is implemented as a simple variation on dead assignment elimination [30]. 90 CHAPTER 5. DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14].
Reference: [65] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Partial Dead Code Elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 147158. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1994. </year> <note> 178 BIBLIOGRAPHY </note>
Reference-contexts: DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. Examples of code sinking optimizations include partial dead code elimination <ref> [65] </ref>, unspeculation [41], global instruction scheduling algorithms that sink code past conditional branches [28], superblock dead code elimination [29], and forward propagation [18]. <p> Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available [76] or dead <ref> [65] </ref>. For instance, the Morel and Renvoise partial redundancy algorithm [76] and its variants [30, 39, 62] compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant. <p> The net effect of this transformation is a hoisting of E. Similarly, the partial dead code elimination algorithm described in <ref> [65] </ref> computes the set of program points where the insertion of an assignment A will make other partially dead instances of A candidates for dead assignment elimination. The net effect of this transformation is a sinking of A. <p> In these cases, I assume that the moved operation is eliminated from its original location and inserted into its new location. Similarly, I consider only partial dead code elimination <ref> [65] </ref> while continuing to use the term code sinking; I make the same elimination and insertion assumptions about an explicitly sunk operation, with the difference that the eliminated assignments are considered useless (rather than available).
Reference: [66] <author> P. Kolte and M. J. Harrold. </author> <title> Load/Store Range Analysis for Global Register Allocation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 268277. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: These two techniques split a variable's live range, but require no shuffle code since the segments are disconnected. There are several variations on the graph simplification approach: The RS/6000 compiler [13] optimizes spill code placement and refines the heuristic spill cost function. Some approaches <ref> [17, 66] </ref> have implemented live range splitting within the graph simplification framework by splitting live ranges before graph coloring; register allocation then tries to minimize the runtime cost of shuffle code by eliminating unnecessary splits via coalescing (coalescing is discussed in the next section).
Reference: [67] <author> M. Lam. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Such resources include instruction-level parallelism, large register files, instruction fetching mechanisms, and memory hierarchies. The compiler speeds up program execution by performing transformations that exploit the resources exposed by a target processor. For example, instruction scheduling <ref> [43, 51, 67, 14, 101] </ref> can increase the efficiency of processors with instruction-level parallelism by statically scheduling independent operations for concurrent execution. Global register allocation [25, 32] can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers.
Reference: [68] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 6374, </pages> <address> Santa Clara, </address> <month> April </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: Using software branch prediction [98, 11, 42], the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors [54, 82]. Transformations such as blocking <ref> [68] </ref> and prefetching [22, 77] can improve a program's data cache locality, thus reducing stalls due to cache misses [77]. 11 The importance of optimizations is not limited to low-level machine-specific transformations.
Reference: [69] <author> J.R. Larus and P.N. Hilfinger. </author> <title> Register Allocation in the SPUR Lisp Compiler. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 255263. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1986. </year>
Reference-contexts: In practice, this is overly conservative when basic blocks are long, so implementations that use the basic blocks representation usually split large basic blocks into smaller ones <ref> [32, 69] </ref>.
Reference: [70] <author> P. Lee and M. Leone. </author> <title> Optimizing ML with Run-Time Code Generation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 137148. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: By reusing registers that are assigned to source-level variables, register allocation com 1 Systems that generated code dynamically (e.g., <ref> [70] </ref> or [6]) are even more challenging to decipher. 15 plicates the debugger's basic task of retrieving the runtime value of a variable.
Reference: [71] <author> P. G. Lowney, S. M. Freudenberger, T. J. Karzes, W. D. Lichtenstein, R. P. Nix, J. O'Donnell, and J. C. Ruttenberg. </author> <title> The Multiflow Trace Scheduling Compiler. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1,2):51142, </volume> <month> March </month> <year> 1993. </year>
Reference-contexts: In this dissertation, I do not consider functional languages, although many of the techniques also apply to functional languages and their compilers. The compiler model used in this research is based on the traditional compiler organization used in most conventional industry and 17 research compilers for imperative languages <ref> [7, 30, 71, 10] </ref>. The compiler first translates a source program into a machine-independent intermediate representation (IR). Classic machine-independent scalar optimizations for example, code hoisting, dead code elimination, strength reduction, and so on are performed on the IR. <p> To allow modification of variable values, a function is converted to its unoptimized form at debug time (the TS compiler also performs incremental re-compilation). 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 37 Gupta [46] proposes a technique for debugging code transformed by trace scheduling compilers <ref> [43, 71] </ref>. During debugging, the user specifies commands for monitoring values and conditions at various points in the program. Code is then added to the program to collect the monitored values and the parts of the program containing the new monitor commands are incrementally recompiled.
Reference: [72] <author> G.Y. Lueh, T. Gross, and A. Adl-Tabatabai. </author> <title> Global Register Allocation Based on Graph Fusion. </title> <type> CMU-CS 96-106, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1996. </year>
Reference-contexts: Partial dead code elimination is repeated until no more changes are detected in the program. The code generation phase of the compiler performs global register allocation and local (i.e., intra-basic block) instruction scheduling. The global register allocator (described in detail in <ref> [72] </ref>) is a unique algorithm that integrates live range splitting with a Chaitin-style graph coloring register allocator [25]. The register allocation phase performs register coalescing and incorporates the improved coloring heuristics described by Briggs et al.[19]. <p> The profiles generated by the instrumented program are used by cmcc to annotate the basic blocks and control-flow edges in the IR with the execution frequencies. Profile information is currently being used to guide spilling, splitting, and register assignment decisions in the register allocator <ref> [72] </ref>. The overall quality of the optimized code generated by cmcc is competitive with the code generated by the native MIPS cc and gcc compilers on a DECStation 5000/200. Table 3.2 shows the relative performance of the code generated by cmcc compared to the code generated by these other compilers. <p> Shuffle code <ref> [72] </ref> is inserted to move L's data value when control passes from a segment L a to another segment L b . <p> Some approaches [17, 66] have implemented live range splitting within the graph simplification framework by splitting live ranges before graph coloring; register allocation then tries to minimize the runtime cost of shuffle code by eliminating unnecessary splits via coalescing (coalescing is discussed in the next section). Other approaches <ref> [23, 45, 79, 72] </ref> have devel 60 CHAPTER 4.
Reference: [73] <author> E. C. Lyle. </author> <title> Debugging VLIW Code After Instruction Scheduling. </title> <type> Master's thesis, </type> <institution> Oregon Graduate Institute, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: The debugger addresses both the code location and data-value problems by exposing them to the user; the user is left with the task of sorting out how optimizations have affected the state of source variables at a break. In his Master's thesis <ref> [73] </ref>, Lyle proposes to expose optimizations to the user by displaying a modified version of the source where operations have been re-ordered and eliminated to reflect the final ordering of operations after instruction scheduling.
Reference: [74] <author> S.A. Mahlke, W.Y. Chen, W.W. Hwu, B.R. Rau, </author> <title> and M.S. Schlansker. Sentinel Scheduling for VLIW and Superscalar Processors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 238247, </pages> <address> Boston,MA, </address> <month> October </month> <year> 1992. </year> <note> ACM. BIBLIOGRAPHY 179 </note>
Reference-contexts: Therefore, speculative code motion is usually guided by profile information. It is assumed that I will not cause a fault, or if it does, the fault is suppressed until the control dependencies of I are resolved <ref> [34, 89, 74] </ref>. As the amount of instruction-level parallelism offered by processors increases, the compiler must look harder for instructions that can be scheduled for parallel execution. Speculative instruction scheduling is becoming an increasingly important technique for extracting such parallelism from a program.
Reference: [75] <author> R. Milner, M. Tofte, and R. Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1990. </year>
Reference-contexts: No implementation of this proposal exists. Tolmach and Appel [93] present an approach to source-level debugging used in the Standard ML of New Jersey (SML-NJ) compiler [9], an optimizing compiler for Standard ML <ref> [75] </ref>. SML-NJ instruments the source program so that information is gathered at runtime in support of debugger queries. The instrumentation code is added to the abstract syntax tree representation of the source and is transformed along with the rest of the program by subsequent transformations.
Reference: [76] <author> E. Morel and C. </author> <title> Renvoise. Global Optimization by Suppression of Partial Redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2):96103, </volume> <month> Feb </month> <year> 1979. </year>
Reference-contexts: Hoisting of assignments causes endangerment by prematurely updating variable values, while sinking of assignments causes endangerment by delaying variable updates. Examples of code hoisting optimizations include partial redundancy elimination <ref> [76, 30, 39, 64] </ref>, 1 In fact in cmcc and other compilers, induction variable elimination is implemented as a simple variation on dead assignment elimination [30]. 90 CHAPTER 5. DETECTING ENDANGERED VARIABLES loop invariant code motion [7], and global instruction scheduling algorithms that perform non-speculative hoisting of instructions [14]. <p> Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available <ref> [76] </ref> or dead [65]. For instance, the Morel and Renvoise partial redundancy algorithm [76] and its variants [30, 39, 62] compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant. <p> Code motion and elimination are related, since some code motion algorithms operate by computing the set of program points where insertions of expressions render other expressions either available <ref> [76] </ref> or dead [65]. For instance, the Morel and Renvoise partial redundancy algorithm [76] and its variants [30, 39, 62] compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant. The net effect of this transformation is a hoisting of E. <p> Variable x is dead after expressions E 2 and E 3 . Dead code elimination has eliminated these assignment expressions, so that no code has been generated for E 2 or E 3 in Figure 5.7 (b). Partial redundancy elimination <ref> [76] </ref> has eliminated expression E 5 and has inserted instruction I 2 into block B4. The expression x=y+z is partially available [76] at B8 since this expression is available along paths that reach B8 from B7, but not along paths that reach B8 from B6. <p> Dead code elimination has eliminated these assignment expressions, so that no code has been generated for E 2 or E 3 in Figure 5.7 (b). Partial redundancy elimination <ref> [76] </ref> has eliminated expression E 5 and has inserted instruction I 2 into block B4. The expression x=y+z is partially available [76] at B8 since this expression is available along paths that reach B8 from B7, but not along paths that reach B8 from B6. In Figure 5.7 (b), a copy of the expression x=y+z has been inserted into block B4 (instruction I 2 ). <p> the source definition I 1 reaches a break at B3, so in addition to telling the user that x is suspect, the debugger can tell the user that the actual value of x is the value assigned by E 1 . 5.5.2 Terminology The Morel and Renvoise partial redundancy algorithm <ref> [76] </ref> and its variants [30, 39, 62] compute the set of program points where the insertion of an expression E will make another partially redundant instance of E fully redundant.
Reference: [77] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 6273, </pages> <address> Boston, </address> <month> October </month> <year> 1992. </year> <month> ACM/IEEE. </month>
Reference-contexts: Using software branch prediction [98, 11, 42], the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors [54, 82]. Transformations such as blocking [68] and prefetching <ref> [22, 77] </ref> can improve a program's data cache locality, thus reducing stalls due to cache misses [77]. 11 The importance of optimizations is not limited to low-level machine-specific transformations. The modern RISC approach to designing processors has also increased the effectiveness and importance of classical (machine-independent) compiler optimizations. <p> Using software branch prediction [98, 11, 42], the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors [54, 82]. Transformations such as blocking [68] and prefetching [22, 77] can improve a program's data cache locality, thus reducing stalls due to cache misses <ref> [77] </ref>. 11 The importance of optimizations is not limited to low-level machine-specific transformations. The modern RISC approach to designing processors has also increased the effectiveness and importance of classical (machine-independent) compiler optimizations.
Reference: [78] <author> Masood Namjoo and Anant Agrawal. </author> <title> Implementing SPARC: A High-Performance 32-Bit RISC Microprocessor. </title> <booktitle> SunTechnology, Winter, </booktitle> <year> 1988. </year>
Reference-contexts: To preserve maximum flexibility, the cmcc optimizer and code generator runs as a separate C++ program, reading in the internal representation emitted by the lcc front end. Table 3.1 lists the optimizations performed by cmcc. At this time, cmcc has been targeted to the MIPS [59], SPARC <ref> [78] </ref>, DLX [50], and iWarp [16] architectures. I obtained the results that I report in this dissertation using the code generator for the MIPS architecture. cmcc consists of two major phases: (1) machine-independent global optimization, and (2) code generation. Each of these phases in part comprises a series of transformations.
Reference: [79] <author> C. Norris and L. L. Pollock. </author> <title> Register Allocation over the Program Dependence Graph. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 266277. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Some approaches [17, 66] have implemented live range splitting within the graph simplification framework by splitting live ranges before graph coloring; register allocation then tries to minimize the runtime cost of shuffle code by eliminating unnecessary splits via coalescing (coalescing is discussed in the next section). Other approaches <ref> [23, 45, 79, 72] </ref> have devel 60 CHAPTER 4.
Reference: [80] <author> K. O'Brien, K.M. O'Brien, M. Hopkins, A. Shepherd, and R. Unrau. XIL and YIL: </author> <booktitle> The Intermediate Languages of TOBEY. In Proceedings of the ACM SIGPLAN Workshop on Intermediate Representations (IR'95), </booktitle> <pages> pages 7182, </pages> <address> San Francisco, </address> <month> Jan </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: The modern RISC approach to designing processors has also increased the effectiveness and importance of classical (machine-independent) compiler optimizations. The simple loadstore instruction sets of RISC processors not only provide a simple code selection target for the compiler, but also act as suitable compiler intermediate representations <ref> [57, 10, 80] </ref>. By modelling each RISC instruction in its intermediate representation, the compiler exposes most of the final instructions to classical optimizations such as partial redundancy elimination, strength reduction, and so on thus reducing path lengths in the object code.
Reference: [81] <author> J. Ousterhout. </author> <title> Tcl and the Tk Toolkit. </title> <address> AddisonWesley, </address> <year> 1994. </year>
Reference-contexts: In addition to the SPEC92 C programs, I have included three (publicly-available) C programs: (1) lcc, version 1.9 of the retargetable ANSI C compiler developed by Fraser and Hanson [44]; (2) tcl, version 7.3 of the the Tcl (tool command language) scripting language system developed by John Ousterhout <ref> [81] </ref>; and (3) triangle, version 1.1 of the two-dimensional quality mesh generator and delaunay triangulator developed by Jonathan Shewchuk at Carnegie Mellon University [87]. It is difficult to say what a typical program that is being debugged under a source 3.2.
Reference: [82] <author> K. Pettis and R.C. Hansen. </author> <title> Profile Guided Code Positioning. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1627. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Using software branch prediction [98, 11, 42], the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors <ref> [54, 82] </ref>. Transformations such as blocking [68] and prefetching [22, 77] can improve a program's data cache locality, thus reducing stalls due to cache misses [77]. 11 The importance of optimizations is not limited to low-level machine-specific transformations.
Reference: [83] <author> L.L. Pollock and M.L. Soffa. </author> <title> High-Level Debugging With the Aid of an Incremental Optimizer. </title> <booktitle> Proceedings of the Twenty-First Annual Hawaii International Conference on System Sciences, </booktitle> <year> 1988. </year>
Reference-contexts: These approaches sacrifice optimizations for debugging and emphasize interactive debugging over post-mortem debugging. These approaches use a combination of techniques: * The program points where the debugger can be invoked are limited and optimizations are constrained so that debugger functionality can be provided at these points <ref> [83, 52, 111, 46] </ref>. The compiler optimizes code between invocation points in a manner that allows the user to query program state when the debugger is invoked; for example, the compiler guarantees that all variables that are in scope at an invocation point are current. <p> Either new instrumentation code is inserted to implement the user request [46] or a function is deoptimized, allowing the user request to be carried out within that function <ref> [83, 52, 111] </ref>. In the following paragraphs, I first present each approach, and then discuss the limitations of the techniques. 36 CHAPTER 2. BACKGROUND Pollock and Soffa [83] propose an incremental compiler to disable optimizations that prohibit the debugger from satisfying a user request. <p> In the following paragraphs, I first present each approach, and then discuss the limitations of the techniques. 36 CHAPTER 2. BACKGROUND Pollock and Soffa <ref> [83] </ref> propose an incremental compiler to disable optimizations that prohibit the debugger from satisfying a user request. The debugger user specifies all requests before program execution and the debugger incrementally recompiles the program in a manner that allows the requests to be satisfied.
Reference: [84] <author> V. Santhanam and D. Odnert. </author> <title> Register Allocation Across Procedure and Module Boundaries. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 2839. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year> <note> 180 BIBLIOGRAPHY </note>
Reference-contexts: Moves: A move instruction transfers a live range's value from one register to another. 4.1.5 Global variables and aggregates Global variables and aggregates (i.e., array and structure elements) can also be register allocation candidates <ref> [100, 21, 84] </ref>. When included as register allocation candidates, globals and aggregates are usually assigned a home location in memory and are promoted to registers over a limited region of the program (e.g., those regions where they are not aliased and are frequently accessed [53]).
Reference: [85] <author> M. Satyanarayanan, J.J. Kistler, P. Kumar, </author> <title> M.E. Okasaki, E.H. Siegel, and D.C. Steere. Coda: A Highly Available File System for a Distributed Workstation Environment. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4), </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Because the ability to debug the production version of a software system is so important, the lack of support for debugging optimized code has discouraged some developers of large complex software systems from using optimizations. For instance, developers of the Coda file system <ref> [85] </ref> at Carnegie Mellon University do not enable optimizations during compilation, so that the system can be debugged in the field when necessary [90]. There are many instances, however, where debugging the unoptimized translation is unacceptable or debugging the optimized translation of a program is desirable: 13 1.
Reference: [86] <author> R. Seidner and N. Tindall. </author> <title> Interactive Debug Requirements. </title> <booktitle> In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on High-Level Debugging, </booktitle> <pages> pages 922. </pages> <publisher> ACM, </publisher> <year> 1983. </year>
Reference-contexts: Optimizations, however, make it difficult to provide source-level debugging functionality. Although researchers and implementors have long acknowledged the need for source-level debugging of optimized code <ref> [102, 49, 86] </ref>, compiler developers have generally avoided supporting source-level debugging of optimized code. Designing a source-level debugger for globally optimized code remains an open problem. Current trends in processor design are towards an increasing reliance on compiler transformations to achieve high performance.
Reference: [87] <author> J. R. Shewchuk. </author> <title> Triangle: Engineering a 2D Quality Mesh Generator and Delaunay Triangulator. </title> <booktitle> In First Workshop on Applied Computational Geometry. ACM, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: ANSI C compiler developed by Fraser and Hanson [44]; (2) tcl, version 7.3 of the the Tcl (tool command language) scripting language system developed by John Ousterhout [81]; and (3) triangle, version 1.1 of the two-dimensional quality mesh generator and delaunay triangulator developed by Jonathan Shewchuk at Carnegie Mellon University <ref> [87] </ref>. It is difficult to say what a typical program that is being debugged under a source 3.2.
Reference: [88] <author> M. Slater. </author> <title> AMD's K5 Designed to Outrun Pentium. </title> <type> Microprocessor Report, </type> <pages> pages 111, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Even newer implementations of CISC architectures are providing a RISC core set of instructions that execute efficiently and are simple to model in a compiler. For example, Intel's Pentium [24] and Pentium Pro [47], AMD's K5 <ref> [88] </ref> and NexGen's Nx686 [48] processors are superscalar implementations of the x86 architecture, which can concurrently dispatch only RISC-like instructions [56]; complex instructions are dispatched one at a time.
Reference: [89] <author> M. D. Smith, M. Horowitz, and M. S. Lam. </author> <title> Efficient Superscalar Performance Through Boosting. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 248259. </pages> <publisher> ACM, </publisher> <month> Oct </month> <year> 1992. </year>
Reference-contexts: Therefore, speculative code motion is usually guided by profile information. It is assumed that I will not cause a fault, or if it does, the fault is suppressed until the control dependencies of I are resolved <ref> [34, 89, 74] </ref>. As the amount of instruction-level parallelism offered by processors increases, the compiler must look harder for instructions that can be scheduled for parallel execution. Speculative instruction scheduling is becoming an increasingly important technique for extracting such parallelism from a program. <p> I do not deal with the case where an indirect store is speculatively hoisted; indirect stores may cause a fault and are usually never executed speculatively unless special hardware support is provided to buffer and squash the stored value <ref> [89] </ref>. In general, stores are not 128 CHAPTER 5. DETECTING ENDANGERED VARIABLES hoisted by instruction scheduling since they are at the end of a dependence chain; memory resources are better utilized by hoisting load instructions. hoisted instruction I from block B 3 to block B 0 .
Reference: [90] <author> D. Steere. </author> <type> Personal communication. </type> <year> 1995. </year>
Reference-contexts: For instance, developers of the Coda file system [85] at Carnegie Mellon University do not enable optimizations during compilation, so that the system can be debugged in the field when necessary <ref> [90] </ref>. There are many instances, however, where debugging the unoptimized translation is unacceptable or debugging the optimized translation of a program is desirable: 13 1. In a production environment, it is desirable to use optimizations, and to debug the optimized production version of a program.
Reference: [91] <editor> L.V. Streepy et al. </editor> <title> CXdb A New View On Optimization. </title> <booktitle> In Proc. Supercomputer Debugging Workshop '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year> <institution> Los Alamos National Laboratory. </institution>
Reference-contexts: a; S 1 : fpadd Rd,R1,R2 I 3 : store 0 (R4),R3 S 2 : fpmul Rb,Rc,Ra I 4 : fpmul R4,R5,R3 S 3 : store 0 (Rp),Ra I 5 : fpadd R6,R1,R2 (a) (b) (c) source-level debugging (a) Source code (b) Intermediate code (c) Object code assembly dump window <ref> [91, 92, 110] </ref> that is, a window that shows the instructions generated for the function being viewed at the source level. Optimizations make it very difficult for a user to debug a program at the assembly level.
Reference: [92] <author> R. </author> <title> Title. Thinking Machines Vendor Update. </title> <booktitle> In Proc. Supercomputer Debugging Workshop '92, </booktitle> <pages> pages 6382, </pages> <address> Dallas, TX, </address> <month> October </month> <year> 1992. </year> <institution> Los Alamos National Laboratory. </institution>
Reference-contexts: a; S 1 : fpadd Rd,R1,R2 I 3 : store 0 (R4),R3 S 2 : fpmul Rb,Rc,Ra I 4 : fpmul R4,R5,R3 S 3 : store 0 (Rp),Ra I 5 : fpadd R6,R1,R2 (a) (b) (c) source-level debugging (a) Source code (b) Intermediate code (c) Object code assembly dump window <ref> [91, 92, 110] </ref> that is, a window that shows the instructions generated for the function being viewed at the source level. Optimizations make it very difficult for a user to debug a program at the assembly level.
Reference: [93] <author> A. P. Tolmach and A. Appel. </author> <title> Debugging Standard ML Without Reverse Engineering. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 112. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Some approaches are invasive: the debugger requires modifications to the program or limitations on the transformations performed by the compiler. Source-level debugging can be made significantly easier if the compiler or debugger is allowed to insert additional code into the program before or after optimizations <ref> [93, 46] </ref>, or if compiler optimizations are constrained so that problems do not occur [111, 52]. But such modifications or limitations no longer qualify a program as optimized and defeat the goal of debugging optimized code. <p> Therefore, providing expected behavior non-invasively is generally not feasible when debugging the optimized translation of a program. Several approaches have been proposed that provide expected behavior at the expense of limiting the scope of optimizations <ref> [52, 111, 46, 93] </ref>. These approaches sacrifice optimizations for debugging and emphasize interactive debugging over post-mortem debugging. <p> Instrumentation code can be placed at a few program points specified by the user [46] or at all source statements <ref> [93] </ref>. Optimizations do not interfere with debugging because the instrumentation code is inserted before optimizations. * In response to a user request, the region of code affected by the request is incrementally recompiled by the debugger. <p> The units of re-compilation are program traces used by the compiler's trace scheduler; therefore, the compiler's trace scheduler must be integrated with the debugger. No implementation of this proposal exists. Tolmach and Appel <ref> [93] </ref> present an approach to source-level debugging used in the Standard ML of New Jersey (SML-NJ) compiler [9], an optimizing compiler for Standard ML [75]. SML-NJ instruments the source program so that information is gathered at runtime in support of debugger queries.
Reference: [94] <author> D. Ungar and R.B. Smith. </author> <title> SELF: The Power of Simplicity. </title> <booktitle> In OOPSLA '87 Conference Proceedings, </booktitle> <pages> pages 227241. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1987. </year> <note> BIBLIOGRAPHY 181 </note>
Reference-contexts: No implementation of this proposal exists. Holzle, Chambers and Ungar [52] describe the approach to debugging optimized code used in the SELF compiler system [27], an optimizing compiler for the object-oriented language SELF <ref> [94] </ref>. This approach restricts the program points at which the debugger can take control to discrete interrupt points. Optimizations are constrained so that the complete source-level state can be reconstructed at each interrupt point.
Reference: [95] <author> J. Uniejewski. </author> <title> SPEC Benchmark Suite: Designed for Today's Advanced Systems. </title> <journal> SPEC Newsletter, </journal> <volume> 1(1), </volume> <month> Fall </month> <year> 1989. </year>
Reference-contexts: The empirical evaluation presented in this dissertation is based on the set of eleven C programs. Eight of these programs are from SPEC92. The SPEC programs are a suite of benchmarks created for comparative measurements of vendor system performance <ref> [95] </ref>. These programs are supposed to represent a variety of typical system workloads and have been used widely to report speedups from optimizing various system components, including the compiler.
Reference: [96] <author> R. Wahbe. </author> <title> Efficient Data Breakpoints. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), pages 200212, </booktitle> <address> Boston, </address> <month> October </month> <year> 1992. </year> <month> ACM/IEEE. </month>
Reference-contexts: Data breakpoints specify the breakpoint condition in terms of source data objects (e.g., halt when a variable or memory location is referenced). Data breakpoints can be implemented in hardware, software, or a combination. The efficient implementation of data breakpoints is a topic of active research <ref> [61, 96, 97, 60, 37] </ref> and beyond the scope of this dissertation. A more general form of breakpoint, conditional breakpoints, suspends execution when a user specified predicate on the program's state becomes true.
Reference: [97] <author> R. Wahbe, S. Lucco, and S. Graham. </author> <title> Practical Data Breakpoints: </title> <booktitle> Design and Implementation. In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: Data breakpoints specify the breakpoint condition in terms of source data objects (e.g., halt when a variable or memory location is referenced). Data breakpoints can be implemented in hardware, software, or a combination. The efficient implementation of data breakpoints is a topic of active research <ref> [61, 96, 97, 60, 37] </ref> and beyond the scope of this dissertation. A more general form of breakpoint, conditional breakpoints, suspends execution when a user specified predicate on the program's state becomes true. <p> A related issue is whether modifications to the object code are done at compile time or at debug time. Code patching can be performed either by the compiler tool chain before the program is run (e.g., as performed in <ref> [97] </ref> to implement data breakpoints), or by the debugger when the program is being debugged (e.g., to implement control breakpoints [61]).
Reference: [98] <author> D. Wall. </author> <title> Predicting Program Behavior using Real or Estimated Profiles. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Compiler Construction, </booktitle> <pages> pages 5970. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Global register allocation [25, 32] can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers. Using software branch prediction <ref> [98, 11, 42] </ref>, the compiler can reorganize object code to increase the instruction fetch efficiency of pipelined or superscalar processors [54, 82].
Reference: [99] <author> D. Wall, A. Srivastava, and F. Templin. </author> <title> A Note on Hennessy's Symbolic Debugging of Optimized Code. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(1):176181, </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: Therefore, the algorithms described by Hennessy in [49] (and corrected by Wall et al. in <ref> [99] </ref>) deal with noncurrency due to local dead code elimination, and re-ordering introduced by local common subexpressions that are assigned to variables; the algorithms do not consider effects of code generation optimizations such as instruction scheduling and register allocation.
Reference: [100] <author> D. W. Wall. </author> <title> Global Register Allocation at Link Time. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <pages> pages 264275, </pages> <address> Palo Alto, </address> <month> June </month> <year> 1986. </year> <note> ACM. </note>
Reference-contexts: Moves: A move instruction transfers a live range's value from one register to another. 4.1.5 Global variables and aggregates Global variables and aggregates (i.e., array and structure elements) can also be register allocation candidates <ref> [100, 21, 84] </ref>. When included as register allocation candidates, globals and aggregates are usually assigned a home location in memory and are promoted to registers over a limited region of the program (e.g., those regions where they are not aliased and are frequently accessed [53]).
Reference: [101] <author> H. Warren. </author> <title> Instruction Scheduling for the IBM RISC System/6000 Processor. </title> <journal> IBM J. Research and Development, </journal> <volume> 34(1):8592, </volume> <month> Jan </month> <year> 1990. </year>
Reference-contexts: Such resources include instruction-level parallelism, large register files, instruction fetching mechanisms, and memory hierarchies. The compiler speeds up program execution by performing transformations that exploit the resources exposed by a target processor. For example, instruction scheduling <ref> [43, 51, 67, 14, 101] </ref> can increase the efficiency of processors with instruction-level parallelism by statically scheduling independent operations for concurrent execution. Global register allocation [25, 32] can effectively exploit architectures that have large registers files and high memory access latencies by keeping the most frequently accessed values in registers.
Reference: [102] <author> H.S. Warren, Jr., </author> <title> and H.P. Schlaeppi. Design of the FDS Interactive Debugging System. </title> <institution> IBM Research Report RC7214, IBM Yorktown Heights, Yorktown Heights, N. Y., </institution> <month> July </month> <year> 1978. </year>
Reference-contexts: Optimizations, however, make it difficult to provide source-level debugging functionality. Although researchers and implementors have long acknowledged the need for source-level debugging of optimized code <ref> [102, 49, 86] </ref>, compiler developers have generally avoided supporting source-level debugging of optimized code. Designing a source-level debugger for globally optimized code remains an open problem. Current trends in processor design are towards an increasing reliance on compiler transformations to achieve high performance.
Reference: [103] <author> M. Weiser. </author> <title> Programmers Use Slices When Debugging. </title> <journal> Communications of the ACM, </journal> <volume> 25(7), </volume> <month> July </month> <year> 1982. </year>
Reference-contexts: This is similar to program slicing and can be accomplished using reaching definitions analysis to determine the set of assignments that may have affected a variable's value <ref> [103, 104] </ref>. The reaching definitions analysis depends only on the stopping instruction, and in contrast to noncurrency determination, does not require the existence of a control reference statement. The reaching assignments can be communicated to the user by some method of marking up the source (e.g., highlighting). <p> Note, that this recovery scheme can be extended to recover values from more than one instruction by computing an executable backward slice <ref> [103, 104] </ref>. the loaded value and one for the updated address). The definition of DestReg can be easily extended to accommodate a set of registers without major changes to the rest of our discussion. <p> This information can be gathered by computing the reaching definitions of V in the object file or by performing a backward slice for V <ref> [103, 104] </ref>. The static measurements presented in this dissertation provide useful insight into the effects of different optimizations on debugging, but these measurements do not indicate how likely it is for a debugger user to encounter unexpected behavior because of optimizations.
Reference: [104] <author> M. Weiser. </author> <title> Program Slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352357, </volume> <month> July </month> <year> 1984. </year> <note> 182 BIBLIOGRAPHY </note>
Reference-contexts: This is similar to program slicing and can be accomplished using reaching definitions analysis to determine the set of assignments that may have affected a variable's value <ref> [103, 104] </ref>. The reaching definitions analysis depends only on the stopping instruction, and in contrast to noncurrency determination, does not require the existence of a control reference statement. The reaching assignments can be communicated to the user by some method of marking up the source (e.g., highlighting). <p> Note, that this recovery scheme can be extended to recover values from more than one instruction by computing an executable backward slice <ref> [103, 104] </ref>. the loaded value and one for the updated address). The definition of DestReg can be easily extended to accommodate a set of registers without major changes to the rest of our discussion. <p> This information can be gathered by computing the reaching definitions of V in the object file or by performing a backward slice for V <ref> [103, 104] </ref>. The static measurements presented in this dissertation provide useful insight into the effects of different optimizations on debugging, but these measurements do not indicate how likely it is for a debugger user to encounter unexpected behavior because of optimizations.
Reference: [105] <author> R. Wilon, R. French, C. Wilson, S. Amarasinghe, J. Anderson, S. Tjiang, S.W. Liao, C.-W. Tseng, M. Hall, M. Lam, and J. Hennessy. </author> <title> SUIF: An Infrastructure for Research on Parallelizing and Optimizing Compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: Unfortunately, sources to a high quality, easily extensible, optimizing compiler are hard to come by: gcc is notoriously difficult to modify, lcc is not an optimizing compiler, and SUIF <ref> [105] </ref> was not available at the beginning of this project. Therefore, I have developed cmcc, a retargetable optimizing C compiler that generates code that is competitive with code generated by native optimizing compilers. <p> Examples of annotations include comments, pointers to source-level statements, or flags marking that a node or instruction was inserted by a particular optimization phase. The idea of providing a flexible annotation mechanism is not new and has been used in other compilers (e.g., the SUIF compiler <ref> [105] </ref>). But annotations are crucial for debugging optimized code because they allow the compiler to keep a trail of how optimizations have transformed a program.
Reference: [106] <author> R. Wismueller. </author> <title> Debugging of Globally Optimized Programs Using Data Flow Analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 278289. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: be to extend the functionality of a traditional debugger so that it can provide interactions that will guide the user in debugging optimized code. 2.2.2 Values in optimized code Most research on debugging optimized code concentrates on the data-value problem and much terminology exists for this aspect of the problem <ref> [49, 2, 1, 36, 106] </ref>. In this section, I review this terminology. At a break, the debugger conveys the state of source variables by allowing the user to query variable values. <p> For example, the debugger can tell the user at which source assignment (s) an endangered variable's actual value was (or may have been) assigned [3]. Several researchers have concentrated on the problems of detecting nonresident variables [2] and endangered variables <ref> [49, 1, 36, 106, 4] </ref>. In the following paragraphs, I summarize each of these prior works. Hennessy [49] introduced the concept of endangered and noncurrent variables, and 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 41 presented the first algorithms to detect endangered variables. <p> BACKGROUND instruction, by detecting store instructions that were moved across statement boundaries by the code scheduler. This information is passed on to the debugger. Copperman [35, 36] and Wismueller <ref> [107, 106] </ref> have investigated the data-value problem of detecting endangered variables caused by global optimizations. Their efforts concentrate only on detecting if a variable is endangered; they do not consider code location problems in any depth. <p> Copperman does not consider language issues such as undefined evaluation order. Moreover, Copperman's approach does not deal with asynchronous breaks and does not consider recovery. Copperman's techniques have not been implemented; therefore, it is difficult to evaluate the practicality of his techniques. Wismueller <ref> [107, 106] </ref> provides a formal framework for detecting current variables. His algorithms concentrate only on detecting whether the expected value of a variable can be displayed to the user; he does not distinguish between nonresident, suspect, and noncurrent variables. <p> Or, if an assignment is eliminated because of backward redundancy, the value must be available somewhere, and the debugger can provide this value to the user. Both Copperman [36] and Wismueller <ref> [106] </ref> assume arbitrary code movement and elimination, and fail to recognize that movement and elimination are not unconstrained. Another major difference between this dissertation, and the earlier work by Copperman and Wismueller is that they attempt to capture a summary effect of all optimizations in auxiliary intermediate representations. <p> Moreover, the algorithms work on the final object representation of a program; this is in contrast to other approaches that keep around a copy of the original source program representation (e.g., <ref> [106] </ref>) or work on an auxiliary intermediate representation (e.g., [36]). 5.5.1 Example I illustrate with an example the data-value problems caused by global optimizations and how these problems can be managed by the debugger. five expressions E 1 :::E 5 that assign to a source-level variable x and exactly two uses <p> These annotations and markers are ignored by optimizations and optimizations are not constrained in any way. This method of tracking the correspondence between the optimized code and the source code is in contrast to the approach taken by Wismeuller <ref> [106] </ref> and Copperman [36]. In those approaches, a representation of the original source program is kept as a copy, and links are maintained between the intermediate representation used for optimizations and the original representation (e.g., an abstract syntax tree). <p> The analyses are very similar to other analyses that are done by the compiler and can thus take advantage of an infrastructure that is already present. This is in contrast to other approaches that require specialized data-flow analyses and program representations <ref> [36, 106] </ref>. To gather the information required for the data-flow analyses, the program intermediate representation is annotated during optimizations to mark hoisted and sunk assignments, and additional markers are inserted to indicate points from which source-level assignments are eliminated. <p> S 1 : S 1 : do f do f S 3 : ...*p... S 3 : ...tmp... S 5 : g while (E) S 5 : g while (E) Source program After code hoisting Some researchers <ref> [109, 35, 106] </ref> have suggested that an alternative strategy exists for setting breakpoints at statements from which code has been hoisted.
Reference: [107] <author> R. Wismueller. </author> <title> Quellsprachorientiertes Debugging von optimierten Programmen. </title> <type> PhD thesis, </type> <institution> Technische Universitaet Muenchen, Munich, Germany, </institution> <month> Dec. </month> <year> 1994. </year> <note> (in German). Published (1995) by Shaker Verlag, Aachen (Germany), ISBN 3-8265-0841-6. </note>
Reference-contexts: BACKGROUND instruction, by detecting store instructions that were moved across statement boundaries by the code scheduler. This information is passed on to the debugger. Copperman [35, 36] and Wismueller <ref> [107, 106] </ref> have investigated the data-value problem of detecting endangered variables caused by global optimizations. Their efforts concentrate only on detecting if a variable is endangered; they do not consider code location problems in any depth. <p> Copperman does not consider language issues such as undefined evaluation order. Moreover, Copperman's approach does not deal with asynchronous breaks and does not consider recovery. Copperman's techniques have not been implemented; therefore, it is difficult to evaluate the practicality of his techniques. Wismueller <ref> [107, 106] </ref> provides a formal framework for detecting current variables. His algorithms concentrate only on detecting whether the expected value of a variable can be displayed to the user; he does not distinguish between nonresident, suspect, and noncurrent variables. <p> By taking advantage of these code motion invariants, the algorithms for detecting endangered variables are greatly simplified. In fact, it is these invariants that have allowed me to produce a solution to the problem that is significantly simpler than the approaches described by Wismueller <ref> [107] </ref> and Copperman [36]. Not all compiler optimizations satisfy these safety constraints. Global instruction scheduling algorithms that schedule instructions for speculative execution [43, 14, 28], can hoist an instruction I from a block B to another block that is not post-dominated by B.
Reference: [108] <author> P. Zellweger. </author> <title> An Interactive High-Level Debugger for Control-Flow Optimized Programs. </title> <booktitle> In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on High-Level Debugging, </booktitle> <pages> pages 159171. </pages> <publisher> ACM, </publisher> <year> 1983. </year>
Reference-contexts: Requiring instrumentation code does not completely address the problem since the user is still faced with the choice between a fully optimized translation of a program and a debuggable but non-optimal translation. 2.4. ALTERNATIVE APPROACHES TO DEBUGGING OPTIMIZED CODE 33 * I preclude the insertion of hidden breakpoints <ref> [108] </ref> by the debugger; hidden break-points are inserted into the program to collect runtime information at key execution points in the program. <p> That is, we would like the debugger to present expected behavior <ref> [108] </ref> to the user: the debugger provides the same behavior when debugging the optimized version of a program as when debugging the unoptimized version. To provide expected behavior without constraining optimizations or debugger functionality, the debugger must detect all nonresident and endangered variables, and recover their expected values. <p> If a variable is endangered, the debugger displays the actual value of the variable with a message qualifying the variable as endangered. Zellweger <ref> [108] </ref> refers to debuggers that detect endangered variables as exhibiting truthful behavior. The debugger can provide additional guidance by conveying how optimizations have affected source values. <p> Detecting and reporting uninitialized variables reduces the number of variables that are reported as nonresident or endangered, and provides additional information to the user [2]. In the absence of support provided by the runtime system (e.g., path determiners <ref> [108] </ref>) or the architecture (e.g., memory tags), detecting uninitialized variables requires that the debugger perform program flow analysis on the source program. If no definition of a user variable V reaches a point S in the source, then V is uninitialized whenever the program breaks at S.
Reference: [109] <author> P. Zellweger. </author> <title> Interactive Source-Level Debugging of Optimized Programs. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <month> May </month> <year> 1984. </year> <note> Published as Xerox PARC Technical Report CSL-84-5. </note>
Reference-contexts: Moreover, providing source-level control breakpoints becomes difficult as mapping an execution point in the source program to an instruction in the object code becomes non-trivial. Similarly, mapping an instruction to the source code, in the case of an exception, becomes difficult. These problems are called code location problems <ref> [109] </ref>. 24 CHAPTER 2. <p> These problems are called data-value problems <ref> [109] </ref>. In general, optimizations make it impossible for the debugger to provide the illusion that the source program is executing one source statement at a time, and that all in scope variables are available for inspection and modification. <p> Note that the compiler can instrument the object code or the debugger can install hidden breakpoints to collect runtime information (e.g., path determiners <ref> [109] </ref>), allowing the debugger to determine which path is taken to B8 and thus report x as either noncurrent or current. This instrumentation is disallowed, however, in our non-invasive debugger model. 3 Of course block B4 will be translated to some program point in the source. <p> S 1 : S 1 : do f do f S 3 : ...*p... S 3 : ...tmp... S 5 : g while (E) S 5 : g while (E) Source program After code hoisting Some researchers <ref> [109, 35, 106] </ref> have suggested that an alternative strategy exists for setting breakpoints at statements from which code has been hoisted. <p> An alternative strategy for setting a breakpoint at statement S is to set a breakpoint at the earliest instruction generated for S (Zellweger <ref> [109] </ref> calls this a semantic breakpoint mapping). This strategy guarantees that the breakpoint for S is taken before any exceptions (or other side effects) 154 CHAPTER 6. BREAKPOINTS IN OPTIMIZED CODE inside S.
Reference: [110] <author> S. Zimmerman. UDB: </author> <title> A Parallel Debugger for the KSR1. </title> <booktitle> In Proc. Supercomputer Debugging Workshop '92, </booktitle> <pages> pages 95102, </pages> <address> Dallas, TX, </address> <month> October </month> <year> 1992. </year> <institution> Los Alamos National Laboratory. </institution>
Reference-contexts: a; S 1 : fpadd Rd,R1,R2 I 3 : store 0 (R4),R3 S 2 : fpmul Rb,Rc,Ra I 4 : fpmul R4,R5,R3 S 3 : store 0 (Rp),Ra I 5 : fpadd R6,R1,R2 (a) (b) (c) source-level debugging (a) Source code (b) Intermediate code (c) Object code assembly dump window <ref> [91, 92, 110] </ref> that is, a window that shows the instructions generated for the function being viewed at the source level. Optimizations make it very difficult for a user to debug a program at the assembly level.
Reference: [111] <author> L. Zurawski and R. Johnson. </author> <title> Debugging Optimized Code With Expected Behavior. </title> <institution> Unpublished draft from Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Source-level debugging can be made significantly easier if the compiler or debugger is allowed to insert additional code into the program before or after optimizations [93, 46], or if compiler optimizations are constrained so that problems do not occur <ref> [111, 52] </ref>. But such modifications or limitations no longer qualify a program as optimized and defeat the goal of debugging optimized code. <p> Therefore, providing expected behavior non-invasively is generally not feasible when debugging the optimized translation of a program. Several approaches have been proposed that provide expected behavior at the expense of limiting the scope of optimizations <ref> [52, 111, 46, 93] </ref>. These approaches sacrifice optimizations for debugging and emphasize interactive debugging over post-mortem debugging. <p> These approaches sacrifice optimizations for debugging and emphasize interactive debugging over post-mortem debugging. These approaches use a combination of techniques: * The program points where the debugger can be invoked are limited and optimizations are constrained so that debugger functionality can be provided at these points <ref> [83, 52, 111, 46] </ref>. The compiler optimizes code between invocation points in a manner that allows the user to query program state when the debugger is invoked; for example, the compiler guarantees that all variables that are in scope at an invocation point are current. <p> Either new instrumentation code is inserted to implement the user request [46] or a function is deoptimized, allowing the user request to be carried out within that function <ref> [83, 52, 111] </ref>. In the following paragraphs, I first present each approach, and then discuss the limitations of the techniques. 36 CHAPTER 2. BACKGROUND Pollock and Soffa [83] propose an incremental compiler to disable optimizations that prohibit the debugger from satisfying a user request. <p> Once a function is deoptimized the debugger may be invoked at any source point within that function. The SELF system generates code incrementally for each function at runtime, thus facilitating the use of dynamic deoptimization. Zurawski and Johnson <ref> [111] </ref> describe a similar approach used in the TS compiler [58], an optimizing compiler for Typed Smalltalk. Like the SELF system, the debugger can be invoked only at pre-determined points that in the TS system are called inspection points.
References-found: 111

