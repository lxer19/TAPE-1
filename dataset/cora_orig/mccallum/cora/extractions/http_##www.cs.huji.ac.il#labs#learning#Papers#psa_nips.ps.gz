URL: http://www.cs.huji.ac.il/labs/learning/Papers/psa_nips.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Title: The Power of Amnesia  
Author: Dana Ron Yoram Singer Naftali Tishby 
Keyword: HMM based methods.  
Address: Jerusalem 91904, Israel  
Affiliation: Institute of Computer Science and Center for Neural Computation Hebrew University,  
Abstract: We propose a learning algorithm for a variable memory length Markov process. Human communication, whether given as text, handwriting, or speech, has multi characteristic time scales. On short scales it is characterized mostly by the dynamics that generate the process, whereas on large scales, more syntactic and semantic information is carried. For that reason the conventionally used fixed memory Markov models cannot capture effectively the complexity of such structures. On the other hand using long memory models uniformly is not practical even for as short memory as four. The algorithm we propose is based on minimizing the statistical prediction error by extending the memory, or state length, adaptively, until the total prediction error is sufficiently small. We demonstrate the algorithm by learning the structure of natural English text and applying the learned model to the correction of corrupted text. Using less than 3000 states the model's performance is far superior to that of fixed memory models with similar number of states. We also show how the algorithm can be applied to intergenic E. coli DNA base prediction with results comparable to 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.G Kemeny and J.L. Snell, </author> <title> Finite Markov Chains, </title> <publisher> Springer-Verlag 1982. </publisher>
Reference-contexts: The nodes of the tree are labeled by pairs (s; fl s ) where s is the string associated with the walk starting from that node and ending in the root of the tree, and fl s : ! <ref> [0; 1] </ref> is the output probability function related with s satisfying P 2 fl s () = 1. A prediction suffix tree induces probabilities on arbitrary long strings in the following manner. <p> Finite State Automata and Markov Processes A Probabilistic Finite Automaton (PFA) A is a 5-tuple (Q; ; t; fl; ), where Q is a finite set of n states, is an alphabet of size k, t : Q fi ! Q is the transition function, fl : Q fi ! <ref> [0; 1] </ref> is the output probability function, and : Q ! [0; 1] is the probability distribution over the starting states. The functions fl and must satisfy the following requirements: for every q 2 Q, P q2Q (q) = 1. <p> A is a 5-tuple (Q; ; t; fl; ), where Q is a finite set of n states, is an alphabet of size k, t : Q fi ! Q is the transition function, fl : Q fi ! <ref> [0; 1] </ref> is the output probability function, and : Q ! [0; 1] is the probability distribution over the starting states. The functions fl and must satisfy the following requirements: for every q 2 Q, P q2Q (q) = 1.
Reference: [2] <author> Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie, </author> <title> Efficient Learning of Typical Finite Automata from Random Walks, </title> <publisher> STOC-93. </publisher>
Reference: [3] <author> F. Jelinek, </author> <title> Self-Organized Language Modeling for Speech Recognition, </title> <year> 1985. </year>
Reference-contexts: The obvious desired solution is a Markov source with a `deep' memory just where it is really needed. Variable memory length Markov models have been in use for language modeling in speech recognition for some time <ref> [3, 4] </ref>, yet no systematic derivation, nor rigorous analysis of such learning mechanism has been proposed. Markov models are a natural candidate for language modeling and temporal pattern recognition, mostly due to their mathematical simplicity.
Reference: [4] <author> A. Nadas, </author> <title> Estimation of Probabilities in the Language Model of the IBM Speech Recognition System, </title> <journal> IEEE Trans. on ASSP Vol. </journal> <volume> 32 No. 4, </volume> <pages> pp. 859-861, </pages> <year> 1984. </year>
Reference-contexts: The obvious desired solution is a Markov source with a `deep' memory just where it is really needed. Variable memory length Markov models have been in use for language modeling in speech recognition for some time <ref> [3, 4] </ref>, yet no systematic derivation, nor rigorous analysis of such learning mechanism has been proposed. Markov models are a natural candidate for language modeling and temporal pattern recognition, mostly due to their mathematical simplicity.
Reference: [5] <author> S. Kullback, </author> <title> Information Theory and Statistics, </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1959. </year>
Reference-contexts: A node s, must be added to the tree if it statistically differs from its parent node s. A natural measure to check the statistical difference is the relative entropy (also known as the Kullback--Liebler (KL) divergence) <ref> [5] </ref>, between the conditional probabilities P (js) and P (js).
Reference: [6] <author> J. Rissanen and G. G. Langdon, </author> <title> Universal modeling and coding, </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> IT-27 (3), </volume> <pages> pp. 12-23, </pages> <year> 1981. </year>
Reference-contexts: It is nevertheless obvious that finite memory Markov models can not in any way capture the recursive nature of the language, nor can they be trained effectively with long enough memory. The notion of a variable length memory seems to appear naturally also in the context of universal coding <ref> [6] </ref>. This information theoretic notion is now known to be closely related to efficient modeling [7]. The natural measure that appears in information theory is the description length, as measured by the statistical predictability via the Kullback- Liebler (KL) divergence.
Reference: [7] <author> J. Rissanen, </author> <title> Stochastic complexity and modeling, </title> <journal> The Ann. of Stat., </journal> <volume> 14(3), </volume> <year> 1986. </year>
Reference-contexts: The notion of a variable length memory seems to appear naturally also in the context of universal coding [6]. This information theoretic notion is now known to be closely related to efficient modeling <ref> [7] </ref>. The natural measure that appears in information theory is the description length, as measured by the statistical predictability via the Kullback- Liebler (KL) divergence.
Reference: [8] <author> A. Krogh, S.I. Mian, and D. Haussler, </author> <title> A Hidden Markov Model that finds genes in E. coli DNA, </title> <type> UCSC Tech. Rep. </type> <institution> UCSC-CRL-93-16. </institution>
Reference-contexts: The alphabet is: A,C,T,G. The result of the algorithm is an automaton having 80 states. The names of the states of the final automaton are depicted in Fig. 5. The performance of the model can be compared to other models, such as the HMM based model <ref> [8] </ref>, by calculating the normalized log-likelihood (NLL) over unseen data. The NLL is an empirical measure of the the entropy of the source as induced by the model. The NLL of bounded memory Markov model is about the same as the one obtained by the HMM based model.
References-found: 8

