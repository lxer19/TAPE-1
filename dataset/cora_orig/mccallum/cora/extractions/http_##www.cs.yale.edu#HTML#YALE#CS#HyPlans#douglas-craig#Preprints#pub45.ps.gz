URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/Preprints/pub45.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/ccd-preprints.html
Root-URL: http://www.cs.yale.edu
Title: A RIGOROUS ANALYSIS OF TIME DOMAIN PARALLELISM  
Author: A. DESHPANDE S. MALHOTRA C. C. DOUGLAS AND M. H. SCHULTZ 
Keyword: Key words. iterative methods, time domain parallelism, partial differential equations. AMS(MOS) subject classifications. G.1.0. Parallel Algorithms [Numerical Analysis]; G.1.3. Linear Systems (Direct and Iterative methods); G.1.8. Parabolic Equations (Partial Differential Equations); D.1.3. Parallel Programming.  
Abstract: Time dependent partial differential equations are often solved using algorithms which parallelize the solution process in the spatial domain. However, as the number of processors increases, the parallel efficiency is limited by the increasing communication/computation ratio. Recently, several researchers have proposed algorithms incorporating time domain parallelism in order to increase efficiency. In this paper we discuss a class of such algorithms and analyze it rigrously. 1. Introduction. We investigate a parallel algorithm for the numerical solution 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Deshpande, S. Malhotra, C. C. Douglas, and M. H. Schultz, </author> <title> Temporal domain parallelism: Does it work?, </title> <type> Tech. Rep. </type> <institution> YALEU/DCS/TR-996, Department of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <year> 1993. </year>
Reference-contexts: Proof. We prove the above statement for the backward Euler method. A proof for the Crank-Nicolson case can be found in <ref> [1] </ref>. <p> The total wall clock times, assuming no communication overhead, satisfy t BE 2 2 ln * t BE p 1 p * t CN Proof. See <ref> [1] </ref>. <p> The number of iterations required to converge to the desired solution increases linearly with the number of timesteps in parallel as predicted by the theory. This negates any potential benefits from paral-lelization. 4. Point Iterative Methods. In this section we state some results (see <ref> [1] </ref> for proofs) for point Jacobi and Gauss-Seidel methods for solving the conventional 5 Fig. 1.
Reference: [2] <author> D. E. Keyes, </author> <year> 1994. </year> <title> Private communication. </title>
Reference-contexts: We note at this stage that the parallel approach has been successfully used to solve certain nonlinear problems <ref> [2] </ref> and time domain parallelism may indeed be a viable and useful approach in those application areas. 2. The Temporal Method. Consider an iterative scheme for time domain parallelism by solving the linear systems at different time steps simultaneously. <p> We have shown that the increase in the number of iterations required for the convergence of block and point Jacobi and Gauss-Seidel methods negates any advantages of time domain parallelism. As noted by others <ref> [2, 4, 7, 8, 9, 10] </ref> these methods can be useful for some classes of problems. Hence, care must be employed when using them.
Reference: [3] <author> K. Miller, </author> <title> Numerical analogs to the Schwarz alternating procedure, </title> <journal> Numer. Math., </journal> <volume> 7 (1965), </volume> <pages> pp. 91-103. </pages>
Reference-contexts: Usually, the entire process is spatially parallelized by splitting the domain into subdomains and distributing problems on the subdomains to multiple processors (see <ref> [3] </ref> and [5]). At each iteration, the processors need to exchange boundary information with processors holding adjacent subdomains. As the number of processors increases, the communication/computation ratio increases making the parallel efficiency decrease.
Reference: [4] <author> J. H. Saltz, </author> <title> Parallel and Adaptive Algorithms for Problems in Scientific and Medical Computing, </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Duke University, Durham, NC, </institution> <year> 1985. </year>
Reference-contexts: J. Watson Research Center, Yorktown Heights, NY 10598-0218. 1 be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism <ref> [4, 7, 8, 9, 10] </ref>. In this paper, we investigate a time domain parallel algorithm for solving (1). Other authors have considered a similar algorithm ([4], [9] and [10]), but not from a rigorous theoretical point of view. <p> We have shown that the increase in the number of iterations required for the convergence of block and point Jacobi and Gauss-Seidel methods negates any advantages of time domain parallelism. As noted by others <ref> [2, 4, 7, 8, 9, 10] </ref> these methods can be useful for some classes of problems. Hence, care must be employed when using them.
Reference: [5] <author> H. A. Schwarz, </author> <title> Uber einige abbildungsaufgaben, </title> <journal> Ges. Math. Abh., </journal> <volume> 11 (1869), </volume> <pages> pp. 65-83. </pages>
Reference-contexts: Usually, the entire process is spatially parallelized by splitting the domain into subdomains and distributing problems on the subdomains to multiple processors (see [3] and <ref> [5] </ref>). At each iteration, the processors need to exchange boundary information with processors holding adjacent subdomains. As the number of processors increases, the communication/computation ratio increases making the parallel efficiency decrease.
Reference: [6] <author> R. S. Varga, </author> <title> Matrix Iterative Analysis, </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1962. </year>
Reference-contexts: While the preceding result is true in general (see <ref> [6] </ref>), this argument holds only if the matrix T has m linearly independent eigenvectors. If T is defective, i.e., lacking eigenvectors, then we may not be able to express e 0 as a linear combination of the eigenvectors and (3) may not hold.
Reference: [7] <author> D. Womble, </author> <title> A time stepping algorithm for parallel computers, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11 (1990), </volume> <pages> pp. 824-837. </pages>
Reference-contexts: J. Watson Research Center, Yorktown Heights, NY 10598-0218. 1 be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism <ref> [4, 7, 8, 9, 10] </ref>. In this paper, we investigate a time domain parallel algorithm for solving (1). Other authors have considered a similar algorithm ([4], [9] and [10]), but not from a rigorous theoretical point of view. <p> We have shown that the increase in the number of iterations required for the convergence of block and point Jacobi and Gauss-Seidel methods negates any advantages of time domain parallelism. As noted by others <ref> [2, 4, 7, 8, 9, 10] </ref> these methods can be useful for some classes of problems. Hence, care must be employed when using them.
Reference: [8] <author> P. Worley, </author> <title> Parallelizing across time when solving time-dependent partial differential equations, </title> <booktitle> in Proc. 5th SIAM Conf. on Parallel Processing for Scientific Computing, </booktitle> <editor> D. Sorensen, ed., </editor> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: J. Watson Research Center, Yorktown Heights, NY 10598-0218. 1 be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism <ref> [4, 7, 8, 9, 10] </ref>. In this paper, we investigate a time domain parallel algorithm for solving (1). Other authors have considered a similar algorithm ([4], [9] and [10]), but not from a rigorous theoretical point of view. <p> We have shown that the increase in the number of iterations required for the convergence of block and point Jacobi and Gauss-Seidel methods negates any advantages of time domain parallelism. As noted by others <ref> [2, 4, 7, 8, 9, 10] </ref> these methods can be useful for some classes of problems. Hence, care must be employed when using them.
Reference: [9] <author> J. Zhu, </author> <title> A new parallel algorithm for the numerical solutions of time dependent partial differential equations, </title> <type> tech. rep., </type> <institution> Mississippi State University, Mississippi State, MS, </institution> <year> 1991. </year> <title> [10] , Solving Partial Differential Equations on Parallel Computers, </title> <publisher> World Scientific Publishing, </publisher> <address> Singapore, </address> <year> 1994. </year> <title> 7 Fig. 3. Efficiencies for m = 1000 Fig. 4. Efficiency for m = 60 and m = 1000 8 </title>
Reference-contexts: J. Watson Research Center, Yorktown Heights, NY 10598-0218. 1 be used effectively, a number of researchers have suggested algorithms which introduce time domain parallelism as well as space domain parallelism <ref> [4, 7, 8, 9, 10] </ref>. In this paper, we investigate a time domain parallel algorithm for solving (1). Other authors have considered a similar algorithm ([4], [9] and [10]), but not from a rigorous theoretical point of view. <p> In this paper, we investigate a time domain parallel algorithm for solving (1). Other authors have considered a similar algorithm ([4], <ref> [9] </ref> and [10]), but not from a rigorous theoretical point of view. We demonstrate that the parallel and serial algorithms do not converge in the same number of iterations. <p> As was shown in <ref> [9] </ref>, T G is a block lower triangular matrix with diagonal blocks equal to (D L) 1 U . Hence, the eigenvalues of T G are the collections of all eigenvalues of all the block matrices on the main diagonal. <p> This qualitative behavior is largely independent of m as can be seen from the graph in Figure 2, in which we show the efficiency of the parallel scheme for different values of m. Clearly, the efficiency does not stay constant with n as claimed in <ref> [9] </ref>. Now consider the point Gauss-Seidel scheme. Unfortunately, we are unable to derive a lower bound for the 2-norm of k th power of the Gauss-Seidel iteration matrix. However, we present numerical results to demonstrate that the efficiency of the parallel 6 Fig. 2. <p> However, we present numerical results to demonstrate that the efficiency of the parallel 6 Fig. 2. Efficiency for m = 60 and m = 1000 scheme goes down with increasing the number of parallel time steps contrary to the claims made in <ref> [9] </ref> that it remains constant. In Figure 3 we show the efficiency of the parallel scheme for two different values of r. As the graph shows, the efficiency decreases with increasing n. In Figure 4, we show the behavior of the parallel scheme for two different values of m. <p> We have shown that the increase in the number of iterations required for the convergence of block and point Jacobi and Gauss-Seidel methods negates any advantages of time domain parallelism. As noted by others <ref> [2, 4, 7, 8, 9, 10] </ref> these methods can be useful for some classes of problems. Hence, care must be employed when using them.
References-found: 9

