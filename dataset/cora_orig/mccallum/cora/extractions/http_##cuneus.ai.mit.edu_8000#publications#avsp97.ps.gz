URL: http://cuneus.ai.mit.edu:8000/publications/avsp97.ps.gz
Refering-URL: http://cuneus.ai.mit.edu:8000/personal/publications.html
Root-URL: 
Email: tonebone@ai.mit.edu tp-temp@ai.mit.edu  
Title: VIDEOREALISTIC TALKING FACES: A MORPHING APPROACH  
Author: Tony Ezzat Tomaso Poggio 
Affiliation: MIT Center for Biological and Computational Learning  
Date: Sept. 26-27, 1997 ESCA ISSN 1018 4554 1  
Address: Rhodes, Greece,  45 Carleton Street E25-201 Cambridge, MA 02141  
Note: Appeared in Proceedings of the AVSP'97 Workshop,  
Abstract: We present a method for the construction of a video-realistic text-to-audiovisual speech synthesizer. A visual corpus of a subject enunciating a set of key words is initially recorded. The key words are chosen so that they collectively contain most of the American English viseme images, which are subsequently identified and extracted from the data by hand. Next, using optical flow methods borrowed from the computer vision literature, we compute realistic transitions between every viseme to every other viseme. The images along these transition paths are generated using a morphing method. Finally, we exploit phoneme and timing information extracted from a text-to-speech synthesizer to determine which viseme transitions to use, and the rate at which the morphing process should occur. In this manner, we are able to synchronize the visual speech stream with the audio speech stream, and hence give the impression of a videorealistic talking face. 
Abstract-found: 1
Intro-found: 1
Reference: <author> T. Beier and S. Neely. </author> <title> (1992) Feature-based image metamorphosis. </title> <booktitle> In SIGGRAPH '92 Proceedings, </booktitle> <pages> pages 35-42, </pages> <address> Chicago, IL. </address>
Reference: <author> J.R. Bergen and R. Hingorani. </author> <title> (1990) Hierarchical motion-based frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center. </institution>
Reference-contexts: However, this approach usually fails to produce realistic transformations, for two reasons: Firstly, in many cases the optical flow algorithms are not able to compute reasonable motion estimates between viseme imagery. The reason for this is that most algorithms, including ours <ref> (Bergen & Hingorani 90) </ref>, typically make a motion smoothness assumption, in which the pixel displacements between images are assumed to be small and locally similar. This assumption is necessary in order to make the problem of determining optical flow well-posed and tractable.
Reference: <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> (1993) Example based image analysis and synthesis. </title> <type> Technical Report 1431, </type> <institution> MIT AI Lab. </institution>
Reference: <author> A. Black and P. Taylor. </author> <title> (1997) The Festival Speech Synthesis System. </title> <institution> University of Edinburgh. </institution>
Reference-contexts: Thirdly, we define, in a manner described in more detail below, a transformation from each viseme image to every other viseme image. Thus, if there are 40 visemes in our final set, we define 40 2 , or 1600, such transformations. 4. Finally, we utilize a text-to-speech system <ref> (Black & Taylor 97) </ref> to convert unconstrained input text into a string of phonemes, along with duration information for each phoneme. Using this information, we determine the appropriate sequence of viseme transitions to make, as well as the rate of the transformations. <p> in the stream. images are morphed intermediates. * Finally, the representation of a viseme transition as an optical flow vector allows us to morph as many intermediate images as necessary to maintain synchrony with the audio produced by the TTS. 8 Audio Synchronization We have incorporated the Festival TTS system <ref> (Black & Taylor 97) </ref>, developed at the University of Edinburgh, into our work. A voice in the Festival system consists of a set of recorded diphones, which are stored as LPC coefficients and corresponding residuals (Hunt, Zwierzynski, et al. 89).
Reference: <author> C. Bregler, M. Covell, and M. Slaney. </author> <title> (1997) Video rewrite: Driving visual speech with audio. </title> <booktitle> In SIG-GRAPH '97 Proceedings, </booktitle> <address> Los Angeles, CA. </address>
Reference: <author> M. M. Cohen and D. W. Massaro. </author> <title> (1993) Modeling coarticulation in synthetic visual speech. </title> <editor> In N. </editor> <publisher> M. </publisher>
Reference-contexts: Unfortunately, much of the recent work in this field falls short of producing the impression of videorealism. The reason for this, we believe, is that most of the current TTAVS systems <ref> (Cohen & Massaro 93, LeGoff & Benoit 96) </ref> have chosen to integrate 3D graphics-based facial models with the audio speech synthesis itself. <p> First, a visual corpus of a subject enunciating a set of key words is initially recorded. Each key word is chosen so that it contains one American English viseme. For simplicity, we assume a one-to-one mapping between phonemes and visemes, and ignore coarticu-lation effects <ref> (Cohen & Massaro 93) </ref>. Consequently, because there are 40-50 American English phonemes (Olive, Greenwood, et al. 93), the subject is asked to enunciate 40-50 words. 2. Next, one single image for each viseme is identified and extracted from the corpus sequence.
Reference: <editor> Thalmann and D. Thalmann, editors, </editor> <booktitle> Models and Techniques in Computer Animation, </booktitle> <pages> pages 139-156. </pages> <publisher> Springer-Verlag, </publisher> <address> Tokyo. </address>
Reference: <author> T. Ezzat. </author> <title> (1996) Example-based analysis and synthesis for images of human faces. </title> <type> Master's thesis, </type> <institution> Mas-sachusetts Institute of Technology. </institution>
Reference: <author> B. K. P. Horn and B. G. Schunck. </author> <title> (1981) Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203. </pages>
Reference: <author> M. J. Hunt, D. A. Zwierzynski, and R. Carr. </author> <title> (1989) Issues in high quality lpc analysis and synthesis. </title> <booktitle> In Proceedings of EUROSPEECH '89, </booktitle> <volume> volume 2, </volume> <pages> pages 348-351, </pages> <address> Paris, France. </address>
Reference-contexts: A voice in the Festival system consists of a set of recorded diphones, which are stored as LPC coefficients and corresponding residuals <ref> (Hunt, Zwierzynski, et al. 89) </ref>. It is interesting to note that the final audio speech stream is constructed by concatenating the appropriate di-phones together, in a manner that is completely analogous to our method for concatenating viseme transitions.
Reference: <author> B. LeGoff and C. Benoit. </author> <title> (1996) A text-to-audiovisual-speech synthesizer for french. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing (ICSLP '96), </booktitle> <address> Philadelphia, USA. </address>
Reference: <author> J. Olive, A. Greenwood, and J. Coleman. </author> <title> (1993) Acoustics of American English Speech: A Dynamic Approach. </title> <publisher> Springer-Verlag, </publisher> <address> New York, USA. </address>
Reference-contexts: Each key word is chosen so that it contains one American English viseme. For simplicity, we assume a one-to-one mapping between phonemes and visemes, and ignore coarticu-lation effects (Cohen & Massaro 93). Consequently, because there are 40-50 American English phonemes <ref> (Olive, Greenwood, et al. 93) </ref>, the subject is asked to enunciate 40-50 words. 2. Next, one single image for each viseme is identified and extracted from the corpus sequence. This is done manually by searching through the recorded frames. 3.
Reference: <author> K.C. Scott, D.S. Kagels, S.H. Watson, H. </author> <title> Rom, </title> <type> J.R. </type>
Reference: <author> Wright, M. Lee, and K.J. Hussey. </author> <title> (1994) Synthesis of speaker facial movement to match selected speech sequences. </title> <booktitle> In Proceedings of the Fifth Australian Conference on Speech Science and Technology, </booktitle> <volume> volume 2, </volume> <pages> pages 620-625. </pages>
Reference: <author> S.H. Watson, J.R. Wright, K.C. Scott, D.S. Kagels, D. Freda, and K.J. Hussey. </author> <title> An advanced morphing algorithm for interpolating phoneme images to simulate speech. </title> <type> Unpublished technical report, </type> <institution> Jet Propulsion Laboratory, California Institute of Technology. </institution>
References-found: 15

