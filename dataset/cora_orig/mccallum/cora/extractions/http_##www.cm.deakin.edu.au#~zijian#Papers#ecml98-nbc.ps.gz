URL: http://www.cm.deakin.edu.au/~zijian/Papers/ecml98-nbc.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: Email: zijian@deakin.edu.au  
Title: Naive Bayesian Classifier Committees  
Author: Zijian Zheng 
Address: Geelong, Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Note: In Proceedings of ECML'98, Berlin: Springer Verlag, 196-207, 1998.  
Abstract: The naive Bayesian classifier provides a very simple yet surprisingly accurate technique for machine learning. Some researchers have examined extensions to the naive Bayesian classifier that seek to further improve the accuracy. For example, a naive Bayesian tree approach generates a decision tree with one naive Bayesian classifier at each leaf. Another example is a constructive Bayesian classifier that eliminates attributes and constructs new attributes using Cartesian products of existing attributes. This paper proposes a simple, but effective approach for the same purpose. It generates a naive Bayesian classifier committee for a given classification task. Each member of the committee is a naive Bayesian classifier based on a subset of all the attributes available for the task. During the classification stage, the committee members vote to predict classes. Experiments across a wide variety of natural domains show that this method significantly increases the prediction accuracy of the naive Bayesian classifier on average. It performs better than the two approaches mentioned above in terms of higher prediction accuracy.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H. and Dietterich, T.G.: </author> <title> Efficient algorithms for identifying relevant features. </title> <booktitle> Proceedings of the 9th Canadian Conference on Artificial Intelligence. </booktitle> <address> Van-couver, BC: </address> <publisher> Morgan Kaufmann (1992) 38-45. </publisher>
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J.: </author> <title> Classification And Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth (1984). </publisher>
Reference-contexts: Leave-1-out cross-validation is used to estimate the error rates of naive Bayesian classifiers, since the leave-1-out cross-validation error rate is a better estimate than the resubstitution error rate <ref> (Breiman, Friedman, Olshen, and Stone 1984) </ref>. In addition, for a naive Bayesian classifier, the operations of removing and adding an example are very easy and efficient. At the beginning, NBC builds a naive Bayesian classifier (called N B base ) using all attributes.
Reference: <author> Breiman, L.: </author> <title> Bagging predictors. </title> <booktitle> Machine Learning. </booktitle> <month> 24 </month> <year> (1996) </year> <month> 123-140. </month>
Reference: <author> Cestnik, B., Kononenko, I., and Bratko, I.: </author> <title> Assistant 86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In I. Bratko & N. Lavrac (Eds.), </editor> <booktitle> Progress in Machine Learning Proceedings of the 2nd European Working Session on Learning (EWSL87). </booktitle> <address> Wilmslow, UK: </address> <note> Sigma Press (1987) 31-45. </note>
Reference: <author> Chatfield, C.: </author> <title> Statistics for Technology: A Course in Applied Statistics. </title> <publisher> London: Chap-man and Hall (1978). </publisher>
Reference-contexts: 23.55 24.55 1.04 1.00 Soybean 9.16 8.65 .94 -0.51 Splice junction 4.38 3.94 .90 -0.44 Tic-Tac-Toe 30.64 29.70 .97 -0.94 Wine 2.22 2.22 1.00 0.00 Zoology 5.45 4.00 .73 -1.45 average 18.62 17.76 .93 -0.85 w/t/l 19/1/9 significance level .0436 significance level better than 0.05 using a two-tailed pairwise t-test <ref> (Chatfield 1978) </ref> on the results of the 20 trials in a domain. The error rate ratios and differences of NBC and NB are also included in the table. A ratio less than 1.00 or a difference less than 0.00 means that NBC has lower error rate than NB. <p> From Table 2, the significant advantage of NBC over NB in terms of lower error rate can be clearly seen. On average over the 29 domains, NBC reduces the error rate of NB by 7%. The one-tailed pairwise sign test <ref> (Chatfield 1978) </ref> 6 on the error rates of NBC and NB in the 29 domains shows that NBC is more accurate than NB at a significance level of 0.0436 (see the last line of the table).
Reference: <author> Domingos, P. and Pazzani, M.: </author> <title> Beyond independence: Conditions for the optimality of the simple Bayesian classifier. </title> <booktitle> Proceedings of the 13th International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann (1996) 105-112. </publisher>
Reference: <author> Duda, R.O. and Hart, P.E.: </author> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley (1973). </publisher> <address> 11 Fayyad, </address> <note> U.M. </note> <author> and Irani, </author> <title> K.B.: Multi-interval discretization of continuous-valued at-tributes for classification learning. </title> <booktitle> Proceedings of the 13th International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann (1993) 1022-1027. </publisher>
Reference: <author> Freund, Y. and Schapire, R.E.: </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <type> Unpublished manuscript, </type> <note> available from the authors' home pages ("http://www.research.att.com/f~yoav,~schapireg") (1996a). </note>
Reference: <author> Freund, Y. and Schapire, R.E.: </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> Proceedings of the 13th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann (1996b) 148-156. </publisher>
Reference: <author> John, G.H., Kohavi, R., and Pfleger, K.: </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> Proceedings of the 11th International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann (1994) 121-129. </publisher>
Reference-contexts: Most existing techniques for improving the performance of the naive Bayesian classifier require complex induction processes. For example, NBTree adopts a hybrid model of decision trees and naive Bayesian classifiers. Each leaf of such a tree contains a naive Bayesian classifier. Bsej employs a wrapper model <ref> (John, Kohavi, and Pfleger 1994) </ref> with the leave-1-out cross-validation estimation to find the best Cartesian product attributes from existing nominal attributes for the naive Bayesian classifier (Pazzani 1996). It also considers deleting existing attributes. This paper proposes a simple method to improve naive Bayesian classifier learning.
Reference: <author> Kira, K. and Rendell, L.A.: </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> Proceedings of the 10th National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press/Cambridge, </publisher> <address> MA: </address> <publisher> MIT Press (1992) 129-134. </publisher>
Reference: <author> Kohavi, R.: </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann (1995) 1137-1143. </publisher>
Reference: <author> Kohavi, R.: </author> <title> Scaling up the accuracy of naive-Bayes classifiers: A decision-tree hybrid. </title> <booktitle> Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining. </booktitle> <address> Menlo Park, CA: </address> <note> The AAAI Press (1996) 202-207. </note>
Reference-contexts: Computational requirement of NBC as a function of training set size 4 Discussion Since the constructive naive Bayesian classifier Bsej (Pazzani 1996) and the naive Bayesian tree learning algorithm NBTree <ref> (Kohavi 1996) </ref> also intend to improve the performance of naive Bayesian classifier learning, it is interesting to compare NBC with them. We implemented Bsej and NBTree based on Pazzani (1996) and Kohavi (1996) respectively. <p> method can be designed to significantly reduce the average error rate of NBC. 5 Related Work From the point of view of improving the performance of naive Bayesian classifier learning, the work related to NBC includes the constructive Bayesian classifier (Bsej) (Pazzani 1996) and the naive Bayesian tree (NBTree) approach <ref> (Kohavi 1996) </ref> mentioned in the introduction section, as well as the semi-naive Bayesian classifier (Kononenko 1991) and the attribute deletion technique (Langley and Sage 1994). Kononenko's semi-naive Bayesian classifier performs exhaustive search to iteratively join pairs of attribute values (Kononenko 1991).
Reference: <author> Kononenko, I.: </author> <title> Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition. </title> <editor> In B. Wielinga et al. (Eds.), </editor> <booktitle> Current Trends in Knowledge Acquisition. </booktitle> <address> Amsterdam: </address> <publisher> IOS Press (1990). </publisher>
Reference: <author> Kononenko, I.: </author> <title> Semi-naive Bayesian classifier. </title> <booktitle> Proceedings of European Conference on Artificial Intelligence (1991) 206-219. </booktitle>
Reference-contexts: Related Work From the point of view of improving the performance of naive Bayesian classifier learning, the work related to NBC includes the constructive Bayesian classifier (Bsej) (Pazzani 1996) and the naive Bayesian tree (NBTree) approach (Kohavi 1996) mentioned in the introduction section, as well as the semi-naive Bayesian classifier <ref> (Kononenko 1991) </ref> and the attribute deletion technique (Langley and Sage 1994). Kononenko's semi-naive Bayesian classifier performs exhaustive search to iteratively join pairs of attribute values (Kononenko 1991). The aim is to optimise the tradeoff between the "non-naivety" and the reliability of estimates of probabilities. <p> classifier (Bsej) (Pazzani 1996) and the naive Bayesian tree (NBTree) approach (Kohavi 1996) mentioned in the introduction section, as well as the semi-naive Bayesian classifier <ref> (Kononenko 1991) </ref> and the attribute deletion technique (Langley and Sage 1994). Kononenko's semi-naive Bayesian classifier performs exhaustive search to iteratively join pairs of attribute values (Kononenko 1991). The aim is to optimise the tradeoff between the "non-naivety" and the reliability of estimates of probabilities. Langley and Sage (1994) have shown that attribute deletion can improve the performance of the naive Bayesian classifier when attributes are inter-dependent, especially when some attributes are redundant.
Reference: <author> Langley, P., Iba, W.F., and Thompson, K.: </author> <title> An analysis of Bayesian classifiers. </title> <booktitle> Proceedings of the 10th National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, CA: </address> <note> The AAAI Press (1992) 223-228. </note>
Reference: <author> Langley, P.: </author> <title> Selection of relevant features in machine learning. </title> <booktitle> Proceeding of the AAAI Fall Symposium on Relevance, </booktitle> <address> New Orleans, LA: </address> <publisher> The AAAI Press (1994). </publisher>
Reference-contexts: on an attribute subset might perform better than a 1 This can be implemented by either changing the weights of training examples directly if the learner can handle it, or drawing a succession of independent bootstrap samples from the original training set. 2 naive Bayesian classifier created using all attributes <ref> (Langley and Sage 1994) </ref>. Therefore, generating naive Bayesian classifier committees could be an approach to improving the performance of the naive Bayesian classifier. In the committee, each member is a naive Bayesian classifier built using a subset of attributes. <p> of improving the performance of naive Bayesian classifier learning, the work related to NBC includes the constructive Bayesian classifier (Bsej) (Pazzani 1996) and the naive Bayesian tree (NBTree) approach (Kohavi 1996) mentioned in the introduction section, as well as the semi-naive Bayesian classifier (Kononenko 1991) and the attribute deletion technique <ref> (Langley and Sage 1994) </ref>. Kononenko's semi-naive Bayesian classifier performs exhaustive search to iteratively join pairs of attribute values (Kononenko 1991). The aim is to optimise the tradeoff between the "non-naivety" and the reliability of estimates of probabilities.
Reference: <author> Langley, P. and Sage, S.: </author> <title> Induction of selective Bayesian classifiers. </title> <booktitle> Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence. </booktitle> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann (1994) 339-406. </publisher>
Reference-contexts: on an attribute subset might perform better than a 1 This can be implemented by either changing the weights of training examples directly if the learner can handle it, or drawing a succession of independent bootstrap samples from the original training set. 2 naive Bayesian classifier created using all attributes <ref> (Langley and Sage 1994) </ref>. Therefore, generating naive Bayesian classifier committees could be an approach to improving the performance of the naive Bayesian classifier. In the committee, each member is a naive Bayesian classifier built using a subset of attributes. <p> of improving the performance of naive Bayesian classifier learning, the work related to NBC includes the constructive Bayesian classifier (Bsej) (Pazzani 1996) and the naive Bayesian tree (NBTree) approach (Kohavi 1996) mentioned in the introduction section, as well as the semi-naive Bayesian classifier (Kononenko 1991) and the attribute deletion technique <ref> (Langley and Sage 1994) </ref>. Kononenko's semi-naive Bayesian classifier performs exhaustive search to iteratively join pairs of attribute values (Kononenko 1991). The aim is to optimise the tradeoff between the "non-naivety" and the reliability of estimates of probabilities.
Reference: <author> Merz, C.J. and Murphy, </author> <title> P.M.: UCI Repository of Machine Learning Databases [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, </title> <institution> CA: University of California, Department of Information and Computer Science (1997). </institution>
Reference-contexts: They include all the domains used by Domingos and Pazzani (1996) for studying the naive Bayesian classifier. These twenty-nine domains cover a wide variety of different domains and all are available from the UCI machine learning repository <ref> (Merz and Mur-phy 1997) </ref>. In each domain, two stratified 10-fold cross-validations (10-CV) (Breiman et al. 1984; Kohavi 1995) are performed for each algorithm. A 10-CV is carried out by randomly splitting the data set into 10 subsets that have similar size and class distribution.
Reference: <author> Pazzani, M.J.: </author> <title> Constructive induction of Cartesian product attributes. </title> <booktitle> Proceedings of the Conference, ISIS'96: Information, Statistics and Induction in Science. Singa-pore: World Scientific (1996) 66-77. </booktitle>
Reference-contexts: However, when the strong attribute independence assumption is violated, which is very common, the performance of the naive Bayesian classifier can be poor. 1 A few techniques have been developed to improve the performance of the naive Bayesian classifier. Two examples are the constructive Bayesian classifier (Bsej) <ref> (Pazzani 1996) </ref>, and the naive Bayesian tree (NBTree) approach (Ko-havi 1996). <p> Each leaf of such a tree contains a naive Bayesian classifier. Bsej employs a wrapper model (John, Kohavi, and Pfleger 1994) with the leave-1-out cross-validation estimation to find the best Cartesian product attributes from existing nominal attributes for the naive Bayesian classifier <ref> (Pazzani 1996) </ref>. It also considers deleting existing attributes. This paper proposes a simple method to improve naive Bayesian classifier learning. It is called the naive Bayesian classifier committee (NBC). <p> Computational requirement of NBC as a function of training set size 4 Discussion Since the constructive naive Bayesian classifier Bsej <ref> (Pazzani 1996) </ref> and the naive Bayesian tree learning algorithm NBTree (Kohavi 1996) also intend to improve the performance of naive Bayesian classifier learning, it is interesting to compare NBC with them. We implemented Bsej and NBTree based on Pazzani (1996) and Kohavi (1996) respectively. <p> It remains an open question whether an appropriate weighting method can be designed to significantly reduce the average error rate of NBC. 5 Related Work From the point of view of improving the performance of naive Bayesian classifier learning, the work related to NBC includes the constructive Bayesian classifier (Bsej) <ref> (Pazzani 1996) </ref> and the naive Bayesian tree (NBTree) approach (Kohavi 1996) mentioned in the introduction section, as well as the semi-naive Bayesian classifier (Kononenko 1991) and the attribute deletion technique (Langley and Sage 1994). Kononenko's semi-naive Bayesian classifier performs exhaustive search to iteratively join pairs of attribute values (Kononenko 1991).
Reference: <author> Quinlan, J.R.: Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. Proceedings of the 13th National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park: </address> <note> The AAAI Press (1996) 725-730. </note>
Reference-contexts: Finally, the individual classifiers are combined through voting to form a composite classifier. Quinlan (1996) shows that boosting can significantly increase the prediction accuracy of decision tree learning. We implemented a boosting algorithm for naive Bayesian classifier using a similar method to that for boosting decision trees <ref> (Quinlan 1996) </ref>. Although the algorithm achieves higher accuracy than the naive Bayesian classifier in some domains, the overall accuracy improvement over the naive Bayesian classifier in a large set of natural domains is very marginal. <p> <ref> (Quinlan 1996) </ref>. Although the algorithm achieves higher accuracy than the naive Bayesian classifier in some domains, the overall accuracy improvement over the naive Bayesian classifier in a large set of natural domains is very marginal. The reason might be that boosting implicitly requires the instability of the boosted learning systems (Quinlan 1996). Naive Bayesian classifier learning is more stable than decision tree learning. A small change to the training set will have little impact on a naive Bayesian classifier. <p> Both boosting and bagging generate different classifiers by deriving different training sets from the original one, while NBC creates different classifiers by deriving different attribute subsets. Boosting and bagging have been applied on weak learning algorithms with great success, such as decision tree learning <ref> (Quinlan 1996) </ref>. No published research has been seen so far on applying boosting or classifier committee techniques to naive Bayesian classifier learning. No effort has been made to explore approaches to generating, as a composite classifier, a set of classifiers using different attribute subsets.
Reference: <author> Schapire, R.E., Freund, Y., Bartlett, P., and Lee W.S.: </author> <title> Boosting the margin: A new explanation for the effectiveness of voting methods. </title> <booktitle> Proceedings of the 11th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann (1997) 322-330. </publisher>
Reference: <author> Ting, K.M.: </author> <title> Discretization of continuous-valued attributes and instance-based learning (Technical Report 491). </title> <address> Sydney, Australia: </address> <institution> University of Sydney, Basser Department of Computer Science (1994). </institution> <note> This article was processed using the L a T E X macro package with LLNCS style 12 </note>
References-found: 23

