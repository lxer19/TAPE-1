URL: http://www.research.att.com/~mkearns/papers/inference.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Email: fmkearns,lsaulg@research.att.com  
Title: Large Deviation Methods for Approximate Probabilistic Inference, with Rates of Convergence a free parameter. The
Author: Michael Kearns and Lawrence Saul 
Note: ln(N )=N, and the other of order N 12fl where is  
Affiliation: AT&T Labs Research  
Abstract: We study layered belief networks of binary random variables in which the conditional probabilities Pr[childjparents] depend monotonically on weighted sums of the parents. For these networks, we give efficient algorithms for computing rigorous bounds on the marginal probabilities of evidence at the output layer. Our methods apply generally to the computation of both upper and lower bounds, as well as to generic transfer function parameterizations of the conditional probability tables (such as sigmoid and noisy-OR). We also prove rates of convergence of the accuracy of our bounds as a function of network size. Our results are derived by applying the theory of large deviations to the weighted sums of parents at each node in the network. Bounds on the marginal probabilities are computed from two contributions: one assuming that these weighted sums fall near their mean values, and the other assuming that they do not. This gives rise to an interesting trade-off between probable explanations of the evidence and improbable deviations from the mean. In networks where each child has N parents, the gap between our upper and lower bounds behaves as a sum of two terms, one of order p In addition to providing such rates of convergence for large networks, our methods also yield efficient algorithms for approximate inference in fixed networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cover, T., & Thomas J. </author> <booktitle> (1991) Elements of Information Theory. </booktitle> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: Our results are derived by applying the theory of large deviations | generalizations of well-known tools such as Hoeffding and Chernoff bounds <ref> [1] </ref> | to the weighted sums of parents at each node in the network. At each node of the network, our algorithms introduce a variational parameter that essentially quantifies what it means for the incoming weighted sum to fall "near" its mean. <p> Section 5 generalizes these results to the multilayer case. In Section 6, we conclude with a discussion and some remarks on the relationship of our work and previous variational methods. 2 Definitions and Preliminaries Definition 1 A transfer function is a mapping f : <ref> [1; 1] </ref> ! [0; 1] that is everywhere differentiable and satisfies f 0 (x) 0 for all x (thus, f is nondecreasing). If f 0 (x) ff for all x, we say that f has slope ff. <p> Section 5 generalizes these results to the multilayer case. In Section 6, we conclude with a discussion and some remarks on the relationship of our work and previous variational methods. 2 Definitions and Preliminaries Definition 1 A transfer function is a mapping f : [1; 1] ! <ref> [0; 1] </ref> that is everywhere differentiable and satisfies f 0 (x) 0 for all x (thus, f is nondecreasing). If f 0 (x) ff for all x, we say that f has slope ff. <p> These large deviation bounds will form the cornerstone of our subsequent analyses and inference algorithms. The following preliminary lemma will prove extremely useful. Lemma 1 For all p 2 <ref> [0; 1] </ref> and jtj &lt; 1, 1 h i 1 (p) (1) where (p) = ln 1p i : (2) Proof: Let g (t) denote the left hand side of Equation (1). Figure 1 (a) shows some plots of g (t) versus t for different (fixed) values of p. <p> A plot of (p) is shown in Figure 1 (b). 2 Equipped with this lemma, we can now give a simple upper bound on the probability of large deviations for weighted sums of independent binary random variables. The following theorem generalizes classical large-deviation results <ref> [1] </ref>, p; (b) (p) = (1 2p)= ln 1p 3 and will serve as the starting point for our analysis of two-layer networks in Section 4. <p> We again propose a gradient descent on P U C and gradient ascent on P L C in order to find the (possibly different) settings of the * ` i that minimize the lower bound and maximize the upper bound. In addition to the constraints * ` i 2 <ref> [0; 1] </ref>, we must now also ensure that we maintain a valid set of *-intervals at all times; however, this is again a simple constraint to obey, since it always translates to a lower bound on a given * ` i in terms of the other intervals.
Reference: [2] <author> Dagum, P., & Luby, M. </author> <year> (1993). </year> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60, </volume> <pages> 141-153. </pages>
Reference-contexts: 1 Introduction The intractability of probabilistic inference in general graphical models <ref> [2] </ref> has led to several interesting lines of research examining specialized algorithms for restricted classes of models.
Reference: [3] <author> Jaakkola, T. </author> <title> (1997) Variational methods for inference and estimation in graphical models. Unpublished doctoral dissertation, </title> <publisher> MIT. </publisher>
Reference-contexts: Inspired by ideas from statistical mechanics and convex duality, several so-called variational methods have been introduced for dense networks <ref> [3, 4, 9] </ref>, along with guarantees that they provide rigorous upper and lower bounds on the desired marginal probabilities of interest. The current work is a contribution to this latter line of results on dense networks. <p> Like previous work on variational methods [4], our approach relates the problem of probabilistic inference to one of optimization. Indeed, it is interesting that large deviation methods lead to algorithms of the same general nature as those derived from convex duality <ref> [3] </ref> and mean field theory [9]. Thus, for example, our *-intervals play a similar role to the dual variables introduced by Legendre transformations of the log-transfer function. In both cases, the introduction of auxiliary or variational parameters makes it possible to perform an otherwise intractable average over hidden variables.
Reference: [4] <author> Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. </author> <title> (1997) An introduction to variational methods for graphical models. </title> <note> To appear in M. </note> <editor> Jordan, ed. </editor> <title> Learning in Graphical Models, </title> <publisher> Kluwer Academic. </publisher>
Reference-contexts: Inspired by ideas from statistical mechanics and convex duality, several so-called variational methods have been introduced for dense networks <ref> [3, 4, 9] </ref>, along with guarantees that they provide rigorous upper and lower bounds on the desired marginal probabilities of interest. The current work is a contribution to this latter line of results on dense networks. <p> More generally, though, for networks of 18 fixed size, we have proposed algorithms for finding the *-intervals that yield the tightest possible bounds on marginal probabilities. Like previous work on variational methods <ref> [4] </ref>, our approach relates the problem of probabilistic inference to one of optimization. Indeed, it is interesting that large deviation methods lead to algorithms of the same general nature as those derived from convex duality [3] and mean field theory [9]. <p> In particular, many of our proofs can be generalized to analyze the gap between other types of lower and upper bounds on marginal probabilities, including those for the variational methods <ref> [4] </ref>. Of course, large deviation methods also have their own limitations. They are designed mainly for very large probabilistic networks whose weights are of order 1=N , where N is the number of parents at each node in the graph.
Reference: [5] <author> Jensen, F.V. </author> <title> (1996) An Introduction to Bayesian Networks. </title> <publisher> London: UCL Press. </publisher>
Reference-contexts: line of work has the well-known poly-tree algorithm [8] for exact inference as its starting point, and can be viewed as studying the limiting case of sparse networks | models that either start with relatively low connectivity, or can be massaged through graph-theoretic operations into equivalent models with low connectivity <ref> [5] </ref>. Another, rather different, approach has been the study of the limiting case of dense networks. Here the intuition is that in large networks with many connections and certain parametric conditional probability tables, "averaging" behavior may simplify the problem of approximate inference even though exact calculations remain hopelessly intractable.
Reference: [6] <author> Neal, R. </author> <title> (1992) Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56, </volume> <pages> 71-113. </pages>
Reference: [7] <author> Parisi, G. </author> <title> (1988) Statistical field theory. </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The limit of large N considered in this paper is analogous to the thermodynamic limit for physical (undirected graphical) models of infinite-range ferromagnets <ref> [7] </ref>. In both directed and undirected graphical models, weights of order 1=N give rise to the simplest type of limiting behavior as N ! 1. It should be noted, however, that other limits (e.g., weights of order 1= p N ) are also possible.
Reference: [8] <author> Pearl, J. </author> <title> (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 Introduction The intractability of probabilistic inference in general graphical models [2] has led to several interesting lines of research examining specialized algorithms for restricted classes of models. One such line of work has the well-known poly-tree algorithm <ref> [8] </ref> for exact inference as its starting point, and can be viewed as studying the limiting case of sparse networks | models that either start with relatively low connectivity, or can be massaged through graph-theoretic operations into equivalent models with low connectivity [5].
Reference: [9] <author> Saul, L., Jordan, M., and Jaakkola, T. </author> <title> (1996) Mean field theory for sigmoid belief networks. </title> <journal> Journal of Artificial Intelligence Research 4, </journal> <pages> 61-76. 20 </pages>
Reference-contexts: Inspired by ideas from statistical mechanics and convex duality, several so-called variational methods have been introduced for dense networks <ref> [3, 4, 9] </ref>, along with guarantees that they provide rigorous upper and lower bounds on the desired marginal probabilities of interest. The current work is a contribution to this latter line of results on dense networks. <p> If f 0 (x) ff for all x, we say that f has slope ff. Common examples of transfer functions of bounded slope include the noisy-OR f (x) = max (0; 1 e x ) and sigmoid f (x) = 1=(1 + e x ) <ref> [9] </ref>. Definition 2 For any transfer function f , a layered probabilistic f -network is defined by: * Binary variables fX ` i g, where ` = 1; : : : ; L and i = 1; : : : ; N ` . <p> Like previous work on variational methods [4], our approach relates the problem of probabilistic inference to one of optimization. Indeed, it is interesting that large deviation methods lead to algorithms of the same general nature as those derived from convex duality [3] and mean field theory <ref> [9] </ref>. Thus, for example, our *-intervals play a similar role to the dual variables introduced by Legendre transformations of the log-transfer function. In both cases, the introduction of auxiliary or variational parameters makes it possible to perform an otherwise intractable average over hidden variables.
References-found: 9

