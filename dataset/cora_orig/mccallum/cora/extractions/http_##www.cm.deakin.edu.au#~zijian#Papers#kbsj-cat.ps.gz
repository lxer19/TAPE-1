URL: http://www.cm.deakin.edu.au/~zijian/Papers/kbsj-cat.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Title: Constructing Conjunctions using Systematic Search on Decision Trees  
Author: Zijian Zheng 
Keyword: Knowledge Discovery; Machine Learning; Constructive Induction  
Address: Geelong, Victoria 3217, Australia  2028  
Affiliation: School of Computing and Mathematics Deakin University,  
Note: Draft, April 1997 (submitted to Knowledge Based Systems Journal)  
Email: Email: zijian@deakin.edu.au  
Phone: Tel: +61 3 5227 1325, Fax: +61 3 5227  
Abstract: This paper investigates a dynamic path-based method for constructing conjunctions as new attributes for decision tree learning. It searches for conditions (attribute-value pairs) from paths to form new attributes. Compared with other hypothesis-driven new attribute construction methods, the new idea of this method is that it carries out systematic search with pruning over each path of a tree to select conditions for generating a conjunction. Therefore, conditions for constructing new attributes are dynamically decided during search. Empirically evaluation in a set of artificial and real-world domains shows that the dynamic path-based method can improve the performance of selective decision tree learning in terms of both higher prediction accuracy and lower theory complexity. In addition, it shows some performance advantages over a fixed path-based method and a fixed rule-based method for learning decision trees. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C.J. Matheus, </author> <title> Feature Construction: an Analytic Framework and an Application to Decision Trees, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, </institution> <year> 1989. </year>
Reference-contexts: The former are expected to be more appropriate than the latter for representing theories to be learned. People have explored fixed path-based and fixed rule-based approaches to constructing new binary attributes for decision tree learning <ref> [1, 2, 3] </ref>. The number and positions 1 of conditions for generating a new attribute in a path of a tree or a rule are predecided (fixed) in these algorithms. <p> It has been shown that the number and locations of conditions in a path of a tree or a rule which are used to form a conjunction affect the performance of this type of constructive induction algorithm <ref> [1, 3] </ref>. This paper presents a novel method of constructing new attributes by performing a search over a path of a tree. It can find relevant conditions to form the best conjunction that can be created from the path in terms of an attribute evaluation function. <p> this can be explained as the oversearching phenomenon [13]. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm <ref> [1, 14] </ref>, the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes.
Reference: [2] <author> G. Pagallo and D. Haussler, </author> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning, </booktitle> <month> 5 </month> <year> (1990) </year> <month> 71-100. </month>
Reference-contexts: The former are expected to be more appropriate than the latter for representing theories to be learned. People have explored fixed path-based and fixed rule-based approaches to constructing new binary attributes for decision tree learning <ref> [1, 2, 3] </ref>. The number and positions 1 of conditions for generating a new attribute in a path of a tree or a rule are predecided (fixed) in these algorithms. <p> It considers all possible combinations of conditions in a path and is efficient in practice as some parts of the search space are eliminated during search. This search method is due to Webb's work on rule learning [4]. 2 The CAT Algorithm Like the Fringe family of algorithms <ref> [2, 5, 6] </ref> and the CI algorithms [3], CAT is also a hypothesis-driven constructive induction algorithm for learning multivariate trees. It constructs new binary attributes by using the dynamic path-based method over previously learned decision trees. <p> On the other hand, this can be explained as the oversearching phenomenon [13]. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe <ref> [2, 5] </ref>, SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes.
Reference: [3] <author> Z. Zheng, </author> <title> Constructing conjunctive tests for decision trees. </title> <booktitle> Proceedings of the Fifth Australian Joint Conference on Artificial Intelligence, World Scientific, Singapore (1992), </booktitle> <pages> 355-360. </pages>
Reference-contexts: The former are expected to be more appropriate than the latter for representing theories to be learned. People have explored fixed path-based and fixed rule-based approaches to constructing new binary attributes for decision tree learning <ref> [1, 2, 3] </ref>. The number and positions 1 of conditions for generating a new attribute in a path of a tree or a rule are predecided (fixed) in these algorithms. <p> It has been shown that the number and locations of conditions in a path of a tree or a rule which are used to form a conjunction affect the performance of this type of constructive induction algorithm <ref> [1, 3] </ref>. This paper presents a novel method of constructing new attributes by performing a search over a path of a tree. It can find relevant conditions to form the best conjunction that can be created from the path in terms of an attribute evaluation function. <p> This search method is due to Webb's work on rule learning [4]. 2 The CAT Algorithm Like the Fringe family of algorithms [2, 5, 6] and the CI algorithms <ref> [3] </ref>, CAT is also a hypothesis-driven constructive induction algorithm for learning multivariate trees. It constructs new binary attributes by using the dynamic path-based method over previously learned decision trees. <p> oversearching phenomenon [13]. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 <ref> [3, 7] </ref>, the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes. As far as systematic search is concerned, the closest related work is Opus [4].
Reference: [4] <author> G.I. Webb, </author> <title> Systematic search for categorical attribute-value data-driven machine learning. </title> <booktitle> Proceedings of the Sixth Australian Joint Conference on Artificial Intelligence, World Scientific, Singapore (1993), </booktitle> <pages> 342-347. </pages>
Reference-contexts: The number of conditions used to generate a new attribute is also dynamically decided. Therefore, this approach is referred to as the dynamic path-based approach. The CAT algorithm is an implementation of this approach. CAT employs a systematic search method with pruning <ref> [4] </ref>. It considers all possible combinations of conditions in a path and is efficient in practice as some parts of the search space are eliminated during search. This search method is due to Webb's work on rule learning [4]. 2 The CAT Algorithm Like the Fringe family of algorithms [2, 5, <p> CAT employs a systematic search method with pruning <ref> [4] </ref>. It considers all possible combinations of conditions in a path and is efficient in practice as some parts of the search space are eliminated during search. This search method is due to Webb's work on rule learning [4]. 2 The CAT Algorithm Like the Fringe family of algorithms [2, 5, 6] and the CI algorithms [3], CAT is also a hypothesis-driven constructive induction algorithm for learning multivariate trees. It constructs new binary attributes by using the dynamic path-based method over previously learned decision trees. <p> A condition that can be added to a conjunction must have a higher order than existing conditions of the conjunction. For example, given four conditions and their order as: A, B, C, and D, Figure 1 shows a search tree. However, following the idea of Opus <ref> [4] </ref>, CAT uses an information-based evaluation function (different from that of Opus) to order the children of a node when generating them. The objective is to increase the size of the regions that can be pruned. <p> They use different constructive operators and different strategies to create new attributes. As far as systematic search is concerned, the closest related work is Opus <ref> [4] </ref>. Some ideas in CAT about the systematic search with pruning are from it. Opus carries out systematic search with pruning over the space of all possible disjuncts at the inner level of a covering rule learning algorithm. <p> The method of systematic search with pruning used in CAT is very similar to that in Opus. The main difference is that CAT uses information gain as the evaluation function, while Opus employs the Laplace function <ref> [4] </ref>. In addition, CAT searches for a conjunction as a new attribute for learning a decision tree, whereas Opus searches for a conjunction as a rule for learning a set of unordered rules.
Reference: [5] <author> G. Pagallo, </author> <title> Adaptive Decision Tree Algorithms for Learning from Examples, </title> <type> Ph.D. Thesis, </type> <institution> University of California at Santa Cruz, </institution> <address> Santa Cruz, CA, </address> <year> 1990. </year> <month> 13 </month>
Reference-contexts: It considers all possible combinations of conditions in a path and is efficient in practice as some parts of the search space are eliminated during search. This search method is due to Webb's work on rule learning [4]. 2 The CAT Algorithm Like the Fringe family of algorithms <ref> [2, 5, 6] </ref> and the CI algorithms [3], CAT is also a hypothesis-driven constructive induction algorithm for learning multivariate trees. It constructs new binary attributes by using the dynamic path-based method over previously learned decision trees. <p> Computational requirements will be briefly addressed in the final subsection. SFringe is a member of the Fringe family of hypothesis-driven constructive decision tree learning algorithms <ref> [5] </ref>. It follows the idea of SymFringe [6] with a straightforward extension. For each leaf, it constructs one new attribute using the conjunction of two conditions at the parent and grandparent nodes of the leaf. SFringe adopts the fixed path-based strategy. <p> Note that SFringe, CI3, and CAT use the same decision tree building and pruning method, tree evaluation function, and stopping criterion. 7 3.1 Experimental Domains and Methods Fourteen artificial logical domains are from Pagallo <ref> [5] </ref>. They cover a variety of well-studied artificial logical concepts in the machine learning community: randomly generated boolean concepts including DNF and CNF concepts, multiplexor concepts, parity concepts, and majority concepts. We adopt the same experimental method used by Pagallo [5], including the sizes of training and test sets. <p> Domains and Methods Fourteen artificial logical domains are from Pagallo <ref> [5] </ref>. They cover a variety of well-studied artificial logical concepts in the machine learning community: randomly generated boolean concepts including DNF and CNF concepts, multiplexor concepts, parity concepts, and majority concepts. We adopt the same experimental method used by Pagallo [5], including the sizes of training and test sets. For each experiment, a training set and a test set are independently drawn from the uniform distribution. Experiments are repeated ten times in each of these domains. <p> On the other hand, this can be explained as the oversearching phenomenon [13]. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe <ref> [2, 5] </ref>, SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes.
Reference: [6] <author> D. Yang, L. Rendell, and G. Blix, </author> <title> A scheme for feature construction and a comparison of empirical methods. </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA (1991), </address> <pages> 699-704. </pages>
Reference-contexts: It considers all possible combinations of conditions in a path and is efficient in practice as some parts of the search space are eliminated during search. This search method is due to Webb's work on rule learning [4]. 2 The CAT Algorithm Like the Fringe family of algorithms <ref> [2, 5, 6] </ref> and the CI algorithms [3], CAT is also a hypothesis-driven constructive induction algorithm for learning multivariate trees. It constructs new binary attributes by using the dynamic path-based method over previously learned decision trees. <p> Computational requirements will be briefly addressed in the final subsection. SFringe is a member of the Fringe family of hypothesis-driven constructive decision tree learning algorithms [5]. It follows the idea of SymFringe <ref> [6] </ref> with a straightforward extension. For each leaf, it constructs one new attribute using the conjunction of two conditions at the parent and grandparent nodes of the leaf. SFringe adopts the fixed path-based strategy. <p> On the other hand, this can be explained as the oversearching phenomenon [13]. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe <ref> [6] </ref>, and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes.
Reference: [7] <author> Z. Zheng, </author> <title> Constructing New Attributes for Decision Tree Learning, </title> <type> Ph.D. Thesis, </type> <institution> Basser Department of Computer Science, The University of Sydney, </institution> <year> 1996. </year>
Reference-contexts: They may consist of both primitive and new attributes. Finally, CAT selects the best pruned tree from the two stages as its output. CAT uses an MDL-inspired heuristic function as its tree evaluation function <ref> [7] </ref>. The function is similar to the coding cost function used by Quinlan and Rivest [8], but exceptions of a tree on the training set are replaced by the more pessimistically estimated exceptions of the tree [9]. In addition, new attributes are encoded [7]. <p> heuristic function as its tree evaluation function <ref> [7] </ref>. The function is similar to the coding cost function used by Quinlan and Rivest [8], but exceptions of a tree on the training set are replaced by the more pessimistically estimated exceptions of the tree [9]. In addition, new attributes are encoded [7]. In the current implementation, CAT adopts the following stopping criterion: * No new attribute can be constructed, or * No better pruned tree has been built in five consecutive iterations, or * A given maximum iteration number is reached (the default value is 20). <p> More than 99.2% of the search space is pruned. 3 Experiments This section uses experiments to evaluate the CAT algorithm by comparing it with the C4.5 [9], SFringe <ref> [7] </ref>, and CI3 [7] algorithms in a set of artificial and real-world domains. Here, we focus on prediction accuracy and theory complexity. The theory complexity [10] is the modified tree size. <p> More than 99.2% of the search space is pruned. 3 Experiments This section uses experiments to evaluate the CAT algorithm by comparing it with the C4.5 [9], SFringe <ref> [7] </ref>, and CI3 [7] algorithms in a set of artificial and real-world domains. Here, we focus on prediction accuracy and theory complexity. The theory complexity [10] is the modified tree size. <p> On the other hand, this can be explained as the oversearching phenomenon [13]. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe <ref> [7] </ref>, the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes. <p> oversearching phenomenon [13]. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 <ref> [3, 7] </ref>, the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes. As far as systematic search is concerned, the closest related work is Opus [4].
Reference: [8] <author> J.R. Quinlan and R.L. Rivest, </author> <title> Inferring decision trees using the minimum description length principle. </title> <booktitle> Information and Computation, </booktitle> <month> 80 </month> <year> (1989) </year> <month> 227-248. </month>
Reference-contexts: They may consist of both primitive and new attributes. Finally, CAT selects the best pruned tree from the two stages as its output. CAT uses an MDL-inspired heuristic function as its tree evaluation function [7]. The function is similar to the coding cost function used by Quinlan and Rivest <ref> [8] </ref>, but exceptions of a tree on the training set are replaced by the more pessimistically estimated exceptions of the tree [9]. In addition, new attributes are encoded [7].
Reference: [9] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: CAT uses an MDL-inspired heuristic function as its tree evaluation function [7]. The function is similar to the coding cost function used by Quinlan and Rivest [8], but exceptions of a tree on the training set are replaced by the more pessimistically estimated exceptions of the tree <ref> [9] </ref>. In addition, new attributes are encoded [7]. <p> Place Figure 1 About Here The new attribute evaluation function used by CAT for carrying out systematic search is information gain. It satisfies the requirement for performing systematic search with pruning. In addition, its disadvantage of favouring attributes with more different values <ref> [9] </ref> does not show up here since all new attributes are binary. <p> The search space contains, on average, 93563.3 states when constructing new attributes from the first tree, while only 732.3 states are really examined. More than 99.2% of the search space is pruned. 3 Experiments This section uses experiments to evaluate the CAT algorithm by comparing it with the C4.5 <ref> [9] </ref>, SFringe [7], and CI3 [7] algorithms in a set of artificial and real-world domains. Here, we focus on prediction accuracy and theory complexity. The theory complexity [10] is the modified tree size.
Reference: [10] <author> Z. Zheng, </author> <title> Constructing nominal Xof-N attributes. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA (1995), </address> <pages> 1064-1070. </pages>
Reference-contexts: Here, we focus on prediction accuracy and theory complexity. The theory complexity <ref> [10] </ref> is the modified tree size. It is the sum of the sizes of all the nodes of the tree rather than the number of decision nodes or all the nodes of the tree. The size of a leaf is 1. <p> related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm <ref> [10] </ref>. They use different constructive operators and different strategies to create new attributes. As far as systematic search is concerned, the closest related work is Opus [4]. Some ideas in CAT about the systematic search with pruning are from it.
Reference: [11] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, http://www.ics.uci.edu/~mlearn/MLRepository.html, 1996. </address>
Reference-contexts: For each experiment, a training set and a test set are independently drawn from the uniform distribution. Experiments are repeated ten times in each of these domains. Besides the fourteen artificial logical domains, three Monks domains [12] and ten domains from the UCI repository of machine learning databases <ref> [11] </ref> are used. The ten UCI domains consist of five medical domains (Cleveland Heart Disease, Hepatitis, Liver Disorders, Pima Indians Diabetes, Wisconsin Breast Cancer), one molecular biology domain (Promoters), three linguistics domains (Nettalk (Phoneme), Nettalk (Stress), Nettalk (Letter)), and one game domain (Tic-Tac-Toe).
Reference: [12] <author> S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang, </author> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CD-91-197, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year>
Reference-contexts: For each experiment, a training set and a test set are independently drawn from the uniform distribution. Experiments are repeated ten times in each of these domains. Besides the fourteen artificial logical domains, three Monks domains <ref> [12] </ref> and ten domains from the UCI repository of machine learning databases [11] are used. <p> For each UCI domain, a 10-fold cross-validation is conducted on the entire data set and all algorithms are run on the same partitions. For each Monks domain, one trial is carried out using the fixed training and test sets because they are provided by the problem designers <ref> [12] </ref>. To compare the accuracies of two algorithms in a domain, a two-tailed block-based pairwise t-test is conducted. An instance-based pairwise sign-test is used in each Monks domain since only one trial is conducted. A difference is considered as significant if the significance level is above 95%.
Reference: [13] <author> J.R. Quinlan and R.M. Cameron-Jones, </author> <title> Oversearching and layered search in empirical learning. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA (1995), </address> <pages> 1019-1024. </pages>
Reference-contexts: One reason might be that information gain is used as the heuristic function to evaluate new attributes. Good new attributes cannot always be selected even if they are examined. On the other hand, this can be explained as the oversearching phenomenon <ref> [13] </ref>. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the <p> Alternative search methods such as simple and less expensive greedy search are worth exploring, especially considering the oversearching phenomenon <ref> [13] </ref>. The experiments found that the dynamic path-based method can significantly improve the performance of decision tree learning in most of the domains studied in terms of both higher prediction accuracy and lower theory complexity. In no case does CAT reduce the accuracy of C4.5.
Reference: [14] <author> C.J. Matheus and L.A. Rendell, </author> <title> Constructive induction on decision trees. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA (1989), </address> <pages> 645-650. </pages>
Reference-contexts: this can be explained as the oversearching phenomenon [13]. 5 Related Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm <ref> [1, 14] </ref>, the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes.
Reference: [15] <author> H. Ragavan and L. Rendell, </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA (1993), </address> <pages> 252-259. 14 </pages>
Reference-contexts: Work As far as constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the LFC algorithm <ref> [15] </ref>, the ID2-of-3 algorithm [16], the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes. As far as systematic search is concerned, the closest related work is Opus [4].
Reference: [16] <author> P.M. Murphy and M.J. Pazzani, ID2-of-3: </author> <title> constructive induction of M-of-N concepts for dis-criminators in decision trees. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA (1991), </address> <pages> 183-187. </pages>
Reference-contexts: constructing new attributes for decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm <ref> [16] </ref>, the Lmdt algorithm [17], and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes. As far as systematic search is concerned, the closest related work is Opus [4]. Some ideas in CAT about the systematic search with pruning are from it.
Reference: [17] <author> C.E. Brodley and P.E. Utgoff, </author> <title> Multivariate versus univariate decision trees. </title> <type> COINS Technical Report 92-8, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1992. </year>
Reference-contexts: decision tree learning is concerned, related work includes the Fringe family of algorithms such as Fringe, Dual Fringe, Symmetric Fringe [2, 5], SymFringe, DCFringe [6], and SFringe [7], the Citre algorithm [1, 14], the CI algorithms 11 [3, 7], the LFC algorithm [15], the ID2-of-3 algorithm [16], the Lmdt algorithm <ref> [17] </ref>, and the XofN algorithm [10]. They use different constructive operators and different strategies to create new attributes. As far as systematic search is concerned, the closest related work is Opus [4]. Some ideas in CAT about the systematic search with pruning are from it.
Reference: [18] <author> R. Rymon, </author> <title> Search through systematic set enumeration. </title> <booktitle> Proceedings of the Third International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA (1992), </address> <pages> 539-550. </pages>
Reference-contexts: In addition, CAT searches for a conjunction as a new attribute for learning a decision tree, whereas Opus searches for a conjunction as a rule for learning a set of unordered rules. Rymon <ref> [18, 19] </ref> uses systematic search with pruning to learn SE-trees, a type of tree structure containing rules that predict classes using attributes. Schlimmer [20] adopts systematic search with pruning for inducing determinations that identify which factors influence others.
Reference: [19] <author> R. Rymon, </author> <title> An SE-tree based characterisation of the induction problem. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA (1993), </address> <pages> 268-275. </pages>
Reference-contexts: In addition, CAT searches for a conjunction as a new attribute for learning a decision tree, whereas Opus searches for a conjunction as a rule for learning a set of unordered rules. Rymon <ref> [18, 19] </ref> uses systematic search with pruning to learn SE-trees, a type of tree structure containing rules that predict classes using attributes. Schlimmer [20] adopts systematic search with pruning for inducing determinations that identify which factors influence others.
Reference: [20] <author> J.C. Schlimmer, </author> <title> Efficiently inducing determinations: a complete and systematic search algorithm that uses optimal pruning. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA (1993), </address> <pages> 284-290. </pages>
Reference-contexts: Rymon [18, 19] uses systematic search with pruning to learn SE-trees, a type of tree structure containing rules that predict classes using attributes. Schlimmer <ref> [20] </ref> adopts systematic search with pruning for inducing determinations that identify which factors influence others. Webb [21, 22] further explores the systematic search in a more general way.
Reference: [21] <author> G.I. Webb, </author> <title> OPUS: an efficient admissible algorithm for unordered search. </title> <journal> Journal of Artificial Intelligence Research, </journal> <month> 3 </month> <year> (1995) </year> <month> 431-465. </month>
Reference-contexts: Rymon [18, 19] uses systematic search with pruning to learn SE-trees, a type of tree structure containing rules that predict classes using attributes. Schlimmer [20] adopts systematic search with pruning for inducing determinations that identify which factors influence others. Webb <ref> [21, 22] </ref> further explores the systematic search in a more general way. In addition, Webb [22] proposes a few new pruning rules for systematic search. 6 Conclusions and Future Work This paper has investigated a dynamic path-based approach to constructing new attributes for decision tree learning.

References-found: 21

