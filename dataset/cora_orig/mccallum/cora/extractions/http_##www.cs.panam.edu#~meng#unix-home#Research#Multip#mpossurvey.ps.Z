URL: http://www.cs.panam.edu/~meng/unix-home/Research/Multip/mpossurvey.ps.Z
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/Research/Multip/
Root-URL: http://www.cs.panam.edu
Email: (bodhi@cc.gatech.edu)  (schwan@cc.gatech.edu)  prabha@ssdc.honeywell.com)  
Title: A Survey of Multiprocessor Operating System Kernels (DRAFT)  
Author: Bodhisattwa Mukherjee Karsten Schwan Prabha Gopinath (gopinath 
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Date: 5 November 1993  
Pubnum: GIT-CC-92/05  
Abstract: Multiprocessors have been accepted as vehicles for improved computing speeds, cost/performance, and enhanced reliability or availability. However, the added performance requirements of user programs and functional capabilities of parallel hardware introduce new challenges to operating system design and implementation. This paper reviews research and commercial developments in multiprocessor operating system kernels from the late 1970's to the early 1990's. The paper first discusses some common operating system structuring techniques and examines the advantages and disadvantages of using each technique. It then identifies some of the major design goals and key issues in multiprocessor operating systems. Issues and solution approaches are illustrated by review of a variety of research or commercial multiprocessor operating system kernels. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jr. A. Tevanian. </author> <title> Architecture-Independent Virtual Memory Management for Parallel and Distributed Environments. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> December </month> <year> 1987. </year> <note> Technical Report CMU-CS-88-106. </note>
Reference-contexts: In effect, Accent carries into the domain of message passing systems the notion that I/O can be performed through virtual memory management. The design of the Mach system's memory management is largely derived from the Accent system <ref> [194, 1, 263, 262] </ref>. <p> Message: A message is a typed collection of data objects used for communication be tween active threads. 5. Memory Object: A memory object is a repository of data which can be mapped into the address space of a task. 4.3.1 Memory Management The Mach virtual memory management <ref> [84, 194, 1, 262, 263] </ref> system is designed to be architecture and operating system independent. Architecture independence is achieved by dividing the virtual memory implementation into machine independent and machine dependent portions. <p> Using these data structures, Mach supports large, sparse virtual address spaces and memory mapped files <ref> [1, 194, 244] </ref>. Mach implements a single level store by treating all primary memory as a cache for virtual memory objects. Mach allows tasks to allocate and deallocate regions of virtual memory, and to set the protection and inheritance of virtual memory regions.
Reference: [2] <author> V. Abrossimov, M. Rozier, and M. Shapiro. </author> <title> Generic virtual memory management for operating system kernels. </title> <booktitle> In Proceedings of the 12th symposium on operating systems principles (SIGOPS Notices vol.23, no.5), </booktitle> <pages> pages 123-36, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: All information important to the management of the virtual memory is maintained in machine-independent data structures and machine-dependent data structures contain only the mappings necessary to run the current mix of programs [194] (See Section 4.3 for more on the implementation of the Mach virtual memory management). Similarly, in <ref> [2] </ref>, the authors present the design and the implementation of a scalable, kernel-independent, Generic Memory Management Interface (GMI) (for the Chorus [197] nucleus) which is suitable for various architectures (e.g. paged and/or segmented) and implementation schemes.
Reference: [3] <author> Mike Accetta, Robert Baron, David Golub, Richard Rashid, Avadis Tevanian, and Michael Young. </author> <title> Mach: A new kernel foundation for unix development. </title> <booktitle> In Proceedings of the Summer 1986 Usenix Conference, </booktitle> <pages> pages 93-112, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Moreover, the reconfiguration module dynamically configures the system to handle hardware and software faults. For example, if the physical environment changes (e.g., addition or removal of clusters), StarOS can be expanded or reduced to accommodate such changes. 4.3 Mach Mach <ref> [3, 191, 242, 243, 95, 192, 15] </ref> is a multiprocessor operating system kernel developed at Carnegie-Mellon University first for distributed systems, then for tightly-coupled UMA multiprocessors. Later extensions of Mach also address NUMA and NORMA machines [255].
Reference: [4] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For example, some memory models exploit the fact that synchronization is used to control access to shared state (e.g., properly labeled memory [90] or data race-free memory <ref> [4, 5] </ref>). This allows the underlying system to weaken memory consistency requirements. The resulting, weakened shared memory abstraction presented to programmers may be implemented efficiently because strong consistency and therefore, interprocessor communication is not required for all memory accesses.
Reference: [5] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <type> Technical Report 1051, </type> <institution> Department of Computer Science, University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: For example, some memory models exploit the fact that synchronization is used to control access to shared state (e.g., properly labeled memory [90] or data race-free memory <ref> [4, 5] </ref>). This allows the underlying system to weaken memory consistency requirements. The resulting, weakened shared memory abstraction presented to programmers may be implemented efficiently because strong consistency and therefore, interprocessor communication is not required for all memory accesses.
Reference: [6] <author> G. S. Almasi and A. Gottlieb. </author> <title> Highly parallel computing. </title> <address> Benjamin/Cummings, Redwood City, Calif., </address> <year> 1989. </year>
Reference-contexts: Brief survey of multiprocessor hardware. Several current textbooks in parallel computing provide good overviews of parallel machine architectures <ref> [111, 6, 233, 188] </ref>. For purposes of this paper, we briefly review some of the major types of parallel machines, eliding architectures for SIMD programs, functional programs, and systolic applications. <p> An interconnection network, which may be static or dynamic, facilitates communication among processors and memory modules. A few sample interconnection networks are: time shared or common buses, crossbar switches, hierarchical switches, and multistage networks. The design, structure and performance of various interconnection networks have been reviewed in other literature <ref> [264, 111, 6, 233, 188] </ref> and are beyond the scope of this survey. The variety of different kinds of multiprocessor architectures coupled with diverse application requirements have resulted in many different designs, goals, features, and implementations of multiprocessor operating systems, in university research projects and in the commercial domain. <p> Mach is the base implementation of Unix on the RP3 15 . Like Mach, Mach/RP3 also has a master processor, which is reserved for Unix system call service. 15 A second operating system for the RP3 developed at NYU was never deployed on the actual machine <ref> [6] </ref>. 49 4.13.1 Process Management and Scheduling Mach/RP3 supports a version of co-scheduling or gang scheduling known as family scheduling. A family is a set of cooperating processes (possibly exchanging messages, sharing part or all of their address space, synchronizing often) working towards a single goal.
Reference: [7] <author> R. Ananthanarayanan, Mustaque Ahamad, and Richard J. LeBlanc. </author> <title> Application specific coherence control for high performance distributed shared memory. </title> <booktitle> In Proceedings of the 3rd USENIX Symposium on Experience with Distributed and Multiprocessor Systems (SEDMS III), </booktitle> <pages> pages 109-28, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Other models of shared memory exploit programmer directives to reduce the cost of coherence maintenance [19], or they provide explicit primitives with which users can maintain application-specific notions of coherence of shared state <ref> [7] </ref> (see Section 3.2.2 for more information on this research). One interesting lesson learned from current work on weakly consistent memories is that message passing and shared memory need not be different or mutually exclusive mechanisms. <p> Other models of shared memory developed for distributed architectures exploit programmer directives to reduce the cost of coherence maintenance [50], or they provide explicit primitives with which users can maintain application-specific notions of coherence of shared state <ref> [7] </ref>. Mechanisms for memory or state sharing have significant effects on the performance of parallel and distributed applications.
Reference: [8] <author> T. E. Anderson. </author> <title> The perfomance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Moreover, instructions such as fetch-and-add [97] allow certain common operations to be performed in parallel without critical sections [126]. Other work has evaluated the effects of other kernel components <ref> [8, 9, 10, 99] </ref> as well as applications on synchronization.
Reference: [9] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> Transactions on Computer Systems, ACM, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: A thread management operation does not require an expensive kernel call. Furthermore, lightweight threads enable an application program to use a thread management system, most appropriate to the problem domain. Mach Cthreads [60, 173, 212], the University of Washington threads <ref> [165, 9] </ref>, SunOS LWP and threads [113, 127, 189], are a few popular lightweight thread implementations. A lightweight thread generally executes in the context of a middleweight or a heavyweight thread. <p> This implies that user-level threads built on top of kernel-level threads are actually scheduled by the kernel's thread scheduler, which has little or no knowledge of the application's scheduling requirements or current state <ref> [9] </ref>. Problems with multi-level scheduling arise from the lack of information flow between different scheduling levels. Anderson et al. in [9] attempt to solve these problems for two-level scheduling by: 1. <p> user-level threads built on top of kernel-level threads are actually scheduled by the kernel's thread scheduler, which has little or no knowledge of the application's scheduling requirements or current state <ref> [9] </ref>. Problems with multi-level scheduling arise from the lack of information flow between different scheduling levels. Anderson et al. in [9] attempt to solve these problems for two-level scheduling by: 1. Explicit vectoring of kernel events to the user level thread scheduler, using upcalls called scheduler activations 1 , and by 1 Scheduler activations are defined to be entities similar to kernel-level threads. <p> Moreover, instructions such as fetch-and-add [97] allow certain common operations to be performed in parallel without critical sections [126]. Other work has evaluated the effects of other kernel components <ref> [8, 9, 10, 99] </ref> as well as applications on synchronization.
Reference: [10] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: A recent paper by Anderson et al. <ref> [10] </ref> also explores data structure alternatives when implementing user-level thread packages. Alternate implementations are evaluated in performance for thread run queues, idle processor queues, and for spinlock management. <p> When using a blocking lock, a waiting process (also called a contender process) blocks until awakened by the process releasing the lock. Such locks are also known as mutex locks. Anderson et al. <ref> [10] </ref> compare the performance of a number of software spin-waiting algorithms. They also propose a few efficient spin-waiting algorithms such as Ethernet style backoff algorithm (introducing delay between successive spins analogous to Ethernet's back-off or Aloha), software queueing of spinning processors, and others. <p> Moreover, instructions such as fetch-and-add [97] allow certain common operations to be performed in parallel without critical sections [126]. Other work has evaluated the effects of other kernel components <ref> [8, 9, 10, 99] </ref> as well as applications on synchronization.
Reference: [11] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (SIGPLAN Notices vol.26, no.4), </booktitle> <pages> pages 108-20, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Namely, since one main reason for using parallel hardware is to improve the performance of large-scale application programs, an operating system or system constructs that perform poorly are simply not acceptable <ref> [11, 89] </ref>.
Reference: [12] <author> A. Appel and K. Li. </author> <title> Virtual memory primitives for user programs. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (SIGPLAN Notices, vol.26, no.4), </booktitle> <pages> pages 96-107, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Some operating systems [84] allow applications to specify the protection level (inaccessible, read-only, read-write) of pages, and allow user programs to handle protection violations. In <ref> [12] </ref>, the authors survey several user-level algorithms that make use of page-protection techniques, and analyze their common characteristics, in an attempt to identify the virtual-memory primitives the operating system should provide to user processes.
Reference: [13] <author> J. Archibald. </author> <title> The Cache Coherence Problem in Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> February </month> <year> 1987. </year>
Reference-contexts: PLATINUM's implementation of coherent memory replicates and migrates data to the processors using it, thus creating the appearance that memory is uniformly and rapidly accessible. The protocol for controlling the data movement is derived by extending a directory-based cache coherency algorithm using selective invalidation <ref> [13] </ref>. 6 PLATINUM is an acronym for Platform for Investigating Non-Uniform Memory 21 Page Placement: Other than the Kendall Square Research Corporation's KSR machines [195] and the experimental Dash multiprocessors [90], NUMA multiprocessors do not have broadcast, invalidate, or snooping mechanisms that maintain consistency among multiple copies of a page when
Reference: [14] <author> R. Atkinson, A. Demers, C. Hauser, C. Jacobi, P. Kessler, and M. Weiser. </author> <title> Experiences creating a portable cedar. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 322-8, </pages> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Another inherent problem with language based systems can be protection, where language-level typing mechanisms must be mapped to the protection mechanisms available in the underlying operating system <ref> [14, 254] </ref>, which is not always easily done. In addition, any language based system will require all cooperating modules to be written in the same language, which precludes the use of mixed language environments.
Reference: [15] <author> R. Baron, D. Black, W. Bolosky, J. Chew, R. Draves, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> Mach Kernel Interface Manual. </title> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Moreover, the reconfiguration module dynamically configures the system to handle hardware and software faults. For example, if the physical environment changes (e.g., addition or removal of clusters), StarOS can be expanded or reduced to accommodate such changes. 4.3 Mach Mach <ref> [3, 191, 242, 243, 95, 192, 15] </ref> is a multiprocessor operating system kernel developed at Carnegie-Mellon University first for distributed systems, then for tightly-coupled UMA multiprocessors. Later extensions of Mach also address NUMA and NORMA machines [255].
Reference: [16] <author> Forest Baskett, John Howard, and John Montague. </author> <title> Task communication in demos. </title> <booktitle> Proceedings of the Sixth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 23-31, </pages> <month> Nov. </month> <year> 1977. </year>
Reference-contexts: A process in the target address space receives the message, interprets it and services the request. THE [72], Thoth [56], and Demos <ref> [16] </ref> are a few examples of the earliest message passing systems. The primary motivation behind the design of these systems was to decentralize the structure of an operating system running on a single computer. <p> The horizontal organization leads to a compartmentalization of the kernel in which all synchronization is subsumed by message passing. The horizontal organization closely mimics the hardware organization of a distributed memory multicomputer. Demos <ref> [16] </ref> and Minix [239] are examples of horizontal kernels. 2.7 Micro-kernel Based Operating Systems Micro-kernel based operating systems are structured as a collection of system servers running on top of a minimal kernel (see Figure 6).
Reference: [17] <author> Gerard M. Baudet. </author> <title> The Design and Analysis of Algorithms for Asynchronous Multiprocessors. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie-Mellon University, </institution> <month> April </month> <year> 1978. </year>
Reference-contexts: the major contributions of HYDRA address protection issues, there are also many research results relevant to parallel systems, including: * basic results establishing the concept of program locality, the effects of bus and memory contentions on parallel program performance, and the importance of asynchrony in parallel program design and implementation <ref> [258, 17, 181] </ref>, * demonstrations that high performance parallel programs require choices in the operating system mechanisms being provided (e.g., a variety of different synchronization mechanisms), since programs differ in granularities of parallelism, in frequencies of access to shared data, etc., and * experiences with the use of specific operating system
Reference: [18] <author> J. Bennett. </author> <title> The design and implementation of distributed smalltalk. </title> <booktitle> In OOPSLA'87 Conference Proceedings, </booktitle> <pages> pages 318-330, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: CLU [148], Eden [134], Distributed Smalltalk <ref> [18] </ref>, Emerald [125], and Linda [49] are a few examples of languages that integrate message based communication into the programming environment, either by defining all control structures in terms of messages, or by using messages as the basis for building Algol-like or entirely new control structures.
Reference: [19] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, Seattle, (SIGPLAN Notices, vol.25, no.3), </booktitle> <pages> pages 168-76. </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1990. </year>
Reference-contexts: Interestingly, recent research is beginning to address this dichotomy by providing a basic `memory' abstraction for representation of both local and remote memory, and by addressing the potential performance penalties arising from providing this abstraction by `weakening' the strong consistency requirements imposed on main memory <ref> [19, 145] </ref>. The resulting, weakened shared memory abstraction presented to programmers may be implemented efficiently because strong consistency and therefore, in-terprocessor communication is not required for all memory accesses. <p> The resulting, weakened shared memory abstraction presented to programmers may be implemented efficiently because strong consistency and therefore, in-terprocessor communication is not required for all memory accesses. Other models of shared memory exploit programmer directives to reduce the cost of coherence maintenance <ref> [19] </ref>, or they provide explicit primitives with which users can maintain application-specific notions of coherence of shared state [7] (see Section 3.2.2 for more information on this research).
Reference: [20] <author> B. Bershad. </author> <title> The increasing irrelevance of ipc performance for microkernel-based operating systems. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 205-211, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: All higher level operating system services are implemented as user-level programs. Therefore, applications must use cross-address space RPC to interact with most operating system services. This implies that the performance of inter-process communication (IPC) mechanism plays a critical role in the performance of such operating systems <ref> [20] </ref>. The primary characteristic of micro-kernel based operating systems is modularity, thereby hoping to improve system extensibility, portability, reconfigurability, and improved support 10 for distribution [251, 93, 92].
Reference: [21] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year> <booktitle> Also appeared in Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Otherwise, the same basic paradigm for control and data transfer is used. Messages are sent by way of the kernel between independent threads bound to different address space. The use and performance of cross-address space RPC is discussed extensively in [25]. In <ref> [21] </ref>, Bershad et al. propose a new kernel-based communication facility called Lightweight Remote Procedure Call (LRPC) which is designed and optimized for communication between address space on the same machine.
Reference: [22] <author> B. Bershad, T. Anderson, E. Lazowska, and H. Levy. </author> <title> User-level interprocess communication for shared memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2) </volume> <pages> 175-198, </pages> <month> May </month> <year> 1991. </year> <month> 54 </month>
Reference-contexts: LRPC combines the control transfer and communication model of capability systems with the programming semantics and large-grained protection 9 It is possible to have more than one kernel in a NUMA machine. 27 model of RPC. In <ref> [22] </ref>, Bershad et al. propose another interprocess communication scheme, called User-level Remote Procedure Call (URPC). URPC decouples processor allocation from data transfer and thread management by combining fast cross-address space communication protocol using shared-memory with lightweight threads managed entirely at the user level.
Reference: [23] <author> B. Bershad, E. Lazowska, and H. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software: Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Examples of such uses are the internally parallel operating system servers offered in the Eden system [134] or in CHAOS [213, 92] and Presto <ref> [24, 23] </ref>, the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro [227, 229, 98] for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines. <p> On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel [163, 162], the Psyche system and its related research [219, 168], Presto <ref> [24, 23] </ref>, and others [24, 23, 52]. <p> On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel [163, 162], the Psyche system and its related research [219, 168], Presto [24, 23], and others <ref> [24, 23, 52] </ref>. <p> The kernel maintains relevant information (e.g., load on each physical processor, mapping between virtual and physical processors, states of virtual processors etc.) in magic pages, which it updates periodically. Such information facilitate decisions regarding user-level scheduling. 4.6 PRESTO PRESTO <ref> [24, 23] </ref>, developed at the University of Washington, is a set of tools for building parallel programs on shared-memory multiprocessors. PRESTO is not an operating system in the true sense. Instead, it is implemented on top of an existing operating system.
Reference: [24] <author> B. Bershad, E. Lazowska, H. Levy, and D. Wagner. </author> <title> An open environment for building parallel programming systems. </title> <booktitle> In Proceedings of the Symposium on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <pages> pages 1-9, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Examples of such uses are the internally parallel operating system servers offered in the Eden system [134] or in CHAOS [213, 92] and Presto <ref> [24, 23] </ref>, the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro [227, 229, 98] for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines. <p> On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel [163, 162], the Psyche system and its related research [219, 168], Presto <ref> [24, 23] </ref>, and others [24, 23, 52]. <p> On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel [163, 162], the Psyche system and its related research [219, 168], Presto [24, 23], and others <ref> [24, 23, 52] </ref>. <p> The kernel maintains relevant information (e.g., load on each physical processor, mapping between virtual and physical processors, states of virtual processors etc.) in magic pages, which it updates periodically. Such information facilitate decisions regarding user-level scheduling. 4.6 PRESTO PRESTO <ref> [24, 23] </ref>, developed at the University of Washington, is a set of tools for building parallel programs on shared-memory multiprocessors. PRESTO is not an operating system in the true sense. Instead, it is implemented on top of an existing operating system. <p> The thread then becomes active. 4.6.1 Customization The lowest levels of PRESTO's runtime kernel can be modified and/or extended by an application, if required. However, PRESTO does not support the idea of reconfiguration at the operating system kernel level. As stated by the authors in <ref> [24] </ref>: ... it is infeasible for an operating system to permit easy redefinition of the concepts of a processor, scheduler, lock or even a thread. These are the most basic components of an operating system, and allowing users the freedom to change them could result in chaos. ....
Reference: [25] <author> Brian N. Bershad. </author> <title> High performance cross-address space communication. </title> <type> Technical Report 90-06-02, </type> <institution> Dept. of Computer Science and Eng., University of Washington, </institution> <month> June </month> <year> 1990. </year> <type> Ph.D. dissertation. </type>
Reference-contexts: Its maintainability, ex-pandability, adaptability, and portability strongly depend on its internal structure. Different techniques for structuring operating systems <ref> [25] </ref> are described in this section, along with a discussion of some of the effects of such structuring on the ease with which an operating system can be adapted for use with multiprocessor computing engines. <p> Furthermore, in order to guarantee the integrity of a system based on language-level decomposition, any executable code must be inspected by a trusted system entity to guarantee type-safety at runtime, es 7 sentially requiring access to its source code <ref> [25] </ref>. Finally, all language based systems will still require the availability of lower-level runtime system support for efficient program execution, as clearly apparent from current efforts to develop a threads-like common runtime system layer for both high performance Fortran and concurrent C++. <p> RPC systems require that the data passed among cooperating modules be strongly typed; within a module, a programmer is free to mix languages, use weakly typed or untyped languages, violate typing if needed, and execute code for which source is not available <ref> [25] </ref>. RPC is used for both local and remote communication between address space. An RPC between address spaces on different machines is often referred to as a cross-machine RPC whereas an RPC between address spaces on the same machine is referred to as a cross-address space RPC. <p> The advantages of middleweight threads are: * The kernel can directly schedule an application's thread on the available physical pro cessors. * Kernel-level threads offer a general programming interface to the application. Kernel-level threads also exhibit some problems that can make them impractical for use in fine-grained parallel programs <ref> [25] </ref>: * The cost of generality of kernel-level threads is not acceptable to fine grained parallel applications. <p> comparisons of message passing with direct use of shared memory often result in inconclusive results, in part because such results strongly depend on the sizes, granularities, and frequencies of communications in parallel programs [138]. 3.4.2 Remote Procedure Calls Most recent shared-memory multiprocessor operating systems support cross-address space remote procedure calls <ref> [25] </ref> (RPC) as a means of inter-process communication. RPC is a higher level abstraction than message passing. It hides the message communication layer beneath a procedure call layer. RPC allows efficient and secure communications. <p> Otherwise, the same basic paradigm for control and data transfer is used. Messages are sent by way of the kernel between independent threads bound to different address space. The use and performance of cross-address space RPC is discussed extensively in <ref> [25] </ref>. In [21], Bershad et al. propose a new kernel-based communication facility called Lightweight Remote Procedure Call (LRPC) which is designed and optimized for communication between address space on the same machine.
Reference: [26] <author> T. Bihari and K. Schwan. </author> <title> A comparison of four adaptation algorithms for increasing the reliability of real-time software. </title> <booktitle> In Proceedings of the Ninth Real-Time Systems Symposium, </booktitle> <address> Huntsville, AL, </address> <pages> pages 232-241. </pages> <publisher> IEEE, </publisher> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: must have direct access to the underlying resources typically controlled by the operating system, and for complex applications, it must deal with uncertainty in operating environments by even permitting programs or operating system components to adapt [215, 129] (i.e., change at runtime) in performance [209] and functionality during system execution <ref> [210, 213, 26, 96] </ref>. While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs.
Reference: [27] <author> T. Bihari and K. Schwan. </author> <title> Dynamic adaptation of real-time software. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(2) </volume> <pages> 143-174, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: is because in contrast to other parallel or distributed application software, the control software of real-time systems cannot be termed reliable unless it exhibits two key attributes [209]: (1) computations must complete within well-defined timing constraints, and (2) programs must exhibit predictable behavior in the presence of uncertain operating environments <ref> [210, 27] </ref>.
Reference: [28] <author> Kenneth P. Birman and et.al. </author> <title> Implementing fault-tolerant distributed objects. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 502-508, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: One self-imposed limitation of this survey is its focus on performance rather than reliability in parallel systems. Reliable systems are surveyed in several recent articles, including <ref> [237, 28, 149] </ref>. A second limitation of this survey is its treatment of operating system kernels rather than operating systems, thereby neglecting system functionalities like file systems, database support, network protocols, and others. <p> A second major reason for using a multiprocessor system is to provide high reliability, and graceful degradation in the event of failure. Hence, several multiprocessor systems have been constructed and designed for improved fault tolerance. Such systems will not be discussed in this survey (see <ref> [28] </ref> for a survey of fault tolerant operating systems).
Reference: [29] <author> A. Birrell, J. Guttag, J. Horning, and R. Levin. </author> <title> Synchronization primitives for a multiprocessor: A formal specification. </title> <booktitle> In Proceedings of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 94-102. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1987. </year>
Reference-contexts: More complicated event structures have been shown useful for several application domains and target machines, most prominently including the event handling facilities for active messages [151, 211] or the synchronization points designed by Gheith for real-time applications [92]. In <ref> [29] </ref>, Birrell et al. present an informal description, implementations and formal specifications of various thread synchronization primitives (such as Acquire and Release, Condition variables, semaphores) supported by the Taos operating system. 3.4 Interprocess Communication 3.4.1 Basic Communication Primitives Cooperating processes or threads in a multiprocessor environment often communicate and synchronize.
Reference: [30] <author> A. Birrell and B. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: It is precisely the structuring of such support that is the concern of this paper. Remote procedure calls. Systems supporting Remote Procedure Calls (RPC) <ref> [30] </ref> occupy a middle ground between message based and single language systems. The use of RPC allows isolated components to be transparently integrated into a single logical system. In an RPC system, a procedure call interface hides the underlying communication mechanism that passes typed data objects among address spaces.
Reference: [31] <author> P. Biswas, K. Ramakrishnan, D. Towsley, and C. Krishna. </author> <title> Performance analysis of distributed file systems with non-volatile caches. </title> <booktitle> In Proceedings of the 2nd International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 252-262, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: It includes industry efforts to offer concurrent I/O or file system support [114, 115] or even concurrent databases on parallel machines [195], work on communication protocols for high performance and parallel machines [110, 147], and research efforts addressing efficient file management on parallel machines <ref> [89, 31] </ref>.
Reference: [32] <author> D. Black, D. Golub, D. Julin, R. Rashid, R. Draves, R. Dean, A. Forin, J. Barrera, H. Tokuda, G. Malan, and D. Bohman. </author> <title> Microkernel operating system architectures and mach. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 11-30, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Furthermore, the use of common underlying services provides support for the coexistence and interoperability of multiple operating system environments on a single host as user-level programs <ref> [32] </ref>. Mach [32], Chorus [197], KeyKOS [42], QNX [106], and BirLiX [208] are a few examples of micro-kernel based operating systems. 2.8 Application-specific Operating Systems Many application domains impose specific requirements on operating system functionality, performance, and structure. <p> Furthermore, the use of common underlying services provides support for the coexistence and interoperability of multiple operating system environments on a single host as user-level programs <ref> [32] </ref>. Mach [32], Chorus [197], KeyKOS [42], QNX [106], and BirLiX [208] are a few examples of micro-kernel based operating systems. 2.8 Application-specific Operating Systems Many application domains impose specific requirements on operating system functionality, performance, and structure. <p> Major changes include the optimization of its IPC implementation (by optimizing ports and port rights) as well as the use of new algorithms, the use of continuations 11 [74, 75, 146] in scheduling, IPC, exception, and new page fault handling facilities <ref> [32] </ref>. The basic facilities provided by the Mach kernel support the implementation of operating systems as Mach applications [32]. Figure 9 shows the organization of an application, the Unix Server and its relationship to the Mach kernel. <p> (by optimizing ports and port rights) as well as the use of new algorithms, the use of continuations 11 [74, 75, 146] in scheduling, IPC, exception, and new page fault handling facilities <ref> [32] </ref>. The basic facilities provided by the Mach kernel support the implementation of operating systems as Mach applications [32]. Figure 9 shows the organization of an application, the Unix Server and its relationship to the Mach kernel. The Unix server is implemented as a Mach task with multiple threads of control managed by the Mach Cthreads package. <p> The 11 A continuation is the address of a routine to call when a thread continues execution, plus a small data structure that contains local state needed by that routine 37 emulation library functions both as a translator for system service requests and as a cache for their results <ref> [32] </ref>. 4.4 Elmwood Elmwood [167, 137] is an object-oriented multiprocessor operating system designed and implemented at the University of Rochester.
Reference: [33] <author> D. Black, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> The mach exception handling facility. </title> <type> Technical Report CMU-CS-88-129, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: support for RPC [76, 122, 123]. * Support for remote file accesses between autonomous systems. 10 Mach is binary compatible with Berkley's Unix 4.3 bsd release 34 * Lightweight user-level threads known as Mach Cthreads [60, 173]. * Miscellaneous other support like debuggers for multithreaded applications [51], excep tion handling <ref> [33] </ref> etc. Structurally, Mach is organized horizontally (developed using micro-kernel technology). The Mach kernel is a minimal, extensible kernel which provides a small set of primitive functions. It provides a base upon which complete system environments may be built.
Reference: [34] <author> David L. Black. </author> <title> The mach cpu server: An implementation of processor allocation. </title> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: On top of the general message primitives, Mach implements various flavors of communication including server-client remote procedure calls, distributed object-oriented programming, and streams. 4.3.3 Scheduling The Mach scheduler <ref> [36, 34, 35] </ref> consists of two parts, one responsible for processor allocation, the other responsible for scheduling threads on individual processors. Processor Allocation. A user-level server performs processor allocation, using the mechanisms provided by the underlying Mach kernel. <p> A server performs processor allocation by assigning processors to the processor sets provided by the clients. Thus, clients have the power to use and manage processors without having direct 36 control over them. A server satisfies a client's requests in strict order in a greedy fashion <ref> [36, 34] </ref>. Thread Scheduling. Mach uses a priority based time-sharing scheduling technique within each processor set. Mach schedules individual threads without using any knowledge about the relationships among threads. The scheduler maintains a global run queue shared by the processors and a local run queue for each processor.
Reference: [35] <author> David. L. Black. </author> <title> Scheduling and Resource Management Techniques for Multiprocessors. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> July </month> <year> 1990. </year> <note> Techreport CMU-CS-90-152. </note>
Reference-contexts: Hence, programmers or operating systems restrict writable pages to a single copy. This gives rise to the page placement problem, which concerns the decision as to which local memory should contain the single page copy. Black et al. <ref> [35] </ref> refer to this problem as the migration problem. A similar problem is observed for read-only pages. Specifically, the replication problem is concerned with determining the set of local memories that should contain copies of a page. <p> On top of the general message primitives, Mach implements various flavors of communication including server-client remote procedure calls, distributed object-oriented programming, and streams. 4.3.3 Scheduling The Mach scheduler <ref> [36, 34, 35] </ref> consists of two parts, one responsible for processor allocation, the other responsible for scheduling threads on individual processors. Processor Allocation. A user-level server performs processor allocation, using the mechanisms provided by the underlying Mach kernel. <p> The scheduler maintains a global run queue shared by the processors and a local run queue for each processor. Each run queue is a priority queue of runnable threads. Priority calculation for threads is discussed in detail in <ref> [36, 35] </ref>. Mach is self scheduling a processor consults the run queue when it needs a thread to run.
Reference: [36] <author> David. L. Black. </author> <title> Scheduling support for concurrency and parallelism in the mach operating system. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Hand-off Scheduling: A kernel level scheduler that accepts user hints is described in <ref> [36] </ref>. Two kinds of hints exist: * Discouragement hints: A discouragement hint is used to discourage the scheduler to run the current thread. A discouragement hint may be either mild, strong, or weak. David Black in [36] discusses a scheduler that accepts discouragement hints. * Hand-off hints: A hand-off hint is <p> Hand-off Scheduling: A kernel level scheduler that accepts user hints is described in <ref> [36] </ref>. Two kinds of hints exist: * Discouragement hints: A discouragement hint is used to discourage the scheduler to run the current thread. A discouragement hint may be either mild, strong, or weak. David Black in [36] discusses a scheduler that accepts discouragement hints. * Hand-off hints: A hand-off hint is used to suggest the scheduler to run a specific thread. Using a hand-off hint, the current thread hands off the processor to another thread without intermediate scheduler interference. <p> Using a hand-off hint, the current thread hands off the processor to another thread without intermediate scheduler interference. Such schedulers are better known as hand off schedulers. two experiments with scheduling hints are described in <ref> [36] </ref>, and it is shown that scheduling hints can be used to improve program performance. <p> On top of the general message primitives, Mach implements various flavors of communication including server-client remote procedure calls, distributed object-oriented programming, and streams. 4.3.3 Scheduling The Mach scheduler <ref> [36, 34, 35] </ref> consists of two parts, one responsible for processor allocation, the other responsible for scheduling threads on individual processors. Processor Allocation. A user-level server performs processor allocation, using the mechanisms provided by the underlying Mach kernel. <p> A server performs processor allocation by assigning processors to the processor sets provided by the clients. Thus, clients have the power to use and manage processors without having direct 36 control over them. A server satisfies a client's requests in strict order in a greedy fashion <ref> [36, 34] </ref>. Thread Scheduling. Mach uses a priority based time-sharing scheduling technique within each processor set. Mach schedules individual threads without using any knowledge about the relationships among threads. The scheduler maintains a global run queue shared by the processors and a local run queue for each processor. <p> The scheduler maintains a global run queue shared by the processors and a local run queue for each processor. Each run queue is a priority queue of runnable threads. Priority calculation for threads is discussed in detail in <ref> [36, 35] </ref>. Mach is self scheduling a processor consults the run queue when it needs a thread to run.
Reference: [37] <author> Ben A. Blake and Karsten Schwan. </author> <title> Experimental evaluation of a real-time scheduler for a multiprocessor system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 34-44, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Before a job is assigned one or more physical processors, the scheduler checks whether the system can satisfy the job's timing constraints. This analysis is known as schedulability analysis. Schedulability analysis and scheduling for real time systems <ref> [53, 269, 37, 108, 150, 71, 268] </ref> are active areas of research and are not within the scope of this paper. 3.2 Memory Management Memory management for UMA multiprocessors is conceptually similar to that for multipro-grammed uniprocessors.
Reference: [38] <author> Toby Bloom. </author> <title> Dynamic Module Replacement in a Distributed Programming System. </title> <type> PhD thesis, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, MIT/LCS/TR-303, </institution> <month> March </month> <year> 1983. </year> <month> 55 </month>
Reference-contexts: Classical synchronization primitives like semaphores [72], monitors [107, 131] etc. are widely discussed in the earlier literature and are therefore, not described here. Also not discussed are more complex synchronization mechanisms like path expressions and serial-izers <ref> [38] </ref>, in part because such mechanisms are not in widespread use. Instead, this section briefly reviews some common and efficient synchronization constructs supported by recent multiprocessor operating systems. 3.3.1 Locks A lock is a shared data structure used to enforce mutual exclusion.
Reference: [39] <author> S.H. Bokhari. </author> <title> Dual processor scheduling with dynamic reassignment. </title> <journal> IEEE Transac--tions on Software Engineering, </journal> <volume> SE-5(4):341-349, </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: Processor load balancing 2 is considered to be a part of a scheduling policy [232]. Basic theoretical results on static process scheduling on parallel machines show that the scheduling problem is NP-hard; static algorithms minimizing average response time include those described in [164] and <ref> [39] </ref>. Other scheduling algorithms appear in [267] and [141]. In this section, we focus on dynamic scheduling [164], and on scheduling for shared memory machines, where variations in distances between different processors on the parallel machine [39, 214] are not considered. <p> Other scheduling algorithms appear in [267] and [141]. In this section, we focus on dynamic scheduling [164], and on scheduling for shared memory machines, where variations in distances between different processors on the parallel machine <ref> [39, 214] </ref> are not considered. Static and Dynamic Scheduling: A static scheduler makes a one time decision per job regarding the number of processors to be allocated. Once decided, the job is guaranteed to have exactly that number of processors whenever it is active.
Reference: [40] <author> W. Bolosky, R. Fitzgerald, and M. Scott. </author> <title> Simple but effective techniques for numa memory management. </title> <booktitle> In Proceedings of the twelfth ACM symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: A similar problem is observed for read-only pages. Specifically, the replication problem is concerned with determining the set of local memories that should contain copies of a page. Here, the assumption is made that the set of local memories that contain copies of the page is monotonically non-decreasing. In <ref> [40] </ref>, the authors present the implementation of a page placement mechanism to automatically assign pages of virtual memory to appropriately located physical memory in the Mach operating system on the IBM ACE multiprocessor workstation. <p> The simple strategy for page replacement uses local memory as a cache over global, managing consistency with a directory-based ownership protocol similar to that used by Li [145] for distributed shared virtual memory. Their experience <ref> [40] </ref> indicates that even simple automatic strategies can produce nearly optimal page replacement. It also suggests that the dominant remaining source of avoidable performance degradation is false sharing 7 [41], which can be reduced by improving language processors or by tuning applications. <p> Current research is exploring how shared memory may be represented on parallel or dis 7 An object that is not writably shared, but that is on a writably shared page is falsely shared <ref> [40] </ref>. 8 Excessive page movement is controlled by freezing the page in place and forcing remote accesses [61] 22 tributed machines such that its performance can approximate that of message passing systems.
Reference: [41] <author> W. Bolosky and M. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <pages> pages 57-71, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Their experience [40] indicates that even simple automatic strategies can produce nearly optimal page replacement. It also suggests that the dominant remaining source of avoidable performance degradation is false sharing 7 <ref> [41] </ref>, which can be reduced by improving language processors or by tuning applications.
Reference: [42] <author> A. Bomberger, N. Hardy, A. Frantz, C. Landau, W. Frantz, J. Shapiro, and A. </author> <title> Hardy. </title> <booktitle> The keykos nanokernel architecture. In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 95-112, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Furthermore, the use of common underlying services provides support for the coexistence and interoperability of multiple operating system environments on a single host as user-level programs [32]. Mach [32], Chorus [197], KeyKOS <ref> [42] </ref>, QNX [106], and BirLiX [208] are a few examples of micro-kernel based operating systems. 2.8 Application-specific Operating Systems Many application domains impose specific requirements on operating system functionality, performance, and structure.
Reference: [43] <author> Ronald F. Brender and Isaac R. Nassi. </author> <title> What is ada? IEEE Computer Magazine, </title> <booktitle> 14(6) </booktitle> <pages> 17-25, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: However, while such optimizations can alleviate certain performance problems with this approach, specific language primitives will inevitably impose performance penalties of which programmers must be aware in order to write efficient parallel programs. For example, the Ada rendezvous mechanism leads to well-known performance problems <ref> [43] </ref>, the global addressing mechanisms and fixed language semantics of Linda can lead to inefficiencies concerning the update and access of remote information [211], and heap maintenance has been shown difficult for languages like Smalltalk [260, 186].
Reference: [44] <author> R. Bryant, H. Y. Chang, and B. Rosenburg. </author> <title> Experience developing the rp3 operating system. </title> <booktitle> In Proceedings of the 2nd Usenix Symposium on Experience with Distributed and Multiprocessor Systems (SEDMSII), </booktitle> <pages> pages 1-18, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Because a NUMA architecture allows a large number of processors in a single machine, many experimental, large-scale multiprocessors are NUMA machines, an example being the IBM RP3 which was designed to contain up to 512 processors <ref> [45, 44] </ref>, and the KSR machine again offering up to 512 processors. NORMA multiprocessors are the simplest to design and build, and have become the architecture of choice for current supercomputers like the Intel Paragon [114, 115], recent Cray machines, and others. <p> Such libraries includes the Uniform System which is used to develop parallel programs for the Butterfly parallel system, a buffer management package for communicating applications, a stream-oriented i/o interface, and a library of performance measurement tools for parallel applications. 4.13 RP3 The RP3 <ref> [45, 44] </ref> is a NUMA shared memory multiprocessor developed at T. J. Watson Research Center, IBM. The RP3 hardware is designed to be scalable up to 512-way multiprocessing. A 64-way prototype machine was built and made operational.
Reference: [45] <author> R. Bryant, H. Y. Chang, and B. Rosenburg. </author> <title> Operating system support for parallel programming on rp3. </title> <journal> IBM Journal of R & D, </journal> <month> November </month> <year> 1991. </year>
Reference-contexts: Because a NUMA architecture allows a large number of processors in a single machine, many experimental, large-scale multiprocessors are NUMA machines, an example being the IBM RP3 which was designed to contain up to 512 processors <ref> [45, 44] </ref>, and the KSR machine again offering up to 512 processors. NORMA multiprocessors are the simplest to design and build, and have become the architecture of choice for current supercomputers like the Intel Paragon [114, 115], recent Cray machines, and others. <p> Such libraries includes the Uniform System which is used to develop parallel programs for the Butterfly parallel system, a buffer management package for communicating applications, a stream-oriented i/o interface, and a library of performance measurement tools for parallel applications. 4.13 RP3 The RP3 <ref> [45, 44] </ref> is a NUMA shared memory multiprocessor developed at T. J. Watson Research Center, IBM. The RP3 hardware is designed to be scalable up to 512-way multiprocessing. A 64-way prototype machine was built and made operational.
Reference: [46] <author> R. Campbell, N. Islam, and P. Madany. </author> <title> Choices, frameworks and refinement. </title> <journal> Computing Systems, </journal> <volume> 5(3) </volume> <pages> 217-57, </pages> <month> Summer </month> <year> 1992. </year>
Reference-contexts: KTK is extensible in that new abstractions and functionality (ie., classes, policies, and attributes) are easily added while potentially maintaining a uniform kernel interface (e.g., when not adding any new kernel classes) [92, 147]. 4.8 Choices The operating system family called Choices (Class Hierarchical Open Interface for Custom Embedded Systems) <ref> [46, 47, 199, 200, 203, 155] </ref> is part of the Embedded Operating System (EOS) project at the University of Illinois at Urbana-Champaign. The Choices kernel is implemented on a 10 processor Encore Multimax multiprocessor using the C++ language.
Reference: [47] <author> R. Campbell, G. Johnston, and V. Russo. </author> <title> Choices (class hierarchical open interface for custom embedded systems). </title> <journal> Operating Systems Review, </journal> <volume> 21(3) </volume> <pages> 9-17, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: An OOOS may or may not support objects at the user level. Examples of OOOSs are Choices <ref> [47] </ref> and Renaissance [202, 172]. Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. <p> An OSOS supporting passive objects offers an object-thread model where a single thread of execution traverses all objects within an invocation chain [68, 128]. One use of object orientation in operating systems is to exploit type hierarchies to achieve operating system configuration (e.g., as done in Choices <ref> [47] </ref>) along with stronger notions of structuring than available in current systems. Another use of this technology is to use object based encapsulations of operating system services in order to represent operating system services internally in different ways, invisibly to services users. <p> Measurements indicate that the standard blocking strategy performs poorly compared to mixed strategies. Among the mixed strategies studied, adaptive algorithms perform better than non-adaptive ones. A few object oriented-operating systems such as Choices <ref> [47] </ref> and Renaissance [202] take an object-oriented approach to lock configuration/customization. These systems define a few basic classes which provide simple and crude locks (implemented using hardware provided instructions). <p> One of the attributes of StarOS of interest to parallel systems is its definition of modules, functions, and module invocations, which are the blueprint for the implementation of similar functionality in the Intel 432's iMAX operating system [63] and in modern object-oriented operating systems like Eden [134], Choices <ref> [47] </ref>, and CHAOS [213, 92]. A module defines an object by exporting a set of invocable functions. A function invocation by a process is performed asynchronously by passing invocation parameters to a process designated to execute the function. <p> KTK is extensible in that new abstractions and functionality (ie., classes, policies, and attributes) are easily added while potentially maintaining a uniform kernel interface (e.g., when not adding any new kernel classes) [92, 147]. 4.8 Choices The operating system family called Choices (Class Hierarchical Open Interface for Custom Embedded Systems) <ref> [46, 47, 199, 200, 203, 155] </ref> is part of the Embedded Operating System (EOS) project at the University of Illinois at Urbana-Champaign. The Choices kernel is implemented on a 10 processor Encore Multimax multiprocessor using the C++ language. <p> A customized system is built using tailored kernel classes and derived germ classes to suit a particular hardware. 4.8.1 Tasks and Threads Choices implement threads in the kernel [200]. It supports multiple threads within a single task. Although they are called lightweight threads in <ref> [47] </ref>, according to the terminology introduced in this paper they are middleweight threads. A task is a collection of address spaces and active threads. 4.8.2 Interprocess Communication Communication in Choices is achieved mainly via shared memory. <p> Events: Events are asynchronous mechanisms generated by hardware (handled by event mechanism in germs) or by software (kernel provided). 2. Traps: Traps, generated by an executing thread, are handled by kernel-provided or user-provided trap handler objects. 4.9 Renaissance Renaissance [202], a predecessor of Choices <ref> [47, 199, 200, 203] </ref> operating system, is currently under development at Purdue University. It extends the ideas of Choices into a distributed object environment. The goal of Renaissance is to provide transparent access to remote objects that are distributed throughout a network of machines.
Reference: [48] <author> R. Campbell, V. Russo, and G. Johnston. </author> <title> The design of a multiprocessor operating system. </title> <booktitle> In Proceedings of the USENIX C++ Conference, </booktitle> <pages> pages 109-25, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Such systems may be broadly classified as object-oriented or object 8 supporting operating systems, depending on their internal structures and on the interfaces they provide to the user level [227, 198, 83]. Object-Oriented Operating Systems (OOOS). In an object-oriented operating system, an object encapsulates a system entity <ref> [156, 48] </ref>. An object-oriented language is primarily used to implement such an operating system [201]; the properties of the language such as data encapsulation, data abstraction, inheritance, polymorphism etc. are used to structure the system. An OOOS may or may not support objects at the user level.
Reference: [49] <author> Nicholas Carriero and David Gelernter. </author> <title> The s/net s linda kernel. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 110-129, </pages> <month> may </month> <year> 1986. </year>
Reference-contexts: CLU [148], Eden [134], Distributed Smalltalk [18], Emerald [125], and Linda <ref> [49] </ref> are a few examples of languages that integrate message based communication into the programming environment, either by defining all control structures in terms of messages, or by using messages as the basis for building Algol-like or entirely new control structures.
Reference: [50] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of munin. </title> <booktitle> In Proceedings of the thirteenth ACM symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The resulting, weakened shared memory abstraction presented to programmers may be implemented efficiently because strong consistency and therefore, interprocessor communication is not required for all memory accesses. Other models of shared memory developed for distributed architectures exploit programmer directives to reduce the cost of coherence maintenance <ref> [50] </ref>, or they provide explicit primitives with which users can maintain application-specific notions of coherence of shared state [7]. Mechanisms for memory or state sharing have significant effects on the performance of parallel and distributed applications.
Reference: [51] <author> D. Casewell and D. Black. </author> <title> Implementing a mach debugger for multithreaded applications. </title> <booktitle> In Proceedings of the Winter 1990 Usenix Technical Conference and Exhibition, </booktitle> <pages> pages 25-39, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: communication facility. * Language support for RPC [76, 122, 123]. * Support for remote file accesses between autonomous systems. 10 Mach is binary compatible with Berkley's Unix 4.3 bsd release 34 * Lightweight user-level threads known as Mach Cthreads [60, 173]. * Miscellaneous other support like debuggers for multithreaded applications <ref> [51] </ref>, excep tion handling [33] etc. Structurally, Mach is organized horizontally (developed using micro-kernel technology). The Mach kernel is a minimal, extensible kernel which provides a small set of primitive functions. It provides a base upon which complete system environments may be built.
Reference: [52] <author> J. Chase, F. Amador, E. Lazowska, H. Levy, and R. Littlefield. </author> <title> The amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems and Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel [163, 162], the Psyche system and its related research [219, 168], Presto [24, 23], and others <ref> [24, 23, 52] </ref>.
Reference: [53] <author> Sheng-Chang Cheng, John A. Stankovic, and Krithi Ramamritham. </author> <title> Scheduling algorithms for hard real-time systems a brief survey. </title> <booktitle> In Tutorial Hard Real-Time Systems, </booktitle> <pages> pages 150-173. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference-contexts: Before a job is assigned one or more physical processors, the scheduler checks whether the system can satisfy the job's timing constraints. This analysis is known as schedulability analysis. Schedulability analysis and scheduling for real time systems <ref> [53, 269, 37, 108, 150, 71, 268] </ref> are active areas of research and are not within the scope of this paper. 3.2 Memory Management Memory management for UMA multiprocessors is conceptually similar to that for multipro-grammed uniprocessors.
Reference: [54] <author> D. Cheriton. </author> <title> The v kernel: A software base for distributed systems. </title> <journal> IEEE Software, </journal> <volume> 1(2) </volume> <pages> 19-42, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: The primary motivation behind the design of these systems was to decentralize the structure of an operating system running on a single computer. On the other hand, the motivation behind the latter message passing systems such as RIG [191], V <ref> [54] </ref>, Accent [193], and various hypercube operating systems [205, 221] was to build an operating system on a structure of distributed computers. In contrast to the fine-grained protection of capability systems, network based message passing systems rely on a coarse-grained protection mechanism.
Reference: [55] <author> D. R. Cheriton. </author> <title> Problem-oriented shared memory: A decentralized approach to distributed system design. </title> <booktitle> Proceedings of the sixth. International Conference on Distributed Computing Systems, Cambridge,MA., </booktitle> <pages> pages 190-197, </pages> <month> May </month> <year> 1986. </year> <month> 56 </month>
Reference-contexts: Since differences in remote to local access times are even more pronounced in distributed machines like sets of workstations connected via high-speed networks <ref> [55] </ref>, notions of distributed objects have been a topic of research for such systems for quite some time, as evidenced by work on Clouds [69] at Georgia Tech, on Chorus [197], and on fragmented objects [227] in France. 3.3 Synchronization When multiple cooperating processes execute simultaneously, synchronization primitives are needed for
Reference: [56] <author> D.R. Cheriton, M.A. Malcolm, L.S. Melen, and G.R. Sager. Thoth, </author> <title> a portable real-time operating system. </title> <journal> Comm. of the Assoc. Comput. Mach., </journal> <volume> 22(2) </volume> <pages> 105-115, </pages> <month> Feb. </month> <year> 1979. </year>
Reference-contexts: When a process in one address space requests a service from another address space, it creates a message describing its requirements, and sends it to the target address space. A process in the target address space receives the message, interprets it and services the request. THE [72], Thoth <ref> [56] </ref>, and Demos [16] are a few examples of the earliest message passing systems. The primary motivation behind the design of these systems was to decentralize the structure of an operating system running on a single computer.
Reference: [57] <author> Christian Clemencon, Bodhisattwa Mukherjee, and Karsten Schwan. </author> <title> Distributed shared abstractions (dsa) on large-scale multiprocessors. </title> <booktitle> In Proc. of the Fourth USENIX Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 227-246. </pages> <publisher> USENIX, </publisher> <month> September </month> <year> 1993. </year>
Reference-contexts: Such operating systems, therefore, might be structured as collections of cooperating objects, where object invocations may result in messages, in memory sharing, or in both, and where objects themselves may internally be structured as collections of cooperating objects or even as fragmented object <ref> [226, 57, 211, 159] </ref>. 2.4 Language Based Mechanisms Single language systems. <p> Eden system [134] or in CHAOS [213, 92] and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro [227, 229, 98] for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' <ref> [57] </ref> for multiprocessor engines. Unresolved issues with object-oriented operating systems include the efficient representation of object invocations, where it has become clear that `not all invocations are equal', ranging from rapid unreliable invocations useful in real-time multiprocessor applications [213, 92] to reliable multicast invocations required for certain distributed programs [103]. <p> This is leading to redesigns of the Mach system paging mechanisms and page servers for its implementation on the Intel Paragon multiprocessor, and it is resulting in more general research on shared state or objects in distributed or shared memory parallel machine <ref> [211, 57] </ref>. <p> This is the ability provided by subcontracts in Spring, by attributes and policies in CHAOS, and by invocation attributes associated with individual accesses to fragmented objects built for hypercube machines in [211] and for multiprocessors in <ref> [57] </ref>. <p> Simulation studies of their hardware proposals show its usefulness in several applications, including dynamic load balancing, sorting, work distribution in DIB [82], etc. As with Livny and Manber's work, research on an active message construct called `topologies' on hypercubes [211] and `distributed shared abstractions' on shared memory multiprocessors <ref> [57] </ref> is based on previous work in message-based operating systems for hypercubes, since it is assuming the presence of reliable message delivery and of processes on the in 50 dividual nodes of the hypercube. <p> A sample object fragment as defined in [211] and in <ref> [57] </ref> appears in Figure 12. The basic idea of active messages is reflected here, in terms of user threads and service routines closely associated with the incoming or outgoing message buffers of the communication topology linking object fragments.
Reference: [58] <author> E. Cohen and D. Jefferson. </author> <title> Protection in the hydra operating system. </title> <booktitle> In Proceedings of the 5th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 141-160. </pages> <publisher> ACM, </publisher> <year> 1975. </year>
Reference-contexts: J. Watson Research Center, IBM), and a few successful commercial systems like Dynix (developed for Sequent Multiprocessors), Chrysalis (developed for BBN Butterfly machines), UMAX (developed for Encore Multimax multiprocessors). 4.1 HYDRA HYDRA <ref> [258, 256, 257, 259, 142, 58] </ref> is one of the earliest successful multiprocessor kernels, developed at Carnegie-Mellon University and implemented on the C.mmp hardware. Extensive descriptions of the HYDRA system appear in [120] and [258].
Reference: [59] <author> R. Colwell, E. Gehringer, and E. Jensen. </author> <title> Performance effects of architectural complexity in the intel 432. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(3) </volume> <pages> 296-339, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: CAP [179] and the Intel/432 [62] are examples of hardware based systems. Despite early favorable predictions, system builders have been largely unsuccessful in implementing and programming capability based systems which perform as well as machines based on more traditional memory reference models <ref> [132, 59] </ref>. This may be due to the fact that most early research and commercial systems focussed on the use of capabilities for enforcement of protection boundaries and system security characteristics, typically by enforcing the Principle of Least Privilege.
Reference: [60] <author> E. Cooper and R. Draves. </author> <title> C threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Dept. of Computer Science, Carnegie Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: User-level threads are managed by runtime library routines linked into each application. A thread management operation does not require an expensive kernel call. Furthermore, lightweight threads enable an application program to use a thread management system, most appropriate to the problem domain. Mach Cthreads <ref> [60, 173, 212] </ref>, the University of Washington threads [165, 9], SunOS LWP and threads [113, 127, 189], are a few popular lightweight thread implementations. A lightweight thread generally executes in the context of a middleweight or a heavyweight thread. <p> Machine independent virtual memory management [194]. * A capability based interprocess communication facility. * Language support for RPC [76, 122, 123]. * Support for remote file accesses between autonomous systems. 10 Mach is binary compatible with Berkley's Unix 4.3 bsd release 34 * Lightweight user-level threads known as Mach Cthreads <ref> [60, 173] </ref>. * Miscellaneous other support like debuggers for multithreaded applications [51], excep tion handling [33] etc. Structurally, Mach is organized horizontally (developed using micro-kernel technology). The Mach kernel is a minimal, extensible kernel which provides a small set of primitive functions.
Reference: [61] <author> A. Cox and R. Fowler. </author> <title> The implementation of a coherent memory abstraction on a numa multiprocessor: experiences with platinum. </title> <booktitle> In Proceedings of the twelfth ACM symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: A few such algorithms are discussed in [234], and some specific NUMA memory management schemes are described for the individual parallel operating systems in Section 4. The PLATINUM 6 operating system kernel, designed to be a platform for research on memory management systems for NUMA machines, implements and evaluates <ref> [61] </ref> a coherent memory abstraction on top of non-uniform access physical memory architectures. Since coherent memory is uniformly accessible from all processors in the system, it makes programming NUMA multiprocessors easier for users. <p> Current research is exploring how shared memory may be represented on parallel or dis 7 An object that is not writably shared, but that is on a writably shared page is falsely shared [40]. 8 Excessive page movement is controlled by freezing the page in place and forcing remote accesses <ref> [61] </ref> 22 tributed machines such that its performance can approximate that of message passing systems. For example, some memory models exploit the fact that synchronization is used to control access to shared state (e.g., properly labeled memory [90] or data race-free memory [4, 5]).
Reference: [62] <author> G. Cox, M. Corwin, K. Lai, and F. Pollack. </author> <title> Interprocess communication and processor dispatching on the intel 432. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 45-66, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: Parameter passing is by reference between the caller and the callee. The implementation of capability based addressing has been carried out using both software and hardware techniques. A few sample software based systems are Hydra [258] and Cal/Tss [235]. CAP [179] and the Intel/432 <ref> [62] </ref> are examples of hardware based systems. Despite early favorable predictions, system builders have been largely unsuccessful in implementing and programming capability based systems which perform as well as machines based on more traditional memory reference models [132, 59].
Reference: [63] <author> George Cox, William M. Corwin, Konrad K. Lai, and Fred J. Pollack. </author> <title> A unified model and implementation for interprocess communication in a multiprocessor environment. </title> <booktitle> In Proceedings of the 8th Symposium on Operating System Principles, Asilomar, </booktitle> <pages> pages 44-53. </pages> <publisher> Assoc. Comput. Mach., </publisher> <month> Dec. </month> <year> 1981. </year>
Reference-contexts: A similar approach to run-queue organization is taken in the Intel 432's iMAX operating system <ref> [63] </ref>. Recent work on real-time schedulers for parallel systems is also considering the effects of sharing alternative policy-level scheduling information on parallel scheduler performance [270]. <p> Exchange of messages is a more abstract form of communication than accessing shared memory locations. Message passing subsumes communication, buffering, and synchronization. 26 Multiprocessor operating systems have experimented with a large variety of different com-munication abstractions, including ports [84, 262] mailboxes <ref> [119, 63] </ref>, links [230, 127] etc. From an implementation point of view, such abstractions are kernel-handled message buffers. They may be either unidirectional or bidirectional. A process may send to or may receive messages from them. There may be rights (like send, receive, or ownership rights) associated with these entities. <p> Users may create new abstract types using these representation types. One of the attributes of StarOS of interest to parallel systems is its definition of modules, functions, and module invocations, which are the blueprint for the implementation of similar functionality in the Intel 432's iMAX operating system <ref> [63] </ref> and in modern object-oriented operating systems like Eden [134], Choices [47], and CHAOS [213, 92]. A module defines an object by exporting a set of invocable functions. A function invocation by a process is performed asynchronously by passing invocation parameters to a process designated to execute the function. <p> If the registration queue is empty and the mailbox is not full, a send buffers a message in the mailbox. A send fails if the mailbox is full. Communication mechanisms similar to those of StarOS can be found in the iMAX operating system <ref> [63] </ref>. 4.2.3 Scheduling Responsibility for scheduling in StarOS is divided between scheduler processes and the multiplexors. A multiplexor is a low level mechanism that makes short term decisions about which process to execute next on its processor, whereas a scheduler implements some specific scheduling policy. <p> Processor Allocation. A user-level server performs processor allocation, using the mechanisms provided by the underlying Mach kernel. The processor allocation facility adds two new objects to the Mach kernel the processor and the processor set. Much like the iMAX kernel's facilities <ref> [63] </ref>, a processor object manipulates the physical processors, whereas a processor set is an independent entity to which processors and threads can be assigned. An application creates a processor set, and uses it as the basis of communication with the server.
Reference: [64] <author> W. Crowther, J. Goodhue, R. Gurwitz, R. Rettberg, and R. Thomas. </author> <title> The butterfly(tm) parallel processor. </title> <type> Technical report, </type> <institution> BBN Laboratories Incorporated. </institution>
Reference-contexts: One type of switch is an interconnection network consisting of multiple levels of internal nodes, where systems are scaled by addition of internal switch nodes, as in the BBN Butterfly multiprocessors <ref> [130, 64] </ref>. A second type of switch consists of a hierarchical set of busses [121, 195], where access times to remote memory depend on either the number of internal switch nodes on the access path between the processor and the memory or on the number of traversed system busses. <p> Locks and semaphores are used hierarchically to prevent deadlock and indefinite postponement. 4.11.3 Memory Management Both versions of UMAX support demand-paged virtual memory to provide up to 16 megabytes of virtual address space per processor. 4.12 Chrysalis The Chrysalis operating system <ref> [64, 130, 81] </ref> provides a Unix-like environment on the Butterfly parallel processor. The Butterfly is a non-uniform memory access (NUMA) shared memory multiprocessor which uses an interconnection network as a processor/memory switch. Each machine node consists of a processor and its local memory.
Reference: [65] <author> S. Curran and M. Stumm. </author> <title> A comparison of basic cpu scheduling algorithms for multiprocessor unix. </title> <journal> Computing Systems, </journal> <volume> 3(4) </volume> <pages> 551-79, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: dynamic distribution of processing loads among the processors of the parallel machine. 14 so that a static processor allocation rapidly becomes inefficient and (2) large-scale parallel machines are often used in multi-user mode, so that scheduling must take into account the requirements of multiple parallel applications sharing a single machine <ref> [158, 157, 225, 65] </ref>. J. Zahorjan and C. McCann compare the performance of static and dynamic schedulers for multi-user workloads.
Reference: [66] <author> Dally, Chao, Chein, Hassoun, Horwat, Kaplan, Song, Totty, and Wills. </author> <title> Architecture of a message driven processor. </title> <booktitle> Proceedings of the 14th Annual International Symposium On Computer Architecture, </booktitle> <volume> 15(2) </volume> <pages> 189-196, </pages> <month> Jun </month> <year> 1987. </year>
Reference-contexts: Communication issues specific to hypercube or mesh machines are reviewed elsewhere [222, 221, 211, 67]. Examples of recent research in communication protocols for high performance or parallel machines are addressing the association of computational activities with messages <ref> [66, 151] </ref>, the user-driven configuration of communication protocols for improved performance [110], and the parallelization of protocol processing [147]. Of interest to this survey is that parallel programs typically use both message mechanisms and shared memory (when available) for inter-process communication. <p> The basic idea of active messages is to associate computing tasks with message transfers such that the system minimizes the delay between message arrival at some node and the initiation of computations enabled by that message. Dally at MIT is constructing hardware support for active message machines <ref> [66] </ref>. Livny and Manber explore a similar idea in their work on `active channels' [152, 151], in which a token ring communication protocol is extended such that three classes of operations could be performed directly on the node interfaces: arithmetic, selection, and counting. <p> Simpler and higher performance implementations of services or threads attached to messages are being implemented in systems like those designed by Dally at MIT <ref> [66] </ref>. In other work, slight generalizations of the functionality of the Crystalline operating system are the MOOSE operating system [205] designed at CalTech and the early commercial operating systems developed at Intel and NCube for their hypercube machines.
Reference: [67] <author> W. Dally and C. Seitz. </author> <title> The torus routing chip. </title> <journal> J. Distributed Computing, </journal> <volume> 1(4), </volume> <year> 1986. </year>
Reference-contexts: Communication issues specific to hypercube or mesh machines are reviewed elsewhere <ref> [222, 221, 211, 67] </ref>. Examples of recent research in communication protocols for high performance or parallel machines are addressing the association of computational activities with messages [66, 151], the user-driven configuration of communication protocols for improved performance [110], and the parallelization of protocol processing [147].
Reference: [68] <author> Partha Dasgupta, Richard J. LeBlanc, Mustaque Ahamad, and Umakishore Ramachan-dran. </author> <title> The CLOUDS distributed operating system. </title> <journal> IEEE Computer, </journal> <volume> 24(11) </volume> <pages> 34-44, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. Sample OSOSs are SOS [228], Cool [102], and CHAOS [213, 92, 91] for parallel machines, and Chorus [197] and Clouds <ref> [68] </ref> for distributed machines. OSOSs can further be classified into different groups depending on the kind of objects they support. In the active-server model of computation, objects are active entities containing threads of execution that service requests to the object [92, 91]. <p> In the active-server model of computation, objects are active entities containing threads of execution that service requests to the object [92, 91]. An OSOS supporting passive objects offers an object-thread model where a single thread of execution traverses all objects within an invocation chain <ref> [68, 128] </ref>. One use of object orientation in operating systems is to exploit type hierarchies to achieve operating system configuration (e.g., as done in Choices [47]) along with stronger notions of structuring than available in current systems. <p> Li at Yale showed that a modified Apollo Aegis kernel can support shared memory on a 10Mhz token ring [144], and similar results are demonstrated by the Clouds project at Georgia Tech <ref> [68] </ref>. Such work is motivated by the fact that parallel programming may be simplified when the underlying system provides a basic `memory' abstraction for representation of both local and shared state. <p> Processes do not have protected address spaces. A process executes the code associated with an object. Such orthogonality of processes and objects allow any number of processes executing simultaneously inside an object (much like is done in the Clouds operating system <ref> [68] </ref>). Any necessary synchronization must be provided by the object's implementation using synchronization primitives like semaphores and condition variables, both of which are implemented by Elmwood system objects.
Reference: [69] <author> Partha Dasgupta, Richard J. LeBlanc Jr., and William F. Appelbe. </author> <title> The clouds distributed operating system: Functional description, implementation details and related work. </title> <booktitle> In Proceedings of the 9th International Conference on Distributed Computing Systems, </booktitle> <address> San Jose, CA., </address> <pages> pages 2-9. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: Since differences in remote to local access times are even more pronounced in distributed machines like sets of workstations connected via high-speed networks [55], notions of distributed objects have been a topic of research for such systems for quite some time, as evidenced by work on Clouds <ref> [69] </ref> at Georgia Tech, on Chorus [197], and on fragmented objects [227] in France. 3.3 Synchronization When multiple cooperating processes execute simultaneously, synchronization primitives are needed for concurrency control. When multiple processes share an address space, synchronization is required for shared memory consistency [99].
Reference: [70] <author> J. Dennis and E. Van Horn. </author> <title> Programming semantics for multiprogrammed computations. </title> <journal> Communications of the ACM, </journal> <volume> 9(3), </volume> <month> March </month> <year> 1966. </year>
Reference-contexts: on Mach or on OSF Unix, both of which have smaller kernels and offer some facilities for kernel and operating system customization for different application domains and target machines (see Section 4.3 for a discussion of the Mach operating system's configuration support). 2.2 Capability Based Systems In a capability-based system <ref> [70] </ref>, each accessible entity exists in its own protection domain, but all entities reside within a single name space. A capability is both a name and a set of access rights for an entity, and is managed by an underlying hardware or software kernel.
Reference: [71] <author> M. L. Dertouzos and A. K. Mok. </author> <title> Multiprocessor on-line scheduling of hard-real-time tasks. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(12) </volume> <pages> 1497-1506, </pages> <month> Dec. </month> <year> 1989. </year> <month> 57 </month>
Reference-contexts: Before a job is assigned one or more physical processors, the scheduler checks whether the system can satisfy the job's timing constraints. This analysis is known as schedulability analysis. Schedulability analysis and scheduling for real time systems <ref> [53, 269, 37, 108, 150, 71, 268] </ref> are active areas of research and are not within the scope of this paper. 3.2 Memory Management Memory management for UMA multiprocessors is conceptually similar to that for multipro-grammed uniprocessors.
Reference: [72] <author> E. Dijkstra. </author> <title> The structure of the the-multiprogramming system. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 341-346, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: When a process in one address space requests a service from another address space, it creates a message describing its requirements, and sends it to the target address space. A process in the target address space receives the message, interprets it and services the request. THE <ref> [72] </ref>, Thoth [56], and Demos [16] are a few examples of the earliest message passing systems. The primary motivation behind the design of these systems was to decentralize the structure of an operating system running on a single computer. <p> When multiple processes share an address space, synchronization is required for shared memory consistency [99]. Two fundamental properties are 23 enforced by synchronization: (1) mutual exclusion (to protect a critical section) and (2) event ordering. Classical synchronization primitives like semaphores <ref> [72] </ref>, monitors [107, 131] etc. are widely discussed in the earlier literature and are therefore, not described here. Also not discussed are more complex synchronization mechanisms like path expressions and serial-izers [38], in part because such mechanisms are not in widespread use. <p> SpinLock is a subclass of PrimitiveSpin-Lock and implements a more sophisticated busy waiting lock. Other important synchronizations classes include the Semaphore Class (implements Dijkstra's counting semaphore <ref> [72] </ref>), the GraciousSemaphore 13 class, the BusyWaitingReadWriteLock and the BlockingReadWrite-Lock classes, the TimedSemaphore class (augments a counting semaphore with a timeout mechanism), and the Event class. 4.10 DYNIX The Dynix operating system [224, 223] is an enhanced version of the Unix operating system developed to run on the Sequent multiprocessors.
Reference: [73] <author> L. Dowdy. </author> <title> On the partitioning of multiprocessor systems. </title> <type> Technical Report 88-06, </type> <institution> Department of Computer Science, Vanderbilt University, </institution> <month> July </month> <year> 1988. </year>
Reference-contexts: Tucker and Gupta [249] propose a different solution for reducing the frequency of context switches and for reducing cache corruption, which is explained next. Dynamic Partitioning: The dynamic partitioning <ref> [187, 73] </ref> (also known as Process control with processor partitioning) policy proposed by Tucker and Gupta [249] has the goal of minimizing context switches, so that less time is spent rebuilding a processor's cache.
Reference: [74] <author> P. D. Draves, B. N. Bershad, R. F. Rashid, and R. W. Dean. </author> <title> Using continuations to implement thread management and communication in operating systems. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 122-136, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Major changes include the optimization of its IPC implementation (by optimizing ports and port rights) as well as the use of new algorithms, the use of continuations 11 <ref> [74, 75, 146] </ref> in scheduling, IPC, exception, and new page fault handling facilities [32]. The basic facilities provided by the Mach kernel support the implementation of operating systems as Mach applications [32]. Figure 9 shows the organization of an application, the Unix Server and its relationship to the Mach kernel.
Reference: [75] <author> R. Draves. </author> <title> The revised ipc interface. </title> <booktitle> In Proceedings of the Usenix Mach Conference, </booktitle> <pages> pages 101-122, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A task gets access to a port by receiving a port capability with send or receive rights. Mach 3.0 also supports send-once rights for ports; these are useful for implementing RPC. It also provides dead names and dead-name notifications, which allow servers and clients to detect each others' terminations <ref> [75] </ref>. Messages are variable size collections of typed data. Mach supports both synchronous and asynchronous message transfers. The copy-on-write technique is employed for large message transfers. The ports and the messages together provide location independence, security, and data type tagging [75, 76, 122, 123]. <p> Messages are variable size collections of typed data. Mach supports both synchronous and asynchronous message transfers. The copy-on-write technique is employed for large message transfers. The ports and the messages together provide location independence, security, and data type tagging <ref> [75, 76, 122, 123] </ref>. Mach 3.0 supports port sets to let a few threads serve requests for multiple objects. A receive operation on a port set returns the next message sent to any of the member ports. <p> A receive operation on a port set returns the next message sent to any of the member ports. A no-sender detection mechanism allows object servers to garbage collect the receive right and the represented object. In <ref> [75] </ref>, the author discusses the design and the implementation of an IPC interface for Mach 3.0. The Mach kernel implements messages only within a single machine. However, the transparency of Mach interprocess communication (IPC) allows a user-level server (network message servers) to extend the IPC across a network [100]. <p> Major changes include the optimization of its IPC implementation (by optimizing ports and port rights) as well as the use of new algorithms, the use of continuations 11 <ref> [74, 75, 146] </ref> in scheduling, IPC, exception, and new page fault handling facilities [32]. The basic facilities provided by the Mach kernel support the implementation of operating systems as Mach applications [32]. Figure 9 shows the organization of an application, the Unix Server and its relationship to the Mach kernel.
Reference: [76] <author> R. Draves, M. Jones, and M. Thompson. </author> <title> Mig the mach interface generator. </title> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: Mach 10 separates the Unix process abstraction into tasks and threads [190]. In addition, Mach provides the following: * Machine independent virtual memory management [194]. * A capability based interprocess communication facility. * Language support for RPC <ref> [76, 122, 123] </ref>. * Support for remote file accesses between autonomous systems. 10 Mach is binary compatible with Berkley's Unix 4.3 bsd release 34 * Lightweight user-level threads known as Mach Cthreads [60, 173]. * Miscellaneous other support like debuggers for multithreaded applications [51], excep tion handling [33] etc. <p> Messages are variable size collections of typed data. Mach supports both synchronous and asynchronous message transfers. The copy-on-write technique is employed for large message transfers. The ports and the messages together provide location independence, security, and data type tagging <ref> [75, 76, 122, 123] </ref>. Mach 3.0 supports port sets to let a few threads serve requests for multiple objects. A receive operation on a port set returns the next message sent to any of the member ports.
Reference: [77] <editor> Eds. I. Durham, S.F. Fuller, and A.K. Jones. </editor> <title> The cm* review report. </title> <type> Technical report, </type> <institution> Comp. Science Dept., Carnegie-Mellon Univ., </institution> <year> 1977. </year>
Reference-contexts: This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS [117, 116, 118, 119] is an experimental operating system for the Cm* <ref> [121, 89, 77, 88, 238] </ref> multi-microprocessor computer developed at Carnegie-Mellon University. The design of StarOS is influenced by the protection mechanisms of Hydra, the underlying Cm* architecture, and its principal goals of achieving high performance and reliability for parallel machine users. Cm* is a Non Uniform Memory Access (NUMA) machine.
Reference: [78] <author> D. Eager, J. Zahorjan, and E. Lazowska. </author> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 408-23, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Static and Dynamic Scheduling: A static scheduler makes a one time decision per job regarding the number of processors to be allocated. Once decided, the job is guaranteed to have exactly that number of processors whenever it is active. A static scheduler offers low runtime scheduling overhead <ref> [78] </ref>, but it also assumes a stable parallel application. This is a reasonable assumption for many large-scale scientific applications in which parallelism is derived by decomposition of regular data domains [204].
Reference: [79] <author> J. Edler, J. Lipkis, and E. Schonberg. </author> <title> Process management for highly parallel unix systems. </title> <booktitle> In Proceedings of the USENIX Workshop on UNIX and Supercomputers, </booktitle> <pages> pages 1-17, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: A more direct solution is proposed by Zahorjan et al. [265], who describe a thread sched-uler that avoids preempting processes inside critical sections. In contrast, Edler et al. <ref> [79] </ref> propose an approach combining coscheduling and preemption avoidance for critical sections. Multiple processes are combined to form a group.
Reference: [80] <author> Jr. </author> <title> E.M. Chaves, P.C. Das, T.L. LeBlanc, B.D. Marsh, and M.L. Scott. Kernel-kernel communication in a shared-memory multiprocessor. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(3) </volume> <pages> 171-192, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Examples of such uses are the internally parallel operating system servers offered in the Eden system [134] or in CHAOS [213, 92] and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche <ref> [80] </ref>, or the internally fragmented objects offered by Shapiro [227, 229, 98] for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines. <p> In addition, as with remote procedure calls, it is unclear what levels of machine and language support are required for efficient implementation of object invocations (e.g., for parameter marshaling [103] or for crossing protection boundaries <ref> [258, 80] </ref>). <p> It is likely that object-oriented operating systems will be constructed using the micro-kernel operating system structuring technique explained in Section 2.7. 2.6 Vertical and Horizontal Organizations Some researchers have identified two broad classes of organization of operating system kernels referred to as horizontal and vertical organization <ref> [80] </ref>. These alternatives roughly correspond to the message-based and procedure-based organizations respectively [133]. 9 Vertical Organizations. In a vertical kernel, there is no fundamental distinction between a process in the user space and a process in the kernel. <p> Processes communicate using a common object as an intermediary. To avoid the need for compiler support, Elmwood requires an object programmer to provide a dispatcher for an object's entries, including initialization. 4.5 Psyche Psyche <ref> [217, 218, 219, 220, 80] </ref> is a general-purpose operating system for large-scale shared-memory multiprocessors developed at the University of Rochester. Psyche is implemented on the BBN Butterfly Plus hardware. <p> The original Psyche implementation uses shared memory as the primary kernel-to-kernel communication mechanism. However, for better performance, the design was later modified so that the kernels communicate with each other via remote invocations <ref> [80] </ref> instead. 4.5.1 Synchronization The Psyche kernel implements four types of synchronization disabled preemption (for processor-local data structures), locked-out interrupts (to synchronize with device handlers), spin locks, and semaphores.
Reference: [81] <author> W. Milliken et al. </author> <note> Chrysalis Programmer's Manual, version 2.2. </note> <institution> BBN Laboratories, </institution> <month> June </month> <year> 1985. </year>
Reference-contexts: Locks and semaphores are used hierarchically to prevent deadlock and indefinite postponement. 4.11.3 Memory Management Both versions of UMAX support demand-paged virtual memory to provide up to 16 megabytes of virtual address space per processor. 4.12 Chrysalis The Chrysalis operating system <ref> [64, 130, 81] </ref> provides a Unix-like environment on the Butterfly parallel processor. The Butterfly is a non-uniform memory access (NUMA) shared memory multiprocessor which uses an interconnection network as a processor/memory switch. Each machine node consists of a processor and its local memory.
Reference: [82] <author> R. Finkel and U. Manber. </author> <title> Dib a distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2) </volume> <pages> 235-255, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Simulation studies of their hardware proposals show its usefulness in several applications, including dynamic load balancing, sorting, work distribution in DIB <ref> [82] </ref>, etc.
Reference: [83] <author> R. Finlayson. </author> <title> Object-oriented operating systems. </title> <journal> TCOS Newsletter, </journal> <volume> 5(1) </volume> <pages> 17-21, </pages> <year> 1991. </year>
Reference-contexts: Such systems may be broadly classified as object-oriented or object 8 supporting operating systems, depending on their internal structures and on the interfaces they provide to the user level <ref> [227, 198, 83] </ref>. Object-Oriented Operating Systems (OOOS). In an object-oriented operating system, an object encapsulates a system entity [156, 48].
Reference: [84] <author> R. Fitzgerald and R. Rashid. </author> <title> The integration of virtual memory management and inter-process communication in accent. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2), </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: Unlike the traditional approach of a three level store, the Accent operating system <ref> [84] </ref>, supports a single level store, in which primary memory acts as a cache of secondary storage. Filesystem data and runtime allocated storage are both implemented as disk-based data objects. Copies of large messages are managed using shadow paging techniques. <p> Similarly, in [2], the authors present the design and the implementation of a scalable, kernel-independent, Generic Memory Management Interface (GMI) (for the Chorus [197] nucleus) which is suitable for various architectures (e.g. paged and/or segmented) and implementation schemes. Some operating systems <ref> [84] </ref> allow applications to specify the protection level (inaccessible, read-only, read-write) of pages, and allow user programs to handle protection violations. <p> The Psyche memory management system is structured into four layers of abstraction - NUMA, UMA, VUMA (virtual memory) and PUMA (Psyche memory). These abstractions are shown in Figure 7 and are discussed in Section 4.5. Weak Memory: The Accent and Mach operating systems <ref> [84, 194, 262] </ref> for uniprocessor and UMA multiprocessors demonstrated the use of copy-on-write paging for message passing. Li at Yale showed that a modified Apollo Aegis kernel can support shared memory on a 10Mhz token ring [144], and similar results are demonstrated by the Clouds project at Georgia Tech [68]. <p> Exchange of messages is a more abstract form of communication than accessing shared memory locations. Message passing subsumes communication, buffering, and synchronization. 26 Multiprocessor operating systems have experimented with a large variety of different com-munication abstractions, including ports <ref> [84, 262] </ref> mailboxes [119, 63], links [230, 127] etc. From an implementation point of view, such abstractions are kernel-handled message buffers. They may be either unidirectional or bidirectional. A process may send to or may receive messages from them. <p> Message: A message is a typed collection of data objects used for communication be tween active threads. 5. Memory Object: A memory object is a repository of data which can be mapped into the address space of a task. 4.3.1 Memory Management The Mach virtual memory management <ref> [84, 194, 1, 262, 263] </ref> system is designed to be architecture and operating system independent. Architecture independence is achieved by dividing the virtual memory implementation into machine independent and machine dependent portions.
Reference: [85] <author> A. Forin, J. Barrera, M. Young, and R. Rashid. </author> <title> Design, implementation and performance evaluation of a distributed shared memory server for mach. </title> <booktitle> In Proceedings of the Winter Usenix Technical Conference, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: This has become particularly important for real-time implementations of Mach and for implementations addressing the needs of specific parallel architectures. The Mach kernel does not have specific support for distributed shared memory. However, DSM can be implemented using a server on top of the kernel <ref> [85] </ref>. 4.3.2 Interprocess Communication A port is a kernel-protected entity that is the basic transport abstraction in Mach. Messages are sent to and received from a port. A task gets access to a port by receiving a port capability with send or receive rights.
Reference: [86] <author> G.C. Fox and A. Kolawa. </author> <title> Implementation of the high performance cystalline operating system on intel ipsc hypercube. </title> <type> Technical report, </type> <institution> Caltech Concurrent Computational Program and Physics Dept., Caltech, </institution> <address> Pasadena CA, </address> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: Based on such a lower-level message protocol, kernel- or user-level services are associated with the receipt or sending of individual messages, somewhat resembling the explicit user-level communication calls issued by user programs in the Crystalline operating system <ref> [86] </ref>. The event-driven execution model of `topologies' is similar to the execution model supported by the reactive kernel [221] for the Symult series of multi-computers, which schedules user processes according to conditions that concern the receipt of messages for which processes are waiting.
Reference: [87] <author> Geoffrey C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker. </author> <title> Solving Problems On Concurrent Processors. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: However, in contrast to the OS support for UMA and NUMA multiprocessors, synchronization abstractions for distributed memory machines can often be optimized substantially if they can be made programmable by application programmers or if synchronization can be combined with other communications being performed in application programs <ref> [211, 87] </ref>. 3.3.2 Other Synchronization Constructs Condition Variables. Condition variables make it possible for a thread to suspend its execution while awaiting an action by some other thread. A condition variable is associated with some shared variables protected by a mutex and a predicate (based on the shared variables).
Reference: [88] <author> S.H. Fuller, J.K. Ousterhout, L. Raskin, P.I. Rubinfeld, P.J. Sindhu, </author> <title> and R.J. Swan. Multiprocessors: An overview and working example. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 66(2) </volume> <pages> 216-228, </pages> <month> Feb. </month> <year> 1978. </year> <month> 58 </month>
Reference-contexts: This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS [117, 116, 118, 119] is an experimental operating system for the Cm* <ref> [121, 89, 77, 88, 238] </ref> multi-microprocessor computer developed at Carnegie-Mellon University. The design of StarOS is influenced by the protection mechanisms of Hydra, the underlying Cm* architecture, and its principal goals of achieving high performance and reliability for parallel machine users. Cm* is a Non Uniform Memory Access (NUMA) machine.
Reference: [89] <author> Edward F. Gehringer, Daniel P. Siewiorek, and Zary Segall. </author> <title> Parallel Processing: The Cm* Experience. </title> <publisher> Digital Press, Digital Equipment Corporation, </publisher> <year> 1987. </year>
Reference-contexts: It includes industry efforts to offer concurrent I/O or file system support [114, 115] or even concurrent databases on parallel machines [195], work on communication protocols for high performance and parallel machines [110, 147], and research efforts addressing efficient file management on parallel machines <ref> [89, 31] </ref>. <p> Namely, since one main reason for using parallel hardware is to improve the performance of large-scale application programs, an operating system or system constructs that perform poorly are simply not acceptable <ref> [11, 89] </ref>. <p> like processes or threads, the provision of alternative communication schemes and synchronization mechanisms, and resource scheduling like process assignment to different processors and data placement in physically distributed memory [214], and finally, the parallelization of the operating system itself, again in order to provide scalable performance for varying application requirements <ref> [89, 109, 270] </ref>. A second major reason for using a multiprocessor system is to provide high reliability, and graceful degradation in the event of failure. Hence, several multiprocessor systems have been constructed and designed for improved fault tolerance. <p> Mohan in his PhD thesis <ref> [170, 89] </ref> addresses this problem by designing a flexible run-queue structure, where scheduler run-queues can be configured such that any number of queues may be used by any number of processors. A similar approach to run-queue organization is taken in the Intel 432's iMAX operating system [63]. <p> More interesting problems arise for NUMA and NORMA machines. For early research on memory management in parallel machines, including the implementation of physically distributed, internally parallel memory managers, the reader is referred to <ref> [119, 120, 89] </ref>, which present the innovative designs and structures of memory or object managers for the Cmmp and Cm* multiprocessor systems. <p> Most hardware supports spin locks by specific instructions in their instruction sets. Although spin-waiting consumes processor, bus, and memory cycles, early research in multiprocessor operating systems clearly demonstrates the performance advantages of simple locking strategies and lock implementations <ref> [258, 89, 120] </ref>, showing that spin locks are useful in two situations when the critical section is small (compared to the cost of blocking and resuming a process) or when no other work is available for the processor (since spin waiting results in minimum latency between lock release and reacquisition) [182]. <p> This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS [117, 116, 118, 119] is an experimental operating system for the Cm* <ref> [121, 89, 77, 88, 238] </ref> multi-microprocessor computer developed at Carnegie-Mellon University. The design of StarOS is influenced by the protection mechanisms of Hydra, the underlying Cm* architecture, and its principal goals of achieving high performance and reliability for parallel machine users. Cm* is a Non Uniform Memory Access (NUMA) machine.
Reference: [90] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The protocol for controlling the data movement is derived by extending a directory-based cache coherency algorithm using selective invalidation [13]. 6 PLATINUM is an acronym for Platform for Investigating Non-Uniform Memory 21 Page Placement: Other than the Kendall Square Research Corporation's KSR machines [195] and the experimental Dash multiprocessors <ref> [90] </ref>, NUMA multiprocessors do not have broadcast, invalidate, or snooping mechanisms that maintain consistency among multiple copies of a page when writes occur. Hence, programmers or operating systems restrict writable pages to a single copy. <p> For example, some memory models exploit the fact that synchronization is used to control access to shared state (e.g., properly labeled memory <ref> [90] </ref> or data race-free memory [4, 5]). This allows the underlying system to weaken memory consistency requirements. The resulting, weakened shared memory abstraction presented to programmers may be implemented efficiently because strong consistency and therefore, interprocessor communication is not required for all memory accesses. <p> Mechanisms for memory or state sharing have significant effects on the performance of parallel and distributed applications. This is demonstrated by recent designs of and experimentation with alternative memory models for large scale multiprocessors that exhibit NUMA memory characteristics due to their use of caches to reduce communication latencies <ref> [90, 236] </ref>. In addition, in distributed memory machines like the Intel iPSC series, efficient state sharing is necessitated by significant differences in access times to local vs. remote information [211].
Reference: [91] <author> Ahmed Gheith, Bodhisattwa Mukherjee, Dilma Silva, and Karsten Schwan. Ktk: </author> <title> Kernel support for configurable objects and invocations. </title> <booktitle> In Second International Workshop on Configurable Distributed Systems. IEEE, ACM, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. Sample OSOSs are SOS [228], Cool [102], and CHAOS <ref> [213, 92, 91] </ref> for parallel machines, and Chorus [197] and Clouds [68] for distributed machines. OSOSs can further be classified into different groups depending on the kind of objects they support. <p> OSOSs can further be classified into different groups depending on the kind of objects they support. In the active-server model of computation, objects are active entities containing threads of execution that service requests to the object <ref> [92, 91] </ref>. An OSOS supporting passive objects offers an object-thread model where a single thread of execution traverses all objects within an invocation chain [68, 128]. <p> the implementation include the utility of alternative internal representations of objects, so that an object can maintain its own internal parallelism and control any concurrency imposed upon it by other objects and the development of synchro object useful for efficient representation of complex events. 4.7 KTK The Kernel ToolKit (KTK) <ref> [213, 92, 91, 178] </ref> under development at the Georgia Institute of Technology is a configurable object-based operating system kernel (designed using micro-kernel technology). The major design goal of the KTK project is to provide explicit support for 42 on- and off-line program configuration.
Reference: [92] <author> Ahmed Gheith and Karsten Schwan. </author> <title> Chaos-arc kernel support for multi-weight objects, invocations, and atomicity in real-time applications. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 33-72, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. Sample OSOSs are SOS [228], Cool [102], and CHAOS <ref> [213, 92, 91] </ref> for parallel machines, and Chorus [197] and Clouds [68] for distributed machines. OSOSs can further be classified into different groups depending on the kind of objects they support. <p> OSOSs can further be classified into different groups depending on the kind of objects they support. In the active-server model of computation, objects are active entities containing threads of execution that service requests to the object <ref> [92, 91] </ref>. An OSOS supporting passive objects offers an object-thread model where a single thread of execution traverses all objects within an invocation chain [68, 128]. <p> Examples of such uses are the internally parallel operating system servers offered in the Eden system [134] or in CHAOS <ref> [213, 92] </ref> and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro [227, 229, 98] for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines. <p> Unresolved issues with object-oriented operating systems include the efficient representation of object invocations, where it has become clear that `not all invocations are equal', ranging from rapid unreliable invocations useful in real-time multiprocessor applications <ref> [213, 92] </ref> to reliable multicast invocations required for certain distributed programs [103]. In addition, as with remote procedure calls, it is unclear what levels of machine and language support are required for efficient implementation of object invocations (e.g., for parameter marshaling [103] or for crossing protection boundaries [258, 80]). <p> This implies that the performance of inter-process communication (IPC) mechanism plays a critical role in the performance of such operating systems [20]. The primary characteristic of micro-kernel based operating systems is modularity, thereby hoping to improve system extensibility, portability, reconfigurability, and improved support 10 for distribution <ref> [251, 93, 92] </ref>. Improvements in distribution, extensibility, and reconfigura--bility [175] result from the separation of system components from each other, and from the use of message passing as the communication mechanism among them [251]. <p> While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs. For instance, in the CHAOS and CHAOS arc operating systems <ref> [96, 213, 92] </ref>, operating software implementing either application or operating system functionality consists of a number of autonomous objects, each providing a number of operations (entry points) that can be invoked by other objects. Such functionality appears no different from what is offered by other object-oriented operating systems. <p> More complicated event structures have been shown useful for several application domains and target machines, most prominently including the event handling facilities for active messages [151, 211] or the synchronization points designed by Gheith for real-time applications <ref> [92] </ref>. <p> Specific results include the provision of mechanisms for implementation of alternative ways to invoke an object, as offered by the Spring operating system's subcontract mechanism [103] or the CHAOS system's policy abstraction <ref> [92] </ref>. <p> of the attributes of StarOS of interest to parallel systems is its definition of modules, functions, and module invocations, which are the blueprint for the implementation of similar functionality in the Intel 432's iMAX operating system [63] and in modern object-oriented operating systems like Eden [134], Choices [47], and CHAOS <ref> [213, 92] </ref>. A module defines an object by exporting a set of invocable functions. A function invocation by a process is performed asynchronously by passing invocation parameters to a process designated to execute the function. <p> the implementation include the utility of alternative internal representations of objects, so that an object can maintain its own internal parallelism and control any concurrency imposed upon it by other objects and the development of synchro object useful for efficient representation of complex events. 4.7 KTK The Kernel ToolKit (KTK) <ref> [213, 92, 91, 178] </ref> under development at the Georgia Institute of Technology is a configurable object-based operating system kernel (designed using micro-kernel technology). The major design goal of the KTK project is to provide explicit support for 42 on- and off-line program configuration. <p> In addition, KTK is extensible in that new abstractions and functionality (ie., classes, policies, and attributes) are easily added while potentially maintaining a uniform kernel interface (e.g., when not adding any new kernel classes) <ref> [92, 147] </ref>. 4.8 Choices The operating system family called Choices (Class Hierarchical Open Interface for Custom Embedded Systems) [46, 47, 199, 200, 203, 155] is part of the Embedded Operating System (EOS) project at the University of Illinois at Urbana-Champaign.
Reference: [93] <author> M. Gien. </author> <title> Micro-kernel design. </title> <journal> UNIX REVIEW, </journal> <volume> 8(11) </volume> <pages> 58-63, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: This implies that the performance of inter-process communication (IPC) mechanism plays a critical role in the performance of such operating systems [20]. The primary characteristic of micro-kernel based operating systems is modularity, thereby hoping to improve system extensibility, portability, reconfigurability, and improved support 10 for distribution <ref> [251, 93, 92] </ref>. Improvements in distribution, extensibility, and reconfigura--bility [175] result from the separation of system components from each other, and from the use of message passing as the communication mechanism among them [251].
Reference: [94] <author> A. Goldberg and R. Tarjan. </author> <title> A new approach to the maximum flow problem. </title> <booktitle> In Proceedings of the 18th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 136-146, </pages> <year> 1986. </year>
Reference-contexts: One such area is real time systems. In a real time 4 The applications considered are a particle-based simulator called MP3D, LU-decomposition on dense matrices, a parallel implementation of Goldberg and Tarjan's Maxflow algorithm <ref> [94] </ref>, and a highly optimized block-based parallel algorithm for multiplying two matrices. 18 system, a scheduling policy must satisfy timing constraints such as deadlines, earliest start times, etc. of an incoming job.
Reference: [95] <author> D. Golub, R. Dean, A. Forin, and R. Rashid. </author> <title> Unix as an application program. </title> <booktitle> In Proceedings of the Summer Usenix Technical Conference, </booktitle> <pages> pages 87-96, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Moreover, the reconfiguration module dynamically configures the system to handle hardware and software faults. For example, if the physical environment changes (e.g., addition or removal of clusters), StarOS can be expanded or reduced to accommodate such changes. 4.3 Mach Mach <ref> [3, 191, 242, 243, 95, 192, 15] </ref> is a multiprocessor operating system kernel developed at Carnegie-Mellon University first for distributed systems, then for tightly-coupled UMA multiprocessors. Later extensions of Mach also address NUMA and NORMA machines [255].
Reference: [96] <author> Prabha Gopinath and Karsten Schwan. </author> <title> Chaos: Why one cannot have only an operating system for real-time applications. </title> <journal> Operating Systems Review, </journal> <volume> 23(3) </volume> <pages> 106-125, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: must have direct access to the underlying resources typically controlled by the operating system, and for complex applications, it must deal with uncertainty in operating environments by even permitting programs or operating system components to adapt [215, 129] (i.e., change at runtime) in performance [209] and functionality during system execution <ref> [210, 213, 26, 96] </ref>. While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs. <p> While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs. For instance, in the CHAOS and CHAOS arc operating systems <ref> [96, 213, 92] </ref>, operating software implementing either application or operating system functionality consists of a number of autonomous objects, each providing a number of operations (entry points) that can be invoked by other objects. Such functionality appears no different from what is offered by other object-oriented operating systems.
Reference: [97] <author> A. Gottlieb, R. Grishman, C. Kruskal, K. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The nyu ultracomputer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(2):175-189, </volume> <month> February </month> <year> 1990. </year>
Reference-contexts: The new generation of hardwares provide a few powerful atomic operations such as test-and-set and compare-and-swap which can simplify implementations of synchronization primitives and can even allow certain concurrent data structures to be implemented without blocking [104, 105]. Moreover, instructions such as fetch-and-add <ref> [97] </ref> allow certain common operations to be performed in parallel without critical sections [126]. Other work has evaluated the effects of other kernel components [8, 9, 10, 99] as well as applications on synchronization.
Reference: [98] <author> Y. Gourhant and M. Shapiro. Fog/c++: </author> <title> a fragmented-object generator. </title> <booktitle> In Proceedings of the USENIX C++ Conference, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Examples of such uses are the internally parallel operating system servers offered in the Eden system [134] or in CHAOS [213, 92] and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro <ref> [227, 229, 98] </ref> for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines.
Reference: [99] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization algorithms for shared-memory multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 60-70, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: When multiple processes share an address space, synchronization is required for shared memory consistency <ref> [99] </ref>. Two fundamental properties are 23 enforced by synchronization: (1) mutual exclusion (to protect a critical section) and (2) event ordering. Classical synchronization primitives like semaphores [72], monitors [107, 131] etc. are widely discussed in the earlier literature and are therefore, not described here. <p> Moreover, instructions such as fetch-and-add [97] allow certain common operations to be performed in parallel without critical sections [126]. Other work has evaluated the effects of other kernel components <ref> [8, 9, 10, 99] </ref> as well as applications on synchronization.
Reference: [100] <institution> MACH Networking Group. Network server design. School of Computer Science, Carnegie Mellon University, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: In [75], the author discusses the design and the implementation of an IPC interface for Mach 3.0. The Mach kernel implements messages only within a single machine. However, the transparency of Mach interprocess communication (IPC) allows a user-level server (network message servers) to extend the IPC across a network <ref> [100] </ref>. On top of the general message primitives, Mach implements various flavors of communication including server-client remote procedure calls, distributed object-oriented programming, and streams. 4.3.3 Scheduling The Mach scheduler [36, 34, 35] consists of two parts, one responsible for processor allocation, the other responsible for scheduling threads on individual processors.
Reference: [101] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The impact of operating systems scheduling policies and synchronization methods of the performance of parallel applications. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-32, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Hand-off scheduling has been shown to perform better when program synchronization is exploited (e.g., the requester thread hands off the processor to the holder of the lock) and when interprocess communication takes place (e.g., the sender hands the processor off to the receiver). In <ref> [101] </ref>, Gupta et al. use a detailed simulation study to evaluate the performance of several scheduling strategies. These include regular priority scheduling, coscheduling or gang scheduling, process control with processor partitioning, hand-off scheduling, and affinity based scheduling [248].
Reference: [102] <author> S. Habert, L. Mosseri, and V. Abrossimov. </author> <title> Cool: Kernel support for object-oriented environments. </title> <booktitle> In ECOOP/OOPSLA'90 Conference (SIGPLAN Notices vol.25, no.10, </booktitle> <pages> pages 269-277. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: Examples of OOOSs are Choices [47] and Renaissance [202, 172]. Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. Sample OSOSs are SOS [228], Cool <ref> [102] </ref>, and CHAOS [213, 92, 91] for parallel machines, and Chorus [197] and Clouds [68] for distributed machines. OSOSs can further be classified into different groups depending on the kind of objects they support.
Reference: [103] <author> G. Hamilton, M. Powell, and J. Mitchell. Subcontract: </author> <title> A flexible base for distributed programming. </title> <type> Technical report, </type> <institution> Sun Microsystems Laboratories Inc., SMLI TR-93-13, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Unresolved issues with object-oriented operating systems include the efficient representation of object invocations, where it has become clear that `not all invocations are equal', ranging from rapid unreliable invocations useful in real-time multiprocessor applications [213, 92] to reliable multicast invocations required for certain distributed programs <ref> [103] </ref>. In addition, as with remote procedure calls, it is unclear what levels of machine and language support are required for efficient implementation of object invocations (e.g., for parameter marshaling [103] or for crossing protection boundaries [258, 80]). <p> from rapid unreliable invocations useful in real-time multiprocessor applications [213, 92] to reliable multicast invocations required for certain distributed programs <ref> [103] </ref>. In addition, as with remote procedure calls, it is unclear what levels of machine and language support are required for efficient implementation of object invocations (e.g., for parameter marshaling [103] or for crossing protection boundaries [258, 80]). <p> Specific results include the provision of mechanisms for implementation of alternative ways to invoke an object, as offered by the Spring operating system's subcontract mechanism <ref> [103] </ref> or the CHAOS system's policy abstraction [92]. <p> One example of an industry effort to apply object-oriented technologies to operating system development is the 52 Spring operating system now being commercialized at SUN microsystems <ref> [103] </ref>. The sample research systems we included in this survey are Elmwood, Kernel ToolKit, Psyche, Choices, and Renaissance. Two insights concerning operating systems for parallel machines underly much of this paper's presentation.
Reference: [104] <author> M. Herlihy. </author> <title> Impossibility and universality results for wait-free synchronization. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 276-90, </pages> <month> August </month> <year> 1988. </year> <month> 59 </month>
Reference-contexts: The new generation of hardwares provide a few powerful atomic operations such as test-and-set and compare-and-swap which can simplify implementations of synchronization primitives and can even allow certain concurrent data structures to be implemented without blocking <ref> [104, 105] </ref>. Moreover, instructions such as fetch-and-add [97] allow certain common operations to be performed in parallel without critical sections [126]. Other work has evaluated the effects of other kernel components [8, 9, 10, 99] as well as applications on synchronization.
Reference: [105] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data structures. </title> <booktitle> In Proceedings of the Second ACM Sigplan Symposium on Principles and Practice of Parallel Programming (SIGPLAN Notices vol.25, no.3), </booktitle> <pages> pages 197-206, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The new generation of hardwares provide a few powerful atomic operations such as test-and-set and compare-and-swap which can simplify implementations of synchronization primitives and can even allow certain concurrent data structures to be implemented without blocking <ref> [104, 105] </ref>. Moreover, instructions such as fetch-and-add [97] allow certain common operations to be performed in parallel without critical sections [126]. Other work has evaluated the effects of other kernel components [8, 9, 10, 99] as well as applications on synchronization.
Reference: [106] <author> D. Hildebrand. </author> <title> An architectural overview of qnx. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 113-126, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Furthermore, the use of common underlying services provides support for the coexistence and interoperability of multiple operating system environments on a single host as user-level programs [32]. Mach [32], Chorus [197], KeyKOS [42], QNX <ref> [106] </ref>, and BirLiX [208] are a few examples of micro-kernel based operating systems. 2.8 Application-specific Operating Systems Many application domains impose specific requirements on operating system functionality, performance, and structure.
Reference: [107] <author> C. A. R. Hoare. </author> <title> Monitors: An operating system structuring concept. </title> <journal> Communications of the ACM, </journal> <volume> 17(10) </volume> <pages> 549-557, </pages> <year> 1974. </year>
Reference-contexts: When multiple processes share an address space, synchronization is required for shared memory consistency [99]. Two fundamental properties are 23 enforced by synchronization: (1) mutual exclusion (to protect a critical section) and (2) event ordering. Classical synchronization primitives like semaphores [72], monitors <ref> [107, 131] </ref> etc. are widely discussed in the earlier literature and are therefore, not described here. Also not discussed are more complex synchronization mechanisms like path expressions and serial-izers [38], in part because such mechanisms are not in widespread use. <p> abstract data type) creates a new execution thread for execution of the called operation, but also does not synchronize concurrent calls. 12 The built-in object classes in KTK are quite similar to concurrent object constructs offered in recent designs and implementations of object-oriented concurrent languages like CC++. 43 A `Monitor' <ref> [107] </ref> is an object without execution threads that only allows a single call to be active at a time. It can also define condition variables on which calls can wait, thereby allowing other calls to proceed, until the condition variable is signaled.
Reference: [108] <author> W. A. Horn. </author> <title> Some simple scheduling algorithms. </title> <journal> Naval Res. Logist. Quart., </journal> <volume> 21 </volume> <pages> 177-185, </pages> <year> 1974. </year>
Reference-contexts: Before a job is assigned one or more physical processors, the scheduler checks whether the system can satisfy the job's timing constraints. This analysis is known as schedulability analysis. Schedulability analysis and scheduling for real time systems <ref> [53, 269, 37, 108, 150, 71, 268] </ref> are active areas of research and are not within the scope of this paper. 3.2 Memory Management Memory management for UMA multiprocessors is conceptually similar to that for multipro-grammed uniprocessors.
Reference: [109] <author> R. Hou and Y. Patt. </author> <title> Trading disk capacity for performance. </title> <booktitle> In Proceedings of the 2nd International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 263-270, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: like processes or threads, the provision of alternative communication schemes and synchronization mechanisms, and resource scheduling like process assignment to different processors and data placement in physically distributed memory [214], and finally, the parallelization of the operating system itself, again in order to provide scalable performance for varying application requirements <ref> [89, 109, 270] </ref>. A second major reason for using a multiprocessor system is to provide high reliability, and graceful degradation in the event of failure. Hence, several multiprocessor systems have been constructed and designed for improved fault tolerance.
Reference: [110] <author> N. Hutchinson, L. Peterson, M. Abbott, and S. O'Malley. </author> <title> Rpc in the x-kernel: Evaluating new design techniques. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 91-101, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Recent work is rapidly correcting such deficiencies. It includes industry efforts to offer concurrent I/O or file system support [114, 115] or even concurrent databases on parallel machines [195], work on communication protocols for high performance and parallel machines <ref> [110, 147] </ref>, and research efforts addressing efficient file management on parallel machines [89, 31]. <p> Subsystems present themselves to one another in terms of interfaces implemented by servers. The absence of a single, uniform address space is compensated by automatic stub compilers and sophisticated runtime libraries <ref> [180, 110, 207] </ref> that transfer complex arguments in messages. <p> Communication issues specific to hypercube or mesh machines are reviewed elsewhere [222, 221, 211, 67]. Examples of recent research in communication protocols for high performance or parallel machines are addressing the association of computational activities with messages [66, 151], the user-driven configuration of communication protocols for improved performance <ref> [110] </ref>, and the parallelization of protocol processing [147]. Of interest to this survey is that parallel programs typically use both message mechanisms and shared memory (when available) for inter-process communication.
Reference: [111] <author> K. Hwang and F. Briggs. </author> <booktitle> Computer Architecture and Parallel Processing. Computer Science Series. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Brief survey of multiprocessor hardware. Several current textbooks in parallel computing provide good overviews of parallel machine architectures <ref> [111, 6, 233, 188] </ref>. For purposes of this paper, we briefly review some of the major types of parallel machines, eliding architectures for SIMD programs, functional programs, and systolic applications. <p> An interconnection network, which may be static or dynamic, facilitates communication among processors and memory modules. A few sample interconnection networks are: time shared or common buses, crossbar switches, hierarchical switches, and multistage networks. The design, structure and performance of various interconnection networks have been reviewed in other literature <ref> [264, 111, 6, 233, 188] </ref> and are beyond the scope of this survey. The variety of different kinds of multiprocessor architectures coupled with diverse application requirements have resulted in many different designs, goals, features, and implementations of multiprocessor operating systems, in university research projects and in the commercial domain.
Reference: [112] <author> H. Burkhardt III, S. Frank, B. Knobe, and J. Rothnie. </author> <title> Overview of the ksr1 computer system. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <address> Boston, </address> <month> Febru-ary </month> <year> 1992. </year>
Reference-contexts: Recent parallel NUMA architectures like the Kendall Square multiprocessor offer consistent global virtual memory. However, the performance reasons for exposing programmers to the underlying machine's NUMA properties persist, leading the system designers to include hardware instructions for page prefetches and poststores <ref> [112] </ref>. Research in memory management for parallel machines has focussed on designing techniques for NUMA multiprocessors that relieve programmers from the responsibility of explicit code and data placement.
Reference: [113] <author> Sun Microsystem Inc. </author> <title> Sun OS 4.0 Reference Manual, </title> <month> November </month> <year> 1987. </year> <note> Section 3L. </note>
Reference-contexts: A thread management operation does not require an expensive kernel call. Furthermore, lightweight threads enable an application program to use a thread management system, most appropriate to the problem domain. Mach Cthreads [60, 173, 212], the University of Washington threads [165, 9], SunOS LWP and threads <ref> [113, 127, 189] </ref>, are a few popular lightweight thread implementations. A lightweight thread generally executes in the context of a middleweight or a heavyweight thread.
Reference: [114] <institution> Intel Corporation, Oregon. Intel iPSC/2 and iPSC/860 User's Guide, </institution> <year> 1989. </year>
Reference-contexts: This focus reflects an unfortunate lack of attention paid to such issues in many previous operating research projects and even in some commercial systems. Recent work is rapidly correcting such deficiencies. It includes industry efforts to offer concurrent I/O or file system support <ref> [114, 115] </ref> or even concurrent databases on parallel machines [195], work on communication protocols for high performance and parallel machines [110, 147], and research efforts addressing efficient file management on parallel machines [89, 31]. <p> In this class of architectures, each processor has its own local memory that is not shared by other processors in the system. Hypercubes like the NCube multiprocessors, past Intel iPSC machines and current Intel iSC mesh machines <ref> [114, 115] </ref>, the Thinking Machines CM-5 [246, 140], and workstation clusters are examples of non-shared memory multiprocessors. <p> NORMA multiprocessors are the simplest to design and build, and have become the architecture of choice for current supercomputers like the Intel Paragon <ref> [114, 115] </ref>, recent Cray machines, and others. In the simplest case, a collection of workstations on a local area network constitutes a NORMA multiprocessor. A typical NORMA multiprocessor consists of a number of processors interconnected on a high speed bus or network; the topology of interconnection varies.
Reference: [115] <author> Intel Corporation, </author> <title> Beaverton, Oregon. Touchstone Delta System User's Guide, </title> <year> 1991. </year>
Reference-contexts: This focus reflects an unfortunate lack of attention paid to such issues in many previous operating research projects and even in some commercial systems. Recent work is rapidly correcting such deficiencies. It includes industry efforts to offer concurrent I/O or file system support <ref> [114, 115] </ref> or even concurrent databases on parallel machines [195], work on communication protocols for high performance and parallel machines [110, 147], and research efforts addressing efficient file management on parallel machines [89, 31]. <p> In this class of architectures, each processor has its own local memory that is not shared by other processors in the system. Hypercubes like the NCube multiprocessors, past Intel iPSC machines and current Intel iSC mesh machines <ref> [114, 115] </ref>, the Thinking Machines CM-5 [246, 140], and workstation clusters are examples of non-shared memory multiprocessors. <p> NORMA multiprocessors are the simplest to design and build, and have become the architecture of choice for current supercomputers like the Intel Paragon <ref> [114, 115] </ref>, recent Cray machines, and others. In the simplest case, a collection of workstations on a local area network constitutes a NORMA multiprocessor. A typical NORMA multiprocessor consists of a number of processors interconnected on a high speed bus or network; the topology of interconnection varies.
Reference: [116] <author> A. Jones, R. Chansler Jr., I. Durham, P. Feller, and K. Schwans. </author> <title> Software management of cm* a distributed multiprocessor. </title> <booktitle> In Proceedings of the National Computer Conference, </booktitle> <pages> pages 657-663, </pages> <year> 1977. </year>
Reference-contexts: This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS <ref> [117, 116, 118, 119] </ref> is an experimental operating system for the Cm* [121, 89, 77, 88, 238] multi-microprocessor computer developed at Carnegie-Mellon University.
Reference: [117] <author> A. Jones and K. Schwans. </author> <title> Task forces: Distributed software for solving problems of substantial size. </title> <booktitle> In Proceedings of the Fourth International Conference on Software Engineering, </booktitle> <year> 1979. </year>
Reference-contexts: This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS <ref> [117, 116, 118, 119] </ref> is an experimental operating system for the Cm* [121, 89, 77, 88, 238] multi-microprocessor computer developed at Carnegie-Mellon University. <p> All other operating system functionality resides in user-level, as present or transient processes, depending on its frequency of use. 4.2.1 Task Forces A task force is the abstraction offered by StarOS for representation of parallel programs. A task force, programmed with the TASK language <ref> [117] </ref> (or with lower-level library support), is simply a collection of cooperating StarOS processes which collectively accomplish some joint task.
Reference: [118] <author> A.K. Jones, R.J. Chansler, I. Durham, P. Feiler, D. Scelza, K. Schwan, and S. Vegdahl. </author> <title> Programming issues raised by a multiprocessor. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 66(2) </volume> <pages> 229-237, </pages> <month> Feb. </month> <year> 1978. </year>
Reference-contexts: This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS <ref> [117, 116, 118, 119] </ref> is an experimental operating system for the Cm* [121, 89, 77, 88, 238] multi-microprocessor computer developed at Carnegie-Mellon University.
Reference: [119] <author> A.K. Jones, R.J. Chansler, I. Durham, J. Mohan, K. Schwan, and S. Vegdahl. Staros, </author> <title> a multiprocessor operating system. </title> <booktitle> In Proceedings of the 7th Symposium on Operating System Principles, Asilomar, </booktitle> <address> CA, </address> <pages> pages 117-127. </pages> <publisher> Assoc. Comput. Mach., </publisher> <month> Dec.10-12 </month> <year> 1979. </year>
Reference-contexts: More interesting problems arise for NUMA and NORMA machines. For early research on memory management in parallel machines, including the implementation of physically distributed, internally parallel memory managers, the reader is referred to <ref> [119, 120, 89] </ref>, which present the innovative designs and structures of memory or object managers for the Cmmp and Cm* multiprocessor systems. <p> Such a barrier lock implementation is reminiscent of lock implementations used in distributed memory machines, called structured locks. Structured Locks: In distributed memory machines like hypercube or mesh multiprocessors, operating system constructs (e.g., I/O, exception handling, multicast communications, etc. <ref> [119, 139] </ref>) are physically distributed in order to offer efficient access to the global operating system functionalities required by application programs. Synchronization is no exception because it is a computation that must be performed globally for many physically distributed processes and processors. <p> Exchange of messages is a more abstract form of communication than accessing shared memory locations. Message passing subsumes communication, buffering, and synchronization. 26 Multiprocessor operating systems have experimented with a large variety of different com-munication abstractions, including ports [84, 262] mailboxes <ref> [119, 63] </ref>, links [230, 127] etc. From an implementation point of view, such abstractions are kernel-handled message buffers. They may be either unidirectional or bidirectional. A process may send to or may receive messages from them. There may be rights (like send, receive, or ownership rights) associated with these entities. <p> This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS <ref> [117, 116, 118, 119] </ref> is an experimental operating system for the Cm* [121, 89, 77, 88, 238] multi-microprocessor computer developed at Carnegie-Mellon University.
Reference: [120] <author> Anita K. Jones and Peter Schwarz. </author> <title> Experience using multiprocessor systems: A status report. </title> <journal> Surveys of the Assoc. Comput. Mach., </journal> <volume> 12(2) </volume> <pages> 121-166, </pages> <month> June </month> <year> 1980. </year>
Reference-contexts: More interesting problems arise for NUMA and NORMA machines. For early research on memory management in parallel machines, including the implementation of physically distributed, internally parallel memory managers, the reader is referred to <ref> [119, 120, 89] </ref>, which present the innovative designs and structures of memory or object managers for the Cmmp and Cm* multiprocessor systems. <p> Motivations for exposing such information include: 1. Giving programmers the ability to minimize relatively expensive remote vs. less expen sive local memory references (i.e., maximize program locality <ref> [120] </ref>), and 2. permitting programmers to avoid several forms of potential contention (switch or memory contention) caused by a large number of remote memory references [261]. Recent parallel NUMA architectures like the Kendall Square multiprocessor offer consistent global virtual memory. <p> Most hardware supports spin locks by specific instructions in their instruction sets. Although spin-waiting consumes processor, bus, and memory cycles, early research in multiprocessor operating systems clearly demonstrates the performance advantages of simple locking strategies and lock implementations <ref> [258, 89, 120] </ref>, showing that spin locks are useful in two situations when the critical section is small (compared to the cost of blocking and resuming a process) or when no other work is available for the processor (since spin waiting results in minimum latency between lock release and reacquisition) [182]. <p> Extensive descriptions of the HYDRA system appear in <ref> [120] </ref> and [258]. <p> This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs <ref> [120, 258] </ref>. 4.2 StarOS StarOS [117, 116, 118, 119] is an experimental operating system for the Cm* [121, 89, 77, 88, 238] multi-microprocessor computer developed at Carnegie-Mellon University.
Reference: [121] <editor> Eds. A.K. Jones and Ed Gehringer. </editor> <title> The cm* multiprocessor project: A research review. </title> <type> Technical report, </type> <institution> School of Computer Science, Carnegie-Mellon University, CMU-CS-80-131, </institution> <month> July </month> <year> 1980. </year> <month> 60 </month>
Reference-contexts: One type of switch is an interconnection network consisting of multiple levels of internal nodes, where systems are scaled by addition of internal switch nodes, as in the BBN Butterfly multiprocessors [130, 64]. A second type of switch consists of a hierarchical set of busses <ref> [121, 195] </ref>, where access times to remote memory depend on either the number of internal switch nodes on the access path between the processor and the memory or on the number of traversed system busses. <p> This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS [117, 116, 118, 119] is an experimental operating system for the Cm* <ref> [121, 89, 77, 88, 238] </ref> multi-microprocessor computer developed at Carnegie-Mellon University. The design of StarOS is influenced by the protection mechanisms of Hydra, the underlying Cm* architecture, and its principal goals of achieving high performance and reliability for parallel machine users. Cm* is a Non Uniform Memory Access (NUMA) machine.
Reference: [122] <author> M. Jones and R. Rashid. </author> <title> Mach and matchmaker: Kernel and language support for object-oriented distributed systems. </title> <type> Technical Report CMU-CS-88-129, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: Mach 10 separates the Unix process abstraction into tasks and threads [190]. In addition, Mach provides the following: * Machine independent virtual memory management [194]. * A capability based interprocess communication facility. * Language support for RPC <ref> [76, 122, 123] </ref>. * Support for remote file accesses between autonomous systems. 10 Mach is binary compatible with Berkley's Unix 4.3 bsd release 34 * Lightweight user-level threads known as Mach Cthreads [60, 173]. * Miscellaneous other support like debuggers for multithreaded applications [51], excep tion handling [33] etc. <p> Messages are variable size collections of typed data. Mach supports both synchronous and asynchronous message transfers. The copy-on-write technique is employed for large message transfers. The ports and the messages together provide location independence, security, and data type tagging <ref> [75, 76, 122, 123] </ref>. Mach 3.0 supports port sets to let a few threads serve requests for multiple objects. A receive operation on a port set returns the next message sent to any of the member ports.
Reference: [123] <author> M. Jones, R. Rashid, and M. Thompson. Matchmaker: </author> <title> An interface specification language. </title> <booktitle> In Proceedings of the ACM Conference on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1985. </year>
Reference-contexts: Mach 10 separates the Unix process abstraction into tasks and threads [190]. In addition, Mach provides the following: * Machine independent virtual memory management [194]. * A capability based interprocess communication facility. * Language support for RPC <ref> [76, 122, 123] </ref>. * Support for remote file accesses between autonomous systems. 10 Mach is binary compatible with Berkley's Unix 4.3 bsd release 34 * Lightweight user-level threads known as Mach Cthreads [60, 173]. * Miscellaneous other support like debuggers for multithreaded applications [51], excep tion handling [33] etc. <p> Messages are variable size collections of typed data. Mach supports both synchronous and asynchronous message transfers. The copy-on-write technique is employed for large message transfers. The ports and the messages together provide location independence, security, and data type tagging <ref> [75, 76, 122, 123] </ref>. Mach 3.0 supports port sets to let a few threads serve requests for multiple objects. A receive operation on a port set returns the next message sent to any of the member ports.
Reference: [124] <author> Richard Larowe Jr., Carla Ellis, and Laurence Kaplan. </author> <title> The robustness of numa memory management. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 137-151, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Their experience [40] indicates that even simple automatic strategies can produce nearly optimal page replacement. It also suggests that the dominant remaining source of avoidable performance degradation is false sharing 7 [41], which can be reduced by improving language processors or by tuning applications. The DUnX kernel <ref> [124] </ref>, developed as a framework for implementing dynamic page replacement policies, introduces a highly tunable parameterized dynamic page placement policy for NUMA multiprocessors addressing issues related to the tuning of that policy to suit different architectures and applications. <p> The policy supports both migration and replication, uses a directory based invalidation scheme to ensure the coherence of replicated pages and uses a freeze/defrost 8 strategy to control page bouncing. Such a parameterized NUMA memory management policy can be tuned for architectural as well as application differences. In <ref> [124] </ref>, the authors perform several experiments with the parameterized page replacement policy to confirm that dynamic placement policies are efficient, therefore a reasonably simple parameterized policy may form the basis for the development of machine-independent memory management subsystems for NUMA machines.
Reference: [125] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-grained mobility in the emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: CLU [148], Eden [134], Distributed Smalltalk [18], Emerald <ref> [125] </ref>, and Linda [49] are a few examples of languages that integrate message based communication into the programming environment, either by defining all control structures in terms of messages, or by using messages as the basis for building Algol-like or entirely new control structures.
Reference: [126] <author> A. Karlin, K. Li, M. Manasse, and S. Owicki. </author> <title> Empirical studies of competitive spinning for a shared-memory multiprocessor. </title> <booktitle> In Proceeding of the thirteenth ACM symposium on operating systems principles, </booktitle> <pages> pages 41-55, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Moreover, instructions such as fetch-and-add [97] allow certain common operations to be performed in parallel without critical sections <ref> [126] </ref>. Other work has evaluated the effects of other kernel components [8, 9, 10, 99] as well as applications on synchronization. <p> In addition, an adaptation policy used to configure adaptive multiprocessor locks [177] is shown to improve application performance. such a lock detects changes in application characteristics and adapt itself to suit such changes. In <ref> [126] </ref>, the authors study seven strategies (including a few competitive strategies) for determining whether and how long to spin before blocking while waiting for a lock. The study concludes that for competitive strategies, performance is no worse than an optimal off-line strategy by some constant factor.
Reference: [127] <author> J. Kepecs. </author> <title> Lightweight processes for unix implementation and application. </title> <booktitle> In Proceedings of the Proc. 1985 USENIX Summer Conference, </booktitle> <pages> pages 299-308, </pages> <year> 1985. </year>
Reference-contexts: A thread management operation does not require an expensive kernel call. Furthermore, lightweight threads enable an application program to use a thread management system, most appropriate to the problem domain. Mach Cthreads [60, 173, 212], the University of Washington threads [165, 9], SunOS LWP and threads <ref> [113, 127, 189] </ref>, are a few popular lightweight thread implementations. A lightweight thread generally executes in the context of a middleweight or a heavyweight thread. <p> Exchange of messages is a more abstract form of communication than accessing shared memory locations. Message passing subsumes communication, buffering, and synchronization. 26 Multiprocessor operating systems have experimented with a large variety of different com-munication abstractions, including ports [84, 262] mailboxes [119, 63], links <ref> [230, 127] </ref> etc. From an implementation point of view, such abstractions are kernel-handled message buffers. They may be either unidirectional or bidirectional. A process may send to or may receive messages from them. There may be rights (like send, receive, or ownership rights) associated with these entities.
Reference: [128] <author> Y. Khalidi and M. Nelson. </author> <title> An implementation of unix on an object oriented operating system. </title> <booktitle> In Proceedings of the 1993 Winter Usenix Conference, </booktitle> <address> San Diego, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: In the active-server model of computation, objects are active entities containing threads of execution that service requests to the object [92, 91]. An OSOS supporting passive objects offers an object-thread model where a single thread of execution traverses all objects within an invocation chain <ref> [68, 128] </ref>. One use of object orientation in operating systems is to exploit type hierarchies to achieve operating system configuration (e.g., as done in Choices [47]) along with stronger notions of structuring than available in current systems.
Reference: [129] <author> Jeff Kramer and Jeff MaGee. </author> <title> Dynamic configuration for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(4):424-436, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: Operating software, then, must have direct access to the underlying resources typically controlled by the operating system, and for complex applications, it must deal with uncertainty in operating environments by even permitting programs or operating system components to adapt <ref> [215, 129] </ref> (i.e., change at runtime) in performance [209] and functionality during system execution [210, 213, 26, 96]. While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs.
Reference: [130] <author> BBN Laboratories. </author> <title> Butterfly(TM) Parallel Processor Overview. </title> <publisher> BBN Computer Company, </publisher> <address> Cambridge, MA, 1st edition, </address> <month> June </month> <year> 1985. </year>
Reference-contexts: As a result, access time to local memory is less than that to nonlocal memory. Sample NUMA machines are the BBN Butterfly parallel processor <ref> [130] </ref> and the Kendall Square Research supercomputer [195]. The BBN machines use an interconnection network to connect all processors to memory units, whereas the KSR machines use cache-based algorithms and a hierarchical set of busses for connecting processors to memory units. <p> One type of switch is an interconnection network consisting of multiple levels of internal nodes, where systems are scaled by addition of internal switch nodes, as in the BBN Butterfly multiprocessors <ref> [130, 64] </ref>. A second type of switch consists of a hierarchical set of busses [121, 195], where access times to remote memory depend on either the number of internal switch nodes on the access path between the processor and the memory or on the number of traversed system busses. <p> Locks and semaphores are used hierarchically to prevent deadlock and indefinite postponement. 4.11.3 Memory Management Both versions of UMAX support demand-paged virtual memory to provide up to 16 megabytes of virtual address space per processor. 4.12 Chrysalis The Chrysalis operating system <ref> [64, 130, 81] </ref> provides a Unix-like environment on the Butterfly parallel processor. The Butterfly is a non-uniform memory access (NUMA) shared memory multiprocessor which uses an interconnection network as a processor/memory switch. Each machine node consists of a processor and its local memory.
Reference: [131] <author> B. Lampson and D. Redell. </author> <title> Experiences with processes and monitors in mesa. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 105-117, </pages> <year> 1980. </year>
Reference-contexts: When multiple processes share an address space, synchronization is required for shared memory consistency [99]. Two fundamental properties are 23 enforced by synchronization: (1) mutual exclusion (to protect a critical section) and (2) event ordering. Classical synchronization primitives like semaphores [72], monitors <ref> [107, 131] </ref> etc. are widely discussed in the earlier literature and are therefore, not described here. Also not discussed are more complex synchronization mechanisms like path expressions and serial-izers [38], in part because such mechanisms are not in widespread use.
Reference: [132] <author> B. Lampson and H. Sturgis. </author> <title> Reflections on an operating system design. </title> <journal> Communications of the ACM, </journal> <volume> 19 </volume> <pages> 25-65, </pages> <year> 1976. </year>
Reference-contexts: CAP [179] and the Intel/432 [62] are examples of hardware based systems. Despite early favorable predictions, system builders have been largely unsuccessful in implementing and programming capability based systems which perform as well as machines based on more traditional memory reference models <ref> [132, 59] </ref>. This may be due to the fact that most early research and commercial systems focussed on the use of capabilities for enforcement of protection boundaries and system security characteristics, typically by enforcing the Principle of Least Privilege.
Reference: [133] <author> H. Lauer and R. Needham. </author> <title> On the duality of operating system structures. </title> <journal> Operating Systems Review, </journal> <volume> 13(2) </volume> <pages> 3-19, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: These alternatives roughly correspond to the message-based and procedure-based organizations respectively <ref> [133] </ref>. 9 Vertical Organizations. In a vertical kernel, there is no fundamental distinction between a process in the user space and a process in the kernel. A user process enters the kernel via a trap when required, performs a kernel operation, and returns to user space.
Reference: [134] <author> E. Lazowska, H. Levy, G. Almes, M. Fischer, R. Fowler, and S. Vestal. </author> <title> The architecture of the eden system. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 148-159, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: CLU [148], Eden <ref> [134] </ref>, Distributed Smalltalk [18], Emerald [125], and Linda [49] are a few examples of languages that integrate message based communication into the programming environment, either by defining all control structures in terms of messages, or by using messages as the basis for building Algol-like or entirely new control structures. <p> Another use of this technology is to use object based encapsulations of operating system services in order to represent operating system services internally in different ways, invisibly to services users. Examples of such uses are the internally parallel operating system servers offered in the Eden system <ref> [134] </ref> or in CHAOS [213, 92] and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro [227, 229, 98] for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor <p> One of the attributes of StarOS of interest to parallel systems is its definition of modules, functions, and module invocations, which are the blueprint for the implementation of similar functionality in the Intel 432's iMAX operating system [63] and in modern object-oriented operating systems like Eden <ref> [134] </ref>, Choices [47], and CHAOS [213, 92]. A module defines an object by exporting a set of invocable functions. A function invocation by a process is performed asynchronously by passing invocation parameters to a process designated to execute the function.
Reference: [135] <author> E. Lazowska and M. Squillante. </author> <title> Using processor-cache affinity in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-43, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: An individual process may choose to override its group scheduling policy. This policy is flexible, but it leave specific solutions to the critical section problem to user code. The problems of cache corruption and context switch frequency are addressed by Lazowska and Squillante <ref> [135] </ref>, who evaluate the performance of several multiprocessor scheduling policies based on the notion of processor affinity. A process's processor affinity is based on the contents of the processor's cache. <p> The basic policy schedules a process on a processor on which it last executed hoping that a large percentage of its working set is still present in the processor's cache. Since the policy inherently discourages process migration, it may lead to severe load imbalance. The authors of <ref> [135] </ref> address this issue by proposing a variation on the basic policy which successfully reduces the cache corruption problem.
Reference: [136] <author> T. Leblanc, B. Marsh, and M. Scott. </author> <title> Memory management for large-scale numa multiprocessors. </title> <type> Technical Report TR 311, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: In [124], the authors perform several experiments with the parameterized page replacement policy to confirm that dynamic placement policies are efficient, therefore a reasonably simple parameterized policy may form the basis for the development of machine-independent memory management subsystems for NUMA machines. In <ref> [136] </ref>, the authors present the implementation of the memory management system in the Psyche [220] multiprocessor operating system. The Psyche memory management system is structured into four layers of abstraction - NUMA, UMA, VUMA (virtual memory) and PUMA (Psyche memory). <p> Interesting experimental results with these primitives are reported in [168]. 40 4.5.2 Memory Management The Psyche virtual memory system (Figure 7) has the goal to integrate NUMA memory management with other kernel functions. The design <ref> [136] </ref> consists of four distinct abstraction layers. The lowest layer encapsulates physical page frames and page tables. The next layer provides an illusion of uniform memory access time through page replication and migration.
Reference: [137] <author> T. Leblanc, J. Mellor-Crummey, N. Gafter, L. Crowl, and P. Dibble. </author> <title> The elmwood multiprocessor operating system. </title> <journal> Software Practice and Experience, </journal> 19(11) 1029-1056, November 1989. 
Reference-contexts: continuation is the address of a routine to call when a thread continues execution, plus a small data structure that contains local state needed by that routine 37 emulation library functions both as a translator for system service requests and as a cache for their results [32]. 4.4 Elmwood Elmwood <ref> [167, 137] </ref> is an object-oriented multiprocessor operating system designed and implemented at the University of Rochester.
Reference: [138] <author> T. J. Leblanc. </author> <title> Shared memory versus message-passing in a tightly-coupled multiprocessor: A case study. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 463-466, </pages> <month> August </month> <year> 1986. </year> <month> 61 </month>
Reference-contexts: Interestingly, comparisons of message passing with direct use of shared memory often result in inconclusive results, in part because such results strongly depend on the sizes, granularities, and frequencies of communications in parallel programs <ref> [138] </ref>. 3.4.2 Remote Procedure Calls Most recent shared-memory multiprocessor operating systems support cross-address space remote procedure calls [25] (RPC) as a means of inter-process communication. RPC is a higher level abstraction than message passing. It hides the message communication layer beneath a procedure call layer.
Reference: [139] <author> Thomas J. LeBlanc and S.A. Friedberg. </author> <title> Hierarchical process composition in distributed operating systems. </title> <booktitle> In Proceedings of the 5th International Conference on Distributed Computing Systems, </booktitle> <address> Denver, Colorado, </address> <pages> pages 26-34, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: Such a barrier lock implementation is reminiscent of lock implementations used in distributed memory machines, called structured locks. Structured Locks: In distributed memory machines like hypercube or mesh multiprocessors, operating system constructs (e.g., I/O, exception handling, multicast communications, etc. <ref> [119, 139] </ref>) are physically distributed in order to offer efficient access to the global operating system functionalities required by application programs. Synchronization is no exception because it is a computation that must be performed globally for many physically distributed processes and processors.
Reference: [140] <author> C. E. Leiserson et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the 1992 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: In this class of architectures, each processor has its own local memory that is not shared by other processors in the system. Hypercubes like the NCube multiprocessors, past Intel iPSC machines and current Intel iSC mesh machines [114, 115], the Thinking Machines CM-5 <ref> [246, 140] </ref>, and workstation clusters are examples of non-shared memory multiprocessors. Workstation clusters differ from hypercube or mesh machines in that the latter typically offer specialized hardware for low-latency inter-machine communication and also for implementation of selected global operations like global synchronization, addition, or broadcast.
Reference: [141] <author> S. Leutenegger and M. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 226-36, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Basic theoretical results on static process scheduling on parallel machines show that the scheduling problem is NP-hard; static algorithms minimizing average response time include those described in [164] and [39]. Other scheduling algorithms appear in [267] and <ref> [141] </ref>. In this section, we focus on dynamic scheduling [164], and on scheduling for shared memory machines, where variations in distances between different processors on the parallel machine [39, 214] are not considered. <p> A scheduler can also be classified according to its scheduling granularity, which is determined by the executable unit being scheduled (For example, schedulers differ in that they may schedule individual or groups of processes). A few well accepted multiprocessor scheduling policies are reviewed next <ref> [141] </ref>. Single Shared Ready Queue: Research addressing UMA multiprocessors has typically assumed the use of a single ready queue shared by all processors. With this queue, policies like First Come First Served (FCFS) or Shortest Job First (SJF) are easily implemented, and have been evaluated in the literature. <p> Ousterhout addresses this issue by designing an undivided algorithm, which is identical to the continuous algorithm except that all of the processes of each new job are required to be contiguous in the linear activity sequence. Leutenegger & Vernon <ref> [141] </ref> slightly modify the undivided algorithm to eliminate some of its performance problems: when a job arrives, its processes are appended to the end of a linked list of processes. Scheduling is done by moving a window of length equal to the number of processors over the linked list.
Reference: [142] <author> R. Levin, E. Cohen, W. Corwin, F. Pollack, and W. Wulf. </author> <title> Policy/mechanism separation in hydra. </title> <booktitle> In Proceedings of the 5th Symposium on Operating System Principles, </booktitle> <address> Austin, Texas, </address> <month> Nov. </month> <year> 1975. </year>
Reference-contexts: J. Watson Research Center, IBM), and a few successful commercial systems like Dynix (developed for Sequent Multiprocessors), Chrysalis (developed for BBN Butterfly machines), UMAX (developed for Encore Multimax multiprocessors). 4.1 HYDRA HYDRA <ref> [258, 256, 257, 259, 142, 58] </ref> is one of the earliest successful multiprocessor kernels, developed at Carnegie-Mellon University and implemented on the C.mmp hardware. Extensive descriptions of the HYDRA system appear in [120] and [258].
Reference: [143] <author> H.M. Levy and R.H. </author> <title> Eckhouse. </title> <booktitle> Computer Programming and Architecture. </booktitle> <publisher> Digital Press, </publisher> <year> 1989. </year>
Reference-contexts: and networking are not unimportant, but their discussion is outside the scope of this paper, in part because their performance will be strongly affected by the performance attributes and basic functionality of the underlying system kernel. 2.1 Monolithic Systems Some operating systems such as Unix [196], OS/360 [166] and VMS <ref> [143] </ref> have been implemented with large, monolithic kernels insulated from user programs by simple hardware boundaries.
Reference: [144] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Yale University, </institution> <year> 1986. </year>
Reference-contexts: Weak Memory: The Accent and Mach operating systems [84, 194, 262] for uniprocessor and UMA multiprocessors demonstrated the use of copy-on-write paging for message passing. Li at Yale showed that a modified Apollo Aegis kernel can support shared memory on a 10Mhz token ring <ref> [144] </ref>, and similar results are demonstrated by the Clouds project at Georgia Tech [68]. Such work is motivated by the fact that parallel programming may be simplified when the underlying system provides a basic `memory' abstraction for representation of both local and shared state.
Reference: [145] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Interestingly, recent research is beginning to address this dichotomy by providing a basic `memory' abstraction for representation of both local and remote memory, and by addressing the potential performance penalties arising from providing this abstraction by `weakening' the strong consistency requirements imposed on main memory <ref> [19, 145] </ref>. The resulting, weakened shared memory abstraction presented to programmers may be implemented efficiently because strong consistency and therefore, in-terprocessor communication is not required for all memory accesses. <p> By managing locality in the operating system, the implementation hides the details of specific memory architectures, thus making programs more portable. The simple strategy for page replacement uses local memory as a cache over global, managing consistency with a directory-based ownership protocol similar to that used by Li <ref> [145] </ref> for distributed shared virtual memory. Their experience [40] indicates that even simple automatic strategies can produce nearly optimal page replacement. It also suggests that the dominant remaining source of avoidable performance degradation is false sharing 7 [41], which can be reduced by improving language processors or by tuning applications.
Reference: [146] <author> J. Liedtke. </author> <title> Fast thread management and communication without continuations. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 213-21, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Major changes include the optimization of its IPC implementation (by optimizing ports and port rights) as well as the use of new algorithms, the use of continuations 11 <ref> [74, 75, 146] </ref> in scheduling, IPC, exception, and new page fault handling facilities [32]. The basic facilities provided by the Mach kernel support the implementation of operating systems as Mach applications [32]. Figure 9 shows the organization of an application, the Unix Server and its relationship to the Mach kernel.
Reference: [147] <author> Bert Lindgren, Bobby Krupczak, Mostafa Ammar, and Karsten Schwan. </author> <title> An architecture and toolkit for parallel and configurable protocols. </title> <booktitle> In Proceedings of the International Conference on Network Protocols (ICNP-93), </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Recent work is rapidly correcting such deficiencies. It includes industry efforts to offer concurrent I/O or file system support [114, 115] or even concurrent databases on parallel machines [195], work on communication protocols for high performance and parallel machines <ref> [110, 147] </ref>, and research efforts addressing efficient file management on parallel machines [89, 31]. <p> Examples of recent research in communication protocols for high performance or parallel machines are addressing the association of computational activities with messages [66, 151], the user-driven configuration of communication protocols for improved performance [110], and the parallelization of protocol processing <ref> [147] </ref>. Of interest to this survey is that parallel programs typically use both message mechanisms and shared memory (when available) for inter-process communication. This is demonstrated by implementations of message systems like PVM on the KSR supercomputer, and by implementations of message systems for the BBN Butterfly NUMA machine. <p> In addition, KTK is extensible in that new abstractions and functionality (ie., classes, policies, and attributes) are easily added while potentially maintaining a uniform kernel interface (e.g., when not adding any new kernel classes) <ref> [92, 147] </ref>. 4.8 Choices The operating system family called Choices (Class Hierarchical Open Interface for Custom Embedded Systems) [46, 47, 199, 200, 203, 155] is part of the Embedded Operating System (EOS) project at the University of Illinois at Urbana-Champaign.
Reference: [148] <author> B. Liskov. </author> <title> Abstraction mechanisms in clu. </title> <journal> Communications of the ACM, </journal> <volume> 20(8) </volume> <pages> 564-576, </pages> <month> March </month> <year> 1977. </year>
Reference-contexts: CLU <ref> [148] </ref>, Eden [134], Distributed Smalltalk [18], Emerald [125], and Linda [49] are a few examples of languages that integrate message based communication into the programming environment, either by defining all control structures in terms of messages, or by using messages as the basis for building Algol-like or entirely new control structures.
Reference: [149] <author> Barbara Liskov and Robert Scheifler. </author> <title> Guardians and actions: Linguistic support for robust, distributed programs. </title> <journal> ACM Trans. on Prog. Lang. and Systems., </journal> <volume> 5(3) </volume> <pages> 381-404, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: One self-imposed limitation of this survey is its focus on performance rather than reliability in parallel systems. Reliable systems are surveyed in several recent articles, including <ref> [237, 28, 149] </ref>. A second limitation of this survey is its treatment of operating system kernels rather than operating systems, thereby neglecting system functionalities like file systems, database support, network protocols, and others.
Reference: [150] <author> C. L. Liu and James W. Layland. </author> <title> Scheduling algorithms for multiprogramming in hard real-time environment. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 20(1) </volume> <pages> 46-61, </pages> <month> January </month> <year> 1973. </year>
Reference-contexts: Before a job is assigned one or more physical processors, the scheduler checks whether the system can satisfy the job's timing constraints. This analysis is known as schedulability analysis. Schedulability analysis and scheduling for real time systems <ref> [53, 269, 37, 108, 150, 71, 268] </ref> are active areas of research and are not within the scope of this paper. 3.2 Memory Management Memory management for UMA multiprocessors is conceptually similar to that for multipro-grammed uniprocessors.
Reference: [151] <author> M. Livny and U. Manber. </author> <title> Distributed computation via active messages. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(12):1185-1190, </volume> <month> Dec </month> <year> 1985. </year>
Reference-contexts: A count may be associated with an event, which enables a process to wait for a particular occurrence of an event. More complicated event structures have been shown useful for several application domains and target machines, most prominently including the event handling facilities for active messages <ref> [151, 211] </ref> or the synchronization points designed by Gheith for real-time applications [92]. <p> Communication issues specific to hypercube or mesh machines are reviewed elsewhere [222, 221, 211, 67]. Examples of recent research in communication protocols for high performance or parallel machines are addressing the association of computational activities with messages <ref> [66, 151] </ref>, the user-driven configuration of communication protocols for improved performance [110], and the parallelization of protocol processing [147]. Of interest to this survey is that parallel programs typically use both message mechanisms and shared memory (when available) for inter-process communication. <p> Dally at MIT is constructing hardware support for active message machines [66]. Livny and Manber explore a similar idea in their work on `active channels' <ref> [152, 151] </ref>, in which a token ring communication protocol is extended such that three classes of operations could be performed directly on the node interfaces: arithmetic, selection, and counting.
Reference: [152] <author> M. Livny and U. Manber. </author> <title> Active channels and their applications to parallel computing. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 367-369, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Dally at MIT is constructing hardware support for active message machines [66]. Livny and Manber explore a similar idea in their work on `active channels' <ref> [152, 151] </ref>, in which a token ring communication protocol is extended such that three classes of operations could be performed directly on the node interfaces: arithmetic, selection, and counting.
Reference: [153] <author> S. Lo and V. Gilgor. </author> <title> A comparative analysis of multiprocessor scheduling algorithms. </title> <booktitle> In Proceedings of the 7th International Conference on Distributed Computing Systems, </booktitle> <month> September </month> <year> 1987. </year>
Reference-contexts: Beyond this work, scheduler structuring remains largely unexplored, but should receive increased attention in operating systems for large-scale parallel machines like the Intel Paragon multiprocessor. 3.1.3 Scheduling Policies A scheduling policy allocates available time and processors to a job or a process statically or dynamically <ref> [153] </ref>. Processor load balancing 2 is considered to be a part of a scheduling policy [232]. Basic theoretical results on static process scheduling on parallel machines show that the scheduling problem is NP-hard; static algorithms minimizing average response time include those described in [164] and [39].
Reference: [154] <author> C. Douglas Locke. </author> <title> Best-Effort Decision Making for Real-Time Scheduling. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1986. </year> <month> 62 </month>
Reference-contexts: Furthermore, invocation semantics can be varied by attachment of real-time attributes like delays and deadlines, where deadline semantics may vary from guaranteed deadlines, which are hard deadlines that must not be missed to weak deadlines <ref> [154] </ref>, which specify that partial or incomplete results are acceptable when the deadline is missed. The resulting direct access to resources is a characteristic such real-time operating systems share only with certain single-user operating systems for parallel machines.
Reference: [155] <author> P. Madany, R. Campbell, V. Russo, D. Leyens, and S. Cook. </author> <title> A class hierarchy for building stream-oriented file systems. </title> <booktitle> In Proceedings of the 1989 European Conference on Object-Oriented Programming (ECOOP '89), </booktitle> <pages> pages 311-28, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: KTK is extensible in that new abstractions and functionality (ie., classes, policies, and attributes) are easily added while potentially maintaining a uniform kernel interface (e.g., when not adding any new kernel classes) [92, 147]. 4.8 Choices The operating system family called Choices (Class Hierarchical Open Interface for Custom Embedded Systems) <ref> [46, 47, 199, 200, 203, 155] </ref> is part of the Embedded Operating System (EOS) project at the University of Illinois at Urbana-Champaign. The Choices kernel is implemented on a 10 processor Encore Multimax multiprocessor using the C++ language.
Reference: [156] <author> P. Madany, D. Leyens, V. Russo, and R. Campbell. </author> <title> A c++ class hierarchy for building unix-like file systems. </title> <booktitle> In Proceedings of the USENIX C++ Conference, </booktitle> <pages> pages 65-79, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Such systems may be broadly classified as object-oriented or object 8 supporting operating systems, depending on their internal structures and on the interfaces they provide to the user level [227, 198, 83]. Object-Oriented Operating Systems (OOOS). In an object-oriented operating system, an object encapsulates a system entity <ref> [156, 48] </ref>. An object-oriented language is primarily used to implement such an operating system [201]; the properties of the language such as data encapsulation, data abstraction, inheritance, polymorphism etc. are used to structure the system. An OOOS may or may not support objects at the user level.
Reference: [157] <author> S. Majumdar, D. Eager, and R. Bunt. </author> <title> Scheduling in multiprogrammed parallel systems. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 104-13, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: dynamic distribution of processing loads among the processors of the parallel machine. 14 so that a static processor allocation rapidly becomes inefficient and (2) large-scale parallel machines are often used in multi-user mode, so that scheduling must take into account the requirements of multiple parallel applications sharing a single machine <ref> [158, 157, 225, 65] </ref>. J. Zahorjan and C. McCann compare the performance of static and dynamic schedulers for multi-user workloads.
Reference: [158] <author> S. Majumdar, D. Eager, and R. Bunt. </author> <title> Characterisation of programs for scheduling in multiprogrammed parallel systems. Performance Evaluation, </title> <booktitle> 13(2) </booktitle> <pages> 109-30, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: dynamic distribution of processing loads among the processors of the parallel machine. 14 so that a static processor allocation rapidly becomes inefficient and (2) large-scale parallel machines are often used in multi-user mode, so that scheduling must take into account the requirements of multiple parallel applications sharing a single machine <ref> [158, 157, 225, 65] </ref>. J. Zahorjan and C. McCann compare the performance of static and dynamic schedulers for multi-user workloads.
Reference: [159] <author> M. Makpangou, Y. Gourhant, and M. Shapiro. Boar: </author> <title> a library of fragmented object types for distributed abstractions. </title> <booktitle> In Proceedings of the 1991 International Workshop on Object Orientation in Operating Systems, </booktitle> <pages> pages 164-8, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Such operating systems, therefore, might be structured as collections of cooperating objects, where object invocations may result in messages, in memory sharing, or in both, and where objects themselves may internally be structured as collections of cooperating objects or even as fragmented object <ref> [226, 57, 211, 159] </ref>. 2.4 Language Based Mechanisms Single language systems.
Reference: [160] <author> B. Marsh, C. Brown, T. Leblanc, M. Scott, T. Becker, C. Quiroz, P. Das, and J. Karlsson. </author> <title> The rochester checkers player: multimodel parallel programming for animate vision. </title> <journal> IEEE Computer, </journal> <volume> 25(2) </volume> <pages> 12-19, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Psyche is implemented on the BBN Butterfly Plus hardware. The major design goals of the Psyche project are to support multi-model parallel computing <ref> [160] </ref> and to provide user-level flexibility in general. The intent is to allow applications or application components that use the machine in different ways to coexist and to interact productively. Psyche provides a low-level kernel interface.
Reference: [161] <author> B. Marsh, M. Scott, T. Leblanc, and E. Markatos. </author> <title> First-class user-level threads. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-21, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Tucker and Gupta [249] propose a similar solution which dyanamically controls the number of processes used by applications. This scheme is discussed in details in Section 3.1.3. Similarly, Marsh et al. in <ref> [161] </ref> propose a set of kernel mechanisms (incorporated in the Psyche operating system) required to implement first-class user-level threads addressing the above problem. <p> The kernel scheduler schedules virtual processors on physical processors, and the user-level scheduler schedules processes on virtual processors. To support user-level scheduling the kernel provides a user with virtual processors, software interrupts, and magic pages <ref> [161] </ref>. The kernel scheduler schedules virtual processors on physical processors in a round-robin fashion. Users create the processes to run on virtual processors and have complete control over the scheduling of these processes. Whenever a scheduling decision has to be made, the kernel communicates with virtual processors using software interrupts.
Reference: [162] <author> H. Massalin and C. Pu. </author> <title> Reimplementing the synthesis kernel on the sony news workstation. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 177-186, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The resulting direct access to resources is a characteristic such real-time operating systems share only with certain single-user operating systems for parallel machines. On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel <ref> [163, 162] </ref>, the Psyche system and its related research [219, 168], Presto [24, 23], and others [24, 23, 52].
Reference: [163] <author> Henry Massalin and Calton Pu. </author> <title> Threads and input/output in the synthesis kernel. </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 191-201. </pages> <publisher> SIGOPS, Assoc. Comput. Mach., </publisher> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: The resulting direct access to resources is a characteristic such real-time operating systems share only with certain single-user operating systems for parallel machines. On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel <ref> [163, 162] </ref>, the Psyche system and its related research [219, 168], Presto [24, 23], and others [24, 23, 52].
Reference: [164] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A dynamic processor scheduling pollicy for multiprogrammed, shared memory multiprocessors. </title> <type> Technical Report 90-03-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Processor load balancing 2 is considered to be a part of a scheduling policy [232]. Basic theoretical results on static process scheduling on parallel machines show that the scheduling problem is NP-hard; static algorithms minimizing average response time include those described in <ref> [164] </ref> and [39]. Other scheduling algorithms appear in [267] and [141]. In this section, we focus on dynamic scheduling [164], and on scheduling for shared memory machines, where variations in distances between different processors on the parallel machine [39, 214] are not considered. <p> Basic theoretical results on static process scheduling on parallel machines show that the scheduling problem is NP-hard; static algorithms minimizing average response time include those described in <ref> [164] </ref> and [39]. Other scheduling algorithms appear in [267] and [141]. In this section, we focus on dynamic scheduling [164], and on scheduling for shared memory machines, where variations in distances between different processors on the parallel machine [39, 214] are not considered. Static and Dynamic Scheduling: A static scheduler makes a one time decision per job regarding the number of processors to be allocated.
Reference: [165] <author> P. McJones and G. Swart. </author> <title> Evolving the unix system interface to support multithreaded programs. </title> <booktitle> In Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 393-404, </pages> <year> 1989. </year>
Reference-contexts: A thread management operation does not require an expensive kernel call. Furthermore, lightweight threads enable an application program to use a thread management system, most appropriate to the problem domain. Mach Cthreads [60, 173, 212], the University of Washington threads <ref> [165, 9] </ref>, SunOS LWP and threads [113, 127, 189], are a few popular lightweight thread implementations. A lightweight thread generally executes in the context of a middleweight or a heavyweight thread.
Reference: [166] <author> G. Mealy, B. Witt, and W. Clark. </author> <title> The functional structure of os/360. </title> <journal> IBM Systems Journal, </journal> <volume> 5(1), </volume> <month> January </month> <year> 1966. </year>
Reference-contexts: systems, database support, and networking are not unimportant, but their discussion is outside the scope of this paper, in part because their performance will be strongly affected by the performance attributes and basic functionality of the underlying system kernel. 2.1 Monolithic Systems Some operating systems such as Unix [196], OS/360 <ref> [166] </ref> and VMS [143] have been implemented with large, monolithic kernels insulated from user programs by simple hardware boundaries.
Reference: [167] <author> J. Mellor-Crummey, T. Leblanc, L. Crowl, N. Gafter, and P. Dibble. </author> <title> Elmwood an object-oriented multiprocessor operating system. </title> <type> Technical Report BPR 20, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: continuation is the address of a routine to call when a thread continues execution, plus a small data structure that contains local state needed by that routine 37 emulation library functions both as a translator for system service requests and as a cache for their results [32]. 4.4 Elmwood Elmwood <ref> [167, 137] </ref> is an object-oriented multiprocessor operating system designed and implemented at the University of Rochester.
Reference: [168] <author> J. Mellor-Crummey and M. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel [163, 162], the Psyche system and its related research <ref> [219, 168] </ref>, Presto [24, 23], and others [24, 23, 52]. <p> Results are demonstrated on a Sequent UMA machine, and it is not apparent how their results generalize to NUMA multiprocessors. Synchronization for NUMA machines is addressed in <ref> [168] </ref> by Mellor-Crummey et al., who survey some spin lock algorithms and propose a new scalable algorithm (a list-based queuing lock, also known as MCS lock) that generates O (1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. <p> Once a process reaches a barrier, it is allowed to proceed if and only if all other cooperating processes reach the barrier. A waiting process may either spin or block depending on the implementation of the lock. In <ref> [168] </ref>, Mellor-Crummey et al. survey some barrier algorithms and propose a new scalable algorithm (a tree-based barrier) that spins on locally-accessible flag variables only, requires only O (p) space for p processors, performs the theoretical minimum number of network transactions (2p 2) on machines without broadcast, and performs O (logp) network <p> Interesting experimental results with these primitives are reported in <ref> [168] </ref>. 40 4.5.2 Memory Management The Psyche virtual memory system (Figure 7) has the goal to integrate NUMA memory management with other kernel functions. The design [136] consists of four distinct abstraction layers. The lowest layer encapsulates physical page frames and page tables.
Reference: [169] <author> J. Mogul and A. Borg. </author> <title> The effects of context switches on cache performance. </title> <booktitle> In Pro--ceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (SIGPLAN Notices vol.26, no.4), </booktitle> <pages> pages 75-84, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Next, there is a cost for updating the virtual memory mapping registers and transferring the processor between address spaces. Finally, there is a long term cost associated with cache and TLB performance due to the address space change <ref> [169] </ref>. 12 Hence, in many contemporary operating system kernels, address spaces and threads are decoupled, so that a single address space can have more than one execution threads.
Reference: [170] <author> Joseph Mohan. </author> <title> Performance of Parallel Programs: Model and Analyses. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie-Mellon University, </institution> <address> Pittsburgh, Pa., </address> <month> July </month> <year> 1984. </year>
Reference-contexts: Mohan in his PhD thesis <ref> [170, 89] </ref> addresses this problem by designing a flexible run-queue structure, where scheduler run-queues can be configured such that any number of queues may be used by any number of processors. A similar approach to run-queue organization is taken in the Intel 432's iMAX operating system [63].
Reference: [171] <author> R. Moore, I. Naasi, and D. Siewiorek J. O'Neil. </author> <title> The encore multimax (tm): A multiprocessor computing environment. </title> <type> Technical Report ETR 86-004, </type> <institution> Encore Computer Corporation, </institution> <year> 1986. </year>
Reference-contexts: Unlike Unix, which uses a global model, Dynix uses a local model. A process has a greater role in its own paging activity. 4.11 UMAX UMAX <ref> [171] </ref> like Dynix is an extension of Unix, and runs on Encore Multimax multiprocessors. The Encore Multimax is a bus based, shared memory multiprocessor. There are two versions of UMAX - UMAX 4.2 and UMAX V.
Reference: [172] <author> P. Muckelbauer and V. Russo. </author> <title> Distributed object interoperability via a network type system. </title> <booktitle> In Proceedings of the 1991 International Workshop on Object Orientation in Operating Systems, </booktitle> <pages> pages 169-72, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: An OOOS may or may not support objects at the user level. Examples of OOOSs are Choices [47] and Renaissance <ref> [202, 172] </ref>. Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent.
Reference: [173] <author> Bodhisattwa Mukherjee. </author> <title> A portable and reconfigurable threads package. </title> <booktitle> In Proceedings of the Sun User Group Technical Conference, </booktitle> <pages> pages 101-112, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: User-level threads are managed by runtime library routines linked into each application. A thread management operation does not require an expensive kernel call. Furthermore, lightweight threads enable an application program to use a thread management system, most appropriate to the problem domain. Mach Cthreads <ref> [60, 173, 212] </ref>, the University of Washington threads [165, 9], SunOS LWP and threads [113, 127, 189], are a few popular lightweight thread implementations. A lightweight thread generally executes in the context of a middleweight or a heavyweight thread. <p> Machine independent virtual memory management [194]. * A capability based interprocess communication facility. * Language support for RPC [76, 122, 123]. * Support for remote file accesses between autonomous systems. 10 Mach is binary compatible with Berkley's Unix 4.3 bsd release 34 * Lightweight user-level threads known as Mach Cthreads <ref> [60, 173] </ref>. * Miscellaneous other support like debuggers for multithreaded applications [51], excep tion handling [33] etc. Structurally, Mach is organized horizontally (developed using micro-kernel technology). The Mach kernel is a minimal, extensible kernel which provides a small set of primitive functions.
Reference: [174] <author> Bodhisattwa Mukherjee, Greg Eisenhauer, and Kaushik Ghosh. </author> <title> A machine independent interface for lightweight threads. </title> <type> Technical Report GIT-CC-93-53, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> August </month> <year> 1993. </year> <note> To be published in Operating System Review. </note>
Reference-contexts: The configurable micro-kernel is the partially machine dependent component <ref> [174] </ref> that implements the basic abstractions used by the remainder of KTK: execution threads, virtual memory regions, synchronization primitives, monitoring support for capture of parallel program and KTK state, and a limited number of basic attributes for the configuration of threads-level abstractions, such as synchronization primitives and low-level scheduling. Objects.
Reference: [175] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Experimentation with a reconfigurable micro-kernel. </title> <booktitle> In Proc. of the USENIX Symposium on Microkernels and Other Kernel Architectures, </booktitle> <pages> pages 45-60, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The primary characteristic of micro-kernel based operating systems is modularity, thereby hoping to improve system extensibility, portability, reconfigurability, and improved support 10 for distribution [251, 93, 92]. Improvements in distribution, extensibility, and reconfigura--bility <ref> [175] </ref> result from the separation of system components from each other, and from the use of message passing as the communication mechanism among them [251].
Reference: [176] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Experiments with a configurable lock for multiprocessors. </title> <booktitle> In Proc. of the twenty secondth International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 205-208, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The waiting processes may either spin or block depending on whether the lock is implemented as a spinning read-write lock or a blocking read-write lock. Configurable Locks: In <ref> [176] </ref>, Mukherjee and Schwan study the effects of application and hardware characteristics on multiprocessor locks, and propose a structure for configurable locks. Such locks allow applications to dynamically alter the waiting (spin, block or both) mechanism and the request handling mechanism (how the lock is scheduled). <p> Synchronization. In addition to primitive spin and blocking locks, KTK supports configurable lock objects which can be configured to suit application's requirements <ref> [176, 177] </ref>. Such locks contain a set of implementation-dependent attributes which can be dynamically altered to result in a continuous spectrum of lock configurations ranging from `busy waiting' to `blocking' (Some useful configurations are: combined locks, advisory locks, priority locks, handoff locks, and adaptive locks).
Reference: [177] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Improving performance by use of adaptive objects: Experimentation with a configurable multiprocessor thread package. </title> <booktitle> In Proc. of the second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 59-66, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Furthermore, hints from lock owners may be used to configure a lock for improving its waiting strategy (`advisory' or `speculative' locks). In addition, an adaptation policy used to configure adaptive multiprocessor locks <ref> [177] </ref> is shown to improve application performance. such a lock detects changes in application characteristics and adapt itself to suit such changes. In [126], the authors study seven strategies (including a few competitive strategies) for determining whether and how long to spin before blocking while waiting for a lock. <p> Synchronization. In addition to primitive spin and blocking locks, KTK supports configurable lock objects which can be configured to suit application's requirements <ref> [176, 177] </ref>. Such locks contain a set of implementation-dependent attributes which can be dynamically altered to result in a continuous spectrum of lock configurations ranging from `busy waiting' to `blocking' (Some useful configurations are: combined locks, advisory locks, priority locks, handoff locks, and adaptive locks). <p> Furthermore, configurable locks implement a customized monitor module that can be used to sense the current state of a lock <ref> [177] </ref>. Configuration. KTK's support for reconfiguration consists of three mechanisms attributes, policies, and a monitoring mechanism (to sense current program state).
Reference: [178] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Survey of real-time operating systems. </title> <type> Technical Report GIT-CC-93/18, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: the implementation include the utility of alternative internal representations of objects, so that an object can maintain its own internal parallelism and control any concurrency imposed upon it by other objects and the development of synchro object useful for efficient representation of complex events. 4.7 KTK The Kernel ToolKit (KTK) <ref> [213, 92, 91, 178] </ref> under development at the Georgia Institute of Technology is a configurable object-based operating system kernel (designed using micro-kernel technology). The major design goal of the KTK project is to provide explicit support for 42 on- and off-line program configuration. <p> The major design goal of the KTK project is to provide explicit support for 42 on- and off-line program configuration. KTK is layered on a portable and configurable threads--based micro-kernel <ref> [178] </ref>. As a result, KTK can be run on diverse platforms, including at user level on SUN SPARCstations, on a Kendall Square Research supercomputer, SGI machines, and as a native operating system kernel on the GP1000 BBN Butterfly multiprocessor. KTK structure.
Reference: [179] <author> R. Needham. </author> <title> The cambridge cap computer and its protection system. </title> <booktitle> In Proceedings of the 6th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 1-10, </pages> <institution> Purdue University, </institution> <month> November </month> <year> 1977. </year> <note> Assoc. Comput. Mach., SIGOPS. </note>
Reference-contexts: Parameter passing is by reference between the caller and the callee. The implementation of capability based addressing has been carried out using both software and hardware techniques. A few sample software based systems are Hydra [258] and Cal/Tss [235]. CAP <ref> [179] </ref> and the Intel/432 [62] are examples of hardware based systems. Despite early favorable predictions, system builders have been largely unsuccessful in implementing and programming capability based systems which perform as well as machines based on more traditional memory reference models [132, 59].
Reference: [180] <author> B.J. Nelson. </author> <title> Remote Procedure Call. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> May </month> <year> 1981. </year>
Reference-contexts: Subsystems present themselves to one another in terms of interfaces implemented by servers. The absence of a single, uniform address space is compensated by automatic stub compilers and sophisticated runtime libraries <ref> [180, 110, 207] </ref> that transfer complex arguments in messages. <p> A server is an address space which contains the code and data necessary to implement a set of procedures which are exported to other address space. A client is an address space which requests a service from a server by sending an appropriate message <ref> [180] </ref>. 2.5 Object-Oriented and Object-Supporting Operating Systems Several ongoing projects are using or exploring the object-oriented paradigm for building operating systems. <p> Sample variations in invocation semantics not concerned with the types of objects or operations being invoked include asynchronous vs. synchronous invocations, the ability to wait for acknowledgements of invocation receipt [211] as in distributed RPC implementations <ref> [180] </ref>, the variation of when an invocation is considered complete (upon successful transmission of the invocation, upon receipt of the invocation, upon invocation completion, etc.), and the association of additional parameters governing how and when invocations are scheduled or processed, the latter being particularly important in real-time systems but also relevant
Reference: [181] <author> P. Oleinick. </author> <title> The Implementation and Evaluation of Parallel Algorithms on a Multiprocessor. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1978. </year>
Reference-contexts: the major contributions of HYDRA address protection issues, there are also many research results relevant to parallel systems, including: * basic results establishing the concept of program locality, the effects of bus and memory contentions on parallel program performance, and the importance of asynchrony in parallel program design and implementation <ref> [258, 17, 181] </ref>, * demonstrations that high performance parallel programs require choices in the operating system mechanisms being provided (e.g., a variety of different synchronization mechanisms), since programs differ in granularities of parallelism, in frequencies of access to shared data, etc., and * experiences with the use of specific operating system
Reference: [182] <author> J. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of Distributed Computing Systems Conference, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: [258, 89, 120], showing that spin locks are useful in two situations when the critical section is small (compared to the cost of blocking and resuming a process) or when no other work is available for the processor (since spin waiting results in minimum latency between lock release and reacquisition) <ref> [182] </ref>. When using a blocking lock, a waiting process (also called a contender process) blocks until awakened by the process releasing the lock. Such locks are also known as mutex locks. Anderson et al. [10] compare the performance of a number of software spin-waiting algorithms. <p> Instead, the reader is referred to the re-engineered version of StarOS built by Ousterhout (called the Medusa system [184]), where several interesting, higher-level scheduling strategies were implemented and evaluated, typically referred to as `co-scheduling' or `gang scheduling' in the literature <ref> [182] </ref>. 4.2.4 Reconfiguration System initialization is performed by the reconfiguration module, which gathers data about the physical resources (e.g., physical memory), creates the first operating system objects, initializes the Kmap, and initializes the nuclei.
Reference: [183] <author> J.K. Ousterhout. </author> <title> Partitioning and Cooperation in a Distributed Operating System. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> April </month> <year> 1980. </year> <month> 64 </month>
Reference-contexts: A coscheduling policy schedules the runnable processes of a job to run simultaneously on different processors. Job preemption implies the simultaneous preemption of all of its processes. Effectively, the system context switches between jobs. Ousterhout proposed and evaluated three different coscheduling algorithms in his PhD thesis <ref> [183] </ref>: matrix, continuous and undivided. In the matrix algorithm, processes of arriving jobs are arranged in a matrix with p columns and a certain number of rows, where p is the total number of processors in the system. <p> They attribute this decreased performance to several problems: * A process may be preempted while inside a spinlock-controlled critical section, while the other processes of the same application busy wait to enter the critical section. This 3 In <ref> [183] </ref>, Ousterhout defines an activity slot as a slot to which an activity (or a process) may be assigned. Scheduling consists of selecting one activity slot in each processor; the assigned activity is executed by the processor. The collection of activity slots is referred to as the activity space.
Reference: [184] <author> John K. Ousterhout, Donald A. Scelza, and Pradeep Sindhu. </author> <title> Medusa: An experiment in distributed operating system structure. </title> <journal> Comm. of the Assoc. Comput. Mach., </journal> <volume> 23(2) </volume> <pages> 92-104, </pages> <month> Feb. </month> <year> 1980. </year>
Reference-contexts: Few interesting higher-level schedulers were constructed for the StarOS system. Instead, the reader is referred to the re-engineered version of StarOS built by Ousterhout (called the Medusa system <ref> [184] </ref>), where several interesting, higher-level scheduling strategies were implemented and evaluated, typically referred to as `co-scheduling' or `gang scheduling' in the literature [182]. 4.2.4 Reconfiguration System initialization is performed by the reconfiguration module, which gathers data about the physical resources (e.g., physical memory), creates the first operating system objects, initializes the <p> Similarly, the StarOS system along with Roscoe [230] and Medusa <ref> [184] </ref> provides early implementations of micro-kernels, of user-level operating system services, of internally parallel or reliable system services, and of alternative operating system constructs providing similar functionalities at differing costs.
Reference: [185] <editor> IEEE POSIX P1003.4a. </editor> <title> Threads Extension for Portable Operating Systems. </title>
Reference-contexts: Such threads are referred to as middleweight threads or kernel-level threads when they are managed by the operating system kernel (POSIX Pthreads <ref> [185] </ref>). The advantages of middleweight threads are: * The kernel can directly schedule an application's thread on the available physical pro cessors. * Kernel-level threads offer a general programming interface to the application.
Reference: [186] <author> J. Pallas and D. Ungar. </author> <title> Multiprocessor smalltalk: a case study of a multiprocessor-based programming environment. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation (SIGPLAN Notices, vol.23, no.7), </booktitle> <pages> pages 268-77, </pages> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: For example, the Ada rendezvous mechanism leads to well-known performance problems [43], the global addressing mechanisms and fixed language semantics of Linda can lead to inefficiencies concerning the update and access of remote information [211], and heap maintenance has been shown difficult for languages like Smalltalk <ref> [260, 186] </ref>. Another inherent problem with language based systems can be protection, where language-level typing mechanisms must be mapped to the protection mechanisms available in the underlying operating system [14, 254], which is not always easily done.
Reference: [187] <author> K. Park and L. Dowdy. </author> <title> Dynamic partitioning of multiprocessor systems. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(2) </volume> <pages> 91-120, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Tucker and Gupta [249] propose a different solution for reducing the frequency of context switches and for reducing cache corruption, which is explained next. Dynamic Partitioning: The dynamic partitioning <ref> [187, 73] </ref> (also known as Process control with processor partitioning) policy proposed by Tucker and Gupta [249] has the goal of minimizing context switches, so that less time is spent rebuilding a processor's cache.
Reference: [188] <author> D. Patterson and J. Hennessy. </author> <title> Computer architecture : a quantitative approach. </title> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Mateo, Calif., </address> <year> 1990. </year>
Reference-contexts: Brief survey of multiprocessor hardware. Several current textbooks in parallel computing provide good overviews of parallel machine architectures <ref> [111, 6, 233, 188] </ref>. For purposes of this paper, we briefly review some of the major types of parallel machines, eliding architectures for SIMD programs, functional programs, and systolic applications. <p> An interconnection network, which may be static or dynamic, facilitates communication among processors and memory modules. A few sample interconnection networks are: time shared or common buses, crossbar switches, hierarchical switches, and multistage networks. The design, structure and performance of various interconnection networks have been reviewed in other literature <ref> [264, 111, 6, 233, 188] </ref> and are beyond the scope of this survey. The variety of different kinds of multiprocessor architectures coupled with diverse application requirements have resulted in many different designs, goals, features, and implementations of multiprocessor operating systems, in university research projects and in the commercial domain.
Reference: [189] <author> M. Powell, S. Kleiman, S. Barton, D. Shah, D. Stein, and M. Weeks. </author> <title> Sunos multi-thread architecture. </title> <booktitle> In Proceedings of the USENIX winter conference, </booktitle> <pages> pages 1-14, </pages> <year> 1991. </year>
Reference-contexts: A thread management operation does not require an expensive kernel call. Furthermore, lightweight threads enable an application program to use a thread management system, most appropriate to the problem domain. Mach Cthreads [60, 173, 212], the University of Washington threads [165, 9], SunOS LWP and threads <ref> [113, 127, 189] </ref>, are a few popular lightweight thread implementations. A lightweight thread generally executes in the context of a middleweight or a heavyweight thread.
Reference: [190] <author> R. Rashid. </author> <title> Threads of a new system. </title> <journal> Unix Review, </journal> <volume> 4(8) </volume> <pages> 37-49. </pages>
Reference-contexts: Mach is also supported as a product by a number of hardware vendors. Mach is the base technology for the OSF/1 operating system from the Open Software Foundation. Mach 10 separates the Unix process abstraction into tasks and threads <ref> [190] </ref>.
Reference: [191] <author> R. Rashid. </author> <title> From rig to accent to mach: The evolution of a network operating system. </title> <booktitle> In Proceedings of the ACM/IEEE Computer Society Fall Joint Computer Conference, </booktitle> <pages> pages 1128-37, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: The primary motivation behind the design of these systems was to decentralize the structure of an operating system running on a single computer. On the other hand, the motivation behind the latter message passing systems such as RIG <ref> [191] </ref>, V [54], Accent [193], and various hypercube operating systems [205, 221] was to build an operating system on a structure of distributed computers. In contrast to the fine-grained protection of capability systems, network based message passing systems rely on a coarse-grained protection mechanism. <p> Moreover, the reconfiguration module dynamically configures the system to handle hardware and software faults. For example, if the physical environment changes (e.g., addition or removal of clusters), StarOS can be expanded or reduced to accommodate such changes. 4.3 Mach Mach <ref> [3, 191, 242, 243, 95, 192, 15] </ref> is a multiprocessor operating system kernel developed at Carnegie-Mellon University first for distributed systems, then for tightly-coupled UMA multiprocessors. Later extensions of Mach also address NUMA and NORMA machines [255].
Reference: [192] <author> R. Rashid, R. Baron, A. Forin, D. Golub, M. Jones, D. Julin, D. Orr, and R. Sanzi. </author> <title> Mach: A foundation for open systems. </title> <booktitle> In Proceedings of the 2nd Workshop on Workstation Operating Systems,IEEE, </booktitle> <pages> pages 109-13, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Moreover, the reconfiguration module dynamically configures the system to handle hardware and software faults. For example, if the physical environment changes (e.g., addition or removal of clusters), StarOS can be expanded or reduced to accommodate such changes. 4.3 Mach Mach <ref> [3, 191, 242, 243, 95, 192, 15] </ref> is a multiprocessor operating system kernel developed at Carnegie-Mellon University first for distributed systems, then for tightly-coupled UMA multiprocessors. Later extensions of Mach also address NUMA and NORMA machines [255].
Reference: [193] <author> R. Rashid and G. Robertson. </author> <title> Accent: A communnication oriented network operating system kernel. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 64-75, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: The primary motivation behind the design of these systems was to decentralize the structure of an operating system running on a single computer. On the other hand, the motivation behind the latter message passing systems such as RIG [191], V [54], Accent <ref> [193] </ref>, and various hypercube operating systems [205, 221] was to build an operating system on a structure of distributed computers. In contrast to the fine-grained protection of capability systems, network based message passing systems rely on a coarse-grained protection mechanism.
Reference: [194] <author> R. Rashid, A. Tevanian, M. Young, D. Golub, R. Baron, D. Black, W. Bolosky, and J. Chew. </author> <title> Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 896-908, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In effect, Accent carries into the domain of message passing systems the notion that I/O can be performed through virtual memory management. The design of the Mach system's memory management is largely derived from the Accent system <ref> [194, 1, 263, 262] </ref>. <p> Some distributed systems support distributed memory schemes. 19 has focussed on designing memory management functionalities and interfaces which are in-dependent of the machine architecture and the operating system kernel. For example, the Mach operating system implements virtual memory management which is machine and operating system independent <ref> [194] </ref>. The machine-dependent portion of Mach's virtual memory subsystem is implemented as a separate module. All information important to the management of the virtual memory is maintained in machine-independent data structures and machine-dependent data structures contain only the mappings necessary to run the current mix of programs [194] (See Section 4.3 <p> operating system independent <ref> [194] </ref>. The machine-dependent portion of Mach's virtual memory subsystem is implemented as a separate module. All information important to the management of the virtual memory is maintained in machine-independent data structures and machine-dependent data structures contain only the mappings necessary to run the current mix of programs [194] (See Section 4.3 for more on the implementation of the Mach virtual memory management). <p> Such consistency is guaranteed only for local memory and caches (i.e., for non-shared memory), or it must be explicitly enforced for shared memory by user- or compiler-generated code performing explicit block or page moves <ref> [194, 262] </ref>. As a result, NUMA architectures implementing a shared memory programming model typically expose the existing memory access hierarchy to the application program, as done in BBN's Uniform System [247]. Motivations for exposing such information include: 1. <p> Realizing that the problem of memory management is similar to the problem of cache management and consistency for UMA multiprocessors, the Mach operating 20 system's UMA implementation of memory management <ref> [194, 262] </ref> attempts to minimize the amount of data-copying and replication by using page copy-on-write and similar techniques for reduction of data movement. <p> The Psyche memory management system is structured into four layers of abstraction - NUMA, UMA, VUMA (virtual memory) and PUMA (Psyche memory). These abstractions are shown in Figure 7 and are discussed in Section 4.5. Weak Memory: The Accent and Mach operating systems <ref> [84, 194, 262] </ref> for uniprocessor and UMA multiprocessors demonstrated the use of copy-on-write paging for message passing. Li at Yale showed that a modified Apollo Aegis kernel can support shared memory on a 10Mhz token ring [144], and similar results are demonstrated by the Clouds project at Georgia Tech [68]. <p> Mach is the base technology for the OSF/1 operating system from the Open Software Foundation. Mach 10 separates the Unix process abstraction into tasks and threads [190]. In addition, Mach provides the following: * Machine independent virtual memory management <ref> [194] </ref>. * A capability based interprocess communication facility. * Language support for RPC [76, 122, 123]. * Support for remote file accesses between autonomous systems. 10 Mach is binary compatible with Berkley's Unix 4.3 bsd release 34 * Lightweight user-level threads known as Mach Cthreads [60, 173]. * Miscellaneous other support <p> Message: A message is a typed collection of data objects used for communication be tween active threads. 5. Memory Object: A memory object is a repository of data which can be mapped into the address space of a task. 4.3.1 Memory Management The Mach virtual memory management <ref> [84, 194, 1, 262, 263] </ref> system is designed to be architecture and operating system independent. Architecture independence is achieved by dividing the virtual memory implementation into machine independent and machine dependent portions. <p> Using these data structures, Mach supports large, sparse virtual address spaces and memory mapped files <ref> [1, 194, 244] </ref>. Mach implements a single level store by treating all primary memory as a cache for virtual memory objects. Mach allows tasks to allocate and deallocate regions of virtual memory, and to set the protection and inheritance of virtual memory regions.
Reference: [195] <institution> Kendall Square Research. Technical summary, </institution> <year> 1992. </year>
Reference-contexts: Recent work is rapidly correcting such deficiencies. It includes industry efforts to offer concurrent I/O or file system support [114, 115] or even concurrent databases on parallel machines <ref> [195] </ref>, work on communication protocols for high performance and parallel machines [110, 147], and research efforts addressing efficient file management on parallel machines [89, 31]. <p> As a result, access time to local memory is less than that to nonlocal memory. Sample NUMA machines are the BBN Butterfly parallel processor [130] and the Kendall Square Research supercomputer <ref> [195] </ref>. The BBN machines use an interconnection network to connect all processors to memory units, whereas the KSR machines use cache-based algorithms and a hierarchical set of busses for connecting processors to memory units. <p> One type of switch is an interconnection network consisting of multiple levels of internal nodes, where systems are scaled by addition of internal switch nodes, as in the BBN Butterfly multiprocessors [130, 64]. A second type of switch consists of a hierarchical set of busses <ref> [121, 195] </ref>, where access times to remote memory depend on either the number of internal switch nodes on the access path between the processor and the memory or on the number of traversed system busses. <p> The protocol for controlling the data movement is derived by extending a directory-based cache coherency algorithm using selective invalidation [13]. 6 PLATINUM is an acronym for Platform for Investigating Non-Uniform Memory 21 Page Placement: Other than the Kendall Square Research Corporation's KSR machines <ref> [195] </ref> and the experimental Dash multiprocessors [90], NUMA multiprocessors do not have broadcast, invalidate, or snooping mechanisms that maintain consistency among multiple copies of a page when writes occur. Hence, programmers or operating systems restrict writable pages to a single copy.
Reference: [196] <author> D. Ritchie and K. Thompson. </author> <title> The unix time-sharing system. </title> <journal> Communications of the Assoc. Comput. Mach., </journal> <volume> 17(7), </volume> <year> 1974. </year>
Reference-contexts: interfaces, file systems, database support, and networking are not unimportant, but their discussion is outside the scope of this paper, in part because their performance will be strongly affected by the performance attributes and basic functionality of the underlying system kernel. 2.1 Monolithic Systems Some operating systems such as Unix <ref> [196] </ref>, OS/360 [166] and VMS [143] have been implemented with large, monolithic kernels insulated from user programs by simple hardware boundaries. <p> A kernel resource is represented by a data structure shared among processes. The vertical organization presents a uniform model for the user and the kernel level processes and closely mimics the hardware organization of a UMA multiprocessor. Most Unix <ref> [196] </ref> kernels are vertically organized. Horizontal Organizations. In a horizontal kernel, each major kernel resource is represented by a separate kernel process (or thread), and a typical kernel operation requires communication among the set of kernel processes that represent the resources needed by the operation.
Reference: [197] <author> M. Rozier, V. Abrossimov, F. Armand, I. Boule, M. Gien, M. Guillemont, F. Herrman, C. Kaiser, S. Langlois, P. Leonard, and W. Neuhauser. </author> <title> Overview of the chorus operating system. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 39-69, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. Sample OSOSs are SOS [228], Cool [102], and CHAOS [213, 92, 91] for parallel machines, and Chorus <ref> [197] </ref> and Clouds [68] for distributed machines. OSOSs can further be classified into different groups depending on the kind of objects they support. In the active-server model of computation, objects are active entities containing threads of execution that service requests to the object [92, 91]. <p> Furthermore, the use of common underlying services provides support for the coexistence and interoperability of multiple operating system environments on a single host as user-level programs [32]. Mach [32], Chorus <ref> [197] </ref>, KeyKOS [42], QNX [106], and BirLiX [208] are a few examples of micro-kernel based operating systems. 2.8 Application-specific Operating Systems Many application domains impose specific requirements on operating system functionality, performance, and structure. <p> Similarly, in [2], the authors present the design and the implementation of a scalable, kernel-independent, Generic Memory Management Interface (GMI) (for the Chorus <ref> [197] </ref> nucleus) which is suitable for various architectures (e.g. paged and/or segmented) and implementation schemes. Some operating systems [84] allow applications to specify the protection level (inaccessible, read-only, read-write) of pages, and allow user programs to handle protection violations. <p> remote to local access times are even more pronounced in distributed machines like sets of workstations connected via high-speed networks [55], notions of distributed objects have been a topic of research for such systems for quite some time, as evidenced by work on Clouds [69] at Georgia Tech, on Chorus <ref> [197] </ref>, and on fragmented objects [227] in France. 3.3 Synchronization When multiple cooperating processes execute simultaneously, synchronization primitives are needed for concurrency control. When multiple processes share an address space, synchronization is required for shared memory consistency [99].
Reference: [198] <author> V. Russo. </author> <title> Object-oriented operating system design. </title> <journal> TCOS Newsletter, </journal> <volume> 5(1) </volume> <pages> 34-38, </pages> <year> 1991. </year>
Reference-contexts: Such systems may be broadly classified as object-oriented or object 8 supporting operating systems, depending on their internal structures and on the interfaces they provide to the user level <ref> [227, 198, 83] </ref>. Object-Oriented Operating Systems (OOOS). In an object-oriented operating system, an object encapsulates a system entity [156, 48].
Reference: [199] <author> V. Russo and R. Campbell. </author> <title> Virtual memory and backing storage management in multiprocessor operating systems using object-oriented design techniques. </title> <booktitle> In OOPSLA'89 Conference Proceedings (SIGPLAN Notices vol.24, no.10), </booktitle> <pages> pages 267-78, </pages> <month> October </month> <year> 1989. </year> <month> 65 </month>
Reference-contexts: KTK is extensible in that new abstractions and functionality (ie., classes, policies, and attributes) are easily added while potentially maintaining a uniform kernel interface (e.g., when not adding any new kernel classes) [92, 147]. 4.8 Choices The operating system family called Choices (Class Hierarchical Open Interface for Custom Embedded Systems) <ref> [46, 47, 199, 200, 203, 155] </ref> is part of the Embedded Operating System (EOS) project at the University of Illinois at Urbana-Champaign. The Choices kernel is implemented on a 10 processor Encore Multimax multiprocessor using the C++ language. <p> Events: Events are asynchronous mechanisms generated by hardware (handled by event mechanism in germs) or by software (kernel provided). 2. Traps: Traps, generated by an executing thread, are handled by kernel-provided or user-provided trap handler objects. 4.9 Renaissance Renaissance [202], a predecessor of Choices <ref> [47, 199, 200, 203] </ref> operating system, is currently under development at Purdue University. It extends the ideas of Choices into a distributed object environment. The goal of Renaissance is to provide transparent access to remote objects that are distributed throughout a network of machines.
Reference: [200] <author> V. Russo, G. Johnston, and R. Campbell. </author> <title> Process management and exception han-dling in multiprocessor operating systems using object-oriented design techniques. </title> <booktitle> In Proceedings of the 3rd Annual Conference on Object-Orientated Programming Systems, Languages, and Applications (OOPSLA 88, SIGPLAN Notices, vol.23, no.11), </booktitle> <pages> pages 248-58, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: KTK is extensible in that new abstractions and functionality (ie., classes, policies, and attributes) are easily added while potentially maintaining a uniform kernel interface (e.g., when not adding any new kernel classes) [92, 147]. 4.8 Choices The operating system family called Choices (Class Hierarchical Open Interface for Custom Embedded Systems) <ref> [46, 47, 199, 200, 203, 155] </ref> is part of the Embedded Operating System (EOS) project at the University of Illinois at Urbana-Champaign. The Choices kernel is implemented on a 10 processor Encore Multimax multiprocessor using the C++ language. <p> A customized system is built using tailored kernel classes and derived germ classes to suit a particular hardware. 4.8.1 Tasks and Threads Choices implement threads in the kernel <ref> [200] </ref>. It supports multiple threads within a single task. Although they are called lightweight threads in [47], according to the terminology introduced in this paper they are middleweight threads. <p> Events: Events are asynchronous mechanisms generated by hardware (handled by event mechanism in germs) or by software (kernel provided). 2. Traps: Traps, generated by an executing thread, are handled by kernel-provided or user-provided trap handler objects. 4.9 Renaissance Renaissance [202], a predecessor of Choices <ref> [47, 199, 200, 203] </ref> operating system, is currently under development at Purdue University. It extends the ideas of Choices into a distributed object environment. The goal of Renaissance is to provide transparent access to remote objects that are distributed throughout a network of machines.
Reference: [201] <author> V. Russo, P. Madany, and R. Campbell. </author> <title> C++ and operating systems performance: a case study. </title> <booktitle> In Proceedings of the USENIX C++ Conference, </booktitle> <pages> pages 103-14, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Object-Oriented Operating Systems (OOOS). In an object-oriented operating system, an object encapsulates a system entity [156, 48]. An object-oriented language is primarily used to implement such an operating system <ref> [201] </ref>; the properties of the language such as data encapsulation, data abstraction, inheritance, polymorphism etc. are used to structure the system. An OOOS may or may not support objects at the user level. Examples of OOOSs are Choices [47] and Renaissance [202, 172]. Object-Supporting Operating Systems (OSOS).
Reference: [202] <author> V. Russo and P. Muckelbauer. </author> <title> Process scheduling and synchronization in the renaissance object-oriented multiprocessor operating system. </title> <booktitle> In Proceedings of the 2nd Usenix Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS II), </booktitle> <pages> pages 117-32, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: An OOOS may or may not support objects at the user level. Examples of OOOSs are Choices [47] and Renaissance <ref> [202, 172] </ref>. Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. <p> Measurements indicate that the standard blocking strategy performs poorly compared to mixed strategies. Among the mixed strategies studied, adaptive algorithms perform better than non-adaptive ones. A few object oriented-operating systems such as Choices [47] and Renaissance <ref> [202] </ref> take an object-oriented approach to lock configuration/customization. These systems define a few basic classes which provide simple and crude locks (implemented using hardware provided instructions). More sophisticated locks are implemented either on top of these existing classes, or by customizing the existing locks (see Section 4.8 and 4.9). <p> Events: Events are asynchronous mechanisms generated by hardware (handled by event mechanism in germs) or by software (kernel provided). 2. Traps: Traps, generated by an executing thread, are handled by kernel-provided or user-provided trap handler objects. 4.9 Renaissance Renaissance <ref> [202] </ref>, a predecessor of Choices [47, 199, 200, 203] operating system, is currently under development at Purdue University. It extends the ideas of Choices into a distributed object environment. The goal of Renaissance is to provide transparent access to remote objects that are distributed throughout a network of machines.
Reference: [203] <author> V. F. Russo. </author> <title> An Object-Oriented Operating System. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: KTK is extensible in that new abstractions and functionality (ie., classes, policies, and attributes) are easily added while potentially maintaining a uniform kernel interface (e.g., when not adding any new kernel classes) [92, 147]. 4.8 Choices The operating system family called Choices (Class Hierarchical Open Interface for Custom Embedded Systems) <ref> [46, 47, 199, 200, 203, 155] </ref> is part of the Embedded Operating System (EOS) project at the University of Illinois at Urbana-Champaign. The Choices kernel is implemented on a 10 processor Encore Multimax multiprocessor using the C++ language. <p> Events: Events are asynchronous mechanisms generated by hardware (handled by event mechanism in germs) or by software (kernel provided). 2. Traps: Traps, generated by an executing thread, are handled by kernel-provided or user-provided trap handler objects. 4.9 Renaissance Renaissance [202], a predecessor of Choices <ref> [47, 199, 200, 203] </ref> operating system, is currently under development at Purdue University. It extends the ideas of Choices into a distributed object environment. The goal of Renaissance is to provide transparent access to remote objects that are distributed throughout a network of machines. <p> A version of Dynix provides a parallel programming library which supports less expensive processes or threads of control. It supports multiple threads of control in one Dynix process. 13 Gracious semaphore is originally introduced in <ref> [203] </ref>. Such a semaphore causes the current process to immediately relinquish the processor when a V message is sent and there are blocked processes. 14 also known as processor/process affinity. 47 4.10.2 Synchronization The Dynix kernel provides locks and semaphores for mutual exclusion and synchronization.
Reference: [204] <author> P. Sadayappan and F. Ercal. </author> <title> Nearest-neighbor mapping of finite element graphs onto processors meshes. </title> <journal> IEEE Transactions On Computers, </journal> <volume> C-36(12):1408-1420, </volume> <month> Dec </month> <year> 1987. </year>
Reference-contexts: A static scheduler offers low runtime scheduling overhead [78], but it also assumes a stable parallel application. This is a reasonable assumption for many large-scale scientific applications in which parallelism is derived by decomposition of regular data domains <ref> [204] </ref>.
Reference: [205] <author> J. Salmon and S. Callahan. Moose: </author> <title> A multitasking os for hypercubes. </title> <booktitle> In Proceedings of the 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Pasadena, CA, </address> <pages> pages 391-396. </pages> <publisher> ACM, JPL, </publisher> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: The primary motivation behind the design of these systems was to decentralize the structure of an operating system running on a single computer. On the other hand, the motivation behind the latter message passing systems such as RIG [191], V [54], Accent [193], and various hypercube operating systems <ref> [205, 221] </ref> was to build an operating system on a structure of distributed computers. In contrast to the fine-grained protection of capability systems, network based message passing systems rely on a coarse-grained protection mechanism. Communication facilities based on messages transparently permit both local and remote communication. <p> Simpler and higher performance implementations of services or threads attached to messages are being implemented in systems like those designed by Dally at MIT [66]. In other work, slight generalizations of the functionality of the Crystalline operating system are the MOOSE operating system <ref> [205] </ref> designed at CalTech and the early commercial operating systems developed at Intel and NCube for their hypercube machines.
Reference: [206] <author> J. Saltzer and M. Schroeder. </author> <title> The protection of information in computer systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 63(9) </volume> <pages> 1278-1308, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: This principle states that each program and each 5 user of a system should operate using the least set of privileges necessary to complete a job <ref> [206] </ref>. Unfortunately, the principle's implementations often implied the Principle of Least Performance, which means that anything that was protected was also expensive to access, so that users attempted to avoid using a system's protection mechanisms, and implementors of commercial operating systems avoided using protection mechanisms to the maximum extent possible.
Reference: [207] <author> M. Schroeder and M. Burrows. </author> <title> Performance of firefly rpc. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Subsystems present themselves to one another in terms of interfaces implemented by servers. The absence of a single, uniform address space is compensated by automatic stub compilers and sophisticated runtime libraries <ref> [180, 110, 207] </ref> that transfer complex arguments in messages.
Reference: [208] <author> P. Schuller, H. Hartig, W. Kuhnhauser, and H. Streich. </author> <title> Performance of the birlix operating system. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 147-160, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Furthermore, the use of common underlying services provides support for the coexistence and interoperability of multiple operating system environments on a single host as user-level programs [32]. Mach [32], Chorus [197], KeyKOS [42], QNX [106], and BirLiX <ref> [208] </ref> are a few examples of micro-kernel based operating systems. 2.8 Application-specific Operating Systems Many application domains impose specific requirements on operating system functionality, performance, and structure.
Reference: [209] <author> Karsten Schwan, Thomas E. Bihari, and Ben Blake. </author> <title> Adaptive, reliable software for distributed and parallel, real-time systems. </title> <booktitle> In Proceedings of the Sixth Symposium on Reliability in Distributed Software, </booktitle> <address> Williamsburg, Virginia, </address> <pages> pages 32-44. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1987. </year>
Reference-contexts: This is because in contrast to other parallel or distributed application software, the control software of real-time systems cannot be termed reliable unless it exhibits two key attributes <ref> [209] </ref>: (1) computations must complete within well-defined timing constraints, and (2) programs must exhibit predictable behavior in the presence of uncertain operating environments [210, 27]. <p> Operating software, then, must have direct access to the underlying resources typically controlled by the operating system, and for complex applications, it must deal with uncertainty in operating environments by even permitting programs or operating system components to adapt [215, 129] (i.e., change at runtime) in performance <ref> [209] </ref> and functionality during system execution [210, 213, 26, 96]. While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs.
Reference: [210] <author> Karsten Schwan, Tom Bihari, Bruce W. Weide, and Gregor Taulbee. </author> <title> High-performance operating system primitives for robotics and real-time control systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(3) </volume> <pages> 189-231, </pages> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: is because in contrast to other parallel or distributed application software, the control software of real-time systems cannot be termed reliable unless it exhibits two key attributes [209]: (1) computations must complete within well-defined timing constraints, and (2) programs must exhibit predictable behavior in the presence of uncertain operating environments <ref> [210, 27] </ref>. <p> must have direct access to the underlying resources typically controlled by the operating system, and for complex applications, it must deal with uncertainty in operating environments by even permitting programs or operating system components to adapt [215, 129] (i.e., change at runtime) in performance [209] and functionality during system execution <ref> [210, 213, 26, 96] </ref>. While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs.
Reference: [211] <author> Karsten Schwan and Win Bo. </author> <title> Topologies distributed objects on multicomputers. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(2) </volume> <pages> 111-157, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Such operating systems, therefore, might be structured as collections of cooperating objects, where object invocations may result in messages, in memory sharing, or in both, and where objects themselves may internally be structured as collections of cooperating objects or even as fragmented object <ref> [226, 57, 211, 159] </ref>. 2.4 Language Based Mechanisms Single language systems. <p> For example, the Ada rendezvous mechanism leads to well-known performance problems [43], the global addressing mechanisms and fixed language semantics of Linda can lead to inefficiencies concerning the update and access of remote information <ref> [211] </ref>, and heap maintenance has been shown difficult for languages like Smalltalk [260, 186]. Another inherent problem with language based systems can be protection, where language-level typing mechanisms must be mapped to the protection mechanisms available in the underlying operating system [14, 254], which is not always easily done. <p> the internally parallel operating system servers offered in the Eden system [134] or in CHAOS [213, 92] and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro [227, 229, 98] for distributed systems, in `Topologies' <ref> [211] </ref> for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines. <p> In addition, in distributed memory machines like the Intel iPSC series, efficient state sharing is necessitated by significant differences in access times to local vs. remote information <ref> [211] </ref>. This is leading to redesigns of the Mach system paging mechanisms and page servers for its implementation on the Intel Paragon multiprocessor, and it is resulting in more general research on shared state or objects in distributed or shared memory parallel machine [211, 57]. <p> This is leading to redesigns of the Mach system paging mechanisms and page servers for its implementation on the Intel Paragon multiprocessor, and it is resulting in more general research on shared state or objects in distributed or shared memory parallel machine <ref> [211, 57] </ref>. <p> However, in contrast to the OS support for UMA and NUMA multiprocessors, synchronization abstractions for distributed memory machines can often be optimized substantially if they can be made programmable by application programmers or if synchronization can be combined with other communications being performed in application programs <ref> [211, 87] </ref>. 3.3.2 Other Synchronization Constructs Condition Variables. Condition variables make it possible for a thread to suspend its execution while awaiting an action by some other thread. A condition variable is associated with some shared variables protected by a mutex and a predicate (based on the shared variables). <p> A count may be associated with an event, which enables a process to wait for a particular occurrence of an event. More complicated event structures have been shown useful for several application domains and target machines, most prominently including the event handling facilities for active messages <ref> [151, 211] </ref> or the synchronization points designed by Gheith for real-time applications [92]. <p> Communication issues specific to hypercube or mesh machines are reviewed elsewhere <ref> [222, 221, 211, 67] </ref>. Examples of recent research in communication protocols for high performance or parallel machines are addressing the association of computational activities with messages [66, 151], the user-driven configuration of communication protocols for improved performance [110], and the parallelization of protocol processing [147]. <p> This is the ability provided by subcontracts in Spring, by attributes and policies in CHAOS, and by invocation attributes associated with individual accesses to fragmented objects built for hypercube machines in <ref> [211] </ref> and for multiprocessors in [57]. Sample variations in invocation semantics not concerned with the types of objects or operations being invoked include asynchronous vs. synchronous invocations, the ability to wait for acknowledgements of invocation receipt [211] as in distributed RPC implementations [180], the variation of when an invocation is considered <p> invocation attributes associated with individual accesses to fragmented objects built for hypercube machines in <ref> [211] </ref> and for multiprocessors in [57]. Sample variations in invocation semantics not concerned with the types of objects or operations being invoked include asynchronous vs. synchronous invocations, the ability to wait for acknowledgements of invocation receipt [211] as in distributed RPC implementations [180], the variation of when an invocation is considered complete (upon successful transmission of the invocation, upon receipt of the invocation, upon invocation completion, etc.), and the association of additional parameters governing how and when invocations are scheduled or processed, the latter being particularly important <p> Simulation studies of their hardware proposals show its usefulness in several applications, including dynamic load balancing, sorting, work distribution in DIB [82], etc. As with Livny and Manber's work, research on an active message construct called `topologies' on hypercubes <ref> [211] </ref> and `distributed shared abstractions' on shared memory multiprocessors [57] is based on previous work in message-based operating systems for hypercubes, since it is assuming the presence of reliable message delivery and of processes on the in 50 dividual nodes of the hypercube. <p> The event-driven execution model of `topologies' is similar to the execution model supported by the reactive kernel [221] for the Symult series of multi-computers, which schedules user processes according to conditions that concern the receipt of messages for which processes are waiting. A sample object fragment as defined in <ref> [211] </ref> and in [57] appears in Figure 12. The basic idea of active messages is reflected here, in terms of user threads and service routines closely associated with the incoming or outgoing message buffers of the communication topology linking object fragments.
Reference: [212] <author> Karsten Schwan, Harold Forbes, Ahmed Gheith, Bodhisattwa Mukherjee, and Yiannis Samiotakis. </author> <title> A cthread library for multiprocessors. </title> <type> Technical Report GIT-ICS-91/02, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: User-level threads are managed by runtime library routines linked into each application. A thread management operation does not require an expensive kernel call. Furthermore, lightweight threads enable an application program to use a thread management system, most appropriate to the problem domain. Mach Cthreads <ref> [60, 173, 212] </ref>, the University of Washington threads [165, 9], SunOS LWP and threads [113, 127, 189], are a few popular lightweight thread implementations. A lightweight thread generally executes in the context of a middleweight or a heavyweight thread.
Reference: [213] <author> Karsten Schwan, Prabha Gopinath, </author> <title> and Win Bo. Chaos kernel support for objects in the real-time domain. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(8):904-916, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. Sample OSOSs are SOS [228], Cool [102], and CHAOS <ref> [213, 92, 91] </ref> for parallel machines, and Chorus [197] and Clouds [68] for distributed machines. OSOSs can further be classified into different groups depending on the kind of objects they support. <p> Examples of such uses are the internally parallel operating system servers offered in the Eden system [134] or in CHAOS <ref> [213, 92] </ref> and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro [227, 229, 98] for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines. <p> Unresolved issues with object-oriented operating systems include the efficient representation of object invocations, where it has become clear that `not all invocations are equal', ranging from rapid unreliable invocations useful in real-time multiprocessor applications <ref> [213, 92] </ref> to reliable multicast invocations required for certain distributed programs [103]. In addition, as with remote procedure calls, it is unclear what levels of machine and language support are required for efficient implementation of object invocations (e.g., for parameter marshaling [103] or for crossing protection boundaries [258, 80]). <p> must have direct access to the underlying resources typically controlled by the operating system, and for complex applications, it must deal with uncertainty in operating environments by even permitting programs or operating system components to adapt [215, 129] (i.e., change at runtime) in performance [209] and functionality during system execution <ref> [210, 213, 26, 96] </ref>. While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs. <p> While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs. For instance, in the CHAOS and CHAOS arc operating systems <ref> [96, 213, 92] </ref>, operating software implementing either application or operating system functionality consists of a number of autonomous objects, each providing a number of operations (entry points) that can be invoked by other objects. Such functionality appears no different from what is offered by other object-oriented operating systems. <p> Such functionality appears no different from what is offered by other object-oriented operating systems. However, addressing the requirements of real-time programs, CHAOS arc object invocations range from reliable invocations that maintain parameters and return information (or even communication `streams' <ref> [213] </ref>) to invocations that implement unreliable `control signals' or `pulses' [213, 253]. <p> Such functionality appears no different from what is offered by other object-oriented operating systems. However, addressing the requirements of real-time programs, CHAOS arc object invocations range from reliable invocations that maintain parameters and return information (or even communication `streams' [213]) to invocations that implement unreliable `control signals' or `pulses' <ref> [213, 253] </ref>. <p> of the attributes of StarOS of interest to parallel systems is its definition of modules, functions, and module invocations, which are the blueprint for the implementation of similar functionality in the Intel 432's iMAX operating system [63] and in modern object-oriented operating systems like Eden [134], Choices [47], and CHAOS <ref> [213, 92] </ref>. A module defines an object by exporting a set of invocable functions. A function invocation by a process is performed asynchronously by passing invocation parameters to a process designated to execute the function. <p> the implementation include the utility of alternative internal representations of objects, so that an object can maintain its own internal parallelism and control any concurrency imposed upon it by other objects and the development of synchro object useful for efficient representation of complex events. 4.7 KTK The Kernel ToolKit (KTK) <ref> [213, 92, 91, 178] </ref> under development at the Georgia Institute of Technology is a configurable object-based operating system kernel (designed using micro-kernel technology). The major design goal of the KTK project is to provide explicit support for 42 on- and off-line program configuration.
Reference: [214] <author> Karsten Schwan and Anita K. Jones. </author> <title> Specifying resource allocation for the cm* multiprocessor. </title> <journal> IEEE Software, </journal> <volume> 3(3) </volume> <pages> 60-70, </pages> <month> May </month> <year> 1984. </year> <month> 66 </month>
Reference-contexts: protection in very large address spaces, deadlock prevention, exception handling for large-scale parallel programs, the efficient representation of asynchronous active entities like processes or threads, the provision of alternative communication schemes and synchronization mechanisms, and resource scheduling like process assignment to different processors and data placement in physically distributed memory <ref> [214] </ref>, and finally, the parallelization of the operating system itself, again in order to provide scalable performance for varying application requirements [89, 109, 270]. A second major reason for using a multiprocessor system is to provide high reliability, and graceful degradation in the event of failure. <p> Other scheduling algorithms appear in [267] and [141]. In this section, we focus on dynamic scheduling [164], and on scheduling for shared memory machines, where variations in distances between different processors on the parallel machine <ref> [39, 214] </ref> are not considered. Static and Dynamic Scheduling: A static scheduler makes a one time decision per job regarding the number of processors to be allocated. Once decided, the job is guaranteed to have exactly that number of processors whenever it is active.
Reference: [215] <author> Karsten Schwan and Rajiv Ramnath. </author> <title> Adaptable operating software for manufacturing systems and robots: </title> <booktitle> A computer science research agenda. In Proceedings of the 5th Real-Time Systems Symposium, </booktitle> <address> Austin, Texas, </address> <pages> pages 255-262. </pages> <publisher> IEEE, </publisher> <month> Dec. </month> <year> 1984. </year>
Reference-contexts: Operating software, then, must have direct access to the underlying resources typically controlled by the operating system, and for complex applications, it must deal with uncertainty in operating environments by even permitting programs or operating system components to adapt <ref> [215, 129] </ref> (i.e., change at runtime) in performance [209] and functionality during system execution [210, 213, 26, 96]. While many embedded real-time operating systems are offering functionality akin to multiuser systems, they do not impose any restrictions on resource use and reservation by application programs.
Reference: [216] <author> Karsten Schwan, Hongyi Zhou, and Ahmed Gheith. </author> <title> Multiprocessor real-time threads. </title> <journal> Operating Systems Review, </journal> <volume> 25(4) </volume> <pages> 35-46, </pages> <month> Oct. </month> <year> 1991. </year> <note> Also appears in the Jan. 1992 issue of Operating Systems Review. </note>
Reference-contexts: We are not aware of general solutions to the multi-level scheduling problem, other than the actual exchange or configuration of the operating system's threads scheduler by application programs, as often done in real-time systems <ref> [216] </ref>. 3.1.2 Scheduler Structures As with other operating system services for parallel machines, schedulers themselves must be structured to be scalable to different size target machines and to different application requirements.
Reference: [217] <author> M. Scott, T. Leblanc, and B. Marsh. </author> <title> Design rationale for psyche, a general purpose multiprocessor operating system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing (V II Software), </booktitle> <pages> pages 255-262, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Processes communicate using a common object as an intermediary. To avoid the need for compiler support, Elmwood requires an object programmer to provide a dispatcher for an object's entries, including initialization. 4.5 Psyche Psyche <ref> [217, 218, 219, 220, 80] </ref> is a general-purpose operating system for large-scale shared-memory multiprocessors developed at the University of Rochester. Psyche is implemented on the BBN Butterfly Plus hardware.
Reference: [218] <author> M. Scott, T. Leblanc, and B. Marsh. </author> <title> Evolution of an operating system for large scale shared-memory multiprocessors. </title> <type> Technical Report TR 309, </type> <institution> Department of Computer Science, University of Rochester, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Processes communicate using a common object as an intermediary. To avoid the need for compiler support, Elmwood requires an object programmer to provide a dispatcher for an object's entries, including initialization. 4.5 Psyche Psyche <ref> [217, 218, 219, 220, 80] </ref> is a general-purpose operating system for large-scale shared-memory multiprocessors developed at the University of Rochester. Psyche is implemented on the BBN Butterfly Plus hardware.
Reference: [219] <author> M. Scott, T. Leblanc, and B. Marsh. </author> <booktitle> Multi-model parallel programming in psyche. In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 70-78, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: On the other hand, system configurability is a property CHAOS arc shares with many current high-performance operating systems, including the Synthesis kernel [163, 162], the Psyche system and its related research <ref> [219, 168] </ref>, Presto [24, 23], and others [24, 23, 52]. <p> Processes communicate using a common object as an intermediary. To avoid the need for compiler support, Elmwood requires an object programmer to provide a dispatcher for an object's entries, including initialization. 4.5 Psyche Psyche <ref> [217, 218, 219, 220, 80] </ref> is a general-purpose operating system for large-scale shared-memory multiprocessors developed at the University of Rochester. Psyche is implemented on the BBN Butterfly Plus hardware.
Reference: [220] <author> M. Scott, T. Leblanc, B. Marsh, T. Becker, C. Dubnicki, E. Markatos, and N. Smithline. </author> <title> Implementation issues for the psyche multiprocessor operating system. </title> <journal> Computing Systems, </journal> <volume> 3(1) </volume> <pages> 101-137, </pages> <month> Winter </month> <year> 1990. </year>
Reference-contexts: In [136], the authors present the implementation of the memory management system in the Psyche <ref> [220] </ref> multiprocessor operating system. The Psyche memory management system is structured into four layers of abstraction - NUMA, UMA, VUMA (virtual memory) and PUMA (Psyche memory). These abstractions are shown in Figure 7 and are discussed in Section 4.5. <p> Processes communicate using a common object as an intermediary. To avoid the need for compiler support, Elmwood requires an object programmer to provide a dispatcher for an object's entries, including initialization. 4.5 Psyche Psyche <ref> [217, 218, 219, 220, 80] </ref> is a general-purpose operating system for large-scale shared-memory multiprocessors developed at the University of Rochester. Psyche is implemented on the BBN Butterfly Plus hardware.
Reference: [221] <author> C. Seitz. </author> <title> Reactive kernel. </title> <booktitle> In Proceedings of the 3rd Conf. On Hypercube Concurrent Computers and Applications, </booktitle> <address> Pasadena, CA, </address> <pages> pages 1520-1528. </pages> <publisher> ACM, </publisher> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: The primary motivation behind the design of these systems was to decentralize the structure of an operating system running on a single computer. On the other hand, the motivation behind the latter message passing systems such as RIG [191], V [54], Accent [193], and various hypercube operating systems <ref> [205, 221] </ref> was to build an operating system on a structure of distributed computers. In contrast to the fine-grained protection of capability systems, network based message passing systems rely on a coarse-grained protection mechanism. Communication facilities based on messages transparently permit both local and remote communication. <p> Communication issues specific to hypercube or mesh machines are reviewed elsewhere <ref> [222, 221, 211, 67] </ref>. Examples of recent research in communication protocols for high performance or parallel machines are addressing the association of computational activities with messages [66, 151], the user-driven configuration of communication protocols for improved performance [110], and the parallelization of protocol processing [147]. <p> The event-driven execution model of `topologies' is similar to the execution model supported by the reactive kernel <ref> [221] </ref> for the Symult series of multi-computers, which schedules user processes according to conditions that concern the receipt of messages for which processes are waiting. A sample object fragment as defined in [211] and in [57] appears in Figure 12.
Reference: [222] <author> Charles Seitz and William Athas. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <journal> IEEE Computer, </journal> <volume> 21(8) </volume> <pages> 9-24, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Communication issues specific to hypercube or mesh machines are reviewed elsewhere <ref> [222, 221, 211, 67] </ref>. Examples of recent research in communication protocols for high performance or parallel machines are addressing the association of computational activities with messages [66, 151], the user-driven configuration of communication protocols for improved performance [110], and the parallelization of protocol processing [147].
Reference: [223] <institution> Sequent Computer Systems, Inc. Dynix Programmer's Manual, </institution> <year> 1986. </year>
Reference-contexts: Other important synchronizations classes include the Semaphore Class (implements Dijkstra's counting semaphore [72]), the GraciousSemaphore 13 class, the BusyWaitingReadWriteLock and the BlockingReadWrite-Lock classes, the TimedSemaphore class (augments a counting semaphore with a timeout mechanism), and the Event class. 4.10 DYNIX The Dynix operating system <ref> [224, 223] </ref> is an enhanced version of the Unix operating system developed to run on the Sequent multiprocessors. The Sequent multiprocessor is a bus based uniform access shared memory (UMA) machine. 4.10.1 Process Management and Scheduling Like Unix, Dynix supports heavyweight processes.
Reference: [224] <institution> Sequent Computer Systems, Inc. </institution> <note> Symmetry Technical Summary, Rev 1.4, </note> <year> 1987. </year>
Reference-contexts: Uniform Memory Access (UMA) multiprocessors. In an UMA architecture, the access time to shared memory is the same for all processors. A sample UMA architecture is the bus based architecture of the Sequent multiprocessor <ref> [224] </ref>, where a common bus links several memory modules to computing modules consisting of a cache shared by two processor elements, and I/O devices are attached directly to the bus. 2. Non-Uniform Memory Access (NUMA) multiprocessors. <p> Other important synchronizations classes include the Semaphore Class (implements Dijkstra's counting semaphore [72]), the GraciousSemaphore 13 class, the BusyWaitingReadWriteLock and the BlockingReadWrite-Lock classes, the TimedSemaphore class (augments a counting semaphore with a timeout mechanism), and the Event class. 4.10 DYNIX The Dynix operating system <ref> [224, 223] </ref> is an enhanced version of the Unix operating system developed to run on the Sequent multiprocessors. The Sequent multiprocessor is a bus based uniform access shared memory (UMA) machine. 4.10.1 Process Management and Scheduling Like Unix, Dynix supports heavyweight processes.
Reference: [225] <author> K. Sevcik. </author> <title> Characterizations of parallelism in applications and their use in scheduling. </title> <booktitle> In Proceedings of the 1989 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 171-80, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: dynamic distribution of processing loads among the processors of the parallel machine. 14 so that a static processor allocation rapidly becomes inefficient and (2) large-scale parallel machines are often used in multi-user mode, so that scheduling must take into account the requirements of multiple parallel applications sharing a single machine <ref> [158, 157, 225, 65] </ref>. J. Zahorjan and C. McCann compare the performance of static and dynamic schedulers for multi-user workloads.
Reference: [226] <author> M. Shapiro. </author> <title> Structure and encapsulation in distributed systems: The proxy principle. </title> <booktitle> In Proceedings of the Sixth International Conference on Distributed Computing Systems, </booktitle> <address> Boston, Mass., </address> <pages> pages 198-204. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1986. </year>
Reference-contexts: Such operating systems, therefore, might be structured as collections of cooperating objects, where object invocations may result in messages, in memory sharing, or in both, and where objects themselves may internally be structured as collections of cooperating objects or even as fragmented object <ref> [226, 57, 211, 159] </ref>. 2.4 Language Based Mechanisms Single language systems.
Reference: [227] <author> M. Shapiro. </author> <title> Object-support operating systems. </title> <journal> TCOS Newsletter, </journal> <volume> 5(1) </volume> <pages> 39-42, </pages> <year> 1991. </year>
Reference-contexts: Such systems may be broadly classified as object-oriented or object 8 supporting operating systems, depending on their internal structures and on the interfaces they provide to the user level <ref> [227, 198, 83] </ref>. Object-Oriented Operating Systems (OOOS). In an object-oriented operating system, an object encapsulates a system entity [156, 48]. <p> Examples of such uses are the internally parallel operating system servers offered in the Eden system [134] or in CHAOS [213, 92] and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro <ref> [227, 229, 98] </ref> for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines. <p> are even more pronounced in distributed machines like sets of workstations connected via high-speed networks [55], notions of distributed objects have been a topic of research for such systems for quite some time, as evidenced by work on Clouds [69] at Georgia Tech, on Chorus [197], and on fragmented objects <ref> [227] </ref> in France. 3.3 Synchronization When multiple cooperating processes execute simultaneously, synchronization primitives are needed for concurrency control. When multiple processes share an address space, synchronization is required for shared memory consistency [99].
Reference: [228] <author> M. Shapiro, Y. Gourhant, S. Habert, L. Mosseri, M. Ruffin, and C. Valot. </author> <title> Sos: An object-oriented operating system assessment and perspectives. </title> <journal> Computing Systems, </journal> <volume> 2(4) </volume> <pages> 287-338, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Examples of OOOSs are Choices [47] and Renaissance [202, 172]. Object-Supporting Operating Systems (OSOS). An object supporting operating system is not necessarily structured in an object-oriented fashion. However, it supports objects at the user level; the objects are typically language independent. Sample OSOSs are SOS <ref> [228] </ref>, Cool [102], and CHAOS [213, 92, 91] for parallel machines, and Chorus [197] and Clouds [68] for distributed machines. OSOSs can further be classified into different groups depending on the kind of objects they support.
Reference: [229] <author> M. Shapiro and M. Makpangou. </author> <title> Distributed abstractions, lightweight references. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 263-7, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Examples of such uses are the internally parallel operating system servers offered in the Eden system [134] or in CHAOS [213, 92] and Presto [24, 23], the association of protection boundaries with certain objects as intended in Psyche [80], or the internally fragmented objects offered by Shapiro <ref> [227, 229, 98] </ref> for distributed systems, in `Topologies' [211] for hypercube machines, and in `Distributed Shared Abstractions' [57] for multiprocessor engines.
Reference: [230] <author> M.H. Solomon and R.A. Finkel. </author> <title> The roscoe distributed operating system. </title> <booktitle> In Proceedings of the 7th Symposium on Operating System Principles, Asilomar, </booktitle> <address> CA, </address> <pages> pages 108-114. </pages> <publisher> Assoc. Comput. Mach., </publisher> <month> Dec.10-12 </month> <year> 1979. </year>
Reference-contexts: Exchange of messages is a more abstract form of communication than accessing shared memory locations. Message passing subsumes communication, buffering, and synchronization. 26 Multiprocessor operating systems have experimented with a large variety of different com-munication abstractions, including ports [84, 262] mailboxes [119, 63], links <ref> [230, 127] </ref> etc. From an implementation point of view, such abstractions are kernel-handled message buffers. They may be either unidirectional or bidirectional. A process may send to or may receive messages from them. There may be rights (like send, receive, or ownership rights) associated with these entities. <p> A third concept in parallel operating systems originating with both StarOS and the Roscoe operating system <ref> [230] </ref> is the structuring of operating systems and even operating system kernels as micro-kernels (called a nucleus in StarOS). Specifically, a small subset of the StarOS functions, called instructions, are defined to execute sequentially and synchronously with the invoking function. Collectively they are referred to as the nucleus. <p> Similarly, the StarOS system along with Roscoe <ref> [230] </ref> and Medusa [184] provides early implementations of micro-kernels, of user-level operating system services, of internally parallel or reliable system services, and of alternative operating system constructs providing similar functionalities at differing costs.
Reference: [231] <author> A. Spector, D. Daniels, D. Duchamp, J. Eppinger, and R. Pausch. </author> <title> Distributed transactions for reliable systems. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operation Systems Principles, </booktitle> <pages> pages 127-146. </pages> <publisher> ACM SIGOPS, </publisher> <month> Nov. </month> <year> 1987. </year> <month> 67 </month>
Reference-contexts: Other examples of application-dependent operating system structures occur for database systems, where an operating system's I/O facilities and networking facilities may be determined or at least strongly affected by the primary application running on this system: a large-scale database performing transaction processing <ref> [231] </ref>. 11 3 Design Issues The basic functionality of a multiprocessor operating system must include most what is present in uniprocessor systems. However, complexities arise due to the additional functional capabilities in multiprocessor hardware and more importantly, due to the extreme requirements of performance imposed on the operating system.
Reference: [232] <author> M. Squillante and R. Nelson. </author> <title> Analysis of task migration in shared-memory multi-processor scheduling. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 143-55, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Processor load balancing 2 is considered to be a part of a scheduling policy <ref> [232] </ref>. Basic theoretical results on static process scheduling on parallel machines show that the scheduling problem is NP-hard; static algorithms minimizing average response time include those described in [164] and [39]. Other scheduling algorithms appear in [267] and [141].
Reference: [233] <author> Harold S. Stone. </author> <title> High-performance computer architecture. </title> <publisher> Addison-Wesley Pub. Co., </publisher> <address> Reading, Mass., </address> <year> 1987. </year>
Reference-contexts: Brief survey of multiprocessor hardware. Several current textbooks in parallel computing provide good overviews of parallel machine architectures <ref> [111, 6, 233, 188] </ref>. For purposes of this paper, we briefly review some of the major types of parallel machines, eliding architectures for SIMD programs, functional programs, and systolic applications. <p> An interconnection network, which may be static or dynamic, facilitates communication among processors and memory modules. A few sample interconnection networks are: time shared or common buses, crossbar switches, hierarchical switches, and multistage networks. The design, structure and performance of various interconnection networks have been reviewed in other literature <ref> [264, 111, 6, 233, 188] </ref> and are beyond the scope of this survey. The variety of different kinds of multiprocessor architectures coupled with diverse application requirements have resulted in many different designs, goals, features, and implementations of multiprocessor operating systems, in university research projects and in the commercial domain.
Reference: [234] <author> M. Stumm and S. Zhou. </author> <title> Algorithms implementing distributed shared memory. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 55-64, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: We briefly discuss several memory management algorithms for NUMA multiprocessors. Various multiprocessor operating systems such as Mach, Psyche, and PLATINUM use a variation or a mix of these algorithms. The algorithms described below are categorized by whether they migrate and/or replicate data <ref> [234, 271] </ref>. One algorithm migrates data to the site where it is accessed in an attempt to exploit locality in data accesses and decreases the number of remote accesses. Two other algorithms replicate data so that multiple read accesses can happen at the same time using local accesses. <p> Keeping the data copies consistent is a major concern in this algorithm. A number of algorithms are available for this purpose. One of them is similar to the write-update algorithm for cache consistency. A few such algorithms are discussed in <ref> [234] </ref>, and some specific NUMA memory management schemes are described for the individual parallel operating systems in Section 4.
Reference: [235] <author> H. Sturgis. </author> <title> A postmortem for a time sharing system. </title> <type> Technical Report CSL-1974-001, </type> <institution> Xerox, </institution> <year> 1974. </year>
Reference-contexts: Parameter passing is by reference between the caller and the callee. The implementation of capability based addressing has been carried out using both software and hardware techniques. A few sample software based systems are Hydra [258] and Cal/Tss <ref> [235] </ref>. CAP [179] and the Intel/432 [62] are examples of hardware based systems. Despite early favorable predictions, system builders have been largely unsuccessful in implementing and programming capability based systems which perform as well as machines based on more traditional memory reference models [132, 59].
Reference: [236] <author> SUN. </author> <title> The SPARC Architecture Manual. </title> <institution> Sun Microsystems Inc., </institution> <note> No. 800-199-12, Version 8, </note> <month> January </month> <year> 1991. </year>
Reference-contexts: Mechanisms for memory or state sharing have significant effects on the performance of parallel and distributed applications. This is demonstrated by recent designs of and experimentation with alternative memory models for large scale multiprocessors that exhibit NUMA memory characteristics due to their use of caches to reduce communication latencies <ref> [90, 236] </ref>. In addition, in distributed memory machines like the Intel iPSC series, efficient state sharing is necessitated by significant differences in access times to local vs. remote information [211].
Reference: [237] <author> Liba Svobodova. </author> <title> Resilient distributed computing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 257-267, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: One self-imposed limitation of this survey is its focus on performance rather than reliability in parallel systems. Reliable systems are surveyed in several recent articles, including <ref> [237, 28, 149] </ref>. A second limitation of this survey is its treatment of operating system kernels rather than operating systems, thereby neglecting system functionalities like file systems, database support, network protocols, and others.
Reference: [238] <author> R. J. Swan, S. H. Fuller, and D. P. Siewiorek. </author> <title> Cm*: A modular, multi-microprocessor. </title> <booktitle> In Proceedings of the National Computer Conference, </booktitle> <pages> pages 637-644. </pages> <publisher> Assoc. Comput. Mach., </publisher> <year> 1977. </year>
Reference-contexts: This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs [120, 258]. 4.2 StarOS StarOS [117, 116, 118, 119] is an experimental operating system for the Cm* <ref> [121, 89, 77, 88, 238] </ref> multi-microprocessor computer developed at Carnegie-Mellon University. The design of StarOS is influenced by the protection mechanisms of Hydra, the underlying Cm* architecture, and its principal goals of achieving high performance and reliability for parallel machine users. Cm* is a Non Uniform Memory Access (NUMA) machine.
Reference: [239] <author> A. Tanenbaum. </author> <title> Operating Systems: Design and Implementation. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1987. </year>
Reference-contexts: The horizontal organization leads to a compartmentalization of the kernel in which all synchronization is subsumed by message passing. The horizontal organization closely mimics the hardware organization of a distributed memory multicomputer. Demos [16] and Minix <ref> [239] </ref> are examples of horizontal kernels. 2.7 Micro-kernel Based Operating Systems Micro-kernel based operating systems are structured as a collection of system servers running on top of a minimal kernel (see Figure 6). The micro-kernel itself only implements the lowest-level (mostly hardware dependent) functions of an operating system. <p> Communication between processes using these primitives may be synchronous or asynchronous, etc. Many issues must be considered when designing an inter-process communication mechanism; they are reviewed in numerous surveys of distributed operating systems <ref> [241, 239, 240] </ref> and are not discussed in detail here.
Reference: [240] <author> A. Tanenbaum. </author> <title> Modern operating systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., 2nd edition, </address> <year> 1992. </year>
Reference-contexts: Communication between processes using these primitives may be synchronous or asynchronous, etc. Many issues must be considered when designing an inter-process communication mechanism; they are reviewed in numerous surveys of distributed operating systems <ref> [241, 239, 240] </ref> and are not discussed in detail here.
Reference: [241] <author> A. Tanenbaum and R. Van Renesse. </author> <title> Distributed operating systems. </title> <journal> Computing Surveys, </journal> <volume> 17(4) </volume> <pages> 419-470, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Communication between processes using these primitives may be synchronous or asynchronous, etc. Many issues must be considered when designing an inter-process communication mechanism; they are reviewed in numerous surveys of distributed operating systems <ref> [241, 239, 240] </ref> and are not discussed in detail here.
Reference: [242] <author> A. Tevanian and R. Rashid. </author> <title> Mach: A basis for future unix development. </title> <type> Technical Report CMU-CS-87-139, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Moreover, the reconfiguration module dynamically configures the system to handle hardware and software faults. For example, if the physical environment changes (e.g., addition or removal of clusters), StarOS can be expanded or reduced to accommodate such changes. 4.3 Mach Mach <ref> [3, 191, 242, 243, 95, 192, 15] </ref> is a multiprocessor operating system kernel developed at Carnegie-Mellon University first for distributed systems, then for tightly-coupled UMA multiprocessors. Later extensions of Mach also address NUMA and NORMA machines [255].
Reference: [243] <author> A. Tevanian, R. Rashid, D. Golub, D. Black, E. Cooper, and M. Young. </author> <title> Mach threads and the unix kernel: The battle for control. </title> <booktitle> In Proceedings of the Summer 1987 USENIX Conference, </booktitle> <pages> pages 185-97, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Moreover, the reconfiguration module dynamically configures the system to handle hardware and software faults. For example, if the physical environment changes (e.g., addition or removal of clusters), StarOS can be expanded or reduced to accommodate such changes. 4.3 Mach Mach <ref> [3, 191, 242, 243, 95, 192, 15] </ref> is a multiprocessor operating system kernel developed at Carnegie-Mellon University first for distributed systems, then for tightly-coupled UMA multiprocessors. Later extensions of Mach also address NUMA and NORMA machines [255].
Reference: [244] <author> A. Tevanian, R. Rashid, M. Young, D. Golub, M. Thompson, W. Bolosky, and R. Sanzi. </author> <title> A unix interface for shared memory and memory mapped files under mach. </title> <booktitle> In Proceedings of the Summer 1987 USENIX Conference, </booktitle> <pages> pages 53-67, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Using these data structures, Mach supports large, sparse virtual address spaces and memory mapped files <ref> [1, 194, 244] </ref>. Mach implements a single level store by treating all primary memory as a cache for virtual memory objects. Mach allows tasks to allocate and deallocate regions of virtual memory, and to set the protection and inheritance of virtual memory regions.
Reference: [245] <author> D. Thiebaut and H. Stone. </author> <title> Footprints in the cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Identical problems arise when programs' processes are engaged in producer/consumer relationships. * Frequent context switches occur when the number of processes greatly exceeds the number of processors. * When a processor is interleaved between multiple address space, cache misses can be a major source of performance degradation <ref> [245] </ref>. Careful application design and coscheduling may handle the problems associated with spinlock-controlled critical sections and those with producer-consumer processes, but they do not address performance degradation due to cache corruption or frequent context switches.
Reference: [246] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <note> The Connection Machine CM-5 Technical Summary, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: In this class of architectures, each processor has its own local memory that is not shared by other processors in the system. Hypercubes like the NCube multiprocessors, past Intel iPSC machines and current Intel iSC mesh machines [114, 115], the Thinking Machines CM-5 <ref> [246, 140] </ref>, and workstation clusters are examples of non-shared memory multiprocessors. Workstation clusters differ from hypercube or mesh machines in that the latter typically offer specialized hardware for low-latency inter-machine communication and also for implementation of selected global operations like global synchronization, addition, or broadcast.
Reference: [247] <author> R. Thomas and W. Crowther. </author> <title> The uniform system: An approach to runtime support for large scale shared memory parallel processors. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, V. II Software, </booktitle> <pages> pages 245-254, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: As a result, NUMA architectures implementing a shared memory programming model typically expose the existing memory access hierarchy to the application program, as done in BBN's Uniform System <ref> [247] </ref>. Motivations for exposing such information include: 1.
Reference: [248] <author> J. Torrellas, A. Tucker, and A. Gupta. </author> <title> Benefits of cache-affinity scheduling in shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 272-4, </pages> <month> May </month> <year> 1993. </year> <month> 68 </month>
Reference-contexts: In [101], Gupta et al. use a detailed simulation study to evaluate the performance of several scheduling strategies. These include regular priority scheduling, coscheduling or gang scheduling, process control with processor partitioning, hand-off scheduling, and affinity based scheduling <ref> [248] </ref>. In addition, tradeoffs between the use of busy waiting and blocking synchronization primitives are explored, in conjunction with their interactions with different scheduling strategies. A key focus of the study is the impact of different scheduling strategies on the caching behavior of an application.
Reference: [249] <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The kernel maintains the invariant that there are always exactly as many runnable scheduler activations as there are processors assigned to each address space. 13 2. notifying the kernel of user-level events affecting processor allocation. Tucker and Gupta <ref> [249] </ref> propose a similar solution which dyanamically controls the number of processes used by applications. This scheme is discussed in details in Section 3.1.3. <p> The other choice is to choose p processes from the job in a round robin fashion, each process executing for one quanta. The first alternative has more scheduling overhead than the second one. In <ref> [249] </ref>, the authors observe that the performance of an application worsens considerably when the number of processes in the system exceeds the total number of processors. <p> However, their experiments demonstrate that affinity scheduling has negligible effect on performance for current multiprocessors. Finally, they conclude that even on future, faster machines, a scheduling policy based on dynamic reallocation of processors among jobs outperforms a more static, equipartition policy. Tucker and Gupta <ref> [249] </ref> propose a different solution for reducing the frequency of context switches and for reducing cache corruption, which is explained next. Dynamic Partitioning: The dynamic partitioning [187, 73] (also known as Process control with processor partitioning) policy proposed by Tucker and Gupta [249] has the goal of minimizing context switches, so <p> Tucker and Gupta <ref> [249] </ref> propose a different solution for reducing the frequency of context switches and for reducing cache corruption, which is explained next. Dynamic Partitioning: The dynamic partitioning [187, 73] (also known as Process control with processor partitioning) policy proposed by Tucker and Gupta [249] has the goal of minimizing context switches, so that less time is spent rebuilding a processor's cache. Their approach is based on the hypothesis that an application performs best when the number of runnable processes is the same as the number of processors.
Reference: [250] <author> R. Vaswani and J. Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> In Proceedings of the thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 26-40, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Similarly, affinity (for local memory) also plays a vital role in scheduling processes in a NUMA machine; the context of a process resides mostly near the processor on which the process executed last. Vaswani and Zahorjan <ref> [250] </ref> also study the effect of cache affinity on kernel processor scheduling discipline for multiprogrammed, shared memory multiprocessors and conclude that the cache effects due to processor reallocation can be significant. However, their experiments demonstrate that affinity scheduling has negligible effect on performance for current multiprocessors.
Reference: [251] <author> J. Walpole, J. Inouye, and R. Konuru. </author> <title> Modularity and interfaces in micro-kernel design and implementation: A case study of chorus on the hp pa-risc. </title> <booktitle> In Proceedings of the USENIX Workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <pages> pages 71-82, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This implies that the performance of inter-process communication (IPC) mechanism plays a critical role in the performance of such operating systems [20]. The primary characteristic of micro-kernel based operating systems is modularity, thereby hoping to improve system extensibility, portability, reconfigurability, and improved support 10 for distribution <ref> [251, 93, 92] </ref>. Improvements in distribution, extensibility, and reconfigura--bility [175] result from the separation of system components from each other, and from the use of message passing as the communication mechanism among them [251]. <p> Improvements in distribution, extensibility, and reconfigura--bility [175] result from the separation of system components from each other, and from the use of message passing as the communication mechanism among them <ref> [251] </ref>. As a result, new services can be added (as new servers) or an existing server can be replaced by another without altering the existing components or the micro-kernel itself.
Reference: [252] <author> M. Wehner, J. Ambrosiano, J. Brown, W. Dannevik, P. Eltgroth, and A. Mirin. </author> <title> Toward a high performance distributed memory climate model. </title> <booktitle> In Proceedings of the 2nd International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 102-113, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Such work is motivated by the intended use of parallel machines for commercial, large-scale data processing, by upcoming programs like NASA's EOS satellites which will generate Terabytes of data that have to be processed and re-processed for use in earth science applications <ref> [252] </ref>, and it is motivated by the recent convergence of high performance computing and networking technologies resulting in large-scale, physically distributed, and heterogeneous parallel machines. Brief survey of multiprocessor hardware. Several current textbooks in parallel computing provide good overviews of parallel machine architectures [111, 6, 233, 188].
Reference: [253] <author> B.W. Weide, M.E. Brown, J.A.S. Alegria, and G.R. Meyer. </author> <title> A graphical interconnection language and its application to concurrent and real- time programming. </title> <booktitle> In Proceedings of the 20th Annual Allerton Conf. on Comm, Control, and Comp., </booktitle> <publisher> Univ. of Ill., </publisher> <pages> pages 567-576. </pages> <publisher> IEEE, </publisher> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: Such functionality appears no different from what is offered by other object-oriented operating systems. However, addressing the requirements of real-time programs, CHAOS arc object invocations range from reliable invocations that maintain parameters and return information (or even communication `streams' [213]) to invocations that implement unreliable `control signals' or `pulses' <ref> [213, 253] </ref>.
Reference: [254] <author> M. Weiser, A. Demers, and C. Hauser. </author> <title> The portable common run-time approach to interoperability. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 114-122, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Another inherent problem with language based systems can be protection, where language-level typing mechanisms must be mapped to the protection mechanisms available in the underlying operating system <ref> [14, 254] </ref>, which is not always easily done. In addition, any language based system will require all cooperating modules to be written in the same language, which precludes the use of mixed language environments.
Reference: [255] <author> J. Wendorf. </author> <title> Operating System/Application Concurrency in Tightly Coupled Multiprocessor Systems. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> December </month> <year> 1987. </year> <note> Technical Report CMU-CS-88-117. </note>
Reference-contexts: Later extensions of Mach also address NUMA and NORMA machines <ref> [255] </ref>. Mach runs on a wide variety of uniprocessor and multiprocessor architectures, including the DEC VAX system, Sun 3 workstations, IBM PCs, the IBM RP3 multiprocessor, the Encore Multimax, and recently, the Intel Paragon. Mach is also supported as a product by a number of hardware vendors.
Reference: [256] <author> W. Wulf. </author> <title> Reliable hardware/software architecture. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2):233-240, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: J. Watson Research Center, IBM), and a few successful commercial systems like Dynix (developed for Sequent Multiprocessors), Chrysalis (developed for BBN Butterfly machines), UMAX (developed for Encore Multimax multiprocessors). 4.1 HYDRA HYDRA <ref> [258, 256, 257, 259, 142, 58] </ref> is one of the earliest successful multiprocessor kernels, developed at Carnegie-Mellon University and implemented on the C.mmp hardware. Extensive descriptions of the HYDRA system appear in [120] and [258].
Reference: [257] <author> W. Wulf, E. Cohen, W. Corwin, A. Jones, R. Levin, C. Pierson, and F. Pollack. Hydra: </author> <title> The kernel of a multiprocessor operating system. </title> <journal> Communications of the ACM, </journal> <volume> 17(6) </volume> <pages> 337-345, </pages> <month> June </month> <year> 1974. </year>
Reference-contexts: J. Watson Research Center, IBM), and a few successful commercial systems like Dynix (developed for Sequent Multiprocessors), Chrysalis (developed for BBN Butterfly machines), UMAX (developed for Encore Multimax multiprocessors). 4.1 HYDRA HYDRA <ref> [258, 256, 257, 259, 142, 58] </ref> is one of the earliest successful multiprocessor kernels, developed at Carnegie-Mellon University and implemented on the C.mmp hardware. Extensive descriptions of the HYDRA system appear in [120] and [258].
Reference: [258] <author> W. Wulf, R. Levin, and S. Harbison. Hydra/C.mmp: </author> <title> An Experimental Computer System. </title> <booktitle> McGraw-Hill Advanced Computer Science Series, </booktitle> <year> 1981. </year>
Reference-contexts: Parameter passing is by reference between the caller and the callee. The implementation of capability based addressing has been carried out using both software and hardware techniques. A few sample software based systems are Hydra <ref> [258] </ref> and Cal/Tss [235]. CAP [179] and the Intel/432 [62] are examples of hardware based systems. Despite early favorable predictions, system builders have been largely unsuccessful in implementing and programming capability based systems which perform as well as machines based on more traditional memory reference models [132, 59]. <p> In addition, as with remote procedure calls, it is unclear what levels of machine and language support are required for efficient implementation of object invocations (e.g., for parameter marshaling [103] or for crossing protection boundaries <ref> [258, 80] </ref>). <p> Most hardware supports spin locks by specific instructions in their instruction sets. Although spin-waiting consumes processor, bus, and memory cycles, early research in multiprocessor operating systems clearly demonstrates the performance advantages of simple locking strategies and lock implementations <ref> [258, 89, 120] </ref>, showing that spin locks are useful in two situations when the critical section is small (compared to the cost of blocking and resuming a process) or when no other work is available for the processor (since spin waiting results in minimum latency between lock release and reacquisition) [182]. <p> J. Watson Research Center, IBM), and a few successful commercial systems like Dynix (developed for Sequent Multiprocessors), Chrysalis (developed for BBN Butterfly machines), UMAX (developed for Encore Multimax multiprocessors). 4.1 HYDRA HYDRA <ref> [258, 256, 257, 259, 142, 58] </ref> is one of the earliest successful multiprocessor kernels, developed at Carnegie-Mellon University and implemented on the C.mmp hardware. Extensive descriptions of the HYDRA system appear in [120] and [258]. <p> Extensive descriptions of the HYDRA system appear in [120] and <ref> [258] </ref>. <p> the major contributions of HYDRA address protection issues, there are also many research results relevant to parallel systems, including: * basic results establishing the concept of program locality, the effects of bus and memory contentions on parallel program performance, and the importance of asynchrony in parallel program design and implementation <ref> [258, 17, 181] </ref>, * demonstrations that high performance parallel programs require choices in the operating system mechanisms being provided (e.g., a variety of different synchronization mechanisms), since programs differ in granularities of parallelism, in frequencies of access to shared data, etc., and * experiences with the use of specific operating system <p> This insight has affected most modern operating system designs for parallel machines in areas ranging from process to thread representations, communication systems designs, and the implementation of synchronization constructs <ref> [120, 258] </ref>. 4.2 StarOS StarOS [117, 116, 118, 119] is an experimental operating system for the Cm* [121, 89, 77, 88, 238] multi-microprocessor computer developed at Carnegie-Mellon University.
Reference: [259] <author> W. Wulf, R. Levin, and C. Pierson. </author> <title> Overview of the hydra operating system. </title> <booktitle> In Proceedings of the 5th Symposium on Operating System Principles, </booktitle> <address> Austin, Texas, </address> <pages> pages 122-131. </pages> <publisher> ACM, </publisher> <month> Nov. </month> <year> 1975. </year>
Reference-contexts: J. Watson Research Center, IBM), and a few successful commercial systems like Dynix (developed for Sequent Multiprocessors), Chrysalis (developed for BBN Butterfly machines), UMAX (developed for Encore Multimax multiprocessors). 4.1 HYDRA HYDRA <ref> [258, 256, 257, 259, 142, 58] </ref> is one of the earliest successful multiprocessor kernels, developed at Carnegie-Mellon University and implemented on the C.mmp hardware. Extensive descriptions of the HYDRA system appear in [120] and [258].
Reference: [260] <author> G. Yaoqing and Y. Kwong. </author> <title> A survey of implementations of concurrent, parallel and distributed smalltalk. </title> <journal> SIGPLAN Notices, </journal> <volume> 28(9) </volume> <pages> 29-35, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: For example, the Ada rendezvous mechanism leads to well-known performance problems [43], the global addressing mechanisms and fixed language semantics of Linda can lead to inefficiencies concerning the update and access of remote information [211], and heap maintenance has been shown difficult for languages like Smalltalk <ref> [260, 186] </ref>. Another inherent problem with language based systems can be protection, where language-level typing mechanisms must be mapped to the protection mechanisms available in the underlying operating system [14, 254], which is not always easily done.
Reference: [261] <author> Pen-Chung Yew, Nian-Feng Tzeng, and Duncan H. Lawrie. </author> <title> Distributing hot-spot addressing in large-scale multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Giving programmers the ability to minimize relatively expensive remote vs. less expen sive local memory references (i.e., maximize program locality [120]), and 2. permitting programmers to avoid several forms of potential contention (switch or memory contention) caused by a large number of remote memory references <ref> [261] </ref>. Recent parallel NUMA architectures like the Kendall Square multiprocessor offer consistent global virtual memory. However, the performance reasons for exposing programmers to the underlying machine's NUMA properties persist, leading the system designers to include hardware instructions for page prefetches and poststores [112].
Reference: [262] <author> M. Young, A. Tevanian, R. Rashid, D. Golub, J. Eppinger, J. Chew, W. Bolosky, D. Black, and R. Baron. </author> <title> The duality of memory and communication in the implementation of a multiprocesor operating system. </title> <booktitle> In Proceedings of the 11th ACM Symposium on Operating Systems Principles (SIGOPS Notices vol.21, no.5), </booktitle> <pages> pages 63-76, </pages> <month> November </month> <year> 1987. </year> <month> 69 </month>
Reference-contexts: In effect, Accent carries into the domain of message passing systems the notion that I/O can be performed through virtual memory management. The design of the Mach system's memory management is largely derived from the Accent system <ref> [194, 1, 263, 262] </ref>. <p> Such consistency is guaranteed only for local memory and caches (i.e., for non-shared memory), or it must be explicitly enforced for shared memory by user- or compiler-generated code performing explicit block or page moves <ref> [194, 262] </ref>. As a result, NUMA architectures implementing a shared memory programming model typically expose the existing memory access hierarchy to the application program, as done in BBN's Uniform System [247]. Motivations for exposing such information include: 1. <p> Realizing that the problem of memory management is similar to the problem of cache management and consistency for UMA multiprocessors, the Mach operating 20 system's UMA implementation of memory management <ref> [194, 262] </ref> attempts to minimize the amount of data-copying and replication by using page copy-on-write and similar techniques for reduction of data movement. <p> The Psyche memory management system is structured into four layers of abstraction - NUMA, UMA, VUMA (virtual memory) and PUMA (Psyche memory). These abstractions are shown in Figure 7 and are discussed in Section 4.5. Weak Memory: The Accent and Mach operating systems <ref> [84, 194, 262] </ref> for uniprocessor and UMA multiprocessors demonstrated the use of copy-on-write paging for message passing. Li at Yale showed that a modified Apollo Aegis kernel can support shared memory on a 10Mhz token ring [144], and similar results are demonstrated by the Clouds project at Georgia Tech [68]. <p> Exchange of messages is a more abstract form of communication than accessing shared memory locations. Message passing subsumes communication, buffering, and synchronization. 26 Multiprocessor operating systems have experimented with a large variety of different com-munication abstractions, including ports <ref> [84, 262] </ref> mailboxes [119, 63], links [230, 127] etc. From an implementation point of view, such abstractions are kernel-handled message buffers. They may be either unidirectional or bidirectional. A process may send to or may receive messages from them. <p> Message: A message is a typed collection of data objects used for communication be tween active threads. 5. Memory Object: A memory object is a repository of data which can be mapped into the address space of a task. 4.3.1 Memory Management The Mach virtual memory management <ref> [84, 194, 1, 262, 263] </ref> system is designed to be architecture and operating system independent. Architecture independence is achieved by dividing the virtual memory implementation into machine independent and machine dependent portions.
Reference: [263] <author> M. W. Young. </author> <title> Exporting a User Interface to Memory Management from a Communication--Oriented Operating System. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> November </month> <year> 1989. </year> <note> Technical Report CMU-CS-89-202. </note>
Reference-contexts: In effect, Accent carries into the domain of message passing systems the notion that I/O can be performed through virtual memory management. The design of the Mach system's memory management is largely derived from the Accent system <ref> [194, 1, 263, 262] </ref>. <p> Message: A message is a typed collection of data objects used for communication be tween active threads. 5. Memory Object: A memory object is a repository of data which can be mapped into the address space of a task. 4.3.1 Memory Management The Mach virtual memory management <ref> [84, 194, 1, 262, 263] </ref> system is designed to be architecture and operating system independent. Architecture independence is achieved by dividing the virtual memory implementation into machine independent and machine dependent portions. <p> The virtual memory system exploits lazy evaluation, such as Copy-on-write and Map-on-reference whenever possible. Another important feature of the Mach virtual memory system is its ability to handle page faults and to page out data requests at the user level <ref> [263] </ref>. A few basic paging services are provided inside the kernel. However, a pager may be specified and implemented outside the kernel at user level. This has become particularly important for real-time implementations of Mach and for implementations addressing the needs of specific parallel architectures.
Reference: [264] <author> Tse yun Feng. </author> <title> A survey of interconnection networks. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 12-27, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: An interconnection network, which may be static or dynamic, facilitates communication among processors and memory modules. A few sample interconnection networks are: time shared or common buses, crossbar switches, hierarchical switches, and multistage networks. The design, structure and performance of various interconnection networks have been reviewed in other literature <ref> [264, 111, 6, 233, 188] </ref> and are beyond the scope of this survey. The variety of different kinds of multiprocessor architectures coupled with diverse application requirements have resulted in many different designs, goals, features, and implementations of multiprocessor operating systems, in university research projects and in the commercial domain.
Reference: [265] <author> J. Zahorjan and E. Lazowska amd D. Eager. </author> <title> Spinning versus blocking in parallel systems with uncertainty. </title> <booktitle> In Proceedings of the International Symposium on Performance of Distributed and Parallel Systems, </booktitle> <month> December </month> <year> 1988. </year>
Reference-contexts: Careful application design and coscheduling may handle the problems associated with spinlock-controlled critical sections and those with producer-consumer processes, but they do not address performance degradation due to cache corruption or frequent context switches. A more direct solution is proposed by Zahorjan et al. <ref> [265] </ref>, who describe a thread sched-uler that avoids preempting processes inside critical sections. In contrast, Edler et al. [79] propose an approach combining coscheduling and preemption avoidance for critical sections. Multiple processes are combined to form a group. <p> Moreover, instructions such as fetch-and-add [97] allow certain common operations to be performed in parallel without critical sections [126]. Other work has evaluated the effects of other kernel components [8, 9, 10, 99] as well as applications on synchronization. For example, Zahorjan, Lazowska and Eager <ref> [265] </ref> first examines the extent to which multiprogramming and data-dependencies in an application 24 complicate an user's decision to spin or block, then evaluate [266] how the overhead of spinning is affected by various scheduling policies.
Reference: [266] <author> J. zahorjan, E. Lazowska, and D. Eager. </author> <title> The effect of scheduling discipline on spin overhead in shared memory parallel systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(2) </volume> <pages> 180-98, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: However, for applications that have sizable working sets that fit into the cache, process control performs better than gang scheduling. For the applications considered 4 , the performance gains due to hand-off scheduling and processor affinity are shown to be small. In <ref> [266] </ref> the authors study the effects of two environmental factors, multiprogramming and data-dependent execution times, on spinning overhead of parallel applications, and how the choice of scheduling discipline can be used to reduce the amount of spinning in each case. <p> For example, Zahorjan, Lazowska and Eager [265] first examines the extent to which multiprogramming and data-dependencies in an application 24 complicate an user's decision to spin or block, then evaluate <ref> [266] </ref> how the overhead of spinning is affected by various scheduling policies. Read-Write Locks: A read-write lock allows either multiple readers or a single writer to enter a critical section at the same time.
Reference: [267] <author> J. Zahorjan and C. McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Basic theoretical results on static process scheduling on parallel machines show that the scheduling problem is NP-hard; static algorithms minimizing average response time include those described in [164] and [39]. Other scheduling algorithms appear in <ref> [267] </ref> and [141]. In this section, we focus on dynamic scheduling [164], and on scheduling for shared memory machines, where variations in distances between different processors on the parallel machine [39, 214] are not considered. <p> J. Zahorjan and C. McCann compare the performance of static and dynamic schedulers for multi-user workloads. Their results include <ref> [267] </ref>: * Independent of workload and overall system load, dynamic scheduling performs best when context switch overheads are small. * The advantage of dynamic scheduling at low context switch costs increases with larger and more rapid changes in the parallelism exhibited by a workload. * Dynamic scheduling performs increasingly well relative <p> One reason for such performance degradation is a possible high rate of processor reallocation. Hence, some researchers have suggested to dampen the rate of processor allocation and release, thereby reducing the rate of useless processors exchange <ref> [267] </ref>. However, such a modification to the dynamic policy was found to be detrimental to performance [267]. As for uniprocessors, multiprocessor schedulers can be classified as preemptive or nonpre-emptive schedulers. <p> Hence, some researchers have suggested to dampen the rate of processor allocation and release, thereby reducing the rate of useless processors exchange <ref> [267] </ref>. However, such a modification to the dynamic policy was found to be detrimental to performance [267]. As for uniprocessors, multiprocessor schedulers can be classified as preemptive or nonpre-emptive schedulers. A scheduler can also be classified according to its scheduling granularity, which is determined by the executable unit being scheduled (For example, schedulers differ in that they may schedule individual or groups of processes).
Reference: [268] <author> Wei Zhao, Krithi Ramamritham, and J. A. Stankovic. </author> <title> Preemptive scheduling under time and resource constraints. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(8):949-960, </volume> <month> August </month> <year> 1987. </year>
Reference-contexts: Before a job is assigned one or more physical processors, the scheduler checks whether the system can satisfy the job's timing constraints. This analysis is known as schedulability analysis. Schedulability analysis and scheduling for real time systems <ref> [53, 269, 37, 108, 150, 71, 268] </ref> are active areas of research and are not within the scope of this paper. 3.2 Memory Management Memory management for UMA multiprocessors is conceptually similar to that for multipro-grammed uniprocessors.
Reference: [269] <author> Hongyi Zhou and Karsten Schwan. </author> <title> Dynamic scheduling for hard real-time systems: Toward real-time threads. </title> <booktitle> In Proceedings of the Joint IEEE Workshop on Real-Time Operating Systems and Software and IFAC Workshop on Real-Time Programming, </booktitle> <address> Atlanta, GA. </address> <publisher> IEEE, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: Before a job is assigned one or more physical processors, the scheduler checks whether the system can satisfy the job's timing constraints. This analysis is known as schedulability analysis. Schedulability analysis and scheduling for real time systems <ref> [53, 269, 37, 108, 150, 71, 268] </ref> are active areas of research and are not within the scope of this paper. 3.2 Memory Management Memory management for UMA multiprocessors is conceptually similar to that for multipro-grammed uniprocessors.
Reference: [270] <author> Hongyi Zhou, Karsten Schwan, and Ian Akyildiz. </author> <title> Performance effects of information sharing in a distributed multiprocessor real-time scheduler. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Tech, GIT-CC-91/40, </institution> <month> Sept. </month> <year> 1991. </year> <booktitle> Abbreviated version in Proceedings of the 1992 IEEE Real-Time Systems Symposium, Phoenix. </booktitle>
Reference-contexts: like processes or threads, the provision of alternative communication schemes and synchronization mechanisms, and resource scheduling like process assignment to different processors and data placement in physically distributed memory [214], and finally, the parallelization of the operating system itself, again in order to provide scalable performance for varying application requirements <ref> [89, 109, 270] </ref>. A second major reason for using a multiprocessor system is to provide high reliability, and graceful degradation in the event of failure. Hence, several multiprocessor systems have been constructed and designed for improved fault tolerance. <p> A similar approach to run-queue organization is taken in the Intel 432's iMAX operating system [63]. Recent work on real-time schedulers for parallel systems is also considering the effects of sharing alternative policy-level scheduling information on parallel scheduler performance <ref> [270] </ref>. Beyond this work, scheduler structuring remains largely unexplored, but should receive increased attention in operating systems for large-scale parallel machines like the Intel Paragon multiprocessor. 3.1.3 Scheduling Policies A scheduling policy allocates available time and processors to a job or a process statically or dynamically [153].
Reference: [271] <author> S. Zhou, M. Stumm, K. Li, and D. Wortman. </author> <title> Heterogeneous distributed shared memory. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 540-54, </pages> <month> september </month> <year> 1992. </year> <month> 70 </month>
Reference-contexts: We briefly discuss several memory management algorithms for NUMA multiprocessors. Various multiprocessor operating systems such as Mach, Psyche, and PLATINUM use a variation or a mix of these algorithms. The algorithms described below are categorized by whether they migrate and/or replicate data <ref> [234, 271] </ref>. One algorithm migrates data to the site where it is accessed in an attempt to exploit locality in data accesses and decreases the number of remote accesses. Two other algorithms replicate data so that multiple read accesses can happen at the same time using local accesses.
References-found: 271

