URL: http://www.research.microsoft.com/~carlk/papers/fit.ps
Refering-URL: http://www.research.microsoft.com/~carlk/papers/fit.htm
Root-URL: http://www.research.microsoft.com
Title: Conceptual Set Covering: Improving Fit-And-Split Algorithms  
Author: Carl M. Kadie 
Address: Urbana, IL 61801  
Affiliation: Knowledge-Based Systems Group, Department of Computer Science Beckman Institute, University of Illinois,  
Abstract: Many learning systems implicitly use the fit-and-split learning method to create a comprehensive hypothesis from a set of partial hypotheses. At the core of the fit-and-split method is the assignment of examples to partial hypotheses. To date, however, this core has been neglected. This paper provides the first definition and model of the fit-and-split assignment problem. Extant systems perform assignment nearly arbitrarily, implicitly using, for example, greedy set covering. This paper also presents Conceptual Set Covering (CSC), a new assignment algorithm. An extensive empirical evaluation over a wide range of learning problems suggests that CSC can improve any fit-and-split learning system. 
Abstract-found: 1
Intro-found: 1
Reference: [Cohen and Feigenbaum, 1982] <author> P. Cohen and E. Feigen-baum. </author> <booktitle> The Handbook of Artificial Intelligence. </booktitle> <volume> Volume 3, </volume> <publisher> HeurisTech Press, Stanford, </publisher> <address> CA, </address> <year> 1982. </year>
Reference-contexts: With these terms in mind, a typical fit-and-split-assignment problem can be characterized by describing a problem generator. The generator is listed in figure 4. 2.4 Related Learning Problems The fit-and-split-assignment problem is a specialization of the overlapping-concept-learning problem <ref> [Michalski, 1983; Cohen and Feigenbaum, 1982] </ref>, where a class corresponds to a partial hypothesis. It differs from standard overlapping concept learning in two ways. First, examples are labeled with every class (that is, partial hypothesis) that covers them.
Reference: [Drastal et al., 1989] <author> George Drastal, Stan Raatz, and Gabe Czako. </author> <title> Induction in an abstract space. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <year> 1989. </year>
Reference-contexts: For example, it is used by operator learners [Kadie, 1989; Shen and Simon, 1989], by automatic programmers [Summers, 1977], by discovery systems [Falkenhainer and Michalski, 1986], and by integrated empirical/explanation-based systems <ref> [Drastal et al., 1989] </ref>. All these systems concentrate on step one of the method (partial-hypothesis creation) or step three (decision-rule creation). Extant systems typically neglect step two, example assignment; they do the assignment without regard to the effect on the final hypothesis using, for example, the greedy-set-covering algorithm.
Reference: [Falkenhainer and Michalski, 1986] <author> Brian Falkenhainer and Ryszard S. Michalski. </author> <title> Integrating qualitative and quantitative discovery: the ABACUS system. </title> <journal> Machine Learning, </journal> <volume> 1(4) </volume> <pages> 367-401, </pages> <year> 1986. </year>
Reference-contexts: For example, it is used by operator learners [Kadie, 1989; Shen and Simon, 1989], by automatic programmers [Summers, 1977], by discovery systems <ref> [Falkenhainer and Michalski, 1986] </ref>, and by integrated empirical/explanation-based systems [Drastal et al., 1989]. All these systems concentrate on step one of the method (partial-hypothesis creation) or step three (decision-rule creation).
Reference: [Garey and Johnson, 1979] <author> M. R. Garey and D.S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: In the one-dimensional case the problem does not look difficult. The example space, however, can be dimensional and non-Euclidean. In general the problem is hard (because any set-covering problem <ref> [Garey and Johnson, 1979] </ref> can be reduced to a fit-and-split assignment problem with an example space consisting of one nominal dimension). -5 -4 -3 -2 -1 0 1 2 3 4 5 -f (x)=-x -f (x)=sin (pi/2 * x)- -f (x)=1, 1c. f (x)=11b. f (x)=11a. f (x)=1 3. f (x)=sin
Reference: [Kadie, 1989] <author> Carl M. Kadie. Diffy-S: </author> <title> Learning Robot Operators from Examples of Operator Effects. </title> <type> Technical Report UIUCDCS-R-89-1550, </type> <institution> Computer Science Department, University of Illinois, Urbana, IL, </institution> <month> October </month> <year> 1989. </year> <type> Masters Thesis. </type>
Reference-contexts: For example, it is used by operator learners <ref> [Kadie, 1989; Shen and Simon, 1989] </ref>, by automatic programmers [Summers, 1977], by discovery systems [Falkenhainer and Michalski, 1986], and by integrated empirical/explanation-based systems [Drastal et al., 1989]. All these systems concentrate on step one of the method (partial-hypothesis creation) or step three (decision-rule creation).
Reference: [McClave and Dietrich, 1985] <author> James T. McClave and Frank H. Dietrich. </author> <title> Statistics. </title> <publisher> Dellen Publishing Company, </publisher> <address> San Francisco, third edition, </address> <year> 1985. </year>
Reference-contexts: Algorithms The target was randomly generated with 5 true partial hypotheses, 3 disjuncts per true partial hypothesis, and a 20% coincidence proba bility. statistical significance of the ranking can be evaluated with the Friedman test for randomized block design with set to 5% <ref> [McClave and Dietrich, 1985] </ref>. 5 Greedy3, and Most Freq. Under the assumption that the tests are representative, the Friedman test indicates that the performance difference between CSC and Greedy0 is significant to at least the 99.5% level.
Reference: [Michalski, 1983] <author> R. S. Michalski. </author> <title> A theory and methodology of inductive inference. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, chapter 4, </booktitle> <pages> pages 83-134, </pages> <address> Palo Alto: </address> <publisher> Tioga Press, </publisher> <year> 1983. </year>
Reference-contexts: With these terms in mind, a typical fit-and-split-assignment problem can be characterized by describing a problem generator. The generator is listed in figure 4. 2.4 Related Learning Problems The fit-and-split-assignment problem is a specialization of the overlapping-concept-learning problem <ref> [Michalski, 1983; Cohen and Feigenbaum, 1982] </ref>, where a class corresponds to a partial hypothesis. It differs from standard overlapping concept learning in two ways. First, examples are labeled with every class (that is, partial hypothesis) that covers them.
Reference: [Michalski and Chilausky, 1980] <author> R. S. Michalski and R. L. Chilausky. </author> <title> Learning by being told and learning from examples: an experimental comparison of the two methods of knowledge acquisition in the context of developing and expert system for soybean disease diagnosis. </title> <journal> Policy Analysis and Information Systems, </journal> <volume> 4(2), </volume> <year> 1980. </year>
Reference-contexts: None of these partial hypotheses cover all examples, so they must be combined in a decision list. The decision rules for the decision list are typically formed by a concept learner such as AQ, ID3, or PLS <ref> [Michalski and Chilausky, 1980; Quinlan, 1986; Rendell, 1986] </ref>. Such a concept learner takes as input a set of positive and negative examples. It returns a decision rule that covers (that is, returns true when applied to) each positive example but does not cover any negative example.
Reference: [Pitt and Reinke, 1988] <author> L. Pitt and R. E. Reinke. </author> <title> Criteria for polynomial time (conceptual) clustering. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 371-396, </pages> <year> 1988. </year>
Reference-contexts: When research on the clustering problem faced a similar situation, conceptual-clustering algorithms were developed. These are algorithms that look for a solution that can be expressed simply <ref> [Pitt and Reinke, 1988; Stepp and Michalski, 1989] </ref>. In a similar manner, the Conceptual-Set-Covering algorithm (CSC) tries to find a solution that can be expressed simply. The CSC is listed in figure 6. The algorithm's application on the example problem is illustrated in figure 7. 1.
Reference: [Quinlan, 1986] <author> J. Ross Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <year> 1986. </year>
Reference-contexts: None of these partial hypotheses cover all examples, so they must be combined in a decision list. The decision rules for the decision list are typically formed by a concept learner such as AQ, ID3, or PLS <ref> [Michalski and Chilausky, 1980; Quinlan, 1986; Rendell, 1986] </ref>. Such a concept learner takes as input a set of positive and negative examples. It returns a decision rule that covers (that is, returns true when applied to) each positive example but does not cover any negative example. <p> With training sets of size 100 and larger, CSC performed significantly better than the other learners. Table 2: The Four Learners Tested Each consists of an assignment algorithm and a concept learner. Name Assigner Concept Learner CSC CSC Decision-tree learner with no pruning <ref> [Quinlan, 1986; Ren dell, 1986] </ref> Greedy0 greedy set cov- Decision-tree learner with no ering pruning Greedy3 greedy set cov- Decision-tree learner with ering pruning parameter set to 3.0 [Rendell, 1986] Most all examples to Decision-tree learner with no Freq the most cover- pruning ing partial hypothesis The first test series showed
Reference: [Rendell, 1986] <author> Larry Rendell. </author> <title> Induction, of and by probability. </title> <editor> In L. Kanal and J Lemmar, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 429-443, </pages> <address> New York: </address> <publisher> North Holland, </publisher> <year> 1986. </year>
Reference-contexts: None of these partial hypotheses cover all examples, so they must be combined in a decision list. The decision rules for the decision list are typically formed by a concept learner such as AQ, ID3, or PLS <ref> [Michalski and Chilausky, 1980; Quinlan, 1986; Rendell, 1986] </ref>. Such a concept learner takes as input a set of positive and negative examples. It returns a decision rule that covers (that is, returns true when applied to) each positive example but does not cover any negative example. <p> Name Assigner Concept Learner CSC CSC Decision-tree learner with no pruning [Quinlan, 1986; Ren dell, 1986] Greedy0 greedy set cov- Decision-tree learner with no ering pruning Greedy3 greedy set cov- Decision-tree learner with ering pruning parameter set to 3.0 <ref> [Rendell, 1986] </ref> Most all examples to Decision-tree learner with no Freq the most cover- pruning ing partial hypothesis The first test series showed that CSC could perform better than greedy set covering. The second test series was designed to find where each algorithm was best.
Reference: [Shen and Simon, 1989] <author> Wei-Min Shen and Herbert A. Simon. </author> <title> Rule creation and rule learning through environmental exploration. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 675-680, </pages> <address> Detroit, MI, </address> <year> 1989. </year>
Reference-contexts: For example, it is used by operator learners <ref> [Kadie, 1989; Shen and Simon, 1989] </ref>, by automatic programmers [Summers, 1977], by discovery systems [Falkenhainer and Michalski, 1986], and by integrated empirical/explanation-based systems [Drastal et al., 1989]. All these systems concentrate on step one of the method (partial-hypothesis creation) or step three (decision-rule creation).
Reference: [Stepp and Michalski, 1986] <author> R. E. Stepp and R. S Michalski. </author> <title> Conceptual clustering: inventing goal-oriented classifications of structured objects. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, Volume II, chapter 20, </booktitle> <pages> pages 471-498, </pages> <address> Los Altos: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference: [Summers, 1977] <author> P. Summers. </author> <title> A methodology for LISP program construction from examples. </title> <journal> J. ACM, </journal> <volume> 24, </volume> <month> Jan-uary </month> <year> 1977. </year>
Reference-contexts: For example, it is used by operator learners [Kadie, 1989; Shen and Simon, 1989], by automatic programmers <ref> [Summers, 1977] </ref>, by discovery systems [Falkenhainer and Michalski, 1986], and by integrated empirical/explanation-based systems [Drastal et al., 1989]. All these systems concentrate on step one of the method (partial-hypothesis creation) or step three (decision-rule creation).
Reference: [Tcheng et al., 1989] <author> David Tcheng, Bruce Lambert, Ste--phen Lu, and Larry Rendell. </author> <title> Building robust learning systems by combining induction and optimization. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 806-812, </pages> <address> Detroit, MI, </address> <year> 1989. </year>
References-found: 15

