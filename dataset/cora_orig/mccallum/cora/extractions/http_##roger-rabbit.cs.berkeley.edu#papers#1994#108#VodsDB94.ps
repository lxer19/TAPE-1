URL: http://roger-rabbit.cs.berkeley.edu/papers/1994/108/VodsDB94.ps
Refering-URL: http://roger-rabbit.cs.berkeley.edu/papers/1994/108/108.html
Root-URL: http://www.cs.berkeley.edu
Email: (rowe@cs.berkeley.edu)  
Title: Indexes for User Access to Large Video Databases built that allows a user to select
Author: Lawrence A. Rowe, John S. Boreczky, and Charles A. Eads 
Note: A mixed-mode query interface was  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division EECS University of California Berkeley  
Abstract: Video-on-Demand systems have received a good deal of attention recently. Few studies, however, have addressed the problem of locating a video of interest in a large video database. This paper describes the design and implementation of a metadata database and query interface that attempts to solve this information retrieval problem. Sample queries were collected by interviewing a variety of users. These queries guided the design of indexes that can be used to answer the types of queries users want to ask. Three types of indexes were identified: 1) bibliographic (e.g., title, abstract, subject keywords, genre, director, cast, etc.), 2) structural (e.g., segments, scenes, shots, transitions, sound effects, etc.), and 3) content (e.g., keyframe sequences, scripts, objects and actors in scenes, etc.). 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Arman, F., Hsu, A., and Chiu, M-Y., </author> <title> Image Processing on Compressed Data for Large Video Databases, </title> <booktitle> Proc. ACM Multimedia 93, </booktitle> <address> Anaheim, CA, </address> <month> August, </month> <year> 1993. </year>
Reference-contexts: There has also been a great deal of work on using computers to automatically extract video content. The simplest and most common technique is video segmentation (i.e., detecting the boundaries between camera shots). Recent research in this area includes work on shot detection using compressed video <ref> [1] </ref> and work on automatic classification of shot transitions [31]. Swanberg has used a knowledge base to do automatic segmenting and scene classification for a highly structured type of video [28]. Research has also been done on object tracking and classification of object motion in video [11].
Reference: 2. <author> Bell, J. and Rowe, L.A., </author> <title> An Exploratory Study of Ad Hoc Query Languages to Databases, </title> <booktitle> Proc. IEEE 8th Intl Conf. on Data Engineering, </booktitle> <month> February, </month> <year> 1992. </year>
Reference-contexts: It is interesting to note that it is easier to phrase these queries in English than in a database query language. Unfortunately, it is not as easy to use a natural language interface as a query interface <ref> [2] </ref>. FIGURE 3.
Reference: 3. <author> Berners-Lee, T.J., Cailliau, R., Groff, J.F., and Pollerman, B., World-Wide-Web: </author> <title> The Information Universe, Electronic Networking: Research, Applications, and Policy, </title> <booktitle> Spring 1992, </booktitle> <volume> Vol. 2, No. 1, </volume> <pages> pp. 52-58. </pages>
Reference-contexts: Bibliographic information is also available as MARC records so the video catalog can be accessed using other bibliographic interfaces such as WAIS [25] and the World Wide Web <ref> [3] </ref>. The main bibliographic data index (DOC_REFERENCE) contains a subclass for each type of document in the database.
Reference: 4. <author> Davenport, G., Smith, T.G.A., and Pincever, N., </author> <title> Cinematic Primitives for Multimedia, </title> <journal> IEEE Computer Graphics & Applications, </journal> <month> July </month> <year> 1991, </year> <pages> pp. 67-74. </pages>
Reference-contexts: movies can be described by a hierarchy of movie, segment, scene, and shot where each entry in the hierarchy is composed of one or more entries at a lower level (e.g., a scene is composed of a sequence of shots and a segment is composed of a sequence of scenes <ref> [4] </ref>). A structural index can also be con structed for the audio track. 3. Content Data. Users want to retrieve videos based on their content.
Reference: 5. <author> Elliot, E., </author> <title> Multiple Views of Digital Video, </title> <institution> MIT Media Lab, Interactive Cinema Working Paper, Massachusetts Institute of Technology, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: From this display a user can play a video or zoom into a a storyboard viewer that shows one keyframe per scene. Another possible viewer that could be implemented is MIT frozen movies that represent frames over time as 3 dimensional solids and allow viewing along any slice <ref> [5] </ref>. 10 pressing buttons in the directed graph that represents the hierarchy. The result set can be reduced by further restricting the path or calling up a panel to restrict the list to videos with a particular word in the title.
Reference: 6. <author> Federighi, C. and Rowe, L., </author> <title> A Distributed Hierarchical Storage Manager for a Video-on-Demand System, </title> <booktitle> 1994 IS&T/ SPIE Symposium on Electronic Imaging: Science and Technology, </booktitle> <address> San Jose, CA, </address> <month> February, </month> <year> 1994. </year>
Reference-contexts: It is also used to schedule the movement of data between tertiary storage and the servers. A description of the storage management aspects of the system is available elsewhere <ref> [6] </ref>. FIGURE 1.
Reference: 7. <author> Haskins, R., </author> <title> The Shark Continuous-Media File Server, </title> <booktitle> Proc. IEEE COMPCON 93, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1993. </year>
Reference: 8. <author> Hsu, A. and Arman F., </author> <type> personal communication, </type> <month> January, </month> <year> 1994. </year>
Reference-contexts: A few people mentioned a desire to browse collections of videos and clips. They wanted an organized collection of items and a way to quickly scan them <ref> [8] </ref>. Visual content queries usually involved actors and their actions. Surprisingly, only people who were involved with multimedia work expressed an interest in asking detailed object or structural queries. 3.
Reference: 9. <author> Keeton, K. and Katz, R., </author> <title> The Evaluation of Video Layout Strategies on a High-Bandwidth File Server, </title> <booktitle> Fourth Intl. Workshop on Operating Systems and Network Support for Digital Audio and Video, </booktitle> <year> 1993. </year>
Reference: 10. <author> Larson, R., </author> <title> Classification Clustering, Probabilistic Information Retrieval and the Online Catalog, </title> <journal> Library Quarterly, </journal> <volume> vol 61, </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: This type of two-dimensional display might also be useful for browsing documents based on other types of attributes. The geographic browser panel is based on the map widget included in the Lassen Text Browser <ref> [10] </ref>. Movies often have text information and other material such as poster images, press clippings, stills, and the shooting script that should be available to users. Course lectures can be supplemented viewgraphs and handouts. <p> Changes in background music, appearances of songs, and sound effects can be time stamped and made available for queries. We are still working on this part of the schema. 3.4. Keyword Indexes Keyword indexes are very similar to the indexes used in the Lassen Text Browser <ref> [10] </ref>. The WordNet database and software, developed by Princeton University, is used to find the stems of input words and place words in groups with their synonyms.
Reference: 11. <author> Lee, S-Y. and Kao, H-M., </author> <title> Video Indexing - an Approach Based on Moving Object and Track, </title> <booktitle> 1993 IS&T/SPIE Symposium on Electronic Imaging: Science and Technology, </booktitle> <address> San Jose, CA, </address> <month> February, </month> <year> 1993. </year>
Reference-contexts: Swanberg has used a knowledge base to do automatic segmenting and scene classification for a highly structured type of video [28]. Research has also been done on object tracking and classification of object motion in video <ref> [11] </ref>. While most of this work will be useful in building the indexes we require, considerable progress will be needed before sophisticated content indexes (e.g., actors entering and leaving a scene) can be automatically created. The system on which our work is built is the Berkeley Distributed VOD System.
Reference: 12. <author> Little, T.D.C, Ahanger, G., Folz, R.J., Gibbon, J.F., Reeve, F.W., Schelleng, D.H., and Venkatesh, D., </author> <title> A Digital On Demand Video Service Supporting Content-Based Queries, </title> <booktitle> Proc. ACM Multimedia 93, </booktitle> <address> Anaheim, CA, </address> <month> August, </month> <year> 1993. </year>
Reference-contexts: These problems have been investigated by other researchers, although little work has been done on combining video indexing with video retrieval. The Virtual Video Browser <ref> [12] </ref> is a video retrieval system that stores limited bibliographic, actor, and scene information. OConnor discusses the use of scene descriptions and keyframes as a surrogate for browsing video material in a library setting where access to the actual video is not always feasible [16].
Reference: 13. <author> Martin, M. and Porter, M., </author> <title> Video Movie Guide 1992, </title> <publisher> Ballantine Books, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: There are also a number of commercial CD-ROM movie databases [14][19][30]. These systems support computerized search of the information found in a typical movie guide published in book form <ref> [13] </ref>. In some cases, still images and short clips are included on the CD-ROM. There has also been a great deal of work on using computers to automatically extract video content. The simplest and most common technique is video segmentation (i.e., detecting the boundaries between camera shots).
Reference: 14. <institution> Microsoft Corporation, Microsoft Cinemania: </institution> <note> Users Guide, </note> <institution> Microsoft Corporation, </institution> <address> Redmond, WA, </address> <year> 1992. </year>
Reference: 15. <author> Niblack, W., Barber, R., Equitz, W., Flickner, M., Petkovic, D., and Yanker, P., </author> <title> The QBIC Project: Querying Images by Content Using Color, Texture, and Shape, </title> <booktitle> 1993 IS&T/SPIE Symposium on Electronic Imaging: Science and Technology, </booktitle> <address> San Jose, CA, </address> <month> February, </month> <year> 1993. </year>
Reference-contexts: The QBIC system allows users to query a small set of still images by color, texture, position, and shape of objects <ref> [15] </ref>. There are also a number of commercial CD-ROM movie databases [14][19][30]. These systems support computerized search of the information found in a typical movie guide published in book form [13]. In some cases, still images and short clips are included on the CD-ROM.
Reference: 16. <author> OConnor, </author> <title> B.C., Access to Film and Video Works: Surrogates for Moving Image Documents, </title> <type> dissertation, </type> <institution> University of California, Berkeley, </institution> <month> Dec. </month> <year> 1984. </year>
Reference-contexts: The Virtual Video Browser [12] is a video retrieval system that stores limited bibliographic, actor, and scene information. OConnor discusses the use of scene descriptions and keyframes as a surrogate for browsing video material in a library setting where access to the actual video is not always feasible <ref> [16] </ref>. The QBIC system allows users to query a small set of still images by color, texture, position, and shape of objects [15]. There are also a number of commercial CD-ROM movie databases [14][19][30].
Reference: 17. <author> Ousterhout, J., </author> <title> Tcl: An embedded command language, </title> <booktitle> Proceedings 1990 Winter USENIX Conference, </booktitle> <year> 1990. </year>
Reference: 18. <author> Ousterhout, J., </author> <title> An X11 Toolkit based on the Tcl language, </title> <booktitle> Proceedings 1991 Winter USENIX Conference, </booktitle> <year> 1991. </year>
Reference: 19. <institution> Paramount Interactive, MovieSelect, Paramount Interactive, </institution> <address> Palo Alto, CA 1992. </address> <month> 12 </month>
Reference: 20. <author> Rangan, P.V., Vin, H.M., Ramanathan, S., </author> <title> Designing an On-Demand Multimedia Service, </title> <journal> IEEE Communications Magazine, </journal> <volume> Vol. 30, No. 7, </volume> <month> July </month> <year> 1992, </year> <pages> pp. 56-64. </pages>
Reference: 21. <author> Rowe, L.A., and Smith, </author> <title> B.C., A Continuous Media Player,, </title> <booktitle> Proceedings 13th Symposium on Operating System Support for Digital Audio and Video, </booktitle> <address> La Jolla, CA, </address> <month> November, </month> <year> 1992. </year>
Reference-contexts: The interface includes general relational operators, navigational operators to express structural queries, and keyword search operators. Once a movie has been selected using the query interface, the Continuous Media Player (CMPlayer) <ref> [21] </ref> is used to play the video. The CMPlayer provides synchronized playback and random access to audio and MPEG and motion-JPEG video over local and wide area networks. VDB also allows a user to play movies on a local VFSs directly.
Reference: 22. <author> Salton, G., </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1988. </year>
Reference-contexts: Lassen adds an information retrieval system, based on the SMART system, that allows documents and queries to be represented by weighted vectors of keywords <ref> [22] </ref>. The vector representation allows for faster processing of keyword queries than would be possible using standard relational database methods.
Reference: 23. <author> Sesek, E., </author> <type> personal communication, </type> <month> Aug. </month> <year> 1993. </year>
Reference: 24. <author> Smith, B.C. and Rowe L.A., </author> <title> An Application-Specific Ad Hoc Query Interface, </title> <journal> IEEE Trans. on Semiconductor Manufacturing, </journal> <volume> Vol. 5, No. 4, </volume> <pages> pp. 281-289, </pages> <month> November </month> <year> 1992. </year>
Reference: 25. <author> Stein, R., </author> <title> Browsing Through Terabytes, </title> <journal> Byte Magazine, </journal> <month> May </month> <year> 1991, </year> <pages> pp. 157-164. </pages>
Reference-contexts: Bibliographic information is also available as MARC records so the video catalog can be accessed using other bibliographic interfaces such as WAIS <ref> [25] </ref> and the World Wide Web [3]. The main bibliographic data index (DOC_REFERENCE) contains a subclass for each type of document in the database.
Reference: 26. <author> Stonebraker, M. and Dozier, J., </author> <title> Sequoia 2000: Large Capacity Object Servers to Support Global Change Research, </title> <type> Sequoia 2000 Technical Report 91/1, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: The indexes contain pointers to the location of the compressed video and audio data on tertiary and secondary storage devices. The video database is integrated into the Sequoia 2000 (S2K) database that contains images, text documents, and global change data sets <ref> [26] </ref>. The subset of the database that covers video material currently contains 62 classes 1 , but this number will grow as different types of movies and movie formats are added and better query interface components are built. Table 1 shows the main classes in the database schema.
Reference: 27. <author> Stonebraker, M. and Kemnitz, G., </author> <title> The POSTGRES Next-Generation Database Mangagement System, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 34, No. 10, </volume> <month> October, </month> <year> 1991, </year> <pages> pp. 78-92. </pages>
Reference-contexts: These four index types are described in further detail below. The POSTGRES relational database management system is used to store the video indexes and to provide stable storage for coordination between components of the Distributed VOD System <ref> [27] </ref>. The indexes contain pointers to the location of the compressed video and audio data on tertiary and secondary storage devices. The video database is integrated into the Sequoia 2000 (S2K) database that contains images, text documents, and global change data sets [26].
Reference: 28. <author> Swanberg, D., Shu C.F., and Jain, R., </author> <title> Knowledge Guided Parsing and Retrieval in Video Databases, </title> <booktitle> 1993 IS&T/SPIE Symposium on Electronic Imaging: Science and Technology, </booktitle> <address> San Jose, CA, </address> <month> February, </month> <year> 1993. </year>
Reference-contexts: Recent research in this area includes work on shot detection using compressed video [1] and work on automatic classification of shot transitions [31]. Swanberg has used a knowledge base to do automatic segmenting and scene classification for a highly structured type of video <ref> [28] </ref>. Research has also been done on object tracking and classification of object motion in video [11].
Reference: 29. <author> Tobagi, F.A. and Pang, J., </author> <title> StarWorks *TM -- A Video Applications Server, </title> <booktitle> Proc. IEEE COMPCON 93, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1993. </year>
Reference: 30. <author> The Voyager Company, </author> <title> Criterion Goes to the Movies, The Voyager Company, </title> <address> Santa Monica, CA, </address> <year> 1992. </year>
Reference: 31. <author> Zhang, H.J., Kankanhalli, A., and Smoliar, </author> <title> S.W., Automatic Partitioning of Full-motion Video, </title> <booktitle> Multimedia Systems (1993) 1 </booktitle> <pages> 10-28. </pages>
Reference-contexts: The simplest and most common technique is video segmentation (i.e., detecting the boundaries between camera shots). Recent research in this area includes work on shot detection using compressed video [1] and work on automatic classification of shot transitions <ref> [31] </ref>. Swanberg has used a knowledge base to do automatic segmenting and scene classification for a highly structured type of video [28]. Research has also been done on object tracking and classification of object motion in video [11].
References-found: 31

