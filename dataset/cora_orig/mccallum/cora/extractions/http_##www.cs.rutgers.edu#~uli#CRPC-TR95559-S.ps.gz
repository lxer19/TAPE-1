URL: http://www.cs.rutgers.edu/~uli/CRPC-TR95559-S.ps.gz
Refering-URL: http://www.cs.rutgers.edu/~uli/pubs.html
Root-URL: http://www.cs.rutgers.edu
Title: Automatic Data Layout for Distributed Memory Machines  
Author: Ulrich Kremer 
Address: 6100 South Main Street Houston, TX 77005-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: October, 1995  
Pubnum: CRPC-TR95-559-S  
Abstract-found: 0
Intro-found: 1
Reference: [ABCC93] <author> D. Applegate, R. Bixby, V. Chvatal, and W. Cook. </author> <title> The traveling salesman problem. </title> <note> 1993. In preparation. </note>
Reference-contexts: an integer programming success story exploiting all of the above advances is the recent work of Applegate, Bixby, Cook and Chvatal in which a 4461 city traveling salesman problem was solved to exact optimality using a complex branch-and-cut code running on a network of up to 60 loosely connected workstations <ref> [ABCC93] </ref>. 11 Chapter 3 Related Work Compiling a single name space program for a physically distributed-memory architecture requires the mapping of the program's data and computation onto the processors of the target machine.
Reference: [ACG + 94] <author> V. Adve, A. Carle, E. Granston, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, J. Mellor-Crummey, C-W. Tseng, and S. Warren. </author> <title> Requirements for data-parallel programming environments. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 48-58, </pages> <year> 1994. </year>
Reference-contexts: Based on the resulting information, an execution model and target architecture machine model computes the overall execution time. The machine model uses our training set approach [BFKK91, HKK + 91]. A prototype based on our framework has been implemented as part of the D system <ref> [ACG + 94] </ref>. Experiments showed that through the use of 0-1 integer programming and our new approach to performance prediction, good data layouts can be deter 5 mined efficiently. <p> The prototype and our experiments are discussed below. 5.2.1 Prototype Implementation A prototype data layout assistant tool has been implemented as part of the D system <ref> [ACG + 94] </ref>. The prototype is a batch system. It takes Fortran 77 programs as input and generates Fortran D data layout specifications for each phase in the program. The prototype tool performs only intra-procedural analysis.
Reference: [AG94] <author> R. Govindarajan E. R. Altman and G. R. Gao. </author> <title> Minimizing register requirements under resource-constrained rate-optimal software pipelining. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <address> San Jose, CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: Pugh developed a dependence analysis test, called the Omega Test based on an integer programming algorithm [Pug91]. Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [Fea94] and Ning, Govindarajan, Altman and Gao <ref> [NG93, AG94, AGG95] </ref>. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [Phi95] and Garcia, Ayguade and Labarta [GAL95].
Reference: [AGG + 94] <author> E. Ayguade, J. Garcia, M. Girones, J. Labarta, J. Torres, and M. Valero. </author> <title> Detecting and using affinity in an automatic data distribution tool. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The identification of program segments in which data can be statically mapped, and the accurate modeling of the potential remapping costs make the dynamic data mapping problem harder than the static problem. The smallest possible statically mapped program regions may be single statements [KN90, CGST93, Phi95], loop nests <ref> [AL93, LT93, AGG + 94, NDG95, PB95] </ref>, or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable [SSP + 95].
Reference: [AGG95] <author> E. R. Altman, R. Govindarajan, and G. R. Gao. </author> <title> Scheduling and mapping: Software pipelining in the presence of structural hazards. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Program Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Pugh developed a dependence analysis test, called the Omega Test based on an integer programming algorithm [Pug91]. Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [Fea94] and Ning, Govindarajan, Altman and Gao <ref> [NG93, AG94, AGG95] </ref>. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [Phi95] and Garcia, Ayguade and Labarta [GAL95].
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: The approach is based on data access summary information such as data access descriptors (DAD [Bal90]) or regular section descriptors (RSD [Cal87]) to determine what data has to be communicated, and on the dependence graph to determine when the data can be communicated <ref> [AK87, Wol89] </ref>. Figure 4.13 shows the message vec-torization algorithm that moves communication out of loop nests as far as possible [BFKK90]. A similar algorithm has been proposed by Gerndt [Ger90].
Reference: [AKLS88] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem. The decomposition of the data mapping into alignment and distribution lead to a two step strategy, where alignment analysis is followed by distribution analysis <ref> [AKLS88, Wei91, LC90a, LC91b, GB91, GB92b, Who92a, CGST93] </ref>. Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. <p> Compass Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape.
Reference: [AL93] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference 108 on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The identification of program segments in which data can be statically mapped, and the accurate modeling of the potential remapping costs make the dynamic data mapping problem harder than the static problem. The smallest possible statically mapped program regions may be single statements [KN90, CGST93, Phi95], loop nests <ref> [AL93, LT93, AGG + 94, NDG95, PB95] </ref>, or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable [SSP + 95]. <p> This is not true for the compilation for MIMD machines with scalar node processors. The node compiler will generate the necessary temporaries. SUIF Anderson and Lam discuss a compiler algorithm for automatic data and computation mapping for dense matrix computations <ref> [AL93] </ref>. The input programs consist of generally nested loops with explicit parallelism. Each loop can be either a forall, doacross, or sequential loop. Loop bounds and array subscripts are assumed to be affine functions of the loop indices and symbolic constants. <p> First, only alignment preferences between arrays are considered. In the second stage, each array is mapped onto the unique program template such that the relative alignment preferences are respected. The second stage of the alignment mapping is called orientation <ref> [AL93] </ref>. This section discusses basic operations that are needed to identify and represent relative alignment preferences, to detect and resolve conflicting relative alignment preferences, and to compare relative candidate alignments. The comparison of alignment candidates is important in order to avoid redundant alignment information in the alignment search spaces. <p> Other heuristics may use a greedy algorithm, for instance, to propagate orientations from the most frequently executed phases to less frequently executed phases <ref> [AL93] </ref>. 4.3.3 Distribution Analysis Distribution analysis is performed after alignment analysis. A candidate distribution maps each template dimension either by BLOCK or CYCLIC (i) onto the target architecture, or replicates or localizes the template dimension.
Reference: [AMCA + 95] <author> V. Adve, J. Mellor-Crummey, M. Anderson, K. Kennedy, J-C. Wang, and D. Reed. </author> <title> An integrated compilation and performance analysis environment for data parallel programs. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Decisions to improve program performance require knowledge about the performance characteristics of the program. Interactive programming tools may use dynamic profiling and/or static performance estimation to identify code segments critical for the overall program performance <ref> [AMCA + 95, MCAK94, HKTW94, Ree94] </ref>. This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots".
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: She*er et al. describe techniques to perform such phase merges [SSP + 95]. Transformations to improve phase recognition are beyond the scope of this thesis. The phase structure of the program is represented in the phase control flow graph (PCFG), an augmented control flow graph <ref> [ASU86] </ref> where each phase is represented by a single node, called a phase node. Structured control flow between phases such as loops and branches are represented by special nodes in the PCFG. The graph is annotated with branch probabilities and loop control information. <p> Given a node program, the estimator's execution model computes the largest local segment for the specified problem size and number of available processors. The execution model performs a tree walk over the abstract syntax tree (AST <ref> [ASU86] </ref>) of the program. The program may only contain structured control flow. If a control-flow branch is visited, the user is queried for the 61 point-wise red-black relaxation node program with column-wise data layout 62 branch probabilities. <p> The proposed data flow problem is similar to the reaching definitions <ref> [ASU86] </ref> and reaching decomposition problems [Tse93]. <p> The costs of global reductions are determined by training sets. Machine Model The identification of basic computations is based on the phase's abstract syntax tree representation (AST <ref> [ASU86] </ref>). Performance estimates for basic computations and communication patterns are based on machine level training sets for Intel's iPSC/860 or Paragon. A general discussion of machine level training sets can be found in Section 4.4.2.
Reference: [Bal90] <author> V. Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This early compilation method is hard to simulate efficiently. We have developed a new compilation approach that allows the analysis and optimization of communication at an abstract level without source-level transformations. The approach is based on data access summary information such as data access descriptors (DAD <ref> [Bal90] </ref>) or regular section descriptors (RSD [Cal87]) to determine what data has to be communicated, and on the dependence graph to determine when the data can be communicated [AK87, Wol89]. Figure 4.13 shows the message vec-torization algorithm that moves communication out of loop nests as far as possible [BFKK90].
Reference: [BFKK90] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Figure 4.13 shows the message vec-torization algorithm that moves communication out of loop nests as far as possible <ref> [BFKK90] </ref>. A similar algorithm has been proposed by Gerndt [Ger90]. Tseng extended our basic compilation approach and showed that it is flexible enough to accommodate other communication optimizations such as message aggregation, message coalescing, and coarse-grain pipelining [Tse93].
Reference: [BFKK91] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The compiler model ignores computations and communications that are not crucial for the overall performance of the compiler generated code. Based on the resulting information, an execution model and target architecture machine model computes the overall execution time. The machine model uses our training set approach <ref> [BFKK91, HKK + 91] </ref>. A prototype based on our framework has been implemented as part of the D system [ACG + 94]. Experiments showed that through the use of 0-1 integer programming and our new approach to performance prediction, good data layouts can be deter 5 mined efficiently. <p> Unfortunately, the authors do not give a detailed description of their approach to modeling the memory hierarchy. Using off-line benchmarking routines to estimate the system performance in terms of basic computation and communication operations is not a new idea <ref> [BFKK91] </ref>. What is new is the representation of the system characteristics in an abstract hierarchy. This representation should allow the performance framework to pick the right level of system and application abstraction for different performance prediction goals. <p> Therefore, a source-level performance estimator is needed that can target different compilers and architectures. One of the main contributions of this thesis is a new approach to source-level performance estimation. The approach is based on so-called training sets <ref> [BFKK91, HKK + 91] </ref> and efficient compiler, execution, and machine models. 4.4.1 Training Sets A training set is a collection of kernel routines that measure the cost of various communication and computation patterns. <p> We have experimented with the chi-square fit method [PFTV88] to approximate cost functions represented by raw data files. If the communication cost function can be decomposed into piece-wise linear functions, the chi-square fit method can be used to approximate each of the linear functions <ref> [BFKK91] </ref>. Other approximation methods may represent only a subset of the data points from a raw data file in an internal cost table. <p> If a computation or communication pattern is encountered, the cost of the pattern is determined using the machine-level training set, and the result is added to the overall estimated execution time of the node program <ref> [BFKK91] </ref>. The estimator's machine-level training sets contain communication patterns that call EXPRESS routines [EXP89]. EXPRESS is a portable communication library similar to PVM [GBD + 94] and MPI [GLS94]. The estimator predicts the performance of node programs with calls to EXPRESS communication routines. <p> However, for bigger problem sizes, the two-dimensional block-wise layout is superior since less data has to be communicated and load balancing is better. For varying problem sizes, the "crossover points" of the two data layouts were determined with high accuracy <ref> [BFKK91, Kre93a] </ref>. It is important to note that the estimator was developed for training-set evaluation purposes only.
Reference: [Bix92] <author> R. Bixby. </author> <title> Implementing the Simplex method: The initial basis. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4(3), </volume> <year> 1992. </year>
Reference-contexts: Each of the 0-1 problem instances was solved by CPLEX y , a state-of-the-art linear integer programming tool and library, partly developed by Robert Bixby at Rice University <ref> [Bix92] </ref>. CPLEX includes an implementation of a general-purpose branch-and-bound code for mixed integer programming. Being general purpose, this code does not exploit the structural properties of our particular 0-1 problems. The following table gives the solution times in seconds of the 24 0-1 problem instances using CPLEX on a SPARC-10.
Reference: [Bix94] <author> R. Bixby. </author> <title> Progress in linear programming. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(1), </volume> <year> 1994. </year>
Reference-contexts: The basic technique for solving integer programming problems is to apply intelligent branch-and-bound using linear programming at the nodes. Important improvements have occurred in three areas. First, linear programming codes are on average approximately two orders of magnitude faster than they were five years ago, particularly for larger problems <ref> [Bix94] </ref>. Combined with the improvements in computing speed over that same period these codes represent an approximate four orders of magnitude improvement in our ability to solve linear programming problems. The second major development is in so-called cutting-plane technology.
Reference: [BKK + 89] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: The performance estimator was implemented as part of the ParaScope interactive parallel programming environment <ref> [BKK + 89] </ref>. Since the estimator takes programs with explicit communication as input, a compiler model is not necessary. Given a node program, the estimator's execution model computes the largest local segment for the specified problem size and number of available processors.
Reference: [BKK94] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT94), </booktitle> <pages> pages 111-122, </pages> <address> Mon-treal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: All encountered instances of the two NP-complete problems were solved in a matter of seconds <ref> [BKK94, KK95] </ref>. Contrary to the common belief in the compiler and programming environment community, this new result suggests that not all NP-complete problems encountered in compilers or programming environments may have to be approximated using heuristics. <p> Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [Phi95] and Garcia, Ayguade and Labarta [GAL95]. The latter two works have been based on our experience with 0-1 integer programming for efficient solutions of NP-complete problems in an automatic data layout tool <ref> [BKK94, KK95] </ref>. Compass Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features [AKLS88, KLS88, KLS90, KN90, Wei91].
Reference: [Cal87] <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1987. </year> <month> 109 </month>
Reference-contexts: We have developed a new compilation approach that allows the analysis and optimization of communication at an abstract level without source-level transformations. The approach is based on data access summary information such as data access descriptors (DAD [Bal90]) or regular section descriptors (RSD <ref> [Cal87] </ref>) to determine what data has to be communicated, and on the dependence graph to determine when the data can be communicated [AK87, Wol89]. Figure 4.13 shows the message vec-torization algorithm that moves communication out of loop nests as far as possible [BFKK90].
Reference: [CCL89] <author> M. Chen, Y. Choo, and J. Li. </author> <title> Theory and pragmatics of compiling efficient parallel code. </title> <type> Technical Report YALEU/DCS/TR-760, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. Crystal Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines.
Reference: [CGST92] <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <booktitle> In Proceedings of the Second Workshop on Languages, Compilers, and Runtime Environments for Distributed Memory Multiprocessors, </booktitle> <address> Bolder, CO, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Performance figures of actual runs are not reported. Projects at RIACS, Xerox PARC, and MIT Chatterjee, Gilbert, Schreiber, and Teng discuss a framework for automatic alignment in an array-based, data-parallel language such as Fortran90 <ref> [CGST93, CGST92, GS91] </ref>. They provide algorithms for automatic alignment of arrays in a single basic block. Each intermediate result of a computations in a basic block is assigned to a temporary array. This allows intermediate results to be mapped explicitly.
Reference: [CGST93] <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Albuquerque, NM, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem. The decomposition of the data mapping into alignment and distribution lead to a two step strategy, where alignment analysis is followed by distribution analysis <ref> [AKLS88, Wei91, LC90a, LC91b, GB91, GB92b, Who92a, CGST93] </ref>. Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. <p> The identification of program segments in which data can be statically mapped, and the accurate modeling of the potential remapping costs make the dynamic data mapping problem harder than the static problem. The smallest possible statically mapped program regions may be single statements <ref> [KN90, CGST93, Phi95] </ref>, loop nests [AL93, LT93, AGG + 94, NDG95, PB95], or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable [SSP + 95]. <p> Performance figures of actual runs are not reported. Projects at RIACS, Xerox PARC, and MIT Chatterjee, Gilbert, Schreiber, and Teng discuss a framework for automatic alignment in an array-based, data-parallel language such as Fortran90 <ref> [CGST93, CGST92, GS91] </ref>. They provide algorithms for automatic alignment of arrays in a single basic block. Each intermediate result of a computations in a basic block is assigned to a temporary array. This allows intermediate results to be mapped explicitly. <p> This is often referred to as "relaxing the owner-computes rule". In the SIMD model of execution, array temporaries must be introduced by the compiler for intermediate values <ref> [CGST93] </ref>. The possibility of using these temporaries to relax the owner computes rule comes therefore `for free'. This is not true for the compilation for MIMD machines with scalar node processors. The node compiler will generate the necessary temporaries. <p> This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots". Compilers use static performance models to predict the performance benefits of program transformations including automatic data layout <ref> [GB92a, WL91, CMT94, CGST93, Wan94] </ref>. Different transformations require different performance estimation accuracy. In this section we will focus on a discussion of performance models that can be used to support automatic data layout at the program source code level. <p> The basic operations and methods form the building blocks for implementing different heuristics and strategies for the alignment search space construction. The heuristic implemented in our prototype tool is discussed in Section 4.3.2. There are two types of alignment preferences, namely inter-dimensional and intra-dimensional alignment <ref> [LC90a, KLS90, CGST93] </ref>. The current framework does not perform intra-dimensional alignment analysis, i.e., assumes canonical offset and stride alignments. The discussion of the basic operations will be restricted to inter-dimensional alignment preferences.
Reference: [CH91] <author> B.M. Chapman and H.M. Herbeck. </author> <title> Knowledge-based parallelization for distributed memory systems. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The framework has been implemented for straight-line code. Unfortunately, experimental data to verify the accuracy and efficiency has not been provided. Superb-2 Chapman, Fahringer, Blasko, Herbeck, and Zima at the University of Vienna propose automatic data decomposition as part of the interactive parallelization system SUPERB-2 <ref> [CHZ91, CH91, FBZ92] </ref>. SUPERB-2 takes Fortran 77 programs as input and generates SPMD node programs with explicit communication. Their approach is based on Gupta's and Banerjee's work at Illinois (see Section 3.1.1).
Reference: [CHZ91] <author> B. Chapman, H. Herbeck, and H. Zima. </author> <title> Automatic support for data distribution. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The framework has been implemented for straight-line code. Unfortunately, experimental data to verify the accuracy and efficiency has not been provided. Superb-2 Chapman, Fahringer, Blasko, Herbeck, and Zima at the University of Vienna propose automatic data decomposition as part of the interactive parallelization system SUPERB-2 <ref> [CHZ91, CH91, FBZ92] </ref>. SUPERB-2 takes Fortran 77 programs as input and generates SPMD node programs with explicit communication. Their approach is based on Gupta's and Banerjee's work at Illinois (see Section 3.1.1).
Reference: [CK88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Early compilation approaches first inserted so-called run-time resolution code for each array reference into the program. When executed, this code performs the necessary communication for a single array reference. Based on the run-time resolution program, the compiler tried to optimize the communication through source-level transformations <ref> [ZBG88, CK88, RP89] </ref>. This early compilation method is hard to simulate efficiently. We have developed a new compilation approach that allows the analysis and optimization of communication at an abstract level without source-level transformations.
Reference: [CLR90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Then the number of distribution schemes for a d-dimensional program template, size, is size = d X 2 i fl @ i A fl @ i 1 A fl fi, O, and denote asymptotically tight bounds, asymptotically upper bound, and asymptotically lower bound, respectively <ref> [CLR90] </ref>. 51 Proof The index i keeps track of the number of dimensions to be partitioned. For any given i, 1 i d, there are 0 @ i 1 A choices to divide the number of processors into i distinct, non-empty groups, each assigned to one distributed dimension. <p> We will show that in this case the data layout selection problem can be solved in polynomial time in the sizes of the candidate layout search spaces. The main idea is to decompose the data layout selection problem into subproblems that can be solved as single-source shortest path problems <ref> [CLR90] </ref>. The "length" of a path includes the weights on the nodes and the weights of the edges along the path. The PCFG is decomposed in a hierarchical fashion, from innermost to outermost loop or branch levels. <p> Note that the edge weights in the loop structure DLG reflect the frequencies of execution. 73 A dynamic programming solution of the single-source shortest paths problem for a directed acyclic graph is described in <ref> [CLR90] </ref>. The time complexity of the algorithm is linear in the number of edges in the graph. Let k denote the maximal number of layout candidates for each phase and p the number of phases. The number of edges in the loop structure DLG is O (pk 2 ).
Reference: [Clu89] <author> The Perfect Club. </author> <title> The Perfect Club benchmarks: efficient performance evaluation of supercomputers. </title> <journal> Int. J. Supercomp. Appl., </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: The automatic techniques have been implemented as part of Parafrase-2. They have been applied to five Fortran programs, namely one routine from the Linpack library (dgefa), one Eispack routine (tred2), and three programs from the Perfect Club Benchmark Suite (trfd, mdg, flo52) <ref> [Clu89] </ref>. In the study, all the steps of the described automatic data layout techniques were simulated by hand. A distributed memory compiler is not part of Parafrase-2. Actual performance figures for the generated data layout schemes are only given for tred2 on an iPSC/2 hypercube system.
Reference: [CMT94] <author> S. Carr, K. S. M c Kinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots". Compilers use static performance models to predict the performance benefits of program transformations including automatic data layout <ref> [GB92a, WL91, CMT94, CGST93, Wan94] </ref>. Different transformations require different performance estimation accuracy. In this section we will focus on a discussion of performance models that can be used to support automatic data layout at the program source code level.
Reference: [CMZ92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year> <month> 110 </month>
Reference-contexts: The design of HPF has been influenced by languages such as Fortran D [FHK + 90], CM Fortran [TMC89], Kali [KM91], and Vienna Fortran <ref> [CMZ92] </ref>. 2.2 0-1 Integer Programming Integer programming can be used to solve many real world problems that require the management and efficient use of scarce resources to improve productivity. Examples of such problems are VLSI circuit design, airline crew scheduling, and communication and transportation network design.
Reference: [DFJ54] <author> G. B. Dantzig, D. R. Fulkerson, and S. M. Johnson. </author> <title> Solution of a large scale traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 7 </volume> <pages> 58-66, </pages> <year> 1954. </year>
Reference-contexts: Combined with the improvements in computing speed over that same period these codes represent an approximate four orders of magnitude improvement in our ability to solve linear programming problems. The second major development is in so-called cutting-plane technology. Motivated by work of Dantzig, Johnson and Fulkerson in the 50's <ref> [DFJ54] </ref>, Padberg, Groetschel and others have shown how cutting-plane techniques could be used to strengthen the linear programming relaxations of many 0-1 integer programming problems [PR91]. The strengthening is effected by studying the facets of the underlying polytope generated by the convex hull of 0-1 solutions.
Reference: [D'H89] <author> E. D'Hollander. </author> <title> Partitioning and labeling of index sets in do loops with constant dependence. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: entire program is considered a simpler problem than allowing dynamic changes of the data mapping. 3.1.1 Static Data Mappings Some early work on static data and computation mapping used data dependence information to determine whether a communication-free partitioning of the iteration 13 space of a single loop nests was possible <ref> [RS89, D'H89, HA90] </ref>. The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem.
Reference: [Dyb87] <author> R. K. Dybvig. </author> <title> The Scheme Programming Language. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> En-glewood Cliffs, New Jersey, </address> <year> 1987. </year>
Reference-contexts: The tool was written in Scheme, a dialect of Lisp <ref> [Dyb87] </ref>. The sole purpose of this tool was to provide a testbed to determine the efficiency of the different 0-1 problem formulations.
Reference: [EXP89] <institution> Parasoft Corporation. </institution> <note> Express User's Manual, </note> <year> 1989. </year>
Reference-contexts: If a computation or communication pattern is encountered, the cost of the pattern is determined using the machine-level training set, and the result is added to the overall estimated execution time of the node program [BFKK91]. The estimator's machine-level training sets contain communication patterns that call EXPRESS routines <ref> [EXP89] </ref>. EXPRESS is a portable communication library similar to PVM [GBD + 94] and MPI [GLS94]. The estimator predicts the performance of node programs with calls to EXPRESS communication routines.
Reference: [Fah92] <author> T. Fahringer. </author> <title> Private communication. </title> <year> 1992. </year>
Reference-contexts: Static performance estimation is used to evaluate the data mappings in a search space of reasonable data layouts. The proposed tool is currently being implemented at the University of Vienna. A prototype performance estimator has been implemented. <ref> [FBZ92, Fah92] </ref>. No experimental results have been published. Syracuse's Performance Interpretation Engine Parashar, Hariri, Haupt, and Fox present an interpretive approach for performance prediction in a high performance computing environment [PHHF94].
Reference: [FBZ92] <author> T. Fahringer, R. Blasko, and H.P. Zima. </author> <title> Automatic performance prediction to support parallelization of Fortran programs for massively parallel systems. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The framework has been implemented for straight-line code. Unfortunately, experimental data to verify the accuracy and efficiency has not been provided. Superb-2 Chapman, Fahringer, Blasko, Herbeck, and Zima at the University of Vienna propose automatic data decomposition as part of the interactive parallelization system SUPERB-2 <ref> [CHZ91, CH91, FBZ92] </ref>. SUPERB-2 takes Fortran 77 programs as input and generates SPMD node programs with explicit communication. Their approach is based on Gupta's and Banerjee's work at Illinois (see Section 3.1.1). <p> Static performance estimation is used to evaluate the data mappings in a search space of reasonable data layouts. The proposed tool is currently being implemented at the University of Vienna. A prototype performance estimator has been implemented. <ref> [FBZ92, Fah92] </ref>. No experimental results have been published. Syracuse's Performance Interpretation Engine Parashar, Hariri, Haupt, and Fox present an interpretive approach for performance prediction in a high performance computing environment [PHHF94].
Reference: [Fea94] <author> P. Feautrier. </author> <title> Fine-grain scheduling under resource constraints. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Pugh developed a dependence analysis test, called the Omega Test based on an integer programming algorithm [Pug91]. Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier <ref> [Fea94] </ref> and Ning, Govindarajan, Altman and Gao [NG93, AG94, AGG95]. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [Phi95] and Garcia, Ayguade and Labarta [GAL95].
Reference: [FHK + 90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Instead, the intent is to provide decompositions that are both powerful enough to express data parallelism in scientific programs, and simple enough to permit the compiler to produce efficient programs. The design of HPF has been influenced by languages such as Fortran D <ref> [FHK + 90] </ref>, CM Fortran [TMC89], Kali [KM91], and Vienna Fortran [CMZ92]. 2.2 0-1 Integer Programming Integer programming can be used to solve many real world problems that require the management and efficient use of scarce resources to improve productivity.
Reference: [FJL + 88] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Communication inside a phase may lead to a pipelined execution of the loop. Communication outside of the phase may result in a loosely synchronous execution scheme <ref> [FJL + 88] </ref>. In addition, 59 special communication patterns may be recognized that represent global operations such as reductions. Based on the synchronization schemes and the costs for simple communication patterns and basic computations, the execution model determines the overall cost estimate for a candidate layout and its phase.
Reference: [FZ93] <author> T. Fahringer and H.P. Zima. </author> <title> A static parameter based performance prediction tool for parallel programs. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: The machine model may be based on machine-level training sets or on other performance models, such as the models discussed by Gupta and Banerjee [GB92a] or Fahringer and Zima <ref> [FZ93] </ref>. Training sets of global communication operations can be used to estimate the cost of remappings between candidate data layouts. A computation training set measures the execution times for arithmetic operations and intrinsic functions for different data types and memory access patterns.
Reference: [GAL95] <author> J. Garcia, E. Ayguade, and J. Labarta. </author> <title> A novel approach towards automatic data distribution. In Proceedings of the Workshop on Automatic Data Layout and Performance Prediction (AP'95), </title> <address> Houston, TX, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time <ref> [Ke93, GAL95] </ref>. Crystal Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b]. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. <p> Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [Fea94] and Ning, Govindarajan, Altman and Gao [NG93, AG94, AGG95]. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [Phi95] and Garcia, Ayguade and Labarta <ref> [GAL95] </ref>. The latter two works have been based on our experience with 0-1 integer programming for efficient solutions of NP-complete problems in an automatic data layout tool [BKK94, KK95].
Reference: [GAY91] <author> E. Gabber, A. Averbuch, and A. Yehudai. </author> <title> Experience with a portable paral-lelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year> <month> 111 </month>
Reference-contexts: ASPAR and P 3 C ASPAR is a compiler for the C language developed by the ParaSoft corporation [IFKF90]. P 3 C is a research Pascal compiler designed and implemented at the Tel-Aviv University by Gabber, Averbuch, and Yehudai <ref> [GAY91] </ref>. Both systems generate SPMD node programs that contain calls to communication library routines. The compilers perform only a simple form of program analysis to generate the correct communications. The set of possible data decomposition schemes is small. Inter-procedural analysis is performed.
Reference: [GB90] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <type> Technical Report CRHC-90-14, </type> <institution> Center for Reliable and High-Performance Computing, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Distribution analysis has not been implemented. The distribution strategy is read in at runtime [Li92]. Parafrase-2 Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer <ref> [GB90, GB91, GB92b] </ref>. The compiler takes Fortran 77 as input and generates SPMD node programs with explicit communication. The compiler performs alignment and distribution analysis based on constraints for each single statement in the program. Constraints represent properties of the data layout and are associated with a quality measure.
Reference: [GB91] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem. The decomposition of the data mapping into alignment and distribution lead to a two step strategy, where alignment analysis is followed by distribution analysis <ref> [AKLS88, Wei91, LC90a, LC91b, GB91, GB92b, Who92a, CGST93] </ref>. Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. <p> Distribution analysis has not been implemented. The distribution strategy is read in at runtime [Li92]. Parafrase-2 Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer <ref> [GB90, GB91, GB92b] </ref>. The compiler takes Fortran 77 as input and generates SPMD node programs with explicit communication. The compiler performs alignment and distribution analysis based on constraints for each single statement in the program. Constraints represent properties of the data layout and are associated with a quality measure.
Reference: [GB92a] <author> M. Gupta and P. Banerjee. </author> <title> Compile-time estimation of communication costs on multicomputers. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <address> Beverly Hills, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots". Compilers use static performance models to predict the performance benefits of program transformations including automatic data layout <ref> [GB92a, WL91, CMT94, CGST93, Wan94] </ref>. Different transformations require different performance estimation accuracy. In this section we will focus on a discussion of performance models that can be used to support automatic data layout at the program source code level. <p> Machine Model The actual costs of communication operations and basic computations for the target machine architecture are determined by a machine model. The machine model may be based on machine-level training sets or on other performance models, such as the models discussed by Gupta and Banerjee <ref> [GB92a] </ref> or Fahringer and Zima [FZ93]. Training sets of global communication operations can be used to estimate the cost of remappings between candidate data layouts. A computation training set measures the execution times for arithmetic operations and intrinsic functions for different data types and memory access patterns.
Reference: [GB92b] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem. The decomposition of the data mapping into alignment and distribution lead to a two step strategy, where alignment analysis is followed by distribution analysis <ref> [AKLS88, Wei91, LC90a, LC91b, GB91, GB92b, Who92a, CGST93] </ref>. Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. <p> Distribution analysis has not been implemented. The distribution strategy is read in at runtime [Li92]. Parafrase-2 Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer <ref> [GB90, GB91, GB92b] </ref>. The compiler takes Fortran 77 as input and generates SPMD node programs with explicit communication. The compiler performs alignment and distribution analysis based on constraints for each single statement in the program. Constraints represent properties of the data layout and are associated with a quality measure.
Reference: [GBD + 94] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM: Parallel Virtual Machine. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: The estimator's machine-level training sets contain communication patterns that call EXPRESS routines [EXP89]. EXPRESS is a portable communication library similar to PVM <ref> [GBD + 94] </ref> and MPI [GLS94]. The estimator predicts the performance of node programs with calls to EXPRESS communication routines.
Reference: [Ger90] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency|Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Figure 4.13 shows the message vec-torization algorithm that moves communication out of loop nests as far as possible [BFKK90]. A similar algorithm has been proposed by Gerndt <ref> [Ger90] </ref>. Tseng extended our basic compilation approach and showed that it is flexible enough to accommodate other communication optimizations such as message aggregation, message coalescing, and coarse-grain pipelining [Tse93]. He also showed that in practice message vectorization is a highly effective communication optimization.
Reference: [GLS94] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: The estimator's machine-level training sets contain communication patterns that call EXPRESS routines [EXP89]. EXPRESS is a portable communication library similar to PVM [GBD + 94] and MPI <ref> [GLS94] </ref>. The estimator predicts the performance of node programs with calls to EXPRESS communication routines.
Reference: [GS91] <author> J.R. Gilbert and R. Schreiber. </author> <title> Optimal expression evaluation for data parallel architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 58-64, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Performance figures of actual runs are not reported. Projects at RIACS, Xerox PARC, and MIT Chatterjee, Gilbert, Schreiber, and Teng discuss a framework for automatic alignment in an array-based, data-parallel language such as Fortran90 <ref> [CGST93, CGST92, GS91] </ref>. They provide algorithms for automatic alignment of arrays in a single basic block. Each intermediate result of a computations in a basic block is assigned to a temporary array. This allows intermediate results to be mapped explicitly.
Reference: [HA90] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: entire program is considered a simpler problem than allowing dynamic changes of the data mapping. 3.1.1 Static Data Mappings Some early work on static data and computation mapping used data dependence information to determine whether a communication-free partitioning of the iteration 13 space of a single loop nests was possible <ref> [RS89, D'H89, HA90] </ref>. The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem.
Reference: [Hec77] <author> M. S. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <publisher> North Holland, </publisher> <address> New York, NY, </address> <year> 1977. </year>
Reference-contexts: Each partition in the partitioning represents a connected component in the CAG. The set of all possible conflict-free, inter-dimensional alignments of a set of arrays forms a semi-lattice <ref> [Hec77] </ref>. The bottom element of the lattice is the CAG that contains no alignment information, i.e., the graph contains no edges and therefore its partitioning consists of partitions that contain only single nodes. <p> Each class of phases is represented by its merged, conflict-free CAG. The current prototype uses a greedy strategy to determine the single phase CAG to be merged next. The algorithm visits the phases, i.e., the nodes in the PCFG in reverse postorder (rPOSTORDER) <ref> [Hec77] </ref>, and merges their CAGs as long as no conflict is detected. Once a conflict is encountered, a new class is created and initialized with the CAG of the single phase that led to the conflict. The greedy merging procedure is resumed based on this new class.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <month> May </month> <year> 1993. </year> <note> To appear in Scientific Programming, vol. 2, no. 1. 112 </note>
Reference-contexts: Section 2.1 reviews the data layout directives of HPF relevant to our work on automatic data layout. The HPF language specification document contains a full language description <ref> [Hig93] </ref>. Some problems encountered in our framework for automatic data layout have been shown to be NP-complete. As part of this thesis we have translated instances of these NP-complete problems to instances of 0-1 integer programming problems.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The compiler model ignores computations and communications that are not crucial for the overall performance of the compiler generated code. Based on the resulting information, an execution model and target architecture machine model computes the overall execution time. The machine model uses our training set approach <ref> [BFKK91, HKK + 91] </ref>. A prototype based on our framework has been implemented as part of the D system [ACG + 94]. Experiments showed that through the use of 0-1 integer programming and our new approach to performance prediction, good data layouts can be deter 5 mined efficiently. <p> Therefore, a source-level performance estimator is needed that can target different compilers and architectures. One of the main contributions of this thesis is a new approach to source-level performance estimation. The approach is based on so-called training sets <ref> [BFKK91, HKK + 91] </ref> and efficient compiler, execution, and machine models. 4.4.1 Training Sets A training set is a collection of kernel routines that measure the cost of various communication and computation patterns.
Reference: [HKTW94] <author> S. Hiranandani, K. Kennedy, C.-W. Tseng, and S. Warren. </author> <title> The D Editor: A new interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Decisions to improve program performance require knowledge about the performance characteristics of the program. Interactive programming tools may use dynamic profiling and/or static performance estimation to identify code segments critical for the overall program performance <ref> [AMCA + 95, MCAK94, HKTW94, Ree94] </ref>. This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots".
Reference: [IFKF90] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: ASPAR and P 3 C ASPAR is a compiler for the C language developed by the ParaSoft corporation <ref> [IFKF90] </ref>. P 3 C is a research Pascal compiler designed and implemented at the Tel-Aviv University by Gabber, Averbuch, and Yehudai [GAY91]. Both systems generate SPMD node programs that contain calls to communication library routines. The compilers perform only a simple form of program analysis to generate the correct communications.
Reference: [Ke93] <author> C.W. Keler. </author> <title> Knowledge-based automatic parallelization by pattern recognition. </title> <editor> In Christoph W. Keler, editor, </editor> <title> Automatic Parallelization | New Approaches to Code Generation, Data Distribution, </title> <booktitle> and Performance Prediction, </booktitle> <pages> pages 110-135. </pages> <booktitle> Vieweg Advanced Studies in Computer Science, </booktitle> <publisher> Verlag Vieweg, Wiesbaden, </publisher> <address> Germany, </address> <year> 1993. </year>
Reference-contexts: Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time <ref> [Ke93, GAL95] </ref>. Crystal Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b]. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout.
Reference: [KK95] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year> <note> To appear. The paper is also available as technical report CRPC-TR94-498-S, Center for Research on Parallel Computation, </note> <institution> Rice University. </institution>
Reference-contexts: All encountered instances of the two NP-complete problems were solved in a matter of seconds <ref> [BKK94, KK95] </ref>. Contrary to the common belief in the compiler and programming environment community, this new result suggests that not all NP-complete problems encountered in compilers or programming environments may have to be approximated using heuristics. <p> Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [Phi95] and Garcia, Ayguade and Labarta [GAL95]. The latter two works have been based on our experience with 0-1 integer programming for efficient solutions of NP-complete problems in an automatic data layout tool <ref> [BKK94, KK95] </ref>. Compass Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features [AKLS88, KLS88, KLS90, KN90, Wei91].
Reference: [KLD92] <author> K. Knobe, J.D. Lukas, and W.J. Dally. </author> <title> Dynamic alignment on distributed memory systems. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: More recently, Knobe, Lukas, and Dally introduced the concept of a control preference. A control preference exists between the corresponding dimensions of an array in a conditional expression and an array occurrence in an operation that is control dependent on this expression <ref> [KLD92] </ref>. The preferences of the program are represented by the undirected preference graph where the arcs correspond to the preferences and the nodes are dimensions of textual occurrences of arrays and array sections.
Reference: [KLS88] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Massively parallel data optimization. </title> <booktitle> In Frontiers88: The 2nd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> Fairfax, VA, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: Compass Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape. <p> The alignment algorithm is based on the usage patterns of arrays and Fortran 8x array sections in the source program. Each pattern generates allocation requests, called preferences, that indicate the optimal layout of the arrays relative to each other <ref> [KLS88, KLS90] </ref>. An identity preference exists between corresponding dimensions of a definition and a use of the same array. It describes a preference to allocate identical 19 elements of the array on the same processors for the two textual occurrences.
Reference: [KLS90] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Dynamic realignment or redistribution is not considered, but inter-procedural performance analysis is performed. Alignment analysis is done at compile time based on the approach by Knobe, Lukas, and Steele <ref> [KLS90] </ref>. Distribution analysis is performed at run-time. Each primitive operation is associated with a cost function that computes the execution time of the operation under a given distribution, problem size, machine size, and machine topology. <p> Compass Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape. <p> The alignment algorithm is based on the usage patterns of arrays and Fortran 8x array sections in the source program. Each pattern generates allocation requests, called preferences, that indicate the optimal layout of the arrays relative to each other <ref> [KLS88, KLS90] </ref>. An identity preference exists between corresponding dimensions of a definition and a use of the same array. It describes a preference to allocate identical 19 elements of the array on the same processors for the two textual occurrences. <p> To locate the cycles, a spanning tree is constructed, using a greedy algorithm that chooses the next arc to add by finding the highest cost arc that is not already processed. If a cycle-creating arc induces a conflict, the corresponding preference will not be honored <ref> [KLS90] </ref>. Knobe and Natarajan have extended this algorithm to optimize the communication resulting from unhonored identity and conformance preferences [KN90]. For the MasPar machine, the data allocation functions generated by the data optimization component have to be transformed from mappings based on virtual processors to mappings based on physical processors. <p> The basic operations and methods form the building blocks for implementing different heuristics and strategies for the alignment search space construction. The heuristic implemented in our prototype tool is discussed in Section 4.3.2. There are two types of alignment preferences, namely inter-dimensional and intra-dimensional alignment <ref> [LC90a, KLS90, CGST93] </ref>. The current framework does not perform intra-dimensional alignment analysis, i.e., assumes canonical offset and stride alignments. The discussion of the basic operations will be restricted to inter-dimensional alignment preferences.
Reference: [KM91] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year> <month> 113 </month>
Reference-contexts: The design of HPF has been influenced by languages such as Fortran D [FHK + 90], CM Fortran [TMC89], Kali <ref> [KM91] </ref>, and Vienna Fortran [CMZ92]. 2.2 0-1 Integer Programming Integer programming can be used to solve many real world problems that require the management and efficient use of scarce resources to improve productivity. Examples of such problems are VLSI circuit design, airline crew scheduling, and communication and transportation network design.
Reference: [KN90] <author> K. Knobe and V. Natarajan. </author> <title> Data optimization: Minimizing residual interpro-cessor data motion on SIMD machines. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: The identification of program segments in which data can be statically mapped, and the accurate modeling of the potential remapping costs make the dynamic data mapping problem harder than the static problem. The smallest possible statically mapped program regions may be single statements <ref> [KN90, CGST93, Phi95] </ref>, loop nests [AL93, LT93, AGG + 94, NDG95, PB95], or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable [SSP + 95]. <p> Compass Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape. <p> Each virtual processor holds at most a single element of each array. The alignment algorithm performs intra-dimensional alignment and inter-dimensional alignment using similar techniques as Li and Chen. However, inter-dimensional permutations are not supported. Arrays may be mapped differently in different sections of the program <ref> [KN90] </ref>. The described techniques work only on single procedures. However, they handle complex control flow. Since the CM-2 supports the concept of virtual processors through its programming environment, data alignment is sufficient to specify the data layout. In contrast, the MasPar machine does not support virtual processors. <p> If a cycle-creating arc induces a conflict, the corresponding preference will not be honored [KLS90]. Knobe and Natarajan have extended this algorithm to optimize the communication resulting from unhonored identity and conformance preferences <ref> [KN90] </ref>. For the MasPar machine, the data allocation functions generated by the data optimization component have to be transformed from mappings based on virtual processors to mappings based on physical processors. Weiss discusses three distribution schemes, namely cyclic (horizontal), block (vertical), and block-cyclic distributions [Wei91].
Reference: [KR94] <author> U. Kremer and M. </author> <title> Rame. Compositional oil reservoir simulation in Fortran D: A feasibility study on Intel iPSC/860. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 8(2) </volume> <pages> 119-128, </pages> <month> Summer </month> <year> 1994. </year> <note> Also available as Technical Report CRPC-TR93335, Center for Research on Parallel Computation, </note> <institution> Rice University. </institution>
Reference-contexts: Shallow was written by Paul Swarztrauber from the National Center for Atmospheric Research (NCAR). All programs are written in a data-parallel programming style that allows good compile time analysis <ref> [KR94] </ref>. The automatic data layout tool was applied to each program for different test cases. A test case consists of a data type for the arrays in the program, a problem size, and a given number of processors used.
Reference: [Kre93a] <author> U. Kremer. </author> <title> Automatic data layout for distributed-memory machines. </title> <type> Technical Report CRPC-TR93-299-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> February </month> <year> 1993. </year> <type> (thesis proposal). </type>
Reference-contexts: However, for bigger problem sizes, the two-dimensional block-wise layout is superior since less data has to be communicated and load balancing is better. For varying problem sizes, the "crossover points" of the two data layouts were determined with high accuracy <ref> [BFKK91, Kre93a] </ref>. It is important to note that the estimator was developed for training-set evaluation purposes only.
Reference: [Kre93b] <author> U. Kremer. </author> <title> NP-completeness of dynamic remapping. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Nether-lands, </address> <month> December </month> <year> 1993. </year> <note> Also available as technical report CRPC-TR93-330-S (D Newsletter #8), </note> <institution> Rice University. </institution>
Reference-contexts: Rather than resorting to heuristics prematurely, our work capitalizes on 0-1 integer programming technology to compute optimal solutions for two NP-complete problems in our framework <ref> [LC90a, Kre93b] </ref>. Experiments indicate that through the use of the latest and most powerful general purpose techniques for linear and integer programming, computing the optimal solution of the two NP-complete problems is efficient in practice in the context of a programming environment.
Reference: [KZBG88] <author> U. Kremer, H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Advanced tools and techniques for automatic parallelization. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 387-393, </pages> <year> 1988. </year>
Reference-contexts: partitions of phase nodes in a greedy fashion. // The minimal number of conflict-free partitions is not guaranteed. // Determine an rPOSTORDER of the phase nodes in the PCFG such that // indices of nodes inside a loop are smaller than the indices of the // nodes following that loop <ref> [KZBG88] </ref> node partitioning = ;; CURRENT PART = ; i = 1 while i jN j do CURRENT PART = fig; current part CAGs = local i ; conflict-free = true while conflict-free and i + 1 jN j do current part CAGs = merge (current part CAGs, local i+1 )
Reference: [LC90a] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> Octo-ber </month> <year> 1990. </year>
Reference-contexts: Rather than resorting to heuristics prematurely, our work capitalizes on 0-1 integer programming technology to compute optimal solutions for two NP-complete problems in our framework <ref> [LC90a, Kre93b] </ref>. Experiments indicate that through the use of the latest and most powerful general purpose techniques for linear and integer programming, computing the optimal solution of the two NP-complete problems is efficient in practice in the context of a programming environment. <p> The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem. The decomposition of the data mapping into alignment and distribution lead to a two step strategy, where alignment analysis is followed by distribution analysis <ref> [AKLS88, Wei91, LC90a, LC91b, GB91, GB92b, Who92a, CGST93] </ref>. Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. <p> To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. Crystal Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines. <p> The goal is to find a partitioning that minimizes the overall sum of weights of edges between nodes in distinct partitions. Note that edges between partitions are alignment requests that cannot be satisfied. The solution of this alignment problem is shown to be NP-complete <ref> [LC90a] </ref>. The intra-dimensional alignment algorithm is based on the affine reference patterns and is straight forward. In the next compiler step the functional program is transformed into an imperative program that allows multiple assignments into the same memory location in order to ensure efficient reuse of memory. <p> Once a distribution strategy is chosen, redundant communication is eliminated. A prototype of the compiler has been implemented as part of Li's Ph.D. thesis at Yale University. Experimental results are reported for a heuristic algorithm that performs inter-dimensional alignment on a set of randomly generated component affinity 15 graphs <ref> [LC90a] </ref>. Distribution analysis has not been implemented. The distribution strategy is read in at runtime [Li92]. Parafrase-2 Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer [GB90, GB91, GB92b]. <p> The compiler does not perform inter-procedural analysis. A single, static decomposition scheme is derived for the entire program, i.e. dynamic realignment or redistribution are not supported. The compiler performs alignment analysis based on Li's and Chen' s approach <ref> [LC90a] </ref>. The communication cost of each statement with an array reference is expressed as a function of the machine size, number of processors in each dimension, and the method of partitioning, namely block or cyclic. <p> The basic operations and methods form the building blocks for implementing different heuristics and strategies for the alignment search space construction. The heuristic implemented in our prototype tool is discussed in Section 4.3.2. There are two types of alignment preferences, namely inter-dimensional and intra-dimensional alignment <ref> [LC90a, KLS90, CGST93] </ref>. The current framework does not perform intra-dimensional alignment analysis, i.e., assumes canonical offset and stride alignments. The discussion of the basic operations will be restricted to inter-dimensional alignment preferences. <p> The discussion of the basic operations will be restricted to inter-dimensional alignment preferences. Identification and Representation of Relative Alignment Preferences A central representation for the relative, inter-dimensional alignment problem is the weighted, undirected component affinity graph (CAG) introduced by Li and Chen at Yale University <ref> [LC90a] </ref>. It represents the alignment preferences of arrays that are coupled in a computation. A d-dimensional array is represented in the CAG by d nodes, one node for each dimension. Alignment preferences between dimensions of distinct arrays are represented as edges between the corresponding nodes. <p> A good solution tries to minimize the weights of the edges that cross partitions and therefore cannot be satisfied. Li and Chen showed that finding the optimal solution for the inter-dimensional alignment problem is NP-complete <ref> [LC90a] </ref>. Instead of using a heuristic, the current framework formulates the inter-dimensional alignment problem as an efficient 0-1 integer programming problem. Section 2.2 constrains an introduction to integer programming and 0-1 integer programming. <p> The target machine is assumed to be a MIMD target architecture. Our sample performance model is pessimistic since it assumes that unsatisfied alignment preferences will always lead to communication. In contrast to the original definition of a CAG by Li and Chen <ref> [LC90a] </ref>, the heuristic uses a directed CAG while computing the edge weights. The edge directions keep track of the flow of values due to the owner-computes rule.
Reference: [LC90b] <author> J. Li and M. Chen. </author> <title> Synthesis of explicit communication from shared-memory program references. </title> <type> Technical Report YALEU/DCS/TR-755, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. Crystal Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines.
Reference: [LC91a] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. Crystal Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines.
Reference: [LC91b] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(4) </volume> <pages> 213-221, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem. The decomposition of the data mapping into alignment and distribution lead to a two step strategy, where alignment analysis is followed by distribution analysis <ref> [AKLS88, Wei91, LC90a, LC91b, GB91, GB92b, Who92a, CGST93] </ref>. Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. <p> To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. Crystal Li, Chen, and Choo investigated techniques for automatic data layout as part of the Crystal compiler and language project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>. Crystal is a high-level, purely functional language. It does not contain statements that specify the data layout. The goal of the Crystal compiler is to generate efficient SPMD node programs with explicit communications or synchronization for a variety of massively parallel machines.
Reference: [Li92] <author> J. Li. </author> <title> Private communication. </title> <booktitle> 1992. </booktitle> <pages> 114 </pages>
Reference-contexts: Experimental results are reported for a heuristic algorithm that performs inter-dimensional alignment on a set of randomly generated component affinity 15 graphs [LC90a]. Distribution analysis has not been implemented. The distribution strategy is read in at runtime <ref> [Li92] </ref>. Parafrase-2 Gupta and Banerjee at the University of Illinois at Urbana-Champaign developed techniques for automatic data layout as part of a compiler based on the Parafrase-2 program restructurer [GB90, GB91, GB92b]. The compiler takes Fortran 77 as input and generates SPMD node programs with explicit communication.
Reference: [LT93] <author> P. Lee and T-B. Tsai. </author> <title> Compiling efficient programs for tightly-coupled distributed memory computers. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The identification of program segments in which data can be statically mapped, and the accurate modeling of the potential remapping costs make the dynamic data mapping problem harder than the static problem. The smallest possible statically mapped program regions may be single statements [KN90, CGST93, Phi95], loop nests <ref> [AL93, LT93, AGG + 94, NDG95, PB95] </ref>, or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable [SSP + 95].
Reference: [MCAK94] <author> John M. Mellor-Crummey, Vikram S. Adve, and Charles Koelbel. </author> <title> The Compiler's Role in Analysis and Tuning of Data-Parallel Programs. </title> <booktitle> In Proceedings of The Second Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 211-220, </pages> <address> Townsend, TN, </address> <month> May </month> <year> 1994. </year> <note> Also available via anonymous ftp from softlib.cs.rice.edu in pub/CRPC-TRs/reports/CRPC-TR94405.ps. </note>
Reference-contexts: Decisions to improve program performance require knowledge about the performance characteristics of the program. Interactive programming tools may use dynamic profiling and/or static performance estimation to identify code segments critical for the overall program performance <ref> [AMCA + 95, MCAK94, HKTW94, Ree94] </ref>. This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots". <p> Performance prediction of a pipelined phase execution is more complicated than in the loosely synchronous case. Execution models that can estimate pipelines of different granularity have been discussed in the literature, for instance in <ref> [MCAK94, PSCB94] </ref>. The actual choice of a particular pipeline model will depend on the desired accuracy. A detailed discussion of the execution model used in the prototype implementation of our layout assistant tool can be found in Section 5.2.1. <p> T high latency comm is used to compute the starting delay of the last processor. Our overall performance model is similar to the model described by Mellor-Crummey, Adve, and Koelbel <ref> [MCAK94] </ref>.
Reference: [NDG95] <author> Q. Ning, V. V. Dongen, and G. R. Gao. </author> <title> Automatic data and computation decomposition for distributed memory machines. </title> <booktitle> In Proceedings of the 28th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Maui, Hawaii, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: The identification of program segments in which data can be statically mapped, and the accurate modeling of the potential remapping costs make the dynamic data mapping problem harder than the static problem. The smallest possible statically mapped program regions may be single statements [KN90, CGST93, Phi95], loop nests <ref> [AL93, LT93, AGG + 94, NDG95, PB95] </ref>, or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable [SSP + 95].
Reference: [NG93] <author> Q. Ning and G. R. Gao. </author> <title> A novel framework of register allocation for software pipelining. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Albuquerque, NM, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: Pugh developed a dependence analysis test, called the Omega Test based on an integer programming algorithm [Pug91]. Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [Fea94] and Ning, Govindarajan, Altman and Gao <ref> [NG93, AG94, AGG95] </ref>. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [Phi95] and Garcia, Ayguade and Labarta [GAL95].
Reference: [NW88] <author> G. L. Nemhauser and L. A. Wolsey. </author> <title> Integer and Combinatorial Optimization. </title> <publisher> John Wiley & Sons, </publisher> <year> 1988. </year>
Reference-contexts: Solving a 0-1 integer programming problem has been shown to be NP-complete. In this thesis we will refer to a 0-1 linear integer programming problem as a 0-1 problem. An in-depth discussion of integer programming can be found in <ref> [NW88] </ref>. For decades, the integer and combinatorical optimization community has been working on methods to solve integer programming problems fast in practice.
Reference: [PB95] <author> D. Palermo and P. Banerjee. </author> <title> Automatic selection of dynamic data partitioning schemes for distributed-memory multicomputers. </title> <type> Technical Report CRHC-95-09, </type> <institution> Center for Reliable and High-Performance Computing, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: The identification of program segments in which data can be statically mapped, and the accurate modeling of the potential remapping costs make the dynamic data mapping problem harder than the static problem. The smallest possible statically mapped program regions may be single statements [KN90, CGST93, Phi95], loop nests <ref> [AL93, LT93, AGG + 94, NDG95, PB95] </ref>, or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable [SSP + 95]. <p> Paradigm Palermo and Banerjee extend previous work on automatic data layout by Gupta and Banerjee to handle dynamic remapping (see Section 3.1.1). Their approach is based on a hierarchical decomposition of the program into components such that remapping is profitable only between these components <ref> [PB95] </ref>. Each decomposition step consists of splitting a single component into two subcomponents if the best static data layout for the entire component has a higher cost than the sum of the costs for the best data layouts for each subcomponent.
Reference: [PFTV88] <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling. </author> <title> Numerical Recipes in C: </title> <booktitle> the Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: These files have to be read and processed by the data layout assistant tool at tool invocation time. Since the amount of data in the raw data files may be large, a compact internal representation is desirable or even necessary. We have experimented with the chi-square fit method <ref> [PFTV88] </ref> to approximate cost functions represented by raw data files. If the communication cost function can be decomposed into piece-wise linear functions, the chi-square fit method can be used to approximate each of the linear functions [BFKK91].
Reference: [PHHF94] <author> M. Parashar, S. Hariri, H. Haupt, and G. Fox. </author> <title> Interpreting the performance of HPF/Fortran90D. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The proposed tool is currently being implemented at the University of Vienna. A prototype performance estimator has been implemented. [FBZ92, Fah92]. No experimental results have been published. Syracuse's Performance Interpretation Engine Parashar, Hariri, Haupt, and Fox present an interpretive approach for performance prediction in a high performance computing environment <ref> [PHHF94] </ref>. An "interpretation engine" uses information about the target machine and the program characteristics to determine different performance metrics such as estimated execution time, estimated communication and computation times. The authors introduce a system abstraction methodology to characterize the behavior of the target system in a hierarchical fashion.
Reference: [Phi95] <author> M. Philippsen. </author> <title> Automatic alignment of array data and processes to reduce communication time on DMPPs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Bar-bara, CA, </address> <month> July </month> <year> 1995. </year> <month> 115 </month>
Reference-contexts: The identification of program segments in which data can be statically mapped, and the accurate modeling of the potential remapping costs make the dynamic data mapping problem harder than the static problem. The smallest possible statically mapped program regions may be single statements <ref> [KN90, CGST93, Phi95] </ref>, loop nests [AL93, LT93, AGG + 94, NDG95, PB95], or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable [SSP + 95]. <p> Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [Fea94] and Ning, Govindarajan, Altman and Gao [NG93, AG94, AGG95]. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen <ref> [Phi95] </ref> and Garcia, Ayguade and Labarta [GAL95]. The latter two works have been based on our experience with 0-1 integer programming for efficient solutions of NP-complete problems in an automatic data layout tool [BKK94, KK95].
Reference: [PIJJ88] <author> D.J. Pease, R. Inamdar, A. Joshi, and S. Jejurikar. </author> <title> Predicting the performance of a scalar program converted to execute on a vector processor. </title> <booktitle> In Proceedings of the 3rd ACM SIGSoft/SIGPlan Conference on Parallel Processing, </booktitle> <pages> pages 355-361, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: These different application scenarios impose different requirements on the performance models, in particular with respect to the precision of the generated estimates. Performance models that support benefit analysis can be used to "recruit" new users for a specific parallel architecture <ref> [PIJJ88] </ref> or to predict the scalability of a parallel application under varying machine and problem parameters [Tol95]. Decisions to improve program performance require knowledge about the performance characteristics of the program.
Reference: [PR91] <author> M. Padberg and G. Rinaldi. </author> <title> A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 60-100, </pages> <year> 1991. </year>
Reference-contexts: The second major development is in so-called cutting-plane technology. Motivated by work of Dantzig, Johnson and Fulkerson in the 50's [DFJ54], Padberg, Groetschel and others have shown how cutting-plane techniques could be used to strengthen the linear programming relaxations of many 0-1 integer programming problems <ref> [PR91] </ref>. The strengthening is effected by studying the facets of the underlying polytope generated by the convex hull of 0-1 solutions. Knowledge of these facets leads to subroutines for recognizing inequalities violated by the current fractional solution.
Reference: [PSCB94] <author> D. Palermo, E. Su, J. A. Chandy, and P. Banerjee. </author> <title> Communication optimizations used in the PARADIGM compiler for distributed-memory multi-computers. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Performance prediction of a pipelined phase execution is more complicated than in the loosely synchronous case. Execution models that can estimate pipelines of different granularity have been discussed in the literature, for instance in <ref> [MCAK94, PSCB94] </ref>. The actual choice of a particular pipeline model will depend on the desired accuracy. A detailed discussion of the execution model used in the prototype implementation of our layout assistant tool can be found in Section 5.2.1.
Reference: [Pug91] <author> W. Pugh. </author> <title> The Omega test: A fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albu-querque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Pugh developed a dependence analysis test, called the Omega Test based on an integer programming algorithm <ref> [Pug91] </ref>. Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [Fea94] and Ning, Govindarajan, Altman and Gao [NG93, AG94, AGG95]. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [Phi95] and Garcia, Ayguade and Labarta [GAL95].
Reference: [Ree94] <author> Daniel A. Reed. </author> <title> Experimental Performance Analysis of Parallel Systems: Techniques and Open Problems. </title> <editor> In Gunter Haring and Gabriele Kotsis, editors, </editor> <booktitle> Proceedings of the 7th International Conference on Modelling Techniques and Tools for Computer Performance Evaluation, </booktitle> <pages> pages 25-51. </pages> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Decisions to improve program performance require knowledge about the performance characteristics of the program. Interactive programming tools may use dynamic profiling and/or static performance estimation to identify code segments critical for the overall program performance <ref> [AMCA + 95, MCAK94, HKTW94, Ree94] </ref>. This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots".
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Early compilation approaches first inserted so-called run-time resolution code for each array reference into the program. When executed, this code performs the necessary communication for a single array reference. Based on the run-time resolution program, the compiler tried to optimize the communication through source-level transformations <ref> [ZBG88, CK88, RP89] </ref>. This early compilation method is hard to simulate efficiently. We have developed a new compilation approach that allows the analysis and optimization of communication at an abstract level without source-level transformations.
Reference: [RS89] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicomputers and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: entire program is considered a simpler problem than allowing dynamic changes of the data mapping. 3.1.1 Static Data Mappings Some early work on static data and computation mapping used data dependence information to determine whether a communication-free partitioning of the iteration 13 space of a single loop nests was possible <ref> [RS89, D'H89, HA90] </ref>. The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem.
Reference: [SSP + 95] <author> T. J. She*er, R. Schreiber, W. Pugh, J. R. Gilbert, and S. Chatterjee. </author> <title> Efficient distribution analysis via graph contraction. In Proceedings of the Workshop on Automatic Data Layout and Performance Prediction (AP'95), </title> <address> Houston, TX, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: The smallest possible statically mapped program regions may be single statements [KN90, CGST93, Phi95], loop nests [AL93, LT93, AGG + 94, NDG95, PB95], or groups of statements or loop nests for which it can be shown that remapping between them can never be profitable <ref> [SSP + 95] </ref>. The approaches for dynamic data mappings differ in the set of static mappings considered for each program region and in the techniques used to perform the final selection among the considered candidate mappings. <p> Other strategies for identifying program phases are a topic of future research. For instance, two adjacent phases can be merged into a single phase if remapping can 31 never be profitable between them. She*er et al. describe techniques to perform such phase merges <ref> [SSP + 95] </ref>. Transformations to improve phase recognition are beyond the scope of this thesis. The phase structure of the program is represented in the phase control flow graph (PCFG), an augmented control flow graph [ASU86] where each phase is represented by a single node, called a phase node.
Reference: [TMC89] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, version 5.2-0.6 edition, </note> <month> September </month> <year> 1989. </year> <month> 116 </month>
Reference-contexts: Instead, the intent is to provide decompositions that are both powerful enough to express data parallelism in scientific programs, and simple enough to permit the compiler to produce efficient programs. The design of HPF has been influenced by languages such as Fortran D [FHK + 90], CM Fortran <ref> [TMC89] </ref>, Kali [KM91], and Vienna Fortran [CMZ92]. 2.2 0-1 Integer Programming Integer programming can be used to solve many real world problems that require the management and efficient use of scarce resources to improve productivity.
Reference: [Tol95] <author> S. Toledo. PERFSIM: </author> <title> A tool for automatic performance analysis of data-parallel Fortran programs. </title> <booktitle> In Frontiers95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Performance models that support benefit analysis can be used to "recruit" new users for a specific parallel architecture [PIJJ88] or to predict the scalability of a parallel application under varying machine and problem parameters <ref> [Tol95] </ref>. Decisions to improve program performance require knowledge about the performance characteristics of the program. Interactive programming tools may use dynamic profiling and/or static performance estimation to identify code segments critical for the overall program performance [AMCA + 95, MCAK94, HKTW94, Ree94].
Reference: [Tse93] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> January </month> <year> 1993. </year> <institution> Rice COMP TR93-199. </institution>
Reference-contexts: Our sample heuristic to determine the CAG edge weights assumes an advanced compilation system that performs communication value caching and uses the owner-computes rule for computation mapping <ref> [Tse93] </ref>. The target machine is assumed to be a MIMD target architecture. Our sample performance model is pessimistic since it assumes that unsatisfied alignment preferences will always lead to communication. <p> A similar algorithm has been proposed by Gerndt [Ger90]. Tseng extended our basic compilation approach and showed that it is flexible enough to accommodate other communication optimizations such as message aggregation, message coalescing, and coarse-grain pipelining <ref> [Tse93] </ref>. He also showed that in practice message vectorization is a highly effective communication optimization. Execution Model Once locations and types of compiler generated communications are known for a candidate layout and its phase, an execution model is used to estimate the performance effects of synchronizations induced by the communications. <p> The proposed data flow problem is similar to the reaching definitions [ASU86] and reaching decomposition problems <ref> [Tse93] </ref>. <p> The corresponding data layout graphs with different weights were generated by hand. Weights were chosen to model different communication costs and the presence or absence of compiler optimizations. For instance, a compiler may be able to generate a coarse-grain pipelined loop if the data layout induces cross-processor dependences <ref> [Tse93] </ref>. Whether the compiler performs such an optimization or not is represented by different weights of the nodes and edges in the DLG. 83 We wrote a tool that generates the three different 0-1 problem formulations discussed in Section 4.5.5 for an input DLG. <p> Distribution analysis generates exhaustive search spaces of only one-dimensional BLOCK distributions. This restriction is due to the fact that the compiler model implementation mimics the program analysis steps in the Fortran D prototype compiler which does not support multi-dimensional distribution <ref> [Tse93] </ref>. Note that since the current prototype generates exhaustive search spaces for one-dimensional BLOCK distributions, the orientation selection is trivial due to the symmetry between orientations and distribution candidates. <p> To perform the comparison, each program was compiled for each data layout in its set of promising data layouts using the Fortran D compiler prototype <ref> [Tse93] </ref> with loop interchange and coarse-grain pipelining disabled. When necessary, the output of the Fortran D compiler was modified by hand to ensure correct code. The resulting SPMD node programs were compiled using the highest optimization level (if77 -O4), and executed and timed on the iPSC/860.
Reference: [Wan93] <author> K-Y. Wang. </author> <title> A framework for static, precise performance prediction for superscalar-based parallel computers. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: The resulting performance estimates have to be precise enough to distinguish the data layouts considered by the corresponding automatic data layout approach. IBM's Static Performance Estimation for High-Level Languages Wang has developed a framework for static performance prediction for high-level languages such as HPF running on superscalar-based, parallel machines <ref> [Wan93, Wan94] </ref>. 25 The goal of the performance estimation is to support the selection of optimizing transformations for different target machines. In a first step, the framework maps high-level language statements into low-level machine instructions.
Reference: [Wan94] <author> K-Y. Wang. </author> <title> Precise compile-time performance prediction for superscalar-based computers. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots". Compilers use static performance models to predict the performance benefits of program transformations including automatic data layout <ref> [GB92a, WL91, CMT94, CGST93, Wan94] </ref>. Different transformations require different performance estimation accuracy. In this section we will focus on a discussion of performance models that can be used to support automatic data layout at the program source code level. <p> The resulting performance estimates have to be precise enough to distinguish the data layouts considered by the corresponding automatic data layout approach. IBM's Static Performance Estimation for High-Level Languages Wang has developed a framework for static performance prediction for high-level languages such as HPF running on superscalar-based, parallel machines <ref> [Wan93, Wan94] </ref>. 25 The goal of the performance estimation is to support the selection of optimizing transformations for different target machines. In a first step, the framework maps high-level language statements into low-level machine instructions.
Reference: [Wei91] <author> M. Weiss. </author> <title> Strip mining on SIMD architectures. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem. The decomposition of the data mapping into alignment and distribution lead to a two step strategy, where alignment analysis is followed by distribution analysis <ref> [AKLS88, Wei91, LC90a, LC91b, GB91, GB92b, Who92a, CGST93] </ref>. Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. <p> Compass Albert, Knobe, Lukas, Natarajan, Steele, and Weiss discuss automatic data layout as part of the design and implementation at Compass of SIMD compilers for Fortran 77 extended by Fortran 8x array features <ref> [AKLS88, KLS88, KLS90, KN90, Wei91] </ref>. The target machines are the Connection Machine CM-2 and the MasPar MP-1. Automatic data layout is an integral part of these compilers. Arrays are aligned by mapping them onto virtual processors based on their usage as opposed to a their declared shape. <p> Since the CM-2 supports the concept of virtual processors through its programming environment, data alignment is sufficient to specify the data layout. In contrast, the MasPar machine does not support virtual processors. The virtual processors have to be mapped onto the physical processors explicitly <ref> [Wei91] </ref>. The alignment algorithm is based on the usage patterns of arrays and Fortran 8x array sections in the source program. Each pattern generates allocation requests, called preferences, that indicate the optimal layout of the arrays relative to each other [KLS88, KLS90]. <p> For the MasPar machine, the data allocation functions generated by the data optimization component have to be transformed from mappings based on virtual processors to mappings based on physical processors. Weiss discusses three distribution schemes, namely cyclic (horizontal), block (vertical), and block-cyclic distributions <ref> [Wei91] </ref>. Most of the described work has been implemented as part of the CM-2 and MP-1 Fortran compilers developed at Compass. The authors report significant performance improvements of up to a factor of 60 due to using the compiler generated data mapping instead of the naive, canonical mapping.
Reference: [Who91] <author> S. Wholey. </author> <title> Automatic Data Mapping for Distributed-Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: A distributed memory compiler is not part of Parafrase-2. Actual performance figures for the generated data layout schemes are only given for tred2 on an iPSC/2 hypercube system. The automatic layout performs well compared to three other data mappings. ALEXI Wholey at Carnegie Mellon University <ref> [Who92a, Who91] </ref> developed a compiler for the high-level, block structured, non-recursive language ALEXI. Communication and parallelism is expressed explicitly by primitive operations that are similar to Fortran90 array constructs and intrinsic communication functions.
Reference: [Who92a] <author> S. Wholey. </author> <title> Automatic data mapping for distributed-memory parallel computers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The communication-free computation mapping induces the corresponding data mapping. The majority of approaches emphasize the data mapping view of the mapping problem. The decomposition of the data mapping into alignment and distribution lead to a two step strategy, where alignment analysis is followed by distribution analysis <ref> [AKLS88, Wei91, LC90a, LC91b, GB91, GB92b, Who92a, CGST93] </ref>. Since alignment and distribution decisions may be mutually dependent, performing distribution after alignment may lead to inferior results. To avoid this problem, some approaches consider alignment and distribution at the same time [Ke93, GAL95]. <p> A distributed memory compiler is not part of Parafrase-2. Actual performance figures for the generated data layout schemes are only given for tred2 on an iPSC/2 hypercube system. The automatic layout performs well compared to three other data mappings. ALEXI Wholey at Carnegie Mellon University <ref> [Who92a, Who91] </ref> developed a compiler for the high-level, block structured, non-recursive language ALEXI. Communication and parallelism is expressed explicitly by primitive operations that are similar to Fortran90 array constructs and intrinsic communication functions.
Reference: [Who92b] <author> S. Wholey. </author> <title> Private communication. </title> <year> 1992. </year>
Reference-contexts: The performance of automatically determined data layouts has not been compared with the best data layout possible <ref> [Who92b] </ref>. The precision of the performance estimation technique in terms of the relative performance of different data layouts is demonstrated by comparing the estimated costs with the actual execution times of a simplex program on a CM-2.
Reference: [WL91] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: This information can be used to guide the user's program tuning efforts or to direct more aggressive compile-time analysis and code improvement techniques to the identified performance "hot spots". Compilers use static performance models to predict the performance benefits of program transformations including automatic data layout <ref> [GB92a, WL91, CMT94, CGST93, Wan94] </ref>. Different transformations require different performance estimation accuracy. In this section we will focus on a discussion of performance models that can be used to support automatic data layout at the program source code level.
Reference: [Wol89] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The approach is based on data access summary information such as data access descriptors (DAD [Bal90]) or regular section descriptors (RSD [Cal87]) to determine what data has to be communicated, and on the dependence graph to determine when the data can be communicated <ref> [AK87, Wol89] </ref>. Figure 4.13 shows the message vec-torization algorithm that moves communication out of loop nests as far as possible [BFKK90]. A similar algorithm has been proposed by Gerndt [Ger90].
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 117 </month>
Reference-contexts: Early compilation approaches first inserted so-called run-time resolution code for each array reference into the program. When executed, this code performs the necessary communication for a single array reference. Based on the run-time resolution program, the compiler tried to optimize the communication through source-level transformations <ref> [ZBG88, CK88, RP89] </ref>. This early compilation method is hard to simulate efficiently. We have developed a new compilation approach that allows the analysis and optimization of communication at an abstract level without source-level transformations.
References-found: 99

