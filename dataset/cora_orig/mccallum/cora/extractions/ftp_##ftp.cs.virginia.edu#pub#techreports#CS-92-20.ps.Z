URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-92-20.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Note: This research was supported in part by NSF Grant #CCR-9108448 and JPL Contract #957721.  
Abstract: Design and Performance Analysis of Hardware Support for Parallel Simulations Paul F. Reynolds, Jr.,Carmen M. Pancerella,Sudhir Srinivasan Computer Science Report No. CS-92-20 June 10, 1992 
Abstract-found: 1
Intro-found: 1
Reference: [AbRi91] <author> Abrams, M. and Richardson, D., </author> <title> "Implementing a Global Termination Condition and Collecting Output Measures in Parallel Simulation", </title> <booktitle> Proceedings of the SCS Multiconference on Advances in Parallel and Distributed Simulation, </booktitle> <address> Anaheim, California, </address> <pages> pp. 86-91, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: Iterative PDES protocols, those which select a computation window in a loosely synchronous manner [Ayan89, ChSh89, Luba88, Nico91, SoBW88], can benefit 1 from hardware support for the computation of the window size. Termination detection and global consensus <ref> [AbRi91] </ref> are other capabilities that can benefit PDES. All of these activities can be carried out in the framework we describe here. <p> In addition to enhanced fossil collection, GVT is essential to committing to irreversible acts such as I/O and to detecting termination conditions <ref> [AbRi91] </ref>. We note that efficient GVT computation is most beneficial to a parallel simulation with a limited amount of memory. In a system with sufficient state memory, fossil collection is not as critical.
Reference: [Ayan89] <author> Ayani, R., </author> <title> "A Parallel Simulation Scheme Based on Distances Between Objects", </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> Tampa, Florida, </address> <pages> pp. 113-118, </pages> <month> (March </month> <year> 1989). </year>
Reference-contexts: We have established elsewhere [RePa92] that such a network could support the rapid dissemination of values used in lookahead computations. Similarly, values reflecting processing rates could be disseminated rapidly for adaptive PDES protocols. Iterative PDES protocols, those which select a computation window in a loosely synchronous manner <ref> [Ayan89, ChSh89, Luba88, Nico91, SoBW88] </ref>, can benefit 1 from hardware support for the computation of the window size. Termination detection and global consensus [AbRi91] are other capabilities that can benefit PDES. All of these activities can be carried out in the framework we describe here.
Reference: [Bell90] <author> Bellenot, S., </author> <title> "Global Virtual Time Algorithms", </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 122-127, </pages> <month> (January </month> <year> 1990). </year>
Reference-contexts: Details of the algorithms executed on host and auxiliary processors appear in [Srin92]. 19 One of the significant challenges facing aggressive protocols is the rapid dissemination of global virtual time (GVT). (See [Jeff85], [JeSo85], [Sama85], [LiLa89], <ref> [Bell90] </ref>, and [CoKe91]) In an aggressive parallel simulation, GVT is the simulation safe time, the guaranteed time for which all events with timestamps less than or equal to it have been processed accurately.
Reference: [BuRo90] <author> Buzzell, C. A. and Robb, M. J., </author> <title> "Modular VME Rollback Hardware for Time Warp", </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 153-156, </pages> <month> (January </month> <year> 1990). </year>
Reference-contexts: In a system with sufficient state memory, fossil collection is not as critical. Fujimoto has designed the rollback chip [FuTG92], a memory management unit that offloads state saving and state restoration inherent in aggressive PDES protocols. As reported in <ref> [BuRo90] </ref> the chip has excellent performance capabilities. However, the chip has one major limitation for which efficient GVT computation can compensate: limited capacity. Efficient GVT computation minimizes the unnecessary state information that must be stored in the rollback chip.
Reference: [ChSh89] <author> Chandy, K. M. and Sherman, R., </author> <title> "The Conditional Event Approach to Distributed Simulation", </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> Tampa, Florida, </address> <pages> pp. 93-99, </pages> <month> (March </month> <year> 1989). </year>
Reference-contexts: We have established elsewhere [RePa92] that such a network could support the rapid dissemination of values used in lookahead computations. Similarly, values reflecting processing rates could be disseminated rapidly for adaptive PDES protocols. Iterative PDES protocols, those which select a computation window in a loosely synchronous manner <ref> [Ayan89, ChSh89, Luba88, Nico91, SoBW88] </ref>, can benefit 1 from hardware support for the computation of the window size. Termination detection and global consensus [AbRi91] are other capabilities that can benefit PDES. All of these activities can be carried out in the framework we describe here.
Reference: [CoKe91] <author> Concepcion, A. I. and Kelly, S. G., </author> <title> "Computing Global Virtual Time Using the Multi-Level Token Passing Algorithm", </title> <booktitle> Proceedings of the SCS Multiconference on Advances in Parallel and Distributed Simulation, </booktitle> <address> Anaheim, California, </address> <pages> pp. 63-68, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: Details of the algorithms executed on host and auxiliary processors appear in [Srin92]. 19 One of the significant challenges facing aggressive protocols is the rapid dissemination of global virtual time (GVT). (See [Jeff85], [JeSo85], [Sama85], [LiLa89], [Bell90], and <ref> [CoKe91] </ref>) In an aggressive parallel simulation, GVT is the simulation safe time, the guaranteed time for which all events with timestamps less than or equal to it have been processed accurately.
Reference: [CrKn85] <author> Crockett, T. W. and Knott, J. D., </author> <title> "System Software for the Finite Element Machine", </title> <type> NASA Contractor Report 3870, </type> <institution> NASA Langley, Hampton, Virginia, </institution> <month> February </month> <year> 1985. </year> <month> 33 </month>
Reference-contexts: Lubachevsky [Luba88] suggests using a binary tree implemented in hardware in order to support synchronization barriers and to compute and broadcast a minimum next event time in a bounded lag PDES. His control synchronization network is presented strictly in support of this PDES protocol. The Finite Element Machine <ref> [CrKn85, JoSc79] </ref>, a NASA prototype, utilizes a binary tree-structured max/summation network to perform the global sum and maximum calculations necessary to support structural analysis algorithms. Like the hardware we propose, the sum and max calculations in the FEM are calculated alternately without processor synchronization.
Reference: [DiRe92] <author> Dickens, P. M. and Reynolds Jr., P. F., </author> <title> "State Saving and Rollback Costs for an Aggressive Global Windowing Algorithm ", Computer Science Report No. </title> <type> Technical Report-92-18, </type> <institution> Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Correctness Criteria Numerous examples have appeared in the parallel simulation literature demonstrating the need for globally reduced values. Lubachevsky [Luba89] requires global establishment of minimum next event time and other values to compute opaque periods. Global windowing algorithms, such as those proposed by Nicol [Nico91] and Dickens 2 <ref> [DiRe92] </ref> require establishment of parameters for the window. Reynolds has demonstrated the utility of values such as global minimum next event time, minimum unreceived message times and others in [Reyn92]. The hardware-based framework we describe in this paper is meant to provide these values correctly and expediently.
Reference: [FiGP91] <author> Filoque, J. M., Gautrin, E. and Pottier, B., </author> <title> "Efficient Global Computations on a Processors Network with Programmable Logic", </title> <type> Report 1374, </type> <institution> Institut National de Recherche en Informatique et en Anutomatique, France, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Our hardware design, however, employs a set of input and output registers which are treated as a single state vector, whereas the FEM uses a single input and a single output register. At about the same time that we introduced our framework, Filoque, et.al., <ref> [FiGP91] </ref> proposed the use of a processor network with programmable logic for efficient global computations, such as the computation of GVT in a Time Warp simulation. This hardware is not a single network like the PRN; it is, however, a distributed system of sockets, one per processor. <p> When the token returns to the controller, the global computation is complete. Therefore, their proposed hardware performs global computations in O (n) time whereas the PRN performs the same computations in O (log n) time. Furthermore, the proposed synchronization algorithms for computing GVT in <ref> [FiGP91] </ref> rely on the host communication network for message acknowledgements and our framework uses the framework hardware for this purpose. The goals of both approaches are similar, but our framework is more efficient, more flexible, and more scalable. Several researchers have proposed the use of hardware to implement barrier synchronization.
Reference: [Fuji90] <author> Fujimoto, R. M., </author> <title> "Parallel Discrete Event Simulation", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 33, No. 10, </volume> <pages> pp. 30-53, </pages> <month> (October </month> <year> 1990). </year>
Reference-contexts: Ensuring the observance of causality constraints is typically left to a "PDES protocol." An excellent survey by Fujimoto <ref> [Fuji90] </ref> explores PDES issues and presents an overview of a wide range of protocols. Our concern here is with respect to efficient support for PDES synchronization protocols. A "PDES framework," a mix of hardware and software designed to provide that support has been introduced in [Reyn91].
Reference: [FuTG92] <author> Fujimoto, R. M., Tsai, J. J. and Gopalakrishnan, G. C., </author> <title> "Design and Evaluation of the Rollback Chip: Special Purpose Hardware for Time Warp", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 41, No. 1, </volume> <pages> pp. 68-82, </pages> <month> (January </month> <year> 1992). </year>
Reference-contexts: For example, without support hardware, state saving and rollback costs in aggressive PDES protocols can overwhelm benefits gained from aggressive processing. Fujimoto has developed the state saving and rollback chip to reduce this substantial cost <ref> [FuTG92] </ref>. Similarly, Reynolds [Reyn92] has established the need for high-speed computation of critical values such as global virtual time (GVT), particularly in support of Fujimoto's rollback chip. In this paper we describe hardware to efficiently compute global reductions, as characterized by the computation of GVT. <p> We note that efficient GVT computation is most beneficial to a parallel simulation with a limited amount of memory. In a system with sufficient state memory, fossil collection is not as critical. Fujimoto has designed the rollback chip <ref> [FuTG92] </ref>, a memory management unit that offloads state saving and state restoration inherent in aggressive PDES protocols. As reported in [BuRo90] the chip has excellent performance capabilities. However, the chip has one major limitation for which efficient GVT computation can compensate: limited capacity.
Reference: [Hosh85] <author> Hoshino, T., </author> <title> PAX Computer: High-Speed Parallel Processing and Scientific Computing, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: The goals of both approaches are similar, but our framework is more efficient, more flexible, and more scalable. Several researchers have proposed the use of hardware to implement barrier synchronization. Hoshino <ref> [Hosh85] </ref> has an efficient barrier synchronization in the PAX computer. Stone [Ston90] suggests the use of global busses to compute maximum values and to implement fetch-and-increment. The hardware that we propose, on the other hand, provides support for a larger class of algorithms than barrier synchronization algorithms.
Reference: [Inte89] <author> Intel Corporation, </author> <title> iPSC/2 Programmer's Reference Manual, Intel Scientific Computers, </title> <institution> Beaverton, Oregon, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: The hardware that we propose, on the other hand, provides support for a larger class of algorithms than barrier synchronization algorithms. Many parallel architectures provide for global binary, associative operations across all processors. Global operations on the Intel iPSC/2 <ref> [Inte89] </ref> are provided for arithmetic and logical operations. The Thinking Machines CM-5 [Thin92] contains two separate networks for different types of communication and synchronization: the data network is the primary message-passing network in the machine and the control network provides hardware support for common cooperative operations.
Reference: [Jeff85] <author> Jefferson, D. R., </author> <title> "Virtual Time", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 7, No. 3, </volume> <pages> pp. 404-425, </pages> <month> (July </month> <year> 1985). </year>
Reference-contexts: Details of the algorithms executed on host and auxiliary processors appear in [Srin92]. 19 One of the significant challenges facing aggressive protocols is the rapid dissemination of global virtual time (GVT). (See <ref> [Jeff85] </ref>, [JeSo85], [Sama85], [LiLa89], [Bell90], and [CoKe91]) In an aggressive parallel simulation, GVT is the simulation safe time, the guaranteed time for which all events with timestamps less than or equal to it have been processed accurately. <p> By definition, GVT is the minimum of two values: the minimum of all local clocks and the minimum of the timestamps of all transient messages <ref> [Jeff85] </ref>. A simulation cannot roll back to a time earlier than GVT. This is important for efficient memory management, which is integral to an aggressive PDES protocol; fossil collection can be performed on any state which is older than GVT.
Reference: [JeSo85] <author> Jefferson, D. and Sowizral, H., </author> <title> "Fast Concurrent Simulation Using the Time Warp Mechanism", </title> <booktitle> Proceedings of the Conference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 63-69, </pages> <month> (January </month> <year> 1985). </year>
Reference-contexts: Details of the algorithms executed on host and auxiliary processors appear in [Srin92]. 19 One of the significant challenges facing aggressive protocols is the rapid dissemination of global virtual time (GVT). (See [Jeff85], <ref> [JeSo85] </ref>, [Sama85], [LiLa89], [Bell90], and [CoKe91]) In an aggressive parallel simulation, GVT is the simulation safe time, the guaranteed time for which all events with timestamps less than or equal to it have been processed accurately.
Reference: [Jeff90] <author> Jefferson, D. R., </author> <title> "Virtual Time II: Storage Management in Distributed Simulation", </title> <booktitle> Proceedings of the Ninth Annual Symposium on Principles of Distributed Computing, </booktitle> <address> Quebec City, Quebec, Canada, </address> <pages> pp. 75-89, </pages> <month> (August </month> <year> 1990). </year>
Reference-contexts: This is necessary if, for example, in the course of state saving, an LP's rollback chip's high-speed memory becomes full. If that LP stops processing aggressively, the risk of deadlock in the simulation is incurred. Saving state to a secondary store has obvious costs. Jefferson's "cancelback" mechanism <ref> [Jeff90] </ref> requires additional processing and message costs. In contrast, our framework is sufficient to avoid deadlock in a parallel simulation [Reyn92] without overhead from host processors or the host network. The LP whose local simulation time is equal to GVT must process its events in order to prevent deadlock.
Reference: [JoSc79] <author> Jordan, H. F., Scalabrin, M. and Calvert, W., </author> <title> "A Comparison of Three Types of Multiprocessor Algorithms", </title> <booktitle> Proceedings of the 1979 International Conference on Parallel Processing, </booktitle> <pages> pp. 231-238, </pages> <month> (August </month> <year> 1979). </year>
Reference-contexts: Lubachevsky [Luba88] suggests using a binary tree implemented in hardware in order to support synchronization barriers and to compute and broadcast a minimum next event time in a bounded lag PDES. His control synchronization network is presented strictly in support of this PDES protocol. The Finite Element Machine <ref> [CrKn85, JoSc79] </ref>, a NASA prototype, utilizes a binary tree-structured max/summation network to perform the global sum and maximum calculations necessary to support structural analysis algorithms. Like the hardware we propose, the sum and max calculations in the FEM are calculated alternately without processor synchronization.
Reference: [LiLa89] <author> Lin, Y. B. and Lazowska, E. D., </author> <title> "Determining the Global Virtual Time in a Distributed Simulation", </title> <type> Technical Report 90-01-02, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, Washington, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Details of the algorithms executed on host and auxiliary processors appear in [Srin92]. 19 One of the significant challenges facing aggressive protocols is the rapid dissemination of global virtual time (GVT). (See [Jeff85], [JeSo85], [Sama85], <ref> [LiLa89] </ref>, [Bell90], and [CoKe91]) In an aggressive parallel simulation, GVT is the simulation safe time, the guaranteed time for which all events with timestamps less than or equal to it have been processed accurately.
Reference: [Luba88] <author> Lubachevsky, B. D., </author> <title> "Bounded Lag Distributed Discrete Event Simulation", </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 183-191, </pages> <month> (February </month> <year> 1988). </year>
Reference-contexts: We have established elsewhere [RePa92] that such a network could support the rapid dissemination of values used in lookahead computations. Similarly, values reflecting processing rates could be disseminated rapidly for adaptive PDES protocols. Iterative PDES protocols, those which select a computation window in a loosely synchronous manner <ref> [Ayan89, ChSh89, Luba88, Nico91, SoBW88] </ref>, can benefit 1 from hardware support for the computation of the window size. Termination detection and global consensus [AbRi91] are other capabilities that can benefit PDES. All of these activities can be carried out in the framework we describe here. <p> Our reduction network is not as complex or expensive as a combining network, yet it performs global synchronization operations very efficiently. We claim no novelty with respect to reduction networks. Lubachevsky <ref> [Luba88] </ref> suggests using a binary tree implemented in hardware in order to support synchronization barriers and to compute and broadcast a minimum next event time in a bounded lag PDES. His control synchronization network is presented strictly in support of this PDES protocol.
Reference: [Luba89] <author> Lubachevsky, B. D., </author> <title> "Efficient Distributed Event-Driven Simulations of Multiple-Loop Networks", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 32, No. 1, </volume> <pages> pp. 111-123, </pages> <month> (January </month> <year> 1989). </year>
Reference-contexts: We close with an evaluation of the expected performance of the framework we describe. 2. Correctness Criteria Numerous examples have appeared in the parallel simulation literature demonstrating the need for globally reduced values. Lubachevsky <ref> [Luba89] </ref> requires global establishment of minimum next event time and other values to compute opaque periods. Global windowing algorithms, such as those proposed by Nicol [Nico91] and Dickens 2 [DiRe92] require establishment of parameters for the window.
Reference: [LuWS91] <author> Lubachevsky, B., Weiss, A. and Shwartz, A., </author> <title> "An Analysis of Rollback-Based Simulation", </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> Vol. 1, No. 2, </volume> <pages> pp. 154-193, </pages> <month> (April </month> <year> 1991). </year>
Reference-contexts: It has been suggested that a limited aggressive PDES protocol, or adaptive protocol, one which is an intelligent combination of aggressive and non-aggressive protocols, may be more powerful than either in a pure form <ref> [LuWS91, Reyn88, Reyn92] </ref>. Adaptive protocols, however, have been neglected because of the costs associated with gathering and using synchronization information for adaptive decisions. In Section 7, our 20 performance results show that global synchronization information can be efficiently reduced and disseminated using the framework hardware. 6.1.
Reference: [Nico91] <author> Nicol, D. M., </author> <title> "The Cost of Conservative Synchronization in Parallel Discrete Event Simulation", </title> <note> to appear in Journal of the ACM, (1991). 34 </note>
Reference-contexts: We have established elsewhere [RePa92] that such a network could support the rapid dissemination of values used in lookahead computations. Similarly, values reflecting processing rates could be disseminated rapidly for adaptive PDES protocols. Iterative PDES protocols, those which select a computation window in a loosely synchronous manner <ref> [Ayan89, ChSh89, Luba88, Nico91, SoBW88] </ref>, can benefit 1 from hardware support for the computation of the window size. Termination detection and global consensus [AbRi91] are other capabilities that can benefit PDES. All of these activities can be carried out in the framework we describe here. <p> Correctness Criteria Numerous examples have appeared in the parallel simulation literature demonstrating the need for globally reduced values. Lubachevsky [Luba89] requires global establishment of minimum next event time and other values to compute opaque periods. Global windowing algorithms, such as those proposed by Nicol <ref> [Nico91] </ref> and Dickens 2 [DiRe92] require establishment of parameters for the window. Reynolds has demonstrated the utility of values such as global minimum next event time, minimum unreceived message times and others in [Reyn92].
Reference: [Panc92] <author> Pancerella, C. M., </author> <title> "Improving the Efficiency of a Framework for Parallel Simulations", </title> <booktitle> Proceedings of the 1992 Western Simulation MultiConference on Parallel and Distributed Simulation, </booktitle> <address> Newport Beach, California, </address> <pages> pp. 22-29, </pages> <month> (January </month> <year> 1992). </year>
Reference-contexts: Employing auxiliary processors provides a separation of the synchronization activity (performed on auxiliary processors) and the application being simulated (performed on host processors). High speed synchronization activity in the parallel simulation framework (See <ref> [Panc92] </ref> and [Srin92]) is performed on the dedicated AP's. The logical processes (LP's) execute on the host processors, and all event messages are sent and received at the host 8 processors. <p> Auxiliary Processor The general layout of an auxiliary processor is depicted in Figure 2. Auxiliary processors are fast, general purpose 32-bit microprocessors. Each AP has its own memory to store synchronization programs and related data structures (See [Reyn92], <ref> [Panc92] </ref>, and [Srin92]). Furthermore, each AP has EPROM to store a boot-up monitor which is executed upon reset. 4.2. Setup Each auxiliary processor boots up in a "listening" state in which it monitors its host processor interface. <p> An alternative, which is the equivalent of sequential consistency, is to use two extra input registers and compute tagged selective operations to perform a double 17 handshake <ref> [Panc92] </ref>, as discussed in Section 6. We note, however, that it is expensive (in terms of computation time) to implement this property in the framework hardware and it should be avoided when possible. 5. <p> Message acknowledgements are needed to determine when it is safe to assume that a message is no longer in the host network. It is undesirable to acknowledge messages in the host network because the message traffic doubles. In <ref> [Panc92] </ref> a scheme for performing message acknowledgements with a double handshake in the reduction network was presented. We now discuss this approach. As mentioned in the previous section, a host processor communicates all messages sent or received to its auxiliary processor. <p> The framework hardware, however, allows state vector loss at the output of the PRN. Hence, an LP may never see an acknowledgement, and its minimum outstanding message time will never advance. <ref> [Panc92] </ref> proposes a double handshake to guarantee that all acknowledgements are seen. In this scheme, a second tagged selective operation is used to "acknowledge the acknowledgement". In other words, the receiver cannot remove its acknowledgement until the sender echoes it using this second globally reduced value in the state vector. <p> In order to optimize this, a contiguous batch of messages from the same sender can be acknowledged as sequences by acknowledging the message ID of the largest in-sequence message; the smallest logical timestamp of the message sequence is used as an AP's input to the minimum operation <ref> [Panc92] </ref>. We now present simulation results which demonstrate the performance gains of using the framework hardware to acknowledge messages and to compute GVT. 7. Performance In this section, we present the results of simulations conducted to evaluate the performance of the framework hardware described earlier in this paper.
Reference: [PfBG85] <author> Pfister, G. F., Brantley, W. C., George, D. A., Harvey, S. L., Kleinfelder, W. J., McAuliffe, K. P., Melton, E. A., Norton, V. A. and Weiss, J., </author> <title> "The IBM Research Parallel Prototype (RP3): Introduction and Architecture", </title> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Illinois, </publisher> <pages> pp. 764-771, </pages> <month> (August </month> <year> 1985). </year>
Reference-contexts: Related Work Using a separate synchronization network for improving system performance is not a new idea. The IBM RP3 <ref> [PfBG85] </ref> was designed as a shared memory multiprocessor that houses both a combining network for synchronization traffic and a low latency network for regular message traffic. Our reduction network is not as complex or expensive as a combining network, yet it performs global synchronization operations very efficiently.
Reference: [RaBJ88] <author> Ranade, A. G., Bhatt, S. N. and Johnsson, S. L., </author> <title> "The Fluent Abstract Machine", </title> <institution> YALEU/Department of Computer Science/Technical Report-573, Department of Computer Science, Yale University, </institution> <address> New Haven, Connecticut, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: This kind of consensus can be achieved, but at a cost. The cost can be temporal: LP's must continually execute the equivalent of barrier synchronizations, or the cost can be monetary: specialized networks such as those proposed in <ref> [RaBJ88] </ref>, [ReWW89] and [ReWW92] would be required in addition to the framework we describe in this paper.
Reference: [Reyn88] <author> Reynolds Jr., P. F., </author> <title> "A Spectrum of Options for Parallel Simulation", </title> <booktitle> Proceedings of the 1988 Winter Simulation Conference, </booktitle> <address> San Diego, California, </address> <pages> pp. 325-332, </pages> <month> (December </month> <year> 1988). </year>
Reference-contexts: It has been suggested that a limited aggressive PDES protocol, or adaptive protocol, one which is an intelligent combination of aggressive and non-aggressive protocols, may be more powerful than either in a pure form <ref> [LuWS91, Reyn88, Reyn92] </ref>. Adaptive protocols, however, have been neglected because of the costs associated with gathering and using synchronization information for adaptive decisions. In Section 7, our 20 performance results show that global synchronization information can be efficiently reduced and disseminated using the framework hardware. 6.1.
Reference: [ReWW89] <author> Reynolds Jr., P. F., Williams, C. and Wagner, R. R., </author> <note> "Parallel Operations", Computer Science Report No. Technical Report-89-16, </note> <institution> Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: This kind of consensus can be achieved, but at a cost. The cost can be temporal: LP's must continually execute the equivalent of barrier synchronizations, or the cost can be monetary: specialized networks such as those proposed in [RaBJ88], <ref> [ReWW89] </ref> and [ReWW92] would be required in addition to the framework we describe in this paper.
Reference: [Reyn91] <author> Reynolds Jr., P. F., </author> <title> "An Efficient Framework for Parallel Simulations", </title> <booktitle> Proceedings of the SCS Multiconference on Advances in Parallel and Distributed Simulation, </booktitle> <address> Anaheim, California, </address> <pages> pp. 167-174, </pages> <month> (January </month> <year> 1991). </year>
Reference-contexts: Our concern here is with respect to efficient support for PDES synchronization protocols. A "PDES framework," a mix of hardware and software designed to provide that support has been introduced in <ref> [Reyn91] </ref>. The purpose of this paper is to describe this framework further, to establish criteria for its proper operation, to describe functional characteristics, to describe a nearly complete prototype, and to present results of performance studies.
Reference: [ReWW92] <author> Reynolds Jr., P. F., Williams, C. and Wagner, R. R., </author> <title> "Empirical Analysis of Isotach Networks", </title> <institution> Computer Science Report No. Technical Report-92-19, Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: This kind of consensus can be achieved, but at a cost. The cost can be temporal: LP's must continually execute the equivalent of barrier synchronizations, or the cost can be monetary: specialized networks such as those proposed in [RaBJ88], [ReWW89] and <ref> [ReWW92] </ref> would be required in addition to the framework we describe in this paper. We have chosen to weaken the desire for total ordering because doing so leads to more efficient and cost-effective solutions and because we can accomplish all we need with a simpler and less expensive approach.
Reference: [Reyn92] <author> Reynolds Jr., P. F., </author> <title> "An Efficient Framework for Parallel Simulations", </title> <note> to appear in International Journal on Computer Simulation, </note> <year> (1992). </year>
Reference-contexts: For example, without support hardware, state saving and rollback costs in aggressive PDES protocols can overwhelm benefits gained from aggressive processing. Fujimoto has developed the state saving and rollback chip to reduce this substantial cost [FuTG92]. Similarly, Reynolds <ref> [Reyn92] </ref> has established the need for high-speed computation of critical values such as global virtual time (GVT), particularly in support of Fujimoto's rollback chip. In this paper we describe hardware to efficiently compute global reductions, as characterized by the computation of GVT. <p> Global windowing algorithms, such as those proposed by Nicol [Nico91] and Dickens 2 [DiRe92] require establishment of parameters for the window. Reynolds has demonstrated the utility of values such as global minimum next event time, minimum unreceived message times and others in <ref> [Reyn92] </ref>. The hardware-based framework we describe in this paper is meant to provide these values correctly and expediently. When computing globally reduced values, it would be best to allow the computation of these values to proceed asynchronously with simulation. <p> Sequential consistency in this case does not address the ordering of operations between pairs of LP's. "Operations" in this case are the resulting global reductions that must be performed whenever an LP changes state values. Thus, if an LP changes its smallest unreceived message time (See <ref> [Reyn92] </ref>) and then changes its next event time, the corresponding global operations should be applied and completed in the same order. We note that sequential consistency does not require that the effects of the global operations be observed strictly in the order in which they were applied. <p> When an LP completes an event and sends a message to another LP, both the next event time and the smallest unreceived message time may change. A simulation-wide invariant that must be maintained, as demonstrated in <ref> [Reyn92] </ref>, is that the smallest event time in the system must always be represented in at least one LP's next event time or smallest unreceived message time. <p> Auxiliary Processor The general layout of an auxiliary processor is depicted in Figure 2. Auxiliary processors are fast, general purpose 32-bit microprocessors. Each AP has its own memory to store synchronization programs and related data structures (See <ref> [Reyn92] </ref>, [Panc92], and [Srin92]). Furthermore, each AP has EPROM to store a boot-up monitor which is executed upon reset. 4.2. Setup Each auxiliary processor boots up in a "listening" state in which it monitors its host processor interface. <p> If that LP stops processing aggressively, the risk of deadlock in the simulation is incurred. Saving state to a secondary store has obvious costs. Jefferson's "cancelback" mechanism [Jeff90] requires additional processing and message costs. In contrast, our framework is sufficient to avoid deadlock in a parallel simulation <ref> [Reyn92] </ref> without overhead from host processors or the host network. The LP whose local simulation time is equal to GVT must process its events in order to prevent deadlock. <p> It has been suggested that a limited aggressive PDES protocol, or adaptive protocol, one which is an intelligent combination of aggressive and non-aggressive protocols, may be more powerful than either in a pure form <ref> [LuWS91, Reyn88, Reyn92] </ref>. Adaptive protocols, however, have been neglected because of the costs associated with gathering and using synchronization information for adaptive decisions. In Section 7, our 20 performance results show that global synchronization information can be efficiently reduced and disseminated using the framework hardware. 6.1. <p> Thus, in this model, the HP's performed all the Time Warp specific activities such as state saving and rollback in addition to simulating the processing of dummy events. The model for the AP was also modified to execute an aggressive version of the framework algorithm <ref> [Reyn92] </ref> which is described in [Srin92]. In this algorithm, the framework hardware is used to rapidly compute GVT. The main focus of this experiment was to demonstrate the correctness of the proposed aggressive synchronization algorithm through simulations.
Reference: [RePa92] <author> Reynolds Jr., P. F. and Pancerella, C. M., </author> <title> "Hardware Support for Parallel Discrete Event Simulations", </title> <institution> Computer Science Report No. Technical Report-92-08, Department of Computer Science, University of Virginia, Charlottesville, Virginia, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: We describe our framework in the context of correctness criteria; a casual, intuitive approach invariably yields nasty surprises. High speed hardware that can quickly disseminate globally reduced values, such as GVT, can be useful to PDES in many other respects as well. We have established elsewhere <ref> [RePa92] </ref> that such a network could support the rapid dissemination of values used in lookahead computations. Similarly, values reflecting processing rates could be disseminated rapidly for adaptive PDES protocols. <p> A Functional View of the Framework Hardware We now focus on hardware to support parallel simulation <ref> [RePa92] </ref>. In this section we present a functional view of a hardware design which satisfies the correctness criteria established earlier. Specific details of this design are discussed in Section 4. All design decisions have been made to ensure the correctness criteria. <p> The first set was aimed at stress-testing the hardware while the second one was to determine the time to perform a global reduction. In both cases we simulated a system of eight HP/AP pairs executing the framework synchronization algorithm described in <ref> [RePa92] </ref> and employing the framework hardware to compute the globally reduced values used in the algorithm. LP's executing on the HP's maintain two local values: the logical time of the next event and the smallest unreceived message time.
Reference: [Sama85] <author> Samadi, B., </author> <title> "Distributed Simulation, Algorithms, and Performance Analysis", </title> <type> PhD Thesis, </type> <institution> Computer Science Department, University of California, </institution> <address> Los Angeles, </address> <month> January </month> <year> 1985. </year>
Reference-contexts: Details of the algorithms executed on host and auxiliary processors appear in [Srin92]. 19 One of the significant challenges facing aggressive protocols is the rapid dissemination of global virtual time (GVT). (See [Jeff85], [JeSo85], <ref> [Sama85] </ref>, [LiLa89], [Bell90], and [CoKe91]) In an aggressive parallel simulation, GVT is the simulation safe time, the guaranteed time for which all events with timestamps less than or equal to it have been processed accurately.
Reference: [SoBW88] <author> Sokol, L. M., Briscoe, D. P. and Wieland, A. P., "MTW: </author> <title> A Strategy for Scheduling Discrete Simulation Events for Concurrent Execution", </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <address> San Diego, California, </address> <pages> pp. 34-42, </pages> <month> (February </month> <year> 1988). </year>
Reference-contexts: We have established elsewhere [RePa92] that such a network could support the rapid dissemination of values used in lookahead computations. Similarly, values reflecting processing rates could be disseminated rapidly for adaptive PDES protocols. Iterative PDES protocols, those which select a computation window in a loosely synchronous manner <ref> [Ayan89, ChSh89, Luba88, Nico91, SoBW88] </ref>, can benefit 1 from hardware support for the computation of the window size. Termination detection and global consensus [AbRi91] are other capabilities that can benefit PDES. All of these activities can be carried out in the framework we describe here.
Reference: [Srin92] <author> Srinivasan, S., </author> <title> "Modeling a Framework for Parallel Simulations", </title> <type> Master's Thesis, </type> <institution> School of Engineering and Applied Science, University of Virginia, Charlottesville, Virginia, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Employing auxiliary processors provides a separation of the synchronization activity (performed on auxiliary processors) and the application being simulated (performed on host processors). High speed synchronization activity in the parallel simulation framework (See [Panc92] and <ref> [Srin92] </ref>) is performed on the dedicated AP's. The logical processes (LP's) execute on the host processors, and all event messages are sent and received at the host 8 processors. <p> Auxiliary Processor The general layout of an auxiliary processor is depicted in Figure 2. Auxiliary processors are fast, general purpose 32-bit microprocessors. Each AP has its own memory to store synchronization programs and related data structures (See [Reyn92], [Panc92], and <ref> [Srin92] </ref>). Furthermore, each AP has EPROM to store a boot-up monitor which is executed upon reset. 4.2. Setup Each auxiliary processor boots up in a "listening" state in which it monitors its host processor interface. <p> In this section we give a summary of how an aggressive PDES synchronization protocol, such as Time Warp, can benefit from this hardware. Details of the algorithms executed on host and auxiliary processors appear in <ref> [Srin92] </ref>. 19 One of the significant challenges facing aggressive protocols is the rapid dissemination of global virtual time (GVT). (See [Jeff85], [JeSo85], [Sama85], [LiLa89], [Bell90], and [CoKe91]) In an aggressive parallel simulation, GVT is the simulation safe time, the guaranteed time for which all events with timestamps less than or equal <p> The nature of this application differed for the two experiments since they tested different aspects of the hardware. <ref> [Srin92] </ref> describes the detailed models built to simulate and test the framework hardware. We present an overview here. The focus of the first experiment was to test the hardware under load. <p> The model for the AP was also modified to execute an aggressive version of the framework algorithm [Reyn92] which is described in <ref> [Srin92] </ref>. In this algorithm, the framework hardware is used to rapidly compute GVT. The main focus of this experiment was to demonstrate the correctness of the proposed aggressive synchronization algorithm through simulations.
Reference: [Ston90] <author> Stone, H. S., </author> <title> High-Performance Computer Architecture, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: The goals of both approaches are similar, but our framework is more efficient, more flexible, and more scalable. Several researchers have proposed the use of hardware to implement barrier synchronization. Hoshino [Hosh85] has an efficient barrier synchronization in the PAX computer. Stone <ref> [Ston90] </ref> suggests the use of global busses to compute maximum values and to implement fetch-and-increment. The hardware that we propose, on the other hand, provides support for a larger class of algorithms than barrier synchronization algorithms. Many parallel architectures provide for global binary, associative operations across all processors.
Reference: [SBus90] <institution> Sun Microsystems, SBus Specification B.0, Sun Microsystems, Inc., Mountain View, California, </institution> <year> 1990. </year>
Reference-contexts: The host processor, a Sparc-1e, accesses the dual-ported RAM via its HP interface, which is the Sun SBus. The SBus has a bandwidth of about 100 megabytes per second for 32-bit words <ref> [SBus90] </ref>. We expect a potential throughput of 25 megabytes per second from host to auxiliary processor. 5.3. Parallel Reduction Network The ALU's in the prototype parallel reduction network require 40 nanoseconds to perform a 32-bit fixed-point addition; hence, it will take 80 nanoseconds to perform a tagged selective operation.
Reference: [Thin92] <institution> Thinking Machines Corporation, The Connection Machine CM-5 Technical Summary , Thinking Machines Corporation, Cambridge, Massachusetts, </institution> <month> January </month> <year> 1992. </year> <month> 35 </month>
Reference-contexts: Many parallel architectures provide for global binary, associative operations across all processors. Global operations on the Intel iPSC/2 [Inte89] are provided for arithmetic and logical operations. The Thinking Machines CM-5 <ref> [Thin92] </ref> contains two separate networks for different types of communication and synchronization: the data network is the primary message-passing network in the machine and the control network provides hardware support for common cooperative operations.
References-found: 37

