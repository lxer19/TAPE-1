URL: http://www.cs.orst.edu/~lierer/aaai97.ps
Refering-URL: http://www.cs.orst.edu:80/~tadepall/research/publications.html
Root-URL: 
Email: lierer@research.cs.orst.edu tadepall@research.cs.orst.edu  
Title: Active Learning with Committees for Text Categorization  
Author: Ray Liere Prasad Tadepalli 
Address: Dearborn Hall 303, Corvallis, OR 97331-3202, USA  
Affiliation: Department of Computer Science, Oregon State University,  
Abstract: In many real-world domains, supervised learning requires a large number of training examples. In this paper, we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning. Our approach is similar to the Query by Committee framework, where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label. Our experiments are conducted in the text categorization domain, which is characterized by a large number of features, many of which are irrelevant. We report here on experiments using a committee of Winnow-based learners and demonstrate that this approach can reduce the number of labeled training examples required over that used by a single Winnow learner by 1-2 orders of magnitude. 
Abstract-found: 1
Intro-found: 1
Reference: [Angluin88] <author> Dana Angluin, </author> <title> Queries and Concept Learn ing, </title> <booktitle> Machine Learning 2 </booktitle> <pages> 319-342, </pages> <year> 1988 </year>
Reference-contexts: One active learning approach is the membership query paradigm, in which the learner can construct new sets of inputs and request that the teacher provide their labels <ref> [Angluin88] </ref>. In this paper, we are specifically considering the type of active learning in which there exists a set of examples, and the learner chooses which of these it will use for learning. Typically, the cycle proceeds as follows.
Reference: [Apte94] <author> Chidanand Apt, Fred Damerau, Sholom M. Weiss, </author> <title> Automated Learning of Decision Rules for Text Categorization, </title> <booktitle> ACM TOIS 12(2) </booktitle> <pages> 233-251, </pages> <month> July </month> <year> 1994 </year>
Reference-contexts: Full corpus statistics for Reuters (after our preprocessing): --- 22,173 documents --- 21,334 unique tokens in titles (maximum --- the actual number depends on the tokenizing method used) --- 679 categories 4.3 Repeated Trials A variety of approaches have been utilized in previous research using the Reuters corpus <ref> [Hayes90, Lewis91, Apte94] </ref>. There are some differences among researchers as to which articles in the corpus are used, and also there are differences in how the corpus is split into training and test sets.
Reference: [Board87] <author> Raymond A. Board, Leonard Pitt, </author> <title> Semi-Supervised Learning, </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <note> Report No. UIUCDCS-R-87-1372, </note> <month> September </month> <year> 1987 </year>
Reference-contexts: Since the method was developed for text categorization, it is able to handle noise as well as large numbers of features [Lewis94]. While approaches and results vary, these and other studies have concluded that active learning greatly improves learning efficiency by reducing the number of labeled examples used <ref> [Board87, Freund92, Dagan95] </ref>. 2.2 Query by Committee (QBC) Query by Committee (QBC) is a learning method which uses a committee of hypotheses to decide for which examples the labels will be requested. It also uses the committee to determine the prediction of the label.
Reference: [Breiman96] <author> Leo Breiman, </author> <title> Bagging Predictors, </title> <booktitle> Machine Learning 24(2) </booktitle> <pages> 123-140, </pages> <month> August </month> <year> 1996 </year>
Reference-contexts: The idea behind using a committee to make predictions is that a committee of several members might be able to outperform a single member <ref> [Freund92, Seung92, Breiman96] </ref>. Each member predicts a label, and these votes are then combined using majority vote. 4. Experimental Results 4.1 The Systems to be Compared We examined the performance of 4 different learning systems.
Reference: [Cohn94] <author> David Cohn, Les Atlas, Richard Ladner, </author> <title> Improving Generalization with Active Learning, </title> <booktitle> Machine Learning 15(2) </booktitle> <pages> 201-221, </pages> <month> May </month> <year> 1994 </year>
Reference-contexts: Active learning refers to machine learning methods that allow the learning program to exert some control over the examples on which it learns <ref> [Cohn94] </ref>. Query by Committee (QBC) is one specific type of active learning which starts with a committee of all possible hypotheses. Each feature vector is presented to the committee. <p> There have been some promising results in the active learning area. Cohn, Atlas, and Ladner developed the theory for an active learning method called selective sampling and then applied it to some small to moderate sized problems as a demonstration of the viability of this new approach <ref> [Cohn94] </ref>. Lewis and Gale developed a method called uncertainty sampling, which is similar conceptually to selective sampling, but which is specifically meant for use in text categorization.
Reference: [Croft95] <author> W. Bruce Croft, </author> <title> Effective Text Retrieval Based on Combining Evidence from the Corpus and Users, </title> <booktitle> IEEE Expert 10(6) </booktitle> <pages> 59-63, </pages> <month> December </month> <year> 1995 </year>
Reference-contexts: However, it has been found that users are often much better at deciding whether or not a particular document is of interest than they are at expressing that interest in a query language. User input in the form of relevance feedback significantly increases retrieval effectiveness <ref> [Croft95] </ref>. We can think of relevance feedback as allowing the system to learn the user's intentions by asking for the labels for selected examples, and use the active learning with committees paradigm to decide which examples to present to the user. 7.
Reference: [Dagan95] <author> Ido Dagan, Sean P. Engelson, </author> <title> Committee-Based Sampling for Training Probabilistic Classifiers, </title> <booktitle> in Proceedings: ICML95, 1995, p. </booktitle> <pages> 150-157 </pages>
Reference-contexts: Since the method was developed for text categorization, it is able to handle noise as well as large numbers of features [Lewis94]. While approaches and results vary, these and other studies have concluded that active learning greatly improves learning efficiency by reducing the number of labeled examples used <ref> [Board87, Freund92, Dagan95] </ref>. 2.2 Query by Committee (QBC) Query by Committee (QBC) is a learning method which uses a committee of hypotheses to decide for which examples the labels will be requested. It also uses the committee to determine the prediction of the label. <p> Dagan and Engelson proposed a similar method, termed committee-based sampling, for selecting examples to be labeled <ref> [Dagan95] </ref>. The informativeness of an example (and so the desirability of having it labeled) is indicated by the entropy of the predictions of the various hypotheses in the committee. 3.
Reference: [Freund92] <author> Yoav Freund, H. Sebastian Seung, Eli Shamir, Naftali Tishby, </author> <title> Information, Prediction, and Query by Committee, </title> <booktitle> NIPS92, p. </booktitle> <pages> 483-490 </pages>
Reference-contexts: The label is then used to remove all hypotheses from the committee that do not predict the actual label <ref> [Freund92, Seung92, Freund95] </ref>. The learning methods which we have inv estigated are similar to QBC, in that they use disagreement among the committee members to determine the need for requesting the actual value of each example's label from the teacher. <p> Since the method was developed for text categorization, it is able to handle noise as well as large numbers of features [Lewis94]. While approaches and results vary, these and other studies have concluded that active learning greatly improves learning efficiency by reducing the number of labeled examples used <ref> [Board87, Freund92, Dagan95] </ref>. 2.2 Query by Committee (QBC) Query by Committee (QBC) is a learning method which uses a committee of hypotheses to decide for which examples the labels will be requested. It also uses the committee to determine the prediction of the label. <p> Freund, Seung, Shamir, and Tishby analyzed QBC in detail and showed that the number of examples required in this learning situation is logarithmic in the number of examples required for random example selection learning <ref> [Freund92] </ref>. Dagan and Engelson proposed a similar method, termed committee-based sampling, for selecting examples to be labeled [Dagan95]. The informativeness of an example (and so the desirability of having it labeled) is indicated by the entropy of the predictions of the various hypotheses in the committee. 3. <p> The idea behind using a committee to make predictions is that a committee of several members might be able to outperform a single member <ref> [Freund92, Seung92, Breiman96] </ref>. Each member predicts a label, and these votes are then combined using majority vote. 4. Experimental Results 4.1 The Systems to be Compared We examined the performance of 4 different learning systems.
Reference: [Freund95] <author> Yoav Freund, H. Sebastian Seung, Eli Shamir, Naftali Tishby, </author> <title> Selective Sampling Using the Query by Committee Algorithm, </title> <month> July </month> <year> 1995, </year> <note> to appear in Machine Learning </note>
Reference-contexts: The label is then used to remove all hypotheses from the committee that do not predict the actual label <ref> [Freund92, Seung92, Freund95] </ref>. The learning methods which we have inv estigated are similar to QBC, in that they use disagreement among the committee members to determine the need for requesting the actual value of each example's label from the teacher. <p> If their predictions form a tie, then the example is assumed to be maximally informative, the algorithm requests the actual label from the teacher and updates the version space <ref> [Fre-und92, Seung92, Freund95] </ref>. Freund, Seung, Shamir, and Tishby analyzed QBC in detail and showed that the number of examples required in this learning situation is logarithmic in the number of examples required for random example selection learning [Freund92]. <p> We present here a brief intuitive argument as to why the active-majority method works so well. Please see [Fre-und92] and <ref> [Freund95] </ref> for a more detailed discussion of this aspect of active learning, as it relates to QBC.
Reference: [Hayes90] <author> Phillip J. Hayes, Peggy M. Andersen, Irene B. Nirenburg, Linda M. Schmandt, </author> <title> TCS: A Shell for Content-Based Text Categorization, </title> <booktitle> in Proceedings of the 6th IEEE CAIA, 1990, IEEE, p. </booktitle> <pages> 320-326 </pages>
Reference-contexts: Full corpus statistics for Reuters (after our preprocessing): --- 22,173 documents --- 21,334 unique tokens in titles (maximum --- the actual number depends on the tokenizing method used) --- 679 categories 4.3 Repeated Trials A variety of approaches have been utilized in previous research using the Reuters corpus <ref> [Hayes90, Lewis91, Apte94] </ref>. There are some differences among researchers as to which articles in the corpus are used, and also there are differences in how the corpus is split into training and test sets.
Reference: [Lewis91] <author> David D. Lewis, </author> <title> Representation and Learning in Information Retrieval, </title> <type> Ph.D. Thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <type> COINS Technical Report 91-93, </type> <month> December </month> <year> 1991 </year>
Reference-contexts: Full corpus statistics for Reuters (after our preprocessing): --- 22,173 documents --- 21,334 unique tokens in titles (maximum --- the actual number depends on the tokenizing method used) --- 679 categories 4.3 Repeated Trials A variety of approaches have been utilized in previous research using the Reuters corpus <ref> [Hayes90, Lewis91, Apte94] </ref>. There are some differences among researchers as to which articles in the corpus are used, and also there are differences in how the corpus is split into training and test sets. <p> We are mainly interested at this point in comparisons among our learning systems. In particular, we want to compare their performance for categories most likely to have ample training data. We used the 10 most frequently occurring topic categories, as listed in <ref> [Lewis91] </ref>, for our experiments. We performed repeated trials for each category, using randomly chosen training-test splits. We used the entire corpus, and split it into 21,000 training examples and 1,173 test examples. We used titles only for our tests. For our experiments, we did the following.
Reference: [Lewis94] <author> David D. Lewis, William A. Gale, </author> <title> A Sequential Algorithm for Training Text Classifiers, </title> <editor> in Proceed ings: SIGIR'94, p. </editor> <month> 3-12 </month>
Reference-contexts: Their method selects for labeling those examples whose membership is most unclear by using an approximation based on Bayes Rule, certain independence assumptions, and logistic regression. Since the method was developed for text categorization, it is able to handle noise as well as large numbers of features <ref> [Lewis94] </ref>.
Reference: [Littlestone88] <author> Nick Littlestone, </author> <title> Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm, </title> <booktitle> Machine Learning 2(4) </booktitle> <pages> 285-318, </pages> <year> 1988 </year>
Reference-contexts: If their predictions disagree, then we ask to see the actual label. 3.2 Updating the Hypotheses After the label is seen, the learners adjust the hypotheses in the committee. Typically, each member of the committee learns individually. We chose Winnow as the learning algorithm <ref> [Littlestone88] </ref>. Winnow is especially suited to large attribute spaces and to situations in which there is a large percentage of irrelevant features. Winnow also has a relatively low space and time complexity and is easy to implement. And, Winnow has been used successfully in other noisy text-based applications [Roth96]. <p> And, Winnow has been used successfully in other noisy text-based applications [Roth96]. Actually, "Winnow" refers to a quite large family of algorithms [Littlestone89]. We hav e thus far used one of the more general (and simpler) Winnow algorithms --- WINNOW2 in <ref> [Littlestone88] </ref>, with some modifications from [Littlestone91]. We will hereafter refer to the algorithm that we use as simply "Winnow". Conceptually, think of each document as being represented by a data point in some feature space.
Reference: [Littlestone89] <author> Nicholas Littlestone, </author> <title> Mistake Bounds and Logarithmic Linear-Threshold Learning Algorithms, </title> <institution> University of California at Santa Cruz, UCSC CRL-89-11, </institution> <month> March </month> <year> 1989 </year>
Reference-contexts: Winnow also has a relatively low space and time complexity and is easy to implement. And, Winnow has been used successfully in other noisy text-based applications [Roth96]. Actually, "Winnow" refers to a quite large family of algorithms <ref> [Littlestone89] </ref>. We hav e thus far used one of the more general (and simpler) Winnow algorithms --- WINNOW2 in [Littlestone88], with some modifications from [Littlestone91]. We will hereafter refer to the algorithm that we use as simply "Winnow".
Reference: [Littlestone91] <author> Nick Littlestone, </author> <title> Redundant Noisy Attributes, Attribute Errors, and Linear-Threshold Learning Using Winnow, </title> <booktitle> COLT'91, p. </booktitle> <pages> 147-156 </pages>
Reference-contexts: And, Winnow has been used successfully in other noisy text-based applications [Roth96]. Actually, "Winnow" refers to a quite large family of algorithms [Littlestone89]. We hav e thus far used one of the more general (and simpler) Winnow algorithms --- WINNOW2 in [Littlestone88], with some modifications from <ref> [Littlestone91] </ref>. We will hereafter refer to the algorithm that we use as simply "Winnow". Conceptually, think of each document as being represented by a data point in some feature space.
Reference: [Perlman] <author> Gary Perlman, </author> | <note> STAT version 5.4, software and documentation, available from: ftp:/archive.- cis.ohio-state.edu/pub/stat/ </note>
Reference-contexts: Acknowledgements This research was partially supported by the National Science Foundation under grant number IRI-9520243. The availability of the Reuters-22173 corpus [Reuters] and of the | STAT Data Manipulation and Analysis Programs <ref> [Perlman] </ref> has greatly assisted in our research.
Reference: [Reuters] <author> Reuters-22173 corpus, </author> <title> a collection of 22,173 indexed documents appearing on the Reuters newswire in 1987; Reuters Ltd, </title> <institution> Carnegie Group, David Lewis, </institution> <note> Information Retrieval Laboratory at the University of Massachusetts; available via ftp from: ciir-ftp.- cs.umass.edu:/pub/reuters1/corpus.tar.Z </note>
Reference-contexts: Prediction is by that same Winnow. This can be thought of as the "base case" --- a single supervised learner and predictor. 4.2 Test Bed All of our experiments were conducted using the titles of newspaper articles from the Reuters-22173 corpus <ref> [Reuters] </ref>, hereafter "Reuters". The Reuters corpus is a collection of 22,173 Reuters newswire articles ("documents") from 1987. It is a 25Mb full text corpus. Each article has been assigned to categories by human indexers. Typical categories are "grain", "gold", "Canada", and "trade". <p> Acknowledgements This research was partially supported by the National Science Foundation under grant number IRI-9520243. The availability of the Reuters-22173 corpus <ref> [Reuters] </ref> and of the | STAT Data Manipulation and Analysis Programs [Perlman] has greatly assisted in our research.
Reference: [Roth96] <author> Dan Roth, </author> <title> Applying Winnow to Context Sensitive Spelling Correction, </title> <booktitle> ICML96, p. </booktitle> <pages> 182-190 </pages>
Reference-contexts: Winnow is especially suited to large attribute spaces and to situations in which there is a large percentage of irrelevant features. Winnow also has a relatively low space and time complexity and is easy to implement. And, Winnow has been used successfully in other noisy text-based applications <ref> [Roth96] </ref>. Actually, "Winnow" refers to a quite large family of algorithms [Littlestone89]. We hav e thus far used one of the more general (and simpler) Winnow algorithms --- WINNOW2 in [Littlestone88], with some modifications from [Littlestone91]. We will hereafter refer to the algorithm that we use as simply "Winnow".
Reference: [Seung92] <author> H. S. Seung, M. Opper, H. Sompolinsky, </author> <title> Query by Committee, </title> <booktitle> COLT92, p. </booktitle> <pages> 287-294 </pages>
Reference-contexts: The label is then used to remove all hypotheses from the committee that do not predict the actual label <ref> [Freund92, Seung92, Freund95] </ref>. The learning methods which we have inv estigated are similar to QBC, in that they use disagreement among the committee members to determine the need for requesting the actual value of each example's label from the teacher. <p> If their predictions form a tie, then the example is assumed to be maximally informative, the algorithm requests the actual label from the teacher and updates the version space <ref> [Fre-und92, Seung92, Freund95] </ref>. Freund, Seung, Shamir, and Tishby analyzed QBC in detail and showed that the number of examples required in this learning situation is logarithmic in the number of examples required for random example selection learning [Freund92]. <p> However, QBC needs to maintain all possible hypotheses consistent with the training data --- the version space --- in some form <ref> [Seung92] </ref>. This is the committee. When data is noisy, this will not be possible. When there is a very large number of candidate hypotheses, explicitly representing them will not be practical. In text categorization, we have data that is noisy. <p> The idea behind using a committee to make predictions is that a committee of several members might be able to outperform a single member <ref> [Freund92, Seung92, Breiman96] </ref>. Each member predicts a label, and these votes are then combined using majority vote. 4. Experimental Results 4.1 The Systems to be Compared We examined the performance of 4 different learning systems.
References-found: 19

