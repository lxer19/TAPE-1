URL: ftp://ftp.cs.toronto.edu/pub/parallel/Kulkarni_Stumm_ACJ95.ps.Z
Refering-URL: http://www.cs.toronto.edu/~kulki/pubs_abs.html
Root-URL: 
Title: Linear Loop Transformations in Optimizing Compilers for Parallel Machines  
Author: Dattatraya Kulkarni and Michael Stumm 
Keyword: Key Words: Dependence Analysis, Iteration Spaces, Parallelism, Locality, Load Balance, Conventional Loop Transformations, Linear Loop Transformations  
Address: 10 King's College Road,  Toronto, Toronto, ON M5S 1A4, CANADA.  
Affiliation: Parallel Systems Group, Department of Computer Science,  University of  
Note: The Australian Computer Journal, pages  Corresponding author.  
Email: Email: kulki@cs.toronto.edu  
Date: 41--50, May 1995.  October 26, 1994  
Abstract: We present the linear loop transformation framework which is the formal basis for state of the art optimization techniques in restructuring compilers for parallel machines. The framework unifies most existing transformations and provides a systematic set of code generation techniques for arbitrary compound loop transformations. The algebraic representation of the loop structure and its transformation give way to quantitative techniques for optimizing performance on parallel machines. We discuss in detail the techniques for generating the transformed loop and deriving the desired linear transformation. 
Abstract-found: 1
Intro-found: 1
Reference: [AL93] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 28, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: The new framework [KSb] reorganizes computations at a much finer granularity than existing techniques and helps implement a class of flexible computation rules. Most of the current work of interest involves combining loop transformation, data alignment and partitioning techniques for local and global optimization <ref> [AL93, KSb] </ref>. Acknowledgements The first author thanks Utpal Banerjee who provided impetus to his and KG Kumar's joint work. He also thanks KG Kumar for his continued encouragement.
Reference: [Ban88] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: The then clause of an if statement is control dependent on the branching condition. A statement that uses the value of a variable assigned by an earlier statement is data dependent on the earlier statement <ref> [Ban88] </ref>. In this paper, we concern ourselves only with data dependence. Control dependence is important to identify functional level parallelism, and to choose between various candidates for data distribution, among other things. <p> Since, our discussion is limited to analysis of dependence between loop iterations, control dependence does not concern us much. We briefly discuss the basic Kulkarni and Stumm: Linear Loop Transformations 8 concepts in data dependence and the computational complexity of deciding the existence of a dependence. <ref> [Ban88] </ref> serves as a very good reference of early development in the area. Recent developments can be found in [LYZ90, WT92, Pug92, MHL91]. There are four types of data dependences: flow, anti, output, and input dependence. The only true dependence is flow dependence. <p> The dependence matrix therefore is D = 1 0 2 # Because the problem of determining the existence of a dependence is NP-complete, one often employs efficient algorithms that solve restricted cases or provide approximate solutions in practice. GCD test <ref> [Ban88] </ref> finds the existence of an integer solution to the dependence problem with only a single subscript in an unbounded iteration space. Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information [LYZ90, WT92].
Reference: [Ban90] <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The concept of linear transformations for loop partitioning were introduced independently by Banerjee <ref> [Ban90] </ref>, and by Wolf and Lam [WL90]. In the conventional approach there exist several loop apparently independent transformations, including loop interchange, loop permutation, skew , reversal , wavefront , and tiling. 2 Loop interchange [Wol90], as the name suggests, exchanges two loop levels. <p> Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations. <p> When the transformation is unimodular the above approach results in the transformed space that corresponds exactly to the original space. The unimodularity of the transformation matrix ensures that the stride in the transformed loop is unit as well. Banerjee <ref> [Ban90] </ref> computes the bounds for a two dimensional loop directly. Kumar and Kulkarni [KKB91] compute the bounds for a loop of any dimension by transforming the bounding hyper planes in the original loop and substituting for the extremes points of the polyhedron.
Reference: [Ban93] <author> Utpal Banerjee. </author> <title> Loop Transformations for Restructuring Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year> <title> Kulkarni and Stumm: Linear Loop Transformations 21 </title>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations.
Reference: [Ban94] <author> Utpal Banerjee. </author> <title> Loop Parallelization. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations.
Reference: [CF87] <author> Ron Cytron and Jeanne Ferrante. </author> <booktitle> What's in a name? In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <year> 1987. </year>
Reference-contexts: There are four types of data dependences: flow, anti, output, and input dependence. The only true dependence is flow dependence. The other dependences are the result of reusing the same location of memory and are hence called pseudo dependences. They can be eliminated by renaming some of the variables <ref> [CF87] </ref>. For this reason we write S1 ffi S2 to mean flow dependence from S 1 to S 2 from now on.
Reference: [Dow90] <author> M.L. Dowling. </author> <title> Optimum code parallelization using unimodular transformations. </title> <journal> Parallel Computing, </journal> <volume> 16 </volume> <pages> 155-171, </pages> <year> 1990. </year>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations. <p> The problem is NP-complete for unrestricted loops, and even affine loops with non-constant dependence distances <ref> [Dow90] </ref>. A unimodular matrix can however be found in polynomial time for affine loops with only constant dependences [Dow90, KKB91]. A dependence matrix can provide a good indication as to the desired transformations. In fact, it is common to start with a dependence matrix augmented with the identity matrix. <p> The problem is NP-complete for unrestricted loops, and even affine loops with non-constant dependence distances [Dow90]. A unimodular matrix can however be found in polynomial time for affine loops with only constant dependences <ref> [Dow90, KKB91] </ref>. A dependence matrix can provide a good indication as to the desired transformations. In fact, it is common to start with a dependence matrix augmented with the identity matrix.
Reference: [IT88] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Conference Record of the 15th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <address> San Diego, CA, </address> <year> 1988. </year>
Reference-contexts: Wavefront [Lam74] identifies sets of independent iterations and restructures the loop to execute these sets sequentially. All the available parallelism is thus at inner loop level which achieves finer grain of parallelism. Loop skewing, parameterized by a skew factor, is an instance of wavefront transformation. Tiling <ref> [RS90, WL91, IT88] </ref> strips several loop levels so that outer loops step through a space of iteration tiles, and inner loops step through the iterations in a tile.
Reference: [KKB] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Mapping nested loops on hierarchical parallel machines using unimodular transformations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> page (revising). </note>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 19 The general framework for internalization and Algorithms to find a good internalization in polynomial time can be found in <ref> [KKB91, KKB92, KKB] </ref>. One can only internalize n1 linearly independent dependences in an n-dimensional loop. The choice of dependences to internalize has an impact on such factors as the validity of the transformation, the size of the outer level in the transformed loop, the load balance, locality etc.
Reference: [KKB91] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Generalized unimodular loop transformations for distributed memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <address> Chicago, MI, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations. <p> The unimodularity of the transformation matrix ensures that the stride in the transformed loop is unit as well. Banerjee [Ban90] computes the bounds for a two dimensional loop directly. Kumar and Kulkarni <ref> [KKB91] </ref> compute the bounds for a loop of any dimension by transforming the bounding hyper planes in the original loop and substituting for the extremes points of the polyhedron. The approach is elegant when the bounds are constants, and becomes complex when the original loop bounds are linear expressions. <p> The problem is NP-complete for unrestricted loops, and even affine loops with non-constant dependence distances [Dow90]. A unimodular matrix can however be found in polynomial time for affine loops with only constant dependences <ref> [Dow90, KKB91] </ref>. A dependence matrix can provide a good indication as to the desired transformations. In fact, it is common to start with a dependence matrix augmented with the identity matrix. <p> The amount of parallelism is the size of the outer loop. Any dependences carried by the outer loop result in non-local accesses. Internalization <ref> [KKBP91, KKB91] </ref> transforms a loop so that as many dependences as possible are independent of the outer loop, and so that the outer loop is as large as possible. <p> Kulkarni and Stumm: Linear Loop Transformations 19 The general framework for internalization and Algorithms to find a good internalization in polynomial time can be found in <ref> [KKB91, KKB92, KKB] </ref>. One can only internalize n1 linearly independent dependences in an n-dimensional loop. The choice of dependences to internalize has an impact on such factors as the validity of the transformation, the size of the outer level in the transformed loop, the load balance, locality etc.
Reference: [KKB92] <author> K.G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hierarchical parallel machines in polynomial time. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 19 The general framework for internalization and Algorithms to find a good internalization in polynomial time can be found in <ref> [KKB91, KKB92, KKB] </ref>. One can only internalize n1 linearly independent dependences in an n-dimensional loop. The choice of dependences to internalize has an impact on such factors as the validity of the transformation, the size of the outer level in the transformed loop, the load balance, locality etc. <p> Kulkarni and Kumar [KKBP91] introduced the notion of weight to a dependence to characterize the net volume of non-local accesses. They also provided metrics for parallelism and load imbalance for the two dimensional case. Internalization can be further generalized to mapping with multiple parallel levels <ref> [KKB92] </ref>. Locality can be improved by internalizing a dependence or a reference with reuse. In other words, internalization is a transformation that enhances parallelism and locality [KKB92]. 4.2 Access Normalization Ideally, we want a processor to own all the data it needs in the course of its computation. <p> Internalization can be further generalized to mapping with multiple parallel levels <ref> [KKB92] </ref>. Locality can be improved by internalizing a dependence or a reference with reuse. In other words, internalization is a transformation that enhances parallelism and locality [KKB92]. 4.2 Access Normalization Ideally, we want a processor to own all the data it needs in the course of its computation. In that case we wish to transform a loop so that it matches the existing layout of the data in the memory of the parallel system.
Reference: [KKBP91] <author> D. Kulkarni, K.G. Kumar, A. Basu, and A. Paulraj. </author> <title> Loop partitioning for distributed memory multiprocessors as unimodular transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations. <p> The parallelism at outer (inner) loop level, volume of communication, the average load, and load balances for the transformed loop can be specified in terms of the elements of the transformation matrix, dependences, and original loop bounds <ref> [KKBP91] </ref>. For example, we may want to find a transformation that minimizes the size of the outer loop level, because it is sequential. <p> The amount of parallelism is the size of the outer loop. Any dependences carried by the outer loop result in non-local accesses. Internalization <ref> [KKBP91, KKB91] </ref> transforms a loop so that as many dependences as possible are independent of the outer loop, and so that the outer loop is as large as possible. <p> One can only internalize n1 linearly independent dependences in an n-dimensional loop. The choice of dependences to internalize has an impact on such factors as the validity of the transformation, the size of the outer level in the transformed loop, the load balance, locality etc. Kulkarni and Kumar <ref> [KKBP91] </ref> introduced the notion of weight to a dependence to characterize the net volume of non-local accesses. They also provided metrics for parallelism and load imbalance for the two dimensional case. Internalization can be further generalized to mapping with multiple parallel levels [KKB92].
Reference: [KP92] <author> W. Kelly and W. Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report UMIACS-TR-92-126, </type> <institution> University of Maryland, </institution> <year> 1992. </year>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations. <p> The algebraic representation of the loop structure and its transformation give way to quantitative techniques for optimizing performance on parallel machines. We also discussed in detail the techniques for generating the transformed loop. The framework is extended recently <ref> [KSb, KP92] </ref> to handle imperfectly nested loops. The new framework [KSb] reorganizes computations at a much finer granularity than existing techniques and helps implement a class of flexible computation rules.
Reference: [KSa] <author> D. Kulkarni and M. Stumm. </author> <title> Architecture specific optimal loop transformations. </title> <note> In In preparation. </note>
Reference: [KSb] <author> D. Kulkarni and M. Stumm. </author> <title> Computational alignment: A new, unified program transformation for local and global optimization. </title> <type> Technical report. </type>
Reference-contexts: The algebraic representation of the loop structure and its transformation give way to quantitative techniques for optimizing performance on parallel machines. We also discussed in detail the techniques for generating the transformed loop. The framework is extended recently <ref> [KSb, KP92] </ref> to handle imperfectly nested loops. The new framework [KSb] reorganizes computations at a much finer granularity than existing techniques and helps implement a class of flexible computation rules. <p> The algebraic representation of the loop structure and its transformation give way to quantitative techniques for optimizing performance on parallel machines. We also discussed in detail the techniques for generating the transformed loop. The framework is extended recently [KSb, KP92] to handle imperfectly nested loops. The new framework <ref> [KSb] </ref> reorganizes computations at a much finer granularity than existing techniques and helps implement a class of flexible computation rules. Most of the current work of interest involves combining loop transformation, data alignment and partitioning techniques for local and global optimization [AL93, KSb]. <p> The new framework [KSb] reorganizes computations at a much finer granularity than existing techniques and helps implement a class of flexible computation rules. Most of the current work of interest involves combining loop transformation, data alignment and partitioning techniques for local and global optimization <ref> [AL93, KSb] </ref>. Acknowledgements The first author thanks Utpal Banerjee who provided impetus to his and KG Kumar's joint work. He also thanks KG Kumar for his continued encouragement.
Reference: [Kum93] <author> K.G. Kumar. </author> <type> Personal communication. </type> <year> 1993. </year>
Reference-contexts: The approach is elegant when the bounds are constants, and becomes complex when the original loop bounds are linear expressions. All of the above methods fail to produce exact bounds when the transformation is not uni-modular. Ramanujam [Ram92] and Kumar <ref> [Kum93] </ref> compute the new bounds for any non-singular transformation by stepping aside the non-existent iterations. The loop strides can be obtained by diagonalizing the transformation.
Reference: [Lam74] <author> L. Lamport. </author> <title> The parallel execution of do loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2), </volume> <year> 1974. </year> <title> Kulkarni and Stumm: Linear Loop Transformations 22 </title>
Reference-contexts: In a doubly nested loop, a loop interchange can expose parallelism at the inner loop level enabling vectorization or it can expose parallelism at the outer loop. Loop permutation is a generalization of loop interchange and corresponds to a sequence of interchanges of loop levels. Wavefront <ref> [Lam74] </ref> identifies sets of independent iterations and restructures the loop to execute these sets sequentially. All the available parallelism is thus at inner loop level which achieves finer grain of parallelism. Loop skewing, parameterized by a skew factor, is an instance of wavefront transformation.
Reference: [LP92] <author> W. Li and K. Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In Proceedings of the Fifth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations. <p> All accesses to B now become local, although A still has some non-local accesses. To be a valid transformation the access matrix has to be invertible. The techniques to make the access matrix invertible do so at the cost of reduced normalization <ref> [LP92] </ref>. 5 Concluding Remarks We presented linear loop transformation framework which is the formal basis for state of the art optimization techniques in restructuring compilers for parallel machines. The framework unifies most existing transformations and provides a systematic set of code generation techniques for arbitrary compound transformations.
Reference: [LYZ90] <author> Z. Li, P. Yew, and C. Zhu. </author> <title> An efficient data dependence analysis for par-allelizing compilers. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 26-34, </pages> <year> 1990. </year>
Reference-contexts: We briefly discuss the basic Kulkarni and Stumm: Linear Loop Transformations 8 concepts in data dependence and the computational complexity of deciding the existence of a dependence. [Ban88] serves as a very good reference of early development in the area. Recent developments can be found in <ref> [LYZ90, WT92, Pug92, MHL91] </ref>. There are four types of data dependences: flow, anti, output, and input dependence. The only true dependence is flow dependence. The other dependences are the result of reusing the same location of memory and are hence called pseudo dependences. <p> GCD test [Ban88] finds the existence of an integer solution to the dependence problem with only a single subscript in an unbounded iteration space. Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information <ref> [LYZ90, WT92] </ref>. Recently, Pugh noted that integer programming solutions, with exponential worst case complexity have much lower average complex Kulkarni and Stumm: Linear Loop Transformations 10 ity [Pug92].
Reference: [MHL91] <author> D.E. Maydan, J.L. Hennessy, and M.S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 26, </volume> <pages> pages 1-14, </pages> <address> Toronto, Ontario, Canada, </address> <year> 1991. </year>
Reference-contexts: We briefly discuss the basic Kulkarni and Stumm: Linear Loop Transformations 8 concepts in data dependence and the computational complexity of deciding the existence of a dependence. [Ban88] serves as a very good reference of early development in the area. Recent developments can be found in <ref> [LYZ90, WT92, Pug92, MHL91] </ref>. There are four types of data dependences: flow, anti, output, and input dependence. The only true dependence is flow dependence. The other dependences are the result of reusing the same location of memory and are hence called pseudo dependences.
Reference: [Pug92] <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> In Communications of the ACM, </journal> <volume> volume 35, </volume> <pages> pages 102-114, </pages> <year> 1992. </year>
Reference-contexts: We briefly discuss the basic Kulkarni and Stumm: Linear Loop Transformations 8 concepts in data dependence and the computational complexity of deciding the existence of a dependence. [Ban88] serves as a very good reference of early development in the area. Recent developments can be found in <ref> [LYZ90, WT92, Pug92, MHL91] </ref>. There are four types of data dependences: flow, anti, output, and input dependence. The only true dependence is flow dependence. The other dependences are the result of reusing the same location of memory and are hence called pseudo dependences. <p> Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information [LYZ90, WT92]. Recently, Pugh noted that integer programming solutions, with exponential worst case complexity have much lower average complex Kulkarni and Stumm: Linear Loop Transformations 10 ity <ref> [Pug92] </ref>.
Reference: [Ram92] <author> J. Ramanujam. </author> <title> Non-singular transformations of nested loops. </title> <booktitle> In Supercomputing 92, </booktitle> <pages> pages 214-223, </pages> <year> 1992. </year>
Reference-contexts: The approach is elegant when the bounds are constants, and becomes complex when the original loop bounds are linear expressions. All of the above methods fail to produce exact bounds when the transformation is not uni-modular. Ramanujam <ref> [Ram92] </ref> and Kumar [Kum93] compute the new bounds for any non-singular transformation by stepping aside the non-existent iterations. The loop strides can be obtained by diagonalizing the transformation.
Reference: [RS90] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling of iteration spaces for multi-computers. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages 179-186, </pages> <year> 1990. </year>
Reference-contexts: Wavefront [Lam74] identifies sets of independent iterations and restructures the loop to execute these sets sequentially. All the available parallelism is thus at inner loop level which achieves finer grain of parallelism. Loop skewing, parameterized by a skew factor, is an instance of wavefront transformation. Tiling <ref> [RS90, WL91, IT88] </ref> strips several loop levels so that outer loops step through a space of iteration tiles, and inner loops step through the iterations in a tile.
Reference: [Sch86] <author> A. Schrijver. </author> <title> Theory of linear and integer programming. </title> <publisher> Wiley, </publisher> <year> 1986. </year>
Reference-contexts: Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information [LYZ90, WT92]. Recently, Pugh noted that integer programming solutions, with exponential worst case complexity have much lower average complex Kulkarni and Stumm: Linear Loop Transformations 10 ity [Pug92]. A modified Fourier-Motzkin variable elimination <ref> [Sch86] </ref> produces exact solutions in a reasonable amount of time for many problems. 3 Linear Loop Transformations 3.1 Motivation for the Theory Until recently, the various loop transformations such as reversal, skewing, permutations, and inner/outer loop parallelization were handled independently, and determining which loop transformations to apply to a given nested <p> If S 0 = S U 1 is lower triangular (in both upper and lower halves) then it is clear that the new loop bounds can be directly obtained from the rows of S 0 . In general, variable elimination techniques such as Fourier-Motzkin <ref> [Sch86] </ref> has to be applied on S 0 to obtain the new loop bounds. The basic idea in a variable elimination procedure is as follows. Suppose ff's and fi's are linear expressions in K 1 ; :::; K n1 , and a's and b's are constants.
Reference: [WL90] <author> M.E. Wolf and M.S. Lam. </author> <title> An algorithmic approach to compound loop transformation. </title> <booktitle> In Proceedings of Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The concept of linear transformations for loop partitioning were introduced independently by Banerjee [Ban90], and by Wolf and Lam <ref> [WL90] </ref>. In the conventional approach there exist several loop apparently independent transformations, including loop interchange, loop permutation, skew , reversal , wavefront , and tiling. 2 Loop interchange [Wol90], as the name suggests, exchanges two loop levels. <p> Kulkarni and Stumm: Linear Loop Transformations 11 evaluating the contribution of a subsequence of transformations to over all "goodness" is not easy. Even when we find the right sequence of transformations, applying them one by one can produce complex loop bounds. Linear loop transformations <ref> [Ban90, Ban93, Ban94, WL90, Dow90, KKBP91, KKB91, LP92, KP92] </ref> unify all possible sequences of most of the existing transformations and alleviate the problems in applying sequences of many different kinds of transformations.
Reference: [WL91] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <volume> volume 26, </volume> <pages> pages 30-44, </pages> <address> Toronto, Ontario, Canada, </address> <year> 1991. </year>
Reference-contexts: Wavefront [Lam74] identifies sets of independent iterations and restructures the loop to execute these sets sequentially. All the available parallelism is thus at inner loop level which achieves finer grain of parallelism. Loop skewing, parameterized by a skew factor, is an instance of wavefront transformation. Tiling <ref> [RS90, WL91, IT88] </ref> strips several loop levels so that outer loops step through a space of iteration tiles, and inner loops step through the iterations in a tile.
Reference: [Wol90] <author> Michael Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The concept of linear transformations for loop partitioning were introduced independently by Banerjee [Ban90], and by Wolf and Lam [WL90]. In the conventional approach there exist several loop apparently independent transformations, including loop interchange, loop permutation, skew , reversal , wavefront , and tiling. 2 Loop interchange <ref> [Wol90] </ref>, as the name suggests, exchanges two loop levels. In a doubly nested loop, a loop interchange can expose parallelism at the inner loop level enabling vectorization or it can expose parallelism at the outer loop.
Reference: [WT92] <author> Michael Wolfe and Chau-Wen Tseng. </author> <title> The power test for data dependence. </title> <journal> IEEE Trans. Parallel Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 591-601, </pages> <year> 1992. </year>
Reference-contexts: We briefly discuss the basic Kulkarni and Stumm: Linear Loop Transformations 8 concepts in data dependence and the computational complexity of deciding the existence of a dependence. [Ban88] serves as a very good reference of early development in the area. Recent developments can be found in <ref> [LYZ90, WT92, Pug92, MHL91] </ref>. There are four types of data dependences: flow, anti, output, and input dependence. The only true dependence is flow dependence. The other dependences are the result of reusing the same location of memory and are hence called pseudo dependences. <p> GCD test [Ban88] finds the existence of an integer solution to the dependence problem with only a single subscript in an unbounded iteration space. Some algorithms find real valued solutions in bounded iteration spaces and dependence direction information <ref> [LYZ90, WT92] </ref>. Recently, Pugh noted that integer programming solutions, with exponential worst case complexity have much lower average complex Kulkarni and Stumm: Linear Loop Transformations 10 ity [Pug92].
References-found: 28

