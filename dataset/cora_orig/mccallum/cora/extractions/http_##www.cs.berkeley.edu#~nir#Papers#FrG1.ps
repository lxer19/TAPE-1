URL: http://www.cs.berkeley.edu/~nir/Papers/FrG1.ps
Refering-URL: http://http.cs.berkeley.edu/~nir/publications.html
Root-URL: 
Email: nir@cs.stanford.edu  moises@rpal.rockwell.com  
Title: Building Classifiers using Bayesian Networks  
Author: Nir Friedman Moises Goldszmidt 
Address: Gates Building 1A Stanford, CA 94305-9010  444 High St., Suite 400 Palo Alto, CA 94301  
Affiliation: Stanford University Dept. of Computer Science  Rockwell Science Center  
Abstract: Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state of the art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we examine and evaluate approaches for inducing classifiers from data, based on recent results in the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayes classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness which are characteristic of naive Bayes. We experimentally tested these approaches using benchmark problems from the U. C. Irvine repository, and compared them against C4.5, naive Bayes, and wrapper-based feature selection methods. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Chow, C. K., and Lui, C. N. </author> <year> 1968. </year> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Trans. on Info. Theory 14 </journal> <pages> 462-467. </pages>
Reference-contexts: Chow and Liu show that there is a simple procedure that constructs the maximal log-probability tree. Let n be the number of random variables and N be the number of training instances. Then Theorem 4.1: <ref> (Chow & Lui 1968) </ref> There is a procedure of time complexity O (n 2 N ), that constructs the tree structure B T that maximizes LL (B T jD). The procedure of Chow and Liu can be summarized as follows. 1.
Reference: <author> Cormen, T. H.; Leiserson, C. E.; and Rivest, R. L. </author> <year> 1990. </year> <title> Introduction to Algorithms. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Build a complete undirected graph in which the vertices are the variables in U. Annotate the weight of an edge connecting X i to X j by I (X i ; X j ). 3. Build a maximum weighted spanning tree of this graph <ref> (Cormen, Leiserson, & Rivest 1990) </ref>. 7 These structures are called Bayesian conditional trees by Geiger. to naive Bayes (dashed line). to C4.5 (dashed line). 4.
Reference: <author> Dougherty, J.; Kohavi, R.; and Sahami, M. </author> <year> 1995. </year> <title> Supervised and unsupervised discretization of continuous features. </title> <note> In ML '95. </note>
Reference-contexts: Currently we also do not handle continuous attributes. Instead, in each invocation of the learning routine, the dataset was pre-discretized using a variant of the method of (Fayyad & Irani 1993) using only the training data, in the manner described in <ref> (Dougherty, Kohavi, & Sahami 1995) </ref>. These preprocessing stages where carried out by the MLC++ system. We note that experiments with the various learning procedures were carried out on exactly the same training sets and evaluated on the same test sets.
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1993. </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In IJCAI '93, </booktitle> <pages> 1022-1027. </pages>
Reference-contexts: Since we do not deal, at the current time, with missing data we had removed instances with missing values from the datasets. Currently we also do not handle continuous attributes. Instead, in each invocation of the learning routine, the dataset was pre-discretized using a variant of the method of <ref> (Fayyad & Irani 1993) </ref> using only the training data, in the manner described in (Dougherty, Kohavi, & Sahami 1995). These preprocessing stages where carried out by the MLC++ system.
Reference: <author> Friedman, N., and Goldszmidt, M. </author> <year> 1996. </year> <title> Discretization of continuous attributes while learning Bayesian networks. </title> <note> In ML '96. </note>
Reference: <author> Geiger, D. </author> <year> 1992. </year> <title> An entropy-based learning algorithm of Bayesian conditional trees. </title> <booktitle> In UAI '92. </booktitle> <pages> 92-97. </pages>
Reference-contexts: Since we usually have that N &gt; log n, we get the resulting complexity. This result can be adapted to learn the maximum likelihood TAN structure. Theorem 4.2: <ref> (Geiger 1992) </ref> There is a procedure of time complexity O (n 2 N ) that constructs the TAN structure B T that maximize LL (B T jD).
Reference: <author> Heckerman, D.; Geiger, D.; and Chickering, D. M. </author> <year> 1995. </year> <title> Learning Bayesian networks: The combination of knowl-ege and statistical data. </title> <booktitle> Machine Learning 20 </booktitle> <pages> 197-243. </pages>
Reference-contexts: This phenomena is called overfitting, since the learned parameters match the training data, but have poor performance on test data. The two main scoring functions commonly used to learn Bayesian networks complement the log-likelihood score with additional terms to address this problem. These are the Bayesian scoring function <ref> (Heckerman, Geiger, & Chicker-ing 1995) </ref>, and the one based on minimal description length (MDL) (Lam & Bacchus 1994). <p> It is important to note that learning based on the MDL score is asymptotically correct: with probability 1 the learned distribution converges to the underlying distribution as the number of samples increases <ref> (Heckerman 1995) </ref>. Regarding the search process, in this paper we will rely on a greedy strategy for the obvious computational reasons. This procedure starts with the empty network and successively applies local operations that maximally improve the score and until a local minima is found. <p> In the full paper we describe results using other search methods (although the methods we examined so far did not lead to significant improvements.) 3 Bayesian Networks as Classifiers 3 There are some well-known connections between these two proposals <ref> (Heckerman 1995) </ref>. networks (solid line) to naive Bayes (dashed line). The horizontal axis lists the datasets, which are sorted so that the curves cross only once. The vertical axis measures fraction of test instances that were misclassified (i.e., prediction errors). Thus, the smaller the value, the better the accuracy. <p> We note that this use of Dirichlet priors is related to the class of Dirichlet priors described in <ref> (Heckerman, Geiger, & Chickering 1995) </ref>. 9 In our experiments we also tried smoothed version of naive Bayes. This did not lead to significant improvement over the unsmoothed naive Bayes.
Reference: <author> Heckerman, D. </author> <year> 1995. </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research. </institution>
Reference-contexts: This phenomena is called overfitting, since the learned parameters match the training data, but have poor performance on test data. The two main scoring functions commonly used to learn Bayesian networks complement the log-likelihood score with additional terms to address this problem. These are the Bayesian scoring function <ref> (Heckerman, Geiger, & Chicker-ing 1995) </ref>, and the one based on minimal description length (MDL) (Lam & Bacchus 1994). <p> It is important to note that learning based on the MDL score is asymptotically correct: with probability 1 the learned distribution converges to the underlying distribution as the number of samples increases <ref> (Heckerman 1995) </ref>. Regarding the search process, in this paper we will rely on a greedy strategy for the obvious computational reasons. This procedure starts with the empty network and successively applies local operations that maximally improve the score and until a local minima is found. <p> In the full paper we describe results using other search methods (although the methods we examined so far did not lead to significant improvements.) 3 Bayesian Networks as Classifiers 3 There are some well-known connections between these two proposals <ref> (Heckerman 1995) </ref>. networks (solid line) to naive Bayes (dashed line). The horizontal axis lists the datasets, which are sorted so that the curves cross only once. The vertical axis measures fraction of test instances that were misclassified (i.e., prediction errors). Thus, the smaller the value, the better the accuracy. <p> We note that this use of Dirichlet priors is related to the class of Dirichlet priors described in <ref> (Heckerman, Geiger, & Chickering 1995) </ref>. 9 In our experiments we also tried smoothed version of naive Bayes. This did not lead to significant improvement over the unsmoothed naive Bayes.
Reference: <author> John, G.; Kohavi, R.; and Pfleger, K. </author> <year> 1995. </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In ML '94. </booktitle> <pages> 121-129. </pages>
Reference-contexts: All of the datasets are from the U. C. Irvine repository (Murphy & Aha 1995), with the exception of mofn-3-7-10 and corral. These two artificial datasets were used for the evaluation of feature subset selection methods by <ref> (John, Kohavi, & Pfleger 1995) </ref>. All these datasets are accessible at the MLC++ ftp site. The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each dataset. <p> NBC-the naive Bayesian classifier; Unsup-unsupervised Bayesian networks learned using the MDL score; TANTAN net works learned according to Theorem 4.2; TAN s smoothed TAN networks; C4.5-the decision-tree classifier of (Quin lan 1993); SNBCthe selective naive Bayesian classifier, a wrapper-based feature selection applied to naive Bayes, us ing the implementation of <ref> (John, Kohavi, & Pfleger 1995) </ref>.
Reference: <author> Kohavi, R.; John, G.; Long, R.; Manley, D.; and Pfleger, K. </author> <year> 1994. </year> <title> MLC++: A machine learning library in C++. </title> <booktitle> In Tools with Artificial Intelligence. </booktitle> <pages> 740-743. </pages>
Reference-contexts: The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each dataset. We estimate the prediction accuracy for each classifier as well as the variance of this accuracy using the MLC++ system <ref> (Kohavi et al. 1994) </ref>. Accuracy was evaluated using the holdout method for the larger datasets, and using 5-fold cross validation (using the methods described in (Kohavi 1995)) for the smaller ones.
Reference: <author> Kohavi, R. </author> <year> 1995. </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In IJCAI '95. </booktitle> <pages> 1137-1143. </pages>
Reference-contexts: All of the datasets are from the U. C. Irvine repository (Murphy & Aha 1995), with the exception of mofn-3-7-10 and corral. These two artificial datasets were used for the evaluation of feature subset selection methods by <ref> (John, Kohavi, & Pfleger 1995) </ref>. All these datasets are accessible at the MLC++ ftp site. The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each dataset. <p> We estimate the prediction accuracy for each classifier as well as the variance of this accuracy using the MLC++ system (Kohavi et al. 1994). Accuracy was evaluated using the holdout method for the larger datasets, and using 5-fold cross validation (using the methods described in <ref> (Kohavi 1995) </ref>) for the smaller ones. Since we do not deal, at the current time, with missing data we had removed instances with missing values from the datasets. Currently we also do not handle continuous attributes. <p> Currently we also do not handle continuous attributes. Instead, in each invocation of the learning routine, the dataset was pre-discretized using a variant of the method of (Fayyad & Irani 1993) using only the training data, in the manner described in <ref> (Dougherty, Kohavi, & Sahami 1995) </ref>. These preprocessing stages where carried out by the MLC++ system. We note that experiments with the various learning procedures were carried out on exactly the same training sets and evaluated on the same test sets. <p> NBC-the naive Bayesian classifier; Unsup-unsupervised Bayesian networks learned using the MDL score; TANTAN net works learned according to Theorem 4.2; TAN s smoothed TAN networks; C4.5-the decision-tree classifier of (Quin lan 1993); SNBCthe selective naive Bayesian classifier, a wrapper-based feature selection applied to naive Bayes, us ing the implementation of <ref> (John, Kohavi, & Pfleger 1995) </ref>.
Reference: <author> Lam, W., and Bacchus, F. </author> <year> 1994. </year> <title> Learning Bayesian belief networks. An approach based on the MDL principle. </title> <booktitle> Computational Intelligence 10 </booktitle> <pages> 269-293. </pages>
Reference-contexts: The search process relies on a scoring metric that asses the merits of each candidate network. We start by examining a straightforward application of current Bayesian networks techniques. We learn networks using the MDL metric <ref> (Lam & Bacchus 1994) </ref> and use them for classification. The results, which are analyzed in Section 3, are mixed: although the learned networks perform significantly better than naive Bayes on some datasets, they perform worse on others. <p> The two main scoring functions commonly used to learn Bayesian networks complement the log-likelihood score with additional terms to address this problem. These are the Bayesian scoring function (Heckerman, Geiger, & Chicker-ing 1995), and the one based on minimal description length (MDL) <ref> (Lam & Bacchus 1994) </ref>. In this paper we concentrate on MDL deferring the discussion on the Bayesian scoring function for the full paper. 3 The motivation underlying the MDL method is to find a compact encoding of the training set D.
Reference: <author> Langley, P., and Sage, S. </author> <year> 1994. </year> <title> Induction of selective Bayesian classifiers. </title> <booktitle> In UAI '94. </booktitle> <pages> 399-406. </pages>
Reference: <author> Langley, P.; Iba, W.; and Thompson, K. </author> <year> 1992. </year> <title> An analysis of bayesian classifiers. </title> <booktitle> In AAAI '90. </booktitle> <pages> 223-228. </pages>
Reference-contexts: One of the most effective classifiers, in the sense that its predictive performance is competitive with state of the art classifiers such as C4.5 (Quinlan 1993), is the so-called naive Bayesian classifier (or simply naive Bayes) <ref> (Langley, Iba, & Thompson 1992) </ref>. This classifier learns the conditional probability of each attribute A i given the label C in the training data. <p> From the definition of conditional probability we get that Pr (CjA 1 ; : : : ; A n ) = ff Pr (C) Q n i=1 Pr (A i jC), where ff is a normalization constant. This is the definition of naive Bayes commonly found in the literature <ref> (Langley, Iba, & Thompson 1992) </ref>. The problem of learning a Bayesian network can be stated as follows. Given a training set D = fu 1 ; : : : ; u N g of instances of U, find a network B that best matches D.
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1995. </year> <note> UCI repository of machine learning databases. http://www.ics.uci. edu/mlearn/MLRepository.html. </note>
Reference-contexts: In this experiment we compared the predictive accuracy of classification using Bayesian networks learned in an unsupervised fashion, versus that of the naive Bayesian classifier. We run this experiment on 22 datasets, 20 of which are from the U. C. Irvine repository <ref> (Murphy & Aha 1995) </ref>. Appendix A describes in detail the experimental setup, evaluation meth ods, and results (Table 1). As can be seen from Figure 2 the classifier based on unsupervised networks performed significantly better than naive Bayes on 6 datasets, and performed significantly worse on 6 datasets. <p> The first author was also supported in part by an IBM Graduate fellowship and NSF Grant IRI-95-03109. A Experimental Methodology and Results We run our experiments on the 22 datasets listed in Table 1. All of the datasets are from the U. C. Irvine repository <ref> (Murphy & Aha 1995) </ref>, with the exception of mofn-3-7-10 and corral. These two artificial datasets were used for the evaluation of feature subset selection methods by (John, Kohavi, & Pfleger 1995). All these datasets are accessible at the MLC++ ftp site.
Reference: <author> Pazzani, M. J. </author> <year> 1995. </year> <title> Searching for dependencies in Bayesian classifiers. </title> <booktitle> In Proc. of the 5'th Int. Workshop on Artificial Intelligence and Statistics. </booktitle>
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This fact naturally begs the question of whether we can improve the performance of Bayesian classifiers by avoiding unrealistic assumptions about independence. In order to effectively tackle this problem we need an appropriate language and effective machinery to represent and manipulate independences. Bayesian networks <ref> (Pearl 1988) </ref> provide both. Bayesian networks are directed acyclic graphs that allow for efficient and effective representation of the joint probability distributions over a set of random variables. Each vertex in the graph represents a random variable, and edges represent direct correlations between the variables. <p> However, there is a restricted sub-class of these structures, for which we can find the best candidate in polynomial time. This result, shown by Geiger (1992), is a consequence of a well-known result by Chow and Liu (1968) (see also <ref> (Pearl 1988) </ref>). This approach, which we call Tree Augmented Naive Bayes (TAN), approximates the interactions between attributes using a tree-structure imposed on the naive Bayes structure. We note that while this method has been proposed in the literature, it has never been rigorously tested in practice. <p> The graph structure G encodes the following set of independence assumptions: each node X i is independent of its non-descendants given its parents in G <ref> (Pearl 1988) </ref>. 2 The second component of the pair, namely fi, represents the set of parameters that quantifies the network. <p> In the naive Bayes structure the class variable 2 Formally there is a notion of minimality associated with this definition, but we will ignore it in this paper (see <ref> (Pearl 1988) </ref>). is the root, i.e., C = ;, and the only parent for each attribute is the class variable, i.e., A i = fCg, for all 1 i n. <p> However, 4 More precisely, for a fixed network structure the Markov blanket of a variable X <ref> (Pearl 1988) </ref> consists of X's parents, X's children, and parents of X's children in G. <p> As we now show, we can exploit this restriction on the number of correlation edges to learn TAN models efficiently. This class of models was previously proposed by Geiger (1992), using a well-known method by Chow and Liu (1968), for learning tree-like Bayesian networks (see also <ref> (Pearl 1988, pp. 387-390) </ref>). 7 We start by reviewing Chow and Liu's result on learning trees.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One of the most effective classifiers, in the sense that its predictive performance is competitive with state of the art classifiers such as C4.5 <ref> (Quinlan 1993) </ref>, is the so-called naive Bayesian classifier (or simply naive Bayes) (Langley, Iba, & Thompson 1992). This classifier learns the conditional probability of each attribute A i given the label C in the training data. <p> This did not lead to significant improvement over the unsmoothed naive Bayes. Finally, we also compared TAN to C4.5 <ref> (Quinlan 1993) </ref>, a state of the art decision-tree learning system, and to the selective naive Bayesian classifier (Langley & Sage 1994; John, Kohavi, & Pfleger 1995). The later approach searches for the subset of attributes over which naive Bayes has the best performance.
Reference: <author> Singh, M., and Provan, G. M. </author> <year> 1995. </year> <title> A comparison of induction algorithms for selective and non-selective bayesian classifiers. </title> <note> In ML '95. </note>
References-found: 19

