URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/94-18.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Title: LINEAR TIME AND MEMORY-EFFICIENT COMPUTATION  
Author: KENNETH W. REGAN 
Keyword: Key Words. Computational complexity, theory of computation, machine models, Turing machines, random-access machines, simulation, memory hierarchies, finite automata, linear time, caching.  
Note: AMS(MOS) subject classification. 68Q05, 68Q15, 68Q25, 68Q68.  
Abstract: A realistic model of computation called the Block Move (BM) model is developed. The BM regards computation as a sequence of finite transductions in memory, and operations are timed according to a memory cost parameter . Unlike previous memory-cost models, the BM provides a rich theory of linear time, and in contrast to what is known for Turing machines, the BM is proved to be highly robust for linear time. Under a wide range of parameters, many forms of the BM model, ranging from a fixed-wordsize RAM down to a single finite automaton iterating itself on a single tape, are shown to simulate each other up to constant factors in running time. The BM is proved to enjoy efficient universal simulation, and to have a tight deterministic time hierarchy. Relationships among BM and TM time complexity classes are studied. 1. Introduction. This paper develops a new theory of linear-time computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, B. Alpern, A. Chandra, and M. Snir, </author> <title> A model for hierarchical memory, </title> <booktitle> in Proc. 19th Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1987, </year> <pages> pp. 305-314. </pages>
Reference-contexts: This is a reasonable reflection of how pipelining can hide memory latency, and accords with the behavior of physical memory devices (see [3], p1117, or [34], p 214). An earlier paper <ref> [1] </ref> studied a model called HMM which lacked the block-transfer construct.
Reference: [2] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> Hierarchical memory with block transfer, </title> <booktitle> in Proc. 28th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1987, </year> <pages> pp. 204-216. </pages>
Reference-contexts: 1. Introduction. This paper develops a new theory of linear-time computation. The Block Move (BM) model introduced here extends ideas and formalism from the Block Transfer (BT) model of Aggarwal, Chandra, and Snir <ref> [2] </ref>. The BT is a random access machine (RAM) with a special block transfer operation, together with a parameter : N ! N called a memory access cost function. The RAM's registers are indexed 0,1,2,: : : , and (a) denotes the cost of accessing register a. <p> In the BT model, Aggarwal, Chandra, and Snir <ref> [2] </ref> proved tight nonlinear lower bounds of fi [n log n] with = 1 , fi [n log log n] with = d , d &gt; 1, and fi [n log fl n] with = log , for the so-called "Touch Problem" of executing a sequence of operations during which every <p> Block moves are timed as above. Both a RAM-TM step and a block move add 1 to the pass count R (n). Other details of computations are the same as for the basic BM model. A fixed-wordsize analogue of the original BT model of <ref> [2] </ref> can now be had by making copy the only GST allowed in block moves. A RAM-BM with address loading can use block moves rather than RAM-TM steps to write addresses. Definition 3.6. <p> It is interesting to ask whether the above can be extended to a linear simulation of a concatenable buffer (cf. [46]), but this appears to run into problems related to the nonlinear lower bounds for the Touch Problem in <ref> [2] </ref>. The proof gives w 0 (n) = O (w (n) + n) and R 0 (n) = O (R (n) log s (n)). For -acc 0 , the charges in the rightward moves are bounded by a constant times P log b j=0 (b=2 j ). <p> A related question is whether every language in TLIN, with or without fixed output delay, has linear-sized circuits. Further avenues for research include analyzing implementations of certain important algorithms on the BM, as done for the BT and UMH in <ref> [2, 5] </ref>. Here the BM is helped by its proximity to the Pratt-Stockmeyer vector machine, since conserving memory-access charges and parallel time often lead to similar methods. <p> Conclusion. In common with motivations expressed in <ref> [2] </ref> and [5], the BM model fosters a finer analysis of many theoretical algorithms in terms of how they use memory, and how they really behave in running time when certain practicalities 34 of implementation are taken into account. <p> Acknowledgments.. I would like to thank Professor Michael C. Loui for comments on preliminary drafts of this work, and for bringing <ref> [2] </ref> and [5] to my attention. Professors Pierre McKenzie and Gilles Brassard gave me a helpful early opportunity to test these ideas at an open forum in a University of Montreal seminar.
Reference: [3] <author> A. Aggarwal and J. Vitter, </author> <title> The input-output complexity of sorting and related problems, </title> <journal> Comm. ACM, </journal> <volume> 31 (1988), </volume> <pages> pp. 1116-1127. </pages>
Reference-contexts: This is a reasonable reflection of how pipelining can hide memory latency, and accords with the behavior of physical memory devices (see <ref> [3] </ref>, p1117, or [34], p 214). An earlier paper [1] studied a model called HMM which lacked the block-transfer construct. <p> da 1=d e with d = 1; 2; 3; : : : ; which model the asymptotic increase in communication time for memory laid out on a d-dimensional grid. (The cited papers write f in place of and ff for 1=d.) The two-level I/O complexity model of Aggarwal and Vitter <ref> [3] </ref> has fixed block-size and a fixed cost for accessing the outer level, while the Uniform Memory Hierarchy (UMH) model of Alpern, Carter, and Feig [5] scales block-size and memory access cost upward in steps at higher levels. The BM makes the following changes to the BT. <p> The longstanding program of showing nonlinear lower bounds in reasonable models of computation has progressed up to machines apparently just below the BM (under 1 ) in power, so that attacking the problems given here seems a logical next step. The authors of <ref> [3] </ref> refer to the "challenging open problem" of extending their results when bit-manipulations for dissecting records are available. The bit operations given to the BM seem to be an appropriate setting for this problem.
Reference: [4] <author> N. Alon and W. Maass, </author> <title> Meanders and their application to lower bound arguments, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 37 (1988), </volume> <pages> pp. 118-129. 35 </pages>
Reference-contexts: That of (b) is proved along the lines of Proposition 4.3. The reverse implications are immediate, and all this needs only the tracking property of . Alon and Maass <ref> [4] </ref> prove substantial time-space tradeoffs for the related "sequence equality" problem SE [n]: given x; y 2 f 0; 1; 2 g n , does Er 2 (x) = Er 2 (y)? We inquire whether their techniques, or those of [54], can be adapted to the BM.
Reference: [5] <author> B. Alpern, L. Carter, and E. Feig, </author> <title> Uniform memory hierarchies, </title> <booktitle> in Proc. 31st Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1990, </year> <pages> pp. 600-608. </pages>
Reference-contexts: on a d-dimensional grid. (The cited papers write f in place of and ff for 1=d.) The two-level I/O complexity model of Aggarwal and Vitter [3] has fixed block-size and a fixed cost for accessing the outer level, while the Uniform Memory Hierarchy (UMH) model of Alpern, Carter, and Feig <ref> [5] </ref> scales block-size and memory access cost upward in steps at higher levels. The BM makes the following changes to the BT. First, the BM fixes the wordsize of the underlying machine, so that registers are essentially the same as cells on a Turing tape. <p> The -time is the sum of these two numbers. Adopting terms from <ref> [5] </ref>, we call a BM M memory-efficient if the total memory access charges stay within a constant factor (depending only on M ) of the work performed, and parsimonious if the ratio of access charges to work approaches 0 as the input length n increases. <p> We argue that languages and functions in TLIN have true linear-time behavior even under the most constrained implementations. We do not separate out the work performed from the total memory access charges in defining BM complexity classes, but do so in adapting the following notions and terms from <ref> [5] </ref> to the BM model. <p> Equivalently, M is memory efficient under if -acc (M; x) = O (w), and parsimonious under if -acc (M; x) = o (w), where the asymptotics are as jxj ! 1. The intuition, also expressed in <ref> [5] </ref>, is that efficient or parsimonious programs make good use of a memory cache. Definition 2.9 does not imply that the given BM M is optimal for the function f it computes. <p> A related question is whether every language in TLIN, with or without fixed output delay, has linear-sized circuits. Further avenues for research include analyzing implementations of certain important algorithms on the BM, as done for the BT and UMH in <ref> [2, 5] </ref>. Here the BM is helped by its proximity to the Pratt-Stockmeyer vector machine, since conserving memory-access charges and parallel time often lead to similar methods. <p> Conclusion. In common with motivations expressed in [2] and <ref> [5] </ref>, the BM model fosters a finer analysis of many theoretical algorithms in terms of how they use memory, and how they really behave in running time when certain practicalities 34 of implementation are taken into account. <p> Acknowledgments.. I would like to thank Professor Michael C. Loui for comments on preliminary drafts of this work, and for bringing [2] and <ref> [5] </ref> to my attention. Professors Pierre McKenzie and Gilles Brassard gave me a helpful early opportunity to test these ideas at an open forum in a University of Montreal seminar.
Reference: [6] <author> J. Balc azar, J. D az, and J. Gabarr o, </author> <title> Structural Complexity Theory, </title> <publisher> Springer Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Proof. (a) Let r be the element size of the normal list ~x . If jyj 6= r, then there is nothing to do. Else, the BM M uses the idea of "recursive doubling" (cf. the section on vector machines in <ref> [6] </ref>) to produce y k , where k = dlog 2 me. This time is linear as a function of n = rm.
Reference: [7] <author> D. M. Barrington, N. Immerman, and H. Straubing, </author> <booktitle> On uniformity within NC 1 , in Proc. 3rd Annual IEEE Conference on Structure in Complexity Theory, </booktitle> <year> 1988, </year> <pages> pp. 47-59. </pages> <note> [8] , On uniformity within NC 1 , J. Comp. Sys. Sci., </note> <month> 41 </month> <year> (1990), </year> <pages> pp. 274-306. </pages>
Reference-contexts: In 3D space, real machines "should have" at most cubic vicinity. The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in <ref> [30, 26, 7, 14, 64] </ref>, and TMs with tree-structured tapes (see [57, 63, 51, 52]). Turing machines with d-dimensional tapes (see [31, 60, 50]) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed.
Reference: [9] <author> A. Ben-Amram and Z. Galil, </author> <title> Lower bounds for data structure problems on RAMs, </title> <booktitle> in Proc. 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1991, </year> <pages> pp. </pages> <month> 622-631. </month> <title> [10] , On pointers versus addresses, </title> <journal> J. ACM, </journal> <volume> 39 (1992), </volume> <pages> pp. 617-648. </pages>
Reference-contexts: Rather than define a form of the BM analogous to the pointer machines of Schonhage and others [45, 66, 67, 49, 10], we move straight to a model that uses "random-access addressing," a mechanism usually considered stronger than pointers (for in-depth comparisons, see <ref> [9, 10] </ref> and also [68]). The following BM form is based on a random-access Turing machine (RAM-TM; cf. "RTM" in [30] and "indexing TM" in [14, 64, 8]), and is closest to the BT. Definition 3.5.
Reference: [11] <author> G. Blelloch, </author> <title> Vector Models for Data-Parallel Computing, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The latter also serves as a measure of parallel time, since finite transductions can be computed by parallel prefix sum. Indeed, the BM is similar to the Pratt-Stockmeyer vector machine [61], and can also be regarded as a fixed-wordsize analogue of Blelloch's "scan" model <ref> [11] </ref>. Results. The first main theorem is that the BM is a very robust model. Many diverse forms of the machine simulate each other up to constant factors in -time, under a wide range of cost functions .
Reference: [12] <author> M. Blum, </author> <title> A machine-independent theory of the complexity of recursive functions, </title> <journal> J. ACM, </journal> <volume> 14 (1967), </volume> <pages> pp. 322-336. </pages>
Reference-contexts: The intuition, also expressed in [5], is that efficient or parsimonious programs make good use of a memory cache. Definition 2.9 does not imply that the given BM M is optimal for the function f it computes. Indeed, from Blum's speed-up theorem <ref> [12] </ref> and the fact that time is a complexity measure, there exist computable functions with no time optimal programs at all.
Reference: [13] <author> R. Book and S. </author> <title> Greibach, </title> <journal> Quasi-realtime languages, Math. Sys. Thy., </journal> <volume> 4 (1970), </volume> <pages> pp. 97-111. </pages>
Reference-contexts: For the DTM, time O (n) properly contains time n+1, while for the NTM these are equal <ref> [13] </ref>. For the BM under cost function , the O (n) term is n + (n). Theorem 5.1. With respect to any unbounded memory cost function that has the tape compression property, all of the BM variants described in Sections 2 and 3 have the linear speed-up property. Proof.
Reference: [14] <author> A. Chandra, D. Kozen, and L. Stockmeyer, </author> <title> Alternation, </title> <journal> J. ACM, </journal> <volume> 28 (1981), </volume> <pages> pp. 114-133. </pages>
Reference-contexts: In 3D space, real machines "should have" at most cubic vicinity. The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in <ref> [30, 26, 7, 14, 64] </ref>, and TMs with tree-structured tapes (see [57, 63, 51, 52]). Turing machines with d-dimensional tapes (see [31, 60, 50]) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed. <p> The following BM form is based on a random-access Turing machine (RAM-TM; cf. "RTM" in [30] and "indexing TM" in <ref> [14, 64, 8] </ref>), and is closest to the BT. Definition 3.5. A RAM-BM has one main tape, four address tapes labeled a 1 ; b 1 ; a 2 ; b 2 and given their own heads, and a finite control comprised of RAM-TM states and GST states.
Reference: [15] <author> J. Chang, O. Ibarra, and A. Vergis, </author> <title> On the power of one-way communication, </title> <journal> J. ACM, </journal> <volume> 35 (1988), </volume> <pages> pp. 697-726. </pages>
Reference-contexts: Each is really a GST that iterates on its own output, a form generally known as a "cascading finite automaton" (CFA). Up to small technical differences, CFAs are comparable to the one-way "sweeping automata" studied by Ibarra et.al. <ref> [39, 41, 40, 37, 38, 15] </ref>. These papers characterize both one-way and two-way arrays of identical finite-state machines in terms of these and other automata and language classes.
Reference: [16] <author> J. Chen and C. Yap, </author> <title> Reversal complexity, </title> <journal> SIAM J. Comput., </journal> <volume> 20 (1991), </volume> <pages> pp. 622-638. </pages>
Reference-contexts: The pass count appears to be sandwiched between two measures of reversals for multitape Turing machines, namely the now-standard one of <ref> [59, 35, 16] </ref>, and the stricter notion of [43] which essentially counts keeping a TM head stationary as a reversal. Definition 2.8. <p> The size of the list is m, while the bit-length of the list is n := P m i=1 jx i j. We let r stand for maxf jx i j : 1 i m g. Following <ref> [16] </ref> we call the list normal if the strings x i all have length r. We number lists beginning with x 1 to emphasize that the x i are not characters.
Reference: [17] <author> M. Conner, </author> <title> Sequential machines realized by group representations, </title> <journal> Info. Comp., </journal> <volume> 85 (1990), </volume> <pages> pp. 183-201. </pages>
Reference-contexts: theorems for finite transducers [47, 32, 48], we could restrict attention to the cases where each g c either is the identity on Q or identifies two states (a "reset machine") or each g c is a permutation of Q and M S is a group (a "permutation machine"; cf. <ref> [17] </ref>). These points do not matter here. We encode each state in Q as a binary string of some fixed length k, and encode each element g of M S by the list q#g (q)# : : : over all q 2 Q.
Reference: [18] <author> S. Cook and R. Reckhow, </author> <title> Time bounded random access machines, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 7 (1973), </volume> <pages> pp. 354-375. </pages>
Reference-contexts: Even under the highest cost function 1 that we consider, many interesting nonregular languages and functions are computable in linear time. Previous models. It has long been realized that the standard unit-cost RAM model <ref> [21, 31, 18] </ref> is too powerful for many practical purposes. <p> Then the model M has vicinity v (t) if for all C and t, jI t j=jH C j v (t). In 3D space, real machines "should have" at most cubic vicinity. The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow <ref> [18] </ref>. So do the random-access Turing machine (RAM-TM) forms described in [30, 26, 7, 14, 64], and TMs with tree-structured tapes (see [57, 63, 51, 52]).
Reference: [19] <author> D. Coppersmith and S. Winograd, </author> <title> Matrix multiplication via arithmetical progressions, </title> <journal> J. Symbolic Computation, </journal> <volume> 9 (1990), </volume> <pages> pp. 251-280. </pages>
Reference-contexts: L is not believed to be solvable in linear time on a RAM at all. The best method known involves computing A 2 + A, and squaring n fi n integer matrices takes time approximately N 1:188 , where N = n 2 , by the methods of <ref> [19] </ref>. (For directed triangles, cubing A is the best way known.) Open Problem 1. Do any of the above languages belong to TLIN? If not, prove nonlinear lower bounds.
Reference: [20] <author> M. Dietzfelbinger, W. Maass, and G. Schnitger, </author> <title> The complexity of matrix transposition on one-tape off-line Turing machines, </title> <journal> Theor. Comp. Sci., </journal> <volume> 82 (1991), </volume> <pages> pp. 113-129. </pages>
Reference-contexts: In general, we advance the BM as a logical next step in the longstanding program of proving nonlinear lower bounds for natural models of computation. In particular, we ask whether the techniques used by Dietzfelbinger, Maass, and Schnitger <ref> [20] </ref> to obtain lower bounds for Boolean matrix transpose and several sorting-related functions on a certain restricted two-tape TM can be applied to the differently-restricted kind of two-tape TM in Theorems 7.1 and 7.3.
Reference: [21] <author> C. Elgot and A. Robinson, </author> <title> Random-access stored-program machines, </title> <journal> J. ACM, </journal> <volume> 11 (1964), </volume> <pages> pp. 365-399. </pages>
Reference-contexts: Even under the highest cost function 1 that we consider, many interesting nonregular languages and functions are computable in linear time. Previous models. It has long been realized that the standard unit-cost RAM model <ref> [21, 31, 18] </ref> is too powerful for many practical purposes.
Reference: [22] <author> Y. Feldman and E. Shapiro, </author> <title> Spatial machines: A more-realistic approach to parallel compu tation, </title> <journal> Comm. ACM, </journal> <volume> 35 (1992), </volume> <pages> pp. 60-73. </pages>
Reference-contexts: Even under the highest cost function 1 that we consider, many interesting nonregular languages and functions are computable in linear time. Previous models. It has long been realized that the standard unit-cost RAM model [21, 31, 18] is too powerful for many practical purposes. Feldman and Shapiro <ref> [22] </ref> contend that realistic models M, both sequential and parallel, should have a property they call "polynomial vicinity" which we state as follows: Let C be a data configuration, and let H C stand for the finite set of memory locations [or data items] designated as "scanned" in C.
Reference: [23] <author> P. Fischer, A. Meyer, and A. Rosenberg, </author> <title> Real-time simulations of multihead tape units, </title> <journal> J. ACM, </journal> <volume> 19 (1972), </volume> <pages> pp. 590-607. </pages>
Reference-contexts: # : : : #x m , y 1 # : : : #y m : (9i; j) x i = y j g: (d) Triangle: L = fA : A is the adjacency matrix of an undirected graph that contains a triangleg. 31 L pat belongs to DLIN (see <ref> [25, 23] </ref>), and was recently shown not to be solvable by a one-way non-sensing multihead DFA [42]. L dup and L int can be solved in linear time by a RAM or RAM-TM that treats list elements as cell addresses.
Reference: [24] <author> M. F urer, </author> <title> Data structures for distributed counting, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 29 (1984), </volume> <pages> pp. 231-243. </pages>
Reference-contexts: The linear speedup theorem and some results on memory-efficiency are in Section 5. The second main result of this paper, in Section 6, shows that like the RAM but unlike what is known for the standard multitape Turing machine model (see <ref> [36, 24] </ref>), the BM carries only a constant factor overhead for universal simulation. The universal BM given is efficient under any d , while separate constructions work for log .
Reference: [25] <author> Z. Galil and J. Seiferas, </author> <title> Time-space optimal string matching, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 26 (1983), </volume> <pages> pp. 280-294. </pages>
Reference-contexts: # : : : #x m , y 1 # : : : #y m : (9i; j) x i = y j g: (d) Triangle: L = fA : A is the adjacency matrix of an undirected graph that contains a triangleg. 31 L pat belongs to DLIN (see <ref> [25, 23] </ref>), and was recently shown not to be solvable by a one-way non-sensing multihead DFA [42]. L dup and L int can be solved in linear time by a RAM or RAM-TM that treats list elements as cell addresses.
Reference: [26] <author> E. Graedel, </author> <title> On the notion of linear-time computability, </title> <journal> International Journal of Foundations of Computer Science, </journal> <volume> 1 (1990), </volume> <pages> pp. 295-307. </pages>
Reference-contexts: In 3D space, real machines "should have" at most cubic vicinity. The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in <ref> [30, 26, 7, 14, 64] </ref>, and TMs with tree-structured tapes (see [57, 63, 51, 52]). Turing machines with d-dimensional tapes (see [31, 60, 50]) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed.
Reference: [27] <author> E. Grandjean, </author> <title> A natural NP-complete problem with a nontrivial lower bound, </title> <journal> SIAM J. Com put., </journal> <volume> 17 (1988), </volume> <pages> pp. </pages> <month> 786-809. </month> <title> [28] , A nontrivial lower bound for an NP problem on automata, </title> <journal> SIAM J. Comput., </journal> <volume> 19 (1990), </volume> <pages> pp. 438-451. </pages>
Reference-contexts: Then all four of the above languages belong to NTLIN. Moreover, they require only O (log n) bits of nondeterminism. Open Problem 2. Is NTLIN 6= TLIN? For reasonable and time bounds t, is there a general separation of NTIME [t (n)] from DTIME [t (n)]? Grandjean <ref> [27, 28] </ref> shows that a few NP-complete languages are also hard for NLIN under TM linear time reductions, and hence by the theorem of [56] lie outside DLIN, not to mention TLIN.
Reference: [29] <author> E. Grandjean and J. Robson, </author> <title> RAM with compact memory: a robust and realistic model of computation, </title> <booktitle> in CSL '90: Proceedings of the 4th Annual Workshop in Computer Science Logic, vol. 533 of Lect. Notes in Comp. </booktitle> <publisher> Sci., Springer Verlag, </publisher> <year> 1991, </year> <pages> pp. 195-233. </pages>
Reference-contexts: The standard TM model, with d = 1, has linear vicinity. The "RAM with polynomially compact memory" of Grandjean and Robson <ref> [29] </ref> limits integers i that can be stored and registers a that can be used to a polynomial in the running time T .
Reference: [30] <author> Y. Gurevich and S. Shelah, </author> <title> Nearly-linear time, </title> <booktitle> in Proceedings, Logic at Botik '89, vol. 363 of Lect. Notes in Comp. </booktitle> <publisher> Sci., Springer Verlag, </publisher> <year> 1989, </year> <pages> pp. 108-118. </pages>
Reference-contexts: In 3D space, real machines "should have" at most cubic vicinity. The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in <ref> [30, 26, 7, 14, 64] </ref>, and TMs with tree-structured tapes (see [57, 63, 51, 52]). Turing machines with d-dimensional tapes (see [31, 60, 50]) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed. <p> The following BM form is based on a random-access Turing machine (RAM-TM; cf. "RTM" in <ref> [30] </ref> and "indexing TM" in [14, 64, 8]), and is closest to the BT. Definition 3.5.
Reference: [31] <author> J. Hartmanis and R. Stearns, </author> <title> On the computational complexity of algorithms, </title> <journal> Transactions of the AMS, </journal> <volume> 117 (1965), </volume> <pages> pp. </pages> <month> 285-306. </month> <title> [32] , Algebraic Structure Theory of Sequential Machines, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1966. </year>
Reference-contexts: Even under the highest cost function 1 that we consider, many interesting nonregular languages and functions are computable in linear time. Previous models. It has long been realized that the standard unit-cost RAM model <ref> [21, 31, 18] </ref> is too powerful for many practical purposes. <p> The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in [30, 26, 7, 14, 64], and TMs with tree-structured tapes (see [57, 63, 51, 52]). Turing machines with d-dimensional tapes (see <ref> [31, 60, 50] </ref>) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed. The standard TM model, with d = 1, has linear vicinity.
Reference: [33] <author> F. Hennie and R. Stearns, </author> <title> Two-way simulation of multitape Turing machines, </title> <journal> J. ACM, </journal> <volume> 13 (1966), </volume> <pages> pp. 533-546. </pages>
Reference-contexts: A similar statement holds for the perhaps-larger log -time classes for the BM variants that do use addressing. 7. Complexity Theory and the BM Model. Our first result shows that the construction in the Hennie-Stearns theorem <ref> [33] </ref>, which states that any multitape TM that runs in time t (n) can be simulated by a 2-tape TM in time t (n) log t (n), is memory-efficient on the BM under 1 . It has been observed in general that this construction is an efficient caching strategy.
Reference: [34] <author> T. Heywood and S. Ranka, </author> <title> A practical hierarchical model of parallel computation I: The model, </title> <journal> J. Par. Dist. Comp., </journal> <volume> 16 (1992), </volume> <pages> pp. 212-232. </pages>
Reference-contexts: This is a reasonable reflection of how pipelining can hide memory latency, and accords with the behavior of physical memory devices (see [3], p1117, or <ref> [34] </ref>, p 214). An earlier paper [1] studied a model called HMM which lacked the block-transfer construct.
Reference: [35] <author> J.-W. Hong, </author> <title> On similarity and duality of computation I, </title> <journal> Info. Comp., </journal> <volume> 62 (1985), </volume> <pages> pp. 109-128. </pages>
Reference-contexts: The pass count appears to be sandwiched between two measures of reversals for multitape Turing machines, namely the now-standard one of <ref> [59, 35, 16] </ref>, and the stricter notion of [43] which essentially counts keeping a TM head stationary as a reversal. Definition 2.8.
Reference: [36] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Automata Theory, Languages, </title> <editor> and Computa tion, </editor> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: The linear speedup theorem and some results on memory-efficiency are in Section 5. The second main result of this paper, in Section 6, shows that like the RAM but unlike what is known for the standard multitape Turing machine model (see <ref> [36, 24] </ref>), the BM carries only a constant factor overhead for universal simulation. The universal BM given is efficient under any d , while separate constructions work for log . <p> We use for the empty string and B for the blank character. N stands for f 0; 1; 2; 3; : : :g. Characters in a string x of length m are numbered x 0 x 1 x m1 . We modify the generalized sequential machine (GSM) of <ref> [36] </ref> so that it can exit without reading all of its input. Definition 2.1. <p> Without loss of generality, B is an endmarker for all GSTs. The intuitive picture of our model is a "circuit board" with GST "chips," each of which can process streams of data drawn from a single tape. The formalism is fairly close to that for Turing machines in <ref> [36] </ref>. Definition 2.2. <p> The -time for this iteration is likewise O (n) even for = 1 . Example 2.4. Simulating a TM. Let T := (Q; ; ; ffi; B; q 0 ; F ) be a single-tape TM in the notation of <ref> [36] </ref>. Define the ID alphabet of T to be I := (Q fi ) [ [ f ^; $ g, where ^; $ =2 . <p> Having b 1 fixed would let us use the original GSM model from <ref> [36] </ref>. However, the machines that follow are always able to drop an endmarker into cell b 1 and force a GST S to read all of [a 1 : : : b 1 ]. Hence we may ignore the distinction and retain `GST' for consistency. 10 Definition 3.3. <p> For any rational d 1, all forms of the BM defined above simulate each other linearly in d -time. If we adapted a standard convention for Turing machines to state that every BM on a given input x takes time at least jxj + 1 (cf. <ref> [36] </ref>), then we could say that all the simulations have constant-factor overheads in d -time. 4. Proof of the Main Robustness Theorem. <p> To balance the charges we take e := (a 0 ). This requires M 0 to calculate (a) dynamically during its computation, and involves a concept of "time-constructible function" similar to that defined for Turing machines in <ref> [36] </ref>. Definition 4.2. Let be a memory cost function, and let t : N ! N be any function. Then t is -time constructible if t (n) is computable in binary notation by a BM M in -time O (t (n)). <p> of computation and measure of time complexity states that for every machine M with running time t (n), and every * &gt; 0, there is a machine M 0 that simulates M and runs in time * t (n) + O (n): In the corresponding definition for Turing machines in <ref> [36] </ref>, the additive O (n) term is n+1 and is used to read the input. For the DTM, time O (n) properly contains time n+1, while for the NTM these are equal [13]. For the BM under cost function , the O (n) term is n + (n). Theorem 5.1. <p> DTIME [t (n)] refers to TM time, and DLIN stands for DTIME [O (n)]: Theorem 7.1. For any time function t, DTIME [t (n)] D 1 TIME [t (n) log t (n)]. 29 Proof. With reference to the treatment in <ref> [36] </ref>, let M 1 be a multitape TM with alphabet that runs in time t (n), and let M 2 be the two-tape TM in the proof.
Reference: [37] <author> O. Ibarra, </author> <title> Systolic arrays: characterizations and complexity, </title> <booktitle> in The Proceedings of the 1986 Conference on Mathematical Foundations of Computer Science, Lecture Notes in Computer Science No. </booktitle> <volume> 233, </volume> <publisher> Springer Verlag, </publisher> <year> 1986, </year> <pages> pp. 140-153. 36 </pages>
Reference-contexts: Each is really a GST that iterates on its own output, a form generally known as a "cascading finite automaton" (CFA). Up to small technical differences, CFAs are comparable to the one-way "sweeping automata" studied by Ibarra et.al. <ref> [39, 41, 40, 37, 38, 15] </ref>. These papers characterize both one-way and two-way arrays of identical finite-state machines in terms of these and other automata and language classes.
Reference: [38] <author> O. Ibarra and T. Jiang, </author> <title> On one-way cellular arrays, </title> <journal> SIAM J. Comput., </journal> <volume> 16 (1987), </volume> <pages> pp. 1135 1153. </pages>
Reference-contexts: Each is really a GST that iterates on its own output, a form generally known as a "cascading finite automaton" (CFA). Up to small technical differences, CFAs are comparable to the one-way "sweeping automata" studied by Ibarra et.al. <ref> [39, 41, 40, 37, 38, 15] </ref>. These papers characterize both one-way and two-way arrays of identical finite-state machines in terms of these and other automata and language classes.
Reference: [39] <author> O. Ibarra and S. Kim, </author> <title> Characterizations and computational complexity of systolic trellis automata, </title> <journal> Theor. Comp. Sci., </journal> <volume> 29 (1984), </volume> <pages> pp. 123-153. </pages>
Reference-contexts: Each is really a GST that iterates on its own output, a form generally known as a "cascading finite automaton" (CFA). Up to small technical differences, CFAs are comparable to the one-way "sweeping automata" studied by Ibarra et.al. <ref> [39, 41, 40, 37, 38, 15] </ref>. These papers characterize both one-way and two-way arrays of identical finite-state machines in terms of these and other automata and language classes.
Reference: [40] <author> O. Ibarra, S. Kim, and M. Palis, </author> <title> Designing systolic algorithms using sequential machines, </title> <journal> IEEE Transactions on Computing, </journal> <volume> 35 (1986), </volume> <pages> pp. 531-542. </pages>
Reference-contexts: Each is really a GST that iterates on its own output, a form generally known as a "cascading finite automaton" (CFA). Up to small technical differences, CFAs are comparable to the one-way "sweeping automata" studied by Ibarra et.al. <ref> [39, 41, 40, 37, 38, 15] </ref>. These papers characterize both one-way and two-way arrays of identical finite-state machines in terms of these and other automata and language classes.
Reference: [41] <author> O. Ibarra, M. Palis, and S. Kim, </author> <title> Some results concerning linear iterative (systolic) arrays, </title> <journal> J. Par. Dist. Comp., </journal> <volume> 2 (1985), </volume> <pages> pp. 182-218. </pages>
Reference-contexts: Each is really a GST that iterates on its own output, a form generally known as a "cascading finite automaton" (CFA). Up to small technical differences, CFAs are comparable to the one-way "sweeping automata" studied by Ibarra et.al. <ref> [39, 41, 40, 37, 38, 15] </ref>. These papers characterize both one-way and two-way arrays of identical finite-state machines in terms of these and other automata and language classes.
Reference: [42] <author> T. Jiang and M. Li, </author> <title> K one-way heads cannot do string-matching, </title> <booktitle> in Proc. 25th Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1993, </year> <pages> pp. 62-70. </pages>
Reference-contexts: (9i; j) x i = y j g: (d) Triangle: L = fA : A is the adjacency matrix of an undirected graph that contains a triangleg. 31 L pat belongs to DLIN (see [25, 23]), and was recently shown not to be solvable by a one-way non-sensing multihead DFA <ref> [42] </ref>. L dup and L int can be solved in linear time by a RAM or RAM-TM that treats list elements as cell addresses. L is not believed to be solvable in linear time on a RAM at all.
Reference: [43] <author> T. Kameda and R. Vollmar, </author> <title> Note on tape reversal complexity of languages, </title> <journal> Info. Control, </journal> <volume> 17 (1970), </volume> <pages> pp. 203-215. </pages>
Reference-contexts: The pass count appears to be sandwiched between two measures of reversals for multitape Turing machines, namely the now-standard one of [59, 35, 16], and the stricter notion of <ref> [43] </ref> which essentially counts keeping a TM head stationary as a reversal. Definition 2.8.
Reference: [44] <author> J. Katajainen, J. van Leeuwen, and M. Penttonen, </author> <title> Fast simulation of Turing machines by random access machines, </title> <journal> SIAM J. Comput., </journal> <volume> 17 (1988), </volume> <pages> pp. 77-88. </pages>
Reference-contexts: Proof. Straightforward simulations give these bounds. (The extra log t (n) factor in (b) dominates a factor of log log n that was observed by <ref> [44] </ref> for the simulation of a TM (or RAM-TM) by a log-cost RAM.) For quasilinear time, i.e. time qlin = n (log n) O (1) , the extra log n factors in Theorem 7.1 and Proposition 7.6 do not matter.
Reference: [45] <author> A. Kolmogorov and V. Uspenskii, </author> <title> On the definition of an algorithm, </title> <journal> Uspekhi Mat. Nauk, </journal> <volume> 13 (1958), </volume> <pages> pp. 3-28. </pages> <note> English transl. in Russian Math Surveys 30 (1963) 217-245. </note>
Reference-contexts: One classical difference between "fingers" and "pointers" is that there is no fixed limit on the number of pointers a program can create. Rather than define a form of the BM analogous to the pointer machines of Schonhage and others <ref> [45, 66, 67, 49, 10] </ref>, we move straight to a model that uses "random-access addressing," a mechanism usually considered stronger than pointers (for in-depth comparisons, see [9, 10] and also [68]).
Reference: [46] <author> R. Kosaraju, </author> <title> Real-time simulation of concatenable double-ended queues by double-ended queues, </title> <booktitle> in Proc. 11th Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1979, </year> <pages> pp. 346-351. </pages>
Reference-contexts: This completes the proof. The converse simulation of a BM by a BM with buffer is clear and has constant-factor overheads in all measures, by remarks following Definition 3.1. It is interesting to ask whether the above can be extended to a linear simulation of a concatenable buffer (cf. <ref> [46] </ref>), but this appears to run into problems related to the nonlinear lower bounds for the Touch Problem in [2]. The proof gives w 0 (n) = O (w (n) + n) and R 0 (n) = O (R (n) log s (n)).
Reference: [47] <author> K. Krohn and J. Rhodes, </author> <title> Algebraic theory of machines. I. prime decomposition theorem for finite semigroups and machines, </title> <journal> Trans. AMS, </journal> <volume> 116 (1965), </volume> <pages> pp. 450-464. </pages>
Reference-contexts: We also remark that M S need not contain the identity mapping on Q, though it does no harm for us to adjoin it. By using known decomposition theorems for finite transducers <ref> [47, 32, 48] </ref>, we could restrict attention to the cases where each g c either is the identity on Q or identifies two states (a "reset machine") or each g c is a permutation of Q and M S is a group (a "permutation machine"; cf. [17]).
Reference: [48] <author> K. Krohn, J. Rhodes, and B. Tilson, </author> <title> The prime decomposition theorem of the algebraic the ory of machines, in Algebraic Theory of Machines, Languages, and Semigroups, </title> <editor> M. Arbib, ed., </editor> <publisher> Academic Press, </publisher> <year> 1968. </year> <note> Ch. 5; see also chs. 4 and 6-9. </note>
Reference-contexts: We also remark that M S need not contain the identity mapping on Q, though it does no harm for us to adjoin it. By using known decomposition theorems for finite transducers <ref> [47, 32, 48] </ref>, we could restrict attention to the cases where each g c either is the identity on Q or identifies two states (a "reset machine") or each g c is a permutation of Q and M S is a group (a "permutation machine"; cf. [17]).
Reference: [49] <author> D. Leivant, </author> <title> Descriptive characterizations of computational complexity, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 39 (1989), </volume> <pages> pp. 51-83. </pages>
Reference-contexts: One classical difference between "fingers" and "pointers" is that there is no fixed limit on the number of pointers a program can create. Rather than define a form of the BM analogous to the pointer machines of Schonhage and others <ref> [45, 66, 67, 49, 10] </ref>, we move straight to a model that uses "random-access addressing," a mechanism usually considered stronger than pointers (for in-depth comparisons, see [9, 10] and also [68]).
Reference: [50] <author> M. Loui, </author> <title> Simulations among multidimensional Turing machines, </title> <journal> Theor. Comp. Sci., </journal> <volume> 21 (1981), </volume> <pages> pp. </pages> <month> 145-161. </month> <title> [51] , Optimal dynamic embedding of trees into arrays, </title> <journal> SIAM J. Comput., </journal> <volume> 12 (1983), </volume> <pages> pp. </pages> <month> 463 472. </month> <title> [52] , Minimizing access pointers into trees and arrays, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 28 (1984), </volume> <pages> pp. 359 378. </pages>
Reference-contexts: The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in [30, 26, 7, 14, 64], and TMs with tree-structured tapes (see [57, 63, 51, 52]). Turing machines with d-dimensional tapes (see <ref> [31, 60, 50] </ref>) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed. The standard TM model, with d = 1, has linear vicinity.
Reference: [53] <author> G. Manacher, </author> <title> Steady-paced-output and fractional-on-line algorithms on a RAM, </title> <journal> Inf. Proc. Lett., </journal> <volume> 15 (1982), </volume> <pages> pp. 47-52. </pages>
Reference-contexts: They relate to generally-known issues of delay in computations. The first definition is the special case for GSTs of Manacher's notion of a "fractional on-line RAM algorithm with steady-paced output" <ref> [53] </ref>. Definition 8.1. Let d 0 and e 1 be integers.
Reference: [54] <author> Y. Mansour, N. Nisan, and P. Tiwari, </author> <title> The computational complexity of universal hashing, </title> <journal> Theor. Comp. Sci., </journal> <volume> 107 (1993), </volume> <pages> pp. 121-133. </pages>
Reference-contexts: Alon and Maass [4] prove substantial time-space tradeoffs for the related "sequence equality" problem SE [n]: given x; y 2 f 0; 1; 2 g n , does Er 2 (x) = Er 2 (y)? We inquire whether their techniques, or those of <ref> [54] </ref>, can be adapted to the BM. The BM in Theorem 7.1 runs in output delay 1=2, 1, or 2 for all passes, so the two kinds of BM can be separated by no more than a log factor.
Reference: [55] <author> D. Muller and F. Preparata, </author> <title> Bounds to complexities of networks for sorting and switching, </title> <journal> J. ACM, </journal> <volume> 22 (1975), </volume> <pages> pp. 195-201. </pages>
Reference-contexts: The counting 19 idea of the next lemma resembles the linear-size circuits constructed for 0-1 sorting in <ref> [55] </ref>. Lemma 4.12. The function #a (x), which gives the number of occurrences of `a' in a string x 2 f a; b g fl , is computable in linear 1 -time by a BM that observes the strict boundary condition. Proof.
Reference: [56] <author> W. Paul, N. Pippenger, E. Szemer edi, and W. Trotter, </author> <title> On determinism versus nonde terminism and related problems, </title> <booktitle> in Proc. 24th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1983, </year> <pages> pp. 429-438. </pages>
Reference-contexts: The BM has polynomial vicinity under d (though not under log ), because any access outside the first t d cells costs more than t time units. The theorem of <ref> [56] </ref> that deterministic linear time on the standard TM (DLIN) is properly contained in nondeterministic TM linear time (NLIN) is not known to carry over to any model of super-linear vicinity. 2 Practical motivations. <p> Is NTLIN 6= TLIN? For reasonable and time bounds t, is there a general separation of NTIME [t (n)] from DTIME [t (n)]? Grandjean [27, 28] shows that a few NP-complete languages are also hard for NLIN under TM linear time reductions, and hence by the theorem of <ref> [56] </ref> lie outside DLIN, not to mention TLIN. However, these languages seem not to belong to NTLIN, nor even to linear time for NBMs of the stronger kind.
Reference: [57] <author> W. Paul and R. Reischuk, </author> <title> On time versus space II, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 22 (1981), </volume> <pages> pp. 312 327. </pages>
Reference-contexts: The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in [30, 26, 7, 14, 64], and TMs with tree-structured tapes (see <ref> [57, 63, 51, 52] </ref>). Turing machines with d-dimensional tapes (see [31, 60, 50]) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed. The standard TM model, with d = 1, has linear vicinity. <p> We write RAM-TIME log for time on the log-cost RAM. A log-cost RAM can be simulated with constant-factor overhead by a TM with one binary tree-structured tape and one standard worktape <ref> [57] </ref>, and the latter is simulated in real time by a RAM-TM. Proposition 7.6. For any time function t, (a) RAM-TIME log [t (n)] D log TIME [t (n) log t (n)]. (b) D log TIME [t (n)] RAM-TIME log [t (n) log t (n)]. Proof.
Reference: [58] <author> W. Paul, J. Seiferas, and J. Simon, </author> <title> An information-theoretic approach to time bounds for on-line computation, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 23 (1981), </volume> <pages> pp. 108-126. </pages>
Reference-contexts: Lemma 7.5 says that it is no less restrictive than the older concept given by d-dimensional Turing machines. For d &gt; 1 we suspect that it is noticeably more restrictive. The d-dimensional tape reduction theorem of Paul, Seiferas, and Simon <ref> [58] </ref> gives t 0 (n) roughly equal to t (n) 1+1=d , and when ported to a BM, incurs memory access charges close to t (n) 1+2=d .
Reference: [59] <author> N. Pippenger, </author> <title> On simultaneous resource bounds, </title> <booktitle> in Proc. 20th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1979, </year> <pages> pp. 307-311. </pages>
Reference-contexts: The pass count appears to be sandwiched between two measures of reversals for multitape Turing machines, namely the now-standard one of <ref> [59, 35, 16] </ref>, and the stricter notion of [43] which essentially counts keeping a TM head stationary as a reversal. Definition 2.8.
Reference: [60] <author> N. Pippenger and M. Fischer, </author> <title> Relations among complexity measures, </title> <journal> J. ACM, </journal> <volume> 26 (1979), </volume> <pages> pp. 361-381. </pages>
Reference-contexts: The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in [30, 26, 7, 14, 64], and TMs with tree-structured tapes (see [57, 63, 51, 52]). Turing machines with d-dimensional tapes (see <ref> [31, 60, 50] </ref>) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed. The standard TM model, with d = 1, has linear vicinity.
Reference: [61] <author> V. Pratt and L. Stockmeyer, </author> <title> A characterization of the power of vector machines, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 12 (1976), </volume> <pages> pp. 198-221. </pages>
Reference-contexts: The latter also serves as a measure of parallel time, since finite transductions can be computed by parallel prefix sum. Indeed, the BM is similar to the Pratt-Stockmeyer vector machine <ref> [61] </ref>, and can also be regarded as a fixed-wordsize analogue of Blelloch's "scan" model [11]. Results. The first main theorem is that the BM is a very robust model.
Reference: [62] <author> K. Regan, </author> <title> A new parallel vector model, with exact characterizations of NC k , in Proc. </title> <booktitle> 11th Annual Symposium on Theoretical Aspects of Computer Science, vol. 778 of Lect. Notes in Comp. </booktitle> <publisher> Sci., Springer Verlag, </publisher> <year> 1994, </year> <pages> pp. 289-300. </pages>
Reference-contexts: A separation of the two kinds can be shown with regard to the pass count measure R (n), which serves as a measure of parallel time (e.g. R (n) = polylog (n) and polynomial work w (n) by deterministic BMs characterizes NC <ref> [62] </ref>).
Reference: [63] <author> R. Reischuk, </author> <title> A fast implementation of multidimensional storage into a tree storage, </title> <journal> Theor. Comp. Sci., </journal> <volume> 19 (1982), </volume> <pages> pp. 253-266. </pages>
Reference-contexts: The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in [30, 26, 7, 14, 64], and TMs with tree-structured tapes (see <ref> [57, 63, 51, 52] </ref>). Turing machines with d-dimensional tapes (see [31, 60, 50]) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed. The standard TM model, with d = 1, has linear vicinity.
Reference: [64] <author> W. Ruzzo, </author> <title> On uniform circuit complexity, </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 22 (1981), </volume> <pages> pp. 365-383. </pages>
Reference-contexts: In 3D space, real machines "should have" at most cubic vicinity. The RAM model, however, has exponential vicinity even under the log-cost criterion advocated by Cook and Reckhow [18]. So do the random-access Turing machine (RAM-TM) forms described in <ref> [30, 26, 7, 14, 64] </ref>, and TMs with tree-structured tapes (see [57, 63, 51, 52]). Turing machines with d-dimensional tapes (see [31, 60, 50]) have vicinity O (t d ), regardless of the number of such tapes or number of heads on each tape, even with head-to-head jumps allowed. <p> The following BM form is based on a random-access Turing machine (RAM-TM; cf. "RTM" in [30] and "indexing TM" in <ref> [14, 64, 8] </ref>), and is closest to the BT. Definition 3.5. A RAM-BM has one main tape, four address tapes labeled a 1 ; b 1 ; a 2 ; b 2 and given their own heads, and a finite control comprised of RAM-TM states and GST states.
Reference: [65] <author> C. Schnorr, </author> <title> Satisfiability is quasilinear complete in NQL, </title> <journal> J. ACM, </journal> <volume> 25 (1978), </volume> <pages> pp. 136-145. </pages>
Reference-contexts: Following Schnorr <ref> [65] </ref>, we write DQL and NQL for the TM time classes DTIME [qlin] and NTIME [qlin].
Reference: [66] <author> A. Sch onhage, </author> <title> Storage modification machines, </title> <journal> SIAM J. Comput., </journal> <volume> 9 (1980), </volume> <pages> pp. </pages> <month> 490-508. </month> <title> [67] , A nonlinear lower bound for random-access machines under logarithmic cost, </title> <journal> J. ACM, </journal> <volume> 37 35 (1988), </volume> <pages> pp. 748-754. </pages>
Reference-contexts: One classical difference between "fingers" and "pointers" is that there is no fixed limit on the number of pointers a program can create. Rather than define a form of the BM analogous to the pointer machines of Schonhage and others <ref> [45, 66, 67, 49, 10] </ref>, we move straight to a model that uses "random-access addressing," a mechanism usually considered stronger than pointers (for in-depth comparisons, see [9, 10] and also [68]).
Reference: [68] <author> P. van Emde Boas, </author> <title> Machine models and simulations, </title> <booktitle> in Handbook of Theoretical Computer Science, </booktitle> <editor> J. V. Leeuwen, ed., </editor> <publisher> Elsevier and MIT Press, </publisher> <year> 1990, </year> <pages> pp. 1-66. </pages>
Reference-contexts: Rather than define a form of the BM analogous to the pointer machines of Schonhage and others [45, 66, 67, 49, 10], we move straight to a model that uses "random-access addressing," a mechanism usually considered stronger than pointers (for in-depth comparisons, see [9, 10] and also <ref> [68] </ref>). The following BM form is based on a random-access Turing machine (RAM-TM; cf. "RTM" in [30] and "indexing TM" in [14, 64, 8]), and is closest to the BT. Definition 3.5. <p> R (n) = polylog (n) and polynomial work w (n) by deterministic BMs characterizes NC [62]). P. van Emde Boas [personal communication, 1994] has observed that while deterministic BMs and NBMs of the weaker kind belong to the second machine class of <ref> [68] </ref> with R (n) as time measure, NBMs of the stronger kind have properties shown there to place models beyond the second machine class. Related to Problem 2 is whether the classes D d TIME [O (n)] differ as d varies.
Reference: [69] <author> D. Willard, </author> <title> A density control algorithm for doing insertions and deletions in a sequentially ordered file in a good worst-case time, </title> <journal> Info. Comp., </journal> <volume> 97 (1992), </volume> <pages> pp. 150-204. </pages>
Reference-contexts: One important point is that the BM does not allow insertions and deletions of the familiar "cut-and-paste" kind; instead, the output flows over the destination block and overwrites or lets stand according to the use of B in Definition 2.3. Willard <ref> [69] </ref> describes a model of a file system that lacks insertion and deletion, and gives fairly efficient algorithms for simulating them. Many text processors allow the user to define and move markers for points of immediate access in a file.
References-found: 62

