URL: http://www.cs.cmu.edu/afs/cs/project/iwarp/member/pdinda/PAPERS/WebFtp/sigmetrics96.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/usr/pdinda/html/papers.html
Root-URL: 
Email: (pdinda,droh)@cs.cmu.edu  
Title: Fast Message Assembly Using Compact Address Relations  
Author: Peter A. Dinda David R. O'Hallaron 
Address: 5000 Forbes Avenue Pittsburgh, PA 15213  
Affiliation: Carnegie Mellon University  
Abstract: Message assembly and disassembly represent a significant fraction of total communication time in many parallel systems. We introduce a run-time approach for fast message assembly and disassembly. The approach is based on generating addresses by decoding a precomputed and compactly stored address relation that describes the mapping of addresses on the source node to addresses on the destination node. The main result is that relations induced by redistributions of regular block-cyclic distributed arrays can be encoded in an extremely compact form that facilitates high throughput message assembly and disassembly. We measure the throughput of decoding-based message assembly and disassembly on several systems and find performance on par with copy throughput. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AGRAWAL, G., SUSSMAN, A., AND SALZ, J. </author> <title> An integrated run-time and compile-time approach for parallelizing structured and block structured applications. </title> <booktitle> IEEE Transaction on Parallel and Distributed Systems 6, </booktitle> <month> 7 (July </month> <year> 1995), </year> <pages> 747-754. </pages>
Reference-contexts: As another example, because of asymmetry in the Cray T3D memory system, a transpose operation implemented with contiguous reads and strided writes runs twice as fast as the equivalent operation implemented with strided reads and contiguous writes [25]. This paper describes a purely run-time, inspector/executor <ref> [18, 1] </ref> approach for message assembly/disassembly and provides an initial characterization of its performance on real systems. In our approach, when a data transfer is first encountered, the inspector precomputes 1 the mapping of source node addresses to destination node addresses and stores this address relation in some encoding. <p> Of course, a parallelizing compiler is also limited to some number of optimizations, but it is practical to support a far larger number because optimizations can be selected at compile-time instead of run-time. We are exploring an alternative approach, based on inspector/executor <ref> [18, 1] </ref>, that allows a general-purpose message assembly routine in a run-time library to run efficiently without any special-case run-time or compile-time optimizations.
Reference: [2] <author> ALPERT, D., AND AVNON, D. </author> <title> Architecture of the Pentium processor. </title> <booktitle> IEEE Micro 13, </booktitle> <month> 3 (June </month> <year> 1993), </year> <pages> 1-21. </pages>
Reference-contexts: A larger 2048 fi 2048 array (2048 KBytes assembled/disassembled) was also tested with only minor changes in measured copy throughput. The identical C code was compiled and measured on the DEC 3000/400 [8], the Intel Paragon [12], the IBM RS/6000 [16] 250, the IBM SP-2 [19], and two Pen-tium <ref> [17, 2] </ref> systems: a Gateway Pentium 90, and a Micron Pentium 133.
Reference: [3] <author> BARREH, J., DHAWAN, S., HICKS, T., AND SHIPPY, D. </author> <title> The POWER2 processor. </title> <booktitle> In Proceedings of COMPCON '94 (San Francisco, </booktitle> <address> CA, March 1994), </address> <publisher> IEEE, </publisher> <pages> pp. 389-398. </pages>
Reference-contexts: The throughput measurements of message assembly and disassembly using DMRLEC on the IBM SP-2 are also close to ideal, tracking the throughput of the strided copy loops closely. The IBM XLC compiler has software-pipelined assembly, disassembly, and each of the strided copy loops. Because the POWER2 <ref> [27, 3] </ref> processor, like the Alpha, can continue to issue integer instructions while a load miss is being handled, instructions associated with DMRLEC are free.
Reference: [4] <author> BECKER, M. C., ALLEN, M. S., MOORE, C. R., MUHICH, J. S., AND TUTTLE, D. P. </author> <title> The PowerPC 601 processor. </title> <booktitle> IEEE Micro 13, </booktitle> <month> 5 (October </month> <year> 1993), </year> <pages> 54-68. </pages>
Reference-contexts: As can be seen by the factor of two difference between a compiler generated stride-1 copy loop and the library memcpy () routine, there is considerable room for improving on the stride-k copy loops generated by the Paragon node compiler. The RS/6000 250 (PowerPC 601 <ref> [4] </ref>) also achieves near ideal assembly and disassembly throughput with DMRLEC. The IBM XLC compiler has software-pipelined each of the assembly, disassembly, and strided copy loops.
Reference: [5] <author> DINDA, P. A., AND O'HALLARON, D. R. </author> <title> High throughput message assembly using compact address relations. </title> <type> Tech. rep., </type> <institution> Carnegie Mellon University, </institution> <year> 1996. </year> <note> To Appear. </note>
Reference-contexts: In previous work [6] and in the extended version of this paper <ref> [5] </ref>, we derive and validate a model that predicts, for a simple index array encoding, that the regime where the inspector/executor approach is faster than recomputing address relations on the fly is quite large, and even extends to cases where computing a member of the relation requires only a few instructions. <p> Overall, DMRLEC is three to four orders of magnitude more space efficient than AAPAIR. 5 Measured performance We have developed analytical models for predicting the performance of the AAPAIR encoding <ref> [6, 5] </ref>. These models show that the inspector/executor approach using AA-PAIR outperforms the recomputing on the fly approach in a surprising number of instances.
Reference: [6] <author> DINDA, P. A., AND O'HALLARON, D. R. </author> <title> The performance impact of address relation caching. </title> <booktitle> In Proceedingsof the Third International Workshop on Languages, Compilers and Run-time Systems for Scalable Computers (1996), </booktitle> <editor> B. K. Szymanski and B. Sinharoy, Eds., </editor> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 213-226. </pages>
Reference-contexts: Our innovation is to encode the address relation in new, compact ways that facilitate fast message assembly/disassembly executors they are fast in the sense that data transfer rates to and from the message buffer approach the copy throughput of the memory system for the reference pattern. In previous work <ref> [6] </ref> and in the extended version of this paper [5], we derive and validate a model that predicts, for a simple index array encoding, that the regime where the inspector/executor approach is faster than recomputing address relations on the fly is quite large, and even extends to cases where computing a <p> Overall, DMRLEC is three to four orders of magnitude more space efficient than AAPAIR. 5 Measured performance We have developed analytical models for predicting the performance of the AAPAIR encoding <ref> [6, 5] </ref>. These models show that the inspector/executor approach using AA-PAIR outperforms the recomputing on the fly approach in a surprising number of instances.
Reference: [7] <author> DOBBERPUHL, D. I. A. </author> <title> A 200-MHz 64-bit dual-issue CMOS microprocessor. </title> <journal> Digital Technical Journal 4, </journal> <volume> 4 (1992), </volume> <pages> 35-50. </pages> <month> ftp://ftp.digital.com/pub/Digital/info/DTJ/axp-cmos.txt. </month>
Reference-contexts: The remainder of this section analyzes each machine in turn. On the DEC 3000/400, DMRLEC gives ideal assembly and disassembly throughputs for all the test cases. DMR-LEC benefits from the separate load/store pipeline in the Alpha 21064 processor <ref> [15, 7] </ref>. Even on a load stall, integer instructions associated with DMRLEC can continue to issue. On a Paragon node (i860 [13] processor), DMR-LEC gives near ideal assembly throughput for all the test cases, as shown in Figure 5. The disassembly results are somewhat unexpected, however.
Reference: [8] <author> DUTTON, T. A., EIREF, D., KURTH, H. R., REIS-ERT, J. J., AND STEWART, R. L. </author> <title> The design of the DEC 3000 AXP systems, two high performance workstations. </title> <journal> Digital Technical Journal 4, </journal> <volume> 4 (1992), </volume> <pages> 66-81. </pages> <month> ftp://ftp.digital.com/pub/Digital/info/DTJ/axp-dec-3000.txt. </month>
Reference-contexts: Next, in Section 5, we measure the performance of message assembly/disassembly routines based on the different compact encodings of address relations. The performance on each of the four representative redistributions of Section 2 is measured. Results are given for several common systems: DEC Alpha <ref> [8] </ref>, Intel Paragon [12], IBM RS/6000 [16], IBM SP/2 [19], Gateway Pentium 90 [17], and Micron Pentium 133. <p> A larger 2048 fi 2048 array (2048 KBytes assembled/disassembled) was also tested with only minor changes in measured copy throughput. The identical C code was compiled and measured on the DEC 3000/400 <ref> [8] </ref>, the Intel Paragon [12], the IBM RS/6000 [16] 250, the IBM SP-2 [19], and two Pen-tium [17, 2] systems: a Gateway Pentium 90, and a Micron Pentium 133.
Reference: [9] <author> GUPTA, S., KAUSHIK, S., HUANG, C., AND SADAYAPPAN, P. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <type> Tech. Rep. </type> <institution> OSU-CISRC-4/94-TR19, Computer and Information Science Research Center, Ohio State University, </institution> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: For example, the running time of the assembly step for a High Performance Fortran (HPF) [10] block-cyclic assignment statement can vary by several orders of magnitude, depending on the nesting of the assembly loops <ref> [9] </ref>. As another example, because of asymmetry in the Cray T3D memory system, a transpose operation implemented with contiguous reads and strided writes runs twice as fast as the equivalent operation implemented with strided reads and contiguous writes [25].
Reference: [10] <author> HIGH PERFORMANCE FORTRAN FORUM. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Tech. Rep. CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The message assembly and disassembly operations needed to implement distributed data structures such as distributed arrays are nontrivial and the running times of different algorithms can vary significantly. For example, the running time of the assembly step for a High Performance Fortran (HPF) <ref> [10] </ref> block-cyclic assignment statement can vary by several orders of magnitude, depending on the nesting of the assembly loops [9].
Reference: [11] <author> HUFFMAN, D. A. </author> <title> A method for the construction of minimum-redundancy codes. </title> <booktitle> Proceedings of the IRE 40, </booktitle> <month> 9 (September </month> <year> 1952), </year> <pages> 1098-1101. </pages>
Reference-contexts: The DMRLE encoding efficiently encodes the basic stride-j to stride-k mapping, but for multidimensional arrays this will be repeated many times. Clearly, because the symbols for inner dimensions will occur more frequently than for outer dimensions, a Huffman-encoding <ref> [11] </ref> of the symbols would most readily reduce their storage cost. However, Huffman codes are of variable length, so keys could span word boundaries and be expensive to decode during message assembly. Instead, we adopt a dictionary substitution encoding [14] of the symbols as fixed length keys. <p> One motivating application for these irregular relations is sparse matrix-vector multiplication over irregular finite-element meshes. Another avenue to explore is the design of more aggressive encoding schemes such as the Huffman-based <ref> [11] </ref> DMRLE encoding mentioned in Section 4. Another possi bility is to use LZW [28, 26] or a variant thereof for further compression. 3 We are also interested in how much the performance of message assembly and disassembly depends on single-node compilers.
Reference: [12] <author> INTEL CORP. </author> <title> Paragon X/PS Product Overview, </title> <month> March </month> <year> 1991. </year>
Reference-contexts: Next, in Section 5, we measure the performance of message assembly/disassembly routines based on the different compact encodings of address relations. The performance on each of the four representative redistributions of Section 2 is measured. Results are given for several common systems: DEC Alpha [8], Intel Paragon <ref> [12] </ref>, IBM RS/6000 [16], IBM SP/2 [19], Gateway Pentium 90 [17], and Micron Pentium 133. <p> A larger 2048 fi 2048 array (2048 KBytes assembled/disassembled) was also tested with only minor changes in measured copy throughput. The identical C code was compiled and measured on the DEC 3000/400 [8], the Intel Paragon <ref> [12] </ref>, the IBM RS/6000 [16] 250, the IBM SP-2 [19], and two Pen-tium [17, 2] systems: a Gateway Pentium 90, and a Micron Pentium 133.
Reference: [13] <author> INTEL CORP. </author> <title> i860 Microprocessor Family Programmer's Reference Manual, </title> <year> 1992. </year>
Reference-contexts: On the DEC 3000/400, DMRLEC gives ideal assembly and disassembly throughputs for all the test cases. DMR-LEC benefits from the separate load/store pipeline in the Alpha 21064 processor [15, 7]. Even on a load stall, integer instructions associated with DMRLEC can continue to issue. On a Paragon node (i860 <ref> [13] </ref> processor), DMR-LEC gives near ideal assembly throughput for all the test cases, as shown in Figure 5. The disassembly results are somewhat unexpected, however.
Reference: [14] <author> LELEWER, D. A., AND HIRSCHBERG, D. S. </author> <title> Data compression. </title> <type> Tech. Rep. 87-10, </type> <institution> University of California - Irvine, </institution> <year> 1987. </year>
Reference-contexts: It takes advantage of no memory access patterns and always requires storage space proportional to the number of data items transferred. The keys and symbols for AAPAIR are the tuples themselves. The AABLK encoding (Address, Address BLocK) run-length encodes <ref> [14] </ref> blocks of stride-1 to stride-1 mappings. The symbols generated are of the form (s; d; l), which means that addresses s through s + l 1 on the source map to address d through d + l 1 on the destination. The keys are simply the symbols. <p> However, Huffman codes are of variable length, so keys could span word boundaries and be expensive to decode during message assembly. Instead, we adopt a dictionary substitution encoding <ref> [14] </ref> of the symbols as fixed length keys. The key length is the minimum power of two bits necessary to represent all the unique symbols. This scheme is efficient for message assembly because keys cannot span word boundaries and there is always the same number of keys per word.
Reference: [15] <author> MCLELLAN, E. </author> <title> The Alpha AXP architecture and 21064 processor. </title> <booktitle> IEEE Micro 13, </booktitle> <month> 3 (June </month> <year> 1993), </year> <pages> 36-47. </pages>
Reference-contexts: The remainder of this section analyzes each machine in turn. On the DEC 3000/400, DMRLEC gives ideal assembly and disassembly throughputs for all the test cases. DMR-LEC benefits from the separate load/store pipeline in the Alpha 21064 processor <ref> [15, 7] </ref>. Even on a load stall, integer instructions associated with DMRLEC can continue to issue. On a Paragon node (i860 [13] processor), DMR-LEC gives near ideal assembly throughput for all the test cases, as shown in Figure 5. The disassembly results are somewhat unexpected, however.
Reference: [16] <author> OEHLER, R., AND GROVES, R. </author> <title> IBM RISC system/6000 processor architecture. </title> <journal> IBM Journal of Research and Development 34, </journal> <month> 1 (Jan </month> <year> 1990), </year> <pages> 23-36. </pages>
Reference-contexts: The performance on each of the four representative redistributions of Section 2 is measured. Results are given for several common systems: DEC Alpha [8], Intel Paragon [12], IBM RS/6000 <ref> [16] </ref>, IBM SP/2 [19], Gateway Pentium 90 [17], and Micron Pentium 133. The encouraging conclusion is that in most cases, assembly and disassembly routines based on a highly compact encoding can run at or near the copy throughput of the memory system for the memory reference pattern of the relation. <p> A larger 2048 fi 2048 array (2048 KBytes assembled/disassembled) was also tested with only minor changes in measured copy throughput. The identical C code was compiled and measured on the DEC 3000/400 [8], the Intel Paragon [12], the IBM RS/6000 <ref> [16] </ref> 250, the IBM SP-2 [19], and two Pen-tium [17, 2] systems: a Gateway Pentium 90, and a Micron Pentium 133.
Reference: [17] <author> SAINI, A. </author> <title> An overview of the Intel Pentium processor. </title> <booktitle> In COMPCON Spring 1993 Digest of Papers (San Francisco, </booktitle> <address> CA, </address> <year> 1993), </year> <pages> pp. 60-62. </pages>
Reference-contexts: The performance on each of the four representative redistributions of Section 2 is measured. Results are given for several common systems: DEC Alpha [8], Intel Paragon [12], IBM RS/6000 [16], IBM SP/2 [19], Gateway Pentium 90 <ref> [17] </ref>, and Micron Pentium 133. The encouraging conclusion is that in most cases, assembly and disassembly routines based on a highly compact encoding can run at or near the copy throughput of the memory system for the memory reference pattern of the relation. <p> A larger 2048 fi 2048 array (2048 KBytes assembled/disassembled) was also tested with only minor changes in measured copy throughput. The identical C code was compiled and measured on the DEC 3000/400 [8], the Intel Paragon [12], the IBM RS/6000 [16] 250, the IBM SP-2 [19], and two Pen-tium <ref> [17, 2] </ref> systems: a Gateway Pentium 90, and a Micron Pentium 133.
Reference: [18] <author> SALTZ, J., , PETITON, S., BERRYMAN, H., AND RIFKIN, A. </author> <title> Performance effects of irregular communication patterns on massively parallel multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing 13 (1991), </journal> <pages> 202-212. </pages>
Reference-contexts: As another example, because of asymmetry in the Cray T3D memory system, a transpose operation implemented with contiguous reads and strided writes runs twice as fast as the equivalent operation implemented with strided reads and contiguous writes [25]. This paper describes a purely run-time, inspector/executor <ref> [18, 1] </ref> approach for message assembly/disassembly and provides an initial characterization of its performance on real systems. In our approach, when a data transfer is first encountered, the inspector precomputes 1 the mapping of source node addresses to destination node addresses and stores this address relation in some encoding. <p> Of course, a parallelizing compiler is also limited to some number of optimizations, but it is practical to support a far larger number because optimizations can be selected at compile-time instead of run-time. We are exploring an alternative approach, based on inspector/executor <ref> [18, 1] </ref>, that allows a general-purpose message assembly routine in a run-time library to run efficiently without any special-case run-time or compile-time optimizations.
Reference: [19] <author> SNIR, M. </author> <title> Scalable Parallel Computing The IBM 9076 Scalable POWERParallel-1. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures (June 1993), ACM, </booktitle> <address> p. </address> <month> 42. </month>
Reference-contexts: The performance on each of the four representative redistributions of Section 2 is measured. Results are given for several common systems: DEC Alpha [8], Intel Paragon [12], IBM RS/6000 [16], IBM SP/2 <ref> [19] </ref>, Gateway Pentium 90 [17], and Micron Pentium 133. The encouraging conclusion is that in most cases, assembly and disassembly routines based on a highly compact encoding can run at or near the copy throughput of the memory system for the memory reference pattern of the relation. <p> A larger 2048 fi 2048 array (2048 KBytes assembled/disassembled) was also tested with only minor changes in measured copy throughput. The identical C code was compiled and measured on the DEC 3000/400 [8], the Intel Paragon [12], the IBM RS/6000 [16] 250, the IBM SP-2 <ref> [19] </ref>, and two Pen-tium [17, 2] systems: a Gateway Pentium 90, and a Micron Pentium 133.
Reference: [20] <author> STEENKISTE, P., ZILL, B., KUNG, H., SCHLICK, S., HUGHES, J., KOWALSKI, B., AND MULLANEY, J. </author> <title> A host interface architecture for high-speed networks. </title> <booktitle> In Proceedings of the 4th IFIP Conference on High Performance Networks (Liege, </booktitle> <address> Belgium, </address> <month> December </month> <year> 1992), </year> <title> IFIP, </title> <publisher> Elsevier, </publisher> <pages> pp. A3 1-16. </pages>
Reference-contexts: This leads us to believe that the architecture or memory system is the overriding factor in the lack-luster performance of the Pentiums. best use of a network adapter such as the one described in <ref> [20] </ref>. It is unclear whether this is the optimal approach, since the main processor could presumably support better encodings than the network adaptor. However, it is an intriguing possibility. Address relations could also be a powerful abstraction in the interface between the layers of a communication system.
Reference: [21] <author> STEENKISTE, P. A. </author> <title> A systematic approach to host interface design for high-speed networks. </title> <booktitle> Computer 27, </booktitle> <month> 3 (March </month> <year> 1994), </year> <pages> 47-57. </pages>
Reference-contexts: In such systems, the memory system tends to be the bottleneck rather than the communication system [25]. The range of these systems is increasing with the advent of high speed networks <ref> [21] </ref>. Further, even on machines with slow communication systems, overly slow message assembly and disassembly can lead to disappointing overall performance. The message assembly and disassembly operations needed to implement distributed data structures such as distributed arrays are nontrivial and the running times of different algorithms can vary significantly.
Reference: [22] <author> STICHNOTH, J. </author> <title> Efficient compilation of array statements for private memory multicomputers. </title> <type> Tech. Rep. </type> <institution> CMU-CS-93-109, School of Computer Science, Carnegie Mellon University, </institution> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: CYCLIC to CYCLIC redistributions result in non-unit strides on both nodes, as do certain forms of transpose operations. With more than one dimension, the access pattern is the Cartesian product of the access patterns generated for each matching dimension <ref> [22] </ref>. This can also be seen in four times. This pattern is then repeated stride-8 to stride-8 16 times. We place array redistributions of BLOCK and CYCLIC distributed arrays into four equivalence classes based on whether their basic source and destination reference patterns have unit or non-unit strides.
Reference: [23] <author> STICHNOTH, J., O'HALLARON, D., AND GROSS, T. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing 21, </journal> <month> 1 (Apr. </month> <year> 1994), </year> <pages> 150-159. </pages>
Reference-contexts: Parallelizing compilers can help a great deal by exploiting compile-time knowledge to generate specialized assembly loops for each data transfer <ref> [23, 24] </ref>. However, if the data access pattern is irregular (and thus unknown to the compiler), or if a parallelizing 1 In general, it would be possible to relax this constraint by associating a binary associative combining operation with each address relation to handle non-unique destination addresses.
Reference: [24] <author> STICHNOTH, J., AND T., G. </author> <title> A communication backend for parallel language compilers. </title> <booktitle> Proc. Fifth Workshop on Compilers for Parallel Languages, Malaga, Spain (Tech Report UMA-DAC-95/09), </booktitle> <pages> pages 65-77, </pages> <year> 1995. </year>
Reference-contexts: Parallelizing compilers can help a great deal by exploiting compile-time knowledge to generate specialized assembly loops for each data transfer <ref> [23, 24] </ref>. However, if the data access pattern is irregular (and thus unknown to the compiler), or if a parallelizing 1 In general, it would be possible to relax this constraint by associating a binary associative combining operation with each address relation to handle non-unique destination addresses.
Reference: [25] <author> STRICKER, T., AND GROSS, T. </author> <title> Optimizing memory system performance for communication in parallel computers. </title> <booktitle> In Proc. 22nd Intl. Symp. on Computer Architecture (Portofino, </booktitle> <address> Italy, </address> <month> June </month> <year> 1995), </year> <month> ACM/IEEE, </month> <pages> pp. 308-319. </pages>
Reference-contexts: On machines with fast communication systems, the performance of the message assembly and disassembly steps can have a significant impact on the overall communication performance of a program. In such systems, the memory system tends to be the bottleneck rather than the communication system <ref> [25] </ref>. The range of these systems is increasing with the advent of high speed networks [21]. Further, even on machines with slow communication systems, overly slow message assembly and disassembly can lead to disappointing overall performance. <p> As another example, because of asymmetry in the Cray T3D memory system, a transpose operation implemented with contiguous reads and strided writes runs twice as fast as the equivalent operation implemented with strided reads and contiguous writes <ref> [25] </ref>. This paper describes a purely run-time, inspector/executor [18, 1] approach for message assembly/disassembly and provides an initial characterization of its performance on real systems. <p> For example, by ordering the relation by destination node addresses first, we can optimize for reference locality on the destination node instead of on the source node. As shown in <ref> [25] </ref>, the choice is architecture-dependent and can have a significant impact on performance. Stored encoded address relations also appear to be a good interface from user programs to the communication system because they are implementation neutral.
Reference: [26] <author> WELCH, J. </author> <title> A technique for high performance data compression. </title> <booktitle> IEEE Computer 17, </booktitle> <month> 6 (June </month> <year> 1984), </year> <pages> 8-19. </pages>
Reference-contexts: One motivating application for these irregular relations is sparse matrix-vector multiplication over irregular finite-element meshes. Another avenue to explore is the design of more aggressive encoding schemes such as the Huffman-based [11] DMRLE encoding mentioned in Section 4. Another possi bility is to use LZW <ref> [28, 26] </ref> or a variant thereof for further compression. 3 We are also interested in how much the performance of message assembly and disassembly depends on single-node compilers. Many parallelizing compilers generate sequential source code and rely on the single-node compiler to produce good results.
Reference: [27] <author> WHITE, S. W. Power2: </author> <title> Architecture and performance. </title> <booktitle> In Proceedings of COMPCON '94 (San Francisco, </booktitle> <address> CA, March 1994), </address> <publisher> IEEE, </publisher> <pages> pp. 384-388. </pages>
Reference-contexts: The throughput measurements of message assembly and disassembly using DMRLEC on the IBM SP-2 are also close to ideal, tracking the throughput of the strided copy loops closely. The IBM XLC compiler has software-pipelined assembly, disassembly, and each of the strided copy loops. Because the POWER2 <ref> [27, 3] </ref> processor, like the Alpha, can continue to issue integer instructions while a load miss is being handled, instructions associated with DMRLEC are free.
Reference: [28] <author> ZIV, J., AND LEMPEL, A. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory 23, </journal> <month> 3 (May </month> <year> 1977), </year> <pages> 337-343. </pages>
Reference-contexts: One motivating application for these irregular relations is sparse matrix-vector multiplication over irregular finite-element meshes. Another avenue to explore is the design of more aggressive encoding schemes such as the Huffman-based [11] DMRLE encoding mentioned in Section 4. Another possi bility is to use LZW <ref> [28, 26] </ref> or a variant thereof for further compression. 3 We are also interested in how much the performance of message assembly and disassembly depends on single-node compilers. Many parallelizing compilers generate sequential source code and rely on the single-node compiler to produce good results.
References-found: 28

