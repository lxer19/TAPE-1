URL: ftp://ftp.research.microsoft.com/users/palarson/tdke94.ps
Refering-URL: http://www.research.microsoft.com/~palarson/publications.htm
Root-URL: http://www.research.microsoft.com
Title: Parallel Hash-Based Join Algorithms for a Shared-Everything Environment  
Author: T. Patrick Martin Per -Ake Larson and Vinay Deshpande 
Keyword: Index Terms algorithms, cost models, parallel systems, query processing, relational database systems.  
Date: September 23, 1996  
Abstract: We analyze the costs, and describe the implementation, of three hashed-based join algorithms for a general-purpose shared-memory multiprocessor. The three algorithms considered are the Hashed Loops, GRACE and Hybrid algorithms. We also describe the results of a set of experiments which validate the cost models presented and demonstrate the relative performance of the three algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bitton, H. Boral, D.J. DeWitt and W.K. Wilkinson, </author> <title> "Parallel Algorithms for the Execution of Relational Database Operations", </title> <journal> ACM Transactions on Database Systems 8(3), pp. </journal> <volume> 324 - 353, </volume> <month> September </month> <year> 1983. </year>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories [6, 19]. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems <ref> [1, 3, 7, 9, 12, 16, 20] </ref>. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms. <p> Similarly, if data is spread across d parallel disks, the transfer rate for the data is IO=d if IO is the transfer rate for a single disk. Most of the previous analyses of join algorithms <ref> [1, 6, 19, 20] </ref> have assumed that there is no overlap between processing and disk I/O. Response time has been calculated as the sum of the processing and the I/O times.
Reference: [2] <author> D. Bitton, D.J. DeWitt and C. Turbyfill, </author> <title> "Benchmarking Database Systems A Systematic Approach", </title> <booktitle> Proceedings of the Ninth International Conference on Very Large Data Bases, </booktitle> <month> October </month> <year> 1983. </year>
Reference-contexts: When the amount of memory was varied the number of CPUs was held constant at six. The amount of main memory available is always expressed as a percentage of the outer relation R. The benchmark relations were based on the standard Wisconsin benchmark <ref> [2] </ref>. Each relation consisted of eight 4 byte integer values and two 48 byte string attributes. The queries were variations on the join query "joinABprime" found in the Wisconsin benchmark.
Reference: [3] <author> K. Bratbergsengen, </author> <title> "Hashing Methods and Relational Algebra Operators", </title> <booktitle> Proceedings of the Tenth International Conference on Very Large Data Bases, Singapore, </booktitle> <pages> pp. 323 - 333, </pages> <year> 1984. </year>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories [6, 19]. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems <ref> [1, 3, 7, 9, 12, 16, 20] </ref>. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms. <p> assume that the size of relation R, with respect to both number of tuples and number of blocks, is always less than the size of relation S. 10 Hashed Loops pseudo-code goes here 5.1 Hashed Loops The Hashed Loops algorithm is a variation of the traditional nested loops join algorithm <ref> [3] </ref>. The algorithm is composed of two phases which we call the build and join phases. The number of passes required in an execution of the algorithm is dependent upon the size of the outer relation R compared with the amount of available memory.
Reference: [4] <author> B.C. Brookes. </author> <title> Bradford's Law and the Bibliography of Science. </title> <journal> Nature 224,5223 , pp. </journal> <volume> 953 - 956, </volume> <year> 1969. </year>
Reference-contexts: The size of the result relation was kept constant across 19 the various queries and only the characteristics of the input relations were varied. The skewed distributions for the join attributes were produced using a Bradford-Zipf distribution <ref> [4] </ref>.
Reference: [5] <author> P.A. Buhr and R.A. Stroobosscher, </author> <title> "The System: Providing Light-Weight Concurrency on Shared-Memory Multiprocessor Computers Running UNIX", </title> <journal> Software: Practice and Experience 20(9), pp. </journal> <volume> 929 - 963, </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: These tasks are implemented using the System, which was developed to provide light-weight concurrency on uniprocessor and multiprocessor hardware running the UNIX 1 operating system <ref> [5] </ref>. The System uses a shared-memory model of concurrency 1 UNIX is a registered trademark of AT&T Bell Laboratories 4 and provides mechanisms to create tasks within the shared memory, to synchronize execution of the tasks and to facilitate communication between the tasks. <p> The tasks are light-weight because of the low execution time cost and space overhead for creating a task and the many forms of communication which are easily and efficiently implemented for them <ref> [5] </ref>. Tasks are run on one or more virtual processors. A virtual processor is conceptually a hardware processor that executes tasks but is actually a UNIX process that is subsequently scheduled for execution on the physical processors by the underlying operating system. <p> A disadvantage of our programming model is the additional overhead required to manage and synchronize tasks. However, investigation by Buhr and Stroobosscher <ref> [5] </ref> indicates that this additional overhead is not unreasonable given the above advantages. Table 1 shows the overhead involved in the three most common System operations in our code, namely task creation and deletion, task context switch, and P/V on a semaphore followed by a context switch.
Reference: [6] <author> D.J. DeWitt, R.H. Katz, F. Olken, L.D. Shapiro, M.R. Stonebraker and D. Wood, </author> <title> "Implementation Techniques for Large Main Memory Databases", </title> <booktitle> Proceedings of the 1984 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pp. 1 - 8, </pages> <address> Boston MA, </address> <year> 1984. </year>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories <ref> [6, 19] </ref>. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems [1, 3, 7, 9, 12, 16, 20]. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms. <p> Similarly, if data is spread across d parallel disks, the transfer rate for the data is IO=d if IO is the transfer rate for a single disk. Most of the previous analyses of join algorithms <ref> [1, 6, 19, 20] </ref> have assumed that there is no overlap between processing and disk I/O. Response time has been calculated as the sum of the processing and the I/O times. <p> The times to read the S buckets and write the results are given by T readSi and T writeRS respectively. 5.3 Hybrid The Hybrid algorithm <ref> [6] </ref> is a variation of the GRACE algorithm. During the partitioning phase, instead of using extra memory to increase the number of buckets, it introduces an R 0 bucket and builds a hash table for it.
Reference: [7] <author> D.J. DeWitt and R. Gerber, </author> <title> "Multiprocessor Hash-Based Join Algorithms", </title> <booktitle> Proceedings of the Eleventh International Conference on Very Large Data Bases, </booktitle> <address> Stockholm, </address> <year> 1985. </year>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories [6, 19]. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems <ref> [1, 3, 7, 9, 12, 16, 20] </ref>. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms.
Reference: [8] <author> G. Graefe, </author> <title> "Encapsulation of Parallelism in the Volcano Query Processing System", </title> <booktitle> Proceedings of the 1990 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pp. 102 - 111, </pages> <address> Atlantic City NJ, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: This difference leads to very different implementations of the same algorithms. Work on database systems in the shared-everything environment has not been as wide-spread as in the shared-nothing environment. The Volcano project <ref> [8] </ref> has goals similar to our LauRel project and is also implemented on a general-purpose shared-memory multiprocessor. Volcano deals with the standard relational model while LauRel has a nested relational model.
Reference: [9] <author> M. Kitsuregawa, H. Tanaka and T. Moto-oka, </author> <title> "Application of Hash to Data Base Machine and Its Architecture", </title> <journal> New Generation Computing 1 , pp. </journal> <volume> 63 - 74, </volume> <year> 1983. </year>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories [6, 19]. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems <ref> [1, 3, 7, 9, 12, 16, 20] </ref>. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms. <p> The times to read S and write the result are given by T readS and T writeRS, respectively. 5.2 GRACE The GRACE algorithm <ref> [9] </ref> has four separate phases. The first two phases "partition", R and S into disjoint subsets, say R 1 ; R 2 ; :::; R B and S 1 ; S 2 ; :::; S B .
Reference: [10] <author> P.A. Larson, </author> <title> "The Data Model and Query Language of LauRel", </title> <journal> IEEE Data Engineering 11(3), pp. </journal> <volume> 23 - 30, </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The LauRel project <ref> [10] </ref> is aimed at extending current relational database technology in three areas: data modelling, exploitation of parallelism, and structuring of the stored database. As part of this project, we are building a prototype database system which runs on a general-purpose shared-memory multiprocessor system.
Reference: [11] <author> H. Lu, K. Tan and M. Shan, </author> <title> "Hash-Based Join Algorithms for Multiprocessor Computers with Shared Memory", </title> <booktitle> Proceedings of the Sixteenth International Conference on Very Large Data Bases, </booktitle> <address> pp.198 - 209, Brisbane, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Volcano deals with the standard relational model while LauRel has a nested relational model. Qadah and Irani [14] have studied join algorithms for a specialized shared-memory database machine, and Lu, Tan and Shan <ref> [11] </ref> have also developed cost models for a set of hash-based join algorithms on a general purpose shared-memory multiprocessor. The latter work is most closely related to the research described in this paper. <p> For example, it would be wasteful to allocate more CPUs to an I/O-bound operation than the minimum number necessary to keep up with the data rate. The work described in Richardson, Lu and Mikkilileni [15] and in Lu, Tan and Shan <ref> [11] </ref> are exceptions and try to represent the overlap. We ignore any processing related to a disk access so that I/O tasks have no associated processing cost. <p> When inserting a tuple into the table the lock required, and hence the path taken, by a task is determined by the tuple currently being processed. We model multiple-path contention with the same technique as described by Lu, Tan and Shan <ref> [11] </ref>.
Reference: [12] <author> E.R. Omiencinski and E.T. Lin, </author> <title> "Hash-Based and Index-Based Join Algorithms for Cube and Ring Connected Multicomputers", </title> <journal> IEEE Transactions on Knowledge and Data Engineering 1(3), </journal> <volume> pp.329 - 343, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories [6, 19]. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems <ref> [1, 3, 7, 9, 12, 16, 20] </ref>. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms.
Reference: [13] <author> H. Pirahesh, C. Mohan, J. Cheng, T.S. Liu and P. Selinger, </author> <title> "Parallelism in Relational Data Base Systems: </title> <booktitle> Architectural Issues and Design Approaches", Proceedings of 2nd International Symposium on Databases in Parallel and Distributed Systems, </booktitle> <address> Dublin, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: A more appropriate criterion suggested by Pirahesh et. al. <ref> [13] </ref> is to maximize the use of given limited resources to minimize the response time up to a threshold.
Reference: [14] <author> Q.Z. Qadah and K.B. Irani, </author> <title> "The Join Algorithms on a Shared-Memory Multiprocessor Database Machine", </title> <journal> IEEE Transactions on Software Engineering 14(11), pp. </journal> <volume> 1668 - 1683, </volume> <month> November </month> <year> 1988. </year>
Reference-contexts: The Volcano project [8] has goals similar to our LauRel project and is also implemented on a general-purpose shared-memory multiprocessor. Volcano deals with the standard relational model while LauRel has a nested relational model. Qadah and Irani <ref> [14] </ref> have studied join algorithms for a specialized shared-memory database machine, and Lu, Tan and Shan [11] have also developed cost models for a set of hash-based join algorithms on a general purpose shared-memory multiprocessor. The latter work is most closely related to the research described in this paper.
Reference: [15] <author> J.P. Richardson, H. Lu and K. Mikkilineni, </author> <title> "Design and Evaluation of Parallel Pipelined Join Algorithms", </title> <booktitle> Proceedings of the 1987 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pp. 399 - 409, </pages> <address> San Francisco CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: For example, it would be wasteful to allocate more CPUs to an I/O-bound operation than the minimum number necessary to keep up with the data rate. The work described in Richardson, Lu and Mikkilileni <ref> [15] </ref> and in Lu, Tan and Shan [11] are exceptions and try to represent the overlap. We ignore any processing related to a disk access so that I/O tasks have no associated processing cost.
Reference: [16] <author> D.A. Schneider and D.J. DeWitt, </author> <title> "A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment", </title> <booktitle> Proceedings of the 1989 ACM SIGMOD International Conference on the Management of Data, </booktitle> <pages> pp. 110 - 121, </pages> <address> Portland OR, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories [6, 19]. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems <ref> [1, 3, 7, 9, 12, 16, 20] </ref>. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms.
Reference: [17] <institution> Sequent Computer Systems, </institution> <type> Symmetry Technical Summary, </type> <year> 1987. </year>
Reference-contexts: The machine used is a Sequent Symmetry <ref> [17] </ref>. The architecture of the system, which is shown in Figure 1, consists of a number of processors, memory modules, disk controllers, and other I/O controllers, which are all linked together by a high-speed bus.
Reference: [18] <author> Sequent Computer Systems, </author> <title> Guide to Parallel Programming on Sequent Computer Systems, 2nd Edition, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs NJ, </address> <year> 1989. </year>
Reference: [19] <author> L.D. Shapiro, </author> <title> "Join Processing in Database Systems with Large Main Memories", </title> <journal> ACM Transactions on Database Systems 11(3), pp. </journal> <volume> 239 - 264, </volume> <month> September </month> <year> 1986. </year> <month> 28 </month>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories <ref> [6, 19] </ref>. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems [1, 3, 7, 9, 12, 16, 20]. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms. <p> Similarly, if data is spread across d parallel disks, the transfer rate for the data is IO=d if IO is the transfer rate for a single disk. Most of the previous analyses of join algorithms <ref> [1, 6, 19, 20] </ref> have assumed that there is no overlap between processing and disk I/O. Response time has been calculated as the sum of the processing and the I/O times. <p> We tried other more complicated representations but did not achieve any better approximations to the actual execution times of the algorithms. Contention points are represented in the cost models by symbols. The cost models used in our analysis are described in a notation similar to that used by Shapiro <ref> [19] </ref>; it is summarized in Appendix A. The parameter values used for the models are given in Appendix B. The models assume a uniform distribution of join attribute values across the relations and ignore the possibility of table or bucket overflow.
Reference: [20] <author> P. Valduriez and G. Gardarin, </author> <title> "Join and Semijoin Algorithms for a Multiprocessor Database Machine", </title> <journal> ACM Transactions on Database Systems 9(1), pp. </journal> <volume> 133 - 161, </volume> <month> March </month> <year> 1984. </year>
Reference-contexts: Recent work has resulted in a number of attractive alternatives to the traditional nested loops and sort-merge algorithms. One environment where join algorithms have been studied is machines with large main memories [6, 19]. Another popular environment for this work has been distributed-memory, or shared-nothing multiprocessor systems <ref> [1, 3, 7, 9, 12, 16, 20] </ref>. In most of these studies it has been found that, for the join of large sequential files, hash-based algorithms outperform the nested loops and the sort-merge algorithms. <p> Similarly, if data is spread across d parallel disks, the transfer rate for the data is IO=d if IO is the transfer rate for a single disk. Most of the previous analyses of join algorithms <ref> [1, 6, 19, 20] </ref> have assumed that there is no overlap between processing and disk I/O. Response time has been calculated as the sum of the processing and the I/O times.
Reference: [21] <author> S.B. Yao, </author> <title> "Approximating Block Access in Database Organizations", </title> <journal> Communications of the ACM 20(4), pp. </journal> <volume> 260 - 261, </volume> <month> April </month> <year> 1977. </year>
Reference-contexts: This formulation is similar to the problem of characterizing the number of granules accessed by a transaction. A solution to this problem was given by Yao <ref> [21] </ref>. Using Yao's theorem we estimate the number of paths followed ( y ) by y = H fi 1 i=1 kRk i + 1 where D = 1 1=H.
References-found: 21

