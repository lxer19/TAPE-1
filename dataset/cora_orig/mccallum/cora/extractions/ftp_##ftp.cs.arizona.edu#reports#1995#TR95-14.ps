URL: ftp://ftp.cs.arizona.edu/reports/1995/TR95-14.ps
Refering-URL: http://www.cs.arizona.edu/research/reports.html
Root-URL: http://www.cs.arizona.edu
Title: Using Fine-Grain Threads and Run-Time Decision Making in Parallel Computing  
Author: David K. Lowenthal Vincent W. Freeh Gregory R. Andrews 
Abstract-found: 0
Intro-found: 1
Reference: [AL93] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <year> 1993. </year>
Reference-contexts: Most current approaches determine data placements statically. They can generally be divided into two categories: using language primitives, such as the ones in HPF [HPF93], or compiler analysis, such as the work reported in <ref> [AL93] </ref>, [GB93], and [KK94]. Language primitives involve the programmer in the choice of data placement; unfortunately, the best placement may be difficult or impossible for the programmer to determine.
Reference: [BKT90] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. </author> <title> Experience with Distributed Programming in Orca. </title> <booktitle> In Proc. of the 1990 Int'l Conf. on Computer Languages, </booktitle> <pages> pages 79-89, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In single-threaded DSM implementations, such as <ref> [FP89, CBZ91, KDCZ94, SFL + 94, BZS93, DJAR91, BKT90] </ref>, all work on a faulting node is suspended until the fault is handled. In a multi-threaded implementation, other work is done while the remote fault is pending. This makes it possible to overlap communication and computation.
Reference: [BZS93] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In COMPCON '93, </booktitle> <pages> pages 528-537, </pages> <year> 1993. </year>
Reference-contexts: In single-threaded DSM implementations, such as <ref> [FP89, CBZ91, KDCZ94, SFL + 94, BZS93, DJAR91, BKT90] </ref>, all work on a faulting node is suspended until the fault is handled. In a multi-threaded implementation, other work is done while the remote fault is pending. This makes it possible to overlap communication and computation.
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of 13th ACM Symposium On Operating Systems, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In single-threaded DSM implementations, such as <ref> [FP89, CBZ91, KDCZ94, SFL + 94, BZS93, DJAR91, BKT90] </ref>, all work on a faulting node is suspended until the fault is handled. In a multi-threaded implementation, other work is done while the remote fault is pending. This makes it possible to overlap communication and computation. <p> On the next iteration, the pools are run starting at the top of the stack, which ensures that all faulting pools are run first. 2 A DSM has to implement one or more page consistency protocols (PCPs). We have implemented several, including write-invalidate [LH89], implicit invalidate [FLA94], write-shared <ref> [CBZ91] </ref>, and writer-owns [Fre95]. (PCPs are discussed in greater detail in Section 4.2.) A DSM also requires reliable communication. <p> This subsection describes the writer-owns protocol [Fre95] that dynamically eliminates false sharing. A page-consistency protocol (PCP) is a method for maintaining data on a page using some memory consistency model [Mos93]. A single-copy PCP, such as migratory <ref> [CBZ91] </ref>, is one in which only one copy of a page ever exists; therefore, it is inherently consistent. In a multiple-reader/single-writer protocol, such as write-invalidate [LH89], a page remains consistent at all times because all read copies are invalidated when a node writes. Multiple-writer protocols, such as write-shared [CBZ91], allow the <p> as migratory <ref> [CBZ91] </ref>, is one in which only one copy of a page ever exists; therefore, it is inherently consistent. In a multiple-reader/single-writer protocol, such as write-invalidate [LH89], a page remains consistent at all times because all read copies are invalidated when a node writes. Multiple-writer protocols, such as write-shared [CBZ91], allow the local copies of pages to become inconsistent; they regain consistency at specific points in the program through some "consistency operation" (e.g., a barrier synchronization). A single-copy PCP is very simple, but limits concurrency because all accesses are serialized. Multiple-copy PCPs are more complicated, but allow greater concurrency.
Reference: [DJAR91] <author> Partha Dasgupta, Richard J. LeBlanc Jr., Mustaque Ahmad, and Umakishore Ramachandran. </author> <title> The Clouds distributed operating system. </title> <journal> Computer, </journal> <volume> 24(11) </volume> <pages> 34-44, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: In single-threaded DSM implementations, such as <ref> [FP89, CBZ91, KDCZ94, SFL + 94, BZS93, DJAR91, BKT90] </ref>, all work on a faulting node is suspended until the fault is handled. In a multi-threaded implementation, other work is done while the remote fault is pending. This makes it possible to overlap communication and computation.
Reference: [EAL93] <author> Dawson R. Engler, Gregory R. Andrews, and David K. Lowenthal. </author> <title> Shared filaments: Efficient support for fine-grain parallelism on shared-memory multiprocessors. </title> <type> Technical Report 93-13, </type> <institution> Dept. of Computer Science, University of Arizona, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: /* receive a localdiff from each process */ if (maxdiff &lt; EPSILON) done = true; broadcast_done (done); /* send the vaue of done to each process */ - recv_grids (); /* receive results from each process */ - 2.2 An Example of a Run-Time Approach Using Filaments The Filaments package <ref> [FLA94, EAL93] </ref> supports fine-grain parallelism and a shared-memory programming model on the entire range of parallel machines, from shared-memory multiprocessors to networks of workstations. In this paper we limit our discussion to networks of uniprocessor workstations, which we refer to as nodes.
Reference: [EZ93] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: In particular, when processing a pool, a server thread executes a loop, the body of which is the code specified by filaments in the pool. This eliminates a function call for each 1 Systems such as Chores <ref> [EZ93] </ref> and the Uniform System [TC88] have a fine-grain specification and a coarse-grain execution model, but use preprocessor support.
Reference: [FLA94] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. Andrews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201-212, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Moreover, this approach provides a simpler interface for application programmers and compilers, because the run-time system solves most of the difficult problems, freeing the application programmer and compiler writer to concentrate on other areas. We have implemented our approach in an experimental software system called Filaments <ref> [FLA94] </ref>. <p> /* receive a localdiff from each process */ if (maxdiff &lt; EPSILON) done = true; broadcast_done (done); /* send the vaue of done to each process */ - recv_grids (); /* receive results from each process */ - 2.2 An Example of a Run-Time Approach Using Filaments The Filaments package <ref> [FLA94, EAL93] </ref> supports fine-grain parallelism and a shared-memory programming model on the entire range of parallel machines, from shared-memory multiprocessors to networks of workstations. In this paper we limit our discussion to networks of uniprocessor workstations, which we refer to as nodes. <p> On the next iteration, the pools are run starting at the top of the stack, which ensures that all faulting pools are run first. 2 A DSM has to implement one or more page consistency protocols (PCPs). We have implemented several, including write-invalidate [LH89], implicit invalidate <ref> [FLA94] </ref>, write-shared [CBZ91], and writer-owns [Fre95]. (PCPs are discussed in greater detail in Section 4.2.) A DSM also requires reliable communication. Our system uses a novel, low overhead reliable datagram subsystem called Packet, which is beyond the scope of this paper (see [FLA94]). 3.3 Application-Independent Overheads The performance of Filaments programs <p> have implemented several, including write-invalidate [LH89], implicit invalidate <ref> [FLA94] </ref>, write-shared [CBZ91], and writer-owns [Fre95]. (PCPs are discussed in greater detail in Section 4.2.) A DSM also requires reliable communication. Our system uses a novel, low overhead reliable datagram subsystem called Packet, which is beyond the scope of this paper (see [FLA94]). 3.3 Application-Independent Overheads The performance of Filaments programs are application-dependent. For example, an application with a large ratio of computation to communication will perform much better than one with a small ratio.
Reference: [FP89] <author> Brett D. Fleisch and Gerald J. Popek. </author> <title> Mirage: a coherent distributed shared memory design. </title> <booktitle> In Proceedings of 12th ACM Symposium On Operating Systems, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: In single-threaded DSM implementations, such as <ref> [FP89, CBZ91, KDCZ94, SFL + 94, BZS93, DJAR91, BKT90] </ref>, all work on a faulting node is suspended until the fault is handled. In a multi-threaded implementation, other work is done while the remote fault is pending. This makes it possible to overlap communication and computation. <p> Alternatively, thrashing can be controlled if the run-time system keeps a page on a node for some minimum period of time (the time window coherence protocol of Mirage <ref> [FP89] </ref> is an example). The write-shared protocol (WS) tolerates false sharing. Because there can be multiple copies 3 When nodes are accessing the same location with at least one node writing, true sharing occurs.
Reference: [Fre95] <author> Vincent W. Freeh. Writer-Owns: </author> <title> a new page consistency protocol for dynamically controlling thrashing on distributed-shared memory systems. </title> <month> December </month> <year> 1995. </year>
Reference-contexts: We have implemented several, including write-invalidate [LH89], implicit invalidate [FLA94], write-shared [CBZ91], and writer-owns <ref> [Fre95] </ref>. (PCPs are discussed in greater detail in Section 4.2.) A DSM also requires reliable communication. Our system uses a novel, low overhead reliable datagram subsystem called Packet, which is beyond the scope of this paper (see [FLA94]). 3.3 Application-Independent Overheads The performance of Filaments programs are application-dependent. <p> This subsection describes the writer-owns protocol <ref> [Fre95] </ref> that dynamically eliminates false sharing. A page-consistency protocol (PCP) is a method for maintaining data on a page using some memory consistency model [Mos93]. A single-copy PCP, such as migratory [CBZ91], is one in which only one copy of a page ever exists; therefore, it is inherently consistent.
Reference: [GB93] <author> M. Gupta and P. Banerjee. </author> <title> PARADIGM: A compiler for automated data distribution on multi-computers. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <pages> pages 357-367, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Most current approaches determine data placements statically. They can generally be divided into two categories: using language primitives, such as the ones in HPF [HPF93], or compiler analysis, such as the work reported in [AL93], <ref> [GB93] </ref>, and [KK94]. Language primitives involve the programmer in the choice of data placement; unfortunately, the best placement may be difficult or impossible for the programmer to determine.
Reference: [Har64] <author> Francis H. Harlow. </author> <title> The particle-in-cell computing method for fluid dynamics. </title> <editor> In Bernie Alder, editor, </editor> <booktitle> Methods in Computational Physics, </booktitle> <pages> pages 319-343. </pages> <publisher> Academic Press, Inc., </publisher> <year> 1964. </year>
Reference-contexts: Block placements tend to work well for stencil-based applications such as Jacobi iteration, because such applications have spatial and temporal locality, a balanced workload, and regular communication between neighboring nodes. Block placements also work well for applications such as particle-in-cell codes <ref> [Har64] </ref>, that have locality and a regular "nearest neighbor" communication pattern.
Reference: [HB92] <author> Matthew Haines and Wim Bohm. </author> <title> The design of VISA: A virtual shared addressing system. </title> <type> Technical Report CS-92-120, </type> <institution> Colorado State University, </institution> <month> May </month> <year> 1992. </year> <month> 20 </month>
Reference-contexts: In a multi-threaded implementation, other work is done while the remote fault is pending. This makes it possible to overlap communication and computation. VISA, a DSM written for the functional language Sisal, allows less general overlap of communication and computation <ref> [HB92] </ref>. The address space of each node contains both shared and private sections. Shared user data (matrices, linked lists, etc.) are stored in the shared section, which is divided into individually protected pages of 4K bytes each.
Reference: [HFM88] <author> D. Hansgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barier synchronization. </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-18, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The final Filaments overhead is due to synchronization, which results from barriers in iterative applications. The overhead of barriers is a function of the number of nodes. Filaments uses a tournament barrier with broadcast dissemination, which has O (p) messages and a latency of O (log p) messages <ref> [HFM88] </ref>. Barrier synchronization times are shown in Figure 3.
Reference: [HKT92] <author> S. Hiranandani, K. Kennedy, and C.W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: We assume that any node can reference any data element. We also assume the owner-computes rule <ref> [HKT92] </ref>. This means each data element has an "owner", which is the only node that updates the element; however, other nodes may reference the element. The elements of a data structure can be placed on the nodes in numerous ways.
Reference: [HMS + 95] <author> Yuan-Shin Hwang, Bongki Moon, Shamik D. Sharma, Ravi Ponnusamy, Raja Das, and Joel H. Saltz. </author> <title> Runtime and language support for compiling adaptive irregular programs on distributed-memory machines. </title> <journal> Software|Practice and Experience, </journal> <volume> 25(6) </volume> <pages> 597-621, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Our approach is to determine data placements dynamically, without requiring programmers or compilers to make such decisions. (Different dynamic approaches are discussed in [Who91] and <ref> [HMS + 95] </ref>.) This approach is implemented in a prototype system called Adapt [LA95], which is a subsystem of the Filaments package. The goal of Adapt is to minimize the overall completion time of an application, which is determined by the completion time of the slowest node.
Reference: [HPF93] <institution> High Performance Fortran language specification. </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Most current approaches determine data placements statically. They can generally be divided into two categories: using language primitives, such as the ones in HPF <ref> [HPF93] </ref>, or compiler analysis, such as the work reported in [AL93], [GB93], and [KK94]. Language primitives involve the programmer in the choice of data placement; unfortunately, the best placement may be difficult or impossible for the programmer to determine.
Reference: [KDCZ94] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: In single-threaded DSM implementations, such as <ref> [FP89, CBZ91, KDCZ94, SFL + 94, BZS93, DJAR91, BKT90] </ref>, all work on a faulting node is suspended until the fault is handled. In a multi-threaded implementation, other work is done while the remote fault is pending. This makes it possible to overlap communication and computation.
Reference: [KK94] <author> Ken Kennedy and Ulrich Kremer. </author> <title> Automatic data layout for High Performance Fortran. </title> <type> Technical Report CRPC-TR94498-S, </type> <institution> Rice University, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Most current approaches determine data placements statically. They can generally be divided into two categories: using language primitives, such as the ones in HPF [HPF93], or compiler analysis, such as the work reported in [AL93], [GB93], and <ref> [KK94] </ref>. Language primitives involve the programmer in the choice of data placement; unfortunately, the best placement may be difficult or impossible for the programmer to determine.
Reference: [LA95] <author> David K. Lowenthal and Gregory R. Andrews. </author> <title> Adaptive data placement for distributed-memory machines. </title> <type> TR 95-13, </type> <institution> University of Arizona, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Our approach is to determine data placements dynamically, without requiring programmers or compilers to make such decisions. (Different dynamic approaches are discussed in [Who91] and [HMS + 95].) This approach is implemented in a prototype system called Adapt <ref> [LA95] </ref>, which is a subsystem of the Filaments package. The goal of Adapt is to minimize the overall completion time of an application, which is determined by the completion time of the slowest node. Three factors affect the completion time of a node: computation time, communication overhead, and delay. <p> Solid lines separate different node's data. independent of the data placement. LU decomposition is an example of an application with a changing workload and a placement-independent communication pattern. Compromise placements can also be useful, such as striping contiguous regions onto each node (see <ref> [LA95] </ref> for details). The Adapt system dynamically selects one of the data placements shown in Figure 4.
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4), </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: On the next iteration, the pools are run starting at the top of the stack, which ensures that all faulting pools are run first. 2 A DSM has to implement one or more page consistency protocols (PCPs). We have implemented several, including write-invalidate <ref> [LH89] </ref>, implicit invalidate [FLA94], write-shared [CBZ91], and writer-owns [Fre95]. (PCPs are discussed in greater detail in Section 4.2.) A DSM also requires reliable communication. <p> A single-copy PCP, such as migratory [CBZ91], is one in which only one copy of a page ever exists; therefore, it is inherently consistent. In a multiple-reader/single-writer protocol, such as write-invalidate <ref> [LH89] </ref>, a page remains consistent at all times because all read copies are invalidated when a node writes. Multiple-writer protocols, such as write-shared [CBZ91], allow the local copies of pages to become inconsistent; they regain consistency at specific points in the program through some "consistency operation" (e.g., a barrier synchronization).
Reference: [McD88] <author> Jeffrey D. McDonald. </author> <title> A direct particle simulation method for hypersonic rarified flow. </title> <type> Technical Report 411, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: The basic structure of this application mimics the behavior of MP3D <ref> [McD88] </ref>. Several molecules move through a two-dimensional grid of cells, colliding with other molecules that occupy the same cell. Colliding molecules move to a random location on the grid, which can in practice lead to a clustering of particles in certain regions of the grid.
Reference: [Mos93] <author> David Mosberger. </author> <title> Memory consistency models. </title> <journal> Operating Systems Review, </journal> <volume> 27(1) </volume> <pages> 18-26, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: This subsection describes the writer-owns protocol [Fre95] that dynamically eliminates false sharing. A page-consistency protocol (PCP) is a method for maintaining data on a page using some memory consistency model <ref> [Mos93] </ref>. A single-copy PCP, such as migratory [CBZ91], is one in which only one copy of a page ever exists; therefore, it is inherently consistent. In a multiple-reader/single-writer protocol, such as write-invalidate [LH89], a page remains consistent at all times because all read copies are invalidated when a node writes.
Reference: [SFL + 94] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Sixth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: In single-threaded DSM implementations, such as <ref> [FP89, CBZ91, KDCZ94, SFL + 94, BZS93, DJAR91, BKT90] </ref>, all work on a faulting node is suspended until the fault is handled. In a multi-threaded implementation, other work is done while the remote fault is pending. This makes it possible to overlap communication and computation.
Reference: [TC88] <author> Robert H. Thomas and Will Crowther. </author> <title> The Uniform system: an approach to runtime support for large scale shared memory parallel processors. </title> <booktitle> In 1988 Conference on Parallel Processing, </booktitle> <pages> pages 245-254, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In particular, when processing a pool, a server thread executes a loop, the body of which is the code specified by filaments in the pool. This eliminates a function call for each 1 Systems such as Chores [EZ93] and the Uniform System <ref> [TC88] </ref> have a fine-grain specification and a coarse-grain execution model, but use preprocessor support. Filaments generates different codes at compile time, but chooses among them at run time. 7 filament, but the server thread still has to traverse the list of filament descriptors and load the arguments.
Reference: [Who91] <author> Skef Wholey. </author> <title> Automatic Data Mapping for Distributed-Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> May </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: Our approach is to determine data placements dynamically, without requiring programmers or compilers to make such decisions. (Different dynamic approaches are discussed in <ref> [Who91] </ref> and [HMS + 95].) This approach is implemented in a prototype system called Adapt [LA95], which is a subsystem of the Filaments package. The goal of Adapt is to minimize the overall completion time of an application, which is determined by the completion time of the slowest node.
References-found: 26

