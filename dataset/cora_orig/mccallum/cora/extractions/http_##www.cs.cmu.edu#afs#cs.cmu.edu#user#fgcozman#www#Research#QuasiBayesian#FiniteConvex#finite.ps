URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/fgcozman/www/Research/QuasiBayesian/FiniteConvex/finite.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/fgcozman/www/Research/QuasiBayesian/FiniteConvex/
Root-URL: 
Title: Robustness Analysis of Bayesian Networks with Finitely Generated Convex Sets of Distributions  
Author: Fabio Cozman CMU-RI-TR - 
Note: This research is supported in part by NASA under Grant NAGW-1175. Fabio Cozman was supported under a scholarship from CNPq, Brazil.  
Date: January 28, 1997  
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute Carnegie Mellon University  
Abstract: This paper presents exact solutions and convergent approximations for inferences in Bayesian networks associated with finitely generated convex sets of distributions. Robust Bayesian inference is the calculation of bounds on posterior values given perturbations in a probabilistic model. The paper presents exact inference algorithms and analyzes the circumstances where exact inference becomes intractable. Two classes of algorithms for numeric approximations are developed through transformations on the original model. The first transformation reduces the robust inference problem to the estimation of probabilistic parameters in a Bayesian network. The second transformation uses Lavine's bracketing algorithm to generate a sequence of maximization problems in a Bayesian network. The analysis is extended to the *-contaminated, the lower density bounded, the belief function, the sub-sigma, the density bounded, the total variation and the density ratio classes of distributions. c fl1996 Carnegie Mellon University
Abstract-found: 1
Intro-found: 1
Reference: [ Berger, 1985 ] <author> Berger, J. O. </author> <year> 1985. </year> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Robustness analysis employs sets of distributions to model perturbations in the parameters of a probability distribution <ref> [ Berger, 1985; 1990; Huber, 1980; Kadane, 1984; Wasserman, 1992b ] </ref> . Robust Bayesian inference is the calculation of bounds on posterior values given such perturbations. <p> Since there are jpa (x)j possible values for c, there are j^xj jpa (x)j vertices for the joint distribution. 10.1 The *-contaminated Class An *-contaminated class is characterized by a distribution p () and a real number * 2 (0; 1) <ref> [ Berger, 1985 ] </ref> : r (x) = (1 *)p (x) + *q (x): (9) This model is appropriate when we expect p (x) to be correct a fraction (1 *) of the time, but otherwise any distribution is possible.
Reference: [ Berger, 1990 ] <author> Berger, J. O. </author> <year> 1990. </year> <title> Robust bayesian analysis: Sensitivity to the prior. </title> <journal> Journal of Statistical Planning and Inference 25 </journal> <pages> 303-328. </pages>
Reference-contexts: Sometimes it is hard to specify all the numbers p (x) for all elements of ^x; a more straightforward approach is to specify probability masses for non-overlapping subsets of ^x. This characterizes a convex set of distributions for x called a sub-sigma class <ref> [ Berger, 1990; Lambert & Duncan, 1986; Manski, 1981 ] </ref> . For example, suppose ^x has 20 values between zero and one, and one wants to specify a distribution for x that is precise for the values around 0.5 but rather imprecise in the tails. <p> The density ratio class is probably the easiest to elicit in a Bayesian network setup. Consider the following equivalent definition <ref> [ Berger, 1990 ] </ref> : a density ratio class consists of all probability densities p (A) so that, for any events A and B: l (A) p (B) u (A) : (10) The class is defined by intervals of probability odds: the ratio between the probability of two events.
Reference: [ Breese & Fertig, 1991 ] <author> Breese, J. S., and Fertig, K. W. </author> <year> 1991. </year> <title> Decision making with interval influence diagrams. </title> <editor> In Bonissone, P. P.; Henrion, M.; Kanal, L. N.; and Lemmer, J. F., eds., </editor> <booktitle> Uncertainty in Artificial Intelligence 6. </booktitle> <publisher> North-Holland: Elsevier Science. </publisher> <pages> 467-478. </pages>
Reference-contexts: Two possibilities arise. One is to focus on particular classes of intervals that admit efficient algorithms [ Chrisman, 1996b ] . Another is to settle for approximations of inference bounds <ref> [ Breese & Fertig, 1991; Tessem, 1992 ] </ref> . So far, no approximation with convergence guarantees or provably small error bounds has been found. This paper derives exact solutions and convergent approximations for robustness analysis of Bayesian networks associated with finitely generated sets of distributions. <p> k where a k and a k are the minimum and maximum values of x respectively. 10.2 The lower density bounded class Consider the set of all distributions p (x) such that p (x) l (x); where l () is an arbitrary non-negative function whose sum is smaller than one <ref> [ Breese & Fertig, 1991 ] </ref> . Call this class the lower density bounded class. There are approximations (without error bounds) for inferences with this class [ Breese & Fertig, 1991 ] . The results in this paper improve this to exact and provably convergent inference algorithms. <p> all distributions p (x) such that p (x) l (x); where l () is an arbitrary non-negative function whose sum is smaller than one <ref> [ Breese & Fertig, 1991 ] </ref> . Call this class the lower density bounded class. There are approximations (without error bounds) for inferences with this class [ Breese & Fertig, 1991 ] . The results in this paper improve this to exact and provably convergent inference algorithms.
Reference: [ Buntine, 1994 ] <author> Buntine, W. L. </author> <year> 1994. </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research 2 </journal> <pages> 159-225. </pages>
Reference-contexts: Note that the optimizing vector must have, for each i, ij = 1 for some j. The difference be tween this and maximum likelihood parameter estimation is that the distribution to be optimized involves the posterior distribution instead of the prior. We can use learning techniques for this problem <ref> [ Buntine, 1994 ] </ref> . Notice that the optimization procedure has to be repeated for each of the values of the queried variable x q . 8.1 Gradient descent techniques Optimization through gradient descent techniques, such as the conjugate gradient algorithm, have been used for learning in Bayesian networks.
Reference: [ Cannings & Thompson, 1981 ] <author> Cannings, C., and Thompson, E. A. </author> <year> 1981. </year> <title> Genealogical and Genetic Structure. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: The calculation of posterior marginals for a queried variable can be done by a technique like variable elimination [ Zhang & Poole, 1996 ] or bucket elimination [ Dechter, 1996 ] , or a more sophisticated clustering method such as peeling <ref> [ Cannings & Thompson, 1981 ] </ref> or graph triangulation [ Jensen, 1996 ] . Another standard problem is the determination of maximum a posteriori hyposthesis (called MAP) [ Dechter, 1996 ] .
Reference: [ Cano, Delgado, & Moral, 1993 ] <author> Cano, J.; Delgado, M.; and Moral, S. </author> <year> 1993. </year> <title> An axiomatic framework for propagating uncertainty in directed acyclic networks. </title> <journal> International Journal of Approximate Reasoning 8 </journal> <pages> 253-280. </pages>
Reference-contexts: So far, no approximation with convergence guarantees or provably small error bounds has been found. This paper derives exact solutions and convergent approximations for robustness analysis of Bayesian networks associated with finitely generated sets of distributions. Exact algorithms which generalize previous ideas <ref> [ Cano, Delgado, & Moral, 1993 ] </ref> are presented and their complexity is characterized. We describe the situations that lead to high complexity in exact algorithms. Two classes of algorithms for numeric approximations are developed. <p> Cano et all. considered the same type of structures analyzed here, focusing more on the axiomatic aspects of inference, and producing explicit algorithms for networks with poly-tree topology <ref> [ Cano, Delgado, & Moral, 1993 ] </ref> . Here we discuss how to generate exact algorithms for arbitrary networks associated with finitely generated credal sets. The technique we use to generate exact algorithms is based on a transformation that operates on the Quasi-Bayesian network and produces a Bayesian network.
Reference: [ Chrisman, 1996a ] <author> Chrisman, L. </author> <year> 1996a. </year> <title> Independence with lower and upper probabilities. </title> <booktitle> Proc. Twelfth Conference Uncertainty in Artificial Intelligence 169-177. </booktitle>
Reference-contexts: Second, we may deal with a group of disagreeing experts, each specifying a particular distribution [ Levi, 1980; Seidenfeld & Schervish, 1990 ] . Third, we may be interested in abstracting away parts of a model and assessing the effects of this abstraction <ref> [ Chrisman, 1996a; Ha & Haddawy, 1996 ] </ref> . To emphasize the relationship between abstraction and robustness analysis, suppose a decision-maker is contemplating a situation with several hundred variables. For a particular inference, some variables may minimally affect the probabilities of interest.
Reference: [ Chrisman, 1996b ] <author> Chrisman, L. </author> <year> 1996b. </year> <title> Propagation of 2-monotone lower probabilities on an undirected graph. </title> <booktitle> Proc. Twelfth Conference Uncertainty in Artificial Intelligence 178-186. </booktitle>
Reference-contexts: The inefficiency comes from the fact that successive application of Bayes rule can create exponentially large number of vertices in a polytope representation. Two possibilities arise. One is to focus on particular classes of intervals that admit efficient algorithms <ref> [ Chrisman, 1996b ] </ref> . Another is to settle for approximations of inference bounds [ Breese & Fertig, 1991; Tessem, 1992 ] . So far, no approximation with convergence guarantees or provably small error bounds has been found.
Reference: [ Dechter, 1996 ] <author> Dechter, R. </author> <year> 1996. </year> <title> Bucket elimination: A unifying framework for probabilistic inference. </title> <booktitle> Proc. of the Twelfth Conference Uncertainty in Artificial Intelligence 211-219. </booktitle>
Reference-contexts: Our algorithms assume efficient solutions for two standard problems in usual Bayesian networks: computation of posterior marginals and maximum a posteriori hypothesis. The calculation of posterior marginals for a queried variable can be done by a technique like variable elimination [ Zhang & Poole, 1996 ] or bucket elimination <ref> [ Dechter, 1996 ] </ref> , or a more sophisticated clustering method such as peeling [ Cannings & Thompson, 1981 ] or graph triangulation [ Jensen, 1996 ] . Another standard problem is the determination of maximum a posteriori hyposthesis (called MAP) [ Dechter, 1996 ] . <p> [ Zhang & Poole, 1996 ] or bucket elimination <ref> [ Dechter, 1996 ] </ref> , or a more sophisticated clustering method such as peeling [ Cannings & Thompson, 1981 ] or graph triangulation [ Jensen, 1996 ] . Another standard problem is the determination of maximum a posteriori hyposthesis (called MAP) [ Dechter, 1996 ] . Suppose we select a set of variables d as decision variables, and the objective is to find values for the decision variables such that we obtain the maximum value of marginal probability p (d). <p> The notation p d i indicates that the function p i has the decision variables fixed. With this notation, the objective of the MAP algorithm is to obtain: max 2 X i i 5 : (2) There are algorithms which attack this problem in the most general form <ref> [ Dechter, 1996 ] </ref> .
Reference: [ Dempster, Laird, & Rubin, 1977 ] <author> Dempster, A. P.; Laird, N. M.; and Rubin, D. B. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal Royal Statistical Society B 44 </journal> <pages> 1-38. </pages>
Reference-contexts: be constructed by selecting an ini tial value for fi and, at each step, normalizing the values of fi to ensure they represent proper distributions [ Russell et al., 1995 ] . 8.2 Expectation Maximization (EM) algorithm The EM algorithm produces a maximum likelihood estimate by maximizing the log-likelihood expression <ref> [ Dempster, Laird, & Rubin, 1977 ] </ref> . To solve the robust inference problem, we must maximize the posterior log-likelihood L (fi). We show how the original EM algo rithm can be extended to a Quasi-Bayesian Expectation Maximization (QEM) algorithm and prove convergence properties. <p> jfi) = x62fx q ;eg p (x q je; fi 0 ) p (xjx q ; e; fi) X log p (x; x q je; fi 0 )p (xjx q ; e; fi): Consequently: L (fi 0 ) = Q (fi 0 jfi) + H (fi 0 jfi) and (by <ref> [ Dempster, Laird, & Rubin, 1977, Lemma 1 ] </ref> ): H (fi k+1 jfi k ) H (fi k jfi k ): Since by construction we have Q (fi k+1 jfi k ) &gt; Q (fi k jfi k ), we obtain: L (fi k+1 ) = Q (fi k+1 jfi
Reference: [ DeRobertis & Hartigan, 1981 ] <author> DeRobertis, L., and Hartigan, J. A. </author> <year> 1981. </year> <title> Bayesian inference using intervals of measures. </title> <journal> The Annals of Statistics 9(2) </journal> <pages> 235-244. </pages>
Reference-contexts: class presents efficiency problems since even this straighfor-ward translation into inequalities requires a substantial amount of space (there are 2 j^xj inequalities, one for each subset of ^x). 10.7 The density ratio class A density ratio class consists of all probability densities p (A) so that for any event A <ref> [ DeRobertis & Hartigan, 1981 ] </ref> : where l (A) and u (A) are arbitrary positive measures such that l () u () and ff is some positive real number. Notice that the distributions defined by a density ratio class form a convex set.
Reference: [ Fine, 1988 ] <author> Fine, T. L. </author> <year> 1988. </year> <title> Lower probability models for uncertainty and nondeterministic processes. </title> <journal> Journal of Statistical Planning and Inference 20 </journal> <pages> 389-411. </pages>
Reference: [ Geman & Geman, 84 ] <author> Geman, S., and Geman, D. </author> <title> 84. Stochastic relaxation, gibbs distribution and the bayesian restoration of images. </title> <journal> IEEE Transactios on Patter Analysis and Machine Intelligence PAMI-6(6):721-741. </journal>
Reference-contexts: The idea is to visit a set of configurations for the artificial variables and select only the configurations that are either better than the current configuration, or that are accepted based on a Gibbs distribution <ref> [ Geman & Geman, 84; Press et al., 1992 ] </ref> . The Gibbs distribution is regulated through a "temperature schedule", which is set heuristically.
Reference: [ Giron & Rios, 1980 ] <author> Giron, F. J., and Rios, S. </author> <year> 1980. </year> <title> Quasi-bayesian behaviour: A more realistic approach to decision making? In Bernardo, </title> <editor> J. M.; DeG-root, J. H.; Lindley, D. V.; and Smith, A. F. M., eds., </editor> <booktitle> Bayesian Statistics. </booktitle> <address> Valencia, Spain: </address> <publisher> University Press. </publisher> <pages> 17-38. </pages>
Reference-contexts: The second class of algorithms uses Lavine's bracketing method [ Lavine, 1991 ] to simplify the optimization procedures required for robust inferences. We use the theory of convex sets of probability distributions, referred to as Quasi-Bayesian theory <ref> [ Giron & Rios, 1980 ] </ref> . Other names for the mathematical theory are theory of lower expectations [ Huber, 1980 ] and theory of lower previsions [ Walley, 1991 ] . <p> A class of algorithms attack this problem when decision problem can be represented as an influence diagram [ Jensen & Jensen, 1994 ] . 4 Quasi-Bayesian theory Quasi-Bayesian theory uses convex sets of distributions to represent beliefs and to evaluate decisions <ref> [ Giron & Rios, 1980 ] </ref> . <p> We denote the convex hull of a finite set of functions f i () by L The set of distributions maintained by an agent is called the credal set, and its existence is postulated on the grounds of axioms about preferences <ref> [ Giron & Rios, 1980 ] </ref> . To simplify terminology, we use the term credal set only when it refers to a set of distributions containing more than one element. Convex sets of conditional distributions are used to represent conditional beliefs. <p> We use two well-known results about posterior credal sets in this paper. First, to obtain a posterior credal set, one has to apply Bayes rule only to the vertices of a prior credal set and take the convex hull of the resulting distributions <ref> [ Giron & Rios, 1980 ] </ref> . Second, to obtain maximum and minimum values of posterior probabilities, we must look only at the vertices of the posterior credal sets [ Walley, 1991 ] .
Reference: [ Good, 1983 ] <author> Good, I. J. </author> <year> 1983. </year> <title> Good Thinking: The Foundations of Probability and its Applications. </title> <publisher> Min-neapolis: University of Minnesota Press. </publisher>
Reference: [ Ha & Haddawy, 1996 ] <author> Ha, V., and Haddawy, P. </author> <year> 1996. </year> <title> Theoretical foundations for abstraction-based probabilistic planning. </title> <booktitle> Proc. Twelfth Conference Uncertainty in Artificial Intelligence 291-298. </booktitle>
Reference-contexts: Second, we may deal with a group of disagreeing experts, each specifying a particular distribution [ Levi, 1980; Seidenfeld & Schervish, 1990 ] . Third, we may be interested in abstracting away parts of a model and assessing the effects of this abstraction <ref> [ Chrisman, 1996a; Ha & Haddawy, 1996 ] </ref> . To emphasize the relationship between abstraction and robustness analysis, suppose a decision-maker is contemplating a situation with several hundred variables. For a particular inference, some variables may minimally affect the probabilities of interest.
Reference: [ Halpern & Fagin, 1992 ] <author> Halpern, J. Y., and Fagin, R. </author> <year> 1992. </year> <title> Two views of belief: Belief as generalized probability and belief as evidence. </title> <booktitle> Artificial Intelligence 54 </booktitle> <pages> 275-317. </pages>
Reference-contexts: There are expressions that generate exact bounds for posterior quantities in univariate distributions <ref> [ Halpern & Fagin, 1992 ] </ref> , but the same does not apply to marginalization of multivariate belief function classes 2 . Our solution is to reduce belief function classes to finitely generated convex sets of distributions and apply the methods derived before.
Reference: [ Heckerman, 1990 ] <author> Heckerman, D. </author> <year> 1990. </year> <title> An empirical comparison of three inference methods. </title> <editor> In Shachter, R. D.; Kanal, L. N.; and Lemmer, J. F., eds., </editor> <booktitle> Uncertainty in Artificial Intelligence 4. </booktitle> <publisher> North-Holland: Elsevier Science Publishers. </publisher> <pages> 283-303. </pages>
Reference-contexts: Odds have been advocated for the elicitation of Bayesian networks, as the most natural method to obtain probability judgements from experts <ref> [ Heckerman, 1990 ] </ref> . It is a short step to accept that intervals of odds can be used to model imprecise judgements of probability.
Reference: [ Huber, 1980 ] <author> Huber, P. J. </author> <year> 1980. </year> <title> Robust Statistics. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: 1 Introduction Robustness analysis employs sets of distributions to model perturbations in the parameters of a probability distribution <ref> [ Berger, 1985; 1990; Huber, 1980; Kadane, 1984; Wasserman, 1992b ] </ref> . Robust Bayesian inference is the calculation of bounds on posterior values given such perturbations. <p> We use the theory of convex sets of probability distributions, referred to as Quasi-Bayesian theory [ Giron & Rios, 1980 ] . Other names for the mathematical theory are theory of lower expectations <ref> [ Huber, 1980 ] </ref> and theory of lower previsions [ Walley, 1991 ] . In this theory, the imprecision in probability assessments can be due ei ther to difficulties in eliciting information from experts, or to difficulties in processing or combining data.
Reference: [ Jensen & Jensen, 1994 ] <author> Jensen, F., and Jensen, F. V. </author> <year> 1994. </year> <title> From influence diagrams to junction trees. </title> <type> Technical Report R94-2013, </type> <institution> Aalborg University. </institution>
Reference-contexts: A class of algorithms attack this problem when decision problem can be represented as an influence diagram <ref> [ Jensen & Jensen, 1994 ] </ref> . 4 Quasi-Bayesian theory Quasi-Bayesian theory uses convex sets of distributions to represent beliefs and to evaluate decisions [ Giron & Rios, 1980 ] . <p> : : : I k1 ; D 1 : : : D m ) = p (I k jI 0 : : : I k1 ; D 1 : : : D k ); if this condition is true, then efficient algorithms can be found to solve the MAP problem <ref> [ Jensen & Jensen, 1994 ] </ref> .
Reference: [ Jensen, 1996 ] <author> Jensen, F. V. </author> <year> 1996. </year> <title> An Introduction to Bayesian Networks. </title> <address> New York: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: The calculation of posterior marginals for a queried variable can be done by a technique like variable elimination [ Zhang & Poole, 1996 ] or bucket elimination [ Dechter, 1996 ] , or a more sophisticated clustering method such as peeling [ Cannings & Thompson, 1981 ] or graph triangulation <ref> [ Jensen, 1996 ] </ref> . Another standard problem is the determination of maximum a posteriori hyposthesis (called MAP) [ Dechter, 1996 ] .
Reference: [ Kadane, 1984 ] <author> Kadane, J. B. </author> <year> 1984. </year> <title> Robustness of Bayesian Analyses, volume 4 of Studies in Bayesian econometrics. </title> <address> New York: </address> <publisher> Elsevier Science Pub. Co. </publisher>
Reference-contexts: 1 Introduction Robustness analysis employs sets of distributions to model perturbations in the parameters of a probability distribution <ref> [ Berger, 1985; 1990; Huber, 1980; Kadane, 1984; Wasserman, 1992b ] </ref> . Robust Bayesian inference is the calculation of bounds on posterior values given such perturbations.
Reference: [ Kennes, 1992 ] <author> Kennes, R. </author> <year> 1992. </year> <title> Computational aspects of the mobius transformation of graphs. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics 22(2) </journal> <pages> 201-223. </pages>
Reference-contexts: Mathematically, the Mobius transform of a belief function Bel () is: m (A) = BA (1) jABj Bel (B); and there are algorithms to translate between belief functions and basic mass assignments <ref> [ Kennes, 1992 ] </ref> . The value of m (A) corresponds to a probability mass that can be arbitrarily spread among the subsets of A.
Reference: [ Kyburg Jr., 1987 ] <author> Kyburg Jr., H. E. </author> <year> 1987. </year> <title> Bayesian and non-bayesian evidential updating. </title> <booktitle> Artificial Intelligence 31 </booktitle> <pages> 271-293. </pages>
Reference: [ Lambert & Duncan, 1986 ] <author> Lambert, D., and Duncan, G. T. </author> <year> 1986. </year> <title> Single-parameter inference based on partial prior information. </title> <journal> The Canadian Journal of Statistics 14(4) </journal> <pages> 297-305. </pages>
Reference-contexts: Sometimes it is hard to specify all the numbers p (x) for all elements of ^x; a more straightforward approach is to specify probability masses for non-overlapping subsets of ^x. This characterizes a convex set of distributions for x called a sub-sigma class <ref> [ Berger, 1990; Lambert & Duncan, 1986; Manski, 1981 ] </ref> . For example, suppose ^x has 20 values between zero and one, and one wants to specify a distribution for x that is precise for the values around 0.5 but rather imprecise in the tails.
Reference: [ Lavine, 1991 ] <author> Lavine, M. </author> <year> 1991. </year> <title> Sensitivity in bayesian statistics, the prior and the likelihood. </title> <journal> Journal of the American Statistical Association 86(414) </journal> <pages> 396-399. </pages>
Reference-contexts: Two classes of algorithms for numeric approximations are developed. The first class of algorithms reduces the robust inference problem to the estimation of probabilistic parameters in a Bayesian network. The second class of algorithms uses Lavine's bracketing method <ref> [ Lavine, 1991 ] </ref> to simplify the optimization procedures required for robust inferences. We use the theory of convex sets of probability distributions, referred to as Quasi-Bayesian theory [ Giron & Rios, 1980 ] . <p> H (fi k+1 jfi k ) = L (fi k ): The algorithm produces a monotonically increasing and bounded sequence, so the sequence converges to a local maximum of L (fi). 9 Approximate inferences through Lavine's bracketing algorithm Here we borrow a technique from robust Bayesian Statistics, called Lavine's algorithm <ref> [ Lavine, 1991; Wasserman, 1992b ] </ref> . We now adapt Lavine's technique to Bayesian networks with finitely generated credal sets. 9.1 The bracketing algorithm In the most general case, we seek a posterior bound for the expected value of a function u (~x), conditional on evidence e. <p> Every sub-sigma class is a belief function class, since the Mobius transform of a sub-sigma class can be directly read from the the specification of the class. Results of the previous section can be applied to this class. 10.5 The density bounded class A density bounded class <ref> [ Lavine, 1991 ] </ref> is the set of all distributions p (x) such that l (x) p (x) u (x); where l () and u () are arbitrary non-negative measures such that P P x u (x) 1.
Reference: [ Levi, 1980 ] <author> Levi, I. </author> <year> 1980. </year> <title> The Enterprise of Knowledge. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: First, we have to face imperfections in an agent's beliefs, either because the agent had no time, resources, patience, or confidence to provide exact probability values. Second, we may deal with a group of disagreeing experts, each specifying a particular distribution <ref> [ Levi, 1980; Seidenfeld & Schervish, 1990 ] </ref> . Third, we may be interested in abstracting away parts of a model and assessing the effects of this abstraction [ Chrisman, 1996a; Ha & Haddawy, 1996 ] .
Reference: [ Manski, 1981 ] <author> Manski, C. F. </author> <year> 1981. </year> <title> Learning and decision making when subjective probabilities have subjective domains. </title> <journal> The Annals of Statistics 9(1) </journal> <pages> 59-65. </pages>
Reference-contexts: Sometimes it is hard to specify all the numbers p (x) for all elements of ^x; a more straightforward approach is to specify probability masses for non-overlapping subsets of ^x. This characterizes a convex set of distributions for x called a sub-sigma class <ref> [ Berger, 1990; Lambert & Duncan, 1986; Manski, 1981 ] </ref> . For example, suppose ^x has 20 values between zero and one, and one wants to specify a distribution for x that is precise for the values around 0.5 but rather imprecise in the tails.
Reference: [ Pearl, 1988 ] <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: Large bounds indicate poor performance and suggest reevaluation of the abstraction procedure. 3 Background We consider a set ~x of discrete variables. Each variable x i has a finite set of values ^x i . A Bayesian network defines a probability distribution through the expression <ref> [ Pearl, 1988 ] </ref> : p (~x) = i where pa (x i ) is a set of variables, the parents of variable x i . p () refers to a single probability distribution; q () refers to an arbitrary distribution and r () refers to an arbitrary member of a
Reference: [ Press et al., 1992 ] <author> Press, W. H.; Teukolsky, S. A.; Vet-terling, W. T.; and Flannery, B. P. </author> <year> 1992. </year> <title> Numerical Recipes in C. </title> <publisher> Cambridgeshire: Cambridge University Press. </publisher>
Reference-contexts: The idea is to visit a set of configurations for the artificial variables and select only the configurations that are either better than the current configuration, or that are accepted based on a Gibbs distribution <ref> [ Geman & Geman, 84; Press et al., 1992 ] </ref> . The Gibbs distribution is regulated through a "temperature schedule", which is set heuristically.
Reference: [ Ruspini, 1987 ] <author> Ruspini, E. H. </author> <year> 1987. </year> <title> The logical foundations of evidential reasoning. </title> <type> Technical Report SRIN408, </type> <institution> SRI International. </institution>
Reference-contexts: Several other theories use probability intervals: inner/outer measures [ Good, 1983; Halpern & Fagin, 1992; Ruspini, 1987; Suppes, 1974 ] , lower probability theory [ Breese & Fertig, 1991; Chrisman, 1996b; Fine, 1988; Smith, 1961 ] ), convex Bayesianism [ Ky-burg Jr., 1987 ] , Dempster-Shafer theory <ref> [ Ruspini, 1987; Shafer, 1987 ] </ref> , probability/utility sets [ Seidenfeld, 1993 ] . Given a set of functions f i , their convex combination is determined by P i ff i f i , where all ff i are positive and P ff i = 1.
Reference: [ Russell et al., 1995 ] <author> Russell, S.; Binder, J.; Koller, D.; and Kanazawa, K. </author> <year> 1995. </year> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> Proc. Fourteenth International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: These algorithms bene fit from the fact that the necessary gradient calculations can be computed with standard Bayesian network algo rithms <ref> [ Russell et al., 1995 ] </ref> . <p> A conjugate gradient descent can be constructed by selecting an ini tial value for fi and, at each step, normalizing the values of fi to ensure they represent proper distributions <ref> [ Russell et al., 1995 ] </ref> . 8.2 Expectation Maximization (EM) algorithm The EM algorithm produces a maximum likelihood estimate by maximizing the log-likelihood expression [ Dempster, Laird, & Rubin, 1977 ] . To solve the robust inference problem, we must maximize the posterior log-likelihood L (fi).
Reference: [ Seidenfeld & Schervish, 1990 ] <author> Seidenfeld, T., and Schervish, M. </author> <year> 1990. </year> <title> Two perspectives on consensus for (bayesian) inference and decisions. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics 20(1). </journal>
Reference-contexts: First, we have to face imperfections in an agent's beliefs, either because the agent had no time, resources, patience, or confidence to provide exact probability values. Second, we may deal with a group of disagreeing experts, each specifying a particular distribution <ref> [ Levi, 1980; Seidenfeld & Schervish, 1990 ] </ref> . Third, we may be interested in abstracting away parts of a model and assessing the effects of this abstraction [ Chrisman, 1996a; Ha & Haddawy, 1996 ] .
Reference: [ Seidenfeld, 1993 ] <author> Seidenfeld, T. </author> <year> 1993. </year> <title> Outline of a theory of partially ordered preferences. </title> <booktitle> Philosophical Topics 21(1) </booktitle> <pages> 173-188. </pages>
Reference-contexts: inner/outer measures [ Good, 1983; Halpern & Fagin, 1992; Ruspini, 1987; Suppes, 1974 ] , lower probability theory [ Breese & Fertig, 1991; Chrisman, 1996b; Fine, 1988; Smith, 1961 ] ), convex Bayesianism [ Ky-burg Jr., 1987 ] , Dempster-Shafer theory [ Ruspini, 1987; Shafer, 1987 ] , probability/utility sets <ref> [ Seidenfeld, 1993 ] </ref> . Given a set of functions f i , their convex combination is determined by P i ff i f i , where all ff i are positive and P ff i = 1.
Reference: [ Shafer, 1976 ] <author> Shafer, G. </author> <year> 1976. </year> <title> A mathematical theory of evidence. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: When the non-zero basic mass assignments carry mass to single elements of ^y, the convex set collapses to a standard Bayesian model. A belief function can be defined from the basic mass assignment <ref> [ Shafer, 1976 ] </ref> : Bel (A) = BA A belief function contains the same information carried by a basic mass assignment, which is called the Mobius transform of the belief function.
Reference: [ Shafer, 1987 ] <author> Shafer, G. </author> <year> 1987. </year> <journal> Probability judgment in artificial intelligence and expert systems. Statistical Science 2(1) </journal> <pages> 3-44. </pages>
Reference-contexts: Several other theories use probability intervals: inner/outer measures [ Good, 1983; Halpern & Fagin, 1992; Ruspini, 1987; Suppes, 1974 ] , lower probability theory [ Breese & Fertig, 1991; Chrisman, 1996b; Fine, 1988; Smith, 1961 ] ), convex Bayesianism [ Ky-burg Jr., 1987 ] , Dempster-Shafer theory <ref> [ Ruspini, 1987; Shafer, 1987 ] </ref> , probability/utility sets [ Seidenfeld, 1993 ] . Given a set of functions f i , their convex combination is determined by P i ff i f i , where all ff i are positive and P ff i = 1.
Reference: [ Shenoy & Shafer, 1990 ] <author> Shenoy, P. P., and Shafer, G. </author> <year> 1990. </year> <title> Axioms for probability and belief-function propagation. </title> <editor> In Shachter, R. D.; Levitt, T. S.; Kanal, L. N.; and Lemmer, J. F., eds., </editor> <booktitle> Uncertainty in Artificial Intelligence 4. </booktitle> <publisher> North-Holland: Elsevier Science Publishers. </publisher> <pages> 169-198. </pages>
Reference-contexts: For each subset A with a non-zero basic mass assingment, a inequality can be written: p (A) Bel (A): The set of all such inequalities defines the belief function class. 2 Belief and plausibility functions have been efficiently used in Dempster-Shafer theory <ref> [ Shenoy & Shafer, 1990 ] </ref> , where inference is not performed through Bayes rule. 10.4 The sub-sigma class Consider a variable x with values ^x.
Reference: [ Smith, 1961 ] <author> Smith, C. A. B. </author> <year> 1961. </year> <title> Consistency in statistical inference and decision. </title> <journal> Journal Royal Statistical Society B 23 </journal> <pages> 1-25. </pages>
Reference: [ Suppes, 1974 ] <author> Suppes, P. </author> <year> 1974. </year> <title> The measurement of belief. </title> <journal> Journal Royal Statistical Society B 2 </journal> <pages> 160-191. </pages>
Reference: [ Tessem, 1992 ] <author> Tessem, B. </author> <year> 1992. </year> <title> Interval probability propagation. </title> <journal> International Journal of Approximate Reasoning 7 </journal> <pages> 95-120. </pages>
Reference-contexts: Two possibilities arise. One is to focus on particular classes of intervals that admit efficient algorithms [ Chrisman, 1996b ] . Another is to settle for approximations of inference bounds <ref> [ Breese & Fertig, 1991; Tessem, 1992 ] </ref> . So far, no approximation with convergence guarantees or provably small error bounds has been found. This paper derives exact solutions and convergent approximations for robustness analysis of Bayesian networks associated with finitely generated sets of distributions.
Reference: [ Walley, 1991 ] <author> Walley, P. </author> <year> 1991. </year> <title> Statistical Reasoning with Imprecise Probabilities. </title> <address> New York: </address> <publisher> Chapman and Hall. </publisher>
Reference-contexts: We use the theory of convex sets of probability distributions, referred to as Quasi-Bayesian theory [ Giron & Rios, 1980 ] . Other names for the mathematical theory are theory of lower expectations [ Huber, 1980 ] and theory of lower previsions <ref> [ Walley, 1991 ] </ref> . In this theory, the imprecision in probability assessments can be due ei ther to difficulties in eliciting information from experts, or to difficulties in processing or combining data. Section 2 discusses the importance of robustness analysis in Bayesian networks. <p> Second, to obtain maximum and minimum values of posterior probabilities, we must look only at the vertices of the posterior credal sets <ref> [ Walley, 1991 ] </ref> . <p> Approximations for the lower and upper variances can be made through generic optimization algorithms, such as gradient descent or simulated annealing, but convergence properties are lost. To produce a convergent algorithm for calculation of lower and upper variances, we can use Walley's variance envelope theorem <ref> [ Walley, 1991, Theorem G2 ] </ref> , which demonstrates that V [x q ] = min (E [(x q ) 2 ]) and V [x q ] = min (E [(x q ) 2 ]) .
Reference: [ Wasserman & Kadane, 1992 ] <author> Wasserman, L., and Kadane, J. B. </author> <year> 1992. </year> <title> Computing bounds on expectations. </title> <journal> Journal of the American Statistical Association 87(418) </journal> <pages> 516-522. </pages>
Reference-contexts: To produce inferences with this class, we use the following result <ref> [ Wasserman & Kadane, 1992 ] </ref> . A density ratio class defined by l () and u () can be reduced to the union of all density bounded classes defined by fil () and fiu () as fi 2 [1=( x u (x)); 1=( x l (x))].
Reference: [ Wasserman, 1990 ] <author> Wasserman, L. A. </author> <year> 1990. </year> <title> Prior envelopes based on belief functions. </title> <journal> The Annals of Statistics 18(1) </journal> <pages> 454-464. </pages>
Reference-contexts: belief function is associated with a plausibility function: Pl (A) = 1 Bel (A c ): The convex set of distributions defined by the associated basic mass assignment can be expressed in terms of the belief and plausibility functions, as the unique convex set of distributions p () such that <ref> [ Wasserman, 1990 ] </ref> : Bel (A) p (A) Pl (A): Application of Bayes rule to belief function classes is a difficult problem given the non-linear character of Bayes rule.
Reference: [ Wasserman, 1992a ] <author> Wasserman, L. </author> <year> 1992a. </year> <title> Invariance properties of density ratio priors. </title> <journal> The Annals of Statistics 20(4) </journal> <pages> 2177-2182. </pages>
Reference-contexts: This computational advantage breaks in multivariate settings as in general we cannot take marginals only on the lower and upper measures <ref> [ Wasserman, 1992a ] </ref> , since for every variable x with j^xj values, there are j^xjj^x 1j probability inequalities represented by expression (11). To produce inferences with this class, we use the following result [ Wasserman & Kadane, 1992 ] .
Reference: [ Wasserman, 1992b ] <author> Wasserman, L. </author> <year> 1992b. </year> <title> Recent methodological advances in robust bayesian inference. </title> <editor> In Bernardo, J. M.; Berger, J. O.; Dawid, A. P.; and Smith, A. F. M., eds., </editor> <booktitle> Bayesian Statistics 4. </booktitle> <publisher> Oxford University Press. </publisher> <pages> 483-502. </pages>
Reference-contexts: 1 Introduction Robustness analysis employs sets of distributions to model perturbations in the parameters of a probability distribution <ref> [ Berger, 1985; 1990; Huber, 1980; Kadane, 1984; Wasserman, 1992b ] </ref> . Robust Bayesian inference is the calculation of bounds on posterior values given such perturbations. <p> H (fi k+1 jfi k ) = L (fi k ): The algorithm produces a monotonically increasing and bounded sequence, so the sequence converges to a local maximum of L (fi). 9 Approximate inferences through Lavine's bracketing algorithm Here we borrow a technique from robust Bayesian Statistics, called Lavine's algorithm <ref> [ Lavine, 1991; Wasserman, 1992b ] </ref> . We now adapt Lavine's technique to Bayesian networks with finitely generated credal sets. 9.1 The bracketing algorithm In the most general case, we seek a posterior bound for the expected value of a function u (~x), conditional on evidence e. <p> case of u (x) 1 leads to the lower density bounded class, but the method used before in the lower density bounded class does not apply to the more general set ting. 10.6 The total variation class The total variation class is the set of distributions p (x) such that <ref> [ Wasserman, 1992b ] </ref> : jp (A) r (A)j * for any event A; where r (x) is a given probability distribution. This class forms the neighborhood of all distributions close to r (x) up to *.
Reference: [ Zhang & Poole, 1996 ] <author> Zhang, N. L., and Poole, D. </author> <year> 1996. </year> <title> Exploiting causal independence in Bayesian network inference. </title> <journal> Journal of Artificial Intelligence Research 301-328. </journal>
Reference-contexts: Our algorithms assume efficient solutions for two standard problems in usual Bayesian networks: computation of posterior marginals and maximum a posteriori hypothesis. The calculation of posterior marginals for a queried variable can be done by a technique like variable elimination <ref> [ Zhang & Poole, 1996 ] </ref> or bucket elimination [ Dechter, 1996 ] , or a more sophisticated clustering method such as peeling [ Cannings & Thompson, 1981 ] or graph triangulation [ Jensen, 1996 ] .
References-found: 46

