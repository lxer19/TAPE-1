URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P348.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: COMPUTING LARGE SPARSE JACOBIAN MATRICES USING AUTOMATIC DIFFERENTIATION  
Phone: 60439  
Author: Brett M. Averick Jorge J. More, Christian H. Bischof, Alan Carle and Andreas Griewank 
Note: This work was supported by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38. Work supported in part by the Center for Research on Parallel Computation, under NSF Cooperative Agreement No. CCR-8809615.  
Date: January 1993  
Address: 9700 South Cass Avenue Argonne, Illinois  Preprint MCS-P348-0193  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> B. M. Averick, R. G. Carter, J. J. Mor e, and G.-L. Xue, </author> <title> The MINPACK-2 test problem collection, </title> <type> Preprint MCS-P153-0692, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1992. </year>
Reference-contexts: The descriptions are brief since the problems are part of the MINPACK-2 test problem collection; the current version of this collection is described by Averick, Carter, More, and Xue <ref> [1] </ref>. For each problem, the MINPACK-2 test collection contains subroutines that define the sparsity pattern of f 0 (x) and evaluate f (x) and f 0 (x)V for any V 2 IR nfip . These problems are representative of computational problems found in applications.
Reference: [2] <author> C. Bischof, A. Carle, G. Corliss, A. Griewank, and P. Hovland, ADIFOR: </author> <title> Generating derivative codes from Fortran programs, </title> <booktitle> Scientific Programming, 1 (1992), </booktitle> <pages> pp. 1-29. </pages>
Reference-contexts: In contrast to fully symbolic differentiation, both operations count and storage requirement can be a priori bounded in terms of the complexity of the original function code. The ADIFOR (Automatic Differentiation in Fortran) tool <ref> [2, 5, 4] </ref> provides automatic differentiation of programs written in Fortran 77.
Reference: [3] <author> C. Bischof, G. Corliss, and A. Griewank, </author> <title> Computing second- and higher-order derivatives through univariate Taylor series, </title> <type> Preprint MCS-P296-0392, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: When there are several independent and dependent variables, the operation count for evaluating the Jacobian matrix may be lowest for certain mixed strategies [15] rather than for the forward or reverse mode. AD can also be extended to the accurate evaluation of second and higher derivatives <ref> [13, 7, 3] </ref>. A comprehensive collection on the theory, implementation, and some earlier applications can be found in [14]. In contrast to the approximation of derivatives by function differences, AD incurs no truncation error.
Reference: [4] <author> C. Bischof and A. Griewank, ADIFOR: </author> <title> A Fortran system for portable automatic differentiation, </title> <booktitle> in Proceedings of the 4th Symposium on Multidisciplinary Analysis and Optimization, </booktitle> <institution> AIAA Paper 92-4744, American Institute of Aeronautics and Astronautics, </institution> <year> 1992, </year> <pages> pp. 433-441. </pages>
Reference-contexts: In contrast to fully symbolic differentiation, both operations count and storage requirement can be a priori bounded in terms of the complexity of the original function code. The ADIFOR (Automatic Differentiation in Fortran) tool <ref> [2, 5, 4] </ref> provides automatic differentiation of programs written in Fortran 77.
Reference: [5] <author> C. Bischof and P. Hovland, </author> <title> Using ADIFOR to compute dense and sparse Ja-cobians, </title> <type> Tech. Report MCS-TM-158, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1991. </year> <month> 10 </month>
Reference-contexts: In contrast to fully symbolic differentiation, both operations count and storage requirement can be a priori bounded in terms of the complexity of the original function code. The ADIFOR (Automatic Differentiation in Fortran) tool <ref> [2, 5, 4] </ref> provides automatic differentiation of programs written in Fortran 77.
Reference: [6] <author> A. Carle, K. D. Cooper, R. T. Hood, K. Kennedy, L. Torczon, and S. K. Warren, </author> <title> A practical environment for scientific programming, </title> <journal> IEEE Computer, </journal> <volume> 20 (1987), </volume> <pages> pp. 75-89. </pages>
Reference-contexts: The resulting decrease in complexity compared with an implementation based entirely on the forward mode is usually substantial. In contrast to some earlier AD implementations [16], the source translator ADIFOR was designed from the outset with large-scale codes in mind. The facilities of the ParaScope Fortran environment <ref> [6] </ref> control flow and data dependence flow information. ADIFOR 4 produces portable Fortran 77 code and accepts almost all of Fortran 77|in particular, arbitrary calling sequences, nested subroutines, common blocks, and equivalences. The ADIFOR-generated code tries to preserve vectorization and parallelism in the original code.
Reference: [7] <author> B. Christianson, </author> <title> Reverse accumulation and accurate rounding error estimates for Taylor series coefficients, Optimization Methods and Software, </title> <booktitle> 1 (1992), </booktitle> <pages> pp. 81-94. </pages>
Reference-contexts: When there are several independent and dependent variables, the operation count for evaluating the Jacobian matrix may be lowest for certain mixed strategies [15] rather than for the forward or reverse mode. AD can also be extended to the accurate evaluation of second and higher derivatives <ref> [13, 7, 3] </ref>. A comprehensive collection on the theory, implementation, and some earlier applications can be found in [14]. In contrast to the approximation of derivatives by function differences, AD incurs no truncation error.
Reference: [8] <author> T. F. Coleman, B. S. Garbow, and J. J. Mor e, </author> <title> Fortran subroutines for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. </pages> <month> 346-347. </month> <title> [9] , Software for estimating sparse Jacobian matrices, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 10 (1984), </volume> <pages> pp. 329-345. </pages>
Reference-contexts: Coleman, Garbow, and More <ref> [9, 8] </ref> describes software for the partitioning problem. Given a representation of the sparsity structure of f 0 (x), these algorithms produce a partitioning of the columns of f 0 (x) into p structurally orthogonal groups. For many sparsity patterns, p is small and independent of n. <p> Subroutines for these tasks are described by Coleman, Garbow, and More <ref> [9, 8] </ref>. Subroutine dsm takes the sparsity pattern of the Jacobian matrix and produces a partitioning of the columns of f 0 (x) into p structurally orthogonal groups, while subroutine fdjs converts the compressed Jacobian matrix into a sparse matrix format.
Reference: [10] <author> T. F. Coleman and J. J. Mor e, </author> <title> Estimation of sparse Jacobian matrices and graph coloring problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 (1983), </volume> <pages> pp. 187-209. </pages>
Reference-contexts: Coleman and More <ref> [10] </ref> showed that the partitioning problem could be analyzed as a graph coloring problem and that, by looking at the problem from the graph coloring point of view, it is possible to improve the CPR algorithm by scanning the columns in a carefully selected order.
Reference: [11] <author> A. R. Curtis, M. J. D. Powell, and J. K. Reid, </author> <title> On the estimation of sparse Jacobian matrices, </title> <journal> J. Inst. Math. Appl., </journal> <volume> 13 (1974), </volume> <pages> pp. 117-119. </pages>
Reference-contexts: Figure 2.1 illustrates the difference between the sparsity structure of f 0 (x) and the sparsity structure of the compressed Jacobian f 0 (x)V for the Inverse Elastic Rod problem from the MINPACK-2 collection (See Section 4). Curtis, Powell, and Reid (CPR) <ref> [11] </ref> were the first to note that a partitioning of the columns into p structurally orthogonal groups allows the approximation of the Jacobian 3 matrix by function differences with p function evaluations.
Reference: [12] <author> A. Griewank, </author> <title> On automatic differentiation, in Mathematical Programming: Recent Developments and Applications, </title> <editor> M. Iri and K. Tanabe, eds., </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989, </year> <pages> pp. </pages> <month> 83 - 108. </month> <title> [13] , Automatic evaluation of first- and higher-derivative vectors, </title> <booktitle> in Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, Applications, </title> <editor> R. Seydel, F. W. Schneider, T. Kupper, and H. Troger, eds., </editor> <volume> vol. 97, </volume> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1991, </year> <pages> pp. 135 - 148. </pages>
Reference-contexts: There are two basic modes of automatic differentiation, which are usually referred to as forward and reverse. As discussed in <ref> [12] </ref>, the reverse mode is closely related to adjoint methods and has a very low operation count for gradients. However, its potentially large memory requirement has been a serious impediment to its application in large-scale scientific computing.
Reference: [14] <author> A. Griewank and G. F. Corliss, eds., </author> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: AD can also be extended to the accurate evaluation of second and higher derivatives [13, 7, 3]. A comprehensive collection on the theory, implementation, and some earlier applications can be found in <ref> [14] </ref>. In contrast to the approximation of derivatives by function differences, AD incurs no truncation error. Hence, at least for noniterative and branch-free codes, the resulting derivative values are usually obtained with the working accuracy of the original function evaluation.
Reference: [15] <author> A. Griewank and S. Reese, </author> <title> On the calculation of Jacobian matrices by the Markowitz rule, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 126-135. </pages>
Reference-contexts: However, its potentially large memory requirement has been a serious impediment to its application in large-scale scientific computing. When there are several independent and dependent variables, the operation count for evaluating the Jacobian matrix may be lowest for certain mixed strategies <ref> [15] </ref> rather than for the forward or reverse mode. AD can also be extended to the accurate evaluation of second and higher derivatives [13, 7, 3]. A comprehensive collection on the theory, implementation, and some earlier applications can be found in [14].
Reference: [16] <author> D. Juedes, </author> <title> A taxonomy of automatic differentiation tools, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 315-329. </pages>
Reference-contexts: ADIFOR employs a hybrid of the forward and reverse modes of automatic differentiation. The resulting decrease in complexity compared with an implementation based entirely on the forward mode is usually substantial. In contrast to some earlier AD implementations <ref> [16] </ref>, the source translator ADIFOR was designed from the outset with large-scale codes in mind. The facilities of the ParaScope Fortran environment [6] control flow and data dependence flow information.
Reference: [17] <author> L. B. Rall, </author> <title> Automatic Differentiation: Techniques and Applications, </title> <booktitle> vol. 120 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1981. </year> <month> 11 </month>
Reference-contexts: For example, if the sparsity structure has bandwidth fi, then p fi. We also note that discretization of an infinite-dimensional problem also leads to sparsity patterns where p is independent of the mesh size. 3 Automatic Differentiation and the ADIFOR Tool Automatic differentiation <ref> [17] </ref> is a chain-rule-based technique for evaluating the derivatives of functions defined by computer programs with respect to their input variables. There are two basic modes of automatic differentiation, which are usually referred to as forward and reverse.
References-found: 15

