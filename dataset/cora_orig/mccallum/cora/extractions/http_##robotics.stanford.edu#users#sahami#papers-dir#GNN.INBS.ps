URL: http://robotics.stanford.edu/users/sahami/papers-dir/GNN.INBS.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: 
Title: Generating Neural Networks Through the Induction of Threshold Logic Unit Trees  
Author: Mehran Sahami 
Date: May, 1995.  
Address: Washington, DC,  Stanford, CA 94305  
Affiliation: Biological Systems,  Robotics Laboratory Computer Science Department Stanford University  
Note: To appear in Proceedings of the First International IEEE Symposium on Intelligence in Neural and  
Abstract: This paper investigates the generation of neural networks through the induction of binary trees of threshold logic units (TLUs). Initially, we describe the framework for our tree construction algorithm and show how it helps to bridge the gap between pure connectionist (neural network) and symbolic (decision tree) paradigms. We also show how the trees of threshold units that we induce can be transformed into an isomorphic neural network topology. Several methods for learning the linear discriminant functions at each node of the tree structure are examined and shown to produce accuracy results that are comparable to classical information theoretic methods for constructing decision trees (which use single feature tests at each node), but produce trees that are smaller and thus easier to understand. Moreover, our results also show that it is possible to simultaneously learn both the topology and weight settings of a neural network simply using the training data set that we are initially given. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brent, R. P. </author> <year> 1990. </year> <title> Fast training algorithms for multi layer neural nets. Numerical Analysis Project Manuscript NA-90-03, </title> <institution> Computer Science Dept., Stanford Univ. </institution>
Reference-contexts: This report helps to address these issues as well. Furthermore, we show how any such TLU tree can be mechanically transformed into a three-layer neural network as first suggested by Brent <ref> [1] </ref> and further developed by Sahami [10, 11]. <p> Thus each node in the second hidden layer represents a single distinct path through T by being connected to those nodes in the first layer which 1 Notation in the description of the TLU tree algorithm is similar to that presented in <ref> [1] </ref>. The algorithm presented here closely follows [10]. correspond to the nodes that were traversed along the given path.
Reference: [2] <author> Brodley, C. E., and Utgoff, P. E. </author> <year> 1992. </year> <title> Multivariate Versus Univariate Decision Trees. </title> <type> COINS Technical Report 92-8, </type> <institution> Dept. of Computer Science, </institution> <address> U. Mass. </address>
Reference-contexts: This notion of using multivariate as opposed to univariate separating functions in the induction of decision trees has only very recently begun to attract the attention of researchers in the machine learning community <ref> [2, 3] </ref>. While earlier attempts in this direction have been made, most notably with the Perceptron Tree algorithm [14], even the Perceptron Tree does not capture the full generality of our structure. In Perceptron Trees only the leaf nodes of the tree may implement a general linear discriminant function. <p> All internal nodes of the tree may only use univariate tests (similar to ID3) to shatter the instance space, again causing this algorithm to suffer from the same shortcomings as ID3. Recently, Brodley and Utgoff <ref> [2] </ref> have shown that learning multivariate (as opposed to univariate) decision trees also has the potential for greater generalization capabilities, but the results of these experiments are still preliminary and conflicting results have also been reported [12]. This report helps to address these issues as well.
Reference: [3] <author> Brodley, C. E., and Utgoff, P. E. </author> <year> 1994. </year> <title> Multivariate Decision Trees. </title> <note> To appear in Machine Learning. </note>
Reference-contexts: This notion of using multivariate as opposed to univariate separating functions in the induction of decision trees has only very recently begun to attract the attention of researchers in the machine learning community <ref> [2, 3] </ref>. While earlier attempts in this direction have been made, most notably with the Perceptron Tree algorithm [14], even the Perceptron Tree does not capture the full generality of our structure. In Perceptron Trees only the leaf nodes of the tree may implement a general linear discriminant function.
Reference: [4] <author> Duda, R. O., and Hart, P. E. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: In our investigation, we compare a number of different methods for learning the linear discriminant function at each node of the TLU tree <ref> [4, 9] </ref> and compare these with both the information theoretic approach to learning univariate tests (as in ID3) and a standard (naive Bayesian [6]) statistical approach to learning multivariate tests. We examine the performance of these algorithms on a wide variety of pattern classification and inductive learning tasks.
Reference: [5] <author> Gallant, S. I. </author> <year> 1986. </year> <title> Optimal Linear Discriminants. </title> <booktitle> In Eighth International Conference on Pattern Recognition, </booktitle> <pages> 849-852. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference-contexts: In applying the Perceptron error-correction rule to nonlinearly separable data, we attempt to minimize the number of errors made at each node of the tree by using Gallants Pocket algorithm <ref> [5] </ref> to keep the weight vector with the longest run of correct classifications at any point in our pocket. The weight vector that remains in our pocket after we have finished training at a given node in the tree is then used as the linear separator at that node.
Reference: [6] <author> Nilsson, N. J. </author> <year> 1965. </year> <title> Learning machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: In our investigation, we compare a number of different methods for learning the linear discriminant function at each node of the TLU tree [4, 9] and compare these with both the information theoretic approach to learning univariate tests (as in ID3) and a standard (naive Bayesian <ref> [6] </ref>) statistical approach to learning multivariate tests. We examine the performance of these algorithms on a wide variety of pattern classification and inductive learning tasks. <p> Comparatively, we examine a number of adaptive techniques for inducing hyperplanes: the Perceptron training rule <ref> [6] </ref>, the Least Mean Square (LMS) error algorithm [15], and BackPropagation [9] applied to one neuron (also known as the Delta rule) to find separating hyperplanes.
Reference: [7] <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: 1 Introduction We present a non-incremental algorithm that learns binary classification tasks by producing decision trees of threshold logic units (TLU trees). While similar to the decision trees produced by classical information theoretic algorithms such as ID3 <ref> [7] </ref>, TLU trees seem to promise more generality and compactness of representation as each node implements a linear discriminant function as opposed to testing only one feature of the instance vector.
Reference: [8] <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs For Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: x where O n is the actual real valued output of the nth trained unit on any instance and O T L U is the output of the "linear threshold unit." There are many possible extensions to the TLU tree induction algorithm, such as tree pruning in a postprocessing phase <ref> [8] </ref>, but such modifications are beyond the scope of this paper, and are only fine tunings to the underlying learning architecture. 4.2 Learning simple Boolean functions The first set of experiments involve learning simple Boolean functions.
Reference: [9] <author> Rumelhart, D. E.; Hinton, G. E.; and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. 1, </volume> <editor> eds. D. E. Rumelhart and J. L. McClelland, </editor> <address> 318-62. Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Such transformed networks, which have two hidden layers and one output layer (the input layer is not counted) can often by trained much more quickly by building the TLU tree and transforming it into a network than attempting to train the corresponding network using the standard BackPropagation algorithm <ref> [9] </ref> applied to the entire network. <p> In our investigation, we compare a number of different methods for learning the linear discriminant function at each node of the TLU tree <ref> [4, 9] </ref> and compare these with both the information theoretic approach to learning univariate tests (as in ID3) and a standard (naive Bayesian [6]) statistical approach to learning multivariate tests. We examine the performance of these algorithms on a wide variety of pattern classification and inductive learning tasks. <p> Comparatively, we examine a number of adaptive techniques for inducing hyperplanes: the Perceptron training rule [6], the Least Mean Square (LMS) error algorithm [15], and BackPropagation <ref> [9] </ref> applied to one neuron (also known as the Delta rule) to find separating hyperplanes.
Reference: [10] <author> Sahami, M. </author> <year> 1993. </year> <title> Learning NonLinearly Separable Boolean Functions With Linear Threshold Unit Trees and Madaline-Style Networks. </title> <booktitle> In AAAI-93 Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 335-41. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This report helps to address these issues as well. Furthermore, we show how any such TLU tree can be mechanically transformed into a three-layer neural network as first suggested by Brent [1] and further developed by Sahami <ref> [10, 11] </ref>. <p> Thus each node in the second hidden layer represents a single distinct path through T by being connected to those nodes in the first layer which 1 Notation in the description of the TLU tree algorithm is similar to that presented in [1]. The algorithm presented here closely follows <ref> [10] </ref>. correspond to the nodes that were traversed along the given path.
Reference: [11] <author> Sahami, M. </author> <year> 1995. </year> <title> Generating Neural Networks Through the Induction of Threshold Logic Unit Trees (Extended Abstract). </title> <booktitle> To appear in ECML-95 Proceedings of the Eighth European Conference on Machine Learning. </booktitle>
Reference-contexts: This report helps to address these issues as well. Furthermore, we show how any such TLU tree can be mechanically transformed into a three-layer neural network as first suggested by Brent [1] and further developed by Sahami <ref> [10, 11] </ref>.
Reference: [12] <author> Schaffer, C. </author> <year> 1994. </year> <title> Linear Discriminant Tree Induction Algorithms Compared (Extended Abstract). </title> <note> Working paper. </note>
Reference-contexts: Recently, Brodley and Utgoff [2] have shown that learning multivariate (as opposed to univariate) decision trees also has the potential for greater generalization capabilities, but the results of these experiments are still preliminary and conflicting results have also been reported <ref> [12] </ref>. This report helps to address these issues as well. Furthermore, we show how any such TLU tree can be mechanically transformed into a three-layer neural network as first suggested by Brent [1] and further developed by Sahami [10, 11].
Reference: [13] <author> Thrun, S. B., and 23 coauthors. </author> <year> 1991. </year> <title> The monks problems: a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: For these experiments, we the Monks Problems <ref> [13] </ref>. The original description of these problems is given below.
Reference: [14] <author> Utgoff, P. E. </author> <year> 1988. </year> <title> Perceptron Trees: A Case Study in Hybrid Concept Representation. </title> <booktitle> In A A A I - 8 8 Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> 601-6. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: While earlier attempts in this direction have been made, most notably with the Perceptron Tree algorithm <ref> [14] </ref>, even the Perceptron Tree does not capture the full generality of our structure. In Perceptron Trees only the leaf nodes of the tree may implement a general linear discriminant function.
Reference: [15] <author> Widrow, B., and Winter, R. G. </author> <year> 1988. </year> <title> Neural Nets for Adaptive Filtering and Adaptive Pattern Recognition. </title> <publisher> IEEE Computer, March:25-39. </publisher>
Reference-contexts: Comparatively, we examine a number of adaptive techniques for inducing hyperplanes: the Perceptron training rule [6], the Least Mean Square (LMS) error algorithm <ref> [15] </ref>, and BackPropagation [9] applied to one neuron (also known as the Delta rule) to find separating hyperplanes.
Reference: [16] <author> Winston, P. </author> <year> 1992. </year> <booktitle> Artificial Intelligence, third edition. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The first method was our own variation of the ID3 algorithm in which single feature tests are selected based on minimizing entropy in the set of instances at each node. The entropy measure we minimized, as presented in <ref> [16] </ref>, is given by: Entropy = n b ( n b c n bc ) where n t is the total number of instances in all branches, n b is the number of instances in branch b, and n bc is the total number of instances in branch b of class
References-found: 16

