URL: ftp://ftp.cs.rochester.edu/pub/u/kyros/tvcg.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/kyros/publicat.htm
Root-URL: 
Email: kyros@cs.rochester.edu  vallino@cs.rochester.edu  
Title: Calibration-Free Augmented Reality  
Author: Kiriakos N. Kutulakos James R. Vallino 
Keyword: Index terms: Augmented reality, real-time computer vision, calibration, registration, affine representations, feature tracking, 3D interaction techniques  
Address: Rochester, NY 14627-0226  
Affiliation: Computer Science Department University of Rochester  
Abstract: Camera calibration and the acquisition of Euclidean 3D measurements have so far been considered necessary requirements for overlaying three-dimensional graphical objects with live video. In this article we describe a new approach to video-based augmented reality that avoids both requirements: it does not use any metric information about the calibration parameters of the camera or the 3D locations and dimensions of the environment's objects. The only requirement is the ability to track across frames at least four fiducial points that are specified by the user during system initialization and whose world coordinates are unknown. Our approach is based on the following observation: given a set of four or more non-coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. We exploit this observation by (1) tracking regions and color fiducial points at frame rate, and (2) representing virtual objects in a non-Euclidean, affine frame of reference that allows their projection to be computed as a linear combination of the projection of the fiducial points. Experimental results on two augmented reality systems, one monitor-based and one head-mounted, demonstrate that the approach is readily implementable, imposes minimal computational and hardware requirements, and generates real-time and accurate video overlays even when the camera parameters vary dynamically. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. T. Azuma, </author> <title> A survey of augmented reality, Presence: </title> <booktitle> Teleoperators and Virtual Environments, </booktitle> <volume> vol. 6, no. 4, </volume> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: 1 Introduction There has been considerable interest recently in mixing live video from a camera with computer-generated graphical objects that are registered in a user's three-dimensional environment <ref> [1] </ref>. Applications of this powerful visualization technique include guiding trainees through complex 3D manipulation and maintenance tasks [2, 3], overlaying clinical 3D data with live video of patients during surgical planning [48], as well as developing three-dimensional user interfaces [9, 10].
Reference: [2] <author> T. P. Caudell and D. Mizell, </author> <title> Augmented reality: An application of heads-up display technology to manual manufacturing processes, </title> <booktitle> in Proc. Hawaii Int. Conf. System Sciences, </booktitle> <volume> vol. 2, </volume> <pages> pp. 659 669, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction There has been considerable interest recently in mixing live video from a camera with computer-generated graphical objects that are registered in a user's three-dimensional environment [1]. Applications of this powerful visualization technique include guiding trainees through complex 3D manipulation and maintenance tasks <ref> [2, 3] </ref>, overlaying clinical 3D data with live video of patients during surgical planning [48], as well as developing three-dimensional user interfaces [9, 10].
Reference: [3] <author> S. Feiner, B. MacIntyre, and D. Soligmann, </author> <title> Knowledge-based augmented reality, </title> <journal> Comm. of the ACM, </journal> <volume> vol. 36, no. 7, </volume> <pages> pp. 5362, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction There has been considerable interest recently in mixing live video from a camera with computer-generated graphical objects that are registered in a user's three-dimensional environment [1]. Applications of this powerful visualization technique include guiding trainees through complex 3D manipulation and maintenance tasks <ref> [2, 3] </ref>, overlaying clinical 3D data with live video of patients during surgical planning [48], as well as developing three-dimensional user interfaces [9, 10].
Reference: [4] <editor> W. Grimson et al., </editor> <title> An automatic registration method for frameless stereotaxy, image guided surgery, and enhanced reality visualization, </title> <booktitle> in Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 430436, </pages> <year> 1994. </year>
Reference-contexts: In practice, camera calibration and position tracking are prone to errors which propagate to the augmented display [13]. Furthermore, initialization of virtual objects requires additional calibration stages <ref> [4, 14] </ref>, and the camera must be dynamically re-calibrated whenever its position or its intrinsic parameters (e.g., focal length) change.
Reference: [5] <author> W. E. L. Grimson et al., </author> <title> Evaluating and validating an automated registration system for enhanced reality visualization in surgery, </title> <booktitle> in Proc. CVRMED'95, </booktitle> <pages> pp. 312, </pages> <year> 1995. </year> <month> 35 </month>
Reference: [6] <author> M. Uenohara and T. Kanade, </author> <title> Vision-based object registration for real-time image overlay, </title> <booktitle> in Proc. CVRMED'95, </booktitle> <pages> pp. 1422, </pages> <year> 1995. </year>
Reference-contexts: Our work is motivated by recent approaches to video-based augmented reality that reduce the effects of calibration errors through real-time processing of the live video images viewed by the user <ref> [6, 1417] </ref>. These approaches rely on tracking the projection of a physical object or a small number of fiducial points in the user's 3D environment to obtain an independent estimate of the camera's position and orientation in space [1820]. <p> Even though highly-accurate video overlays have been achieved in this manner by either complementing measurements from a magnetic position tracker [15, 16] or by eliminating such measurements entirely <ref> [6, 14] </ref>, current approaches require that (1) a precise Euclidean 3D model is available for the object or the fiducials being tracked, (2) the camera's calibration parameters are known at system initialization, and (3) the 3D world coordinates of all virtual objects are known in advance. <p> To our knowledge, only two systems have been reported <ref> [6, 14] </ref> that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed or perfectly calibrated. The system of Mellor [14] is capable of overlaying 3D medical data over live video of patients in a surgical environment. <p> The most closely related work to our own is the work of Uenohara and Kanade <ref> [6] </ref>. Their system allows overlay of planar diagrams onto live video by tracking fiducial points in an unknown configuration that lie on the same plane as the diagram. Calibration is avoided by expressing diagram points as linear combinations of the co-planar fiducial points. <p> 1 ; : : : ; p 4 . 5 Tracking and Projection Update The ability to track the projection of 3D points undergoing rigid transformations with respect to the camera becomes crucial in any method that relies on image information to represent the position and orientation of the camera <ref> [6, 14, 15, 29] </ref>. Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see [17, 18, 4144]). <p> Both difficulties can be overcome by using recursive estimation techniques that explicitly take into account fiducial occlusions and reappearances [64], by processing images in a coarse-to-fine fashion, and by using fiducials that can be efficiently identified and accurately localized in each frame <ref> [6, 14, 65] </ref>.
Reference: [7] <author> M. Bajura, H. Fuchs, and R. Ohbuchi, </author> <title> Merging virtual objects with the real world: Seeing ultrasound imagery within the patient, </title> <booktitle> in Proc. SIGGRAPH'92, </booktitle> <pages> pp. </pages> <address> 203210, </address> <year> 1992. </year>
Reference: [8] <author> A. State, M. A. Livingston, W. F. Garrett, G. Hirota, M. C. Whitton, E. D. Pisano, and H. Fuchs, </author> <title> Technologies for augmented reality systems: </title> <booktitle> Realizing ultrasound-guided needle biopsies, in Proc. SIGGRAPH'96, </booktitle> <pages> pp. 439446, </pages> <year> 1996. </year>
Reference: [9] <author> P. Wellner, </author> <title> Interacting with paper on the digitaldesk, </title> <journal> Comm. of the ACM, </journal> <volume> vol. 36, no. 7, </volume> <pages> pp. 86 95, </pages> <year> 1993. </year>
Reference-contexts: Applications of this powerful visualization technique include guiding trainees through complex 3D manipulation and maintenance tasks [2, 3], overlaying clinical 3D data with live video of patients during surgical planning [48], as well as developing three-dimensional user interfaces <ref> [9, 10] </ref>.
Reference: [10] <author> T. Darrell, P. Maes, B. Blumberg, and A. P. Pentland, </author> <title> A novel environment for situated vision and action, </title> <booktitle> in IEEE Workshop on Visual Behaviors, </booktitle> <pages> pp. 6872, </pages> <year> 1994. </year>
Reference-contexts: Applications of this powerful visualization technique include guiding trainees through complex 3D manipulation and maintenance tasks [2, 3], overlaying clinical 3D data with live video of patients during surgical planning [48], as well as developing three-dimensional user interfaces <ref> [9, 10] </ref>. <p> At the heart of these issues lies the ability to register the camera's motion, the user's environment and the embedded virtual objects in the same frame of reference (Fig. 1). Typical approaches use a stationary camera <ref> [10] </ref> or rely on 3D position tracking devices [11] and precise camera calibration [12] to ensure that the entire sequence of transformations between the internal reference frames of the virtual and physical objects, the camera tracking device, and the user's display is known exactly.
Reference: [11] <author> M. M. Wloka and B. G. Anderson, </author> <title> Resolving occlusion in augmented reality, </title> <booktitle> in Proc. Symposium on Interactive 3D Graphics, </booktitle> <pages> pp. 512, </pages> <year> 1995. </year>
Reference-contexts: At the heart of these issues lies the ability to register the camera's motion, the user's environment and the embedded virtual objects in the same frame of reference (Fig. 1). Typical approaches use a stationary camera [10] or rely on 3D position tracking devices <ref> [11] </ref> and precise camera calibration [12] to ensure that the entire sequence of transformations between the internal reference frames of the virtual and physical objects, the camera tracking device, and the user's display is known exactly. <p> These regions were tracked at frame rate by an uncalibrated camera. The shape and 3D configuration of the regions was unknown. considered necessary requirements for achieving augmented reality displays [12, 13] and have created a need for additional equipment such as laser range finders [14], position tracking devices <ref> [11] </ref>, and mechanical arms [16]. <p> respect to those points (see Section 4). (a) Initial augmented view. (b) Augmented view after a clockwise rotation of the object containing the affine basis points. (c) Hidden-surface elimination occurs only between virtual objects; correct occlusion resolution between physical and virtual objects requires information about the geometric relations between them <ref> [11] </ref>. (d) Real-time visible surface rendering with occlusion resolution between virtual and real objects. Visibility interactions between the virtual towers and the L-shaped object were resolved by first constructing an affine graphical model for the object.
Reference: [12] <author> M. Tuceyran et al., </author> <title> Calibration requirements and procedures for a monitor-based augmented reality system, </title> <journal> IEEE Trans. Visualization and Computer Graphics, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 255273, </pages> <year> 1995. </year>
Reference-contexts: At the heart of these issues lies the ability to register the camera's motion, the user's environment and the embedded virtual objects in the same frame of reference (Fig. 1). Typical approaches use a stationary camera [10] or rely on 3D position tracking devices [11] and precise camera calibration <ref> [12] </ref> to ensure that the entire sequence of transformations between the internal reference frames of the virtual and physical objects, the camera tracking device, and the user's display is known exactly. In practice, camera calibration and position tracking are prone to errors which propagate to the augmented display [13]. <p> These regions were tracked at frame rate by an uncalibrated camera. The shape and 3D configuration of the regions was unknown. considered necessary requirements for achieving augmented reality displays <ref> [12, 13] </ref> and have created a need for additional equipment such as laser range finders [14], position tracking devices [11], and mechanical arms [16]. <p> This graphics-to-video transformation is described by a 2 fi 3 matrix and can be computed if correspondences between three points in the two buffers are available. The procedure is a generalization of the Image Calibration Procedure detailed in <ref> [12] </ref> and is outlined in Fig. 12 (a). The recovered transformation is subsequently applied to all images generated by the graphics subsystem (Fig. 12 (b)). Initialization of the affine basis establishes the frame in which all virtual objects will be represented during a run of the system.
Reference: [13] <author> R. L. Holloway, </author> <title> Registration Errors in Augmented Reality Systems. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1995. </year>
Reference-contexts: In practice, camera calibration and position tracking are prone to errors which propagate to the augmented display <ref> [13] </ref>. Furthermore, initialization of virtual objects requires additional calibration stages [4, 14], and the camera must be dynamically re-calibrated whenever its position or its intrinsic parameters (e.g., focal length) change. <p> These regions were tracked at frame rate by an uncalibrated camera. The shape and 3D configuration of the regions was unknown. considered necessary requirements for achieving augmented reality displays <ref> [12, 13] </ref> and have created a need for additional equipment such as laser range finders [14], position tracking devices [11], and mechanical arms [16]. <p> Solid lines correspond to the white dot tracked in the live video signal and dotted lines to the generated overlay. The plot shows that a significant component of the overlay error is due to a lag between the actual position of the dot and the generated overlay <ref> [13] </ref>. This lag is due to Ethernet-related communication delays between the tracking and graphics subsystems and the fact that no effort was put into synchronizing the graphics and live video streams. 29 Fig. 19. Configuration of our HMD-based augmented reality system.
Reference: [14] <author> J. Mellor, </author> <title> Enhanced reality visualization in a surgical environment, </title> <type> Master's thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: In practice, camera calibration and position tracking are prone to errors which propagate to the augmented display [13]. Furthermore, initialization of virtual objects requires additional calibration stages <ref> [4, 14] </ref>, and the camera must be dynamically re-calibrated whenever its position or its intrinsic parameters (e.g., focal length) change. <p> Even though highly-accurate video overlays have been achieved in this manner by either complementing measurements from a magnetic position tracker [15, 16] or by eliminating such measurements entirely <ref> [6, 14] </ref>, current approaches require that (1) a precise Euclidean 3D model is available for the object or the fiducials being tracked, (2) the camera's calibration parameters are known at system initialization, and (3) the 3D world coordinates of all virtual objects are known in advance. <p> These regions were tracked at frame rate by an uncalibrated camera. The shape and 3D configuration of the regions was unknown. considered necessary requirements for achieving augmented reality displays [12, 13] and have created a need for additional equipment such as laser range finders <ref> [14] </ref>, position tracking devices [11], and mechanical arms [16]. <p> To our knowledge, only two systems have been reported <ref> [6, 14] </ref> that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed or perfectly calibrated. The system of Mellor [14] is capable of overlaying 3D medical data over live video of patients in a surgical environment. <p> To our knowledge, only two systems have been reported [6, 14] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed or perfectly calibrated. The system of Mellor <ref> [14] </ref> is capable of overlaying 3D medical data over live video of patients in a surgical environment. The system tracks circular fiducials in a known 3D configuration to invert the object-to-image transformation using a linear method. <p> 1 ; : : : ; p 4 . 5 Tracking and Projection Update The ability to track the projection of 3D points undergoing rigid transformations with respect to the camera becomes crucial in any method that relies on image information to represent the position and orientation of the camera <ref> [6, 14, 15, 29] </ref>. Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see [17, 18, 4144]). <p> Both difficulties can be overcome by using recursive estimation techniques that explicitly take into account fiducial occlusions and reappearances [64], by processing images in a coarse-to-fine fashion, and by using fiducials that can be efficiently identified and accurately localized in each frame <ref> [6, 14, 65] </ref>.
Reference: [15] <author> M. Bajura and U. Neumann, </author> <title> Dynamic registration correction in video-based augmented reality systems, </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> vol. 15, no. 5, </volume> <pages> pp. 5260, </pages> <year> 1995. </year>
Reference-contexts: Even though highly-accurate video overlays have been achieved in this manner by either complementing measurements from a magnetic position tracker <ref> [15, 16] </ref> or by eliminating such measurements entirely [6, 14], current approaches require that (1) a precise Euclidean 3D model is available for the object or the fiducials being tracked, (2) the camera's calibration parameters are known at system initialization, and (3) the 3D world coordinates of all virtual objects are <p> 1 ; : : : ; p 4 . 5 Tracking and Projection Update The ability to track the projection of 3D points undergoing rigid transformations with respect to the camera becomes crucial in any method that relies on image information to represent the position and orientation of the camera <ref> [6, 14, 15, 29] </ref>. Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see [17, 18, 4144]). <p> The objects were then rotated together through the sequence of views in Figs. 13 (b)-(e) while tracking was maintained on the two black regions. More examples are shown in Fig. 14. The accuracy of the image overlays is limited by radial distortions of the camera <ref> [15, 48] </ref> and the affine approximation to perspective projection. Radial distortions are not currently taken into account. In order to assess the limitations resulting from the affine approximation to perspective we computed mis-registration errors as follows.
Reference: [16] <author> A. State, G. Hirota, D. T. Chen, W. F. Garrett, and M. A. Livingston, </author> <title> Superior augmented reality registration by integrating landmark tracking and magnetic tracking, </title> <booktitle> in Proc. SIGGRAPH'96, </booktitle> <pages> pp. 429438, </pages> <year> 1996. </year>
Reference-contexts: Even though highly-accurate video overlays have been achieved in this manner by either complementing measurements from a magnetic position tracker <ref> [15, 16] </ref> or by eliminating such measurements entirely [6, 14], current approaches require that (1) a precise Euclidean 3D model is available for the object or the fiducials being tracked, (2) the camera's calibration parameters are known at system initialization, and (3) the 3D world coordinates of all virtual objects are <p> The shape and 3D configuration of the regions was unknown. considered necessary requirements for achieving augmented reality displays [12, 13] and have created a need for additional equipment such as laser range finders [14], position tracking devices [11], and mechanical arms <ref> [16] </ref>. <p> The ability to correctly update the affine view transformation matrix even under rapid and frequent head rotations becomes critical for any augmented reality system that employs a head-mounted display <ref> [16] </ref>. Camera rotations induced by such head motions can cause a significant shift in the projection of individual affine basis points and can cause one or more of these points to leave the field of view. <p> we use the sequential information available in the pointer's trace to increase modeling accuracy, (3) what surface representations are appropriate for supporting interactive growth, display and refinement of the reconstructed model, and (4) how can we accurately texture map in real time the incrementally-constructed model from the object's live image <ref> [16] </ref>? 8 Limitations The use of an affine framework for formulating the video overlay problem is both a strength and a limitation of our calibration-free augmented reality approach. <p> Complete reliance on the live video stream to extract the information required for merging graphics and video implies that the approach is inherently limited by the accuracy, speed, and robustness of point and region tracking <ref> [16] </ref>. Significant changes in the camera's position inevitably lead to tracking errors or occlusions of one or more of the tracked fiducial points.
Reference: [17] <author> S. Ravela, B. Draper, et al., </author> <title> Adaptive tracking and model registration across distinct aspects, </title> <booktitle> in Proc. 1995 IEEE/RSJ Int. Conf. Intelligent Robotics and Systems, </booktitle> <pages> pp. 174180, </pages> <year> 1995. </year>
Reference-contexts: Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see <ref> [17, 18, 4144] </ref>). Here we describe a simple approach that exploits the existence of more than the minimum number of fiducial points to increase robustness and automatically provides an updated affine view transformation matrix for rendering virtual objects.
Reference: [18] <author> D. G. Lowe, </author> <title> Robust model-based tracking through the integration of search and estimation, </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 8, no. 2, </volume> <pages> pp. 113122, </pages> <year> 1992. </year>
Reference-contexts: Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see <ref> [17, 18, 4144] </ref>). Here we describe a simple approach that exploits the existence of more than the minimum number of fiducial points to increase robustness and automatically provides an updated affine view transformation matrix for rendering virtual objects.
Reference: [19] <author> D. G. Lowe, </author> <title> Fitting parameterized three-dimensional models to images, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 13, no. 5, </volume> <pages> pp. 441449, </pages> <year> 1991. </year>
Reference: [20] <author> G. Verghese, K. L. Gale, and C. R. Dyer, </author> <title> Real-time motion tracking of three-dimensional objects, </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. </pages> <address> 19982003, </address> <year> 1990. </year> <month> 36 </month>
Reference: [21] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> Affine structure from motion, </title> <journal> J. Opt. Soc. Am., </journal> <volume> vol. A, no. 2, </volume> <pages> pp. 377385, </pages> <year> 1991. </year>
Reference-contexts: To eliminate these requirements our approach uses the following observation, pointed out by Koen-derink and van Doorn <ref> [21] </ref> and Ullman and Basri [22]: given a set of four or more non-coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. <p> We exploit this observation by (1) tracking regions and color fiducial points at frame rate, and (2) representing virtual objects so that their projection can be computed as a linear combination of the projection of the fiducial points. The resulting affine virtual object representation is a non-Euclidean representation <ref> [21, 2325] </ref> in which the coordinates of vertices on a virtual object are relative to an affine reference frame defined by the fiducial points (Fig. 2). <p> Affine object representations have been a topic of active research in computer vision in the context of 3D reconstruction <ref> [21, 24, 26] </ref> and recognition [27]. While our results draw heavily from this research, the use of affine object models in the context of augmented reality has not been previously studied. <p> We use the following two properties of affine point representations <ref> [21, 24, 26] </ref> (Fig. 4): Property 1 (Re-Projection Property) When the projection of the origin and basis points is known in an image I m , we can compute the projection of a point p from its affine coordinates: 2 u m v m 3 2 u m p o u
Reference: [22] <author> S. Ullman and R. Basri, </author> <title> Recognition by linear combinations of models, </title> <journal> IEEE Trans. on Pattern Anal. and Mach. Intell., </journal> <volume> vol. 13, no. 10, </volume> <pages> pp. 9921006, </pages> <year> 1991. </year>
Reference-contexts: To eliminate these requirements our approach uses the following observation, pointed out by Koen-derink and van Doorn [21] and Ullman and Basri <ref> [22] </ref>: given a set of four or more non-coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points.
Reference: [23] <author> O. Faugeras, </author> <title> Stratification of three-dimensional vision: projective, affine, and metric representations, </title> <journal> J. Opt. Soc. Am. A, </journal> <volume> vol. 12, no. 3, </volume> <pages> pp. 465484, </pages> <year> 1995. </year>
Reference: [24] <author> J. L. Mundy and A. Zisserman, eds., </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Affine object representations have been a topic of active research in computer vision in the context of 3D reconstruction <ref> [21, 24, 26] </ref> and recognition [27]. While our results draw heavily from this research, the use of affine object models in the context of augmented reality has not been previously studied. <p> We use the following two properties of affine point representations <ref> [21, 24, 26] </ref> (Fig. 4): Property 1 (Re-Projection Property) When the projection of the origin and basis points is known in an image I m , we can compute the projection of a point p from its affine coordinates: 2 u m v m 3 2 u m p o u <p> This restriction can be overcome by formulating the video overlay process within a more general projective framework; the analysis presented in this article can be directly generalized to account for the perspective projection model by representing virtual objects in a projective frame of reference defined by five fiducial points <ref> [24, 59] </ref>.
Reference: [25] <author> O. D. Faugeras, </author> <title> Three-Dimensional Computer Vision: A Geometric Viewpoint. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Our approach for placing virtual objects in the 3D environment borrows from a few simple results in stereo vision <ref> [25] </ref>: given a point in space, its 3D location is uniquely determined by the point's projection in two images taken at different positions of the camera (Fig. 6 (a)). <p> While considerable progress has been achieved in the fields of image-based shape recovery <ref> [25, 46] </ref> and active range sensing [50], currently no general purpose systems exist that can rely on one or more cameras to autonomously construct accurate 3D models of objects that are physically present in a user's environment.
Reference: [26] <author> D. Weinshall and C. Tomasi, </author> <title> Linear and incremental acquisition of invariant shape models from image sequences, </title> <booktitle> in Proc. 4th Int. Conf. on Computer Vision, </booktitle> <pages> pp. 675682, </pages> <year> 1993. </year>
Reference-contexts: Affine object representations have been a topic of active research in computer vision in the context of 3D reconstruction <ref> [21, 24, 26] </ref> and recognition [27]. While our results draw heavily from this research, the use of affine object models in the context of augmented reality has not been previously studied. <p> We use the following two properties of affine point representations <ref> [21, 24, 26] </ref> (Fig. 4): Property 1 (Re-Projection Property) When the projection of the origin and basis points is known in an image I m , we can compute the projection of a point p from its affine coordinates: 2 u m v m 3 2 u m p o u <p> In order to increase resistance to noise due to image localization errors we use all detected fiducial points to define the affine basis and to update the matrix. We employ a variant of Tomasi and Kanade's factorization method <ref> [26, 46] </ref> that allows the matrix M of affine coordinates of n 4 fiducial points in Eq. (10) to be recovered from m 2 views of the points. <p> This matrix, which is of rank 3 under noise-free conditions, is constructed by tracking the selected fiducials while the camera is repositioned manually. As shown in <ref> [26, 46] </ref>, the rank-3 property of the measurement matrix allows us to assign a set of affine coordinates to each fiducial point and a view transformation matrix to each of the m views 21 through a singular-value decomposition of the matrix: 2 6 6 6 6 4 p 1 u 1
Reference: [27] <author> Y. Lamdan, J. T. Schwartz, and H. J. Wolfson, </author> <title> Object recognition by affine invariant matching, </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 335344, </pages> <year> 1988. </year>
Reference-contexts: Affine object representations have been a topic of active research in computer vision in the context of 3D reconstruction [21, 24, 26] and recognition <ref> [27] </ref>. While our results draw heavily from this research, the use of affine object models in the context of augmented reality has not been previously studied.
Reference: [28] <author> R. Cipolla, P. A. Hadfield, and N. J. Hollinghurst, </author> <title> Uncalibrated stereo vision with pointing for a man-machine interface, </title> <booktitle> in Proc. IAPR Workshop on Machine Vision Applications, </booktitle> <year> 1994. </year>
Reference-contexts: we show that placement of affine virtual objects as well as visible-surface rendering can be performed efficiently using simple linear methods that operate at frame rate, do not require camera calibration or 3 Euclidean 3D measurements, and exploit the ability of the augmented reality system to interact with its user <ref> [28, 29] </ref>. To our knowledge, only two systems have been reported [6, 14] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed or perfectly calibrated. <p> The entire process is shown in Fig. 7. 4.2 Object Snapping Constraints Affine object representations lead naturally to a through-the-lens method <ref> [28, 34, 40] </ref> for further constraining the interactive placement of virtual objects. We call the resulting constraints object snapping constraints because they allow a user to interactively position virtual objects relative to physical objects in the camera's view volume.
Reference: [29] <author> A. Azarbayejani, T. Starner, B. Horowitz, and A. Pentland, </author> <title> Visually controlled graphics, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 15, no. 6, </volume> <pages> pp. 602605, </pages> <year> 1993. </year>
Reference-contexts: we show that placement of affine virtual objects as well as visible-surface rendering can be performed efficiently using simple linear methods that operate at frame rate, do not require camera calibration or 3 Euclidean 3D measurements, and exploit the ability of the augmented reality system to interact with its user <ref> [28, 29] </ref>. To our knowledge, only two systems have been reported [6, 14] that operate without specialized camera tracking devices and without relying on the assumption that the camera is always fixed or perfectly calibrated. <p> 1 ; : : : ; p 4 . 5 Tracking and Projection Update The ability to track the projection of 3D points undergoing rigid transformations with respect to the camera becomes crucial in any method that relies on image information to represent the position and orientation of the camera <ref> [6, 14, 15, 29] </ref>. Real-time tracking of image features has been the subject of extensive research in computer vision (e.g., see [17, 18, 4144]).
Reference: [30] <author> J. D. Foley, A. van Dam, S. K. Feiner, and J. F. Hughes, </author> <title> Computer Graphics Principles and Practice. </title> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1990. </year>
Reference-contexts: Second, we show that by representing virtual objects in an affine reference frame and by performing computer graphics operations such as projection and visible-surface determination directly on affine models, the entire video overlay process is described by a single 4fi4 homogeneous view transformation matrix <ref> [30] </ref>. Furthermore, the elements of this matrix are simply the image x- and y- coordinates of fiducial points. <p> Limitations of our approach are summarized in Section 8. 2 Geometrical Foundations Accurate projection of a virtual object requires knowing precisely the combined effect of the object-to-world, world-to-camera and camera-to-image transformations <ref> [30] </ref>. <p> object-to-affine transformation A such that [p 0 2 p 0 4 ] = A [p 1 p 2 p 3 p 4 ]: (7) One of the key aspects of affine object representations is that even though they are non-Euclidean, they nevertheless allow rendering operations such as z-buffering and clipping <ref> [30] </ref> to be performed accurately. This is because both depth order as well as the intersection of lines and planes is preserved under affine transformations. More specifically, z-buffering relies on the ability to order in depth two object points that project to the same pixel in the image. <p> We call the approach 3D stenciling because the physical object being modeled is treated as a three-dimensional analog of a stencil. In 3D stenciling, an ordinary pen or a hand-held pointer plays the role of a 3D digitizer <ref> [30, 55] </ref>: the user moves the pointer over the object being modeled while constantly maintaining contact between 31 the pointer's tip and the surface of the object (Fig. 20 (a)).
Reference: [31] <author> Y. Bar-Shalom and T. E. Fortmann, </author> <title> Tracking and Data Association. </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: We use two independent constant velocity Kalman filters <ref> [31] </ref> whose states consist of the first and second row of the matrix 2fi4 , respectively, as well as their time derivatives. The filters' measurement equations are given by Eq. (10). Interpreted as physical systems, these filters estimate the motion of the coordinate frame of the affine camera (Section 2.2). <p> Region tracking uses the (monochrome) intensity signal of the video stream, runs on a single processor at rates between 30Hz and 60Hz for simultaneous tracking of two regions, and provides updated Kalman filter estimates for the elements of the affine view transformation matrix <ref> [31] </ref>. Conceptually, the tracking subsystem can be thought of as an affine camera position tracker that returns the current affine view transformation matrix asynchronously upon request. This matrix is sent to the graphics subsystem.
Reference: [32] <author> J.-R. Wu and M. Ouhyoung, </author> <title> A 3D tracking experiment on latency and its compensation methods in virtual environments, </title> <booktitle> in Proc. 8th ACM Symp. on User Interface Software and Technology, </booktitle> <pages> pp. 4149, </pages> <year> 1995. </year>
Reference: [33] <author> R. Azuma and G. Bishop, </author> <title> Improving static and dynamic registration in an optical see-through HMD, </title> <booktitle> in Proc. SIGGRAPH'94, </booktitle> <pages> pp. </pages> <address> 197204, </address> <year> 1994. </year>
Reference: [34] <author> M. Gleicher and A. Witkin, </author> <title> Through-the-lens camera control, </title> <booktitle> in Proc. SIGGRAPH'92, </booktitle> <pages> pp. 331 340, </pages> <year> 1992. </year>
Reference-contexts: This not only enables the efficient estimation of the view transformation matrix but also leads to the use of optimal estimators such as the Kalman filter [3133] to track the fiducial points and to compute the matrix. Third, the use of affine models leads to a simple through-the-lens method <ref> [34] </ref> for interactively placing virtual objects within the user's 3D environment. Fourth, efficient execution of computer graphics operations on affine virtual objects and real-time (30Hz) generation of overlays are achieved by implementing affine projection computations directly on dedicated graphics hardware. <p> The entire process is shown in Fig. 7. 4.2 Object Snapping Constraints Affine object representations lead naturally to a through-the-lens method <ref> [28, 34, 40] </ref> for further constraining the interactive placement of virtual objects. We call the resulting constraints object snapping constraints because they allow a user to interactively position virtual objects relative to physical objects in the camera's view volume.
Reference: [35] <author> L. S. Shapiro, A. Zisserman, and M. Brady, </author> <title> 3D motion recovery via affine epipolar geometry, </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 16, no. 2, </volume> <pages> pp. 147182, </pages> <year> 1995. </year>
Reference-contexts: The basic principles behind these representations are briefly reviewed next. We will assume in the following that the camera-to-image transformation can modeled using the weak perspective projection model <ref> [35] </ref> (Fig. 3 (a),(b)). 2.1 Affine Point Representations A basic operation in our method for computing the projection of a virtual object is that of re-projection [37, 38]: given the projection of a collection of 3D points at two positions of the camera, compute the projection of these points at a <p> Once p's projection is specified in one image, its projection in the second image must lie on a line satisfying the epipolar constraint <ref> [35] </ref>. This line is computed automatically and is used to constrain the user's selection of q R in the second image. <p> camera's 3D position is required to achieve correct video overlays, this manual repositioning of the camera does not impose additional computational or calibration steps. 4 The simultaneous enforcement of the coplanarity and epipolar constraints leads to an over-determined system of equations that can be solved using a least squares technique <ref> [35] </ref>. 16 (a) (b) (c) Fig. 9. Aligning a virtual parallelepiped with a mousepad. Crosses show the points selected in each image. Dotted lines in (b) show the epipolars associated with the points selected in (a).
Reference: [36] <author> W. B. Thompson and J. L. Mundy, </author> <title> Three-dimensional model matching from an unconstrained viewpoint, </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. </pages> <address> 208220, </address> <year> 1987. </year>
Reference-contexts: Image scaling is used to model the effect of object-to-camera distance on the object's projection; it is a good approximation to perspective projection when the camera's distance to the object is much larger than the size of the object itself <ref> [36] </ref>. (c) The image projection of an affinely-represented point p is given by [p T p T ] T , where and are the directions of the rows and columns of the camera, respectively, in the reference frame of the affine basis points. <p> Results are shown in Figs. 15 and 16. While errors remain within 15 pixels for the range of motions we considered (in a 640 fi 480 image), the results show that, as expected, the affine approximation to perspective leads to errors as the distance to the object decreases <ref> [36, 49] </ref>. These effects suggest the utility of projectively-invariant representations for representing virtual objects when the object-camera distance is small. The accuracy of the real-time video overlays generated by our system was measured as follows. <p> The affine approximation to perspective projection can introduce errors in the re-projection process and restricts system operation to relatively large object-to-camera distances (greater than 10 times the object's size 6 <ref> [36] </ref>).
Reference: [37] <author> A. Shashua, </author> <title> A geometric invariant for visual recognition and 3D reconstruction from two perspective/orthographic views, </title> <booktitle> in Proc. IEEE Workshop on Qualitative Vision, </booktitle> <pages> pp. 107117, </pages> <year> 1993. </year> <month> 37 </month>
Reference-contexts: We will assume in the following that the camera-to-image transformation can modeled using the weak perspective projection model [35] (Fig. 3 (a),(b)). 2.1 Affine Point Representations A basic operation in our method for computing the projection of a virtual object is that of re-projection <ref> [37, 38] </ref>: given the projection of a collection of 3D points at two positions of the camera, compute the projection of these points at a third camera position.
Reference: [38] <author> E. B. Barrett, M. H. Brill, N. N. Haag, and P. M. Payton, </author> <title> Invariant linear methods in photogram--metry and model-matching, </title> <booktitle> in Geometric Invariance in Computer Vision, </booktitle> <pages> pp. 277292, </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: We will assume in the following that the camera-to-image transformation can modeled using the weak perspective projection model [35] (Fig. 3 (a),(b)). 2.1 Affine Point Representations A basic operation in our method for computing the projection of a virtual object is that of re-projection <ref> [37, 38] </ref>: given the projection of a collection of 3D points at two positions of the camera, compute the projection of these points at a third camera position.
Reference: [39] <author> S. M. Seitz and C. R. Dyer, </author> <title> Complete scene structure from four point correspondences, </title> <booktitle> in Proc. 5th Int. Conf. on Computer Vision, </booktitle> <pages> pp. 330337, </pages> <year> 1995. </year>
Reference-contexts: The affine frame was defined by the workstation's vertices, which were tracked at frame rate. No information about the camera's position or the Euclidean shape of the workstation is used in the above steps. the set 2 <ref> [39] </ref> R [( L ) 1 q L + t L ] j t 2 &lt; : (9) In practice, the position of q R is specified by interactively dragging a pointer along the epipolar line of q L .
Reference: [40] <author> G. D. Hager, </author> <title> Calibration-free visual control using projective invariance, </title> <booktitle> in Proc. 5th Int. Conf. Computer Vision, </booktitle> <pages> pp. 10091015, </pages> <year> 1995. </year>
Reference-contexts: The entire process is shown in Fig. 7. 4.2 Object Snapping Constraints Affine object representations lead naturally to a through-the-lens method <ref> [28, 34, 40] </ref> for further constraining the interactive placement of virtual objects. We call the resulting constraints object snapping constraints because they allow a user to interactively position virtual objects relative to physical objects in the camera's view volume.
Reference: [41] <author> A. Blake and A. Yuille, eds., </author> <title> Active Vision. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [42] <author> C. M. Brown and D. Terzopoulos, eds., </author> <title> Real-Time Computer Vision. </title> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference: [43] <author> A. Blake and M. Isard, </author> <title> 3D position, attitude and shape input using video tracking of hands and lips, </title> <booktitle> in ACM SIGGRAPH'94, </booktitle> <pages> pp. 185192, </pages> <year> 1994. </year>
Reference: [44] <author> G. D. Hager and P. N. Belhumeur, </author> <title> Real-time tracking of image regions with changes in geometry and illumination, </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 403410, </pages> <year> 1996. </year>
Reference-contexts: We are currently planning to enhance our computational and network resources to reduce communication delays and allow simultaneous merging of two live video streams. We are also investigating the use of efficient and general purpose correlation-based trackers <ref> [44, 66] </ref> to improve tracking accuracy and versatility. 9 Concluding Remarks We have demonstrated that fast and accurate merging of graphics and live video can be achieved using a simple approach that requires no metric information about the camera's calibration parameters or about the 3D locations and dimensions of the environment's
Reference: [45] <author> D. H. Ballard and C. M. Brown, </author> <title> Computer Vision. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: Our region tracking algorithm proceeds in three steps: (1) searching for points on the boundary of the region's projection in the current image, (2) grouping the localized boundary points into linear segments using the polyline curve approximation algorithm <ref> [45] </ref>, and (3) fitting lines to the grouped points using least squares in order to construct a polygonal representation of the region's projection in the current image. <p> space, (2) using a lookup table to map every pixel in the image to a binary value that indicates whether or not the pixel's hue and saturation are close to those of one of the a priori-specified marker colors, and (3) computing the connected components in the resulting binary image <ref> [45] </ref>. Because these steps are performed by the video processor at a rate of 30Hz, accurate localization of multiple color blobs as small as 8 fi 8 pixels is accomplished in each frame independently. <p> System initialization follows the steps outlined in Section 6. The only exception is the Interactive Object Placement step which is not required to achieve 3D stenciling. In its present form, the system uses normalized correlation <ref> [45] </ref> to track the tip of a hand-held pointer in two live video streams at frame rate. Once the affine coordinates of the pointer's tip are computed by the tracking subsystem, they are transmitted to the graphics subsystem in order to generate the video overlay.
Reference: [46] <author> C. Tomasi and T. Kanade, </author> <title> Shape and motion from image streams under orthography: A factorization method, </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 137154, </pages> <year> 1992. </year>
Reference-contexts: In order to increase resistance to noise due to image localization errors we use all detected fiducial points to define the affine basis and to update the matrix. We employ a variant of Tomasi and Kanade's factorization method <ref> [26, 46] </ref> that allows the matrix M of affine coordinates of n 4 fiducial points in Eq. (10) to be recovered from m 2 views of the points. <p> This matrix, which is of rank 3 under noise-free conditions, is constructed by tracking the selected fiducials while the camera is repositioned manually. As shown in <ref> [26, 46] </ref>, the rank-3 property of the measurement matrix allows us to assign a set of affine coordinates to each fiducial point and a view transformation matrix to each of the m views 21 through a singular-value decomposition of the matrix: 2 6 6 6 6 4 p 1 u 1 <p> While considerable progress has been achieved in the fields of image-based shape recovery <ref> [25, 46] </ref> and active range sensing [50], currently no general purpose systems exist that can rely on one or more cameras to autonomously construct accurate 3D models of objects that are physically present in a user's environment.
Reference: [47] <author> K. Jack, </author> <title> Video Demystified: A Handbook for the Digital Engineer. </title> <publisher> HighText Publications Inc., </publisher> <year> 1993. </year>
Reference-contexts: The position and intrinsic camera parameters were not computed. Video output is generated by merging the analog video signal from one of the cameras with the output of the graphics subsystem. This merging operation is performed in hardware using a Celect Translator luminance keyer <ref> [47] </ref>. Operation of the system involves four steps: (1) alignment of the graphics frame buffer with the digitizer frame buffer, (2) initialization of the affine basis, (3) virtual object placement, and (4) affine basis tracking and projection update.
Reference: [48] <author> R. Y. Tsai, </author> <title> A versatile camera calibration technique for high accuracy 3D machine vision metrology using off-the shelf tv cameras and lenses, </title> <journal> IEEE Trans. Robotics and Automat., </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 323344, </pages> <year> 1987. </year>
Reference-contexts: Applications of this powerful visualization technique include guiding trainees through complex 3D manipulation and maintenance tasks [2, 3], overlaying clinical 3D data with live video of patients during surgical planning <ref> [48] </ref>, as well as developing three-dimensional user interfaces [9, 10]. <p> The objects were then rotated together through the sequence of views in Figs. 13 (b)-(e) while tracking was maintained on the two black regions. More examples are shown in Fig. 14. The accuracy of the image overlays is limited by radial distortions of the camera <ref> [15, 48] </ref> and the affine approximation to perspective projection. Radial distortions are not currently taken into account. In order to assess the limitations resulting from the affine approximation to perspective we computed mis-registration errors as follows.
Reference: [49] <author> B. Boufama, D. Weinshall, and M. Werman, </author> <title> Shape from motion algorithms: a comparative analysis of scaled orthography and perspective, </title> <booktitle> in Proc. European Conf. on Computer Vision (J.-O. </booktitle> <address> Eklundh, </address> <publisher> ed.), </publisher> <pages> pp. </pages> <address> 199204, </address> <year> 1994. </year>
Reference-contexts: Results are shown in Figs. 15 and 16. While errors remain within 15 pixels for the range of motions we considered (in a 640 fi 480 image), the results show that, as expected, the affine approximation to perspective leads to errors as the distance to the object decreases <ref> [36, 49] </ref>. These effects suggest the utility of projectively-invariant representations for representing virtual objects when the object-camera distance is small. The accuracy of the real-time video overlays generated by our system was measured as follows. <p> Similarly, radial camera distortions can be corrected by automatically computing an image warp that maps lines in space to lines in the image [60]. 6 The approximation is not only valid at such large object-to-camera distances, but has been shown to yield more accurate results in structure-from-motion computations <ref> [49, 58] </ref>. 33 The use on non-Euclidean models for representing virtual objects implies that only rendering oper-ations that rely on non-metric information can be implemented directly.
Reference: [50] <author> G. Turk and M. Levoy, </author> <title> Zippered polygon meshes from range images, </title> <booktitle> in Proc. SIGGRAPH'94, </booktitle> <pages> pp. 311318, </pages> <year> 1994. </year>
Reference-contexts: While considerable progress has been achieved in the fields of image-based shape recovery [25, 46] and active range sensing <ref> [50] </ref>, currently no general purpose systems exist that can rely on one or more cameras to autonomously construct accurate 3D models of objects that are physically present in a user's environment.
Reference: [51] <author> E. K.-Y. Jeng and Z. Xiang, </author> <title> Moving cursor plane for interactive sculpting, </title> <journal> ACM Trans. on Graphics, </journal> <volume> vol. 15, no. 3, </volume> <pages> pp. 211222, </pages> <year> 1996. </year>
Reference: [52] <author> S. W. Wang and A. E. Kaufman, </author> <title> Volume sculpting, </title> <booktitle> in Proc. Symposium on Interactive 3D Graphics, </booktitle> <pages> pp. 151156, </pages> <year> 1995. </year>
Reference: [53] <author> H. Qin and D. Terzopoulos, D-Nurbs: </author> <title> A physics-based framework for geometric design, </title> <journal> IEEE Trans. Visualization and Computer Graphics, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 8596, </pages> <year> 1996. </year> <month> 38 </month>
Reference: [54] <author> P. E. Debevec, C. J. Taylor, and J. Malik, </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach, </title> <booktitle> in Proc. SIGGRAPH'96, </booktitle> <pages> pp. 1120, </pages> <year> 1996. </year>
Reference: [55] <author> S. A. Tebo, D. A. Leopold, D. M. Long, S. J. Zinreich, and D. W. Kennedy, </author> <title> An optical 3D digitizer for frameless stereotactic surgery, </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> vol. 16, </volume> <pages> pp. 5564, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: We call the approach 3D stenciling because the physical object being modeled is treated as a three-dimensional analog of a stencil. In 3D stenciling, an ordinary pen or a hand-held pointer plays the role of a 3D digitizer <ref> [30, 55] </ref>: the user moves the pointer over the object being modeled while constantly maintaining contact between 31 the pointer's tip and the surface of the object (Fig. 20 (a)).
Reference: [56] <author> M. Agrawala, A. C. Beers, and M. Levoy, </author> <title> 3D painting on scanned surfaces, </title> <booktitle> in Proc. Symposium on Interactive 3D Graphics, </booktitle> <pages> pp. 145150, </pages> <year> 1995. </year>
Reference-contexts: In effect, 3D stenciling allows the user to cover the object being modeled with a form of virtual 3D paint <ref> [56] </ref>; the affine model of a physical object is complete when the object's entire surface is painted. From a technical point of view, the theoretical underpinnings of the 3D affine reconstruction and overlay generation operations for 3D stenciling are completely described in Sections 2 and 3.
Reference: [57] <author> K. N. Kutulakos and J. Vallino, </author> <title> Affine object representations for calibration-free augmented reality: Example MPEG sequences. </title> <note> http://www.cs.rochester.edu:/u/kyros/mpegs/TVCG.html, 1996. </note>
Reference-contexts: Once the affine coordinates of the pointer's tip are computed by the tracking subsystem, they are transmitted to the graphics subsystem in order to generate the video overlay. MPEG video sequences demonstrating the system in operation can be found in <ref> [57] </ref>.
Reference: [58] <author> C. Wiles and M. Brady, </author> <title> On the appropriateness of camera models, </title> <booktitle> in Proc. 4th European Conf. Computer Vision, </booktitle> <pages> pp. 228237, </pages> <year> 1996. </year>
Reference-contexts: Similarly, radial camera distortions can be corrected by automatically computing an image warp that maps lines in space to lines in the image [60]. 6 The approximation is not only valid at such large object-to-camera distances, but has been shown to yield more accurate results in structure-from-motion computations <ref> [49, 58] </ref>. 33 The use on non-Euclidean models for representing virtual objects implies that only rendering oper-ations that rely on non-metric information can be implemented directly.
Reference: [59] <author> O. D. Faugeras, </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig?, </title> <booktitle> in Proc. 2nd European Conf. on Computer Vision, </booktitle> <pages> pp. 563578, </pages> <year> 1992. </year>
Reference-contexts: This restriction can be overcome by formulating the video overlay process within a more general projective framework; the analysis presented in this article can be directly generalized to account for the perspective projection model by representing virtual objects in a projective frame of reference defined by five fiducial points <ref> [24, 59] </ref>.
Reference: [60] <author> R. Mohr, B. Boufama, and P. Brand, </author> <title> Accurate projective reconstruction, in Applications of Invariance in Computer Vision (J. Mundy, </title> <editor> A. Zisserman, and D. Forsyth, eds.), pp. </editor> <volume> 257276, </volume> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Similarly, radial camera distortions can be corrected by automatically computing an image warp that maps lines in space to lines in the image <ref> [60] </ref>. 6 The approximation is not only valid at such large object-to-camera distances, but has been shown to yield more accurate results in structure-from-motion computations [49, 58]. 33 The use on non-Euclidean models for representing virtual objects implies that only rendering oper-ations that rely on non-metric information can be implemented directly.
Reference: [61] <author> J. Dorsey, J. Arvo, and D. Greenberg, </author> <title> Interactive design of complex time dependent lighting, </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> vol. 15, </volume> <pages> pp. 2636, </pages> <month> March </month> <year> 1996. </year>
Reference: [62] <author> A. Shashua, </author> <title> Geometry and Photometry in 3D Visual Recognition. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1992. </year>
Reference: [63] <author> P. N. B. D. J. Kriegman, </author> <title> What is the set of images of an object under all possible lighting conditions, </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 270277, </pages> <year> 1996. </year>
Reference: [64] <author> P. F. McLauchlan, I. D. Reid, and D. W. Murray, </author> <title> Recursive affine structure and motion from image sequences, </title> <booktitle> in Proc. 3rd European Conf. on Computer Vision, </booktitle> <pages> pp. 217224, </pages> <year> 1994. </year>
Reference-contexts: In addition, unless real-time video processing hardware is available, fast rotational motions of the camera will make tracking particularly difficult due to large fiducial point displacements across frames. Both difficulties can be overcome by using recursive estimation techniques that explicitly take into account fiducial occlusions and reappearances <ref> [64] </ref>, by processing images in a coarse-to-fine fashion, and by using fiducials that can be efficiently identified and accurately localized in each frame [6, 14, 65].
Reference: [65] <author> L. O. Gorman, </author> <title> Subpixel precision of straight-edged shapes for registration and measurement, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 18, no. 7, </volume> <pages> pp. 746751, </pages> <year> 1996. </year>
Reference-contexts: Both difficulties can be overcome by using recursive estimation techniques that explicitly take into account fiducial occlusions and reappearances [64], by processing images in a coarse-to-fine fashion, and by using fiducials that can be efficiently identified and accurately localized in each frame <ref> [6, 14, 65] </ref>.
Reference: [66] <author> K. Toyama and G. D. Hager, </author> <title> Incremental focus of attention for robust visual tracking, </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 189195, </pages> <year> 1996. </year>
Reference-contexts: We are currently planning to enhance our computational and network resources to reduce communication delays and allow simultaneous merging of two live video streams. We are also investigating the use of efficient and general purpose correlation-based trackers <ref> [44, 66] </ref> to improve tracking accuracy and versatility. 9 Concluding Remarks We have demonstrated that fast and accurate merging of graphics and live video can be achieved using a simple approach that requires no metric information about the camera's calibration parameters or about the 3D locations and dimensions of the environment's

References-found: 66

