URL: ftp://ftp.cs.toronto.edu/pub/zoubin/linear-gaussian.ps.gz
Refering-URL: http://www.cs.utoronto.ca/~zoubin/
Root-URL: 
Email: roweis@cns.caltech.edu  zoubin@cs.toronto.edu  
Title: A Unifying Review of Linear Gaussian Models  
Author: Sam Roweis Zoubin Ghahramani 
Note: Submitted for Publication  
Date: August 1997  
Address: Toronto  
Affiliation: Computation and Neural Systems California Institute of Technology  Department of Computer Science University of  
Abstract: Factor analysis, principal component analysis (PCA), mixtures of Gaussian clusters, vector quantization (VQ), Kalman filter models and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities we show how independent component analysis (ICA) is also a variation of the same basic generative model. We show that factor analysis and mixtures of Gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data known as sensible principal component analysis (SPCA) as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudo-code for inference and learning for all the basic models. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H. H. </author> <year> (1996). </year> <title> A new learning algorithm for blind signal separation. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 757-763. </pages> <publisher> The MIT Press. </publisher>
Reference: <author> Baldi, P. and Hornik, K. </author> <year> (1989). </year> <title> Neural networks and principal components analysis: Learning from examples without local minima. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 53-58. </pages> <note> 28 Baram, </note> <author> Y. and Roth, Z. </author> <year> (1994). </year> <title> Density shaping by neural networks with application to classification, estimation and forecasting. </title> <type> Technical Report TR-CIS-94-20, </type> <institution> Center for Intelligent Systems, Technion, Israel Institute for Technology, Haifa, Israel. </institution>
Reference-contexts: Therefore, ICA could potentially be implemented using EM for Generative Topographic Mappings in the limit of zero output noise. 8 Network interpretations and regularization Since early in the modern history of neural networks it was realized that principal component analysis could be implemented using a linear autoencoder network <ref> (Baldi and Hornik, 1989) </ref>. The data is fed both as the input and target of the network, and the network parameters are learned using the squared error cost function.
Reference: <author> Baum, L. E. </author> <year> (1972). </year> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of a markov process. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8. </pages>
Reference: <author> Baum, L. E. and Eagon, J. A. </author> <year> (1967). </year> <title> An inequality with applications to statistical estimation for probabilistic functions of markov processes and to a model for ecology. </title> <journal> Bulletin of American Mathematical Society, </journal> <volume> 73 </volume> <pages> 360-363. </pages>
Reference: <author> Baum, L. E. and Petrie, T. </author> <year> (1966). </year> <title> Statistical inference for probabilistic functions of finite state Markov chains. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 41. </volume>
Reference: <author> Baum, L. E., Petrie, T., Soules, G., and Weiss, N. </author> <year> (1970). </year> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 41(1) </volume> <pages> 164-171. </pages>
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7(6) </volume> <pages> 1129-1159. </pages>
Reference: <author> Bishop, C. M., Svensen, M., and Williams, C. K. I. </author> <title> (In Press). GTM: A principled alternative to the self-organizing map. </title> <booktitle> Neural Computation. </booktitle>
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis: A new concept. </title> <booktitle> Signal Processing, </booktitle> <volume> 36 </volume> <pages> 287-314. </pages>
Reference-contexts: We will focus on a modified, but by now classic, version due to Bell and Sejnowski (1995) and Baram and Roth (1994) of the original independent component analysis algorithm <ref> (Comon, 1994) </ref>.
Reference: <author> Delyon, B. </author> <year> (1993). </year> <title> Remarks on filtering of semi-Markov data. </title> <type> Technical Report 733, </type> <institution> Institute de Recherche en Informatique et Systems Aleatiores, Campus de Beaulieu - 35042 Rennes Cedex France. </institution>
Reference: <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm (with discussion). </title> <journal> Journal of the Royal Statistical Society series B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference: <author> Digalakis, V., Rohlicek, J. R., and Ostendorf, M. </author> <year> (1993). </year> <title> ML estimation of a stochastic linear system with the EM algorithm and its application to speech recognition. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 1(4) </volume> <pages> 431-442. </pages>
Reference: <author> Duda, R. O. and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Elliott, R. J., Aggoun, L., and Moore, J. B. </author> <year> (1995). </year> <title> Hidden Markov Models: Estimation and Control, </title> <booktitle> volume 29 of Applications of Mathematics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York Berlin Heidelberg. </address>
Reference: <author> Everitt, B. S. </author> <year> (1984). </year> <title> An Introducction to Latent Variable Models. </title> <publisher> Chapman and Hill, </publisher> <address> London and New York. </address>
Reference-contexts: The directions 8 The correction k (k 1)=2 comes in because of the degeneracy in ordering the factors see for example <ref> (Everitt, 1984) </ref>. 9 Since isotropic scaling of the data space is arbitrary we could just as easily take the limit as the diagonal elements of Q became infinite while holding R finite or take both limits at once.
Reference: <author> Fraser, A. M. and Dimitriadis, A. </author> <year> (1993). </year> <title> Forecasting probability densities by using hidden markov models with mixed states. </title> <editor> In Weigend, A. S. and Gershenfeld, N. A., editors, </editor> <title> Time series prediction: </title> <booktitle> Forecasting the future and understanding the past, </booktitle> <pages> pages 265-282. </pages> <publisher> Addison Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: As another example, it is easy to design and work with models having a mixed continuous and discrete state vector (such as for example Hidden Filter HMMs <ref> (Fraser and Dimitriadis, 1993) </ref>) which is something not directly addressed by the individual literatures on discrete or continuous models. Another practical advantage is the ease with which natural extensions to the basic models can be developed.
Reference: <author> Ghahramani, Z. and Hinton, G. </author> <year> (1996a). </year> <title> Parameter estimation for linear dynamical systems. </title> <type> Technical Report CRG-TR-96-2, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference: <author> Ghahramani, Z. and Hinton, G. </author> <year> (1996b). </year> <title> Switching state-space models. </title> <type> Technical Report CRG-TR-96-3 DRAFT, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference: <author> Ghahramani, Z. and Hinton, G. </author> <year> (1996 </year> <month> (revised Feb. </month> <year> 1997)). </year> <title> The EM algorithm for mixtures of factor analyzers. </title> <type> Technical Report CRG-TR-96-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference: <author> Ghahramani, Z. and Jordan, M. I. </author> <year> (1994). </year> <title> Supervised learning from incomplete data via an EM approach. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 120-127. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Goodwin, G. and Sin, K. </author> <year> (1984). </year> <title> Adaptive filtering prediction and control. </title> <publisher> Prentice-Hall. 29 Hinton, </publisher> <editor> G. and Ghahramani, Z. </editor> <title> ((to appear 1997)). Generative models for discovering sparse distributed representations. </title> <journal> Philosophical Transactions of the Royal Society, B. </journal>
Reference-contexts: log jQ 1 j + t (p + k) log 2 5 More precisely, in a model where all the matrices are full-rank, the problem of inferring the state from a sequence of t consecutive observations is well-defined as long k tp (a notion related to observability in systems theory <ref> (Goodwin and Sin, 1984) </ref>).
Reference: <author> Hinton, G. E., Dayan, P., and Revow, M. </author> <year> (1997). </year> <title> Modeling the manifolds of Images of handwritten digits. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 8 </volume> <pages> 65-74. </pages>
Reference-contexts: In fact, most of these mixtures have already been considered: mixtures of linear dynamical systems are known as switching state-space models (see (Shumway and Stoffer, 1991; Ghahramani and Hinton, 1996b) and references therein); mixtures of factor analyzers <ref> (Ghahramani and Hinton, 1997) </ref> and of pancakes (PCA) (Hinton et al., 1995); mixtures of hidden Markov models (Smyth, 1997). A mixture of m of our constrained mixtures of Gaussians each with k clusters gives a mixture model with mk components in which there are only m possible covariance matrices.
Reference: <author> Hinton, G. E., Revow, M., and Dayan, P. </author> <year> (1995). </year> <title> Recognizing handwritten digits using mixtures of linear models. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 1015-1022. </pages> <publisher> The MIT Press. </publisher>
Reference-contexts: In fact, most of these mixtures have already been considered: mixtures of linear dynamical systems are known as switching state-space models (see (Shumway and Stoffer, 1991; Ghahramani and Hinton, 1996b) and references therein); mixtures of factor analyzers (Ghahramani and Hinton, 1997) and of pancakes (PCA) <ref> (Hinton et al., 1995) </ref>; mixtures of hidden Markov models (Smyth, 1997). A mixture of m of our constrained mixtures of Gaussians each with k clusters gives a mixture model with mk components in which there are only m possible covariance matrices.
Reference: <author> Hinton, G. E. and Zemel, R. S. </author> <year> (1994). </year> <title> Autoencoders, minimum description length, and Helmholtz free energy. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 3-10. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Jordan, M. I. and Jacobs, R. A. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214. </pages>
Reference: <author> Kalman, R. E. </author> <year> (1960). </year> <title> A new approach to linear filtering and prediction problems. </title> <journal> Trans. ASME, Series D, Journal of Basis Engineering, </journal> (8):35-45. 
Reference-contexts: The algorithms for computing the posterior means and covariances consists of two parts: a forward recursion which uses the observations from y 1 to y t , known as the Kalman filter <ref> (Kalman, 1960) </ref>, and a backward recursion which uses the observations from y t to y t+1 (Rauch, 1963). The combined forward and backward recursions are known as the Kalman or Rauch-Tung-Streibel (RTS) smoother.
Reference: <author> Kalman, R. E. and Bucy, R. S. </author> <year> (1961). </year> <title> New results in linear filtering and prediction theory. </title> <journal> Trans. ASME, Series D, Journal of Basis Engineering, </journal> (83):95-108. 
Reference: <author> Kohonen, T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69. </pages>
Reference: <author> Little, R. J. A. and Rubin, D. B. </author> <year> (1987). </year> <title> Statistical Analysis with Missing Data. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Ljung, L. and Soderstrom, T. </author> <year> (1983). </year> <title> Theory and Practice of Recursive Identification. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: There is a corresponding area of study in control theory known as system identification which investigates learning in continuous state models. For linear Gaussian models there are several approaches to system identification <ref> (Ljung and Soderstrom, 1983) </ref>, but to clarify the relationship between these models and the others we review in this paper, we focus on system identification methods based on the EM algorithm, described below.
Reference: <author> Lloyd, S. P. </author> <year> (1982). </year> <title> Least squares quantization in PCM. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28 </volume> <pages> 128-137. </pages>
Reference: <author> Lyttkens, E. </author> <year> (1966). </year> <title> On the fixpoint property of wold's iterative estimation method for principal components. </title> <editor> In Krishnaiah, P., editor, </editor> <title> Paper in Multivariate Analysis. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> MacKay, D. J. C. </author> <year> (1996). </year> <title> Maximum likelihood and covariant algorithms for independent component analysis. </title> <type> Technical Report Draft 3.7, </type> <institution> Cavendish Laboratory, University of Cambridge, </institution> <address> Madingley Road, Cambridge CB3 0HE. </address>
Reference-contexts: Recent experience has shown the surprising result that for non-Gaussian distributed sources this problem can often be solved even with no prior knowledge about the sources or about C. It is widely believed <ref> (and beginning to be proven theoretically, see MacKay, 1996) </ref> that high kurtosis source distributions are most easily separated. We will focus on a modified, but by now classic, version due to Bell and Sejnowski (1995) and Baram and Roth (1994) of the original independent component analysis algorithm (Comon, 1994). <p> The gradient learning rule to increase the likelihood is: W / W T + f (Wy * )y T 15 where the learning rule nonlinearity f () is the derivative of the implicit log prior: f (x) = d log p x (x) <ref> (MacKay, 1996) </ref>. Therefore, any generative nonlinearity g () results in a non-Gaussian prior p x (), which in turn results in a nonlinearity f () in the maximum likelihood learning rule. <p> A popular choice for the ICA learning rule nonlinearity f () is the tanh function, which corresponds a heavy tailed prior over the sources <ref> (MacKay, 1996) </ref>: p x (x) = cosh (x) : (29) From (27) we obtain a general relationship between the cumulative distribution function of the prior on the sources, cdf x (x), and of the zero-mean, unit variance noise w: cdf x (g (w)) = cdf w (w) = 2 1 erf
Reference: <author> Neal, R. M. and Hinton, G. E. </author> <year> (1993). </year> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <note> Submitted. </note>
Reference-contexts: of a global configuration (X; Y) to be log P (X; Yj), then some readers may notice that the lower bound F (Q; ) L () is the negative of a quantity known in statistical physics as the free energy: the expected energy under Q minus the entropy of Q <ref> (Neal and Hinton, 1993) </ref>. The EM algorithm alternates between maximizing F with respect to the distribution Q and the parameters , respectively, holding the other fixed. <p> is obtained by maximizing the the first term in (13), since the entropy of Q does not depend on : M step: k+1 arg max X This is the expression most often associated with the EM algorithm, but it obscures the elegant interpretation of EM as coordinate ascent in F <ref> (Neal and Hinton, 1993) </ref>. Since F = L at the beginning of each M step, and since the E step does not change , we are guaranteed not to decrease the likelihood after each combined EM step.
Reference: <author> Nowlan, S. J. </author> <year> (1991). </year> <title> Maximum likelihood competitive learning. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 574-582. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Pearlmutter, B. A. and Parra, L. C. </author> <year> (1997). </year> <title> Maximum likelihood blind source separation: A context-sensitive generalization of ica. </title> <editor> In Mozer, M., Jordan, M., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 9, </volume> <pages> pages 613-619. </pages> <publisher> The MIT Press. </publisher>
Reference-contexts: The algorithm, originally derived for unordered data, has also been extended to modeling time series <ref> (Pearlmutter and Parra, 1997) </ref>. We now show that the generative model underlying ICA can be obtained by modifying slightly the basic model we have considered thus far. The modification is to replace the WTA [] nonlinearity introduced above with a general nonlinearity g [] which operates componentwise on its input.
Reference: <author> Rabiner, L. R. and Juang, B. H. </author> <year> (1986). </year> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16. </pages>
Reference-contexts: So the regular Kalman filter and RTS smoothing recursions suffice. It is possible (see for example <ref> (Rabiner and Juang, 1986) </ref>) to learn the discrete state model parameters based on the results of the Viterbi decoding instead of the forward-backward smoothing | in other words to maximize the joint likelihood of the observations and the single most likely state sequence rather than the total likelihood summed over all <p> For 20 our (constrained) mixture of Gaussians model this is another way to get a "full" mixture. For hidden Markov models this is a well known extension and is usually the standard approach for emission density modeling <ref> (Rabiner and Juang, 1986) </ref>. However we are not aware of any work which considers this variation in the continuous state cases, either for static or dynamic data. Another important natural extension is spatially adaptive observation noise.
Reference: <author> Rauch, H. E. </author> <year> (1963). </year> <title> Solutions to the linear smoothing problem. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 8 </volume> <pages> 371-372. </pages>
Reference-contexts: The algorithms for computing the posterior means and covariances consists of two parts: a forward recursion which uses the observations from y 1 to y t , known as the Kalman filter (Kalman, 1960), and a backward recursion which uses the observations from y t to y t+1 <ref> (Rauch, 1963) </ref>. The combined forward and backward recursions are known as the Kalman or Rauch-Tung-Streibel (RTS) smoother.
Reference: <author> Rauch, H. E., Tung, F., and Striebel, C. T. </author> <year> (1965). </year> <title> Maximum likelihood estimates of linear dynamic systems. </title> <journal> AIAA Journal, </journal> <volume> 3(8) </volume> <pages> 1445-1450. </pages>
Reference: <author> Roweis, S. </author> <year> (1997a). </year> <note> EM algorithms for PCA and sensible PCA [to appear in NIPS10]. Technical Report CNS-TR-97-02, </note> <month> (April </month> <year> 1997), </year> <title> Computation and Neural Systems, </title> <institution> California Institute of Technology. </institution> <note> 30 Roweis, </note> <author> S. </author> <year> (1997b). </year> <title> Learning spatially adaptive observation noise in linear dynamical systems. </title> <type> Technical Report CNS-TR-97-03, </type> <institution> Computation and Neural Systems, California Institute of Technology. </institution>
Reference-contexts: 5.3 SPCA and PCA If instead of restricting R to be merely diagonal, we require it to be a multiple of the identity matrix (in other words the covariance ellipsoid of v * is spherical) then we have a model which we will call sensible principal component analysis or SPCA <ref> (Roweis, 1997a) </ref>. The columns of C are known as the sensible principal components and we will call the scalar value on the diagonal of R the global noise level. Note that SPCA uses 1+pk k (k 1)=2 free parameters to model the covariance. <p> Note that SPCA uses 1+pk k (k 1)=2 free parameters to model the covariance. Once again, inference is done with equation (19b) and learning by the EM algorithm (except that we now take the trace of the maximum likelihood estimate for R to learn the noise level see <ref> (Roweis, 1997a) </ref>). Unlike factor analysis, SPCA considers the original axis rotation in which the data arrived to be unimportant: if the measurement coordinate system were rotated SPCA could (left) multiply C by the same rotation and the likelihood of the new data would not change. <p> There is still an EM algorithm for learning <ref> (Roweis, 1997a) </ref> although it can of course only learn C. For PCA, we could just diagonalize the sample covariance of the data and take the leading k eigenvectors multiplied by their eigenvalues to be the columns of C.
Reference: <author> Rubin, D. B. and Thayer, D. T. </author> <year> (1982). </year> <title> EM algorithms for ML factor analysis. </title> <journal> Psychmetrika, </journal> <volume> 47(1) </volume> <pages> 69-76. </pages>
Reference: <author> Shumway, R. H. and Stoffer, D. S. </author> <year> (1982). </year> <title> An approach to time series smoothing and forecasting using the EM algorithm. </title> <journal> Journal of Time Series Analysis, </journal> <volume> 3(4) </volume> <pages> 253-264. </pages>
Reference: <author> Shumway, R. H. and Stoffer, D. S. </author> <year> (1991). </year> <title> Dynamic linear models with switching. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 86(415) </volume> <pages> 763-769. </pages>
Reference: <author> Sirovich, L. </author> <year> (1987). </year> <title> Turbulence and the dynamics of coherent structures. </title> <journal> Quarterly Applied Mathematics, </journal> <volume> 45(3) </volume> <pages> 561-590. </pages>
Reference-contexts: So if we are working with patterns in a large (thousands) number of dimensions and want to extract only a few (tens) principal components we cannot naively try to diagonalize the sample covariance of our data. Techniques like the snap-shot method <ref> (Sirovich, 1987) </ref> attempt to address this but still require the diagonalization of an N by N matrix where N is the number of data points. The EM algorithm approach solves all of these problems, requiring no explicit diagonalization whatsoever and the inversion of only a k by k matrix.
Reference: <author> Smyth, P. </author> <year> (1997). </year> <title> Clustering sequences with hidden Markov models. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 9. </volume> <publisher> The MIT Press. </publisher>
Reference-contexts: these mixtures have already been considered: mixtures of linear dynamical systems are known as switching state-space models (see (Shumway and Stoffer, 1991; Ghahramani and Hinton, 1996b) and references therein); mixtures of factor analyzers (Ghahramani and Hinton, 1997) and of pancakes (PCA) (Hinton et al., 1995); mixtures of hidden Markov models <ref> (Smyth, 1997) </ref>. A mixture of m of our constrained mixtures of Gaussians each with k clusters gives a mixture model with mk components in which there are only m possible covariance matrices.
Reference: <author> Tipping, M. E. and Bishop, C. M. </author> <year> (1997). </year> <title> Mixtures of probabilistic principal component analysers. </title> <type> Technical Report TR-NCRG/97/003, </type> <institution> Neural Computing Research Group, Aston University, Birmingham B4 7ET. </institution>
Reference-contexts: If we were to rescale one of the components of y the model could not be easily corrected since v has spherical covariance (R = *I). The SPCA model is very similar to the independently proposed probabilistic principal component analysis <ref> (Tipping and Bishop, 1997) </ref>. If we go even further and take the limit R = lim *!0 *I (while keeping the diagonal elements of Q finite 9 ) then we obtain the standard principal component analysis or PCA model.
Reference: <author> Tresp, V., Ahmad, S., and Neuneier, R. </author> <year> (1994). </year> <title> Training neural networks with deficient data. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Francisco, CA. </address> <publisher> Morgan Kaufman Publishers. </publisher>
Reference: <author> Viterbi, A. J. </author> <year> (1967). </year> <title> Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. </title> <journal> IEEE Transactions on Information Theory, IT-13:260-269. </journal> <volume> 31 </volume>
Reference-contexts: For the discrete state models much of the literature stems from the work of Baum and colleagues (Baum and Petrie, 1966; Baum and Eagon, 1967; Baum et al., 1970; Baum, 1972) on hidden Markov models and of Viterbi <ref> (Viterbi, 1967) </ref> and others on optimal decoding. <p> This creates the need for separate inference algorithms to find the single most likely state sequence given the observations. Such algorithms for filtering and smoothing are called Viterbi decoding methods <ref> (Viterbi, 1967) </ref>.
References-found: 48

