URL: http://www.cc.gatech.edu/faculty/ashwin/papers/er-96-07.ps.Z
Refering-URL: http://www.cs.gatech.edu/faculty/ashwin/ABSTRACTS-summary.html
Root-URL: 
Email: markd@cc.gatech.edu  ashwin@cc.gatech.edu  
Title: ICML-96 Workshop "Learning in context-sensitive domains" Bari, Italy. Dynamically Adjusting Concepts to Accommodate Changing Contexts  
Author: Mark Devaney Ashwin Ram 
Address: Atlanta, GA 30332-0280  Atlanta, GA 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  College of Computing Georgia Institute of Technology  
Abstract: In concept learning, objects in a domain are grouped together based on similarity as determined by the attributes used to describe them. Existing concept learners require that this set of attributes be known in advance and presented in entirety before learning begins. Additionally, most systems do not possess mechanisms for altering the attribute set after concepts have been learned. Consequently, a veridical attribute set relevant to the task for which the concepts are to be used must be supplied at the onset of learning, and in turn, the usefulness of the concepts is limited to the task for which the attributes were originally selected. In order to efficiently accommodate changing contexts, a concept learner must be able to alter the set of descriptors without discarding its prior knowledge of the domain. We introduce the notion of attribute-incrementation, the dynamic modification of the attribute set used to describe instances in a problem domain. We have implemented the capability in a concept learning system that has been evaluated along several dimensions using an existing concept formation system for com parison.
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. R., & Matessa, M. </author> <year> (1990). </year> <title> A rational analysis of categorization. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 76-84). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Anderson, J. R., & Matessa, M. </author> <title> (1992) Explorations of an incremental, Bayesian algorithm for categorization. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 275-308. </pages>
Reference: <author> Barsalou, L. W. </author> <year> (1991). </year> <title> Deriving categories to achieve goals. </title> <editor> In G. H. Bower (Ed.), </editor> <booktitle> The psychology of learning and motivation: Advances in research and theory, </booktitle> <volume> (Vol. </volume> <pages> 27). </pages> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Similarly, it has been shown that when constructing plans, people frequently add attributes to the set they are considering as the plan progresses as well as create ad hoc categories in response to novel goals that may arise <ref> (Barsalou, 1991) </ref>. The domain theory of the agent also affects the set of attributes considered relevant (Pazzani, 1991; Wisniewski & Medin, 1991), and domain theories change as experience and knowledge are gained through interaction and experimentation in the environment.
Reference: <author> Beach, K. </author> <year> (1988). </year> <title> The role of external mnemonic symbols in acquiring an occupation. </title> <editor> In M. M. Gruneberg, P. E. Morris, & R. N. Sykes (Eds.), </editor> <booktitle> Practical aspects of memory: Current research and issues, </booktitle> <volume> (Vol. </volume> <pages> 1). </pages> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Boster, J. S., & Johnson, J. C. </author> <year> (1989). </year> <title> Form or function: a comparison of expert and novice judgements of similarity among fish. </title> <journal> American Anthropologist, </journal> <volume> 91, </volume> <pages> 866-889. </pages>
Reference: <author> Doak, J. </author> <year> (1992). </year> <title> An evaluation of feature selection methods and their application to computer security (Tech. </title> <type> Rep. </type> <institution> CSE-92-18). Davis: University of Cali-fornia. </institution>
Reference: <author> Domingos, P. </author> <year> (1996). </year> <title> Context-sensitive feature selection or lazy learners. </title> <note> To appear in Artificial Intelligence Review. </note>
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 139-172. </pages>
Reference: <author> Fisher, D. H., & Langley, P. </author> <year> (1991). </year> <title> The structure and formation of natural categories. </title> <editor> In G. H. Bower (Ed.), </editor> <booktitle> The psychology of learning and motivation: Advances in research and theory, </booktitle> <volume> (Vol. </volume> <pages> 27). </pages> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Existing inductive concept learners may be described as either incremental or non-incremental based on the way in which they process instances being classified. Incremental learners incorporate instances one-at-a-time and refine their concepts dynamically <ref> (Fisher & Langley, 1991) </ref>, whereas non-incremental learners require that all instances to be classified be supplied at once before any concepts are formed. The incremental approach allows a system to efficiently incorporate new information from the environment and adjust its knowledge accordingly.
Reference: <author> Gennari, J. H. </author> <year> (1989). </year> <title> Focused concept formation. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 379-382). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gennari, J. H. </author> <year> (1990). </year> <title> An experimental study of concept formation (Tech. </title> <type> Rep. 90-06). </type> <institution> Irvine: University of California, Department of Information and Computer Science. </institution>
Reference: <author> Gluck, M. A., & Corter, J. E. </author> <year> (1985). </year> <title> Information, uncertainty, and the utility of categories. </title> <booktitle> Proceedings of the Seventh Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 283-287). </pages> <address> Irvine, CA: </address> <publisher> Lawrence Erl-baum Associates. </publisher>
Reference-contexts: WHILE CU m &gt; CU 0 stance. The concept hierarchy is created dynamically as instances are incorporated through the use of simple operators and an evaluation function called category utility <ref> (Gluck & Corter, 1985) </ref>. The major components of COBWEB are the category utility metric and the operators used to create and modify the concept hierarchy, described below. 1 3.2 AICC AICC is an inductive concept formation system that incorporates attributes incrementally rather than instances.
Reference: <author> Hadzikadic, M. & Yun, D. Y. Y. </author> <year> (1988). </year> <title> Concept formation by goal-driven, context-dependent classification. </title> <booktitle> Methodologies for intelligent systems, </booktitle> <volume> 3, </volume> <pages> 322-333. </pages>
Reference: <author> Hirsh, H. & Japkowicz, N. </author> <year> (1994). </year> <title> Bootstrapping training-data representations for inductive learning: a case study in molecular biology. </title> <booktitle> Proceedings of the Twelfth International Conference on Artificial Intelligence (pp. </booktitle> <pages> 639-644). </pages> <address> Seattle, WA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Martin, J. D. & Billman, D. O. </author> <year> (1991). </year> <title> Variability bias and category learning. </title> <booktitle> Proceedings of the Eighth International Machine Learning Workshop (pp. </booktitle> <pages> 90-94). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Martin, J. D. </author> <year> (1991). </year> <title> Transfer of predictive structure: Improving concept formation. </title> <type> Doctoral Dissertation, </type> <institution> Department of Information and Computer Science, Georgia Institute of Technology, Atlanta. </institution>
Reference: <author> McKusick, K. & Thompson, K. </author> <year> (1990). </year> <title> COBWEB/3: A portable implementation (Tech. </title> <type> Rep. </type> <institution> FIA-90-6-18-2). Moffet Field, CA: NASA Ames Research Center. </institution>
Reference: <author> Merz, C.J., & Murphy, P.M. </author> <year> (1996). </year> <note> UCI Repository of machine learning databases [http://www.ics.uci.edu/ mlearn/MLRepository.html]. </note> <institution> Irvine, CA: University of California, Department of Information and Computer Science. </institution>
Reference-contexts: The systems were evaluated using two data sets from the UCI Machine Learning Database <ref> (Merz & Mur-phy, 1996) </ref>, the soybean small database originally used in Fisher's (1987) evaluation of COBWEB, and the lymphography database 3 . The soybean data consists of 47 instances described by 35 attributes having four classes based on diagnostic condition.
Reference: <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, the invention of the microscope provided a wealth of "new" attributes with which to describe the world. Context can also change as a result of the learning algorithms themselves. One such algorithm is the technique of constructive induction <ref> (Michalski, 1983) </ref>, in which the learning system itself formulates new descriptors for use in describing its input. <p> Dynamically altering the attribute set used for a given problem has been addressed to some degree by Gen nari's (1989) CLASSIT-2 which processes attributes incrementally in order of salience, eventually ignoring those that are not relevant to a given performance task. Similarly, the technique of constructive induction <ref> (Michalski, 1983) </ref> creates new attributes during the concept formation process by combining existing attributes in certain ways.
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1989). </year> <title> Two algorithms that learn DNF by discovering relevant features. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 119-123). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Additionally, the introduction of new descriptors can be viewed as altering their relevance from zero to some non-zero value. For example, the FRINGE algorithm <ref> (Pagallo & Haussler, 1989) </ref> learns concepts by first constructing a decision tree for a set of training examples. Then, new features are generated through Boolean combinations of attributes that occur near the "fringe" of the tree.
Reference: <author> Pazzani, M. J. </author> <year> (1991). </year> <title> Influence of prior knowledge on concept acquisition: Experimental and computational results. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 17, </volume> <pages> 416-432. </pages>
Reference: <author> Ram, A. & Leak, D. </author> <year> (1995). </year> <title> Learning, goals, </title> <editor> and learning-goals. In A. Ram & D. B. Leake (Eds.), </editor> <booktitle> Goal-driven learning. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press/Bradford Books. </publisher>
Reference: <author> Rendel, L. </author> <year> (1990). </year> <title> Some issues in concept learning. </title>
Reference: <editor> In J. W. Shavlik & T. G. Dietterich (Eds.), </editor> <booktitle> Machine learning : An artificial intelligence approach (Vol. 3). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Seifert, C. M. </author> <year> (1989). </year> <title> A retrieval model using feature selection. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 52-54). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Shavlik, J. W., & Dietterich, T. G. </author> <year> (1990). </year> <booktitle> Readings in machine learning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Cost-sensitive learning of classification knowledge and its applications in robotics. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 7-33. </pages>
Reference-contexts: The ability to incrementally incorporate attributes from the environment is also important when there is a cost associated with the acquisition of information <ref> (Tan, 1993) </ref>. This type of system illustrates the trend toward creating systems which must face real-world tasks and their corresponding constraints in the pursuit of a goal.
Reference: <author> Thompson, K., Langley, P., & Iba, W. </author> <year> (1991). </year> <title> Using background knowledge in concept formation. </title> <booktitle> Proceedings of the Eighth International Machine Learning Workshop (pp. </booktitle> <pages> 554-558). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Wisniewski, E. J. & Medin, D. L. </author> <year> (1991). </year> <title> Harpoons and long sticks: The interaction of theory and similarity in rule induction. </title> <editor> In D. H. Fisher, M. J. Pazzani, & P. Langley (Eds.), </editor> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 29

