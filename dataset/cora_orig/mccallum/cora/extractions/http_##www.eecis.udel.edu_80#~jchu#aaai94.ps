URL: http://www.eecis.udel.edu:80/~jchu/aaai94.ps
Refering-URL: http://www.eecis.udel.edu:80/~jchu/
Root-URL: http://www.cis.udel.edu
Email: E-mail: jchu@cis.udel.edu  E-mail: carberry@cis.udel.edu  
Title: A Plan-Based Model for Response Generation in Collaborative Task-Oriented Dialogues  
Author: Jennifer Chu-Carroll Sandra Carberry 
Address: 19716, USA  19716, USA  Pennsylvania  
Affiliation: Department of Computer Science University of Delaware Newark, DE  Department of Computer Science University of Delaware Newark, DE  Visitor: Institute for Research in Cognitive Science University of  
Date: 799-805, 1994  
Note: Proceedings of the 12th AAAI, pages  
Abstract: This paper presents a plan-based architecture for response generation in collaborative consultation dialogues, with emphasis on cases in which the system (consultant) and user (executing agent) disagree. Our work contributes to an overall system for collaborative problem-solving by providing a plan-based framework that captures the Propose-Evaluate-Modify cycle of collaboration, and by allowing the system to initiate subdialogues to negotiate proposed additions to the shared plan and to provide support for its claims. In addition, our system handles in a unified manner the negotiation of proposed domain actions, proposed problem-solving actions, and beliefs proposed by discourse actions. Furthermore, it captures cooperative responses within the collaborative framework and accounts for why questions are sometimes never answered. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. </author> <year> 1991. </year> <title> Discourse structure in the TRAINS project. </title> <booktitle> In Darpa Speech and Natural Language Workshop. </booktitle>
Reference-contexts: Hence, once a set of actions is proposed by an agent, the other agent must first evaluate the proposal based on his own private beliefs <ref> (Allen 1991) </ref> and determine whether or not to accept the proposal.
Reference: <author> Bratman, M. </author> <year> 1990. </year> <title> What is intention? In Cohen, </title> <editor> P.; Morgan, J.; and Pollack, M., eds., </editor> <booktitle> Intentions in Communication. chapter 2, </booktitle> <pages> 15-31. </pages>
Reference-contexts: For example, the consultant presumably has more extensive and accurate domain knowledge than does the executing agent, but the executing agent has knowledge about his particular circumstances, intentions, and preferences that are either restrictions on or potential influencers <ref> (Bratman 1990) </ref> of the domain plan being constructed. In agreeing to collaborate on constructing the domain plan, the consultant assumes a stake in the quality of the resultant plan and in how the agents go about constructing it.
Reference: <author> Cawsey, A. </author> <year> 1993. </year> <title> Planning interactive explanations. </title> <journal> International Journal of Man-Machine Studies 169-199. </journal>
Reference: <author> Edmonds, P. </author> <year> 1993. </year> <title> A computational model of collaboration on reference in direction-giving dialogues. </title> <type> Technical Report CSRI-289, </type> <institution> Univ. of Toronto. </institution>
Reference: <author> Eller, R., and Carberry, S. </author> <year> 1992. </year> <title> A meta-rule approach to flexible plan recognition in dialogue. </title> <booktitle> User Modeling and User-Adapted Interaction 2 </booktitle> <pages> 27-53. </pages>
Reference-contexts: Furthermore, we augment Lambert's plan recognition algorithm (Lambert & Carberry 1992) with a simplified version of Eller's relaxation algorithm <ref> (Eller & Carberry 1992) </ref> to recognize ill-formed plans. We adopt a plan-based mechanism because it is general and easily extendable, allows the same declarative knowledge about collaborative problem-solving to be used both in generation and understanding, and allows the recursive nature of our theory to be represented by recursive meta-plans.
Reference: <author> Elzer, S.; Chu, J.; and Carberry, S. </author> <year> 1994. </year> <title> Recognizing and utilizing user preferences in collaborative consultation dialogues. </title> <booktitle> In Progress. </booktitle>
Reference-contexts: For instance, prefers (UserA, Difficulty ( course, easy), Take-Course, weak) indicates that UserA has a weak preference for taking easy courses. A companion paper describes our mechanism for recognizing user preferences during the course of a dialogue <ref> (Elzer, Chu, & Carberry 1994) </ref>. <p> A weight is computed for each candidate instanti-ation by summing the products of corresponding terms of 4 We model six degrees each of positive and negative preferences based on the conversational circumstances and the semantic representation of the utterance used to express the preferences <ref> (Elzer, Chu, & Carberry 1994) </ref>. <p> Maybury (1993) developed plan operators for persuasive utterances, but does not provide a framework for negotiation of conflicting views. In suggesting better alternatives, our system differs from van Beek's (1987) in a number of ways. The most significant are that our system dynamically recognizes user preferences <ref> (Elzer, Chu, & Carberry 1994) </ref>, takes into account both the strength of the preferences and the closeness of the matches in ranking instantiations, and captures the response generation process in an overall collaborative framework that can negotiate proposals with the user.
Reference: <author> Grosz, B., and Sidner, C. </author> <year> 1990. </year> <title> Plans for discourse. </title> <editor> In Cohen, P.; Morgan, J.; and Pollack, M., eds., </editor> <booktitle> Intentions in Communication. chapter 20, </booktitle> <pages> 417-444. </pages>
Reference: <author> Guinn, C., and Biermann, A. </author> <year> 1993. </year> <title> Conflict resolution in collaborative discourse. </title> <booktitle> In Proceedings of the IJCAI-93 Workshop:Computational Models of Conflict Management in Cooperative Problem Solving, </booktitle> <pages> 84-88. </pages>
Reference: <author> Heeman, P., and Hirst, G. </author> <year> 1992. </year> <title> Collaborating on referring expressions. </title> <type> Technical Report 435, </type> <institution> Univ. of Rochester. </institution>
Reference: <author> Heeman, P. </author> <year> 1993. </year> <title> Speech actions and mental states in task-oriented dialogues. </title> <booktitle> In AAAI 1993 Spring Symposium on Reasoning About Mental States: Formal Theories and Applications. </booktitle>
Reference-contexts: Heeman and Hirst (1992) and Edmonds (1993) use meta-plans to account for collaboration, but their mechanisms are limited to understanding and generating referring expressions. Although Heeman is extending his model to account for collaboration in task-oriented dialogues <ref> (Heeman 1993) </ref>, his extension is limited to the recognition of actions in such dialogues. Guinn and Biermann (1993) developed a model of collaborative problem-solving which attempts to resolve conflicts between agents regarding the best path for achieving a goal.
Reference: <author> Joshi, A.; Webber, B.; and Weischedel, R. </author> <year> 1984. </year> <title> Living up to expectations: Computing expert responses. </title> <booktitle> In Proceedings of the AAAI, </booktitle> <pages> 169-175. </pages>
Reference: <author> Joshi, A. </author> <year> 1982. </year> <title> Mutual beliefs in question-answer systems. </title>
Reference-contexts: Each specialization eventually decomposes into some primitive action which modifies the proposal. However, an agent will be considered uncooperative if he modifies a proposed shared plan without the collaborating agent's consent; thus, the four specializations share a common precondition that the discrepancies in beliefs must be squared away <ref> (Joshi 1982) </ref> before any modification can take place. It is the attempt to satisfy this precondition that causes the system to generate natural language utterances to accomplish the change in the user's beliefs. the former.
Reference: <editor> In Smith, N., ed., </editor> <title> Mutual Knowledge. </title> <booktitle> chapter 4, </booktitle> <pages> 181-197. </pages>
Reference: <author> Lambert, L., and Carberry, S. </author> <year> 1991. </year> <title> A tripartite plan-based model of dialogue. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> 47-54. </pages>
Reference-contexts: We assume that the current status of the interaction is represented by a tripartite dialogue model <ref> (Lambert & Carberry 1991) </ref> that captures intentions on three levels: domain, problem-solving, and discourse. The domain level con-tains the domain plan being constructed for later execution.
Reference: <author> Lambert, L., and Carberry, S. </author> <year> 1992. </year> <title> Modeling negotiation dialogues. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> 193-200. </pages>
Reference-contexts: Furthermore, we augment Lambert's plan recognition algorithm <ref> (Lambert & Carberry 1992) </ref> with a simplified version of Eller's relaxation algorithm (Eller & Carberry 1992) to recognize ill-formed plans.
Reference: <author> Litman, D., and Allen, J. </author> <year> 1987. </year> <title> A plan recognition model for subdialogues in conversation. </title> <booktitle> Cognitive Science 11 </booktitle> <pages> 163-200. </pages>
Reference: <author> Lochbaum, K. </author> <year> 1991. </year> <title> An algorithm for plan recognition in collaborative discourse. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> 33-38. </pages>
Reference: <author> Maybury, M. </author> <year> 1992. </year> <title> Communicative acts for explanation generation. </title> <journal> International Journal of Man-Machine Studies 37 </journal> <pages> 135-172. </pages>
Reference: <author> Maybury, M. </author> <year> 1993. </year> <title> Communicative acts for generating natural language arguments. </title> <booktitle> In Proceedings of the AAAI, </booktitle> <pages> 357-364. </pages>
Reference: <author> Moore, J., and Paris, C. </author> <year> 1993. </year> <title> Planning text for advisory dialogues: Capturing intentional, rhetorical and attentional information. </title> <booktitle> Computational Linguistics 19(4) </booktitle> <pages> 651-694. </pages>
Reference: <author> Pollack, M. </author> <year> 1986. </year> <title> A model of plan inference that distinguishes between the beliefs of actors and observers. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> 207-214. </pages>
Reference-contexts: Thus, the evaluator should check for two types of discrepancies in beliefs: one that causes the proposal to be viewed by the system as invalid <ref> (Pollack 1986) </ref>, and one in which the system believes that a better alternative to the user's 1 A recipe (Pollack 1986) is a template for performing an action. <p> Thus, the evaluator should check for two types of discrepancies in beliefs: one that causes the proposal to be viewed by the system as invalid <ref> (Pollack 1986) </ref>, and one in which the system believes that a better alternative to the user's 1 A recipe (Pollack 1986) is a template for performing an action. It encodes the preconditions for an action, the effects of an action, the subactions comprising the body of an action, etc. proposal exists (Joshi, Webber, & Weischedel 1984; van Beek 1987). <p> This is because it is meaningless to suggest, for example, a better alternative to an action when one believes that its parent action is infeasible. Detecting Conflicts About Plan Validity Pollack argues that a plan can fail because of an infeasible action or because the plan itself is ill-formed <ref> (Pollack 1986) </ref>. An action is infeasible if it cannot be performed by its agent; thus, the evaluator performs a feasibility check by examining whether the applicability conditions of the action are satisfied and if its preconditions can be satisfied 2 .
Reference: <author> Quilici, A. </author> <year> 1991. </year> <title> The Correction Machine: A computer Model of Recognizing and Producing Belief Justifications in Argumentative Dialogs. </title> <type> Ph.D. Dissertation, </type> <institution> UCLA. </institution>
Reference: <author> Reed, S. </author> <year> 1982. </year> <journal> Cognition: Theory and Applications. </journal> <volume> chapter 14, </volume> <pages> 337-365. </pages>
Reference-contexts: The instantiation with the highest weight is considered the best instantiation for the action under consideration. Thus, the selection strategy employed by our ranking advisor corresponds to an additive model of human decision-making <ref> (Reed 1982) </ref>. Example We demonstrate the ranking advisor by showing how two different instantiations, CS601 and CS621, of the Take-Course action are ranked. Figure 1 shows the relevant domain knowledge and user model information.
Reference: <author> Sidner, C. </author> <year> 1992. </year> <title> Using discourse to negotiate in collaborative activity: </title> <booktitle> An artificial language. In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent Systems, </booktitle> <pages> 121-128. </pages>
Reference: <author> Sycara, K. </author> <year> 1989. </year> <title> Argumentation: Planning other agents' plans. </title> <booktitle> In Proceedings of the IJCAI, </booktitle> <pages> 517-523. </pages> <editor> van Beek, P. </editor> <year> 1987. </year> <title> A model for generating better explanations. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> 215-220. </pages>
References-found: 25

