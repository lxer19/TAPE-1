URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-710.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr700.html
Root-URL: 
Title: Demand-based Coscheduling of Parallel Jobs on Multiprogrammed Multiprocessors  
Author: by Patrick Gregory Sobalvarro Arthur C. Smith 
Degree: (1992) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Author  Certified by William E. Weihl Associate Professor Department of Electrical Engineering and Computer Science Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: January 1997  January 10, 1997  
Address: (1988)  
Affiliation: S.B., Massachusetts Institute of Technology  S.M., Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1997.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> David L. Black. </author> <title> Scheduling support for concurrency and parallelism in the mach operating system. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference: [2] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. </author> <title> Myrinet|a gigabit-per-second local-area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year> <note> Available from http://www.myri.com/research/publications/Hot.ps. </note>
Reference-contexts: The implementation of Illinois Fast Messages we used ran on a Myrinet network <ref> [2] </ref> connecting eight SPARCstation-2 workstations. The Myrinet switch provides a relatively low-latency, high-bandwidth interconnection for workstations.
Reference: [3] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <address> Santa Clara, California, </address> <year> 1991. </year>
Reference-contexts: Dynamic coscheduling should also work on many distributed-shared-memory multiprocessors. In a cache-coherence scheme such as the software schemes presented by Chaiken et al. in <ref> [3] </ref>, cache line invalidations can be treated in the same fashion as arriving messages. We can do even better on systems with network interface processors, such as FLASH [15] or Typhoon [20].
Reference: [4] <author> Rohit Chandra, Scott Devine, Ben Verghese, Anoop Gupta, and Mendel Rosen-blum. </author> <title> Scheduling and page migration for multiprocessor compute servers. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 12-24, </pages> <address> San Jose, Cali-fornia, </address> <year> 1994. </year>
Reference-contexts: timesharing results in poor performance Crovella et al. have presented results in [5] that show that independent timesharing without regard for synchronization produced significantly greater slowdowns than coscheduling, in some cases a factor of two worse in total runtime of applications. 1 Chandra et al. have reported similar results in <ref> [4] </ref>: in some cases independent timesharing is as much as 40% slower than coscheduling. In [7], Feitelson and Rudolph compared the performance of gang scheduling using busy-waiting synchronization to that of independent (uncoordinated) timesharing using blocking synchronization. <p> Gupta et al. report in [10] that when coscheduling was used with 25-millisecond timeslices on a simulated system, it achieved 71% utilization, as compared to 74% for batch scheduling (poorer performance is reported with 10-millisecond timeslices). Chandra et al. conclude in <ref> [4] </ref> that coscheduling and process control achieve similar speedups running on the Stanford DASH distributed-shared-memory multiprocessor as compared to independent timesharing. However, traditional coscheduling suffers from two problems. <p> Several published works <ref> [4, 10, 23] </ref> cite good performance for process control, but these works also find that coscheduling can be modified to have equivalently good performance. <p> In [10], the LU application is found to perform very poorly under process control when run on three processors, and the authors point out that a drastically increased cache miss rate is to blame. Similarly, in <ref> [4] </ref>, the Ocean application suffers a twofold decrease in efficiency when run on eight processors as compared to when it is run on sixteen processors.
Reference: [5] <author> Mark Crovella, Prakash Das, Cezary Dubnicki, Tom LeBlanc, and Evangelos Markatos. </author> <title> Multiprogramming on multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 590-597, </pages> <year> 1991. </year>
Reference-contexts: Furthermore, it can be difficult to know in advance how many processes a job will require or which processes will communicate with other processes | in an interactive environment, these might depend on user input. 1.2.2 Independent timesharing results in poor performance Crovella et al. have presented results in <ref> [5] </ref> that show that independent timesharing without regard for synchronization produced significantly greater slowdowns than coscheduling, in some cases a factor of two worse in total runtime of applications. 1 Chandra et al. have reported similar results in [4]: in some cases independent timesharing is as much as 40% slower than <p> The result is that even in the two-job case examined by Crovella et al. in <ref> [5] </ref>, when approximately 25% of the cycles in the multiprocessor were devoted to running alternates, their use decreased the runtime of the application to which they were devoted only about 1%. 2.2 Distributed Hierarchical Control Distributed hierarchical control was presented by Feitelson and Rudolph in [8].
Reference: [6] <author> Andrea C. Dusseau, Remzi H. Arpaci, and David E. Culler. </author> <title> Effective distributed scheduling of parallel workloads. </title> <booktitle> In ACM SIGMETRICS '96 Conference on the Measurement and Modeling of Computer Systems, </booktitle> <year> 1996. </year> <note> Available from http://www.cs.berkeley.edu/~dusseau/Papers/sigmetrics96.ps. </note>
Reference-contexts: The experiments described in <ref> [6] </ref> were performed used the Solaris 2.4 scheduler code in a simulation of 32 workstations running 3 parallel jobs, each having one process residing on each of the 32 workstations. <p> The paper correctly states that the priority-boosting mechanism of the Solaris 2.4 scheduler is responsible (as we confirm experimentally in Section 5.4.1), but provides an account of this that states 2 There is some ambiguity in <ref> [6] </ref> about whether the term "implicit scheduling" is also intended to cover all approaches for achieving coordinated scheduling in a network of workstations by making local decisions based on information about communication; but because so broad a description would also cover demand-based coscheduling, described here and in [22], we will use <p> making local decisions based on information about communication; but because so broad a description would also cover demand-based coscheduling, described here and in [22], we will use the term "implicit scheduling" to describe only the combination of spin-block message receipt with the algorithm for adaptively determining spin times described in <ref> [6] </ref>. 3 Given the variable amount of computation that was performed in each cycle of these processes, it seems possible that the use of spinning message receipt was not the most optimistic choice for the idealized gang scheduler. <p> This is incorrect; we examine these issues further in Section 5.4.1. A later section in <ref> [6] </ref> describes an experiment with a "round-robin" scheduler (simulating only one run queue for the Solaris 2.4 scheduler rather than the normal sixty), 4 which found very poor performance for spin-block message receipt. This was attributed to the absence of priorities in round-robin schedulers, which is an overly narrow statement. <p> Dusseau et al. also argue for this fixed spin time in <ref> [6] </ref>, on the basis that two context-switch times might be required for a processor to respond to a message if the message arrives at the beginning of a context switch to a process that is not the one to which the message is directed. <p> However, as we mentioned in Section 2.5, Dusseau et al. reported in <ref> [6] </ref> that SPMD programs that used fixed spin times and spin-block message receipt when running on a simulated workstation cluster under the Solaris 2.4 scheduler had performance that was within a factor of two of that found under an idealized gang scheduler. <p> Dusseau et al. attributed this relatively good performance to the priority-boosting behavior of the Solaris 2.4 scheduler in <ref> [6] </ref>. Their attribution of this performance describes coordinated scheduling arising as follows. First, a process awaiting a message from a descheduled process on another node will block when its maximum spin time has elapsed. <p> Then, when the descheduled process is scheduled and sends its message, on the receiving node, the interrupt routine for the network interface will unblock the receiving process, and the operating system will move it from a sleep queue to a run queue. In <ref> [6] </ref>, the next step is described as one in which the newly-awakened process receives a significant priority boost from the Solaris 2.4 scheduler; although, as we shall see below, this is not invariably true. <p> with fine-grain message-passing and short spin times, where it is quite unlikely that the process has been preempted while runnable but spinning, this mechanism as described does indeed implement the most significant part of dynamic coscheduling. 2 2 Although our earlier paper [22] on dynamic coscheduling is briefly cited in <ref> [6] </ref>, Dusseau et al. apparently failed to notice that achieving coscheduling by scheduling a process when a message 67 Quantum Prio. (ms) ts slpret lowest priority 0- 9 200 50 10-19 160 51 30-34 80 53 40-44 40 55 50-54 40 57 highest priority 59 20 59 Table 5.2: Default Solaris <p> arrives, which is the phenomenon underlying the relatively good performance of parallel processes using spin-block message receipt under Solaris 2.4, is first proposed and analyzed in our work. 3 We had also believed that this was the behavior of the Solaris 2.4 scheduler, because, in additions to the description in <ref> [6] </ref>, the Solaris 2.4 time-sharing dispatcher parameter table manual page (ts dptbl) [13] describes it this way; but a series of experiments we undertook to show that such behavior would allow a user-mode program to receive an unfairly large proportion of CPU time failed to demonstrate that the priority boosting happened <p> Further investigation, using different sorts of competitors, will be necessary to determine whether this behavior is in fact arising. Reasons for improved performance under spin-block message receipt We sought to confirm experimentally the claim in <ref> [6] </ref> that the priority boosting performed by the scheduler was responsible for the relatively good performance we observed in programs using spin-block message receipt. To do so, we ran a simple experiment. <p> It was hypothesized in <ref> [6] </ref> that the reason others who had investigated the performance of parallel programs on timeshared multiprocessors using uncoordinated 71 scheduling with spin-block synchronization had found poor performance was that they had used a round-robin scheduler, rather than a priority scheduler. <p> Performance for spin-block message receipt without DCS was surprisingly good, although not as good as that with DCS. We experimentally verified the claim made in <ref> [6] </ref> that the reason for this relatively good performance was priority boosting on process wakeup by the Solaris 2.4 scheduler. By causing descheduled processes to sometimes be scheduled on message arrival, this behavior has the effect of a partial implementation of DCS. <p> It would be best, however, to implement a fairness mechanism in a scheduler in which precise processor shares could be allotted to processes. Priority-decay sched-ulers confuse execution order (which is important in coscheduling) with processor share (which should be separately modifiable). Dusseau et al. stated in <ref> [6] </ref> that fairness was not yet a solved problem in implicit scheduling; they observed that fine-grained processes suffered decreased processor shares when sharing a processor with coarse-grained processes in their simulation. They conjectured that this was because the fine-grained processes blocked often, yielding the processor. <p> Such techniques might serve to maximize time spent cosched-uled. 6.2.7 Other issues We found we achieved good performance with a fixed spin time and after initial experimentation settled on one. However, in <ref> [6] </ref>, Dusseau found that adaptive variation of spin times enhanced the performance of SPMD programs using spin-block message receipt under Solaris 2.4. We believe that some of the conclusions reached in [6] were peculiar to the communication patterns of the limited set of applications examined in that work; however, it is <p> However, in <ref> [6] </ref>, Dusseau found that adaptive variation of spin times enhanced the performance of SPMD programs using spin-block message receipt under Solaris 2.4. We believe that some of the conclusions reached in [6] were peculiar to the communication patterns of the limited set of applications examined in that work; however, it is possible that adaptive variation of spin times would also improve the performance of a broader class of applications, and that further work in this area might prove fruitful.
Reference: [7] <author> D. G. Feitelson and L. Rudolph. </author> <title> Gang scheduling performance benefits for fine-grain synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(4) </volume> <pages> 306-318, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In <ref> [7] </ref>, Feitelson and Rudolph compared the performance of gang scheduling using busy-waiting synchronization to that of independent (uncoordinated) timesharing using blocking synchronization. They found that for applications with fine-grain synchronization, performance could degrade severely under uncoordinated timesharing as compared to gang scheduling.
Reference: [8] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: by Crovella et al. in [5], when approximately 25% of the cycles in the multiprocessor were devoted to running alternates, their use decreased the runtime of the application to which they were devoted only about 1%. 2.2 Distributed Hierarchical Control Distributed hierarchical control was presented by Feitelson and Rudolph in <ref> [8] </ref>. The algorithm logically structures the multiprocessor as a binary tree in which the processing nodes are at the leaves and all the children of a tree node are considered a partition.
Reference: [9] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Coscheduling based on run-time identification of activity working sets. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 23(2) </volume> <pages> 135-160, </pages> <month> April </month> <year> 1995. </year> <month> 92 </month>
Reference-contexts: If process control as it 21 is described in [24] were used as the only means of timesharing a multiprocessor, we would expect that such applications would show poor performance when the job load was high. 2.4 Runtime Activity Working Set Identification Feitelson and Rudolph describe in <ref> [9] </ref> an algorithm called "runtime activity working set identification" for scheduling parallel programs on a timeshared multiprocessor (we shall call this algorithm RAWSI, for brevity's sake).
Reference: [10] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operat-ing system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year> <note> Available from http://xenon.stanford.edu/~tucker/papers/sigmetrics.ps. </note>
Reference-contexts: The extra context switches result from attempts to synchronize with descheduled processes resulting in blocking. As Gupta et al. have shown in <ref> [10] </ref>, the use of non-blocking (spinning) synchronization primitives will result in even worse performance under moderate multiprogrammed loads, because, while the extra context switches are avoided, the spinning time is large. <p> Relatively good performance has been reported for competent implementations of traditional coscheduling. Gupta et al. report in <ref> [10] </ref> that when coscheduling was used with 25-millisecond timeslices on a simulated system, it achieved 71% utilization, as compared to 74% for batch scheduling (poorer performance is reported with 10-millisecond timeslices). <p> Several published works <ref> [4, 10, 23] </ref> cite good performance for process control, but these works also find that coscheduling can be modified to have equivalently good performance. <p> This implies that in fact the jobs in question show superlinear speedup. In fact this is true in two examples in published works on process control. In <ref> [10] </ref>, the LU application is found to perform very poorly under process control when run on three processors, and the authors point out that a drastically increased cache miss rate is to blame. <p> Message receipt was by the popular spin-block mechanism described in [18], <ref> [10] </ref>, and others. Dusseau et al. found that performance with fixed-spin-time spin-block messaging was quite good under the Solaris 2.4 scheduler when fixed spin times on the order of a context switch were used. <p> interested in the effects of varying the granularity of communication, and of using the Unix nice () command to boost base priorities of parallel programs under the unmodified Solaris 2.4 scheduler. 66 Each of these issues is examined below. 5.4.1 Spin-block message receipt under Solaris 2.4 Prior work Other researchers <ref> [10, 23] </ref> have reported improved response times and efficiency with spin-block synchronization with no active coscheduling mechanism, and the reasons for this are clear: with spin-block synchronization, CPU time that would otherwise be wasted in spinning can be used by other processes present on the node.
Reference: [11] <author> D. P. Helmbold and C. E. McDowell. </author> <title> Modeling speedup(n) greater than n. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 250-256, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Helmbold and McDowell have documented this sort of "superunitary speedup due to increasing cache size" in <ref> [11] </ref>. Because of this property of certain parallel applications, their ideal "operating point" is larger than one | possibly considerably larger than one. Thus forcing them to run on fewer processors will be very inefficient.
Reference: [12] <institution> IEEE Computer Society TCCA and ACM SIGARCH. Chicago, Illinois, </institution> <month> April 18-21, </month> <year> 1994. </year> <title> Computer Architecture News, </title> <type> 22(2), </type> <month> April </month> <year> 1994. </year>
Reference: [13] <institution> Sun Microsystems Inc. </institution> <note> ts dptbl(4) manual page. SunOS 5.4 Manual. Section 4. </note>
Reference-contexts: processes using spin-block message receipt under Solaris 2.4, is first proposed and analyzed in our work. 3 We had also believed that this was the behavior of the Solaris 2.4 scheduler, because, in additions to the description in [6], the Solaris 2.4 time-sharing dispatcher parameter table manual page (ts dptbl) <ref> [13] </ref> describes it this way; but a series of experiments we undertook to show that such behavior would allow a user-mode program to receive an unfairly large proportion of CPU time failed to demonstrate that the priority boosting happened invariably, and a reading of the sources showed how the mechanism actually
Reference: [14] <author> Anna R. Karlin, Kai Li, Mark S. Manasse, and Susan Owicki. </author> <title> Empirical studies of competitive spinning for a shared-memory multiprocessor. </title> <booktitle> In 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 41-55, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Ousterhout claims in [18] that a two-context-switch fixed spin time is competitive (he calls the spin time the pause in his description of two-phase waiting). The competitive arguments presented in <ref> [14] </ref> can be used to show that this spin time is indeed competitive, with a competitive ratio of at worst 3 + M=C times the optimal spin time, for M the message round-trip time and C the context-switch time. <p> This worst-case performance is of course worse than one could do with a spin time equal to the context-switch time. However, as Karlin et al. note in <ref> [14] </ref>, the competitive ratio says nothing about the mean cost of spinning, which we found to be higher with a spin time of approximately the context switch time than with the 1600 sec time we picked.
Reference: [15] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture [12], </booktitle> <pages> pages 302-313. </pages> <booktitle> Computer Architecture News, </booktitle> <volume> 22(2), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: In a cache-coherence scheme such as the software schemes presented by Chaiken et al. in [3], cache line invalidations can be treated in the same fashion as arriving messages. We can do even better on systems with network interface processors, such as FLASH <ref> [15] </ref> or Typhoon [20]. In these systems, some of the scheduler state can be cached in the interface processor, so that the scheduling decision can be made without consulting the computation processor. The computation processor could be interrupted only when a preemption was needed. <p> In the case of a shared-memory implementation, for example, if the platform allows reads to be effected regardless of which process is currently scheduled on the node (this is possible in, e.g., the FLASH multiprocessor <ref> [15] </ref>), it might not be necessary or desirable to treat every memory read as a demand for coscheduling; but cache line invalidations might need to be treated as demanding coscheduling. An implementation would allow experimentation with different schemes.
Reference: [16] <author> Mario Lauria. </author> <title> Mpi-fm: A high performance cluster implementation the message passing interface on fast messages. </title> <type> Master's thesis, </type> <institution> University of Illinois, Department of Computer Science, 1304 W. Springfield Avenue, Urbana, Illinois., </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: we used in the barrier test were all processes that ran in a simple spin loop for the duration of the test. 5.2.3 Mixed workload test We also ran an MPI FORTRAN application kernel, a two-dimensional Laplace equation solver using a successive over-relaxation technique, using an FM implementation of MPI <ref> [16] </ref>. Because a rectangular grid of nodes was required, we used only six nodes in the cluster for this test. Each of six nodes in the workstation cluster ran a workload consisting of the applications shown in Table 5.1.
Reference: [17] <author> S. J. Le*er, M. K. McKusick, M. J. Karels, and J. S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: It is indeed remarkable that the priority-boosting mechanism that has been present in Unix at least since 4.3BSD and which was originally intended to enhance responsiveness for serial interactive processes (as described in <ref> [17] </ref>) has the effect of coscheduling communicating processes on separate workstations. Passive coscheduling We conjectured that a particular behavior, which we called passive coscheduling, might arise in systems where spin-block message receipt was used without any active coscheduling mechanism.
Reference: [18] <author> John K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: Because demand-based coscheduling uses more information than does Ousterhout's form of coscheduling <ref> [18] </ref>, it can reduce the difficulty of the scheduling problem and exploit opportunities for coscheduling that traditional coscheduling cannot. Because it does not rely on a particular programming technique, such as task-queue-based multithreading, demand-based coscheduling is applicable in domains where process control [24] is not. <p> In an example where processes synchronized about every 160sec on a NUMA multiprocessor with 4-MIPS processing nodes, applications took roughly twice as long to execute under uncoordinated scheduling as they did under gang scheduling. In general, the results cited above agree with the claims advanced by Ouster-hout in <ref> [18] </ref>: under independent timesharing, multiprogrammed parallel job loads will suffer large numbers of context switches, with attendant overhead due to cache and TLB reloads. The extra context switches result from attempts to synchronize with descheduled processes resulting in blocking. <p> one has a large number of jobs to run on a small number of processors. 13 may expect that more heavily loaded systems will suffer even higher synchronization blocking rates under independent timesharing, and commensurately higher context switching overhead. 1.3 Goals Ousterhout compared parallel scheduling and virtual memory systems in <ref> [18] </ref>. He suggested that coscheduling is necessary on timeshared multiprocessors running parallel jobs in order to avoid a kind of process thrashing that is analogous to virtual memory thrashing. <p> This is as distinct from traditional coscheduling <ref> [18] </ref>, in which there is no clear means for scheduling jobs with more processes than there are nodes on the multiprocessor. Finally, we want an approach that is dynamic, and can adapt to changing conditions of load and communication between processes. <p> We now review some of the other work in the field. 2.1 Traditional Coscheduling Ousterhout's pioneering scheduler is described in <ref> [18] </ref>. Under this traditional form of coscheduling, the processes constituting a parallel job are scheduled simultaneously across as many of the nodes of a multiprocessor as they require. <p> Message receipt was by the popular spin-block mechanism described in <ref> [18] </ref>, [10], and others. Dusseau et al. found that performance with fixed-spin-time spin-block messaging was quite good under the Solaris 2.4 scheduler when fixed spin times on the order of a context switch were used. <p> Ousterhout suggested spin-block message receipt in <ref> [18] </ref> for cases where the workload included jobs with very coarse-grain computation, because alternates might be able to perform additional computation before blocking for message receipt. 23 that the priority-boosting happens whenever a process is returned from a sleep queue to a run queue. <p> It is also to be noted that 1600 sec is slightly greater than twice the mean context-switch time plus the message round-trip time. Ousterhout claims in <ref> [18] </ref> that a two-context-switch fixed spin time is competitive (he calls the spin time the pause in his description of two-phase waiting). <p> This is the processor-thrashing behavior that Ousterhout originally described in <ref> [18] </ref>, and which we have reproduced by disabling priority boosts on process wakeup in our experiment of Section 5.4.1, the results of which are shown in Figure 5-18.
Reference: [19] <author> Scott Pakin, Mario Lauria, and Andrew Chien. </author> <title> High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/ myrinet-fm-sc95.ps. </note>
Reference-contexts: that a real workstation cluster would perform well under dynamic coscheduling. 32 Chapter 4 An Implementation of Dynamic Coscheduling We describe in this chapter the version of dynamic coscheduling (DCS) we implemented to run with Illinois Fast Messages, a user-level messaging layer developed at the University of Illinois at Urbana-Champaign <ref> [19] </ref>. 4.1 Experimental Platform: Illinois Fast Messages and Myrinet Illinois Fast Messages is a high-performance messaging layer that uses a user-level library to provide messaging primitives without the overhead of domain crossing that would be required by a kernel-resident device driver.
Reference: [20] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture [12], </booktitle> <pages> pages 325-336. </pages> <booktitle> Computer Architecture News, </booktitle> <volume> 22(2), </volume> <month> April </month> <year> 1994. </year> <month> 93 </month>
Reference-contexts: In a cache-coherence scheme such as the software schemes presented by Chaiken et al. in [3], cache line invalidations can be treated in the same fashion as arriving messages. We can do even better on systems with network interface processors, such as FLASH [15] or Typhoon <ref> [20] </ref>. In these systems, some of the scheduler state can be cached in the interface processor, so that the scheduling decision can be made without consulting the computation processor. The computation processor could be interrupted only when a preemption was needed.
Reference: [21] <author> Patrick G. Sobalvarro. </author> <title> Adaptive gang-scheduling for distributed-memory mul-tiprocessors. </title> <type> Technical Report MIT-LCS-TR-622, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> Available from http://www.nchpc.lcs.mit.edu/ Workshop94/Papers/s4 p1.ps. </note>
Reference-contexts: While demand-based co-scheduling was developed independently from RAWSI, 1 the two have significant similarities: in both approaches, runtime mechanisms are used to identify communicating processes so that they can be coscheduled. However, RAWSI differs significantly from both dynamic coscheduling (described in Chapter 3 and <ref> [21] </ref>) and predictive cosche-duling (described in Appendix A), the two approaches to demand-based coscheduling we present here. One major difference between RAWSI in a message-passing system and dynamic coscheduling is that RAWSI does not make the decision of what process to schedule immediately upon receiving a message. <p> Also, we believe that use of virtual memory system structures as described in Appendix A will be less expensive than the execution of instruction sequences to record accesses to shared data structures. 1 A description of dynamic coscheduling was first published in <ref> [21] </ref>. 22 2.5 Implicit Scheduling Implicit scheduling is the name given by Dusseau et al.[6] to their algorithms for adaptively modifying the spin times in spin-block message receipt to achieve good performance on "bulk-synchronous" applications (those which perform regular barriers, possibly with other communication taking place in between barriers). 2 This <p> 2.4, it is always placed at the end of the run queue (even if it has received a priority boost), and so execution order is varied even with a single run queue. 24 Chapter 3 Dynamic Coscheduling In this chapter we describe dynamic coscheduling, the demand-based coscheduling algorithm we implemented <ref> [21, 22] </ref>. Dynamic coscheduling is well suited to implementation on message-passing processors, because it uses the arrival of a message as a means of signalling the scheduler that the process to which the message is addressed should be scheduled immediately.
Reference: [22] <author> Patrick G. Sobalvarro and William E. Weihl. </author> <title> Demand-based coscheduling of parallel jobs on multiprogrammed multiprocessors. </title> <booktitle> In Proceedings of the Parallel Job Scheduling Workshop at IPPS '95, </booktitle> <year> 1995. </year> <note> Available from http://www.psg.lcs.mit.edu/~pgs/papers/jsw-for-springer.ps. Also appears in Springer-Verlag Lecture Notes in Computer Science, Vol. 949. </note>
Reference-contexts: some ambiguity in [6] about whether the term "implicit scheduling" is also intended to cover all approaches for achieving coordinated scheduling in a network of workstations by making local decisions based on information about communication; but because so broad a description would also cover demand-based coscheduling, described here and in <ref> [22] </ref>, we will use the term "implicit scheduling" to describe only the combination of spin-block message receipt with the algorithm for adaptively determining spin times described in [6]. 3 Given the variable amount of computation that was performed in each cycle of these processes, it seems possible that the use of <p> 2.4, it is always placed at the end of the run queue (even if it has received a priority boost), and so execution order is varied even with a single run queue. 24 Chapter 3 Dynamic Coscheduling In this chapter we describe dynamic coscheduling, the demand-based coscheduling algorithm we implemented <ref> [21, 22] </ref>. Dynamic coscheduling is well suited to implementation on message-passing processors, because it uses the arrival of a message as a means of signalling the scheduler that the process to which the message is addressed should be scheduled immediately. <p> However, for the case of spin-block message receipt with fine-grain message-passing and short spin times, where it is quite unlikely that the process has been preempted while runnable but spinning, this mechanism as described does indeed implement the most significant part of dynamic coscheduling. 2 2 Although our earlier paper <ref> [22] </ref> on dynamic coscheduling is briefly cited in [6], Dusseau et al. apparently failed to notice that achieving coscheduling by scheduling a process when a message 67 Quantum Prio. (ms) ts slpret lowest priority 0- 9 200 50 10-19 160 51 30-34 80 53 40-44 40 55 50-54 40 57 highest
Reference: [23] <author> Andrew Tucker. </author> <title> Efficient scheduling on multiprogrammed shared-memory multiprocessors. </title> <type> Technical Report CSL-TR-94-601, </type> <institution> Stanford University Department of Computer Science, </institution> <month> November </month> <year> 1993. </year> <note> Available from http://elib.stanford.edu/ Dienst/UI/2.0/Describe/stanford.cs/CSL-TR94-601. </note>
Reference-contexts: Later work has shown that the context switches themselves can be expensive due to the cache reloads they entail; also the overhead of spinning while awaiting a message can consume a large amount of CPU time <ref> [23] </ref>. <p> Several published works <ref> [4, 10, 23] </ref> cite good performance for process control, but these works also find that coscheduling can be modified to have equivalently good performance. <p> FM did not include any means of implementing spin-block message receipt; a strict polling model was assumed. As a result, spinning message receipt was used for earlier experiments. This had two negative effects. The first was that, as other researchers have reported <ref> [23] </ref>, spinning message receipt results in very poor performance for fine-grain programs in a timeshared environment. Thus the Solaris 2.4 scheduler with spinning message receipt served as implausible competition for DCS. The second was that DCS itself was conceived and modeled for use with spin-block or blocking message receipt. <p> interested in the effects of varying the granularity of communication, and of using the Unix nice () command to boost base priorities of parallel programs under the unmodified Solaris 2.4 scheduler. 66 Each of these issues is examined below. 5.4.1 Spin-block message receipt under Solaris 2.4 Prior work Other researchers <ref> [10, 23] </ref> have reported improved response times and efficiency with spin-block synchronization with no active coscheduling mechanism, and the reasons for this are clear: with spin-block synchronization, CPU time that would otherwise be wasted in spinning can be used by other processes present on the node.
Reference: [24] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM SIGOPS Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-186, </pages> <year> 1989. </year> <note> Available from http://xenon.stanford.edu/~tucker/papers/sosp.ps. 94 </note>
Reference-contexts: Because it does not rely on a particular programming technique, such as task-queue-based multithreading, demand-based coscheduling is applicable in domains where process control <ref> [24] </ref> is not. Demand-based coscheduling is intended for scheduling timeshared loads of parallel jobs or mixed loads of parallel and serial jobs. <p> For example, while demand-based coscheduling could be compatible with a task-queue-based multithreaded approach like process control <ref> [24] </ref>, we do not want to require that all parallel applications be coded in a multithreaded fashion in order not to suffer excessive context-switching. <p> However, distributed hierarchical control was not designed for smaller machines, such as the desktop machines and departmental servers we have described, on which we expect that it would suffer from the same problems as traditional coscheduling. 19 2.3 Process Control Tucker and Gupta suggested in <ref> [24] </ref> a strategy called process control , which has some of the characteristics of space partitioning and some of the characteristics of timesharing. Under process control, parallel jobs must be written as multithreaded applications keeping their threads in a task queue. <p> This pushing at the boundaries of available memory will probably mean that many commercial applications will show superlinear speedup. If process control as it 21 is described in <ref> [24] </ref> were used as the only means of timesharing a multiprocessor, we would expect that such applications would show poor performance when the job load was high. 2.4 Runtime Activity Working Set Identification Feitelson and Rudolph describe in [9] an algorithm called "runtime activity working set identification" for scheduling parallel programs
References-found: 24

