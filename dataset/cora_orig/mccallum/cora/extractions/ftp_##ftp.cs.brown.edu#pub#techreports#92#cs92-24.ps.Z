URL: ftp://ftp.cs.brown.edu/pub/techreports/92/cs92-24.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-92-24.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Douglas E. Appelt. </author> <title> A theory of abduction based on model preference. </title> <booktitle> In Proceedings of the AAAI Symposium on Abduction, </booktitle> <year> 1990. </year>
Reference-contexts: The final choice for best explanation will be the one with least cost. The main difficulty of this approach, however, is the lack of any clear semantics 6 for the cost assignments. Appelt <ref> [1] </ref> attempted to provide semantics, but it was found to be incomplete and inextensible. Furthermore, it failed to give an intuitive feel for what the numbers really mean. 2.2 Cost-Based Abduction Charniak and Shimony presented a minor variant of weighted abduction called cost-based abduction [7].
Reference: [2] <author> Colin Bell, Anil Nerode, Raymond T. Ng, and V. S. Subrahmanian. </author> <title> Implementing deductive databases by linear programming. </title> <type> Technical Report CS-TR-2747, </type> <institution> University of Maryland, </institution> <year> 1991. </year>
Reference-contexts: In particular, we have in mind the following: * Solving Constraint Satisfaction Problems (CPS's) such as the n-queens problems through Diophantine equations [51]. * Solving the relaxation labeling process through a Simplex-like algorithm [76]. * Updating deductive databases though linear programming techniques <ref> [2] </ref>. * Reduction of independence-based MAPs in Bayesian networks to linear con straint satisfaction [69]. * The problem of path planning in robotics in relation to routing and trans portation problems [49]. * Work done with distributed intelligent agents [37]. 13 3 Cost-Based Abduction Our basic approach towards knowledge representation involves
Reference: [3] <author> Eugene Charniak and Robert Goldman. </author> <title> A logic for semantic interpretation. </title> <booktitle> In Proceedings of the AAAI Conference, </booktitle> <year> 1988. </year>
Reference-contexts: Since then, many common problems have been identified in its terms. For example, such problems include medical diagnosis [8, 42], circuit fault detection [12, 11, 20] and story understanding <ref> [22, 23, 3] </ref>. 3 Clearly, we extensively use abductive reasoning in our everyday tasks from explaining why the ground is wet to performing sophisticated inferencing in medical diagnosis. Thus, we need an approach to modeling abduction which is representa-tionally robust and permits a practical implementation. <p> The waodags were again generated by wimp, a natural language story comprehension system for parsing and understanding written English sentences <ref> [3, 22, 23] </ref>. We generated initial solutions for each of the waodags by using a quick (unfortunately, non-admissible) heuristic outlined in [4]. <p> Proposition 3.18 and Theorem 3.19 together demonstrate that in a monotonic waodag, "simpler" explanations are preferred due to the lower associated costs. The assumption of monotonicity is reasonable in many cases as pointed out by [7] and characterized in <ref> [3] </ref>. Thus, we are only interested in the "simplest" explanations. Our goal is to generate these explanations in order of cost without having to consider the remaining exponential number of explanations. <p> Also, since none of the hypotheses are used, no cost is incurred which can make this explanation, the best explanation. A more sophisticated example involving cyclicity often occurs in the rule bases of the wimp story understanding system <ref> [3, 23, 22] </ref>. In its knowledge base, you can find the logical rules: (foo a) ^ (= a b ) =) (foo b) A method is available in wimp to eliminate this logical cyclicity. However, it is rather ad hoc.
Reference: [4] <author> Eugene Charniak and Saadia Husain. </author> <title> A new admissible heuristic for minimal-cost proofs. </title> <booktitle> In Proceedings of the AAAI Conference, </booktitle> <year> 1991. </year>
Reference-contexts: In cost-based abduction, hypotheses have associated costs, and the cost of a proof is simply the sum of the costs of the hypotheses required to complete that proof. (Examples of such proofs can be found in <ref> [7, 4] </ref>.) Central to this approach is the use of directed acyclic graphs called waodags (or, weighted and/or directed acyclic graphs) [7, 4] to represent relationships between hypotheses and the evidence to be explained. <p> and the cost of a proof is simply the sum of the costs of the hypotheses required to complete that proof. (Examples of such proofs can be found in <ref> [7, 4] </ref>.) Central to this approach is the use of directed acyclic graphs called waodags (or, weighted and/or directed acyclic graphs) [7, 4] to represent relationships between hypotheses and the evidence to be explained. Each node represents some piece of knowledge, and the connections explicitly detail the relationships between different pieces. Furthermore, each node in a waodag corresponds to a logical and or or operation on its immediate parents. <p> The assignment of no-one-home to true and bad-songs, blackout and no-shows to false results in lights-out, radio-off, tv-off, house-dark and house-quiet to be true. This proof has a cost of 7 and is the minimal cost proof. 8 proof <ref> [4] </ref>. (We will continue with a more detailed discussion of cost-based abduc-tion in Section 3.) 2.3 Belief Revision Pearl presented an approach to modeling belief revision by using Bayesian networks [41]. <p> However, efficient admissible heuristics seem to be difficult to find. (In Section 3.4, we compare the computationally efficiency of our approach against a search heuristic designed for use on waodags generated by the WIMP story understanding system <ref> [4] </ref>.) 3.2 Constraint System Formulation Basically, cost-based abduction is ultimately an optimization problem. So, instead of treating it as a traditional graph search problem, we consider the problem in terms of constraint satisfaction. <p> The first involves a real application of our technique to solve cost-based abduction problems created by the story understanding system wimp [22, 23]. This allows us to make a comparison against the search-style heuristic described in <ref> [4] </ref> to solve these graphs. Our second experiment involves 9 Techniques V1 and V2 seem to work best when values can propagate in a top-down fashion. <p> It utilizes belief networks to perform the necessary abductive inference tasks necessary to solve problems like pronoun reference and word-sense disambiguation. The belief networks can then be transformed into equivalent cost-based abductions problems as shown in [7]. The algorithm for determining the minimal cost proof in <ref> [4] </ref> is based on a best-first search of the waodag. The basic idea is that one starts with the partial proof consisting only of the evidence nodes in the waodag and then creating alternative partial proofs. 10 In each iteration, a partial proof is chosen to be expanded. <p> One of the few basic admissible heuristics that has been used is cost-so-far in [7, 73]. Simply put, the partial proofs are weighted according to the costs of the hypotheses which they currently contain. A much more efficient heuristic has been found which utilizes a more sophisticated cost estimator <ref> [4] </ref>. The difficulty in the cost-so-far approach is that no estimation on the "goodness" of the partial 10 A partial proof is a subgraph of the waodag. 31 proof can be made until hypothesis nodes have been reached. <p> The admissibility of this heuristic is guaranteed by the special care the heuristic takes in expanding partial proofs. (For precise details and the admissibility proof of this heuristic, see <ref> [4] </ref>.) Comparing our linear constraint satisfaction approach to the search heuristic is not an obvious task. The method and intuitions behind solving linear programming problems is radically different from those involved in best-first search. Our only course of action seems to be in making a direct CPU timing comparison. <p> The waodags were again generated by wimp, a natural language story comprehension system for parsing and understanding written English sentences [3, 22, 23]. We generated initial solutions for each of the waodags by using a quick (unfortunately, non-admissible) heuristic outlined in <ref> [4] </ref>. The basic idea is that one starts with the partial proof consisting only of the evidence nodes in the waodag and then creating alternative partial proofs. 16 In each iteration, a partial proof is chosen to be expanded.
Reference: [5] <author> Eugene Charniak and Drew McDermott. </author> <title> Introduction to Artificial Intelligence. </title> <publisher> Addison Wesley, </publisher> <year> 1985. </year>
Reference-contexts: Formally called abduction, it was not widely considered as a form of 2 See [41] for discussion on the limitations of these approaches. 1 reasoning by the AI community until its popular introduction by Charniak and McDermott in <ref> [5] </ref>. Since then, many common problems have been identified in its terms.
Reference: [6] <author> Eugene Charniak and Eugene Santos, Jr. </author> <title> Dynamic map calculations for abduction. </title> <booktitle> In Proceedings of the AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: Especially in the case of cost-based abduction, thorough experimentation has shown that our approach has now made a computationally difficult problem extremely feasible for extensive use in existing applications such as the wimp story comprehension system <ref> [6] </ref>. Furthermore, we consider some issues which remain unaddressed by the existing models. Mainly, this is due to the additional complexities imposed by these issues, thus making an already difficult problem impossible given their approach. <p> Furthermore, we found that various optimizations such as incorporating an initial solution significantly improved our performance without increasing the complexity of our formulation. As a result, this now provided wimp a practical means of performing abductive reasoning, a facility it was lacking earlier <ref> [6] </ref>. We further demonstrated that our approach could naturally generate the alternative explanations. This can be important in domains like medical diagnosis where the availability of alternatives is critical. Next, we extended our approach and modelled Bayesian networks.
Reference: [7] <author> Eugene Charniak and Solomon E. Shimony. </author> <title> Probabilistic semantics for cost based abduction. </title> <booktitle> In Proceedings of the AAAI Conference, </booktitle> <year> 1990. </year>
Reference-contexts: Thus, we need an approach to modeling abduction which is representa-tionally robust and permits a practical implementation. To our chagrin, however, it seems that abductive reasoning is an inherently difficult process. Indeed, various abductive models have been shown to be NP-hard <ref> [9, 41, 7, 64, 42] </ref>. To better understand the difficulty inherent in abduction, let us attempt to model John's situation above. <p> For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available. <p> This would serve to precisely define the notion of a best explanation as well as subsequent next best which is critical to have in domains such as medical diagnosis. Several ordering measures are available such as least specific abduction [26, 73], cost-based abduction <ref> [7] </ref>, parsimonious covering theory [43] and belief revision [41]. Each approach offers different perspectives on the problem and provides individual frameworks capable of modeling certain aspects of abductive reasoning. <p> Appelt [1] attempted to provide semantics, but it was found to be incomplete and inextensible. Furthermore, it failed to give an intuitive feel for what the numbers really mean. 2.2 Cost-Based Abduction Charniak and Shimony presented a minor variant of weighted abduction called cost-based abduction <ref> [7] </ref>. It has been shown in [7] that belief revision in Bayesian networks [41] can be accurately modeled by cost-based abduction. <p> Furthermore, it failed to give an intuitive feel for what the numbers really mean. 2.2 Cost-Based Abduction Charniak and Shimony presented a minor variant of weighted abduction called cost-based abduction <ref> [7] </ref>. It has been shown in [7] that belief revision in Bayesian networks [41] can be accurately modeled by cost-based abduction. <p> In cost-based abduction, hypotheses have associated costs, and the cost of a proof is simply the sum of the costs of the hypotheses required to complete that proof. (Examples of such proofs can be found in <ref> [7, 4] </ref>.) Central to this approach is the use of directed acyclic graphs called waodags (or, weighted and/or directed acyclic graphs) [7, 4] to represent relationships between hypotheses and the evidence to be explained. <p> and the cost of a proof is simply the sum of the costs of the hypotheses required to complete that proof. (Examples of such proofs can be found in <ref> [7, 4] </ref>.) Central to this approach is the use of directed acyclic graphs called waodags (or, weighted and/or directed acyclic graphs) [7, 4] to represent relationships between hypotheses and the evidence to be explained. Each node represents some piece of knowledge, and the connections explicitly detail the relationships between different pieces. Furthermore, each node in a waodag corresponds to a logical and or or operation on its immediate parents. <p> Furthermore, each hypothesis used in a proof will incur a cost. Consequently, each such proof will have an associated cost which is simply the sum of the hypothesis costs incurred. The goal is to find an assignment which has minimal cost (see Figure 2.1). Charniak and Shimony <ref> [7] </ref> also showed that by interpreting the costs as negative log probabilities, cost-based abduction can be reduced to belief revision in Bayesian networks. Thus, the cost semantics problem of weighted abduction is not encountered. Unfortunately, finding minimal cost proofs has been shown to be NP-hard [7]. <p> Charniak and Shimony <ref> [7] </ref> also showed that by interpreting the costs as negative log probabilities, cost-based abduction can be reduced to belief revision in Bayesian networks. Thus, the cost semantics problem of weighted abduction is not encountered. Unfortunately, finding minimal cost proofs has been shown to be NP-hard [7]. Current approaches to finding the best proof have centered around using a best-first search technique and expanding partial proofs to search for the best 7 Fig. 2.1. A simple waodag. The and-node house-dark-quiet is the ob-servation. <p> Our goal in this section is to model cost-based abduction using this approach. This and the subsequent section which models Bayesian networks should demonstrate the representational capabilities of our linear constraints formulation. 3.1 waodags The keystone of cost-based abduction <ref> [7] </ref> is the weighted and/ or directed acyclic graph (abbreviated waodag) which models the relationships between objects and/or concepts in the world. Each node in the graph embodies some object or concept while each edge represents direct causal/logical relationships between nodes incident to the edge. <p> In Figure 3.1, if fhouse-quiet = trueg is the observation to be explained, one 4 Slight generalization of Charniak and Shimony <ref> [7] </ref>. 16 possible proof would be the following assignment of truth values: fhouse-quiet, radio-off, bad-songs, tv-off, no-showsg are assigned true and fno-one-homeg is assigned false. A second possibility is to assign all of them to true. <p> Using brute force enumeration techniques is fine for small problems but grows exponentially in complexity. Charniak and Shimony <ref> [7] </ref> have proven that this problem is NP-complete by transforming it into the vertex cover problem [17]. Straightforwardly, this problem can be transformed into a search problem on and/or graphs. <p> It utilizes belief networks to perform the necessary abductive inference tasks necessary to solve problems like pronoun reference and word-sense disambiguation. The belief networks can then be transformed into equivalent cost-based abductions problems as shown in <ref> [7] </ref>. The algorithm for determining the minimal cost proof in [4] is based on a best-first search of the waodag. <p> This continues until all the goals (such as evidence) are satisfied and results in a minimal cost proof. How this is actually done is outlined in <ref> [7] </ref>. Naturally, the success of this algorithm depends on having a good heuristic function for deciding which partial proof should be worked on next. Furthermore, the heuristic function must be admissible to guarantee that the first proof generated is the minimal cost proof. Efficient heuristics have been difficult to find. <p> Furthermore, the heuristic function must be admissible to guarantee that the first proof generated is the minimal cost proof. Efficient heuristics have been difficult to find. One of the few basic admissible heuristics that has been used is cost-so-far in <ref> [7, 73] </ref>. Simply put, the partial proofs are weighted according to the costs of the hypotheses which they currently contain. A much more efficient heuristic has been found which utilizes a more sophisticated cost estimator [4]. <p> This set-covering approach can be straightforwardly modeled using cost-based abduction where the rigid graphical structures play a key role in defining the solution spaces of the associated linear programs. We also point out that in the original formulation of cost-based abduction presented in <ref> [7] </ref>, the proof of NP-completeness was accomplished by transforming the vertex covering problem into cost-based abduction, thus strongly suggesting a close relationship between cost-based abduction and set covering. 3.5 Domain-Dependent Optimization Since the success of branch and bound techniques depends on the ability to prune the active nodes early and as <p> Proposition 3.18 and Theorem 3.19 together demonstrate that in a monotonic waodag, "simpler" explanations are preferred due to the lower associated costs. The assumption of monotonicity is reasonable in many cases as pointed out by <ref> [7] </ref> and characterized in [3]. Thus, we are only interested in the "simplest" explanations. Our goal is to generate these explanations in order of cost without having to consider the remaining exponential number of explanations. <p> In probabilistic terms, this seems to suggest that random variables which are "irrelevant" to the evidence are not instantiated. This approach of only instantiating a subset of the random variables is currently being explored by [41] and <ref> [7, 67, 66] </ref>.
Reference: [8] <author> Gregory F. Cooper. NESTOR: </author> <title> A Computer-based Medical Diagnostic Aid That Integrates Causal and Probabilistic Knowledge. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> 1984. </year>
Reference-contexts: Since then, many common problems have been identified in its terms. For example, such problems include medical diagnosis <ref> [8, 42] </ref>, circuit fault detection [12, 11, 20] and story understanding [22, 23, 3]. 3 Clearly, we extensively use abductive reasoning in our everyday tasks from explaining why the ground is wet to performing sophisticated inferencing in medical diagnosis.
Reference: [9] <author> Gregory F. Cooper. </author> <title> Probabilistic inference using belief networks is np-hard. </title> <type> Technical Report KSL-87-27, </type> <institution> Medical Computer Science Group, Stanford University, </institution> <year> 1987. </year>
Reference-contexts: Thus, we need an approach to modeling abduction which is representa-tionally robust and permits a practical implementation. To our chagrin, however, it seems that abductive reasoning is an inherently difficult process. Indeed, various abductive models have been shown to be NP-hard <ref> [9, 41, 7, 64, 42] </ref>. To better understand the difficulty inherent in abduction, let us attempt to model John's situation above.
Reference: [10] <author> Steve B. Cousins, William Chen, and Mark E. Frisse. Caben: </author> <title> A collection of algorithms for belief networks. </title> <type> Technical Report WUCS-91-25, </type> <institution> Department of Computer Science, Washington University, </institution> <address> St. Louis, Mo., </address> <year> 1991. </year>
Reference-contexts: Through various tests, we have noticed that our method performs best on highly deterministic networks, that is, networks whose conditional probabilities are near 0 or 1. For belief updating, we compared our approach against stochastic simulation methods <ref> [72, 34, 28, 10, 24, 62] </ref>. We found that our approach performed much better than stochastic simulation on highly deterministic networks. Basically, as we discussed earlier, simulation approaches could very rarely come up with an answer on these types of networks.
Reference: [11] <author> R. Davis. </author> <title> Diagnostic reasoning based on structure and behavior. </title> <journal> Artificial Intelligence, </journal> <volume> 24 </volume> <pages> 347-410, </pages> <year> 1984. </year>
Reference-contexts: Since then, many common problems have been identified in its terms. For example, such problems include medical diagnosis [8, 42], circuit fault detection <ref> [12, 11, 20] </ref> and story understanding [22, 23, 3]. 3 Clearly, we extensively use abductive reasoning in our everyday tasks from explaining why the ground is wet to performing sophisticated inferencing in medical diagnosis.
Reference: [12] <author> J. de Kleer and B. C. Williams. </author> <title> Diagnosing multiple faults. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 97-130, </pages> <year> 1987. </year>
Reference-contexts: Since then, many common problems have been identified in its terms. For example, such problems include medical diagnosis [8, 42], circuit fault detection <ref> [12, 11, 20] </ref> and story understanding [22, 23, 3]. 3 Clearly, we extensively use abductive reasoning in our everyday tasks from explaining why the ground is wet to performing sophisticated inferencing in medical diagnosis. <p> Other approaches are certainly available. However, aside from the five methods we have just studied, the remaining ones handle abductive reasoning sort of as an afterthought to their main goals. Such systems include truth maintenance systems <ref> [12] </ref>, influence diagrams [61], probabilistic logic [40], Dempster-Shafer theory [13, 63] and fuzzy logic [75]. 12 2.7 Related Work There are also various strands of work which, while somewhat related to the work described in this thesis, are nevertheless sufficiently distant not to warrant a full fledged review.
Reference: [13] <author> A. P. Dempster. </author> <title> A generalization of bayesian inference. </title> <journal> J. Royal Statistical Society, </journal> <volume> 30 </volume> <pages> 205-47, </pages> <year> 1968. </year> <month> 114 </month>
Reference-contexts: Other approaches are certainly available. However, aside from the five methods we have just studied, the remaining ones handle abductive reasoning sort of as an afterthought to their main goals. Such systems include truth maintenance systems [12], influence diagrams [61], probabilistic logic [40], Dempster-Shafer theory <ref> [13, 63] </ref> and fuzzy logic [75]. 12 2.7 Related Work There are also various strands of work which, while somewhat related to the work described in this thesis, are nevertheless sufficiently distant not to warrant a full fledged review.
Reference: [14] <author> R. O. Duda, P. E. Hart, and N. J. Nilsson. </author> <title> Subjective bayesian methods for rule-based inference systems. </title> <booktitle> In Proceedings of the National Computer Conference, </booktitle> <year> 1976. </year>
Reference-contexts: Common approaches to this problem have advocated augmenting propositional logic with certainty factors, probabilities, costs, etc. in an attempt to preserve deduction. This was often used in classical expert systems such as MYCIN [70, 71], PROSPECTOR <ref> [14] </ref> and INTERNIST [38, 46]. However, the resulting models were clumsy and restrictive. A case in point is Shortliffe's MYCIN system.
Reference: [15] <author> J. A. Feldman and D. H. Ballard. </author> <title> Connectionist models and their properties. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 51-74, </pages> <year> 1985. </year>
Reference-contexts: Thagard continues by refining the four possibilities into seven distinct principles. In this way, it is hoped that further gradations can be made in "best". Although a seemingly sound theory, it is rather complex. A connectionist <ref> [52, 15, 35] </ref> implementation has been attempted, however, its feasibility in applications seems questionable. 2.6 Other Approaches The above methods are directed mainly towards modeling abduction. Other approaches are certainly available.
Reference: [16] <author> Watson Fulks. </author> <title> Advanced Calculus. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1978. </year>
Reference-contexts: From the computations above, we can make the following observation: Since multiplication of probabilities is a monotonically decreasing function <ref> [16] </ref> and is strictly decreasing when probabilities are less than 1, it is expected that the more probabilistic information we have, the smaller the joint probabilities will become.
Reference: [17] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Using brute force enumeration techniques is fine for small problems but grows exponentially in complexity. Charniak and Shimony [7] have proven that this problem is NP-complete by transforming it into the vertex cover problem <ref> [17] </ref>. Straightforwardly, this problem can be transformed into a search problem on and/or graphs.
Reference: [18] <author> Robert S. Garfinkel and George L. Nemhauser. </author> <title> Integer Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1972. </year>
Reference-contexts: It has been frequently observed that matching and set covering problems on graphs are very amenable to linear programming formulations in that they very 37 Fig. 3.5. Logarithmic plot of linear constraint satisfaction timings on random waodags. 38 often have integral optimal solutions <ref> [18] </ref>. The abduction model for medical diagnoses presented in [42] is such a set-covering approach. Thus, the associated linear programs to solve problems using this model will generally have integral optimal solutions.
Reference: [19] <author> Stuart Geman and Donald Geman. </author> <title> Stochastic relaxation, gibbs distribution, and the bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-41, </pages> <year> 1984. </year>
Reference-contexts: Although our experiments in cost-based abduction were rather successful, our approach to belief revision may easily suffer the exponential explosions in the size of the conditional probability tables. For example, consider image processing where each pixel is represented by a r.v. <ref> [19] </ref>. 87 Yet, of the other existing algorithms for performing belief revision, namely Pearl's message-passing scheme [41] and Shimony and Charniak's cost-based approach [68], message-passing is incapable of generating alternative explanations while the cost-based method suffers from the heuristic problems outlined in Section 3.
Reference: [20] <author> Michael R. Genesereth. </author> <title> The use of design descriptions in automated diagnosis. </title> <journal> Artificial Intelligence, </journal> <volume> 24 </volume> <pages> 411-436, </pages> <year> 1984. </year>
Reference-contexts: Since then, many common problems have been identified in its terms. For example, such problems include medical diagnosis [8, 42], circuit fault detection <ref> [12, 11, 20] </ref> and story understanding [22, 23, 3]. 3 Clearly, we extensively use abductive reasoning in our everyday tasks from explaining why the ground is wet to performing sophisticated inferencing in medical diagnosis. <p> For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available.
Reference: [21] <author> B. E. Gillett. </author> <title> Introduction to Operations Research: A Computer-Oriented Algorithmic Approach. </title> <publisher> McGraw Hill, </publisher> <year> 1976. </year>
Reference-contexts: For example, there are approximation methods for solving integer linear programming problems which have theoretically proven error bounds. Can we also find such a technique tailored to our abductive models? * Parallel algorithms exist for solving linear programming problems <ref> [21, 25] </ref>. Can we get the expected sub-linear run-times from these algorithms? * In systems like wimp and tacitus [26], the reasoning mechanisms are essentially broken down into two components. The first component involves constructing a network of propositional rules from a first-order logic knowledge-base.
Reference: [22] <author> Robert P. Goldman. </author> <title> A Probabilistic Approach to Language Understanding. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1990. </year>
Reference-contexts: Since then, many common problems have been identified in its terms. For example, such problems include medical diagnosis [8, 42], circuit fault detection [12, 11, 20] and story understanding <ref> [22, 23, 3] </ref>. 3 Clearly, we extensively use abductive reasoning in our everyday tasks from explaining why the ground is wet to performing sophisticated inferencing in medical diagnosis. Thus, we need an approach to modeling abduction which is representa-tionally robust and permits a practical implementation. <p> The first involves a real application of our technique to solve cost-based abduction problems created by the story understanding system wimp <ref> [22, 23] </ref>. This allows us to make a comparison against the search-style heuristic described in [4] to solve these graphs. Our second experiment involves 9 Techniques V1 and V2 seem to work best when values can propagate in a top-down fashion. <p> Also, waodags larger than those found in the first experiment are used. For both experiments, we employed active node method N1 and branching variable technique V3 above. 3.4.1 Experiment #1 wimp is a natural language story comprehension system for parsing and understanding written English sentences <ref> [22, 23] </ref>. It utilizes belief networks to perform the necessary abductive inference tasks necessary to solve problems like pronoun reference and word-sense disambiguation. The belief networks can then be transformed into equivalent cost-based abductions problems as shown in [7]. <p> The waodags were again generated by wimp, a natural language story comprehension system for parsing and understanding written English sentences <ref> [3, 22, 23] </ref>. We generated initial solutions for each of the waodags by using a quick (unfortunately, non-admissible) heuristic outlined in [4]. <p> Also, since none of the hypotheses are used, no cost is incurred which can make this explanation, the best explanation. A more sophisticated example involving cyclicity often occurs in the rule bases of the wimp story understanding system <ref> [3, 23, 22] </ref>. In its knowledge base, you can find the logical rules: (foo a) ^ (= a b ) =) (foo b) A method is available in wimp to eliminate this logical cyclicity. However, it is rather ad hoc.
Reference: [23] <author> Robert P. Goldman and Eugene Charniak. </author> <title> Probabilistic text understanding. </title> <booktitle> In Proceedings of the Third International Workshop on AI and Statistics, </booktitle> <address> Fort Lauderdale, FL, </address> <year> 1991. </year>
Reference-contexts: Since then, many common problems have been identified in its terms. For example, such problems include medical diagnosis [8, 42], circuit fault detection [12, 11, 20] and story understanding <ref> [22, 23, 3] </ref>. 3 Clearly, we extensively use abductive reasoning in our everyday tasks from explaining why the ground is wet to performing sophisticated inferencing in medical diagnosis. Thus, we need an approach to modeling abduction which is representa-tionally robust and permits a practical implementation. <p> The first involves a real application of our technique to solve cost-based abduction problems created by the story understanding system wimp <ref> [22, 23] </ref>. This allows us to make a comparison against the search-style heuristic described in [4] to solve these graphs. Our second experiment involves 9 Techniques V1 and V2 seem to work best when values can propagate in a top-down fashion. <p> Also, waodags larger than those found in the first experiment are used. For both experiments, we employed active node method N1 and branching variable technique V3 above. 3.4.1 Experiment #1 wimp is a natural language story comprehension system for parsing and understanding written English sentences <ref> [22, 23] </ref>. It utilizes belief networks to perform the necessary abductive inference tasks necessary to solve problems like pronoun reference and word-sense disambiguation. The belief networks can then be transformed into equivalent cost-based abductions problems as shown in [7]. <p> The waodags were again generated by wimp, a natural language story comprehension system for parsing and understanding written English sentences <ref> [3, 22, 23] </ref>. We generated initial solutions for each of the waodags by using a quick (unfortunately, non-admissible) heuristic outlined in [4]. <p> Also, since none of the hypotheses are used, no cost is incurred which can make this explanation, the best explanation. A more sophisticated example involving cyclicity often occurs in the rule bases of the wimp story understanding system <ref> [3, 23, 22] </ref>. In its knowledge base, you can find the logical rules: (foo a) ^ (= a b ) =) (foo b) A method is available in wimp to eliminate this logical cyclicity. However, it is rather ad hoc. <p> For example, wimp constructs a network and then uses cost-based abduction. So far, we have only concentrated on the second component in our formulation since it has been the major stumbling block for systems like wimp <ref> [23] </ref>. Obviously, we should now consider how we might model the first component with our approach with the 112 eventual goal of merging both within one constraint systems formulation. * Splined Bayesian networks introduce a new flexibility in transforming and manipulating our data.
Reference: [24] <author> M. Henrion. </author> <title> Propagating uncertainty by logic sampling in bayes' networks. </title> <type> Technical report, </type> <institution> Department of Engineering and Public Policy, Carnegie-Mellon University, </institution> <year> 1986. </year>
Reference-contexts: As we can clearly see, a r.v.s instantiation cannot be determined until all the r.v.s it depends on have already been instantiated, thus forcing a top-down construction paralleling the causality information built into the topology of the network. This sort of approach was introduced in <ref> [24] </ref> called logic sampling. 80 Fig. 4.3. Simple Bayesian network. Still, eventhough logic sampling takes into account a fair amount of information inside the Bayesian network, its approach can also miss those "significant" instantiation-sets. <p> Through various tests, we have noticed that our method performs best on highly deterministic networks, that is, networks whose conditional probabilities are near 0 or 1. For belief updating, we compared our approach against stochastic simulation methods <ref> [72, 34, 28, 10, 24, 62] </ref>. We found that our approach performed much better than stochastic simulation on highly deterministic networks. Basically, as we discussed earlier, simulation approaches could very rarely come up with an answer on these types of networks.
Reference: [25] <author> F. S. Hillier and G. J. Lieberman. </author> <title> Introduction to Operations Research. </title> <publisher> Holden-Day, Inc., </publisher> <year> 1967. </year>
Reference-contexts: Although our constraint systems seem similar in nature to linear programs, linear programs are incapable of making restrictions which cannot be modeled by 7 For a quick overview of the simplex method, see <ref> [25] </ref>. 22 linear inequalities. Thus, solutions which minimize the objective function may not be strictly 0 and 1. <p> For example, there are approximation methods for solving integer linear programming problems which have theoretically proven error bounds. Can we also find such a technique tailored to our abductive models? * Parallel algorithms exist for solving linear programming problems <ref> [21, 25] </ref>. Can we get the expected sub-linear run-times from these algorithms? * In systems like wimp and tacitus [26], the reasoning mechanisms are essentially broken down into two components. The first component involves constructing a network of propositional rules from a first-order logic knowledge-base.
Reference: [26] <author> Jerry R. Hobbs, Mark Stickel, Paul Martin, and Douglas Edwards. </author> <title> Interpretation as abduction. </title> <booktitle> In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1988. </year> <month> 115 </month>
Reference-contexts: For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available. <p> This would serve to precisely define the notion of a best explanation as well as subsequent next best which is critical to have in domains such as medical diagnosis. Several ordering measures are available such as least specific abduction <ref> [26, 73] </ref>, cost-based abduction [7], parsimonious covering theory [43] and belief revision [41]. Each approach offers different perspectives on the problem and provides individual frameworks capable of modeling certain aspects of abductive reasoning. <p> Another simple approach is to designate some set of propositions as assumable. Thus, any set of hypotheses must consist only of assumable propositions. However, we often run into the problem of the explanations either being too detailed or not detailed enough. 2.1 Weighted Abduction Hobbs and Stickel <ref> [26, 73] </ref> proposed an approach called weighted abduction. It involves levying numerical costs to making individual assumptions. The cost of an explanation is a function of the cost of the individual assumptions made in the explanation. <p> In its knowledge base, you can find the logical rules: (foo a) ^ (= a b ) =) (foo b) A method is available in wimp to eliminate this logical cyclicity. However, it is rather ad hoc. A similar situation arises in Hobbs et al. <ref> [26] </ref> where we find the rules: (dog x) =) (mammal x) (mammal x) ^ (dog-features x) =) (dog x) The second rule is needed by [26] (and probably by most cost-based schemes) to allow us to use the fact that "something is a mammal" as (weak) evidence that it is a <p> However, it is rather ad hoc. A similar situation arises in Hobbs et al. <ref> [26] </ref> where we find the rules: (dog x) =) (mammal x) (mammal x) ^ (dog-features x) =) (dog x) The second rule is needed by [26] (and probably by most cost-based schemes) to allow us to use the fact that "something is a mammal" as (weak) evidence that it is a dog. Our (dog-features x) corresponds to the etcetera attribute, (etc x), found in [26]. Finally, cyclicity can also occur in modeling causal information. <p> (dog-features x) =) (dog x) The second rule is needed by <ref> [26] </ref> (and probably by most cost-based schemes) to allow us to use the fact that "something is a mammal" as (weak) evidence that it is a dog. Our (dog-features x) corresponds to the etcetera attribute, (etc x), found in [26]. Finally, cyclicity can also occur in modeling causal information. Suppose we are modeling faulty electrical outlets. Furthermore, suppose that our television 98 set and radio are both plugged into such an outlet. <p> Can we also find such a technique tailored to our abductive models? * Parallel algorithms exist for solving linear programming problems [21, 25]. Can we get the expected sub-linear run-times from these algorithms? * In systems like wimp and tacitus <ref> [26] </ref>, the reasoning mechanisms are essentially broken down into two components. The first component involves constructing a network of propositional rules from a first-order logic knowledge-base. The second component then takes this network as the basis for its abductive computations.
Reference: [27] <author> V. Isham. </author> <title> An introduction to spatial point processes and markov random fields. </title> <journal> International Statistical Review, </journal> <volume> 49 </volume> <pages> 21-43, </pages> <year> 1981. </year>
Reference-contexts: Almost all alternative uncertainty models have failed to provide working implementations. Models which are closely related to Bayesian networks can also be formulated using linear constraints as well. For example, Markov random fields <ref> [27, 33] </ref>. 4.4 Discussion We have just demonstrated that our linear constraint satisfaction approach can completely model Bayesian networks. In particular, the reasoning task of belief revision and belief updating can be solved using our tools and techniques.
Reference: [28] <author> F. V. Jensen, S. L. Lauritzen, and K. G. Olesen. </author> <title> Bayesian updating in recursive graphical models by local computations. </title> <type> Technical Report Report R 89-15, </type> <institution> Institute for Electronic Systems, Department of Mathematics and Computer Science, University of Aalborg, Denmark, </institution> <year> 1989. </year>
Reference-contexts: Through various tests, we have noticed that our method performs best on highly deterministic networks, that is, networks whose conditional probabilities are near 0 or 1. For belief updating, we compared our approach against stochastic simulation methods <ref> [72, 34, 28, 10, 24, 62] </ref>. We found that our approach performed much better than stochastic simulation on highly deterministic networks. Basically, as we discussed earlier, simulation approaches could very rarely come up with an answer on these types of networks.
Reference: [29] <author> John R. Josephson. </author> <title> Abduction: Conceptual analysis of a fundamental pattern of inference. </title> <type> Technical Report 91-JJ-DRAFT, </type> <institution> The Ohio State University, </institution> <year> 1991. </year>
Reference-contexts: We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see <ref> [29] </ref>. 2 explanations available. From traditional symbolic logic, the only measure of a set's viability as an explanation is the simple fact concerning whether the evidence can be deductively inferred from the set.
Reference: [30] <author> N. Karmarkar and R. M. Karp. </author> <title> An efficient approximation scheme for the one-dimensional bin-packing problem. </title> <booktitle> In Proceedings of the 23rd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 206-13, </pages> <year> 1982. </year>
Reference-contexts: A question naturally arises as to whether there is something special about our domain which causes this. Can we predict when a 0-1 solution will be the optimal solution? * There exists domain dependent methods for solving linear programming problems other than the general ones like Simplex <ref> [44, 30, 32, 48, 76] </ref>. These methods have been shown to outperform the general ones for their specific domains. For example, there are approximation methods for solving integer linear programming problems which have theoretically proven error bounds.
Reference: [31] <author> Henry A. Kautz and James F. Allen. </author> <title> Generalized plan recognition. </title> <booktitle> In Proceedings of the AAAI Conference, </booktitle> <year> 1986. </year>
Reference-contexts: For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available.
Reference: [32] <author> Philip Klein, Serge A. Plotkin, C. Stein, and Eva Tardos. </author> <title> Faster approximation algorithms for the unit capacity concurrent flow problem with applications to routing and finding sparse cuts. </title> <type> Technical Report Technical Report 961, </type> <institution> Schools of Operations Research and Industrial Engineering, Cornell University, </institution> <year> 1991. </year>
Reference-contexts: A question naturally arises as to whether there is something special about our domain which causes this. Can we predict when a 0-1 solution will be the optimal solution? * There exists domain dependent methods for solving linear programming problems other than the general ones like Simplex <ref> [44, 30, 32, 48, 76] </ref>. These methods have been shown to outperform the general ones for their specific domains. For example, there are approximation methods for solving integer linear programming problems which have theoretically proven error bounds.
Reference: [33] <author> S. L. Lauritzen. </author> <title> Lectures on Contigency Tables. </title> <publisher> University of Aalborg Press, </publisher> <year> 1982. </year>
Reference-contexts: Almost all alternative uncertainty models have failed to provide working implementations. Models which are closely related to Bayesian networks can also be formulated using linear constraints as well. For example, Markov random fields <ref> [27, 33] </ref>. 4.4 Discussion We have just demonstrated that our linear constraint satisfaction approach can completely model Bayesian networks. In particular, the reasoning task of belief revision and belief updating can be solved using our tools and techniques.
Reference: [34] <author> S. L. Lauritzen and D. J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their applications to expert systems. </title> <journal> J. Royal Statistical Society, </journal> <volume> 50(2) </volume> <pages> 157-224, </pages> <year> 1988. </year>
Reference-contexts: Through various tests, we have noticed that our method performs best on highly deterministic networks, that is, networks whose conditional probabilities are near 0 or 1. For belief updating, we compared our approach against stochastic simulation methods <ref> [72, 34, 28, 10, 24, 62] </ref>. We found that our approach performed much better than stochastic simulation on highly deterministic networks. Basically, as we discussed earlier, simulation approaches could very rarely come up with an answer on these types of networks.
Reference: [35] <author> Richard Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Thagard continues by refining the four possibilities into seven distinct principles. In this way, it is hoped that further gradations can be made in "best". Although a seemingly sound theory, it is rather complex. A connectionist <ref> [52, 15, 35] </ref> implementation has been attempted, however, its feasibility in applications seems questionable. 2.6 Other Approaches The above methods are directed mainly towards modeling abduction. Other approaches are certainly available.
Reference: [36] <author> C. McMillan. </author> <title> Mathematical Programming. </title> <publisher> John-Wiley & Sons, Inc., </publisher> <year> 1975. </year>
Reference-contexts: Linear constraint satisfaction is a very well understood problem in Operations Research. Our reasoning engine is thus formed from highly efficient tools and techniques developed in OR. Such tools as the Simplex method and Kar-markar's projective scaling algorithm <ref> [36, 39, 59] </ref> provide us with a firm foundation to building a practical system. Experimental results strongly indicate that our linear constraint satisfaction approach is quite promising. <p> Taking the set of linear inequalities I and the objective function fi L , we observe that we have the elements known in operations research as a linear program <ref> [59, 36, 39] </ref>. The goal of a linear program is to minimize an objective function according to some set of linear constraints. Highly efficient methods such as the simplex method 7 and Karmarkar's projective scaling algorithm are used to solve linear programs [59, 36, 39]. <p> known in operations research as a linear program <ref> [59, 36, 39] </ref>. The goal of a linear program is to minimize an objective function according to some set of linear constraints. Highly efficient methods such as the simplex method 7 and Karmarkar's projective scaling algorithm are used to solve linear programs [59, 36, 39]. Empirical studies have shown that the running time of the simplex method is roughly linear with respect to the number of constraints and the number of variables in the linear program [39]. Proposition 3.4. <p> This is a standard technique used in many domains to speed up processing time. The basic idea is as follows: To find an optimal 0-1 solution, we solve a sequence of linear programs. This sequence can be represented by a tree where 8 See integer programming techniques <ref> [59, 36, 39] </ref>. 26 each node in the tree is identified with a linear program that is derived from the linear programs on the path leading to the root of the tree. The root of the tree is identified with the linear program induced by our constraint system. <p> Let L k := (; I k ; ). 6. Go to step 2. 7. (Solutions) Print s 1 ; s 2 ; . . . ; s k1 . The above method we have just described can be classified as a cutting plane method in operations research <ref> [36, 59, 39] </ref>. Since each derived constraint system differs only in an additional constraint from some previously solved problem, efficient incremental techniques such as the dual simplex method can be applied here in a fashion similar to the one used in the branch and bound algorithm. Theorem 3.11. <p> Let L k = (; I k ; ). 6. Go to step 2. 7. (Solutions) Print s 1 ; s 2 ; . . . ; s k1 . The above method can again be classified as a cutting plane method in operations research <ref> [36, 59, 39] </ref>. Likewise, since each derived constraint system differs only in an additional constraint from some previously solved problem, incremental computational methods such as the dual simplex method can be applied here in a fashion similar to the one used in the branch and bound algorithm. Theorem 4.12.
Reference: [37] <author> V. S. Mikhalevich, V. L. Volkovich, and G. V. Kolenov. </author> <title> An algorithm for coordination of solutions in a distributed system of interdependent problems with linear models. </title> <journal> Kibernetika, </journal> <volume> 3 </volume> <pages> 1-8+22, </pages> <year> 1988. </year> <month> 116 </month>
Reference-contexts: Simplex-like algorithm [76]. * Updating deductive databases though linear programming techniques [2]. * Reduction of independence-based MAPs in Bayesian networks to linear con straint satisfaction [69]. * The problem of path planning in robotics in relation to routing and trans portation problems [49]. * Work done with distributed intelligent agents <ref> [37] </ref>. 13 3 Cost-Based Abduction Our basic approach towards knowledge representation involves a mapping from objects and/or propositions in the world to real variables. The values that a real variable may attain are analogous to the changing states of the associated object or proposition.
Reference: [38] <author> R. A. Miller, H. E. Poole, and J. P. Myers. Internist-1: </author> <title> An experimental computer-based diagnostic consultant for general internal medicine. </title> <journal> New England Journal of Medicine, </journal> <volume> 307 </volume> <pages> 468-70, </pages> <year> 1982. </year>
Reference-contexts: Common approaches to this problem have advocated augmenting propositional logic with certainty factors, probabilities, costs, etc. in an attempt to preserve deduction. This was often used in classical expert systems such as MYCIN [70, 71], PROSPECTOR [14] and INTERNIST <ref> [38, 46] </ref>. However, the resulting models were clumsy and restrictive. A case in point is Shortliffe's MYCIN system.
Reference: [39] <editor> G. L. Nemhauser, A. H. G. Rinnooy Kan, and M. J. Todd, editors. </editor> <booktitle> Optimization: Handbooks in Operations Research and Management Science Volume 1, </booktitle> <volume> volume 1. </volume> <publisher> North Holland, </publisher> <year> 1989. </year>
Reference-contexts: Linear constraint satisfaction is a very well understood problem in Operations Research. Our reasoning engine is thus formed from highly efficient tools and techniques developed in OR. Such tools as the Simplex method and Kar-markar's projective scaling algorithm <ref> [36, 39, 59] </ref> provide us with a firm foundation to building a practical system. Experimental results strongly indicate that our linear constraint satisfaction approach is quite promising. <p> Taking the set of linear inequalities I and the objective function fi L , we observe that we have the elements known in operations research as a linear program <ref> [59, 36, 39] </ref>. The goal of a linear program is to minimize an objective function according to some set of linear constraints. Highly efficient methods such as the simplex method 7 and Karmarkar's projective scaling algorithm are used to solve linear programs [59, 36, 39]. <p> known in operations research as a linear program <ref> [59, 36, 39] </ref>. The goal of a linear program is to minimize an objective function according to some set of linear constraints. Highly efficient methods such as the simplex method 7 and Karmarkar's projective scaling algorithm are used to solve linear programs [59, 36, 39]. Empirical studies have shown that the running time of the simplex method is roughly linear with respect to the number of constraints and the number of variables in the linear program [39]. Proposition 3.4. <p> Empirical studies have shown that the running time of the simplex method is roughly linear with respect to the number of constraints and the number of variables in the linear program <ref> [39] </ref>. Proposition 3.4. Given a waodag W = (G; c; r; S) where G = (V; E), if L E (W ) = (; I; ) is induced evidentially from W, then jIj = jEj+jV V H j+jSj. <p> This is a standard technique used in many domains to speed up processing time. The basic idea is as follows: To find an optimal 0-1 solution, we solve a sequence of linear programs. This sequence can be represented by a tree where 8 See integer programming techniques <ref> [59, 36, 39] </ref>. 26 each node in the tree is identified with a linear program that is derived from the linear programs on the path leading to the root of the tree. The root of the tree is identified with the linear program induced by our constraint system. <p> Next, we choose one of the problems identified with an active node and attempt to solve it. It is not necessary to run a complete simplex method on the linear program. Using methods such as the dual simplex algorithm <ref> [59, 39] </ref>, information is utilized in an incremental manner from other runs resulting in a quick and efficient computation. If the optimal solution is not a 0-1 solution, then two new problems are defined based on the current linear program. <p> Branching continues in this manner until there are no active nodes in the tree. At the end, the current best solution is guaranteed to be the optimal 0-1 solution. This technique is generally classified as a branch and bound technique in the area of integer linear programming <ref> [59, 39] </ref>. Also, it can be applied to any con straint system. 27 Notation. We denote an active node by an ordered pair (I; l) where I is a set of linear constraints and l is the value of the optimal solution for the associated linear program. Algorithm 3.1. <p> Let L k := (; I k ; ). 6. Go to step 2. 7. (Solutions) Print s 1 ; s 2 ; . . . ; s k1 . The above method we have just described can be classified as a cutting plane method in operations research <ref> [36, 59, 39] </ref>. Since each derived constraint system differs only in an additional constraint from some previously solved problem, efficient incremental techniques such as the dual simplex method can be applied here in a fashion similar to the one used in the branch and bound algorithm. Theorem 3.11. <p> Let L k = (; I k ; ). 6. Go to step 2. 7. (Solutions) Print s 1 ; s 2 ; . . . ; s k1 . The above method can again be classified as a cutting plane method in operations research <ref> [36, 59, 39] </ref>. Likewise, since each derived constraint system differs only in an additional constraint from some previously solved problem, incremental computational methods such as the dual simplex method can be applied here in a fashion similar to the one used in the branch and bound algorithm. Theorem 4.12.
Reference: [40] <author> N. J. Nilsson. </author> <title> Probabilistic logic. </title> <journal> Artificial Intelligence, </journal> <volume> 28 </volume> <pages> 71-87, </pages> <year> 1986. </year>
Reference-contexts: Other approaches are certainly available. However, aside from the five methods we have just studied, the remaining ones handle abductive reasoning sort of as an afterthought to their main goals. Such systems include truth maintenance systems [12], influence diagrams [61], probabilistic logic <ref> [40] </ref>, Dempster-Shafer theory [13, 63] and fuzzy logic [75]. 12 2.7 Related Work There are also various strands of work which, while somewhat related to the work described in this thesis, are nevertheless sufficiently distant not to warrant a full fledged review.
Reference: [41] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Pople in [47, 45, 46] was one of the first researchers to point this out through his work on the Caduceus medical diagnosis system. Formally called abduction, it was not widely considered as a form of 2 See <ref> [41] </ref> for discussion on the limitations of these approaches. 1 reasoning by the AI community until its popular introduction by Charniak and McDermott in [5]. Since then, many common problems have been identified in its terms. <p> Thus, we need an approach to modeling abduction which is representa-tionally robust and permits a practical implementation. To our chagrin, however, it seems that abductive reasoning is an inherently difficult process. Indeed, various abductive models have been shown to be NP-hard <ref> [9, 41, 7, 64, 42] </ref>. To better understand the difficulty inherent in abduction, let us attempt to model John's situation above. <p> For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available. <p> Several ordering measures are available such as least specific abduction [26, 73], cost-based abduction [7], parsimonious covering theory [43] and belief revision <ref> [41] </ref>. Each approach offers different perspectives on the problem and provides individual frameworks capable of modeling certain aspects of abductive reasoning. The complexity of abduction quickly becomes apparent in that the problem now involves the search through a most likely exponential space of solutions for a single maximal or minimal. <p> Furthermore, it failed to give an intuitive feel for what the numbers really mean. 2.2 Cost-Based Abduction Charniak and Shimony presented a minor variant of weighted abduction called cost-based abduction [7]. It has been shown in [7] that belief revision in Bayesian networks <ref> [41] </ref> can be accurately modeled by cost-based abduction. <p> This proof has a cost of 7 and is the minimal cost proof. 8 proof [4]. (We will continue with a more detailed discussion of cost-based abduc-tion in Section 3.) 2.3 Belief Revision Pearl presented an approach to modeling belief revision by using Bayesian networks <ref> [41] </ref>. Based upon the tenets of probability theory, events are represented by random variables and direct and indirect causal relationships between events are modeled by conditional probabilities and conditional independence. For example, if the lights are off, then the house is dark. <p> A probability assignment for our story. 10 most-probable explanation. However, as the Bayesian networks become increas-ingly sophisticated, the current methods used to compute them also become extremely complicated. Furthermore, the best computational method which uses message-passing schemes <ref> [41] </ref> are incapable of generating the subsequent next best explanations beyond the second best one. (We will continue with a more detailed discussion of Bayesian networks including belief updating in Section 4.) 2.4 Parsimonious Covering Theory Parsimonious covering theory is an approach presented by Peng and Reggia [43] for medical diagnosis. <p> Mary's Bayesian network. 60 very promising approach. In particular, belief revision in Bayesian networks is the process of finding the best interpretation for some given piece of evidence. This, of course, is a cornerstone of abductive explanation. One algorithm for belief revision is given by Pearl in <ref> [41] </ref> which is based on a message passing scheme. However, except for simple networks such as polytrees, the method is rather complicated to apply. Also, as Pearl points out in Chapter 5 in [41], this algorithm cannot guarantee the generation of alternative explanations beyond the second best. <p> One algorithm for belief revision is given by Pearl in <ref> [41] </ref> which is based on a message passing scheme. However, except for simple networks such as polytrees, the method is rather complicated to apply. Also, as Pearl points out in Chapter 5 in [41], this algorithm cannot guarantee the generation of alternative explanations beyond the second best. In this section, our goal is to apply our linear constraint satisfaction approach to belief revision as well as belief updating in Bayesian networks. <p> Theorem 4.12. Constraint system L n in Algorithm 4.1 determines the n-th best 0-1 solution for L. 4.1.3 Circumscribing Explanations and Focusing Consider the following problem with most-probable explanation (MPE) criterion pointed out by Pearl in <ref> [41] </ref>: Suppose a medical test reveals that my friend Glenn has an 80% chance of being totally healthy. If you are not healthy, then you are going to die. Since Glenn believes himself to likely be healthy, he begins to daydream about going on vacation and not staying at home. <p> As we can clearly see, this result is mathematically correct although somewhat counterintuitive. As Pearl points out <ref> [41] </ref>, this has much to do with what sort of information we are currently thinking about or focusing on. In the above case, 70 since we started with information concerning our health, we naturally expect all our attention to shift heavily to thoughts involving being healthy. <p> The problem of determining what the solution should be seems to be highly dependent on what we are currently focused on. Pearl in <ref> [41] </ref> presents one possible conservative approach which suggests that what we are focusing on is precisely the evidence we wish to explain. Furthermore, the evidence should be the determining factor in what we consider as information which is relevant to any explanation we may be interested in. <p> In probabilistic terms, this seems to suggest that random variables which are "irrelevant" to the evidence are not instantiated. This approach of only instantiating a subset of the random variables is currently being explored by <ref> [41] </ref> and [7, 67, 66]. <p> We now show that this weakening of constraints results in computations using the circumscribing explanation approach suggested by Pearl <ref> [41] </ref>. Definition 4.10. <p> For example, consider image processing where each pixel is represented by a r.v. [19]. 87 Yet, of the other existing algorithms for performing belief revision, namely Pearl's message-passing scheme <ref> [41] </ref> and Shimony and Charniak's cost-based approach [68], message-passing is incapable of generating alternative explanations while the cost-based method suffers from the heuristic problems outlined in Section 3. Given this dearth of algorithms, we believe ours to be a plausible alternative.
Reference: [42] <author> Y. Peng and J. A. Reggia. </author> <title> Plausibility of diagnostic hypothese: The nature of simplicity. </title> <booktitle> In Proceedings of the AAAI Conference, </booktitle> <year> 1986. </year>
Reference-contexts: Since then, many common problems have been identified in its terms. For example, such problems include medical diagnosis <ref> [8, 42] </ref>, circuit fault detection [12, 11, 20] and story understanding [22, 23, 3]. 3 Clearly, we extensively use abductive reasoning in our everyday tasks from explaining why the ground is wet to performing sophisticated inferencing in medical diagnosis. <p> Thus, we need an approach to modeling abduction which is representa-tionally robust and permits a practical implementation. To our chagrin, however, it seems that abductive reasoning is an inherently difficult process. Indeed, various abductive models have been shown to be NP-hard <ref> [9, 41, 7, 64, 42] </ref>. To better understand the difficulty inherent in abduction, let us attempt to model John's situation above. <p> Logarithmic plot of linear constraint satisfaction timings on random waodags. 38 often have integral optimal solutions [18]. The abduction model for medical diagnoses presented in <ref> [42] </ref> is such a set-covering approach. Thus, the associated linear programs to solve problems using this model will generally have integral optimal solutions.
Reference: [43] <author> Y. Peng and J. A. Reggia. </author> <title> Abductive Inference Models for Diagnostic Problem-Solving. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available. <p> This would serve to precisely define the notion of a best explanation as well as subsequent next best which is critical to have in domains such as medical diagnosis. Several ordering measures are available such as least specific abduction [26, 73], cost-based abduction [7], parsimonious covering theory <ref> [43] </ref> and belief revision [41]. Each approach offers different perspectives on the problem and provides individual frameworks capable of modeling certain aspects of abductive reasoning. <p> uses message-passing schemes [41] are incapable of generating the subsequent next best explanations beyond the second best one. (We will continue with a more detailed discussion of Bayesian networks including belief updating in Section 4.) 2.4 Parsimonious Covering Theory Parsimonious covering theory is an approach presented by Peng and Reggia <ref> [43] </ref> for medical diagnosis. A diagnostic problem is defined as a two-layer network consisting of a layer of manifestations which are causally affected by a layer of disorders. Given a subset of the manifestations as evidence, a subset of disorders must be chosen to best explain the manifestations. <p> A cover is a best explanation if none of its proper subsets is also a cover. Such a cover is also said to be irredundant. A limitation of this theory as pointed out by Peng and Reggia <ref> [43] </ref> is the large number of covers which are considered "best". In order to further select from these potential explanations, some additional criteria must be used. Thus, basic parsimonious covering theory is extended to incorporate probability theory. The potential of an explanation is now measured by some probability. <p> Thus, basic parsimonious covering theory is extended to incorporate probability theory. The potential of an explanation is now measured by some probability. With the addition of probabilities, care must be taken in choosing which covers are to be inspected. Peng and Reggia <ref> [43] </ref> proposed a 2-layer Bayesian network to probabilistically model their approach. However, extending their approach to more general problems is not readily obvious. 2.5 Coherence Thagard [74] proposed an approach for modeling explanation in general. <p> Intuitively, a cardinal explanation is among the "simplest" of explanations we wish to consider. Our notion of cardinal explanations is equivalent to the notion of irredundancy found in parsimonious covering theory for modeling medical diagnosis <ref> [43] </ref>. A cover is said to be irredundant if none of its proper subsets is also a cover. Theorem 3.20. If W is strictly monotonic, then any best explanation for W is cardinal. 50 All the definitions given above involving waodags can be carried over to waodag induced constraint systems. <p> For waodags, the only strongly dominant set is the set corresponding to V H . Similarly for dags representing boolean gate functions, the only strongly dominant set is the set corresponding to the set of all nodes with zero indegree. For diagnostic problem <ref> [43] </ref>, the only strongly dominant set is the set corresponding to the set of all disorders. Definition 3.17.
Reference: [44] <author> Serge A. Plotkin, David B. Shmoys, and Eva Tardos. </author> <title> Fast approximation algorithms for fractional packing and covering problems. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 495-504, </pages> <year> 1991. </year>
Reference-contexts: A question naturally arises as to whether there is something special about our domain which causes this. Can we predict when a 0-1 solution will be the optimal solution? * There exists domain dependent methods for solving linear programming problems other than the general ones like Simplex <ref> [44, 30, 32, 48, 76] </ref>. These methods have been shown to outperform the general ones for their specific domains. For example, there are approximation methods for solving integer linear programming problems which have theoretically proven error bounds.
Reference: [45] <author> Harry Pople. </author> <title> The formation of composite hypotheses in diagnostic problem solving: An exercise in synthetic reasoning. </title> <booktitle> In Proceedings of the IJCAI Conference, </booktitle> <year> 1977. </year>
Reference-contexts: Although a highly successful system within its restricted domain, its inferencing lacked a proper mathematical as well as semantic basis which stemmed from its treatment of diagnosis as deduction. 2 Only until recently has explanatory reasoning been properly identified as being separate from deductive reasoning. Pople in <ref> [47, 45, 46] </ref> was one of the first researchers to point this out through his work on the Caduceus medical diagnosis system.
Reference: [46] <author> Harry Pople. </author> <title> Heuristic methods for imposing structures on ill-structured problems. </title> <editor> In P. Szolovits, editor, </editor> <booktitle> Artificial Intelligence in Medicine. </booktitle> <address> Boulder, CO: Westview, </address> <year> 1982. </year>
Reference-contexts: Common approaches to this problem have advocated augmenting propositional logic with certainty factors, probabilities, costs, etc. in an attempt to preserve deduction. This was often used in classical expert systems such as MYCIN [70, 71], PROSPECTOR [14] and INTERNIST <ref> [38, 46] </ref>. However, the resulting models were clumsy and restrictive. A case in point is Shortliffe's MYCIN system. <p> Although a highly successful system within its restricted domain, its inferencing lacked a proper mathematical as well as semantic basis which stemmed from its treatment of diagnosis as deduction. 2 Only until recently has explanatory reasoning been properly identified as being separate from deductive reasoning. Pople in <ref> [47, 45, 46] </ref> was one of the first researchers to point this out through his work on the Caduceus medical diagnosis system.
Reference: [47] <author> Harry Pople, Jack Myers, and Randolph Miller. </author> <title> Dialog: A model of diagnostic logic for internal medicine. </title> <booktitle> In Proceedings of the IJCAI Conference, </booktitle> <year> 1975. </year>
Reference-contexts: Although a highly successful system within its restricted domain, its inferencing lacked a proper mathematical as well as semantic basis which stemmed from its treatment of diagnosis as deduction. 2 Only until recently has explanatory reasoning been properly identified as being separate from deductive reasoning. Pople in <ref> [47, 45, 46] </ref> was one of the first researchers to point this out through his work on the Caduceus medical diagnosis system.
Reference: [48] <author> P. Raghavan. </author> <title> Probabilistic construction of deterministic algorithms: Approximating packing intereger programs. </title> <journal> J. Comput. System Sciences, </journal> <volume> 37 </volume> <pages> 130-43, </pages> <year> 1988. </year>
Reference-contexts: A question naturally arises as to whether there is something special about our domain which causes this. Can we predict when a 0-1 solution will be the optimal solution? * There exists domain dependent methods for solving linear programming problems other than the general ones like Simplex <ref> [44, 30, 32, 48, 76] </ref>. These methods have been shown to outperform the general ones for their specific domains. For example, there are approximation methods for solving integer linear programming problems which have theoretically proven error bounds.
Reference: [49] <author> Prabhakar Raghavan. </author> <title> A provably good routing in graphs: Regular arrays. </title> <booktitle> ACM, </booktitle> <pages> pages 79-87, </pages> <year> 1985. </year> <month> 117 </month>
Reference-contexts: * Solving the relaxation labeling process through a Simplex-like algorithm [76]. * Updating deductive databases though linear programming techniques [2]. * Reduction of independence-based MAPs in Bayesian networks to linear con straint satisfaction [69]. * The problem of path planning in robotics in relation to routing and trans portation problems <ref> [49] </ref>. * Work done with distributed intelligent agents [37]. 13 3 Cost-Based Abduction Our basic approach towards knowledge representation involves a mapping from objects and/or propositions in the world to real variables.
Reference: [50] <author> Edward M. Reingold, Jurg Nievergelt, and Narsingh Deo. </author> <title> Combinatorial Algorithms: Theory and Practice. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1977. </year>
Reference-contexts: Basically, a depth-first search is used whereby through an elegant approach to adding edges will generate each cycle. Details and analysis of the algorithm can be found in <ref> [50] </ref>. 5.3 Constraints Formulation Topological The cycles formulation of the previous section was simple and computationally efficient. It only required the additional process of identifying the cycles in the cost-based graph. This could be easily done in time linear to the size of the graph for each cycle.
Reference: [51] <author> Igor Rivin and Ramin Zabih. </author> <title> An algebraic approach to constraint satisfaction problems. </title> <booktitle> In Proceedings of the IJCAI Conference, </booktitle> <year> 1989. </year>
Reference-contexts: In particular, we have in mind the following: * Solving Constraint Satisfaction Problems (CPS's) such as the n-queens problems through Diophantine equations <ref> [51] </ref>. * Solving the relaxation labeling process through a Simplex-like algorithm [76]. * Updating deductive databases though linear programming techniques [2]. * Reduction of independence-based MAPs in Bayesian networks to linear con straint satisfaction [69]. * The problem of path planning in robotics in relation to routing and trans portation problems
Reference: [52] <author> David E. Rumelhart and James L. McClelland. </author> <title> Parallel Distributed Processing: </title> <journal> Explorations in the Microstrctures of Cognition, </journal> <volume> Volume 1. </volume> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Thagard continues by refining the four possibilities into seven distinct principles. In this way, it is hoped that further gradations can be made in "best". Although a seemingly sound theory, it is rather complex. A connectionist <ref> [52, 15, 35] </ref> implementation has been attempted, however, its feasibility in applications seems questionable. 2.6 Other Approaches The above methods are directed mainly towards modeling abduction. Other approaches are certainly available.
Reference: [53] <author> Eugene Santos, Jr. </author> <title> Cost-based abduction and linear constraint satisfaction. </title> <type> Technical Report CS-91-13, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1991. </year>
Reference-contexts: Experimental results strongly indicate that our linear constraint satisfaction approach is quite promising. Studies comparing our approach against heuristic search techniques on existing abduction problems has shown our approach to be superior in both time and space, and actually exhibiting an expected polynomial run-time growth rate <ref> [53, 55, 54] </ref>. Our goal is to show that our framework is both flexible and powerful enough to solve interesting problems in abductive reasoning. With our linear constraint satisfaction approach, we can completely model existing approaches such as cost-based abduction [53, 55, 54] and belief revision [57]. <p> space, and actually exhibiting an expected polynomial run-time growth rate <ref> [53, 55, 54] </ref>. Our goal is to show that our framework is both flexible and powerful enough to solve interesting problems in abductive reasoning. With our linear constraint satisfaction approach, we can completely model existing approaches such as cost-based abduction [53, 55, 54] and belief revision [57]. Especially in the case of cost-based abduction, thorough experimentation has shown that our approach has now made a computationally difficult problem extremely feasible for extensive use in existing applications such as the wimp story comprehension system [6].
Reference: [54] <author> Eugene Santos, Jr. </author> <title> Cost-based abduction, linear constraint satisfaction, and alternative explanations. </title> <booktitle> In Proceedings of the AAAI Workshop on Abduction, </booktitle> <year> 1991. </year>
Reference-contexts: Experimental results strongly indicate that our linear constraint satisfaction approach is quite promising. Studies comparing our approach against heuristic search techniques on existing abduction problems has shown our approach to be superior in both time and space, and actually exhibiting an expected polynomial run-time growth rate <ref> [53, 55, 54] </ref>. Our goal is to show that our framework is both flexible and powerful enough to solve interesting problems in abductive reasoning. With our linear constraint satisfaction approach, we can completely model existing approaches such as cost-based abduction [53, 55, 54] and belief revision [57]. <p> space, and actually exhibiting an expected polynomial run-time growth rate <ref> [53, 55, 54] </ref>. Our goal is to show that our framework is both flexible and powerful enough to solve interesting problems in abductive reasoning. With our linear constraint satisfaction approach, we can completely model existing approaches such as cost-based abduction [53, 55, 54] and belief revision [57]. Especially in the case of cost-based abduction, thorough experimentation has shown that our approach has now made a computationally difficult problem extremely feasible for extensive use in existing applications such as the wimp story comprehension system [6]. <p> Furthermore, having the 2nd best, 3rd best, and so on, can provide a useful gauge on the quality of the best explanation. In this section, we present techniques for extracting alternative explanations in order of their associated costs <ref> [54, 57] </ref>. To generate the alternative explanations, we solve a sequence of constraint systems. This sequence consists of constraint systems each of which are derived from the constraint systems earlier in the sequence. The initial constraint system is the original constraint system which determines the first optimal solution. <p> Although this could be done by first transforming the Bayesian network into a cost-based abduction graph [68] and then transforming the graph into a constraint system, a more natural and straightforward method will be given below. (See also <ref> [54, 57] </ref>.) We will show how to directly transform a Bayesian network into an equivalent constraint system. 4.1.1 Constraints Formulation We first observe that a Bayesian network can be completely described by a finite collection of random variables and a finite set of conditional probabilities based 61 on the r.v.s. 19 <p> Unfortunately, as we mentioned earlier, message-passing schemes are incapable of generating beyond the second best. We now present a generation technique for Bayesian networks which is similar to the cutting plane approach in Section 3.7 <ref> [54, 54] </ref>. 67 Again, our approach is to solve a sequence of constraint systems. This se-quence consists of constraint systems each of which are derived from the constraint systems earlier in the sequence. The initial constraint system is the original constraint system which determines the first optimal solution.
Reference: [55] <author> Eugene Santos, Jr. </author> <title> A linear constraint satisfaction approach to cost-based abduction. </title> <note> Submitted to Artificial Intelligence Journal, </note> <year> 1991. </year>
Reference-contexts: Experimental results strongly indicate that our linear constraint satisfaction approach is quite promising. Studies comparing our approach against heuristic search techniques on existing abduction problems has shown our approach to be superior in both time and space, and actually exhibiting an expected polynomial run-time growth rate <ref> [53, 55, 54] </ref>. Our goal is to show that our framework is both flexible and powerful enough to solve interesting problems in abductive reasoning. With our linear constraint satisfaction approach, we can completely model existing approaches such as cost-based abduction [53, 55, 54] and belief revision [57]. <p> space, and actually exhibiting an expected polynomial run-time growth rate <ref> [53, 55, 54] </ref>. Our goal is to show that our framework is both flexible and powerful enough to solve interesting problems in abductive reasoning. With our linear constraint satisfaction approach, we can completely model existing approaches such as cost-based abduction [53, 55, 54] and belief revision [57]. Especially in the case of cost-based abduction, thorough experimentation has shown that our approach has now made a computationally difficult problem extremely feasible for extensive use in existing applications such as the wimp story comprehension system [6].
Reference: [56] <author> Eugene Santos, Jr. </author> <title> Modelling cyclicity and generalized cost-based abduction using linear constraint satisfaction. </title> <note> Submitted for publication, </note> <year> 1991. </year>
Reference-contexts: We call this problem and any "non-acyclic" knowledge bases, cyclicity. We can show that by using linear constraints, such issues are naturally handled within our framework <ref> [58, 56] </ref>. In Section 2, we briefly examine some of the existing frameworks for modeling abductive reasoning. We begin our linear constraints approach in Section 3 by modeling cost-based abduction. We will provide a detailed analysis of our approach plus extensive experimental data comparing it against existing graphical search solutions.
Reference: [57] <author> Eugene Santos, Jr. </author> <title> On the generation of alternative explanations with implications for belief revision. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: Our goal is to show that our framework is both flexible and powerful enough to solve interesting problems in abductive reasoning. With our linear constraint satisfaction approach, we can completely model existing approaches such as cost-based abduction [53, 55, 54] and belief revision <ref> [57] </ref>. Especially in the case of cost-based abduction, thorough experimentation has shown that our approach has now made a computationally difficult problem extremely feasible for extensive use in existing applications such as the wimp story comprehension system [6]. Furthermore, we consider some issues which remain unaddressed by the existing models. <p> Furthermore, having the 2nd best, 3rd best, and so on, can provide a useful gauge on the quality of the best explanation. In this section, we present techniques for extracting alternative explanations in order of their associated costs <ref> [54, 57] </ref>. To generate the alternative explanations, we solve a sequence of constraint systems. This sequence consists of constraint systems each of which are derived from the constraint systems earlier in the sequence. The initial constraint system is the original constraint system which determines the first optimal solution. <p> Although this could be done by first transforming the Bayesian network into a cost-based abduction graph [68] and then transforming the graph into a constraint system, a more natural and straightforward method will be given below. (See also <ref> [54, 57] </ref>.) We will show how to directly transform a Bayesian network into an equivalent constraint system. 4.1.1 Constraints Formulation We first observe that a Bayesian network can be completely described by a finite collection of random variables and a finite set of conditional probabilities based 61 on the r.v.s. 19
Reference: [58] <author> Eugene Santos, Jr. </author> <title> A linear constraint satisfaction approach to cyclicity. </title> <type> Technical Report CS-92-03, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1992. </year>
Reference-contexts: We call this problem and any "non-acyclic" knowledge bases, cyclicity. We can show that by using linear constraints, such issues are naturally handled within our framework <ref> [58, 56] </ref>. In Section 2, we briefly examine some of the existing frameworks for modeling abductive reasoning. We begin our linear constraints approach in Section 3 by modeling cost-based abduction. We will provide a detailed analysis of our approach plus extensive experimental data comparing it against existing graphical search solutions. <p> Just consider introducing (foo c) and other appropriate accoutrements. Obviously, we need a general solution to cyclicity in linear constraint satisfaction which could compactly capture the essence of our problem. 21 An earlier cycles formulation for generalized cost-based abduction can be found in <ref> [58] </ref> 103 Disallowing the presence of logical cycles in our solution graphs is tantamount to removing the associated 0-1 assignments from our 0-1 solution space.
Reference: [59] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley & Sons Ltd., </publisher> <year> 1986. </year>
Reference-contexts: Linear constraint satisfaction is a very well understood problem in Operations Research. Our reasoning engine is thus formed from highly efficient tools and techniques developed in OR. Such tools as the Simplex method and Kar-markar's projective scaling algorithm <ref> [36, 39, 59] </ref> provide us with a firm foundation to building a practical system. Experimental results strongly indicate that our linear constraint satisfaction approach is quite promising. <p> Taking the set of linear inequalities I and the objective function fi L , we observe that we have the elements known in operations research as a linear program <ref> [59, 36, 39] </ref>. The goal of a linear program is to minimize an objective function according to some set of linear constraints. Highly efficient methods such as the simplex method 7 and Karmarkar's projective scaling algorithm are used to solve linear programs [59, 36, 39]. <p> known in operations research as a linear program <ref> [59, 36, 39] </ref>. The goal of a linear program is to minimize an objective function according to some set of linear constraints. Highly efficient methods such as the simplex method 7 and Karmarkar's projective scaling algorithm are used to solve linear programs [59, 36, 39]. Empirical studies have shown that the running time of the simplex method is roughly linear with respect to the number of constraints and the number of variables in the linear program [39]. Proposition 3.4. <p> This is a standard technique used in many domains to speed up processing time. The basic idea is as follows: To find an optimal 0-1 solution, we solve a sequence of linear programs. This sequence can be represented by a tree where 8 See integer programming techniques <ref> [59, 36, 39] </ref>. 26 each node in the tree is identified with a linear program that is derived from the linear programs on the path leading to the root of the tree. The root of the tree is identified with the linear program induced by our constraint system. <p> Next, we choose one of the problems identified with an active node and attempt to solve it. It is not necessary to run a complete simplex method on the linear program. Using methods such as the dual simplex algorithm <ref> [59, 39] </ref>, information is utilized in an incremental manner from other runs resulting in a quick and efficient computation. If the optimal solution is not a 0-1 solution, then two new problems are defined based on the current linear program. <p> Branching continues in this manner until there are no active nodes in the tree. At the end, the current best solution is guaranteed to be the optimal 0-1 solution. This technique is generally classified as a branch and bound technique in the area of integer linear programming <ref> [59, 39] </ref>. Also, it can be applied to any con straint system. 27 Notation. We denote an active node by an ordered pair (I; l) where I is a set of linear constraints and l is the value of the optimal solution for the associated linear program. Algorithm 3.1. <p> Let L k := (; I k ; ). 6. Go to step 2. 7. (Solutions) Print s 1 ; s 2 ; . . . ; s k1 . The above method we have just described can be classified as a cutting plane method in operations research <ref> [36, 59, 39] </ref>. Since each derived constraint system differs only in an additional constraint from some previously solved problem, efficient incremental techniques such as the dual simplex method can be applied here in a fashion similar to the one used in the branch and bound algorithm. Theorem 3.11. <p> Let L k = (; I k ; ). 6. Go to step 2. 7. (Solutions) Print s 1 ; s 2 ; . . . ; s k1 . The above method can again be classified as a cutting plane method in operations research <ref> [36, 59, 39] </ref>. Likewise, since each derived constraint system differs only in an additional constraint from some previously solved problem, incremental computational methods such as the dual simplex method can be applied here in a fashion similar to the one used in the branch and bound algorithm. Theorem 4.12.
Reference: [60] <author> Bart Selman and Hector J. Levesque. </author> <title> Abductive and default reasoning: A computational core. </title> <booktitle> In Proceedings of the AAAI Conference, </booktitle> <pages> pages 343-348, </pages> <year> 1990. </year>
Reference-contexts: For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available.
Reference: [61] <author> Ross D. Shachter. </author> <title> Evaluating influence diagrams. </title> <journal> Operations Research, </journal> <volume> 36 </volume> <pages> 871-82, </pages> <year> 1986. </year>
Reference-contexts: Other approaches are certainly available. However, aside from the five methods we have just studied, the remaining ones handle abductive reasoning sort of as an afterthought to their main goals. Such systems include truth maintenance systems [12], influence diagrams <ref> [61] </ref>, probabilistic logic [40], Dempster-Shafer theory [13, 63] and fuzzy logic [75]. 12 2.7 Related Work There are also various strands of work which, while somewhat related to the work described in this thesis, are nevertheless sufficiently distant not to warrant a full fledged review.
Reference: [62] <author> Ross D. Shachter and Mark A. Peot. </author> <title> Simulation approaches to general probabilistic inference on belief networks. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1989. </year>
Reference-contexts: Through various tests, we have noticed that our method performs best on highly deterministic networks, that is, networks whose conditional probabilities are near 0 or 1. For belief updating, we compared our approach against stochastic simulation methods <ref> [72, 34, 28, 10, 24, 62] </ref>. We found that our approach performed much better than stochastic simulation on highly deterministic networks. Basically, as we discussed earlier, simulation approaches could very rarely come up with an answer on these types of networks.
Reference: [63] <author> G. Shafer. </author> <title> A Mathematical Theory of Evidence. </title> <publisher> Princeton University Press, </publisher> <year> 1976. </year>
Reference-contexts: Other approaches are certainly available. However, aside from the five methods we have just studied, the remaining ones handle abductive reasoning sort of as an afterthought to their main goals. Such systems include truth maintenance systems [12], influence diagrams [61], probabilistic logic [40], Dempster-Shafer theory <ref> [13, 63] </ref> and fuzzy logic [75]. 12 2.7 Related Work There are also various strands of work which, while somewhat related to the work described in this thesis, are nevertheless sufficiently distant not to warrant a full fledged review.
Reference: [64] <author> Murray Shanahan. </author> <title> Prediction is deduction but explanation is abduction. </title> <booktitle> In Proceeding of IJCAI Conference, </booktitle> <year> 1989. </year>
Reference-contexts: Thus, we need an approach to modeling abduction which is representa-tionally robust and permits a practical implementation. To our chagrin, however, it seems that abductive reasoning is an inherently difficult process. Indeed, various abductive models have been shown to be NP-hard <ref> [9, 41, 7, 64, 42] </ref>. To better understand the difficulty inherent in abduction, let us attempt to model John's situation above. <p> For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available.
Reference: [65] <author> Solomon E. Shimony. </author> <title> On irrelevance and partial assignments to belief networks. </title> <type> Technical Report CS-90-14, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1990. </year>
Reference-contexts: The focus for well-foundedness is simply the single set [ A2span (e) cond fl (A): The best explanation generated will be precisely the hypothesis which maximizes this focus. A similar notion of focusing in circumscribing explanations has previously been discussed in <ref> [65] </ref>. However, the main difference between our approach and the approach presented in [65] lies in the treatment of focusing. In [65], focusing is considered a special type of evidence called the (null) evidence. This type of evidence simply requires that the r.v.s specified must be instantiated. <p> A similar notion of focusing in circumscribing explanations has previously been discussed in <ref> [65] </ref>. However, the main difference between our approach and the approach presented in [65] lies in the treatment of focusing. In [65], focusing is considered a special type of evidence called the (null) evidence. This type of evidence simply requires that the r.v.s specified must be instantiated. <p> A similar notion of focusing in circumscribing explanations has previously been discussed in <ref> [65] </ref>. However, the main difference between our approach and the approach presented in [65] lies in the treatment of focusing. In [65], focusing is considered a special type of evidence called the (null) evidence. This type of evidence simply requires that the r.v.s specified must be instantiated. The approach then uses these (null) evidences and combines it with circumscribing explanation in the hopes of solving problems like the daydream above. <p> The approach then uses these (null) evidences and combines it with circumscribing explanation in the hopes of solving problems like the daydream above. Basically, 76 any r.v. which causally affects the special (null) evidence r.v. must be instantiated. However, as <ref> [65] </ref> points out, certain Bayesian networks do exist which results in counterintuitive solutions using their approach. For example, suppose we are interested in whether we will live or not which involves making the r.v. Dead into a (null) evidence. <p> It is a general approach which precisely defines the problem of computing incomplete instantiation-sets. As we also saw above, well-foundedness is a special type of focusing under our approach. Similarly, the approaches found in <ref> [65, 67, 66] </ref> can also be classified within our model. 77 4.2 Belief Updating In the previous sections, we have demonstrated how belief revision can be modeled using our linear constraint satisfaction approach.
Reference: [66] <author> Solomon E. Shimony. </author> <title> Algorithms for finding irrelevance-based map assignments to belief networks. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: In probabilistic terms, this seems to suggest that random variables which are "irrelevant" to the evidence are not instantiated. This approach of only instantiating a subset of the random variables is currently being explored by [41] and <ref> [7, 67, 66] </ref>. <p> It is a general approach which precisely defines the problem of computing incomplete instantiation-sets. As we also saw above, well-foundedness is a special type of focusing under our approach. Similarly, the approaches found in <ref> [65, 67, 66] </ref> can also be classified within our model. 77 4.2 Belief Updating In the previous sections, we have demonstrated how belief revision can be modeled using our linear constraint satisfaction approach.
Reference: [67] <author> Solomon E. Shimony. </author> <title> Explanation, irrelevance and statistical independence. </title> <booktitle> In Proceedings of the AAAI Conference, </booktitle> <year> 1991. </year>
Reference-contexts: In probabilistic terms, this seems to suggest that random variables which are "irrelevant" to the evidence are not instantiated. This approach of only instantiating a subset of the random variables is currently being explored by [41] and <ref> [7, 67, 66] </ref>. <p> It is a general approach which precisely defines the problem of computing incomplete instantiation-sets. As we also saw above, well-foundedness is a special type of focusing under our approach. Similarly, the approaches found in <ref> [65, 67, 66] </ref> can also be classified within our model. 77 4.2 Belief Updating In the previous sections, we have demonstrated how belief revision can be modeled using our linear constraint satisfaction approach.
Reference: [68] <author> Solomon E. Shimony and Eugene Charniak. </author> <title> A new algorithm for finding map assignments to belief networks. </title> <booktitle> In Proceedings of the Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: This measure is Pearl's most-probable explanation criterion (MPE). In this section, our goal is to apply our linear constraint satisfaction approach to belief revision in Bayesian networks. Although this could be done by first transforming the Bayesian network into a cost-based abduction graph <ref> [68] </ref> and then transforming the graph into a constraint system, a more natural and straightforward method will be given below. (See also [54, 57].) We will show how to directly transform a Bayesian network into an equivalent constraint system. 4.1.1 Constraints Formulation We first observe that a Bayesian network can be <p> For example, consider image processing where each pixel is represented by a r.v. [19]. 87 Yet, of the other existing algorithms for performing belief revision, namely Pearl's message-passing scheme [41] and Shimony and Charniak's cost-based approach <ref> [68] </ref>, message-passing is incapable of generating alternative explanations while the cost-based method suffers from the heuristic problems outlined in Section 3. Given this dearth of algorithms, we believe ours to be a plausible alternative.
Reference: [69] <author> Solomon Eyal Shimony. </author> <title> A Probabilistic Framework for Explanation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1991. </year>
Reference-contexts: the following: * Solving Constraint Satisfaction Problems (CPS's) such as the n-queens problems through Diophantine equations [51]. * Solving the relaxation labeling process through a Simplex-like algorithm [76]. * Updating deductive databases though linear programming techniques [2]. * Reduction of independence-based MAPs in Bayesian networks to linear con straint satisfaction <ref> [69] </ref>. * The problem of path planning in robotics in relation to routing and trans portation problems [49]. * Work done with distributed intelligent agents [37]. 13 3 Cost-Based Abduction Our basic approach towards knowledge representation involves a mapping from objects and/or propositions in the world to real variables.
Reference: [70] <author> E. H. Shortliffe. </author> <title> Computer-Based Medical Consultation: MYCIN. </title> <publisher> Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: Common approaches to this problem have advocated augmenting propositional logic with certainty factors, probabilities, costs, etc. in an attempt to preserve deduction. This was often used in classical expert systems such as MYCIN <ref> [70, 71] </ref>, PROSPECTOR [14] and INTERNIST [38, 46]. However, the resulting models were clumsy and restrictive. A case in point is Shortliffe's MYCIN system.
Reference: [71] <author> E. H. Shortliffe and B. G. Buchanan. </author> <title> A model of inexact reasoning in medicine. </title> <journal> Mathematical Biosciences, </journal> <volume> 23 </volume> <pages> 351-379, </pages> <year> 1975. </year>
Reference-contexts: Common approaches to this problem have advocated augmenting propositional logic with certainty factors, probabilities, costs, etc. in an attempt to preserve deduction. This was often used in classical expert systems such as MYCIN <ref> [70, 71] </ref>, PROSPECTOR [14] and INTERNIST [38, 46]. However, the resulting models were clumsy and restrictive. A case in point is Shortliffe's MYCIN system.
Reference: [72] <author> Sampath Srinivas and Jack Breese. </author> <title> Ideal: Influence diagram evaluation and analysis in lisp documentation and users guide. </title> <type> Technical Report Technical Memorandum No. 23, </type> <institution> Rockwell International Science Center, </institution> <address> Palo Alto, CA, </address> <year> 1989. </year>
Reference-contexts: Through various tests, we have noticed that our method performs best on highly deterministic networks, that is, networks whose conditional probabilities are near 0 or 1. For belief updating, we compared our approach against stochastic simulation methods <ref> [72, 34, 28, 10, 24, 62] </ref>. We found that our approach performed much better than stochastic simulation on highly deterministic networks. Basically, as we discussed earlier, simulation approaches could very rarely come up with an answer on these types of networks.
Reference: [73] <author> Mark E. Stickel. </author> <title> A prolog-like inference system for computing minimum-cost abductive explanations in natural-language interpretation. </title> <type> Technical Report Technical Note 451, </type> <institution> SRI International, </institution> <year> 1988. </year> <month> 119 </month>
Reference-contexts: For example, assuming that no one is home is a possible explanation for the house being dark and quiet. Abductive explanation has been formalized in AI as the process of searching for some set of assumptions that can prove the things to be explained <ref> [7, 26, 60, 64, 73, 31, 20, 43, 41] </ref>. We call each such set an explanation for the given evidence. A basic problem which naturally arises is that there maybe many different possible 3 For a good general discussion of abduction, see [29]. 2 explanations available. <p> This would serve to precisely define the notion of a best explanation as well as subsequent next best which is critical to have in domains such as medical diagnosis. Several ordering measures are available such as least specific abduction <ref> [26, 73] </ref>, cost-based abduction [7], parsimonious covering theory [43] and belief revision [41]. Each approach offers different perspectives on the problem and provides individual frameworks capable of modeling certain aspects of abductive reasoning. <p> Another simple approach is to designate some set of propositions as assumable. Thus, any set of hypotheses must consist only of assumable propositions. However, we often run into the problem of the explanations either being too detailed or not detailed enough. 2.1 Weighted Abduction Hobbs and Stickel <ref> [26, 73] </ref> proposed an approach called weighted abduction. It involves levying numerical costs to making individual assumptions. The cost of an explanation is a function of the cost of the individual assumptions made in the explanation. <p> Furthermore, the heuristic function must be admissible to guarantee that the first proof generated is the minimal cost proof. Efficient heuristics have been difficult to find. One of the few basic admissible heuristics that has been used is cost-so-far in <ref> [7, 73] </ref>. Simply put, the partial proofs are weighted according to the costs of the hypotheses which they currently contain. A much more efficient heuristic has been found which utilizes a more sophisticated cost estimator [4].
Reference: [74] <author> Paul Thagard. </author> <title> Explanatory coherence. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 12 </volume> <pages> 435-502, </pages> <year> 1989. </year>
Reference-contexts: With the addition of probabilities, care must be taken in choosing which covers are to be inspected. Peng and Reggia [43] proposed a 2-layer Bayesian network to probabilistically model their approach. However, extending their approach to more general problems is not readily obvious. 2.5 Coherence Thagard <ref> [74] </ref> proposed an approach for modeling explanation in general. Called explanatory coherence, the theory consists of several principles that establish relations of local coherence between a hypothesis and other propositions. Vaguely: 11 Propositions P and Q cohere if and only if there is some explanatory relation between them.
Reference: [75] <author> R. Yager. </author> <title> Using approximate reasoning to represent default knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 31 </volume> <pages> 99-112, </pages> <year> 1987. </year>
Reference-contexts: However, aside from the five methods we have just studied, the remaining ones handle abductive reasoning sort of as an afterthought to their main goals. Such systems include truth maintenance systems [12], influence diagrams [61], probabilistic logic [40], Dempster-Shafer theory [13, 63] and fuzzy logic <ref> [75] </ref>. 12 2.7 Related Work There are also various strands of work which, while somewhat related to the work described in this thesis, are nevertheless sufficiently distant not to warrant a full fledged review.
Reference: [76] <author> Xinhua Zhuang, Robert M. Haralick, and Hyonam Joo. </author> <title> A simplex-like algorithm for the relaxation labeling process. </title> <journal> IEEE Transactions on PAMI, </journal> <volume> 11(12) </volume> <pages> 1316-21, </pages> <year> 1989. </year>
Reference-contexts: In particular, we have in mind the following: * Solving Constraint Satisfaction Problems (CPS's) such as the n-queens problems through Diophantine equations [51]. * Solving the relaxation labeling process through a Simplex-like algorithm <ref> [76] </ref>. * Updating deductive databases though linear programming techniques [2]. * Reduction of independence-based MAPs in Bayesian networks to linear con straint satisfaction [69]. * The problem of path planning in robotics in relation to routing and trans portation problems [49]. * Work done with distributed intelligent agents [37]. 13 3 <p> A question naturally arises as to whether there is something special about our domain which causes this. Can we predict when a 0-1 solution will be the optimal solution? * There exists domain dependent methods for solving linear programming problems other than the general ones like Simplex <ref> [44, 30, 32, 48, 76] </ref>. These methods have been shown to outperform the general ones for their specific domains. For example, there are approximation methods for solving integer linear programming problems which have theoretically proven error bounds.
References-found: 76

