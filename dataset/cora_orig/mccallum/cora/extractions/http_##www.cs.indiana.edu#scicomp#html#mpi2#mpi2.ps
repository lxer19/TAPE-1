URL: http://www.cs.indiana.edu/scicomp/html/mpi2/mpi2.ps
Refering-URL: http://www.cs.indiana.edu/scicomp/html/mpi2/mpi2.html
Root-URL: http://www.cs.indiana.edu
Title: MPI Performance Comparison on Distributed and Shared Memory Machines MPI and memory performance compare on
Author: Tom Loos and Randall Bramley 
Note: How do  
Date: April 29, 1996  
Abstract: The widely implemented MPI Standard [10] defines primitives for point-to-point inter-processor communication (IPC), collective IPC, and synchronization based on message passing. The main reason to use a message passing standard is to ease the development, porting, and execution of applications on the variety of parallel computers that can support the paradigm, including shared memory, distributed memory, and shared memory array multiprocessors. This paper compares the SGI Power Challenge, a shared memory multiprocessor, with the Intel Paragon, a distributed memory machine. This paper addresses two questions: Memory and communications tests written in C++ using messages of double precision arrays show that both memory and MPI blocking IPC performance on the Power Challenge degrade once total message sizes grow larger than the second level cache. Comparing the MPI and memory performance curves indicate Power Challenge native MPI point-to-point communication is implemented using memory copying and synchronization is implemented using a binary tree algorithm. A model of blocking IPC for the SGI Power Challenge was developed, which is validated with performance results. A new measure of communications efficiency and overhead, the ratio of IPC time to memory copy time, is used to compare relative IPC performance for different machines. Comparison of Power Challenge and the Paragon show that the Paragon is more efficient for small messages, but the Power Challenge is better for large messages. Power Challenge observations do not generally correspond with those on the Paragon, indicating shared memory multiprocessor results should not be used to predict distributed memory multiprocessor performance. This suggests that parallel algorithms should not judged solely on their performance on one type of machine. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Casanova, J. Dongarra, and W. Jiang, </author> <title> The performance of pvm on mpp systems, </title> <type> Tech. Rep. </type> <institution> CS-95-301, University of Tennessee, Knoxville, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: sb S 10 (small) t llb + 10t sb + (S 10)t lb S &gt; 10 (large) Fig. 9 shows broadcast IPC performance at selected fixed message sizes over all CPUs along with model performance. 4 Intel Paragon Comparison The Paragon has been fully described in numerous sources, including [2], <ref> [1] </ref> and [7], and performance results are presented in the last 2 references. This section is not meant to fully duplicate those papers, but as a comparison with the Power Challenge. Fig. 10 shows MEMCPY-V2 (both versions) results run on one node and blocking IPC results.
Reference: [2] <institution> Center for Innovative Computer Applications, Indiana University, </institution> <note> A Guide to the Paragon XP/S-A7 Supercomputer at Indiana Univerity. http://www.cica.indiana.edu/iu hpc/paragon/paragon.text.html, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: The Intel Paragon is a distributed memory multiprocessor with the processors or nodes arranged in a two-dimensional mesh and no global memory <ref> [2] </ref>. The studied Paragon is a model XP/S-A7 with 92 general purpose and 4 I/O and service nodes connected in a 12 fi 8 mesh. <p> St sb S 10 (small) t llb + 10t sb + (S 10)t lb S &gt; 10 (large) Fig. 9 shows broadcast IPC performance at selected fixed message sizes over all CPUs along with model performance. 4 Intel Paragon Comparison The Paragon has been fully described in numerous sources, including <ref> [2] </ref>, [1] and [7], and performance results are presented in the last 2 references. This section is not meant to fully duplicate those papers, but as a comparison with the Power Challenge. Fig. 10 shows MEMCPY-V2 (both versions) results run on one node and blocking IPC results.
Reference: [3] <author> J. Dongarra and T. Dunigan, </author> <title> Message-passing performance of various computers, </title> <type> Tech. Rep. </type> <institution> CS-95-299, University of Tennessee, Knoxville, </institution> <month> August </month> <year> 1995. </year>
Reference: [4] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI: Portable Parallel Programming the Message-Passing Interface, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <note> first ed., </note> <year> 1994. </year>
Reference: [5] <author> B. Hendrickson and R. Leland, </author> <title> The Chaco User's Guide Version 1.0, </title> <type> Tech. Rep. SAND 93-2339, </type> <institution> Sandia National Laboratories, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: on the Power Challenge, which has 12 physical CPUs. 1 to 16 solver processes used the Bi-Congugate Gradient solver [11], block Jacobi global preconditioning, and ILU (0) local preconditioning to solve a test system of order 1 000 000 with 6:94 fi 10 6 non-zeroes partitioned with 32-way linear partitioning <ref> [5] </ref> to a residual tolerance of 10 10 . For example, when the HyperMatrix package uses 8 processes to solve the above system, it performs one synchronization and 2 non-blocking IPC transmissions with an average message size of about 77 500 doubles per process during initialization.
Reference: [6] <author> R. W. Hockney and C. R. Jesshope, </author> <title> Parallel Computers: Architecture, Programming, and Algorithms, </title> <editor> A. Hilger, </editor> <address> Bristol, UK, </address> <note> first ed., </note> <year> 1981. </year>
Reference-contexts: The model plots given in Figs. 4 and 5 are estimated bandwidth bw ec plots, where bw ec = 8S=t ec . The large message size model can be transformed into the pipelined operation model in <ref> [6, pp. 47-52] </ref> t ipcest = r 1 (S + n 1=2 ) , where n 1=2 = t l + (0:5s u )(t c + t i ) s u t m = :143 for the original test, and n 1=2 = :123 for the initializing test, and r 1
Reference: [7] <author> R. Berrendorf, et. al., </author> <title> Intel Paragon XP/S architecture, software environment, and performance, </title> <type> Tech. Rep. </type> <institution> KFA-ZAM-IB-9409, Central Institute for Applied Mathematics, Research Centre Julich (KFA), </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: 10 (small) t llb + 10t sb + (S 10)t lb S &gt; 10 (large) Fig. 9 shows broadcast IPC performance at selected fixed message sizes over all CPUs along with model performance. 4 Intel Paragon Comparison The Paragon has been fully described in numerous sources, including [2], [1] and <ref> [7] </ref>, and performance results are presented in the last 2 references. This section is not meant to fully duplicate those papers, but as a comparison with the Power Challenge. Fig. 10 shows MEMCPY-V2 (both versions) results run on one node and blocking IPC results.
Reference: [8] <author> E. Salo, </author> <title> personal correspondence. </title> <booktitle> Details on SGI's MPI implementation., </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: MPI Send () registers a send and waits on a semaphore until the IPC is complete. MPI Recv () waits for a registered send, gets a pointer to the source's data buffer, copies the data, and clears the source's semaphore <ref> [8] </ref>. Since the copying is done by one CPU, its memory has to accomodate both the source's and destination's data, so at most half of the destination's cache would be available for a message. The performance drop after the source initialized its array would be primarily from servicing cache misses.
Reference: [9] <author> Silicon Graphics Inc., </author> <title> The Power Challenge Technical Report, </title> <type> Tech. Rep. </type> <institution> http://www.sgi.com/Products/software/PDF/pwr-chlg, Silicon Graphics, Inc., </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The last two are collective operations utilizing all CPUs. 2 Test Environment The Power Challenge is a symmetric shared memory multiprocessor <ref> [9] </ref> equipped with either of two CPUs (the MIPS R8000 or MIPS R10000 chip) and up to 16 GB of global memory, connected by a 1.2 GB/second common bus that uses piggy-back reads. <p> The studied Power Challenge has 12 75 MHz R8000 chips and a three tier RAM hierarchy: a 16 KB cache on the integer unit, which is unused during floating point loads and stores <ref> [9, Ch. 3, p. 47] </ref>, a 4 MB second-level off-chip "data streaming" cache, and a 2 GB global shared memory. The Intel Paragon is a distributed memory multiprocessor with the processors or nodes arranged in a two-dimensional mesh and no global memory [2].
Reference: [10] <author> The Message Passing Interface Forum, </author> <title> MPI: A Message-Passing Interface Standard, </title> <type> Tech. Rep. </type> <institution> UT-CS-94-230, University of Tennessee, Knoxville, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The source CPU is responsible to assure the message is complete before writing in its buffer again by use of either MPI Wait () or MPI Test () <ref> [10, pp. 25-34] </ref>. The non-blocking IPC test had the destination process wait until it has received a message, but allowed the source process to immediately turn off the timer after posting the send.
Reference: [11] <author> H. van der Vorst, </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 13 (1992), </volume> <pages> pp. 631-644. </pages>
Reference-contexts: Fig. 16 shows the speedup per iteration results for single user runs of the HyperMatrix package on the Power Challenge, which has 12 physical CPUs. 1 to 16 solver processes used the Bi-Congugate Gradient solver <ref> [11] </ref>, block Jacobi global preconditioning, and ILU (0) local preconditioning to solve a test system of order 1 000 000 with 6:94 fi 10 6 non-zeroes partitioned with 32-way linear partitioning [5] to a residual tolerance of 10 10 .
Reference: [12] <author> W. Gropp and E. Lusk, </author> <title> User's Guide for mpich, a Portable Implementation of MPI. http://www.mcs.anl.gov/mpi/mpiuserguide/paper.html, February 1996. 8 Init., and Model Comparison 9 Comparison Messages 10 Comparison at Selected Values with Initialization Test Results IPC Test Source Time IPC Test - Dest. </title> <type> Bandwidth 11 12 </type>
Reference-contexts: Since n 1=2 is negative, this pipelined operation model is only suited for large message sizes. 3.4 Synchronization Tests Fig. 7 below shows MPI Barrier () performance on two to twelve CPUs averaged over n = 50000 observations along with estimated barrier cost values derived by analyzing the MPICH <ref> [12] </ref> MPI Barrier () implementation.
References-found: 12

