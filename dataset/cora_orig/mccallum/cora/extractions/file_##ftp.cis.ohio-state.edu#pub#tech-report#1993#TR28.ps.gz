URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1993/TR28.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Title: Scalable Architectures with k-ary n-cube cluster-c organization 1  
Author: Debashis Basak and Dhabaleswar K. Panda 
Keyword: parallel architectures, interconnection networks, clustered architec tures, hierarchical systems, wormhole-routing.  
Address: Columbus, OH 43210-1277  
Affiliation: Department of Computer and Information Science Ohio State University  
Email: Email: basak,panda@cis.ohio-state.edu  
Phone: Tel: (614)-292-5199  
Date: August 5, 1993  
Abstract: Recent advancements in VLSI and packaging technologies are making it cost effective to integrate multiple processing units into a chip or a board. This demonstrates attractiveness in building scalable parallel systems using clustered configurations while exploiting communication locality. Variety of clustered architectures, proposed in the past, using buses or MINs as the inter-cluster interconnection do not satisfy both the above objectives. This paper proposes a new class of k-ary n-cube cluster-c scalable architectures by combining the scalability of k-ary n-cube wormhole-routed networks with the cost-effectiveness of processor cluster designs. Each cluster consists of c processors and the possible intra-cluster interconnections can be k1-ary n1-cube direct network or indirect networks like MIN or bus-based. This paper focuses on direct cluster interconnection and analyzes the interplay between various parameters such as cluster size, inter- and intra-cluster topologies, channel widths, routing schemes, message length, and locality of communication. System throughput and average message latency are used to determine optimal configurations under the constant bisection bandwidth constraint. Our analysis indicates that small sized clusters with a ring intra-cluster topology and a 2D/3D/4D inter-cluster network connecting these clusters offer best system performance. This provides guidelines to system designers for building large-scale parallel systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dally William J., </author> <title> "Performance Analysis of k-ary n-cube Interconnection Networks." </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 39, No. 6, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: The analysis in [3] worked with fixed cluster sizes and did not consider the optimality of the network configuration, in terms of inter-cluster and intra-cluster network sizes and topologies. Research by Dally <ref> [1] </ref> has demonstrated the attractiveness of k-ary n-cube topology with wormhole routing switching technique. It has been shown that low dimensional (n) k-ary n-cube networks have potential to build scalable systems with low average message latency under physical wiring constraint. <p> Systems with buses as the inter-cluster network are not very scalable. A MIN-based inter-cluster network scales well but the locality of communication between adjacent clusters is not exploited. On the other hand, direct networks offer both these features. Since k-ary n-cube direct networks with wormhole routing <ref> [1] </ref> have been shown to be attractive for building scalable systems, it makes a good choice for the inter-cluster network. 2 2.1 Definition Let us formally introduce our k-ary n-cube cluster-c architecture. The k-ary n-cube represents the scalable inter-cluster network with k n clusters. <p> These properties depend on various parameters like cluster size, the intranet and the internet topologies, and the routing schemes used. Our objective is to select the configuration that offers best system performance. For a given hierarchical configuration, we use the constant bisection bandwidth constraint <ref> [1, 2] </ref> to determine channel widths in each network. The number of wires that need to cross a bisection of the network is called bisection width. In general, bisection width cannot be increased arbitrarily. <p> The number of wires that need to cross a bisection of the network is called bisection width. In general, bisection width cannot be increased arbitrarily. Factors like available layout area <ref> [1] </ref>, allowable system size, cost, and power considerations put limitations on the bisection width. In such cases, the bisection width can be held constant at some limit. This limit directly affects the channel width and indirectly determines message length in flits. 7 Consider a k-ary n-cube with W bit channels. <p> We refer to this problem as flit mismatch problem, since it arises because of mismatch in flit sizes in the two networks. We suggest the use of virtual channels with demand multiplexing to alleviate this flit mismatch problem. Virtual channels as proposed by Dally <ref> [1] </ref> are logical channels which are time-multiplexed over a physical link. Routing is done over the virtual channel network. The NAB scheme now works like this. We use the strategy of composing larger flits from smaller flits and then releasing them into the larger flit network (say internet).
Reference: [2] <author> Agarwal Anant, </author> <title> "Limits on Interconnection Network Performance." </title> <journal> IEEE Trans. on Parallel and Distributed Systems, Vol.2,No.4,Oct.91. </journal>
Reference-contexts: These properties depend on various parameters like cluster size, the intranet and the internet topologies, and the routing schemes used. Our objective is to select the configuration that offers best system performance. For a given hierarchical configuration, we use the constant bisection bandwidth constraint <ref> [1, 2] </ref> to determine channel widths in each network. The number of wires that need to cross a bisection of the network is called bisection width. In general, bisection width cannot be increased arbitrarily. <p> In such cases, the bisection width can be held constant at some limit. This limit directly affects the channel width and indirectly determines message length in flits. 7 Consider a k-ary n-cube with W bit channels. It has a bisection width of 4W k n1 <ref> [2] </ref>. With N being total number of nodes, this width becomes 4W N=k. For a linear array of processors, this width is 2W . For comparison across different topologies, we normalize bisection widths to that of a 2-ary n-cube with unit-width bidirectional channels. <p> However on clustered architectures one would expect communication patterns to be more localized. There are various ways of defining locality of communication of applications. We used the locality model developed by Agarwal in <ref> [2] </ref> in which a processor generates messages with equal probability to some nearest subset of processors.
Reference: [3] <author> Hsu W. & Yew P., </author> <title> "The Performance of Hierarchical Systems with wiring constraints." </title> <booktitle> Int. Conf. on Parallel Processing, </booktitle> <address> Aug.1991. </address>
Reference-contexts: Some examples include cluster of processors with buses [7], cluster of processors with crossbar switches [8], combination of crossbar and omega networks [9], local and global meshes [6], two-level systems based on hypercube and other network topologies [10, 11], and combination of intra-cluster bus and inter-cluster mesh/hypercube networks <ref> [3] </ref>. Two desired features in parallel architectures are scalability of the system and its ability to exploit the locality of communication inherent in most applications. Most of the above architectures using buses or MINs as the inter-cluster interconnection do not satisfy both the above objectives. <p> In the architectures supporting scalable inter-cluster interconnections, the focus has been restricted to 2D meshes or hypercubes as the inter-cluster topology with packet switched or circuit switched routing. The analysis in <ref> [3] </ref> worked with fixed cluster sizes and did not consider the optimality of the network configuration, in terms of inter-cluster and intra-cluster network sizes and topologies. Research by Dally [1] has demonstrated the attractiveness of k-ary n-cube topology with wormhole routing switching technique.
Reference: [4] <author> Hsu W. & Yew P., </author> " <title> The Impact of Wiring Constraints on Hierarchical Network Performance." </title> <booktitle> Int. Parallel Processing symp, </booktitle> <month> Mar </month> <year> 1992 </year>
Reference: [5] <author> Hsu W. & Yew P., </author> " <title> The Performance Evaluation of Wire-Limited Hierarchical Network." </title> <type> CSRD report no. 1179, </type> <month> Aug </month> <year> 1992 </year>
Reference: [6] <author> Carlson D., </author> <title> "The Mesh with a Global Mesh: a Flexible, High-speed Organization for Parallel Computation." </title> <booktitle> 1 st Int. Conf. on Supercomputer Systems, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1985. </year>
Reference-contexts: Some examples include cluster of processors with buses [7], cluster of processors with crossbar switches [8], combination of crossbar and omega networks [9], local and global meshes <ref> [6] </ref>, two-level systems based on hypercube and other network topologies [10, 11], and combination of intra-cluster bus and inter-cluster mesh/hypercube networks [3]. Two desired features in parallel architectures are scalability of the system and its ability to exploit the locality of communication inherent in most applications.
Reference: [7] <author> Wu S., & Liu M., </author> <title> "A Cluster Structure as an Interconnection Network for Large Multiprocessor Systems." </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-30, No. 4,Apr.1981. </volume> <pages> 21 </pages>
Reference-contexts: Such clusters can be interconnected together to build large-scale systems. A variety of hierarchical configurations have been proposed by researchers in the last decade to build scalable systems, using either single processor or processor cluster per node. Some examples include cluster of processors with buses <ref> [7] </ref>, cluster of processors with crossbar switches [8], combination of crossbar and omega networks [9], local and global meshes [6], two-level systems based on hypercube and other network topologies [10, 11], and combination of intra-cluster bus and inter-cluster mesh/hypercube networks [3].
Reference: [8] <author> Agrawal A., & Mahgoub I., </author> <title> "Performance Analysis of cluster-based supersystems." </title> <booktitle> 1st Int. Conf. on Supercomputer Systems, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1985. </year>
Reference-contexts: A variety of hierarchical configurations have been proposed by researchers in the last decade to build scalable systems, using either single processor or processor cluster per node. Some examples include cluster of processors with buses [7], cluster of processors with crossbar switches <ref> [8] </ref>, combination of crossbar and omega networks [9], local and global meshes [6], two-level systems based on hypercube and other network topologies [10, 11], and combination of intra-cluster bus and inter-cluster mesh/hypercube networks [3].
Reference: [9] <author> Kuck D.J., Kuhn R.H., Leasure B.R. and Sameh A., </author> <title> "Parallel Computing Today-the Cedar Approach." </title> <publisher> Science, Feb.1986. </publisher>
Reference-contexts: A variety of hierarchical configurations have been proposed by researchers in the last decade to build scalable systems, using either single processor or processor cluster per node. Some examples include cluster of processors with buses [7], cluster of processors with crossbar switches [8], combination of crossbar and omega networks <ref> [9] </ref>, local and global meshes [6], two-level systems based on hypercube and other network topologies [10, 11], and combination of intra-cluster bus and inter-cluster mesh/hypercube networks [3].
Reference: [10] <author> Dandamudi S. & Eager D., </author> <title> "Hierarchical Interconnection Networks for Multicomputer Systems." </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-39, No.6, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: Some examples include cluster of processors with buses [7], cluster of processors with crossbar switches [8], combination of crossbar and omega networks [9], local and global meshes [6], two-level systems based on hypercube and other network topologies <ref> [10, 11] </ref>, and combination of intra-cluster bus and inter-cluster mesh/hypercube networks [3]. Two desired features in parallel architectures are scalability of the system and its ability to exploit the locality of communication inherent in most applications.
Reference: [11] <author> Padmanabhan K., </author> <title> "Effective Architectures for Data Access in a Shared Memory Hierarchy." </title> <booktitle> Jour. of Parallel and Distributed Computing, </booktitle> <address> 11,1991. </address>
Reference-contexts: Some examples include cluster of processors with buses [7], cluster of processors with crossbar switches [8], combination of crossbar and omega networks [9], local and global meshes [6], two-level systems based on hypercube and other network topologies <ref> [10, 11] </ref>, and combination of intra-cluster bus and inter-cluster mesh/hypercube networks [3]. Two desired features in parallel architectures are scalability of the system and its ability to exploit the locality of communication inherent in most applications.
Reference: [12] <author> Mabbs S.A. </author> & <title> Forward K.E., "Performance Analysis of a Hierarchical Communication Architecture for a Shared-Memory Multiprocessor." </title> <type> Report No. </type> <month> 2 - Feb. </month> <year> 1990, </year> <institution> Dept. of Electrical and Electronic Engineering, The University of Melbourne. </institution>
Reference: [13] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: Each cluster consists of c processors and the possible intra-cluster interconnections can be k1-ary n1-cube direct network or indirect networks like MIN or bus-based. Such clustered organizations, using wormhole-routing as the switching technique, are becoming the trend for building scalable parallel systems. Example commercial systems are Intel Paragon <ref> [13] </ref>, Stanford DASH [14], and the KSR system [15]. In the Paragon system, a cluster consists of 2-4 processors connected by bus and the clusters are connected through a 2D mesh. The KSR system uses a ring-ring topology. The Stanford DASH system interconnects processor clusters through a mesh network. <p> In fact, many of the state-of-the-art commercial and research machines are already following this trend. For example, Intel's proposed Paragon <ref> [13] </ref> system is a k-ary 2-cube bus-5 organization with a mesh connecting 5 6 the clusters and each cluster having 4 application and 1 communication processor connected through a bus. A 64-processor DASH prototype, developed by Hennessy at Stanford [16], is a 4-ary 2-cube bus-5 system.
Reference: [14] <author> Lenoski D. et al, </author> <title> "The Stanford DASH Multiprocessor." </title> <booktitle> IEEE Computer 90, </booktitle> <pages> pages 63-79. </pages>
Reference-contexts: Such clustered organizations, using wormhole-routing as the switching technique, are becoming the trend for building scalable parallel systems. Example commercial systems are Intel Paragon [13], Stanford DASH <ref> [14] </ref>, and the KSR system [15]. In the Paragon system, a cluster consists of 2-4 processors connected by bus and the clusters are connected through a 2D mesh. The KSR system uses a ring-ring topology. The Stanford DASH system interconnects processor clusters through a mesh network.
Reference: [15] <institution> Kendall Square Research, </institution> <type> KSR Technical Summary, </type> <year> 1992. </year>
Reference-contexts: Such clustered organizations, using wormhole-routing as the switching technique, are becoming the trend for building scalable parallel systems. Example commercial systems are Intel Paragon [13], Stanford DASH [14], and the KSR system <ref> [15] </ref>. In the Paragon system, a cluster consists of 2-4 processors connected by bus and the clusters are connected through a 2D mesh. The KSR system uses a ring-ring topology. The Stanford DASH system interconnects processor clusters through a mesh network.
Reference: [16] <author> Lenoski D. et al, </author> <title> "The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor." </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pp 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For example, Intel's proposed Paragon [13] system is a k-ary 2-cube bus-5 organization with a mesh connecting 5 6 the clusters and each cluster having 4 application and 1 communication processor connected through a bus. A 64-processor DASH prototype, developed by Hennessy at Stanford <ref> [16] </ref>, is a 4-ary 2-cube bus-5 system. This system has clusters connected through two-layers of mesh network. The cluster consists of 4 processors and a directory-controller interconnected through a bus. The KSR machine is 32-ary 1-cube 32-ary 1-cube system. It is basically a 2-level hierarchy of rings of processors.
Reference: [17] <author> Asthana A., Jagdish H. and Mathews B., </author> <title> "Impact of Advanced VLSI packaging on the design of a large parallel computer." </title> <booktitle> 1989 Int. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Traditionally, research in interconnection networks have assumed one processing element per node. However advancements in VLSI and packaging technologies are making it cost-effective to integrate multiple processing elements into a chip, multiple chips into a board, and multiple boards into a system <ref> [17] </ref>. Any of these building blocks can represent a processor cluster. Such clusters can be interconnected together to build large-scale systems. A variety of hierarchical configurations have been proposed by researchers in the last decade to build scalable systems, using either single processor or processor cluster per node.
Reference: [18] <author> Chien A. A. and Kim J.H., </author> <title> "Planar Adaptive Routing: Low cost adaptive networks for multiprocessors." </title> <booktitle> Proc. 19 th Ann.Int.Symp.on Comp.Arch,Pgs 268-277,1992. </booktitle>
Reference-contexts: Both AB and NAB are general strategies in the sense that they do not specify the actual routing algorithm to be used in each network e.g. dimension-order, fully-adaptive [19], or planar <ref> [18] </ref> etc. For the NAB strategy proving deadlock freedom for these algorithms is not trivial as worms can simultaneously occupy channels in more than one network. The discussion on deadlock-freedom of hierarchical routing schemes is not presented here.
Reference: [19] <author> Duato J. </author> <title> "On the design of deadlock-free adaptive routing algorithms for multicomput-ers: </title> <booktitle> Theoretical aspects." PARLE 91:Parallel Architectures and Languages, </booktitle> <pages> pp 234-243. 22 </pages>
Reference-contexts: Both AB and NAB are general strategies in the sense that they do not specify the actual routing algorithm to be used in each network e.g. dimension-order, fully-adaptive <ref> [19] </ref>, or planar [18] etc. For the NAB strategy proving deadlock freedom for these algorithms is not trivial as worms can simultaneously occupy channels in more than one network. The discussion on deadlock-freedom of hierarchical routing schemes is not presented here.
References-found: 19

