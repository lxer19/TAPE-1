URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1997/tr-97-030.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1997.html
Root-URL: http://www.icsi.berkeley.edu
Title: Hybrid Approaches to Neural Network-based Language Processing  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Stefan Wermter 
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-97-030  
Abstract: In this paper we outline hybrid approaches to artificial neural network-based natural language processing. We start by motivating hybrid symbolic/connectionist processing. Then we suggest various types of symbolic/connectionist integration for language processing: connectionist structure architectures, hybrid transfer architectures, hybrid processing architectures. Furthermore, we focus particularly on loosely coupled, tightly coupled, and fully integrated hybrid processing architectures. We give particular examples of these hybrid processing architectures and argue that the hybrid approach to artificial neural network-based language processing has a lot of potential to overcome the gap between a neural level and a symbolic conceptual level. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bailey and J. Feldman and S. Narayanan and G. </author> <title> Lakoff. Modelling Embodied Lexical Development. </title> <booktitle> In Proceedings of the Meeting of the Cognitive Science Society, </booktitle> <address> Stanford, </address> <year> 1997. </year>
Reference-contexts: This model was developed around processing mechanisms like schemas and petri-networks which could also be transferred into structured connectionist networks. In related work Bailey developed a computational motor-control model of children's acquisition of verb semantics for words like push, pull <ref> [1] </ref>. This work supports the claim that the semantics of verb actions is grounded in sensory-motor primitives. Statistic Bayesian model merging is used for learning the task of verb semantics.
Reference: [2] <author> J. A. Barnden and K. J. Holyoak, </author> <title> editors. </title> <booktitle> Advances in connectionist and neural computation theory, </booktitle> <volume> volume 3. </volume> <publisher> Ablex Publishing Corporation, </publisher> <year> 1994. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56].
Reference: [3] <author> L. Bookman. </author> <title> Trajectories through knowledge space. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: There are modular network architectures for case role assignment in Gestalt-networks [52], confluent dual RAAM-networks for translation [5], the mixture of expert networks for hierarchical control [28] and the two-tier architecture for corpus analysis <ref> [3] </ref>. All this recent work contains various forms of modularity and structuring within an overall connectionist architecture. Furthermore, symbolic interpretation of the results is possible, at least at certain nodes. 4 Hybrid transfer architectures Hybrid transfer architectures transfer symbolic representations into connectionist representations or vice versa.
Reference: [4] <author> Y. Cheng, P. Fortier, and Y. Normandin. </author> <title> A system integrating connectionist and symbolic approaches for spoken language understanding. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <pages> pages 1511-1514, </pages> <address> Yokohama, </address> <year> 1994. </year>
Reference-contexts: On the other hand, this simple sequential sequence of symbolic and connectionist processing does not support feedback mechanisms. There are several other loosely coupled hybrid processing architectures. For instance, in the SICSA system, connectionist recurrent networks and symbolic case role parsing are combined for semantic analysis of database queries <ref> [4] </ref>. In BerP, connectionist feedforward networks for speech processing and symbolic dialog understanding are combined in a spoken 14 language dialog system [30, 65]. In FeasPar, connectionist feedforward networks for feature assignment are combined with symbolic search for finding better analysis results.
Reference: [5] <author> L. Chrisman. </author> <title> Learning recursive distributed representations for holistic computation. </title> <journal> Connection Science, </journal> <volume> 3(4) </volume> <pages> 345-365, </pages> <year> 1991. </year>
Reference-contexts: Furthermore, it is possible to interpret the distributed representations symbolically. There are many more connectionist architectures which allow a symbolic interpretation for instance at the output layers of their network. There are modular network architectures for case role assignment in Gestalt-networks [52], confluent dual RAAM-networks for translation <ref> [5] </ref>, the mixture of expert networks for hierarchical control [28] and the two-tier architecture for corpus analysis [3]. All this recent work contains various forms of modularity and structuring within an overall connectionist architecture.
Reference: [6] <author> R. Cooper and B. Franks. </author> <title> How hybrid should a hybrid model be? In Proceedings of the ECAI Workshop on Combining Symbolic and Connectionist Processing, </title> <address> pages 59-66, Amsterdam, </address> <year> 1994. </year>
Reference: [7] <author> M. W. Craven and J. W. Shavlik. </author> <title> Using sampling and queries to extract rules from trained neural networks. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <pages> pages 37-45, </pages> <institution> Rutgers University, </institution> <year> 1994. </year>
Reference-contexts: The activation-based extraction of automata is just one example of a hybrid transfer architecture and there are several further possibilities. For instance, a weight-based transfer between symbolic rules and feedforward networks has been extensively examined in knowl 11 edge based artificial neural networks (KBANN) <ref> [50, 7] </ref>. This weight-based transfer uses the weights rather than the activations as the main knowledge source for induction and extraction. While an activation-based transfer is based on the activations of certain units a weight-based transfer focuses on a more detailed weight level.
Reference: [8] <author> J. Diederich and D. L. </author> <title> Long. Efficient question answering in a hybrid system. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Singapore, </address> <year> 1992. </year>
Reference-contexts: Much early work on structured connectionism can be traced back to work by Feldman and Ballard, who provided a general framework of structured connectionism [15]. This framework was extended for natural language processing in many different directions, for instance for parsing [12, 13, 46], language acquisition [14, 25], explanation <ref> [8] </ref>, and semantic processing [26, 47]. More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior [16, 49].
Reference: [9] <author> G. Dorffner. </author> <title> Neural Networks and a New AI. </title> <publisher> Chapman and Hall, </publisher> <address> London, UK, </address> <year> 1996. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56].
Reference: [10] <author> M. G. Dyer. </author> <title> Distributed symbol formation and processing in connectionist networks. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 215-239, </pages> <year> 1990. </year>
Reference-contexts: A good example of a connectionist structure architecture of this modular distributed category was developed for automatic feature acquisition. Connectionist architectures can provide an automatic formation of distributed representations for input and output (e.g., <ref> [10, 11, 38] </ref>) which have the potential to increase robustness and facilitate learning. One way of building distributed input representations is by determining them dynamically during the learning process. <p> A distributed connectionist representation of a symbol is held in a global lexicon and is updated automatically based on its relationships to the distributed representations of other lexical entries. This update of the representation has been called symbolic recirculation <ref> [10, 38] </ref> since the representations are constantly changed and recir 8 culated within the architecture during learning. Symbolic recirculation can be used within a single network, for instance a feedforward network, but symbolic recirculation can also be used within a larger architecture.
Reference: [11] <author> M. G. Dyer. </author> <title> Symbolic neuroengineering for natural language processing: a multilevel research approach. </title> <editor> In J. A. Barnden and J. B. Pollack, editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, Vol.1: High Level Connectionist Models, </booktitle> <pages> pages 32-86. </pages> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, NJ, </address> <year> 1991. </year> <month> 23 </month>
Reference-contexts: Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language <ref> [51, 11, 27, 56] </ref>. However, since human language capabilities are based on real neural networks in the brain, artificial neural networks (also called connectionist networks) provide one essential starting point for modeling language processing. <p> A good example of a connectionist structure architecture of this modular distributed category was developed for automatic feature acquisition. Connectionist architectures can provide an automatic formation of distributed representations for input and output (e.g., <ref> [10, 11, 38] </ref>) which have the potential to increase robustness and facilitate learning. One way of building distributed input representations is by determining them dynamically during the learning process.
Reference: [12] <author> M. Fanty. </author> <title> Context-free parsing in connectionist networks. </title> <type> Technical Report 174, </type> <institution> Uni--versity of Rochester, Rochester, </institution> <address> NY, </address> <year> 1985. </year>
Reference-contexts: Much early work on structured connectionism can be traced back to work by Feldman and Ballard, who provided a general framework of structured connectionism [15]. This framework was extended for natural language processing in many different directions, for instance for parsing <ref> [12, 13, 46] </ref>, language acquisition [14, 25], explanation [8], and semantic processing [26, 47]. More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior [16, 49].
Reference: [13] <author> M. A. Fanty. </author> <title> Learning in structured connectionist networks. </title> <type> Technical Report 252, </type> <institution> University of Rochester, Rochester, </institution> <address> NY, </address> <year> 1988. </year>
Reference-contexts: Much early work on structured connectionism can be traced back to work by Feldman and Ballard, who provided a general framework of structured connectionism [15]. This framework was extended for natural language processing in many different directions, for instance for parsing <ref> [12, 13, 46] </ref>, language acquisition [14, 25], explanation [8], and semantic processing [26, 47]. More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior [16, 49].
Reference: [14] <author> J. Feldman. </author> <title> Structured connectionist models and language learning. </title> <journal> Artificial Intelligence Review, </journal> <volume> 7(5) </volume> <pages> 301-312, </pages> <year> 1993. </year>
Reference-contexts: Much early work on structured connectionism can be traced back to work by Feldman and Ballard, who provided a general framework of structured connectionism [15]. This framework was extended for natural language processing in many different directions, for instance for parsing [12, 13, 46], language acquisition <ref> [14, 25] </ref>, explanation [8], and semantic processing [26, 47]. More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior [16, 49].
Reference: [15] <author> J. A. Feldman and D. H. Ballard. </author> <title> Connectionist models and their properties. </title> <journal> Cognitive Science, </journal> <volume> 6 </volume> <pages> 205-254, </pages> <year> 1982. </year>
Reference-contexts: Knowledge of some task is built into the connectionist structure architecture, and a symbolic interpretation is assigned by an observer. Much early work on structured connectionism can be traced back to work by Feldman and Ballard, who provided a general framework of structured connectionism <ref> [15] </ref>. This framework was extended for natural language processing in many different directions, for instance for parsing [12, 13, 46], language acquisition [14, 25], explanation [8], and semantic processing [26, 47].
Reference: [16] <author> J. A. Feldman, G. Lakoff, D. R. Bailey, S. Narayanan, T. Regier, and A. Stolcke. </author> <title> L 0 - the first five years of an automated language acquisition project. </title> <journal> AI Review, </journal> <volume> 8, </volume> <year> 1996. </year>
Reference-contexts: More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior <ref> [16, 49] </ref>. The NTL framework is challenging since it tries to study neural processing mechanisms for high conceptual cognitive processing like embodied semantics, reasoning, metaphor interpretation, etc. 4 In order to attack such a challenging task it is necessary to focus on different processing mechanisms and certain representative problems.
Reference: [17] <author> L. Fu. </author> <title> Rule generation from neural networks. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 24 </volume> <pages> 1114-1124, </pages> <year> 1994. </year>
Reference: [18] <author> S. I. Gallant. </author> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56].
Reference: [19] <author> C. Lee Giles and C. W. Omlin. </author> <title> Extraction, insertion and refinement of symbolic rules in dynamically driven recurrent neural networks. </title> <journal> Connection Science, </journal> <volume> 5 </volume> <pages> 307-337, </pages> <year> 1993. </year>
Reference-contexts: At the end of this section we will refer to additional approaches using hybrid transfer architectures. One major problem for hybrid transfer architectures is that the architecture itself has to support the transfer. A good example is the work on activation-based automata extraction from recurrent networks <ref> [19, 42, 43] </ref>. First simple finite state automata are used for generating training examples of a given regular grammar.
Reference: [20] <author> S. Goonatilake and S. Khebbal. </author> <title> Intelligent Hybrid Systems. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1995. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56].
Reference: [21] <author> D. Grannes and L. Shastri and S. Narayanan and J. Feldman. </author> <title> A connectionist encoding of schemas and reactive plans. </title> <booktitle> In Proceedings of the Meeting of the Cognitive Science Society, </booktitle> <address> Stanford, </address> <year> 1997. </year>
Reference-contexts: While these two related models for embodied event modeling can be viewed at a computational level, there is more neurally inspired work for event modeling and temporal processing at a connectionist level as well (e.g. <ref> [49, 21] </ref>. For instance, schemas from a computational level can be modeled using structured connectionist terms from the SHRUTI system which was previously developed by Shastri and Ajjanagadde [21]. A particularly interesting recent work is a model of rapid memory formation in the Hippocampal System [49]. <p> For instance, schemas from a computational level can be modeled using structured connectionist terms from the SHRUTI system which was previously developed by Shastri and Ajjanagadde <ref> [21] </ref>. A particularly interesting recent work is a model of rapid memory formation in the Hippocampal System [49].
Reference: [22] <author> J. Hendler. </author> <title> Developing hybrid symbolic/connectionist models. </title> <editor> In J. A. Barnden and J. B. Pollack, editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, Vol.1: High Level Connectionist Models, </booktitle> <pages> pages 165-179. </pages> <publisher> Ablex Publishing Corporation, </publisher> <address> Norwood, NJ, </address> <year> 1991. </year>
Reference-contexts: During the process of parsing, control is switched back and forth between these two modules, but processing is confined to a single module at a time. Therefore, a tightly coupled hybrid architecture has the potential for feedback to and from modules. Other tightly coupled hybrid processing architectures include <ref> [23, 22] </ref> where the control changes between symbolic marker passing and connectionist similarity determination. In ProPars two connectionist feedforward networks are loosely coupled with a symbolic module 15 for syntactic shift reduce parsing [45].
Reference: [23] <author> J. A. Hendler. </author> <title> Marker passing over microfeatures: towards a hybrid symbolic connectionist model. </title> <journal> Cognitive Science, </journal> <volume> 13 </volume> <pages> 79-106, </pages> <year> 1989. </year>
Reference-contexts: During the process of parsing, control is switched back and forth between these two modules, but processing is confined to a single module at a time. Therefore, a tightly coupled hybrid architecture has the potential for feedback to and from modules. Other tightly coupled hybrid processing architectures include <ref> [23, 22] </ref> where the control changes between symbolic marker passing and connectionist similarity determination. In ProPars two connectionist feedforward networks are loosely coupled with a symbolic module 15 for syntactic shift reduce parsing [45].
Reference: [24] <author> M. Hilario. </author> <title> An overview of strategies for neurosymbolic integration. </title> <booktitle> In Proceedings of the Workshop on Connectionist-Symbolic Integration: From Unified to Hybrid Approaches, </booktitle> <pages> pages 1-6, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: Previous characterizations of architectures have covered certain specific connectionist architectures, for instance recurrent networks [31, 39], or they have covered expert systems/knowledge-based systems <ref> [37, 24, 55] </ref>. In contrast, here we will concentrate on various types of hybrid connectionist natural language processing. In figure 1 there is an overview of different possibilities for integration in natural language processing. Continuous connectionist representations are represented by a circle, discrete symbolic representations by a square.
Reference: [25] <author> S. Hollbach Weber and A. Stolcke. L0: </author> <title> A testbed for miniature language acquisition. </title> <type> Technical Report TR-90-010, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <year> 1990. </year>
Reference-contexts: Much early work on structured connectionism can be traced back to work by Feldman and Ballard, who provided a general framework of structured connectionism [15]. This framework was extended for natural language processing in many different directions, for instance for parsing [12, 13, 46], language acquisition <ref> [14, 25] </ref>, explanation [8], and semantic processing [26, 47]. More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior [16, 49].
Reference: [26] <author> S. Hollbach Weber. </author> <title> A structured connectionist approach to direct inferences and figurative adjective noun combination. </title> <type> Technical Report 289, </type> <institution> University of Rochester, Rochester, </institution> <address> NY, </address> <year> 1989. </year> <month> 24 </month>
Reference-contexts: This framework was extended for natural language processing in many different directions, for instance for parsing [12, 13, 46], language acquisition [14, 25], explanation [8], and semantic processing <ref> [26, 47] </ref>. More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior [16, 49].
Reference: [27] <author> V. Honavar and L. Uhr. </author> <title> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration. </title> <publisher> Academic Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language <ref> [51, 11, 27, 56] </ref>. However, since human language capabilities are based on real neural networks in the brain, artificial neural networks (also called connectionist networks) provide one essential starting point for modeling language processing.
Reference: [28] <author> R. A. Jacobs, M. I. Jordan, and A. G. Barto. </author> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 219-250, </pages> <year> 1991. </year>
Reference-contexts: There are many more connectionist architectures which allow a symbolic interpretation for instance at the output layers of their network. There are modular network architectures for case role assignment in Gestalt-networks [52], confluent dual RAAM-networks for translation [5], the mixture of expert networks for hierarchical control <ref> [28] </ref> and the two-tier architecture for corpus analysis [3]. All this recent work contains various forms of modularity and structuring within an overall connectionist architecture.
Reference: [29] <author> A. N. Jain. </author> <title> Parsec: A connectionist learning architecture for parsing spoken language. </title> <type> Technical Report CMU-CS-91-208, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year>
Reference-contexts: While all modules have a symbolically interpretable interface the connectionist or symbolic techniques are hidden within the modules. Other integrated architectures can be found in PARSEC, a system for language analysis in the conference registration domain <ref> [29] </ref>, which uses Jordan networks to trigger symbolic transformations within the modules, and CONNCERT [64], which has been developed for technical control problems using a mixture of expert networks controlled by symbolic supervisors. 6 Summary and Conclusions Recently there has been an increasing interest in hybrid symbolic/connectionist interpretation, combination and integration.
Reference: [30] <author> D. Jurafsky, C. Wooters, G. Tajchman, J. Segal, A. Stolcke, E. Fosler, and N. Morgan. </author> <title> The Berkeley Restaurant Project. </title> <booktitle> In Proceedings of the International Conference on Speech and Language Processing, </booktitle> <pages> pages 2139-2142, </pages> <address> Yokohama, </address> <year> 1994. </year>
Reference-contexts: For instance, in the SICSA system, connectionist recurrent networks and symbolic case role parsing are combined for semantic analysis of database queries [4]. In BerP, connectionist feedforward networks for speech processing and symbolic dialog understanding are combined in a spoken 14 language dialog system <ref> [30, 65] </ref>. In FeasPar, connectionist feedforward networks for feature assignment are combined with symbolic search for finding better analysis results.
Reference: [31] <author> S. C. Kremer. </author> <title> A theory of grammatical induction in the connectionist paradigm. </title> <type> Technical Report PhD dissertation, </type> <institution> Dept. of Computing Science, </institution> <year> 1996. </year>
Reference-contexts: Previous characterizations of architectures have covered certain specific connectionist architectures, for instance recurrent networks <ref> [31, 39] </ref>, or they have covered expert systems/knowledge-based systems [37, 24, 55]. In contrast, here we will concentrate on various types of hybrid connectionist natural language processing. In figure 1 there is an overview of different possibilities for integration in natural language processing.
Reference: [32] <author> S. C. Kwasny and K. A. Faisal. </author> <title> Connectionism and determinism in a syntactic parser. </title> <editor> In N. Sharkey, editor, </editor> <booktitle> Connectionist natural language processing, </booktitle> <pages> pages 119-162. </pages> <publisher> Lawrence Erlbaum, </publisher> <year> 1992. </year>
Reference-contexts: Processing is still a single module at a time, but the result of a connectionist module can have a direct influence on a symbolic module (or vice versa) before it finishes its global processing. For instance, CDP is a system for connectionist deterministic parsing <ref> [32] </ref>. While the choice of the next action is performed in a connectionist feedforward network, the action itself is performed in a symbolic module (see figure 8).
Reference: [33] <author> R. Maclin. </author> <title> Learning from Instruction and Experience: Methods for Incorporating Procedural Domain Theories into Knowledge-Based Neural Networks. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, University of Wisconsin-Madison, </institution> <year> 1995. </year> <note> (Also appears as UW Technical Report CS-TR-95-1285). </note>
Reference-contexts: In further work, activation-based transfer between context-free rules and feedforward networks is described in a symbolic manipulator for context-free languages [40], and another form of weight-based insertion of symbolic rules has been proposed for recurrent networks <ref> [33] </ref>. Extraction of symbolic rules can also be based on multiplicative networks which control the association of conditions and actions [35, 34]. 5 Hybrid processing architectures Hybrid transfer architectures do not apply symbolic and connectionist knowledge simultaneously to combine the complementary advantages of each representation in solving a given task.
Reference: [34] <author> C. McMillan, M. Mozer, and P. Smolensky. </author> <title> Dynamic conflict resolution in a connectionist rule-based system. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1366-1371, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: Extraction of symbolic rules can also be based on multiplicative networks which control the association of conditions and actions <ref> [35, 34] </ref>. 5 Hybrid processing architectures Hybrid transfer architectures do not apply symbolic and connectionist knowledge simultaneously to combine the complementary advantages of each representation in solving a given task. Hybrid transfer architectures just transfer symbolic and connectionist representations into each other.
Reference: [35] <author> C. McMillan, M. C. Mozer, and P. Smolensky. </author> <title> The connectionist scientist game: rule extraction and refinement in a neural network. </title> <booktitle> In Proceedings of the 13th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 424-430, </pages> <year> 1991. </year>
Reference-contexts: Extraction of symbolic rules can also be based on multiplicative networks which control the association of conditions and actions <ref> [35, 34] </ref>. 5 Hybrid processing architectures Hybrid transfer architectures do not apply symbolic and connectionist knowledge simultaneously to combine the complementary advantages of each representation in solving a given task. Hybrid transfer architectures just transfer symbolic and connectionist representations into each other.
Reference: [36] <author> L. R. Medsker. </author> <title> Hybrid Neural Network and Expert Systems. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56].
Reference: [37] <author> L. R. Medsker. </author> <title> Hybrid Intelligent Systems. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1995. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56]. <p> Previous characterizations of architectures have covered certain specific connectionist architectures, for instance recurrent networks [31, 39], or they have covered expert systems/knowledge-based systems <ref> [37, 24, 55] </ref>. In contrast, here we will concentrate on various types of hybrid connectionist natural language processing. In figure 1 there is an overview of different possibilities for integration in natural language processing. Continuous connectionist representations are represented by a circle, discrete symbolic representations by a square.
Reference: [38] <author> R. Miikkulainen. </author> <title> Subsymbolic Natural Language Processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56]. <p> A good example of a connectionist structure architecture of this modular distributed category was developed for automatic feature acquisition. Connectionist architectures can provide an automatic formation of distributed representations for input and output (e.g., <ref> [10, 11, 38] </ref>) which have the potential to increase robustness and facilitate learning. One way of building distributed input representations is by determining them dynamically during the learning process. <p> A distributed connectionist representation of a symbol is held in a global lexicon and is updated automatically based on its relationships to the distributed representations of other lexical entries. This update of the representation has been called symbolic recirculation <ref> [10, 38] </ref> since the representations are constantly changed and recir 8 culated within the architecture during learning. Symbolic recirculation can be used within a single network, for instance a feedforward network, but symbolic recirculation can also be used within a larger architecture.
Reference: [39] <author> M. C. Mozer. </author> <title> Neural net architectures for temporal sequence processing. </title> <editor> In A. Weigend and N. Gershenfeld, editors, </editor> <title> Time series prediction: </title> <booktitle> Forecasting the future and understanding the past, </booktitle> <pages> pages 243-264. </pages> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1993. </year>
Reference-contexts: Previous characterizations of architectures have covered certain specific connectionist architectures, for instance recurrent networks <ref> [31, 39] </ref>, or they have covered expert systems/knowledge-based systems [37, 24, 55]. In contrast, here we will concentrate on various types of hybrid connectionist natural language processing. In figure 1 there is an overview of different possibilities for integration in natural language processing.
Reference: [40] <author> M. C. Mozer and S. Das. </author> <title> A connectionist symbol manipulator that discovers the structure of context-free languages. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 863-870. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year> <month> 25 </month>
Reference-contexts: In further work, activation-based transfer between context-free rules and feedforward networks is described in a symbolic manipulator for context-free languages <ref> [40] </ref>, and another form of weight-based insertion of symbolic rules has been proposed for recurrent networks [33].
Reference: [41] <author> S. Narayanan. </author> <title> Talking the talk is like walking the walk: a computational model of verbal aspect. </title> <booktitle> In Proceedings of the Meeting of the Cognitive Science Society, </booktitle> <address> Stanford, </address> <year> 1997. </year>
Reference-contexts: One such problem is verbal aspect which describes the temporal character of events (like past tense or progressive form). Narayanan has developed and implemented a computational model of verbal aspect and he argues that the semantics of aspect is grounded in sensory-motor primitives <ref> [41] </ref>. This model was developed around processing mechanisms like schemas and petri-networks which could also be transferred into structured connectionist networks. In related work Bailey developed a computational motor-control model of children's acquisition of verb semantics for words like push, pull [1].
Reference: [42] <author> C. W. Omlin and C. L. Giles. </author> <title> Extraction and insertion of symbolic information in recurrent neural networks. </title> <editor> In V. Honavar and L. Uhr, editors, </editor> <booktitle> Artificial Intelligence and Neural Networks: Steps towards principled Integration, </booktitle> <pages> pages 271-299. </pages> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: At the end of this section we will refer to additional approaches using hybrid transfer architectures. One major problem for hybrid transfer architectures is that the architecture itself has to support the transfer. A good example is the work on activation-based automata extraction from recurrent networks <ref> [19, 42, 43] </ref>. First simple finite state automata are used for generating training examples of a given regular grammar.
Reference: [43] <author> C. W. Omlin and C. L. Giles. </author> <title> Extraction of rules from discrete-time recurrent neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 9(1) </volume> <pages> 41-52, </pages> <year> 1996. </year>
Reference-contexts: At the end of this section we will refer to additional approaches using hybrid transfer architectures. One major problem for hybrid transfer architectures is that the architecture itself has to support the transfer. A good example is the work on activation-based automata extraction from recurrent networks <ref> [19, 42, 43] </ref>. First simple finite state automata are used for generating training examples of a given regular grammar.
Reference: [44] <author> J. B. Pollack. </author> <title> On connectionist models of natural language processing. </title> <type> Technical Report PhD thesis, Technical Report MCCS-87-100, </type> <institution> New Mexico State University, </institution> <address> Las Cruces, NM, </address> <year> 1987. </year>
Reference-contexts: In a loosely coupled hybrid architecture processing has to be completed in one module before the next module can begin. For instance, the WP model <ref> [57, 44] </ref> for phenomenologically plausible parsing used a symbolic chart parser to construct syntactic localist networks which were also connected to semantic networks. Although the network itself may be viewed as a purely connectionist architecture, the overall architecture is a hybrid processing architecture.
Reference: [45] <author> T. Polzin. </author> <title> Parsing spontaneous speech: a hybrid approach. </title> <booktitle> In Proceedings of the ECAI Workshop on Combining Symbolic and Connectionist Processing, </booktitle> <pages> pages 104-113, </pages> <address> Amsterdam, </address> <year> 1994. </year>
Reference-contexts: Other tightly coupled hybrid processing architectures include [23, 22] where the control changes between symbolic marker passing and connectionist similarity determination. In ProPars two connectionist feedforward networks are loosely coupled with a symbolic module 15 for syntactic shift reduce parsing <ref> [45] </ref>. These tightly coupled hybrid processing architectures interleave symbolic and connectionist processing at the module level and allow the control switch between these different modules. Therefore, this tight coupling has the potential for more powerful interactions.
Reference: [46] <author> J. E. Rager. </author> <title> Self-correcting connectionist parsing. </title> <editor> In R. G. Reilly and N. E. Sharkey, editors, </editor> <title> Connectionist Approaches to Natural Language Processing. </title> <publisher> Erlbaum, </publisher> <year> 1992. </year>
Reference-contexts: Much early work on structured connectionism can be traced back to work by Feldman and Ballard, who provided a general framework of structured connectionism [15]. This framework was extended for natural language processing in many different directions, for instance for parsing <ref> [12, 13, 46] </ref>, language acquisition [14, 25], explanation [8], and semantic processing [26, 47]. More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior [16, 49].
Reference: [47] <author> T. Regier. </author> <title> The acquisition of lexical semantics for spatial terms: a connectionist model of perceptual categorization. </title> <type> Technical Report Technical Report, </type> <institution> International Computer Science Institute, </institution> <year> 1992. </year>
Reference-contexts: This framework was extended for natural language processing in many different directions, for instance for parsing [12, 13, 46], language acquisition [14, 25], explanation [8], and semantic processing <ref> [26, 47] </ref>. More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior [16, 49].
Reference: [48] <author> R. G. Reilly and N. E. Sharkey. </author> <title> Connectionist Approaches to Natural Language Processing. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1992. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56].
Reference: [49] <author> L. Shastri. </author> <title> A model of rapid memory formation in the hippocampal system. </title> <booktitle> In Proceedings of the Meeting of the Cognitive Science Society, </booktitle> <address> Stanford, </address> <year> 1997. </year>
Reference-contexts: More recent work along these lines focuses on the so-called NTL, neural theory of language which attempts to bridge the large gap between neurons and cognitive behavior <ref> [16, 49] </ref>. The NTL framework is challenging since it tries to study neural processing mechanisms for high conceptual cognitive processing like embodied semantics, reasoning, metaphor interpretation, etc. 4 In order to attack such a challenging task it is necessary to focus on different processing mechanisms and certain representative problems. <p> While these two related models for embodied event modeling can be viewed at a computational level, there is more neurally inspired work for event modeling and temporal processing at a connectionist level as well (e.g. <ref> [49, 21] </ref>. For instance, schemas from a computational level can be modeled using structured connectionist terms from the SHRUTI system which was previously developed by Shastri and Ajjanagadde [21]. A particularly interesting recent work is a model of rapid memory formation in the Hippocampal System [49]. <p> For instance, schemas from a computational level can be modeled using structured connectionist terms from the SHRUTI system which was previously developed by Shastri and Ajjanagadde [21]. A particularly interesting recent work is a model of rapid memory formation in the Hippocampal System <ref> [49] </ref>. Focusing on events like "John gave Mary a book on Tuesday", it is studied how a neural code can be transfered rapidly into a structural code so that it can be retrieved later.
Reference: [50] <author> J. Shavlik. </author> <title> A framework for combining symbolic and neural learning. </title> <editor> In V. Honavar and L. Uhr, editors, </editor> <booktitle> Artificial Intelligence and Neural Networks: Steps towards principled Integration, </booktitle> <pages> pages 561-580. </pages> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: The activation-based extraction of automata is just one example of a hybrid transfer architecture and there are several further possibilities. For instance, a weight-based transfer between symbolic rules and feedforward networks has been extensively examined in knowl 11 edge based artificial neural networks (KBANN) <ref> [50, 7] </ref>. This weight-based transfer uses the weights rather than the activations as the main knowledge source for induction and extraction. While an activation-based transfer is based on the activations of certain units a weight-based transfer focuses on a more detailed weight level. <p> Therefore, the architectures are fairly simple, in this case three-layer feedforward networks. The underlying assumption of this approach is that simple rule knowledge is often available, and that this knowledge should be used for initializing connectionist networks <ref> [50] </ref>. Then connectionist learning can be used to refine this knowledge. Ultimately the symbolic knowledge can be extracted from the connectionist network yielding an improved set of interpretation rules.
Reference: [51] <author> P. Smolensky. </author> <title> On the proper treatment of connectionism. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 11 </volume> <pages> 1-74, </pages> <year> 1988. </year>
Reference-contexts: Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language <ref> [51, 11, 27, 56] </ref>. However, since human language capabilities are based on real neural networks in the brain, artificial neural networks (also called connectionist networks) provide one essential starting point for modeling language processing.
Reference: [52] <author> M. F. St. John and J. L. McClelland. </author> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 217-257, </pages> <year> 1990. </year>
Reference-contexts: Furthermore, it is possible to interpret the distributed representations symbolically. There are many more connectionist architectures which allow a symbolic interpretation for instance at the output layers of their network. There are modular network architectures for case role assignment in Gestalt-networks <ref> [52] </ref>, confluent dual RAAM-networks for translation [5], the mixture of expert networks for hierarchical control [28] and the two-tier architecture for corpus analysis [3]. All this recent work contains various forms of modularity and structuring within an overall connectionist architecture.
Reference: [53] <author> R. Sun. </author> <title> Integrating Rules and Connectionism for Robust Commonsense Reasoning. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This spreading activation process in the localist network stops after the different possibly competing constraints have been integrated. The final activation of the nodes determines the structural disambiguation of a phrase. Another recent connectionist structure architecture is CONSYDERR <ref> [53] </ref>. The task of this system is to make inferences, as they are required for instance in natural language understanding systems. The system has an interesting architecture since it consists of a structured connectionist architecture with two different components.
Reference: [54] <author> R. Sun. </author> <title> Hybrid connectionist-symbolic models: A report from the ijcai95 workshop on connectionist-symbolic integration. </title> <journal> Artificial Intelligence Magazine, </journal> <year> 1996. </year>
Reference-contexts: Furthermore, different cognitive processes are not homogeneous and it is to be expected that they are based on different representations <ref> [54] </ref>. Therefore, there is evidence from the brain that multiple architectural representations may also be involved in language processing. From the perspective of knowledge-based natural language processing, hybrid symbolic/connectionist representations are advantageous, since different mutually complementary properties can be combined. <p> new research area and there is no general theory of hybrid architectures [6]:"The development of additional constraints for defining appropriate mappings between hybrid models and the resultant types of hybrid models remain areas for future research." However, the question for an appropriate architecture for a given task is very important <ref> [54] </ref>: "We need to consider seriously ways of structuring these different components; in other words, we need to consider architectures, which thus occupy a clearly more prominent place in this area of research compared with other areas in AI." In general, there are different global possibilities for a symbolic/connectionist integra 1
Reference: [55] <author> R. Sun and F. Alexandre. </author> <title> Proceedings of the Workshop on Connectionist-Symbolic Integration: From Unified to Hybrid Approaches. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> Montreal, </address> <year> 1995. </year> <month> 26 </month>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56]. <p> Previous characterizations of architectures have covered certain specific connectionist architectures, for instance recurrent networks [31, 39], or they have covered expert systems/knowledge-based systems <ref> [37, 24, 55] </ref>. In contrast, here we will concentrate on various types of hybrid connectionist natural language processing. In figure 1 there is an overview of different possibilities for integration in natural language processing. Continuous connectionist representations are represented by a circle, discrete symbolic representations by a square.
Reference: [56] <author> Ron Sun and L.A. Bookman. </author> <title> Computational Architectures Integrating Neural and Symbolic Processes. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1995. </year>
Reference-contexts: Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language <ref> [51, 11, 27, 56] </ref>. However, since human language capabilities are based on real neural networks in the brain, artificial neural networks (also called connectionist networks) provide one essential starting point for modeling language processing.
Reference: [57] <author> D. L. Waltz and J. B. Pollack. </author> <title> Massively parallel parsing: a strongly interactive model of natural language interpretation. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 51-74, </pages> <year> 1985. </year>
Reference-contexts: In a loosely coupled hybrid architecture processing has to be completed in one module before the next module can begin. For instance, the WP model <ref> [57, 44] </ref> for phenomenologically plausible parsing used a symbolic chart parser to construct syntactic localist networks which were also connected to semantic networks. Although the network itself may be viewed as a purely connectionist architecture, the overall architecture is a hybrid processing architecture.
Reference: [58] <editor> S. Wermter. Hybride symbolische und subsymbolische Verarbeitung am Beispiel der Sprachverarbeitung. In I. Duwe, F. Kurfe, G. Paa, and S. Vogel, editors, Herbstschule Konnektionismus und Neuronale Netze. GMD, </editor> <address> Sankt Augustin, </address> <year> 1994. </year>
Reference-contexts: In order to give the reader an impression of the state of the art of current hybrid language technology we focus on this architecture in slightly more detail. Here we focus primarily on the architectural principles of this approach while other task-related details can be found in <ref> [58, 60, 62, 63] </ref>. One main architectural motivation is the use of a common interface between symbolic and connectionist modules which are externally indistinguishable. This allows incremental and parallel processing involving many different modules. <p> See table 1 for abbreviations. Although we cannot go into all the details of the whole hybrid system, we believe it is 21 important to demonstrate the applicability of hybrid techniques in real world environments of language processing. For more details the interested reader is referred to <ref> [58, 60, 62, 63] </ref>. During the development of the SCREEN system we have taken advantage of the integrated hybrid processing architecture which provides the same interface independently of whether it contains a connectionist or symbolic module.
Reference: [59] <author> S. Wermter. </author> <title> Hybrid Connectionist Natural Language Processing. </title> <publisher> Chapman and Hall, Thompson International, </publisher> <address> London, UK, </address> <month> January, </month> <year> 1995. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56]. <p> We will now focus on a few examples of different structured connectionist architectures which have been developed in recent years. One example of a connectionist structure architecture is provided as part of the SCAN project <ref> [59] </ref>. One task within this framework is structural disambiguation of a given sentence or phrase. In general many different structural interpretations of phrases are possible and only the integration of different constraints allows the system to make a correct structural disambiguation decision. <p> Another architecture where the division of symbolic and connectionist work is even more loosely coupled has been described in a model for structural parsing within the SCAN framework <ref> [59] </ref>. First, a chart parser is used to provide a structural tree representation for a given input, for instance for phrases or sentences (see figure 7). This symbolic structural tree representation is used to extract important head nouns and their relationships.
Reference: [60] <author> S. Wermter and M. Lochel. </author> <title> Learning dialog act processing. </title> <booktitle> In Proceedings of the International Conference on Computational Linguistics, </booktitle> <address> Kopenhagen, Denmark, </address> <year> 1996. </year>
Reference-contexts: In order to give the reader an impression of the state of the art of current hybrid language technology we focus on this architecture in slightly more detail. Here we focus primarily on the architectural principles of this approach while other task-related details can be found in <ref> [58, 60, 62, 63] </ref>. One main architectural motivation is the use of a common interface between symbolic and connectionist modules which are externally indistinguishable. This allows incremental and parallel processing involving many different modules. <p> See table 1 for abbreviations. Although we cannot go into all the details of the whole hybrid system, we believe it is 21 important to demonstrate the applicability of hybrid techniques in real world environments of language processing. For more details the interested reader is referred to <ref> [58, 60, 62, 63] </ref>. During the development of the SCREEN system we have taken advantage of the integrated hybrid processing architecture which provides the same interface independently of whether it contains a connectionist or symbolic module.
Reference: [61] <author> S. Wermter, E. Riloff, and G. Scheler. </author> <title> Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: 1 Motivation for hybrid symbolic/connectionist processing In recent years, the field of hybrid symbolic/connectionist processing has seen a remarkable development <ref> [48, 38, 18, 2, 36, 59, 55, 20, 37, 61, 9] </ref>. Currently it is still an open issue whether connectionist or symbolic approaches alone will be sufficient to provide a general framework for processing natural language [51, 11, 27, 56].
Reference: [62] <author> S. Wermter and V. Weber. </author> <title> SCREEN: Learning a flat syntactic and semantic spoken language analysis using artificial neural networks. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 6(1) </volume> <pages> 35-86, </pages> <year> 1997. </year>
Reference-contexts: In order to give the reader an impression of the state of the art of current hybrid language technology we focus on this architecture in slightly more detail. Here we focus primarily on the architectural principles of this approach while other task-related details can be found in <ref> [58, 60, 62, 63] </ref>. One main architectural motivation is the use of a common interface between symbolic and connectionist modules which are externally indistinguishable. This allows incremental and parallel processing involving many different modules. <p> See table 1 for abbreviations. Although we cannot go into all the details of the whole hybrid system, we believe it is 21 important to demonstrate the applicability of hybrid techniques in real world environments of language processing. For more details the interested reader is referred to <ref> [58, 60, 62, 63] </ref>. During the development of the SCREEN system we have taken advantage of the integrated hybrid processing architecture which provides the same interface independently of whether it contains a connectionist or symbolic module.
Reference: [63] <author> S. Wermter and M. Meurer. </author> <title> Building lexical representations dynamically using artificial neural networks. </title> <booktitle> In Proceedings of the International Conference of the Cognitive Science Society, </booktitle> <address> Stanford, </address> <year> 1997. </year>
Reference-contexts: In order to give the reader an impression of the state of the art of current hybrid language technology we focus on this architecture in slightly more detail. Here we focus primarily on the architectural principles of this approach while other task-related details can be found in <ref> [58, 60, 62, 63] </ref>. One main architectural motivation is the use of a common interface between symbolic and connectionist modules which are externally indistinguishable. This allows incremental and parallel processing involving many different modules. <p> See table 1 for abbreviations. Although we cannot go into all the details of the whole hybrid system, we believe it is 21 important to demonstrate the applicability of hybrid techniques in real world environments of language processing. For more details the interested reader is referred to <ref> [58, 60, 62, 63] </ref>. During the development of the SCREEN system we have taken advantage of the integrated hybrid processing architecture which provides the same interface independently of whether it contains a connectionist or symbolic module.
Reference: [64] <author> A. Wilson and J. Hendler. </author> <title> Linking symbolic and subsymbolic computing. </title> <journal> Connection Science, </journal> <volume> 5 </volume> <pages> 395-414, </pages> <year> 1993. </year>
Reference-contexts: Other integrated architectures can be found in PARSEC, a system for language analysis in the conference registration domain [29], which uses Jordan networks to trigger symbolic transformations within the modules, and CONNCERT <ref> [64] </ref>, which has been developed for technical control problems using a mixture of expert networks controlled by symbolic supervisors. 6 Summary and Conclusions Recently there has been an increasing interest in hybrid symbolic/connectionist interpretation, combination and integration.
Reference: [65] <author> C. C. Wooters. </author> <title> Lexical modeling in a speaker independent speech understanding system. </title> <type> Technical Report TR-93-068, </type> <institution> International Computer Science Institute, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: For instance, in the SICSA system, connectionist recurrent networks and symbolic case role parsing are combined for semantic analysis of database queries [4]. In BerP, connectionist feedforward networks for speech processing and symbolic dialog understanding are combined in a spoken 14 language dialog system <ref> [30, 65] </ref>. In FeasPar, connectionist feedforward networks for feature assignment are combined with symbolic search for finding better analysis results.
References-found: 65

