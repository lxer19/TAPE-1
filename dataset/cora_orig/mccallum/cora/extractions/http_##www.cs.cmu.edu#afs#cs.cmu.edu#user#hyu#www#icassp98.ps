URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/hyu/www/icassp98.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/hyu/www/papers.html
Root-URL: 
Email: Email: fhyu,cortis,malkin,ahwg@cs.cmu.edu  
Title: EXPERIMENTS IN AUTOMATIC MEETING TRANSCRIPTION USING JRTK  
Author: Hua Yu, Cortis Clark, Robert Malkin, Alex Waibel 
Address: Pittsburgh, PA, USA  
Affiliation: Interactive Systems Laboratories Carnegie Mellon University,  
Abstract: In this paper we describe our early exploration of automatic recognition of conversational speech in meetings for use in automatic summarizers and browsers to produce meeting minutes effectively and rapidly. To achieve optimal performance we started from two different baseline English rec-ognizers adapted to meeting conditions and tested resulting performance. The data was found to be highly disflu-ent (conversational human to human speech), noisy (due to lapel microphones and environment), and overlapped with background noise, resulting in error rates comparable so far to those on the CallHome conversational database (40-50% WER). A meeting browser is presented that allows the user to search and skim through highlights from a meeting efficiently despite the recognition errors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Finke, Michael, </author> <title> et.al. </title> <booktitle> The JanusRTk Switchboard/Callhome 1997 Evaluation System. Proceedings of the LVCSR Hub5-e Workshop, </booktitle> <address> Baltimore, USA, </address> <year> 1997. </year>
Reference-contexts: The disadvantage is of course, degraded sound quality. Since it's not uni-directional, there's significant channel mixing. There's also a lot of crosstalk, laughter, electric humming, paper scratching noise, etc. in the recording. 2.2. System Specification Our system is built upon Janus Recognition Toolkit (JRTk), which is summarized in <ref> [1] </ref>. Incorporated into our continuous HMM system are techniques like linear discriminant analysis (LDA) for feature space dimension reduction, vocal tract length normalization (VTLN) for speaker normalization, cepstral mean normalization (CMN) for channel normalization, and wide-context phone modeling (Polyphone modeling).
Reference: [2] <author> Zeppenfeld, Torsten, et. al. </author> <title> Recognition of Conversational Telephone Speech Using the Janus Speech Engine. </title> <booktitle> IEEE International Conference on Acoustics, Speech, and Singal Processing, </booktitle> <address> Germany, </address> <year> 1997. </year>
Reference: [3] <author> Westphal, Martin. </author> <title> The Use of Cepstral Means in Conversational Speech Recognition. </title> <booktitle> Proceedings of Eurospeech Conference, </booktitle> <address> Greece, </address> <year> 1997. </year>
Reference-contexts: Guided adaptation To see how far away we still are from perfect adaptation, we conducted supervised adaptation with transcripts, as can be seen from the Table 5, there's still a lot of room for possible further improvements. 5. Other experiments As noted in <ref> [3] </ref>, there're several variations to the basic cepstral mean normalization (CMN) Speaker Adaptation Iterations 0 1 2 Adaptation Gain maxl 37.0 48.2 51.3 22% fdmg 40.7 46.9 49.7 15% flsl 20.8 32.3 33.3 16% Total 32.6 42.5 44.8 18% Table 3: Word Accuracy with ESST Acoustic Model Speaker Adaptation Iterations 0
Reference: [4] <author> Zhan, Puming. </author> <title> Speaker Normalization and Speaker Adaptation a Combination for Conversational Speech Recognition. </title> <booktitle> Proceedings of Eurospeech Conference, </booktitle> <address> Greece, </address> <year> 1997. </year>
Reference-contexts: On the signal side, we used vocal tract length normalization (VTLN) for speaker normalization, cepstral mean subtraction (CMN) for channel normalization. On the model side, we used MLLR adaptation to move the model to fit data. 1. MLLR MLLR <ref> [4] </ref> has recently achieved much popularity as a dependable technique. In our system, we used a regression tree to define regression classes, the tree is constructed based on the criterion of acoustic similarity.
Reference: [5] <author> Suhm, Bernhard, et. al. </author> <title> Interactive Recovery from Speech Recognition Errors in Speech User Interfaces. </title> <booktitle> Internation Conference on Spoken Language Processing, </booktitle> <address> Philadelphia, USA, </address> <year> 1996. </year>
Reference-contexts: All of this information can be included in the streams passed to the Meeting Browser interface. This interface is being extended in numerous ways to increase usability and user acceptance, including security features to restrict access to portions of some streams and incorporating multimodal repair facilities <ref> [5] </ref> into the interface. We are also exploring ways to produce and include information describing the topical and discourse structure of a meeting, as well as multimedia presentations of such structures. 5.
References-found: 5

