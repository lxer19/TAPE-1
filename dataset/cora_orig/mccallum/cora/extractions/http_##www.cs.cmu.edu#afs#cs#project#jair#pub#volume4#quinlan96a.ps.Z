URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/quinlan96a.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/quinlan96a.html
Root-URL: 
Email: quinlan@cs.su.oz.au  
Title: Improved Use of Continuous Attributes in C4.5  
Author: J. R. Quinlan 
Date: 2006  
Address: Sydney, Sydney Australia  
Affiliation: Basser Department of Computer Science University of  
Note: Journal of Artificial Intelligence Research 4 (1996) 77-90 Submitted 10/95; published 3/96  
Abstract: A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.
Abstract-found: 1
Intro-found: 1
Reference: <author> Auer, P., Holte, R. C., & Maass, W. </author> <year> (1995). </year> <title> Theory and application of agnostic pac-learning with small decision trees. </title> <booktitle> In Proceedings of Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 21-29. </pages> <address> San Francisco: </address> <publisher> Morgam Kaufmann. </publisher>
Reference-contexts: The clear trend shows that global discretization degrades performance more as data sets become larger, but can be beneficial for tasks with fewer cases. 5.2 Multi-threshold splits In contrast, T2 <ref> (Auer et al., 1995) </ref> determines thresholds locally but allows the values of a continuous attribute to be partitioned into multiple intervals. These intervals are not found heuristically by a recursive application of binary splitting, as above.
Reference: <author> Breiman, L. </author> <year> (1996). </year> <note> Bagging predictors. Machine Learning, (to appear). </note>
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> On changing continuous attributes into ordered discrete attributes. </title> <editor> In Kodratoff, Y. (Ed.), </editor> <booktitle> Proceedings European Working Session on Learning - EWSL-91, </booktitle> <pages> pp. 164-178. </pages> <address> Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Dougherty, J., Kohavi, R., & Sahami, M. </author> <year> (1995). </year> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Proceedings Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 194-202. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1992). </year> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 87-102. </pages>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1993). </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1022-1027. </pages> <address> San Francisco: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1996). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <type> Unpublished manuscript, </type> <note> available from the authors' home pages ("http://www.research.att.com/orgs/ssr/people/fyoav,schapireg"). </note>
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 63-91. </pages>
Reference: <author> Hunt, E. B., Marin, J., & Stone, P. J. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: All rights reserved. Quinlan compares the performance of the new version to results obtained with the two alternative methods of exploiting continuous attributes quoted above. 2. Constructing Decision Trees C4.5 uses a divide-and-conquer approach to growing decision trees that was pioneered by Hunt and his co-workers <ref> (Hunt, Marin, & Stone, 1966) </ref>. Only a brief description of the method is given here; see Quinlan (1993) for a more complete treatment.
Reference: <author> John, G. H., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. 121-129. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As the tasks are a representative selection from those in the UCI Repository that involve continuous attributes, similar learning tasks should also benefit. Of course, C4.5's performance on domains with continuous attributes can also be improved in other complementary ways, such as by selecting attributes <ref> (John, Kohavi, & Pfleger, 1994) </ref>, exploring the space of parameter settings (Kohavi & John, 1995), or generating multiple classifiers (Breiman, 1996; Freund & Schapire, 1996).
Reference: <author> Kohavi, R., & John, G. H. </author> <year> (1995). </year> <title> Automatic parameter selection by minimizing estimated error. </title> <booktitle> In Proceedings Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. 304-312. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Of course, C4.5's performance on domains with continuous attributes can also be improved in other complementary ways, such as by selecting attributes (John, Kohavi, & Pfleger, 1994), exploring the space of parameter settings <ref> (Kohavi & John, 1995) </ref>, or generating multiple classifiers (Breiman, 1996; Freund & Schapire, 1996). Comparisons with a well-known global discretization scheme, and with a system that carries out a thorough search over the space of two-level decision trees, also favor the modified C4.5.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: For example, the description of a person might include weight in kilograms, with a value such as 73.5, and color of eyes whose value is one of `brown', `blue', etc. C4.5 <ref> (Quinlan, 1993) </ref> is one such system that learns decision-tree classifiers. Several authors have recently noted that C4.5's performance is weaker in domains with a preponderance of continuous attributes than for learning tasks that have mainly discrete attributes. <p> Non-binary splits on continuous attributes make the trees easier to understand and also seem to lead to more accurate trees in some domains. It would also be interesting to investigate 5. The files necessary to update C4.5 Release 5 <ref> (available with Quinlan, 1993) </ref> to the new Release 8 can be obtained by anonymous ftp from ftp.cs.su.oz.au, file pub/ml/patch.tar.Z. 88 Improved Use of Continuous Attributes in C4.5 Kohavi's suggestion to use discretization within a tree when the local number of training cases is small.
Reference: <author> Quinlan, J. R., & Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference: <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11, </volume> <pages> 416-431. 90 </pages>
Reference-contexts: That is, the choice of a test will be biased towards continuous attributes with numerous distinct values. This paper proposes a correction for this bias that consists of two modifications to C4.5. The first of these, inspired by the Minimum Description Length principle <ref> (Rissanen, 1983) </ref>, adjusts the apparent information gain from a test of a continuous attribute. Discussion of this change is prefaced by a brief introduction to MDL.
References-found: 14

