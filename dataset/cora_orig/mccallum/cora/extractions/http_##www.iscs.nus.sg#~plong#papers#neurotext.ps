URL: http://www.iscs.nus.sg/~plong/papers/neurotext.ps
Refering-URL: 
Root-URL: 
Title: Text Compression via Alphabet Re-Representation (extended abstract)  
Author: Philip M. Long Apostol I. Natsev Jeffrey Scott Vitter 
Abstract: We consider re-representing the alphabet so that a representation of a character reflects its properties as a predictor of future text. This enables us to use an estimator from a restricted class to map contexts to predictions of upcoming characters. We describe an algorithm that uses this idea in conjunction with neural networks. The performance of this implementation is compared to other compression methods, such as UNIX compress, gzip, PPMC, and an alternative neural network approach.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bell, T., Cleary, J.G., and Witten, I.H. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction In this paper, we describe a new avenue for improving the compressibility of text. The main idea is that changing the representation of the alphabet may prove beneficial for various text processing tasks, including compression. The current state-of-the-art methods for compression such as PPM <ref> [1, 3] </ref> generally work in two stages: first they try to estimate the probability distribution of the next character in the given context, and then they entropy-code it using the predicted probabilities.
Reference: [2] <author> Chauvin, Y., and Rumelhart, D. E. </author> <title> Backpropagation: Theory, Architectures, and Applications. </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <year> 1995. </year>
Reference-contexts: cost function is selected to maximize the log-likelihood of the data given the network, and we argue that our specific choice of a cost function is optimal for compression purposes. 4.1 Background on design issues In this section we address some issues related to the design of neural networks (cf. <ref> [2] </ref>). Let D = h~x i ; ~ t i i denote the observed data where ~x i is the ith input vector (i.e., the context), and ~ t i is the target vector (i.e., the character which appeared next). <p> At this point, in order to get an expression for an optimal cost function we need to make some assumption about the type of the probability distribution P ( ~ t i j ~x i ^ N ). In <ref> [2] </ref> Rumelhart, Durbin, Golden, and Chauvin consider the general family of distributions P ( ~ t j ~x ^ N ) = exp X (t i B ()) + C ( ~ t) ! where is related to the mean of the distribution, is the overall variance, and the functions A <p> The corresponding energy function is given by E multinomial = P P where index i ranges over the observations, and index j ranges over the output nodes. The most appropriate activation function for the multinomial case is the normalized exponential function. In <ref> [2] </ref>, the authors argued that the multinomial case is most suitable when the network is supposed to make 1-out-of-n classification. In that case the output is treated as a probability distribution, and the ith output node corresponds to the probability that the pattern goes to the ith class. <p> we obtain the following expression: S j (net j ) = 1 0 1 1 2 2 = a j (1 a j ); (5) leading to the following final form for a hidden node's gradient: ffi j = a j (1 a j ) P As we know from <ref> [2] </ref> our particular choice of an output activation function for our cost function results in ffi j = t j a j as the gradient at the jth output node. For completeness purposes, however, we shall include the full derivation of that fact.
Reference: [3] <author> Cleary, J.G., and Witten, I.H. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Transactions on Communication 32 (1984), </journal> <pages> 396-402. </pages>
Reference-contexts: 1 Introduction In this paper, we describe a new avenue for improving the compressibility of text. The main idea is that changing the representation of the alphabet may prove beneficial for various text processing tasks, including compression. The current state-of-the-art methods for compression such as PPM <ref> [1, 3] </ref> generally work in two stages: first they try to estimate the probability distribution of the next character in the given context, and then they entropy-code it using the predicted probabilities.
Reference: [4] <author> Rumelhart, D., Hinton, D., and Williamson, R. </author> <title> Parallel Distributed Processing, </title> <journal> Explorations in the Microstructure of Cognition, </journal> <volume> Vol. 1: </volume> <booktitle> Foundations. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: While the general backpropagation model can be found in standard references such as <ref> [4] </ref>, we are not aware of references that derive it in the context of normalized exponential outputs and entropy-like cost functions. Therefore, we include the full derivation here, differing from the standard one mainly in equations (6)-(8).
Reference: [5] <author> Schmidhuber, J., and Heil, S. </author> <title> Sequential neural text compression. </title> <journal> IEEE Transactions on Neural Networks 7, </journal> <month> 1 (January </month> <year> 1996), </year> <pages> 142-146. </pages>
Reference-contexts: The first set is identical to the experiments reported in <ref> [5] </ref>, where the authors propose a similar neural network approach that consists of the same input/output structure but uses only one hidden layer. As reported in [5], the number of hidden nodes used in their single hidden layer is 440, the context size is 5, and the alphabet size is 80. <p> The first set is identical to the experiments reported in <ref> [5] </ref>, where the authors propose a similar neural network approach that consists of the same input/output structure but uses only one hidden layer. As reported in [5], the number of hidden nodes used in their single hidden layer is 440, the context size is 5, and the alphabet size is 80. <p> The training time for PSM was very slow, though the complexity of the network is lower than that of the alternative neural network approach <ref> [5] </ref>. The learning rate used in the experiments reported in [5] was fixed to 0.2 but for our approach we used a decaying learning rate which started off at 0.2. The training set consisted of 40 articles from the German newspaper Munchner Merkur. <p> The training time for PSM was very slow, though the complexity of the network is lower than that of the alternative neural network approach <ref> [5] </ref>. The learning rate used in the experiments reported in [5] was fixed to 0.2 but for our approach we used a decaying learning rate which started off at 0.2. The training set consisted of 40 articles from the German newspaper Munchner Merkur. <p> Method's Average Compression Ratio (Variance) Name Munchner Merkur Frankenpost Jack London UNIX pack 1.74 (0.0002) 1.67 (0.0003) 1.78 (0.0001) UNIX compress 1.99 (0.0014) 1.71 (0.0036) 2.45 (0.0060) UNIX gzip -9 2.30 (0.0033) 2.05 (0.0097) 2.64 (0.0049) PPMC method 2.70 (0.0069) 2.27 (0.0131) 3.54 (0.0984) NN method in <ref> [5] </ref> 2.72 (0.0234) 2.20 (0.0112) | PSM method 3.09 (0.0142) 2.61 (0.0047) 3.56 (0.0083) Table 1: Compression performance of various methods on three test sets consisting of newspaper articles from Munchner Merkur and Frankenpost, and of books by Jack London. <p> Since all the files in the previous experiment were relatively short (i.e., &lt; 20 kilobytes), we designed a second experiment which uses longer files. While the purpose of the first experiment was to compare our approach with the alternative neural net approach proposed in <ref> [5] </ref>, in the second experiment we wanted to primarily test our method against PPMC, which was at a disadvantage with the small files used in experiment 1. As we expected, PPMC performed competitively on the longer files, yielding compression ratios of over 3.5.
Reference: [6] <author> Welch, T.A. </author> <title> A technique for high performance data compression. </title> <booktitle> Computer, </booktitle> <year> (1984), </year> <pages> 8-19. </pages>
Reference-contexts: The compression improvement ranges anywhere from about 15% (for PPMC and the other neural network approach) to more than 50% (for pack and compress). Compress and gzip use Ziv-Lempel algorithms <ref> [7, 8, 6] </ref>, and are often used in practice due in part to their computational efficiency.
Reference: [7] <author> Ziv, J., and Lempel, A. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory 23 (1977), </journal> <pages> 337-343. </pages>
Reference-contexts: The compression improvement ranges anywhere from about 15% (for PPMC and the other neural network approach) to more than 50% (for pack and compress). Compress and gzip use Ziv-Lempel algorithms <ref> [7, 8, 6] </ref>, and are often used in practice due in part to their computational efficiency.
Reference: [8] <author> Ziv, J., and Lempel, A. </author> <title> Compression of individual sequences via variable-rate coding. </title> <journal> IEEE Transactions on Information Theory 24 (1978), </journal> <pages> 530-536. </pages>
Reference-contexts: The compression improvement ranges anywhere from about 15% (for PPMC and the other neural network approach) to more than 50% (for pack and compress). Compress and gzip use Ziv-Lempel algorithms <ref> [7, 8, 6] </ref>, and are often used in practice due in part to their computational efficiency.
References-found: 8

