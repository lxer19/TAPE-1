URL: http://www-cse.ucsd.edu/users/gary/pubs/jrank.ps
Refering-URL: http://www.csi.uottawa.ca/~debruijn/irbib.html
Root-URL: 
Title: Optimizing Similarity Using Multi-Query Relevance Feedback  
Author: Brian T. Bartell** Garrison W. Cottrell*** Richard K. Belew*** 
Note: This work was supported by NSF grant IRI-9221276. The first author also received support from Encyclopdia Britannica, Inc., Chicago, Il. Address all correspondence to the second author. Email: gary@cs.ucsd.edu Surface mail:  
Address: 5226 Caminito Vista Lujo San Diego, CA 92130  La Jolla, California 92093-0114  Engineering-0114, UC San Diego, La Jolla, CA 92093-0114.  
Affiliation: **Conceptual Dimensions, Inc.  ***Department of Computer Science Engineering-0114 University of California, San Diego  Dept. of Computer Science  
Abstract: We propose a novel method for automatically adjusting parameters in ranked-output text retrieval systems to improve retrieval performance. A ranked-output text retrieval system implements a ranking function which orders documents, placing documents estimated to be more relevant to the user's query before less relevant ones. The system adjusts its parameters to maximize the match between the system's document ordering and a target ordering. The target ordering is typically given by user feedback on a set of sample queries, but is more generally any document preference relation. We demonstrate the utility of the approach by using it to estimate a similarity measure (scoring the relevance of documents to queries) in a vector space model of information retrieval. 
Abstract-found: 1
Intro-found: 1
Reference: [Bartell, 1994] <author> Bartell, B. T. </author> <year> (1994). </year> <title> Optimizing Ranking Functions: A Connectionist Approach to Adaptive Information Retrieval. </title> <type> PhD thesis, </type> <institution> Department of Computer Science & Engineering, The University of California, </institution> <address> San Diego. </address>
Reference-contexts: This analysis is useful to determine bounds on the approach's computation time as we scale to larger collections. However, it is also useful to consider raw computation times on particular problems. In <ref> [Bartell, 1994] </ref> we report computation times for a number of model types on a number of different collections.
Reference: [Bartell et al., 1992] <author> Bartell, B. T., Cottrell, G. W., and Belew, R. K. </author> <year> (1992). </year> <title> Latent semantic indexing is an optimal special case of multidimensional scaling. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <address> Copenhagen. </address>
Reference-contexts: Other "implicit" approaches include Belew's adaptive connectionist IR model [Belew, 1986, Belew, 1989] (see also [Rose, 1991]), which increases the association between the documents and document features (e.g., terms and citations) that contribute to finding relevant documents for a query. Bartell, et al., <ref> [Bartell et al., 1992, Bartell et al., 1995] </ref> propose a method to find reduced dimension vector representations of documents so that the semantic similarity between co-relevant documents is reflected in the representation. This is found to improve retrieval performance for novel queries.
Reference: [Bartell et al., 1995] <author> Bartell, B. T., Cottrell, G. W., and Belew, R. K. </author> <year> (1995). </year> <title> Representing documents using an explicit model of their similarities. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 46(4) </volume> <pages> 254-271. </pages> <address> Bartell 53 </address>
Reference-contexts: Other "implicit" approaches include Belew's adaptive connectionist IR model [Belew, 1986, Belew, 1989] (see also [Rose, 1991]), which increases the association between the documents and document features (e.g., terms and citations) that contribute to finding relevant documents for a query. Bartell, et al., <ref> [Bartell et al., 1992, Bartell et al., 1995] </ref> propose a method to find reduced dimension vector representations of documents so that the semantic similarity between co-relevant documents is reflected in the representation. This is found to improve retrieval performance for novel queries.
Reference: [Belew, 1989] <author> Belew, R. </author> <year> (1989). </year> <title> Adaptive information retrieval: Using a connectionist representation to retrieve and learn about documents. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 11-20, </pages> <address> Cambridge, MA. </address>
Reference-contexts: Similar to Ide's approach, Crestani's [Crestani, 1993] neural network method learns query representations that are more similar to relevant document representations in order to find additional documents relevant to the user's need. Other "implicit" approaches include Belew's adaptive connectionist IR model <ref> [Belew, 1986, Belew, 1989] </ref> (see also [Rose, 1991]), which increases the association between the documents and document features (e.g., terms and citations) that contribute to finding relevant documents for a query.
Reference: [Belew, 1986] <author> Belew, R. K. </author> <year> (1986). </year> <title> Adaptive Information Retrieval: Machine Learning in Associative Networks. </title> <type> PhD thesis, </type> <institution> The University of Michigan. </institution>
Reference-contexts: Similar to Ide's approach, Crestani's [Crestani, 1993] neural network method learns query representations that are more similar to relevant document representations in order to find additional documents relevant to the user's need. Other "implicit" approaches include Belew's adaptive connectionist IR model <ref> [Belew, 1986, Belew, 1989] </ref> (see also [Rose, 1991]), which increases the association between the documents and document features (e.g., terms and citations) that contribute to finding relevant documents for a query. <p> function R fi;q (d) is meant to be quite general, so that many (or most) of the common models of ranked-output text retrieval, e.g., the Vec Bartell 11 tor Space Model [Salton and McGill, 1983, Raghavan and Wong, 1986], probabilistic retrieval [Bookstein and Swanson, 1974], and many spreading activation models <ref> [Belew, 1986, Mozer, 1984] </ref>, can be included as instances of R fi;q (d). As an example, consider the well-known Vector Space Model (VSM). In this model, documents and queries are represented as weighted vectors in a many-dimensional space.
Reference: [Belkin et al., 1993] <author> Belkin, N. J., Cool, C., Croft, W. B., and Callan, J. P. </author> <year> (1993). </year> <title> Effect of multiple query representations on information retrieval system performance. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 339-346, </pages> <address> Pittsburgh, PA. </address>
Reference-contexts: The methods are exhaustive because all possible choices of system parameter values are explicitly enumerated and exhaustively evaluated. The parameter values which result in the best ranking performance are chosen as the optimal parameters. The excellent comparative studies by Harman [Harman, 1986], Dumais [Dumais, 1990], and Belkin, et al., <ref> [Belkin et al., 1993] </ref> are examples of this methodology. In addition [Frants et al., 1993] suggest an exhaustive search over a small number of system parameters: their system is a collection of competing retrieval algorithms.
Reference: [Bookstein and Swanson, 1974] <author> Bookstein, A. and Swanson, D. R. </author> <year> (1974). </year> <title> Probabilistic models for automatic indexing. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 25 </volume> <pages> 312-318. </pages>
Reference-contexts: Our definition of the ranking function R fi;q (d) is meant to be quite general, so that many (or most) of the common models of ranked-output text retrieval, e.g., the Vec Bartell 11 tor Space Model [Salton and McGill, 1983, Raghavan and Wong, 1986], probabilistic retrieval <ref> [Bookstein and Swanson, 1974] </ref>, and many spreading activation models [Belew, 1986, Mozer, 1984], can be included as instances of R fi;q (d). As an example, consider the well-known Vector Space Model (VSM). In this model, documents and queries are represented as weighted vectors in a many-dimensional space.
Reference: [Borg and Lingoes, 1987] <author> Borg, I. and Lingoes, J. </author> <year> (1987). </year> <title> Multidimensional Similarity Structure Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: We propose Guttman's Point Alienation, a rank order statistic motivated by the statistics of rank order [Press et al., 1988] and by techniques in the field of Multidimensional Scaling (MDS) <ref> [Shepard, 1962, Kruskal, 1964, Borg and Lingoes, 1987] </ref>, as a useful criterion for how well the system is matching the users' target rankings. Optimizing the criterion automatically via gradient descent techniques adjusts parameter values so that the system ranks documents more similarly to the users' own preference order. <p> We note here that there are numerous other possible criteria which could be used in place of our derivative of Guttman's Point Alienation. Alternatives from the statistics literature and Multidimensional Scaling (MDS) include Kruskal's stress function [Kruskal, 1964], a variation of Pearson's correlation coefficient <ref> [Borg and Lingoes, 1987] </ref>, and the mono-tonicity coefficient of Guttman [Guttman, 1978]. <p> A number of criteria which were examined in addition to Guttman's Point Alienation measure have more favorable evaluation bounds. In particular, measures which make use of the rank image of the relevance scores to calculate goodness (e.g., the variation on Pearson's correlation coefficient <ref> [Borg and Lingoes, 1987] </ref> and Guttman's monotonicity coefficient [Guttman, 1978]) are worst case order O (q d log d) rather than worst case O (q d 2 ). However, these measures are apparently only applicable to a restricted class of document ordering relations, in which q is a total order.
Reference: [Brauen, 1971] <author> Brauen, T. L. </author> <year> (1971). </year> <title> Document vector modification. </title> <booktitle> In The Smart Retrieval System Experiments in Automatic Document Processing, </booktitle> <pages> pages 456-484. </pages> <address> NJ: </address> <publisher> Prentice Hall, Inc. </publisher>
Reference-contexts: That is, the parameter modifications may have a tendency to improve the system's ranking performance, but this is not an explicit goal of the procedure. Term weight modification algorithms are examples of these methods <ref> [Brauen, 1971, Ide, 1971, Rocchio Jr., 1971] </ref> (a number of methods are compared in [Salton and Buckley, 1990]). <p> In addition, the method remains a natural way to incorporate user's relevance feedback into an adaptive system. This emphasis on rank order departs from a number of the typical methods for adaptively incorporating relevance assessments into the system's operation. As exemplified by the work of Brauen, et al., <ref> [Brauen, 1971] </ref>, Ide [Ide, 1971], and Rocchio [Rocchio Jr., 1971], numerous methods use relevance assessments as an absolute measure of relevance. For example, documents are made more or less similar to a query each time they are found to be relevant or irrelevant.
Reference: [Cormen et al., 1992] <author> Cormen, T. H., Lieserson, C. E., and Rivest, R. L. </author> <year> (1992). </year> <title> Introduction to Algorithms. </title> <publisher> MIT Press. </publisher>
Reference-contexts: If we let q be the number of training queries and d be the number of documents, then evaluation of J will at worst be O (q d 2 ), where O () denotes the worst-case time complexity of the algorithm <ref> [Cormen et al., 1992] </ref>. However, this is worst case; better scenarios are possible. For example, there may be a limit, imposed by the system design, to the number of equivalence classes in q .
Reference: [Crestani, 1993] <author> Crestani, F. </author> <year> (1993). </year> <title> Learning strategies for an adaptive information retrieval system using neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <address> San Francisco, CA. Bartell 54 </address>
Reference-contexts: However, none of these methods explicitly improve the system's ability to rank documents; rather, they adjust parameters to achieve a secondary goal (making documents more similar to a query) which is empirically correlated with the task of improving ranking performance. Similar to Ide's approach, Crestani's <ref> [Crestani, 1993] </ref> neural network method learns query representations that are more similar to relevant document representations in order to find additional documents relevant to the user's need.
Reference: [Cutting et al., 1992] <author> Cutting, D. R., Pedersen, J. O., Karger, D. R., and Tukey, J. W. </author> <year> (1992). </year> <title> Scatter/Gather: A cluster-based approach to browsing large document collections. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 318-329, </pages> <address> Copenhagen. </address>
Reference-contexts: Perhaps contrary to this assumption, retrieval systems have been proposed which emphasize alternative methods for displaying retrieved documents other than the linear list (e.g., <ref> [Cutting et al., 1992, Rose et al., 1993] </ref>). However, whether or not the retrieved documents are displayed to the user in a sorted list, the proposed method may still be applicable if the system makes any differentiation based on the system-generated relevance estimates.
Reference: [Deerwester et al., 1990] <author> Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. </author> <year> (1990). </year> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407. </pages>
Reference-contexts: Experimentally, this has been demonstrated to improve the system's ability to rank relevant documents before irrelevant ones using new documents or queries. Another implicit method is Latent Semantic Indexing <ref> [Furnas et al., 1988, Deerwester et al., 1990] </ref>, where documents are linearly mapped into a lower dimensional space that hopefully reflects "semantic" dimensions.
Reference: [Dumais, 1990] <author> Dumais, S. T. </author> <year> (1990). </year> <title> Enhancing performance in latent semantic indexing (LSI) retrieval. </title> <type> Technical Report Technical Memorandum, </type> <institution> Bellcore. </institution>
Reference-contexts: The methods are exhaustive because all possible choices of system parameter values are explicitly enumerated and exhaustively evaluated. The parameter values which result in the best ranking performance are chosen as the optimal parameters. The excellent comparative studies by Harman [Harman, 1986], Dumais <ref> [Dumais, 1990] </ref>, and Belkin, et al., [Belkin et al., 1993] are examples of this methodology. In addition [Frants et al., 1993] suggest an exhaustive search over a small number of system parameters: their system is a collection of competing retrieval algorithms.
Reference: [Frants et al., 1993] <author> Frants, V. I., Shapiro, J., and Voiskunskii, V. G. </author> <year> (1993). </year> <title> Multiversion information retrieval systems and feedback with mechanism of selection. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 44(1) </volume> <pages> 19-27. </pages>
Reference-contexts: The parameter values which result in the best ranking performance are chosen as the optimal parameters. The excellent comparative studies by Harman [Harman, 1986], Dumais [Dumais, 1990], and Belkin, et al., [Belkin et al., 1993] are examples of this methodology. In addition <ref> [Frants et al., 1993] </ref> suggest an exhaustive search over a small number of system parameters: their system is a collection of competing retrieval algorithms. Relevance feedback from each query is used to select the retrieval algorithm which performed best in the initial retrieval of documents for the query.
Reference: [Fuhr and Buckley, 1991] <author> Fuhr, N. and Buckley, C. </author> <year> (1991). </year> <title> A probabilistic learning approach for document indexing. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3) </volume> <pages> 223-248. </pages>
Reference-contexts: Like the implicit methods, both the pseudoclassification methods and Wong & Yao's method can only be used to optimize a restricted kind of parameter: synonym classes and linear term weights, respectively. Another heuristic method is Fuhr & Buckley's <ref> [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993] </ref> Probabilistic Learning method, in which system parameters are adjusted to minimize the deviation of document relevance scores from value 0:0 (if the particular document is irrelevant) or 1:0 (if it is relevant). <p> Squared Error, used by Fuhr & Buckley <ref> [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993] </ref>, is an additional option, though it is applicable only to binary relevant/irrelevant relevance assessments and not to the more general quasi-orders. We have examined these measures, Bartell 15 as well as some derivatives of these measures. <p> This gives evidence for the robustness of this correlation across different collections. 4 Comparison With Alternative Methods We now compare our method to two alternatives which are motivated by approaches proposed by Wong & Yao [Wong and Yao, 1990, Wong et al., 1993] and Fuhr & Buckley <ref> [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993] </ref>. Both of the alternatives differ from our approach in the choice of the criterion used to evaluate the performance of the retrieval system. Wong & Yao have advocated the use of a Perceptron criterion to measure how well documents are ranked. <p> The minima of the Perceptron criterion has no relation to parameters which rank documents well. 4.2 Squared Error Criterion Fuhr & Buckley have proposed a method for learning to combine different term indexing strategies to result in improved overall performance <ref> [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993] </ref>. The approach is to weight each term occurring in a document by a number of different standard term weighting methods (such as term frequency (tf), inverse document frequency (idf), etc.) rather than by picking a single one.
Reference: [Fuhr and Buckley, 1993] <author> Fuhr, N. and Buckley, C. </author> <year> (1993). </year> <title> Optimizing document indexing and search term weighting based on probabilistic models. </title> <editor> In Harman, D. K., editor, </editor> <title> The First Text REtrieval Conference (TREC-1). </title> <note> NIST Special Publication 500-207. </note>
Reference-contexts: Like the implicit methods, both the pseudoclassification methods and Wong & Yao's method can only be used to optimize a restricted kind of parameter: synonym classes and linear term weights, respectively. Another heuristic method is Fuhr & Buckley's <ref> [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993] </ref> Probabilistic Learning method, in which system parameters are adjusted to minimize the deviation of document relevance scores from value 0:0 (if the particular document is irrelevant) or 1:0 (if it is relevant). <p> Squared Error, used by Fuhr & Buckley <ref> [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993] </ref>, is an additional option, though it is applicable only to binary relevant/irrelevant relevance assessments and not to the more general quasi-orders. We have examined these measures, Bartell 15 as well as some derivatives of these measures. <p> This gives evidence for the robustness of this correlation across different collections. 4 Comparison With Alternative Methods We now compare our method to two alternatives which are motivated by approaches proposed by Wong & Yao [Wong and Yao, 1990, Wong et al., 1993] and Fuhr & Buckley <ref> [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993] </ref>. Both of the alternatives differ from our approach in the choice of the criterion used to evaluate the performance of the retrieval system. Wong & Yao have advocated the use of a Perceptron criterion to measure how well documents are ranked. <p> The minima of the Perceptron criterion has no relation to parameters which rank documents well. 4.2 Squared Error Criterion Fuhr & Buckley have proposed a method for learning to combine different term indexing strategies to result in improved overall performance <ref> [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993] </ref>. The approach is to weight each term occurring in a document by a number of different standard term weighting methods (such as term frequency (tf), inverse document frequency (idf), etc.) rather than by picking a single one.
Reference: [Furnas et al., 1988] <author> Furnas, G. W., Deerwester, S., Dumais, S. T., Landauer, T. K., Harsh-man, R. A., Streeter, L. A., and Lochbaum, K. E. </author> <year> (1988). </year> <title> Information retrieval using a singular value decomposition model of latent semantic structure. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <address> Grenoble, France. Bartell 55 </address>
Reference-contexts: Experimentally, this has been demonstrated to improve the system's ability to rank relevant documents before irrelevant ones using new documents or queries. Another implicit method is Latent Semantic Indexing <ref> [Furnas et al., 1988, Deerwester et al., 1990] </ref>, where documents are linearly mapped into a lower dimensional space that hopefully reflects "semantic" dimensions.
Reference: [Gordon, 1988] <author> Gordon, M. </author> <year> (1988). </year> <title> Probabilistic and genetic algorithms in document retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 31(10). </volume>
Reference-contexts: A limitation of the heuristic methods is that they typically are not guaranteed to find the optimal parameters, since they do not consider all possibilities of parameter values. However, since the space can often be prohibitively large or infinite in size, this is likely a necessary concession. For example, <ref> [Gordon, 1988] </ref> uses a genetic algorithm [Holland, 1976] to search the space of document representations. Document representations are learned which optimize a measure of goodness based on fallout and recall.
Reference: [Guttman, 1978] <author> Guttman, L. </author> <year> (1978). </year> <title> What is not what in statistics. </title> <journal> The Statistician, </journal> <volume> 26 </volume> <pages> 81-107. </pages>
Reference-contexts: The criterion is derived from Guttman's Point Alienation measure <ref> [Guttman, 1978] </ref>, a statistical estimate of the rank correlation between two variables. <p> Alternatives from the statistics literature and Multidimensional Scaling (MDS) include Kruskal's stress function [Kruskal, 1964], a variation of Pearson's correlation coefficient [Borg and Lingoes, 1987], and the mono-tonicity coefficient of Guttman <ref> [Guttman, 1978] </ref>. <p> In particular, measures which make use of the rank image of the relevance scores to calculate goodness (e.g., the variation on Pearson's correlation coefficient [Borg and Lingoes, 1987] and Guttman's monotonicity coefficient <ref> [Guttman, 1978] </ref>) are worst case order O (q d log d) rather than worst case O (q d 2 ). However, these measures are apparently only applicable to a restricted class of document ordering relations, in which q is a total order.
Reference: [Harman, 1986] <author> Harman, D. </author> <year> (1986). </year> <title> An experimental study of factors important in document ranking. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 186-193, </pages> <address> Pisa, Italy. </address>
Reference-contexts: The methods are exhaustive because all possible choices of system parameter values are explicitly enumerated and exhaustively evaluated. The parameter values which result in the best ranking performance are chosen as the optimal parameters. The excellent comparative studies by Harman <ref> [Harman, 1986] </ref>, Dumais [Dumais, 1990], and Belkin, et al., [Belkin et al., 1993] are examples of this methodology. In addition [Frants et al., 1993] suggest an exhaustive search over a small number of system parameters: their system is a collection of competing retrieval algorithms. <p> Ranking performance can vary significantly when switching between different measures; for example, <ref> [Harman, 1986] </ref> demonstrated a 12% improvement in average precision by switching between two different normalized inner product measures (similar differences are observed in our experiments, reported later).
Reference: [Harman, 1993] <author> Harman, D. </author> <year> (1993). </year> <title> Overview of the first Text REtrieval Conference. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 36-48, </pages> <address> Pittsburgh, PA. </address>
Reference-contexts: Average precision is a commonly used and reasonably well accepted method for measuring retrieval performance in the Information Retrieval literature, as witnessed by its use in the Text REtrieval Conference (TREC-1) evaluations <ref> [Harman, 1993] </ref>. A contour plot of the average precision surface is provided in Figure 3. Note that for average precision, it is Bartell 24 The correlation is negative because maxima of the average precision surface correspond to minima of J. preferable to have large values rather than small.
Reference: [Holland, 1976] <author> Holland, J. </author> <year> (1976). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Michigan Press. </publisher>
Reference-contexts: However, since the space can often be prohibitively large or infinite in size, this is likely a necessary concession. For example, [Gordon, 1988] uses a genetic algorithm <ref> [Holland, 1976] </ref> to search the space of document representations. Document representations are learned which optimize a measure of goodness based on fallout and recall. Gordon's approach has great potential for applications involving parameters other than document term representations, though application of the technique can be exceptionally expensive computationally.
Reference: [Ide, 1971] <author> Ide, E. </author> <year> (1971). </year> <title> New experiments in relevance feedback. In The Smart System - Experiments in Automatic Document Processing, </title> <address> pages 337-354. NJ: </address> <publisher> Prentice Hall, Inc. </publisher>
Reference-contexts: That is, the parameter modifications may have a tendency to improve the system's ranking performance, but this is not an explicit goal of the procedure. Term weight modification algorithms are examples of these methods <ref> [Brauen, 1971, Ide, 1971, Rocchio Jr., 1971] </ref> (a number of methods are compared in [Salton and Buckley, 1990]). <p> We have not pursued this direction in the current research, though others have demonstrated that this general approach can be very productive <ref> [Ide, 1971, Salton and Buckley, 1990] </ref>. In addition, as expected, the system's ability to properly handle novel queries improves when additional queries contribute to the parameter estimation. <p> This emphasis on rank order departs from a number of the typical methods for adaptively incorporating relevance assessments into the system's operation. As exemplified by the work of Brauen, et al., [Brauen, 1971], Ide <ref> [Ide, 1971] </ref>, and Rocchio [Rocchio Jr., 1971], numerous methods use relevance assessments as an absolute measure of relevance. For example, documents are made more or less similar to a query each time they are found to be relevant or irrelevant.
Reference: [Jones and Furnas, 1987] <author> Jones, W. P. and Furnas, G. W. </author> <year> (1987). </year> <title> Pictures of relevance: A geometric analysis of similarity measures. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 38(6) </volume> <pages> 420-442. </pages>
Reference-contexts: This problem is interesting for both historical and pragmatic reasons. Historically, a large number of similarity measures have been proposed ([McGill et al., 1979] and <ref> [Jones and Furnas, 1987] </ref> contain lengthy enumerations), and no single similarity measure has gained universal acceptance. <p> This suggests that there is some uniformity, considering a broad class of similarity measures, in which measures are generally better than others. However, in agreement with Jones & Furnas' conjecture <ref> [Jones and Furnas, 1987] </ref>, there can be important differences between different collections which warrant the use of special similarity measures optimized for each particular retrieval environment.
Reference: [Kruskal, 1964] <author> Kruskal, J. B. </author> <year> (1964). </year> <title> Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. </title> <journal> Psychometrika, </journal> <volume> 29(1) </volume> <pages> 1-27. </pages>
Reference-contexts: We propose Guttman's Point Alienation, a rank order statistic motivated by the statistics of rank order [Press et al., 1988] and by techniques in the field of Multidimensional Scaling (MDS) <ref> [Shepard, 1962, Kruskal, 1964, Borg and Lingoes, 1987] </ref>, as a useful criterion for how well the system is matching the users' target rankings. Optimizing the criterion automatically via gradient descent techniques adjusts parameter values so that the system ranks documents more similarly to the users' own preference order. <p> We note here that there are numerous other possible criteria which could be used in place of our derivative of Guttman's Point Alienation. Alternatives from the statistics literature and Multidimensional Scaling (MDS) include Kruskal's stress function <ref> [Kruskal, 1964] </ref>, a variation of Pearson's correlation coefficient [Borg and Lingoes, 1987], and the mono-tonicity coefficient of Guttman [Guttman, 1978].
Reference: [McGill et al., 1979] <author> McGill, M., Koll, M., and Noreault, T. </author> <year> (1979). </year> <title> An evaluation of factors affecting document ranking by information retrieval systems. </title> <type> Technical report, </type> <institution> School of Information Studies, Syracuse University, Syracuse, </institution> <address> New York 13210. Bartell 56 </address>
Reference: [Mozer, 1984] <author> Mozer, M. C. </author> <year> (1984). </year> <title> Inductive information retrieval using parallel distributed computation. </title> <type> Technical Report Technical Report TR8406, </type> <institution> Institute for Cognitive Science, </institution> <address> UCSD, La Jolla, CA. </address>
Reference-contexts: function R fi;q (d) is meant to be quite general, so that many (or most) of the common models of ranked-output text retrieval, e.g., the Vec Bartell 11 tor Space Model [Salton and McGill, 1983, Raghavan and Wong, 1986], probabilistic retrieval [Bookstein and Swanson, 1974], and many spreading activation models <ref> [Belew, 1986, Mozer, 1984] </ref>, can be included as instances of R fi;q (d). As an example, consider the well-known Vector Space Model (VSM). In this model, documents and queries are represented as weighted vectors in a many-dimensional space.
Reference: [Olafsen and Vokac, 1982] <author> Olafsen, T. and Vokac, L. </author> <year> (1982). </year> <title> Optimal values of recall and precision. </title> <journal> Journal of the American Society for Information Science, </journal> <pages> pages 92-96. </pages>
Reference-contexts: However, it is more likely that a system should be evaluated based on many different measures of correctness. For example, <ref> [Olafsen and Vokac, 1982] </ref> suggests that a cost-theoretic analysis finds different good values for precision/recall depending on the user's cost scheme. In addition, a number of different measures are identified by van Ri-jsbergen [van Rijsbergen, 1983]. Our approach is not antithetical to these alternatives.
Reference: [Plutowski et al., 1993] <author> Plutowski, M., Sakata, S., and White, H. </author> <year> (1993). </year> <title> Cross-validation estimates imse. </title> <booktitle> In Proceedings of Neural Information Processing Systems (NIPS) 1993, </booktitle> <address> Vail, Colorado. </address> <publisher> in press. </publisher>
Reference-contexts: Our approach is to allow as many query samples as are available to contribute to determining the optimal parameter settings, to increase the likelihood of good generalization [Valiant, 1984]. That is, we have not applied any cross-validation techniques to the training set <ref> [Plutowski et al., 1993] </ref>. <p> This problem may especially manifest when the model, R fi , is overly expressive and is able to readily over-fit the samples. Techniques exist to alleviate this problem, such as using a cross-validation set of samples to determine when to terminate optimization <ref> [Plutowski et al., 1993] </ref>, or adding noise to the optimization [Sietsma and Dow, 1988]. However, over-fitting has not been a problem in the experiments reported in this paper, likely due to the limitations of the similarity model chosen.
Reference: [Press et al., 1988] <author> Press, W. H., Flannery, B. P., Teukolsky, S. A., and Vetterling, W. T. </author> <year> (1988). </year> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press. </publisher>
Reference-contexts: The parameters are adjusted by numerically optimizing a measure of how well the system is ranking documents with respect to the users' target ranking for the set of queries. We propose Guttman's Point Alienation, a rank order statistic motivated by the statistics of rank order <ref> [Press et al., 1988] </ref> and by techniques in the field of Multidimensional Scaling (MDS) [Shepard, 1962, Kruskal, 1964, Borg and Lingoes, 1987], as a useful criterion for how well the system is matching the users' target rankings. <p> Wong & Yao [Wong and Yao, 1990, Wong et al., 1993] have proposed an adaptive method for learning query and document term weights which best rank relevant documents before irrelevant ones. Wong & Yao use gradient descent <ref> [Press et al., 1988] </ref>, a numerical local "hill-climbing" search algorithm, to heuristically search the infinite space of linear term weights. <p> We have used both gradient descent and conjugate gradient <ref> [Press et al., 1988] </ref> with great success. All optimizations reported in this paper use conjugate gradient, as it is well known, available publically, and for most purposes is a parameter free method. <p> Though this can be a problem in general, we have not found it to be a significant problem in the application presented in this paper. In the event that local minima do cause problems for these optimization methods, alternatives are available (e.g., Simulated Annealing <ref> [Press et al., 1988] </ref>) which can increase the likelihood of finding the global optimum, though typically at greater computational expense. An additional limitation is that these optimization methods may "over-fit" the training samples.
Reference: [Raghavan and Wong, 1986] <author> Raghavan, V. V. and Wong, S. K. M. </author> <year> (1986). </year> <title> A critical analysis of vector space model for information retrieval. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 37(5) </volume> <pages> 279-287. </pages>
Reference-contexts: Our definition of the ranking function R fi;q (d) is meant to be quite general, so that many (or most) of the common models of ranked-output text retrieval, e.g., the Vec Bartell 11 tor Space Model <ref> [Salton and McGill, 1983, Raghavan and Wong, 1986] </ref>, probabilistic retrieval [Bookstein and Swanson, 1974], and many spreading activation models [Belew, 1986, Mozer, 1984], can be included as instances of R fi;q (d). As an example, consider the well-known Vector Space Model (VSM).
Reference: [Raveh, 1989] <author> Raveh, A. </author> <year> (1989). </year> <title> A nonmetric approach to linear discriminant analysis. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 84(405) </volume> <pages> 176-183. </pages>
Reference-contexts: A similar criterion has been used in other fields, for example, in statistical pattern classification to separate classes in a nonmetric discriminant analysis task <ref> [Raveh, 1989] </ref>. This function is an average, over the queries in Q, of the rank match between q and R fi;q for the documents in D.
Reference: [Rocchio Jr., 1971] <author> Rocchio Jr., J. J. </author> <year> (1971). </year> <title> Relevance feedback in information retrieval. In The Smart System Experiments in Automatic Document Processing, </title> <address> pages 337-354. NJ: </address> <publisher> Prentice Hall, Inc. </publisher>
Reference-contexts: That is, the parameter modifications may have a tendency to improve the system's ranking performance, but this is not an explicit goal of the procedure. Term weight modification algorithms are examples of these methods <ref> [Brauen, 1971, Ide, 1971, Rocchio Jr., 1971] </ref> (a number of methods are compared in [Salton and Buckley, 1990]). <p> This emphasis on rank order departs from a number of the typical methods for adaptively incorporating relevance assessments into the system's operation. As exemplified by the work of Brauen, et al., [Brauen, 1971], Ide [Ide, 1971], and Rocchio <ref> [Rocchio Jr., 1971] </ref>, numerous methods use relevance assessments as an absolute measure of relevance. For example, documents are made more or less similar to a query each time they are found to be relevant or irrelevant. This modification is performed irrespective of the relationship of other documents to the query.
Reference: [Rose, 1991] <author> Rose, D. E. </author> <year> (1991). </year> <title> A symbolic and connectionist approach to legal information retrieval. </title> <type> PhD thesis, </type> <institution> UC San Diego; Computer Science and Cognitive Science. </institution> <address> Bartell 57 </address>
Reference-contexts: Similar to Ide's approach, Crestani's [Crestani, 1993] neural network method learns query representations that are more similar to relevant document representations in order to find additional documents relevant to the user's need. Other "implicit" approaches include Belew's adaptive connectionist IR model [Belew, 1986, Belew, 1989] (see also <ref> [Rose, 1991] </ref>), which increases the association between the documents and document features (e.g., terms and citations) that contribute to finding relevant documents for a query.
Reference: [Rose et al., 1993] <author> Rose, D. E., Mander, R., Oren, T., Ponceleon, D. B., Salomon, G., and Wong, Y. Y. </author> <year> (1993). </year> <title> Content awareness in a file system interface: Implementing the `Pile' metaphor for organizing information. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 260-269, </pages> <address> Pittsburgh, PA. </address>
Reference-contexts: Perhaps contrary to this assumption, retrieval systems have been proposed which emphasize alternative methods for displaying retrieved documents other than the linear list (e.g., <ref> [Cutting et al., 1992, Rose et al., 1993] </ref>). However, whether or not the retrieved documents are displayed to the user in a sorted list, the proposed method may still be applicable if the system makes any differentiation based on the system-generated relevance estimates.
Reference: [Rosenblatt, 1962] <author> Rosenblatt, F. </author> <year> (1962). </year> <title> Principle of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. </title> <publisher> Spartan Books, </publisher> <address> Washington, D.C. </address>
Reference-contexts: Alternatives from the statistics literature and Multidimensional Scaling (MDS) include Kruskal's stress function [Kruskal, 1964], a variation of Pearson's correlation coefficient [Borg and Lingoes, 1987], and the mono-tonicity coefficient of Guttman [Guttman, 1978]. In addition, the Perceptron Learning Rule <ref> [Rosenblatt, 1962] </ref> is a well known criterion from the neural network literature that can also be used as a measure of rank correctness, as has been done by Wong and Yao [Wong and Yao, 1990, Wong et al., 1993] in their adaptive information retrieval approach. <p> As in our approach, Wong & Yao optimize the document preference relation by defining a differentiable criterion measuring how well the system is ranking documents, and optimizing this function using gradient methods. The criterion is derived from the Perceptron Learning Rule <ref> [Rosenblatt, 1962] </ref>, one of the classic (and most well understood) learning rules in the field of Neural Networks.
Reference: [Ross and Wright, 1988] <author> Ross, K. A. and Wright, C. R. B. </author> <year> (1988). </year> <note> Discrete Mathematics. Prentice-Hall, 2nd edition. </note>
Reference-contexts: We formalize this "desired ordering" using the notation of Wong and Yao [Wong and Yao, 1990]: q is defined as the Document Preference Relation, a binary relation over document pairs for a given query, q. q is a quasi-order <ref> [Ross and Wright, 1988] </ref> over document pairs and defines the preference structure of documents for the query q.
Reference: [Salton, 1971] <author> Salton, G., </author> <title> editor (1971). The Smart Retrieval System Experiments in Automatic Document Processing. </title> <address> NJ: </address> <publisher> Prentice Hall, Inc. </publisher>
Reference-contexts: In all of our experiments, q and d are derived from query q and document d using the Term Frequency/Inverse Document Frequency weighting scheme [Salton and Buckley, 1988] implemented by the SMART retrieval engine <ref> [Salton, 1971] </ref>. The vectors are not length normalized, because the parameterized similarity measure will perform that task. That is, different values of fi instantiate different vector normalizations.
Reference: [Salton, 1980] <author> Salton, G. </author> <year> (1980). </year> <title> Automatic term class construction using relevance a summary of work in automatic pseudoclassification. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 16 </volume> <pages> 1-15. </pages>
Reference-contexts: Document representations are learned which optimize a measure of goodness based on fallout and recall. Gordon's approach has great potential for applications involving parameters other than document term representations, though application of the technique can be exceptionally expensive computationally. Many pseudoclassification methods (e.g., <ref> [Salton, 1980] </ref>) are also heuristic. These methods search for optimal term equiva Bartell 8 lence classes (synonym classes) in an explicit attempt to improve the ranking performance of the retrieval system.
Reference: [Salton and Buckley, 1988] <author> Salton, G. and Buckley, C. </author> <year> (1988). </year> <title> Term-weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523. </pages>
Reference-contexts: In all of our experiments, q and d are derived from query q and document d using the Term Frequency/Inverse Document Frequency weighting scheme <ref> [Salton and Buckley, 1988] </ref> implemented by the SMART retrieval engine [Salton, 1971]. The vectors are not length normalized, because the parameterized similarity measure will perform that task. That is, different values of fi instantiate different vector normalizations.
Reference: [Salton and Buckley, 1990] <author> Salton, G. and Buckley, C. </author> <year> (1990). </year> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297. </pages>
Reference-contexts: That is, the parameter modifications may have a tendency to improve the system's ranking performance, but this is not an explicit goal of the procedure. Term weight modification algorithms are examples of these methods [Brauen, 1971, Ide, 1971, Rocchio Jr., 1971] (a number of methods are compared in <ref> [Salton and Buckley, 1990] </ref>). These methods ma Bartell 5 nipulate vector-space representations of documents and queries, typically adjusting term weights to make relevant documents more similar to applicable queries, and occasionally irrelevant documents less similar to applicable queries. <p> We have not pursued this direction in the current research, though others have demonstrated that this general approach can be very productive <ref> [Ide, 1971, Salton and Buckley, 1990] </ref>. In addition, as expected, the system's ability to properly handle novel queries improves when additional queries contribute to the parameter estimation. <p> A second response is, if specialized performance is required for any single query, then the Bartell 50 parameters can be optimized using only that single query. This general approach is well known to enhance performance <ref> [Salton and Buckley, 1990] </ref>. A third response is that this "averaging" problem can be avoided, in most cases, by increasing the expressiveness of the retrieval model. That is, the model is made powerful enough to differentiate between queries that require alternative parameter settings.
Reference: [Salton and McGill, 1983] <author> Salton, G. and McGill, M. J. </author> <year> (1983). </year> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, Inc. </publisher>
Reference-contexts: Our definition of the ranking function R fi;q (d) is meant to be quite general, so that many (or most) of the common models of ranked-output text retrieval, e.g., the Vec Bartell 11 tor Space Model <ref> [Salton and McGill, 1983, Raghavan and Wong, 1986] </ref>, probabilistic retrieval [Bookstein and Swanson, 1974], and many spreading activation models [Belew, 1986, Mozer, 1984], can be included as instances of R fi;q (d). As an example, consider the well-known Vector Space Model (VSM).
Reference: [Shepard, 1962] <author> Shepard, R. N. </author> <year> (1962). </year> <title> The analysis of proximities: Multidimensional scaling with an unknown distance function. I. </title> <journal> Psychometrika, </journal> <volume> 27(2) </volume> <pages> 125-139. </pages> <address> Bartell 58 </address>
Reference-contexts: We propose Guttman's Point Alienation, a rank order statistic motivated by the statistics of rank order [Press et al., 1988] and by techniques in the field of Multidimensional Scaling (MDS) <ref> [Shepard, 1962, Kruskal, 1964, Borg and Lingoes, 1987] </ref>, as a useful criterion for how well the system is matching the users' target rankings. Optimizing the criterion automatically via gradient descent techniques adjusts parameter values so that the system ranks documents more similarly to the users' own preference order.
Reference: [Sietsma and Dow, 1988] <author> Sietsma, J. and Dow, R. J. F. </author> <year> (1988). </year> <title> Neural net pruning Why and how. </title> <booktitle> In Proceedings of the IEEE Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 325-333, </pages> <address> San Diego. </address>
Reference-contexts: Techniques exist to alleviate this problem, such as using a cross-validation set of samples to determine when to terminate optimization [Plutowski et al., 1993], or adding noise to the optimization <ref> [Sietsma and Dow, 1988] </ref>. However, over-fitting has not been a problem in the experiments reported in this paper, likely due to the limitations of the similarity model chosen.
Reference: [Valiant, 1984] <author> Valiant, L. G. </author> <year> (1984). </year> <title> Theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142. </pages> <editor> [van Rijsbergen, 1983] van Rijsbergen, C. J. </editor> <booktitle> (1983). Information Retrieval. </booktitle> <publisher> Butterworths, </publisher> <address> London, England, </address> <note> second edition. </note>
Reference-contexts: Our approach is to allow as many query samples as are available to contribute to determining the optimal parameter settings, to increase the likelihood of good generalization <ref> [Valiant, 1984] </ref>. That is, we have not applied any cross-validation techniques to the training set [Plutowski et al., 1993]. <p> In general, we should expect that the larger the number of queries used to optimize the parameters, the more likely the parameter values will be useful (that is, generalize well) to new situations involving new queries or new documents <ref> [Valiant, 1984] </ref>. In the CISI collection, there are a total of 76 queries with corresponding relevance-tagged documents. In the optimization in the preceding section, 51 of these queries are used to train and 25 to test.
Reference: [Wong et al., 1993] <author> Wong, S. K. M., Cai, Y. J., and Yao, Y. Y. </author> <year> (1993). </year> <title> Computation of term associations by a neural network. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <address> Pittsburgh, PA. </address>
Reference-contexts: Many pseudoclassification methods (e.g., [Salton, 1980]) are also heuristic. These methods search for optimal term equiva Bartell 8 lence classes (synonym classes) in an explicit attempt to improve the ranking performance of the retrieval system. Wong & Yao <ref> [Wong and Yao, 1990, Wong et al., 1993] </ref> have proposed an adaptive method for learning query and document term weights which best rank relevant documents before irrelevant ones. <p> In addition, the Perceptron Learning Rule [Rosenblatt, 1962] is a well known criterion from the neural network literature that can also be used as a measure of rank correctness, as has been done by Wong and Yao <ref> [Wong and Yao, 1990, Wong et al., 1993] </ref> in their adaptive information retrieval approach. Squared Error, used by Fuhr & Buckley [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993], is an additional option, though it is applicable only to binary relevant/irrelevant relevance assessments and not to the more general quasi-orders. <p> This gives evidence for the robustness of this correlation across different collections. 4 Comparison With Alternative Methods We now compare our method to two alternatives which are motivated by approaches proposed by Wong & Yao <ref> [Wong and Yao, 1990, Wong et al., 1993] </ref> and Fuhr & Buckley [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993]. Both of the alternatives differ from our approach in the choice of the criterion used to evaluate the performance of the retrieval system. <p> In one application of their method to a small text collection (the ADINUL database, with 85 documents and 35 queries), the method learned document term weights which resulted in an average 62% improvement in average precision performance <ref> [Wong et al., 1993] </ref>. As in our approach, Wong & Yao optimize the document preference relation by defining a differentiable criterion measuring how well the system is ranking documents, and optimizing this function using gradient methods. <p> The criterion is derived from the Perceptron Learning Rule [Rosenblatt, 1962], one of the classic (and most well understood) learning rules in the field of Neural Networks. The criterion they use <ref> [Wong et al., 1993] </ref> is: P (A) = (q;b)2 (A) where (A) = fq; d 0 d j d q d 0 and d 0 T This notation requires some clarification. First, q is a column vector representing the terms in a query q using standard vector space methods.
Reference: [Wong and Yao, 1990] <author> Wong, S. K. M. and Yao, Y. Y. </author> <year> (1990). </year> <title> Query formulation in linear retrieval models. </title> <journal> Journal of the American Society for Information Retrieval, </journal> <volume> 41(5) </volume> <pages> 334-341. </pages>
Reference-contexts: Many pseudoclassification methods (e.g., [Salton, 1980]) are also heuristic. These methods search for optimal term equiva Bartell 8 lence classes (synonym classes) in an explicit attempt to improve the ranking performance of the retrieval system. Wong & Yao <ref> [Wong and Yao, 1990, Wong et al., 1993] </ref> have proposed an adaptive method for learning query and document term weights which best rank relevant documents before irrelevant ones. <p> The notion of "best ranking" is with respect to a desired ordering over the documents for each query. We formalize this "desired ordering" using the notation of Wong and Yao <ref> [Wong and Yao, 1990] </ref>: q is defined as the Document Preference Relation, a binary relation over document pairs for a given query, q. q is a quasi-order [Ross and Wright, 1988] over document pairs and defines the preference structure of documents for the query q. <p> In addition, the Perceptron Learning Rule [Rosenblatt, 1962] is a well known criterion from the neural network literature that can also be used as a measure of rank correctness, as has been done by Wong and Yao <ref> [Wong and Yao, 1990, Wong et al., 1993] </ref> in their adaptive information retrieval approach. Squared Error, used by Fuhr & Buckley [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993], is an additional option, though it is applicable only to binary relevant/irrelevant relevance assessments and not to the more general quasi-orders. <p> This gives evidence for the robustness of this correlation across different collections. 4 Comparison With Alternative Methods We now compare our method to two alternatives which are motivated by approaches proposed by Wong & Yao <ref> [Wong and Yao, 1990, Wong et al., 1993] </ref> and Fuhr & Buckley [Fuhr and Buckley, 1991, Fuhr and Buckley, 1993]. Both of the alternatives differ from our approach in the choice of the criterion used to evaluate the performance of the retrieval system.
References-found: 48

