URL: http://www.sls.lcs.mit.edu/~ilh/thesis/thesis.ps
Refering-URL: http://www.sls.lcs.mit.edu/ilh/
Root-URL: 
Title: A Characterization of the Problem of New, Out-of-Vocabulary Words in Continuous-Speech Recognition and Understanding  
Author: by Irvine Lee Hetherington S.B. and S.M., Victor W. Zue 
Degree: (1989) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Electrical Engineering and Computer Science at the  c Irvine Lee Hetherington, MCMXCV. All rights reserved. The author hereby grants to MIT permission to reproduce and distribute publicly paper and electronic copies of this thesis document in whole or in part, and to grant others the right to do so. Author  Certified by  Senior Research Scientist Thesis Supervisor Accepted by Frederic R. Morgenthaler Chair, Departmental Committee on Graduate Students  
Date: February 1995  October 13, 1994  
Affiliation: Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> F. Alleva and K.-F. Lee, </author> <title> "Automatic new word acquisition: Spelling from acoustics," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 266-270, </pages> <address> Harwichport, MA, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: Such a reversible system could be particularly useful in the context of learning new words, because both directions could be put to use. We should point out that by "sound," they meant phonemes plus stress markers. Alleva and Lee <ref> [1] </ref> developed an HMM-based system in which they modeled the acoustics of letters directly. Associated with each context-dependent letter, a letter trigram, was an HMM model. Sound-to-letter was achieved by decoding the most likely sequence of letters directly, eliminating the need to go through the intermediate step of phonetic recognition.
Reference: [2] <author> A. Asadi, R. Schwartz, and J. Makhoul, </author> <title> "Automatic detection of new words in a large-vocabulary continuous speech recognition system," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 125-128, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Here, the realization of both words has been affected. Such phonological effects at the boundaries of new words will also complicate the precise location of them during detection. 1.2.4 Comments There was virtually no work on the problem of new words before Asadi et al. <ref> [2] </ref> first investigated the detection problem. Since then, the amount of research on the detection and learning problems has increased. While this is encouraging, we feel that the new-word problem is still not getting the attention it deserves.
Reference: [3] <author> A. Asadi, R. Schwartz, and J. Makhoul, </author> <title> "Automatic modeling for adding new words to a large-vocabulary continuous speech recognition system," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 305-308, </pages> <address> Toronto, </address> <month> May </month> <year> 1991. </year>
Reference: [4] <author> A. O. Asadi, </author> <title> Automatic Detection and Modeling of New Words in a Large-Vocabulary Continuous Speech Recognition System, </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical and Computer Engineering, Northeastern University, </institution> <address> Boston, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: They also examined the learning problem (see Section 1.2.3). Their research was carried out on the Resource Management (RM) task [58], using the BBN BYBLOS continuous speech recognition system <ref> [4, 14, 15] </ref>. The BYBLOS system used HMMs and a statistical class bigram language model. It is important to note that because the utterances in the RM task were generated artificially from a finite-state grammar, there were no true new words.
Reference: [5] <author> A. O. Asadi and H. C. Leung, </author> <title> "New-word addition and adaptation in a stochastic explicit-segment speech recognition system," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 5, </volume> <pages> pp. 642-645, </pages> <address> Minneapolis, </address> <month> April </month> <year> 1993. </year>
Reference: [6] <institution> Association for Computational Linguistics Data Collection Initiative, "CD-ROM I," </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: We removed spontaneous speech disfluencies from the orthographic transcriptions. The wsj [55] and nyt corpora consist of English text from the Wall Street Journal and the New York Times newspapers, respectively. The text for wsj was made available by the ACL Data Collection Initiative <ref> [6] </ref> and represents three years (1987-1989) of newspaper text. The text for nyt was collected over a period of three months in early 1994 via a newswire service. The bref corpus consists of read utterances collected by LIMSI-CNRS [24,39].
Reference: [7] <author> X. Aubert, C. Dugast, H. Ney, and V. Steinbiss, </author> <title> "Large vocabulary continuous speech recognition of the Wall Street Journal data," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 129-132, </pages> <address> Adelaide, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The development of our A* word graph algorithm, first reported in [27], was conducted independently of the other word graph research presented in this section. Oerder and Ney [48] and Aubert et al. <ref> [7] </ref> at Philips present word graphs, similar to those we present in this chapter, which interface speech recognition and natural language components. Their algorithm also makes use of two passes in opposite directions. In the first pass forward in time, the "word hypotheses generator" produces word hypotheses 4.5. <p> Furthermore, the score thresholds used in their system are undoubtedly different from ours, and they did not relate their word graph size to N -best list size. Overall, the word graph algorithm developed at Philips <ref> [7, 48] </ref> sounds promising. The fact that it can begin generating word edges in the first pass may mean that the first and second passes could run in parallel, whereas our second pass cannot begin until the first Viterbi pass is complete. <p> Murveit et al. at SRI [44] use a multi-stage search technique that produces word lattices as an intermediate representation. Their "forward-backward word-life" algorithm generates word lattices in forward and backward passes. The algorithm is similar to that used in the Philips system <ref> [7, 48] </ref> in that word edges are accumulated during the first Viterbi-style pass and then pruned in the second pass in the opposite direction.
Reference: [8] <author> A. Barr, E. Feigenbaum, and P. Cohen, </author> <booktitle> The Handbook of Artificial Intelligence. </booktitle> <publisher> William Kaufman, </publisher> <address> Los Altos, CA, </address> <year> 1981. </year>
Reference-contexts: We first describe the general A* algorithm, then the N -best algorithm, and finally the word graph algorithm. 4.2.1 A* Search An A* search <ref> [8, 47] </ref> is a best-first search with a particular evaluation function f fl (p) for a (partial) path, or hypothesis, p in the search space: f fl (p) = g (p) + h fl (p): Here, f fl (p) is the estimated score of the best complete path containing p, g
Reference: [9] <author> R. A. Becker, J. M. Chambers, and A. R. Wilks, </author> <title> The New S Language: a Programming Environment for Data Analysis and Graphics. </title> <publisher> Wadsworth and Brooks/Cole, </publisher> <address> Pacific Grove, CA, </address> <year> 1988. </year>
Reference-contexts: The word graph search is better behaved, requiring about two orders of strings. 6 The smoother is lowess procedure in S <ref> [9] </ref>, which produces smooth, robust, locally linear fits of the scatter plot points. The line for N-best stops at 10 seconds because some of the longer utterances required too much computation to reach the search depth and were not included in the plot.
Reference: [10] <author> H. Bonneau-Maynard, J.-L. Gauvain, D. Goodine, L. F. Lamel, J. Polifroni, and S. Seneff, </author> <title> "A French version of the MIT-ATIS system: Portability issues," </title> <booktitle> in Proc. European Conf. Speech Communication and Technology, </booktitle> <pages> pp. 2059-2062, </pages> <address> Berlin, </address> <month> September </month> <year> 1993. </year> <note> 167 168 BIBLIOGRAPHY </note>
Reference-contexts: Table 2-2: Example utterances/sentences from the corpora. 36 CHAPTER 2. A LEXICAL, PHONOLOGICAL, AND LINGUISTIC STUDY vocabulary size, primarily due to a larger number of cities and airports. 1 In contrast, f-atis <ref> [10] </ref> includes only those cities and airports that are a part of ATIS-2. Utterances were collected from users trying to solve travel planning problems through interaction with a spoken language system.
Reference: [11] <author> E. Brill, </author> <title> A Corpus-Based Approach to Language Learning, </title> <type> Ph.D. thesis, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, Philadel-phia, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In general, the problem of adding new words to a language model is probably more difficult, as evidenced by the work of Jelinek et al. However, it is evident that a language model defined in terms of word classes may be advantageous for learning. Brill <ref> [11] </ref> studied the problem of assigning part-of-speech to new words. This work was a component of a part-of-speech tagger used to tag large bodies of text automatically. The system needed to assign tags to new words that were not seen in training material. <p> We chose to examine parts-of-speech of potential new words using the wsj and atis corpora. For wsj, we used the 57,712 hand-tagged sentences that came with the corpus. For atis, we automatically tagged the entire corpus of 26,583 utterances. We performed the tagging using Brill's part-of-speech tagger <ref> [11] </ref> trained using nearly 1,800 hand-tagged utterances. For atis, we corrected by hand the tags of the words that occurred only once. For the purposes of part-of-speech analysis, we set the vocabulary for each task to include all the words that occurred at least twice (i.e., n = 2).
Reference: [12] <author> B. Chigier and J. Spitz, </author> <title> "Are laboratory databases appropriate for training and testing telephone speech recognizers?," </title> <booktitle> in Proc. Int. Conf. Spoken Language Processing, </booktitle> <pages> pp. 1017-1020, </pages> <address> Kobe, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: The i-voyager corpus is similar, except that the utterances are in Italian [22]. Utterances were again collected using a human "wizard." Again, we removed spontaneous speech disfluencies from the orthographic transcriptions. The citron corpus consists of utterances collected by NYNEX from actual directory assistance telephone calls <ref> [12, 68] </ref>. The users interacted with human operators. The switchboard corpus consists of spontaneous human/human dialogs collected by Texas Instruments [25]. The dialogs are based on a large set of predefined topics. The topics were selected to be of general interest and to encourage active discussion.
Reference: [13] <author> Y. Chow and R. Schwartz, </author> <title> "The N-best algorithm," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 199-202, </pages> <address> Harwichport, MA, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: In SUMMIT, this beam is so wide that this pruning introduces negligible search errors. 3.7.2 A* Backward Search SUMMIT does not compute the N -best word sequences in the first-stage Viterbi search, even though it is possible <ref> [13, 62, 64, 73] </ref>, because an A* search is more efficient in terms of both time and memory. SUMMIT's A* search is performed backwards in time from the end of the utterance and uses information computed during the forward Viterbi search [73]. <p> Such a list represents a drastically reduced search space in which to evaluate these more expensive models. The general practice of re-scoring and re-sorting hypotheses is called N -best re-sorting and is used by many systems <ref> [13, 51, 62-64] </ref>. In summary, N -best lists are useful for two purposes: to provide alternative recognizer hypotheses for natural language processing, and to provide a multi-stage mechanism for applying more computationally expensive modeling (e.g., context-dependent acoustic modeling) and constraints. <p> Initial work in combining speech recognition and natural language technology used a modification of the Viterbi search to provide the N -best sentence hypotheses, as proposed by Chow and Schwartz at BBN <ref> [13] </ref>, and showed that at least for some tasks, the correct answer was very often in the top N sentence hypotheses for fairly small N , and therefore an N -best list would provide a useful interface between a speech recognition system and a natural language parser.
Reference: [14] <author> Y. L. Chow, M. O. Dunham, O. A. Kimball, M. A. Krasner, G. F. Kubala, J. Makhoul, P. J. Price, S. Roucos, and R. M. Schwartz, </author> <title> "BYBLOS: The BBN continuous speech recognition system," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 89-92, </pages> <address> Dallas, </address> <month> April </month> <year> 1987. </year>
Reference-contexts: They also examined the learning problem (see Section 1.2.3). Their research was carried out on the Resource Management (RM) task [58], using the BBN BYBLOS continuous speech recognition system <ref> [4, 14, 15] </ref>. The BYBLOS system used HMMs and a statistical class bigram language model. It is important to note that because the utterances in the RM task were generated artificially from a finite-state grammar, there were no true new words.
Reference: [15] <author> Y. L. Chow, M. O. Dunham, O. A. Kimball, M. A. Krasner, G. F. Kubala, J. Makhoul, P. J. Price, S. Roucos, and R. M. Schwartz, </author> <title> "BYBLOS: The BBN continuous speech recognition system," </title> <editor> in A. Waibel and K.-F. Lee (eds.), </editor> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pp. 596-599. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: They also examined the learning problem (see Section 1.2.3). Their research was carried out on the Resource Management (RM) task [58], using the BBN BYBLOS continuous speech recognition system <ref> [4, 14, 15] </ref>. The BYBLOS system used HMMs and a statistical class bigram language model. It is important to note that because the utterances in the RM task were generated artificially from a finite-state grammar, there were no true new words.
Reference: [16] <author> K. W. Church, </author> <title> "A stochastic parts program and noun phrase parser for unrestricted text," </title> <booktitle> in Proc. 2nd Conf. on Applied Nat. Lang. Processing, </booktitle> <pages> pp. 136-143, </pages> <address> Austin, TX, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: Words that occur only once in a corpus are the words most likely to be missed when building a vocabulary empirically. For our analysis of new-word usage we chose to examine syntactic part-of-speech tags. We collapsed a large set of forty-eight tags <ref> [16] </ref> down to eleven: proper nouns, nouns, adjectives, adverbs, verbs, conjunctions, pronouns, numbers, determiners, prepositions, and "other." One aspect of the new-word problem that we were particularly interested in examining was the fraction of new words that are names (i.e., proper nouns).
Reference: [17] <author> D. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-Smith, D. Pallett, C. Pao, A. Rudnicky, and L. Shriberg, </author> <title> "Expanding the scope of the ATIS task: The ATIS-3 corpus," </title> <booktitle> in Proc. ARPA Human Lang. Tech. Workshop, </booktitle> <address> Princeton, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: For example, in December of 1993, fourteen sites, four of them outside the U. S., took part in Advanced Research Project Agency's (ARPA) evaluation of speech recognition and understanding systems. The Air Travel Information Service (ATIS) task <ref> [17, 59] </ref> was used for recognition and understanding and consisted of spontaneous utterances regarding airline flight information. The Wall Street Journal (WSJ) task [55] was used for recognition only and consisted of read speech drawn from newspaper articles from the Wall Street Journal. <p> The atis and f-atis corpora consist of spontaneous speech utterances collected interactively for ARPA's Air Travel Information Service (ATIS) common task and are in English and French, respectively. The atis corpus contains utterances from both the so-called ATIS-2 and ATIS-3 sets <ref> [17, 29, 53] </ref>. ATIS-3 represents an increase in the 2.2.
Reference: [18] <author> V. Digalakis, </author> <title> Segment-Based Stochastic Models of Spectral Dynamics for Continuous Speech Recognition, </title> <type> Ph.D. thesis, </type> <institution> Boston University, </institution> <address> Boston, </address> <month> June </month> <year> 1992. </year>
Reference: [19] <author> V. Digalakis, J. R. Rohlicek, and M. Ostendorf, </author> <title> "A dynamical system approach to continuous speech recognition," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 253-257, </pages> <address> Pacific Grove, CA, </address> <month> February </month> <year> 1991. </year>
Reference: [20] <author> V. Digalakis, J. R. Rohlicek, and M. Ostendorf, </author> <title> "A dynamical system approach to continuous speech recognition," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 289-292, </pages> <address> Toronto, </address> <month> May </month> <year> 1991. </year>
Reference: [21] <author> V. Digalakis, J. R. Rohlicek, and M. Ostendorf, </author> <title> "ML estimation of a stochastic linear system with the EM algorithm and its application to speech recognition," </title> <journal> IEEE Trans. Speech and Audio Processing, </journal> <volume> 1(4) </volume> <pages> 431-442, </pages> <month> October </month> <year> 1993. </year>
Reference: [22] <author> G. Flammia, J. Glass, M. Phillips, J. Polifroni, S. Seneff, and V. Zue, </author> <title> "Porting the bilingual VOYAGER system to Italian," </title> <booktitle> in Proc. Int. Conf. Spoken Language Processing, </booktitle> <pages> pp. 911-914, </pages> <address> Yokohama, </address> <month> September </month> <year> 1994. </year> <note> BIBLIOGRAPHY 169 </note>
Reference-contexts: We used orthographic transcriptions of the utterances with spontaneous speech disfluencies removed. The voyager corpus consists of spontaneous speech utterances in English collected interactively for the MIT voyager urban navigation and exploration system [76]. The i-voyager corpus is similar, except that the utterances are in Italian <ref> [22] </ref>. Utterances were again collected using a human "wizard." Again, we removed spontaneous speech disfluencies from the orthographic transcriptions. The citron corpus consists of utterances collected by NYNEX from actual directory assistance telephone calls [12, 68]. The users interacted with human operators.
Reference: [23] <author> J.-L. Gauvain, L. F. Lamel, G. Adda, and M. Adda-Decker, </author> <title> "The LIMSI Nov93 WSJ system," </title> <booktitle> in Proc. ARPA Spoken Lang. Sys. Tech. Workshop, </booktitle> <address> Princeton, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Three years ago, the sentence error rate was nearly three times larger on the same task but with a smaller vocabulary. On the WSJ task, the lowest word-error rate for a 20,000-word system was 11.2%, and for a 5,000-word system the lowest was 5.3% <ref> [23] </ref>. 1 The word-error rate takes into account, for each utterance, the number of word substitutions, deletions, and insertions. The %word-error is defined as %substitutions + %deletions + %insertions. 2 The "answerable" utterances included the class "A" (dialog-independent) and class "D" (dialog-dependent) utterances.
Reference: [24] <author> J.-L. Gauvain, L. F. Lamel, and M. Eskenazi, </author> <title> "Design considerations and text selection for BREF, a large French read-speech corpus," </title> <booktitle> in Proc. Int. Conf. Spoken Language Processing, </booktitle> <pages> pp. 1097-1100, </pages> <address> Kobe, </address> <month> November </month> <year> 1990. </year>
Reference: [25] <author> J. J. Godfrey, E. C. Holliman, and J. McDaniel, </author> <title> "SWITCHBOARD: Telephone speech corpus for research and development," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 517-520, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: The citron corpus consists of utterances collected by NYNEX from actual directory assistance telephone calls [12, 68]. The users interacted with human operators. The switchboard corpus consists of spontaneous human/human dialogs collected by Texas Instruments <ref> [25] </ref>. The dialogs are based on a large set of predefined topics. The topics were selected to be of general interest and to encourage active discussion. We include both sides of dialogs in our study. We removed spontaneous speech disfluencies from the orthographic transcriptions.
Reference: [26] <author> D. Goodine, S. Seneff, L. Hirschman, and M. Phillips, </author> <title> "Full integration of speech and language understanding in the MIT spoken language system," </title> <booktitle> in Proc. Euro-pean Conf. Speech Communication and Technology, </booktitle> <pages> pp. 845-848, </pages> <address> Genoa, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The words "beef fried" are joined by a geminate /f/, which is longer than a normal /f/. The range 0.55-0.58 s, which appears to be a pause, is the closure for the /d/ in "fried." natural language system is closely integrated into the search <ref> [26] </ref>, providing linguistically sensible word extensions to restrict the search space. The ideal speech recognition system is speaker-independent, has a large vocabulary, and can operate on spontaneous, continuous speech. <p> N -best algorithms have found widespread use in systems that combine speech recognition and natural language understanding, such as the systems at BBN [62] and MIT [73]. Although there have been efforts toward integrating the natural language constraints into the search itself, such as at MIT <ref> [26] </ref> and SRI [45], N -best strategies have remained popular not only because of their ease of implementation, but also because they greatly improve the efficiency of the development effort, since one can precompute N -best lists for a large corpus to use as input for natural language experiments.
Reference: [27] <author> I. L. Hetherington, M. S. Phillips, J. R. Glass, and V. W. Zue, </author> <title> "A* word network search for continuous speech recognition," </title> <booktitle> in Proc. European Conf. Speech Communication and Technology, </booktitle> <pages> pp. 1533-1536, </pages> <address> Berlin, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: The search makes use of a priority queue 2 which ranks entries using the scoring function f fl . In general, the A* search falls somewhere between a best-first and a breadth-first search, depending on the quality of the heuristic and the actual (data-dependent) search space. 1 Previously, in <ref> [27] </ref> we called this algorithm the A* word network search algorithm. 2 The priority queue is often called a sorted stack in speech recognition literature after its use in the stack-decoding algorithm [32], which is closely related to the A* algorithm. 4.2. <p> Evidently, word graphs/networks/lattices are becoming an increasingly popular alternative to N -best lists. Judging by the dates of publication, all of this research, including our own, was performed in the same time period of 1992-1994. The development of our A* word graph algorithm, first reported in <ref> [27] </ref>, was conducted independently of the other word graph research presented in this section. Oerder and Ney [48] and Aubert et al. [7] at Philips present word graphs, similar to those we present in this chapter, which interface speech recognition and natural language components.
Reference: [28] <author> I. L. Hetherington and V. W. Zue, </author> <title> "New words: Implications for continuous speech recognition," </title> <booktitle> in Proc. European Conf. Speech Communication and Technology, </booktitle> <pages> pp. 2121-2124, </pages> <address> Berlin, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: However, when compared to the vocabulary in an unweighted fashion, the length distribution was very similar. Suhm et al. also studied the introduction of a new-word class into a statistical word 5 The study of Suhm et al. [69] was reported concurrently with our initial study <ref> [28] </ref> at Eurospeech '93. However, their study is less general in that it involved only one language (English) and one domain (Wall Street Journal). 1.2. PRIOR RESEARCH 23 trigram language model. In this study they mapped all out-of-vocabulary words to the new-word class.
Reference: [29] <author> L. Hirschman, M. Bates, D. Dahl, W. Fisher, J. Garofolo, K. Hunicke-Smith, D. Pallett, C. Pao, P. Price, and A. Rudnicky, </author> <title> "Multi-site data collection for a spoken language corpus," </title> <booktitle> in Proc. Int. Conf. Spoken Language Processing, </booktitle> <pages> pp. 903-906, </pages> <address> Banff, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The atis and f-atis corpora consist of spontaneous speech utterances collected interactively for ARPA's Air Travel Information Service (ATIS) common task and are in English and French, respectively. The atis corpus contains utterances from both the so-called ATIS-2 and ATIS-3 sets <ref> [17, 29, 53] </ref>. ATIS-3 represents an increase in the 2.2.
Reference: [30] <author> S. Hunnicutt, H. Meng, S. Seneff, and V. Zue, </author> <title> "Reversible letter-to-sound sound-to-letter generation based on parsing word morphology," </title> <booktitle> in Proc. European Conf. Speech Communication and Technology, </booktitle> <pages> pp. 763-766, </pages> <address> Berlin, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Conversely, if the user 28 CHAPTER 1. INTRODUCTION types in the spelling of a new word when adding it to a system, letter-to-sound rules could be used to generate a pronunciation model of it as Asadi et al. [3-5] did. Meng et al. <ref> [30, 42, 43] </ref> developed a reversible letter-to-sound/sound-to-letter generation system using an approach that combined a multi-level rule-based formalism with data-driven techniques. Such a reversible system could be particularly useful in the context of learning new words, because both directions could be put to use.
Reference: [31] <author> K. Itou, S. Hayamizu, and H. Tanaka, </author> <title> "Detection of unknown words and automatic estimation of their transcriptions in continuous speech recognition," </title> <booktitle> in Proc. Int. Conf. Spoken Language Processing, </booktitle> <pages> pp. 799-802, </pages> <address> Banff, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: In their evaluation data, they had only fourteen phrases that contained new words. The number of new-word occurrences was so small that it is difficult to draw any conclusions from their results. Itou et al. <ref> [31] </ref> also performed joint recognition and new-word transcription in continuous Japanese speech. They used HMMs with context-independent phone models and a stochastic phone grammar. Overall, their system achieved a correct-detection rate of 75% at a false-alarm rate of 11%.
Reference: [32] <author> F. Jelinek, </author> <title> "Fast sequential decoding using a stack," </title> <journal> IBM J. Res. Develop., </journal> <volume> 13(6) </volume> <pages> 675-685, </pages> <month> November </month> <year> 1969. </year>
Reference-contexts: breadth-first search, depending on the quality of the heuristic and the actual (data-dependent) search space. 1 Previously, in [27] we called this algorithm the A* word network search algorithm. 2 The priority queue is often called a sorted stack in speech recognition literature after its use in the stack-decoding algorithm <ref> [32] </ref>, which is closely related to the A* algorithm. 4.2. A* WORD GRAPH SEARCH ALGORITHM 77 The A* search begins with one entry in the queue, an empty path.
Reference: [33] <author> F. Jelinek, </author> <title> "Self-organized language modeling for speech recognition," </title> <editor> in A. Waibel and K.-F. Lee (eds.), </editor> <booktitle> Readings in Speech Recognition, </booktitle> <pages> pp. 450-506. </pages> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: The acoustic models generally model sub-word units such as phones and may be context-independent or context-dependent. The lexical models model the pronunciation of words in the vocabulary and constrain the sequence of sub-word units. The language model, often a statistical n-gram <ref> [33] </ref>, constrains the word order. The search component makes use of acoustic, lexical, and language models to score word sequences. Typically, the best N complete-sentence hypotheses are determined. <p> LEXICAL ACCESS SEARCH 67 interpolated probability P i n () is computed as follows: P i The 's are a function of word-condition counts observed in training data and favor the higher-order models if the training data are sufficient. This interpolated n-gram model is not unique to SUMMIT. Jelinek <ref> [33] </ref> presents a good tutorial of the issues related to n-gram models. Another type of smoothing affects P 1 (w j ). A "floor" constant fl is added to all unigram counts. Thus, P 1 (w j ) = P ; where n (w) is the unigram count for w.
Reference: [34] <author> F. Jelinek, R. Mercer, and S. Roukous, </author> <title> "Classifying words for improved statistical language models," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 621-624, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year> <note> 170 BIBLIOGRAPHY </note>
Reference-contexts: The acoustic, lexical, and language models all may need to be updated when a new word is to be added to a system. If adding words to a system were easy, perhaps even automatic, then the system vocabularies could better adapt to the tasks at hand. Jelinek et al. <ref> [34] </ref> studied the problem of incorporating a new word into a statistical word n-gram language model. Such a model typically requires a very large amount of training data to estimate all the word n-tuple frequencies. <p> If we want to add a new word to such a model, we are faced with a problem: how do we estimate the needed probabilities associated with the new word if we have little or no text containing the new word. Jelinek et al. <ref> [34] </ref> presented a method requiring a few pieces of text containing the new word. Their technique involved finding "statistical synonyms" for the new word, and basing the probabilities for the new word on the statistical synonyms. 5.5. <p> A technique for identifying similar words, or "statistical synonyms," such as that of Jelinek et al. <ref> [34] </ref> could help in such cases. Finally, we did not address the problem of adding new words to a natural language understanding system. Presumably, it might be possible for such a system to automatically determine some semantics of new words.
Reference: [35] <author> P. Kenny, R. Hollan, V. Gupta, M. Lennig, P. Mermelstein, and D. O'Shaughnessy, </author> <title> "A*-admissible heuristics for rapid lexical access," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 689-692, </pages> <address> Toronto, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Based on this success, other more efficient N -best search strategies were developed, including other modifications of the Viterbi search at BBN [62] and MIT [73] as well as algorithms based on the A* search, such as the work of Kenny et al. <ref> [35] </ref>, Soong and Huang [67], and Zue et al. [73]. N -best algorithms have found widespread use in systems that combine speech recognition and natural language understanding, such as the systems at BBN [62] and MIT [73].
Reference: [36] <author> P. Kenny, P. Labute, Z. Li, and D. O'Shaughnessy, </author> <title> "New graph search techniques for speech recognition," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 553-556, </pages> <address> Adelaide, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Rather, they use their lattices as a word-transition "grammar" for subsequent search stages utilizing more detailed models. The word edges in the SRI word lattices contain only word labels and begin/end times. Kenny et al. at INRS-Telecommunications <ref> [36] </ref> present a multi-pass approach to the speech recognition search problem that produces word graphs after three passes. In the first pass, backward in time, a phonetic graph is produced by using one- or two-phone look-ahead.
Reference: [37] <author> V. Khazatsky, </author> <title> Speech Recognition: Personalization of Vocabulary (Information Theoretical Approach), </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> May </month> <year> 1985. </year>
Reference-contexts: This is a powerful technique for incorporating a new word into a statistical language model. This work represented an extension of the thesis work of Khazatsky at MIT <ref> [37] </ref>. Asadi et al. [3-5] studied the problem of building lexical (pronunciation) models for new words. They experimented with automatic phonetic transcription and a text-to-speech system (DECtalk) that generated pronunciations given new-word spellings. that they chose to make all of their new words be names for their detection study.
Reference: [38] <author> K. Kita, T. Ehara, and T. Morimoto, </author> <title> "Processing unknown words in continuous speech recognition," </title> <journal> IEICE Trans., </journal> <volume> E74(7):1811-1816, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: It was a contemporary task, and there was a relatively large quantity of data available for experimentation. However, the artificial nature of the utterance scripts casts doubt on the realism of the new words studied. Nonetheless, this was pioneering research. Kita et al. <ref> [38] </ref> experimented with new-word detection and transcription in continuous Japanese speech using HMMs and generalized LR parsing. Basically, they use two models running in parallel, one with a grammar describing their recognition task and the other with a stochastic grammar describing syllables in Japanese. They used context-independent phone models throughout.
Reference: [39] <author> L. F. Lamel, J.-L. Gauvain, and M. Eskenazi, "BREF, </author> <title> a large vocabulary spoken corpus for French," </title> <booktitle> in Proc. European Conf. Speech Communication and Technology, </booktitle> <pages> pp. 505-508, </pages> <address> Genoa, </address> <month> September </month> <year> 1991. </year>
Reference: [40] <author> H. C. Leung, I. L. Hetherington, and V. W. Zue, </author> <title> "Speech recognition using stochastic explicit-segment modeling," </title> <booktitle> in Proc. European Conf. Speech Communication and Technology, </booktitle> <pages> pp. 931-934, </pages> <address> Genoa, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: SUMMIT SYSTEM mental measurements are statistically independent, but because entire phonetic units are modeled, this assumption seems less severe. The SUMMIT system is not the only segmental system. Other segmental systems include the Stochastic Explicit-Segment Model of Leung et al. <ref> [40, 41] </ref>, the Stochastic Segment Model of Ostendorf et al. [50, 52], and the Dynamical System Segment Model of Digalakis et al. [18-21]. All of these, like SUMMIT, model entire phonetic units. They differ in the way segments are proposed and modeled. TINA, a natural language (NL) processing system [65].
Reference: [41] <author> H. C. Leung, I. L. Hetherington, and V. W. Zue, </author> <title> "Speech recognition using stochastic segment neural networks," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 613-616, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: SUMMIT SYSTEM mental measurements are statistically independent, but because entire phonetic units are modeled, this assumption seems less severe. The SUMMIT system is not the only segmental system. Other segmental systems include the Stochastic Explicit-Segment Model of Leung et al. <ref> [40, 41] </ref>, the Stochastic Segment Model of Ostendorf et al. [50, 52], and the Dynamical System Segment Model of Digalakis et al. [18-21]. All of these, like SUMMIT, model entire phonetic units. They differ in the way segments are proposed and modeled. TINA, a natural language (NL) processing system [65].
Reference: [42] <author> H. M. Meng, S. Seneff, and V. W. Zue, </author> <title> "Phonological parsing for bi-directional letter-to-sound/sound-to-letter generation," </title> <booktitle> in Proc. ARPA Human Lang. Tech. Workshop, </booktitle> <address> Princeton, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Conversely, if the user 28 CHAPTER 1. INTRODUCTION types in the spelling of a new word when adding it to a system, letter-to-sound rules could be used to generate a pronunciation model of it as Asadi et al. [3-5] did. Meng et al. <ref> [30, 42, 43] </ref> developed a reversible letter-to-sound/sound-to-letter generation system using an approach that combined a multi-level rule-based formalism with data-driven techniques. Such a reversible system could be particularly useful in the context of learning new words, because both directions could be put to use.
Reference: [43] <author> H. M. Meng, S. Seneff, and V. W. Zue, </author> <title> "Phonological parsing for reversible letter-to-sound/sound-to-letter generation," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 1-4, </pages> <address> Adelaide, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Conversely, if the user 28 CHAPTER 1. INTRODUCTION types in the spelling of a new word when adding it to a system, letter-to-sound rules could be used to generate a pronunciation model of it as Asadi et al. [3-5] did. Meng et al. <ref> [30, 42, 43] </ref> developed a reversible letter-to-sound/sound-to-letter generation system using an approach that combined a multi-level rule-based formalism with data-driven techniques. Such a reversible system could be particularly useful in the context of learning new words, because both directions could be put to use.
Reference: [44] <author> H. Murveit, J. Butzberger, V. Digalakis, and M. Weintraub, </author> <title> "Large-vocabulary dictation using SRI's DECIPHER speech recognition system: Progressive search techniques," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 319-322, </pages> <address> Minneapolis, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The fact that it can begin generating word edges in the first pass may mean that the first and second passes could run in parallel, whereas our second pass cannot begin until the first Viterbi pass is complete. Murveit et al. at SRI <ref> [44] </ref> use a multi-stage search technique that produces word lattices as an intermediate representation. Their "forward-backward word-life" algorithm generates word lattices in forward and backward passes.
Reference: [45] <author> H. Murveit and R. Moore, </author> <title> "Integrating natural language constraints into HMM-based speech recognition," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 573-576, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: N -best algorithms have found widespread use in systems that combine speech recognition and natural language understanding, such as the systems at BBN [62] and MIT [73]. Although there have been efforts toward integrating the natural language constraints into the search itself, such as at MIT [26] and SRI <ref> [45] </ref>, N -best strategies have remained popular not only because of their ease of implementation, but also because they greatly improve the efficiency of the development effort, since one can precompute N -best lists for a large corpus to use as input for natural language experiments.
Reference: [46] <author> L. Nguyen, R. Schwartz, F. Kubala, G. Chou, C. Lapre, Y. Zhao, J. Makhoul, G. Zavaliagkos, and A. Anastasakos, </author> <title> "Spoke 9: Spontaneous WSJ dictation," </title> <booktitle> in Proc. ARPA Spoken Lang. Sys. Tech. Workshop, </booktitle> <address> Princeton, </address> <month> March </month> <year> 1994. </year> <note> BIBLIOGRAPHY 171 </note>
Reference-contexts: Unfortunately, the literature is lacking in this subject. It seems that many researchers in the field attempt to solve the problem without first demonstrating the magnitude of 4 In fact, in a subset of WSJ containing spontaneously produced dictation, 1.4-1.9% of the words were out-of-vocabulary for a 40,000-word vocabulary <ref> [46] </ref>. 22 CHAPTER 1. INTRODUCTION the problem, characterizing new words and their usage, and quantifying their effects on recognition (without detection). However, the work of Suhm et al. [69] is an exception. 5 They chose to characterize the problem before attempting to solve the detection problem.
Reference: [47] <author> N. J. Nilsson, </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1980. </year>
Reference-contexts: SUMMIT's A* search is performed backwards in time from the end of the utterance and uses information computed during the forward Viterbi search [73]. Briefly, the A* search is a best-first search that uses a heuristic evaluation function <ref> [47] </ref>. This evaluation function takes into account the actual score for a (partial) sequence and a heuristic estimate for the best completion of the sequence. If this heuris 3.8. RECOGNIZER OUTPUT 69 tic estimate is an upper bound 3 then the search is admissible. <p> We first describe the general A* algorithm, then the N -best algorithm, and finally the word graph algorithm. 4.2.1 A* Search An A* search <ref> [8, 47] </ref> is a best-first search with a particular evaluation function f fl (p) for a (partial) path, or hypothesis, p in the search space: f fl (p) = g (p) + h fl (p): Here, f fl (p) is the estimated score of the best complete path containing p, g
Reference: [48] <author> M. Oerder and H. Ney, </author> <title> "Word graphs: An efficient interface between continuous-speech recognition and language understanding," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 2, </volume> <pages> pp. 119-122, </pages> <address> Minneapolis, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The development of our A* word graph algorithm, first reported in [27], was conducted independently of the other word graph research presented in this section. Oerder and Ney <ref> [48] </ref> and Aubert et al. [7] at Philips present word graphs, similar to those we present in this chapter, which interface speech recognition and natural language components. Their algorithm also makes use of two passes in opposite directions. <p> Furthermore, the score thresholds used in their system are undoubtedly different from ours, and they did not relate their word graph size to N -best list size. Overall, the word graph algorithm developed at Philips <ref> [7, 48] </ref> sounds promising. The fact that it can begin generating word edges in the first pass may mean that the first and second passes could run in parallel, whereas our second pass cannot begin until the first Viterbi pass is complete. <p> Murveit et al. at SRI [44] use a multi-stage search technique that produces word lattices as an intermediate representation. Their "forward-backward word-life" algorithm generates word lattices in forward and backward passes. The algorithm is similar to that used in the Philips system <ref> [7, 48] </ref> in that word edges are accumulated during the first Viterbi-style pass and then pruned in the second pass in the opposite direction.
Reference: [49] <author> D. O'Shaughnessy, </author> <title> Speech Communication: Human and Machine. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: To derive the MFCCs, a 256-point discrete Fourier transform (DFT) is first computed for every frame from the pre-emphasized waveform using a 25.6ms Hamming window. These spectral coefficients are passed through a set of 40 triangular filters along the mel-frequency scale, resulting in mel-frequency spectral coefficients (MFSCs) <ref> [49, 60] </ref>. 2 Finally, the MFSCs are transformed from the spectral domain to the cepstral domain by taking logarithms and applying the inverse discrete Fourier transform (IDFT). MFCCs are a popular signal representation for several reasons.
Reference: [50] <author> M. Ostendorf, I. Bechwati, and O. Kimball, </author> <title> "Context modeling with the stochastic segment model," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 389-392, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: The SUMMIT system is not the only segmental system. Other segmental systems include the Stochastic Explicit-Segment Model of Leung et al. [40, 41], the Stochastic Segment Model of Ostendorf et al. <ref> [50, 52] </ref>, and the Dynamical System Segment Model of Digalakis et al. [18-21]. All of these, like SUMMIT, model entire phonetic units. They differ in the way segments are proposed and modeled. TINA, a natural language (NL) processing system [65].
Reference: [51] <author> M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. Schwartz, and J. R. Rohlicek, </author> <title> "Integration of diverse recognition methodologies through reevaluation of N-best sentence hypotheses," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 83-87, </pages> <address> Pacific Grove, CA, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: Such a list represents a drastically reduced search space in which to evaluate these more expensive models. The general practice of re-scoring and re-sorting hypotheses is called N -best re-sorting and is used by many systems <ref> [13, 51, 62-64] </ref>. In summary, N -best lists are useful for two purposes: to provide alternative recognizer hypotheses for natural language processing, and to provide a multi-stage mechanism for applying more computationally expensive modeling (e.g., context-dependent acoustic modeling) and constraints. <p> One can use N -best re-sorting experiments as a mechanism for applying computationally expensive constraints in order to improve recognition systems. For example, one can test a new acoustic model by using it to re-sort N -best lists rather than integrating this new model into the search directly <ref> [51, 56, 63] </ref>. Re-sorting N -best lists can require many orders of magnitude less computation than performing the complete search and may even allow the use of constraints that would not be possible in the complete search (e.g., acoustic models that depend on long-distance contextual factors).
Reference: [52] <author> M. Ostendorf and S. Roukos, </author> <title> "A stochastic segment model for phoneme-based continuous speech recognition," </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 37(12) </volume> <pages> 1857-1869, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The SUMMIT system is not the only segmental system. Other segmental systems include the Stochastic Explicit-Segment Model of Leung et al. [40, 41], the Stochastic Segment Model of Ostendorf et al. <ref> [50, 52] </ref>, and the Dynamical System Segment Model of Digalakis et al. [18-21]. All of these, like SUMMIT, model entire phonetic units. They differ in the way segments are proposed and modeled. TINA, a natural language (NL) processing system [65].
Reference: [53] <author> D. S. Pallett, J. G. Fiscus, W. M. Fisher, and J. S. Garofolo, </author> <title> "Benchmark tests for the DARPA spoken language program," </title> <booktitle> in Proc. ARPA Human Lang. Tech. Workshop, </booktitle> <address> Plainsboro, NJ, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: The atis and f-atis corpora consist of spontaneous speech utterances collected interactively for ARPA's Air Travel Information Service (ATIS) common task and are in English and French, respectively. The atis corpus contains utterances from both the so-called ATIS-2 and ATIS-3 sets <ref> [17, 29, 53] </ref>. ATIS-3 represents an increase in the 2.2. <p> Therefore, the word graphs contain more information that might prove useful for subsequent processing. Even so, we have found significant efficiency improvements with the word graph search. 4.3.1 Experimental Conditions The corpus used for this evaluation was a subset of the DARPA November 1992 ATIS evaluation test set <ref> [53] </ref>. To reduce the amount of computation needed, only the utterances from the first session for each speaker were used. We also discarded a few of the longest utterances, because we were not able to compute the N -best search to the search depth used in the experiments.
Reference: [54] <author> D. S. Pallett, W. M. Fisher, and J. G. Fiscus, </author> <title> "Tools for the analysis of benchmark speech recognition tests," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 97-100, </pages> <address> Albuquerque, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Finally, we could build the baseline system with a larger vocabulary that covered the ATIS-3 vocabulary. In examining the effects of new words on recognizer performance, we chose to use word-error rate as our performance measure <ref> [54] </ref>. To compute word-error rate, we aligned, word-for-word, recognizer output with reference orthographies. Once aligned, the number of word substitutions, deletions, and insertions can be measured.
Reference: [55] <author> D. B. Paul and J. M. Baker, </author> <title> "The design for the Wall Street Journal-based CSR corpus," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 357-362, </pages> <address> Harri-man, NY, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: S., took part in Advanced Research Project Agency's (ARPA) evaluation of speech recognition and understanding systems. The Air Travel Information Service (ATIS) task [17, 59] was used for recognition and understanding and consisted of spontaneous utterances regarding airline flight information. The Wall Street Journal (WSJ) task <ref> [55] </ref> was used for recognition only and consisted of read speech drawn from newspaper articles from the Wall Street Journal. The vocabulary size used for ATIS was on the order of 2,500 words, and the size used for WSJ ranged from 5,000 to 40,000 words. <p> The dialogs are based on a large set of predefined topics. The topics were selected to be of general interest and to encourage active discussion. We include both sides of dialogs in our study. We removed spontaneous speech disfluencies from the orthographic transcriptions. The wsj <ref> [55] </ref> and nyt corpora consist of English text from the Wall Street Journal and the New York Times newspapers, respectively. The text for wsj was made available by the ACL Data Collection Initiative [6] and represents three years (1987-1989) of newspaper text.
Reference: [56] <author> M. Phillips, J. Glass, and V. Zue, </author> <title> "Modelling context dependency in acoustic-phonetic and lexical representations," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 71-76, </pages> <address> Pacific Grove, CA, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: In this chapter we briefly describe SUMMIT, the continuous-speech recognition system developed by the Spoken Language Systems Group of the MIT Laboratory of Computer Science. The SUMMIT speech recognition system <ref> [56, 66, 72, 74-77] </ref> is different from most other systems in that it is segment-based instead of frame-based. Most systems today utilize hidden Markov models (HMMs) to model acoustic features measured over a sequence of fixed-rate frames. 1 These frames are usually very short in duration, typically 10 ms. <p> One can use N -best re-sorting experiments as a mechanism for applying computationally expensive constraints in order to improve recognition systems. For example, one can test a new acoustic model by using it to re-sort N -best lists rather than integrating this new model into the search directly <ref> [51, 56, 63] </ref>. Re-sorting N -best lists can require many orders of magnitude less computation than performing the complete search and may even allow the use of constraints that would not be possible in the complete search (e.g., acoustic models that depend on long-distance contextual factors).
Reference: [57] <author> M. Phillips and V. Zue, </author> <title> "Automatic discover of acoustic measurements for phonetic classification," </title> <booktitle> in Proc. Int. Conf. Spoken Language Processing, </booktitle> <pages> pp. 795-798, </pages> <address> Banff, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The real power of a segmental system is that it can make acoustic measurements that are relevant to entire phonetic units. In the case of SUMMIT, these measurements include duration, MFCC averages, and a set of automatically learned acoustic measurements <ref> [57] </ref>. The set of learned measurements includes time averages of parameters over different parts of a segment, average spectral peak frequencies, and average change of spectral peaks. These last two types are related to formant frequencies and their slopes.
Reference: [58] <author> P. Price, W. M. Fisher, J. Bernstein, and D. S. Pallett, </author> <title> "The DARPA 1000-word Resource Managment database for continuous speech recognition," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 651-654, </pages> <address> New York, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: The first ARPA common task for speech recognition was the Resource Management (RM) task <ref> [58] </ref>. The speech data consisted of read speech from a naval resource management domain, in which the scripts used during data collection were generated by an artificial language model. This language model, a finite-state grammar, generated sentences with a closed, or limited, vocabulary. <p> They also examined the learning problem (see Section 1.2.3). Their research was carried out on the Resource Management (RM) task <ref> [58] </ref>, using the BBN BYBLOS continuous speech recognition system [4, 14, 15]. The BYBLOS system used HMMs and a statistical class bigram language model. It is important to note that because the utterances in the RM task were generated artificially from a finite-state grammar, there were no true new words.
Reference: [59] <author> P. J. Price, </author> <title> "Evaluation of spoken language systems: the ATIS domain," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 91-95, </pages> <address> Hidden Valley, CA, </address> <month> June </month> <year> 1990. </year> <note> 172 BIBLIOGRAPHY </note>
Reference-contexts: For example, in December of 1993, fourteen sites, four of them outside the U. S., took part in Advanced Research Project Agency's (ARPA) evaluation of speech recognition and understanding systems. The Air Travel Information Service (ATIS) task <ref> [17, 59] </ref> was used for recognition and understanding and consisted of spontaneous utterances regarding airline flight information. The Wall Street Journal (WSJ) task [55] was used for recognition only and consisted of read speech drawn from newspaper articles from the Wall Street Journal.
Reference: [60] <author> L. Rabiner and B.-H. Juang, </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1993. </year>
Reference-contexts: To derive the MFCCs, a 256-point discrete Fourier transform (DFT) is first computed for every frame from the pre-emphasized waveform using a 25.6ms Hamming window. These spectral coefficients are passed through a set of 40 triangular filters along the mel-frequency scale, resulting in mel-frequency spectral coefficients (MFSCs) <ref> [49, 60] </ref>. 2 Finally, the MFSCs are transformed from the spectral domain to the cepstral domain by taking logarithms and applying the inverse discrete Fourier transform (IDFT). MFCCs are a popular signal representation for several reasons.
Reference: [61] <author> L. R. Rabiner, </author> <title> "A tutorial on hidden Markov models and selected applications in speech recognition," </title> <journal> Proc. of the IEEE, </journal> <volume> 77(2) </volume> <pages> 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: A signal processing component computes a set of acoustic measurements for each segment of speech. In the case of frame-based systems (e.g., hidden Markov models or HMMs <ref> [61] </ref>), the segments are simply fixed-rate frames. In the case of a segmental system, these segments are typically of variable duration and may overlap one another. The acoustic models generally model sub-word units such as phones and may be context-independent or context-dependent.
Reference: [62] <author> R. Schwartz and S. Austin, </author> <title> "A comparison of several approximate algorithms for finding multiple (N-best) sentence hypotheses," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 701-704, </pages> <address> Toronto, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: In SUMMIT, this beam is so wide that this pruning introduces negligible search errors. 3.7.2 A* Backward Search SUMMIT does not compute the N -best word sequences in the first-stage Viterbi search, even though it is possible <ref> [13, 62, 64, 73] </ref>, because an A* search is more efficient in terms of both time and memory. SUMMIT's A* search is performed backwards in time from the end of the utterance and uses information computed during the forward Viterbi search [73]. <p> Based on this success, other more efficient N -best search strategies were developed, including other modifications of the Viterbi search at BBN <ref> [62] </ref> and MIT [73] as well as algorithms based on the A* search, such as the work of Kenny et al. [35], Soong and Huang [67], and Zue et al. [73]. <p> N -best algorithms have found widespread use in systems that combine speech recognition and natural language understanding, such as the systems at BBN <ref> [62] </ref> and MIT [73].
Reference: [63] <author> R. Schwartz, S. Austin, F. Kubala, J. Makhoul, L. Nguyen, P. Placeway, and G. Zavaliagkos, </author> <title> "New uses for the N-best sentence hypotheses within the BYBLOS speech recognition system," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 1-4, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: One can use N -best re-sorting experiments as a mechanism for applying computationally expensive constraints in order to improve recognition systems. For example, one can test a new acoustic model by using it to re-sort N -best lists rather than integrating this new model into the search directly <ref> [51, 56, 63] </ref>. Re-sorting N -best lists can require many orders of magnitude less computation than performing the complete search and may even allow the use of constraints that would not be possible in the complete search (e.g., acoustic models that depend on long-distance contextual factors).
Reference: [64] <author> R. Schwartz and Y.-L. Chow, </author> <title> "The N-best algorithm," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 81-84, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: In SUMMIT, this beam is so wide that this pruning introduces negligible search errors. 3.7.2 A* Backward Search SUMMIT does not compute the N -best word sequences in the first-stage Viterbi search, even though it is possible <ref> [13, 62, 64, 73] </ref>, because an A* search is more efficient in terms of both time and memory. SUMMIT's A* search is performed backwards in time from the end of the utterance and uses information computed during the forward Viterbi search [73].
Reference: [65] <author> S. Seneff, "TINA: </author> <title> A natural language system for spoken language applications," </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 61-86, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: All of these, like SUMMIT, model entire phonetic units. They differ in the way segments are proposed and modeled. TINA, a natural language (NL) processing system <ref> [65] </ref>. We will briefly describe each of the components in the next few sections. In this thesis, we did not use any of the components below the dashed line. Briefly, the speech waveform is digitized and fed into the signal processing component, where frame-based measurements are computed. <p> The N -best hypotheses resulting from the n-gram language modeling component can be further re-sorted based on more accurate context-dependent acoustic modeling. Finally, any of the N -best lists can be input into the TINA system for natural language 61 62 CHAPTER 3. SUMMIT SYSTEM processing <ref> [65] </ref>. TINA can be used to understand utterances, or it can be used to filter N -best lists with its powerful language modeling capabilities. 3.1 Signal Processing The signal processing used by the SUMMIT system involves transforming a 16 kHz, 16-bit sampled waveform to 14 mel-frequency cepstral coefficients (MFCCs).
Reference: [66] <author> S. Seneff, J. Glass, D. Goddeau, D. Goodine, L. Hirschman, H. Leung, M. Phillips, J. Polifroni, and V. Zue, </author> <title> "Development and preliminary evaluation of the MIT ATIS system," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 88-93, </pages> <address> Pacific Grove, CA, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: In this chapter we briefly describe SUMMIT, the continuous-speech recognition system developed by the Spoken Language Systems Group of the MIT Laboratory of Computer Science. The SUMMIT speech recognition system <ref> [56, 66, 72, 74-77] </ref> is different from most other systems in that it is segment-based instead of frame-based. Most systems today utilize hidden Markov models (HMMs) to model acoustic features measured over a sequence of fixed-rate frames. 1 These frames are usually very short in duration, typically 10 ms.
Reference: [67] <author> F. K. Soong and E.-F. Huang, </author> <title> "A tree-trellis based fast search for finding the N best sentence hypotheses in continuous speech recognition," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 705-708, </pages> <address> Toronto, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: In general, the A* algorithm is sensitive to the tightness of the upper bound estimate. Since SUMMIT's heuristic is exact, the A* search is as efficient as possible. SUMMIT's use of the forward Viterbi scores is similar to the tree-trellis search of Soong and Huang <ref> [67] </ref>. In Chapter 4 we describe the A* search in more detail while introducing the A* word graph search (that is now a part of the SUMMIT system). This latter search produces a graph of words that represents very long N -best lists compactly. <p> Based on this success, other more efficient N -best search strategies were developed, including other modifications of the Viterbi search at BBN [62] and MIT [73] as well as algorithms based on the A* search, such as the work of Kenny et al. [35], Soong and Huang <ref> [67] </ref>, and Zue et al. [73]. N -best algorithms have found widespread use in systems that combine speech recognition and natural language understanding, such as the systems at BBN [62] and MIT [73].
Reference: [68] <author> J. Spitz, </author> <title> "Collection and analysis of data from real users: Implications for speech recognition/understanding systems," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 164-169, </pages> <address> Pacific Grove, CA, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: The i-voyager corpus is similar, except that the utterances are in Italian [22]. Utterances were again collected using a human "wizard." Again, we removed spontaneous speech disfluencies from the orthographic transcriptions. The citron corpus consists of utterances collected by NYNEX from actual directory assistance telephone calls <ref> [12, 68] </ref>. The users interacted with human operators. The switchboard corpus consists of spontaneous human/human dialogs collected by Texas Instruments [25]. The dialogs are based on a large set of predefined topics. The topics were selected to be of general interest and to encourage active discussion.
Reference: [69] <author> B. Suhm, M. Woszczyna, and A. Waibel, </author> <title> "Detection and transcription of new words," </title> <booktitle> in Proc. European Conf. Speech Communication and Technology, </booktitle> <pages> pp. 2179-2182, </pages> <address> Berlin, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: INTRODUCTION the problem, characterizing new words and their usage, and quantifying their effects on recognition (without detection). However, the work of Suhm et al. <ref> [69] </ref> is an exception. 5 They chose to characterize the problem before attempting to solve the detection problem. <p> However, when compared to the vocabulary in an unweighted fashion, the length distribution was very similar. Suhm et al. also studied the introduction of a new-word class into a statistical word 5 The study of Suhm et al. <ref> [69] </ref> was reported concurrently with our initial study [28] at Eurospeech '93. However, their study is less general in that it involved only one language (English) and one domain (Wall Street Journal). 1.2. PRIOR RESEARCH 23 trigram language model. <p> This resulted in 110 out of 220 test utterances containing new words. However, because neither the task not the selection of simulated new words is described adequately, it is difficult to interpret their reported level of detection performance. Suhm et al. <ref> [69] </ref>, in addition to providing one of the very few characterizations of the new-word problem, experimented with detection in English. In the context of a conference registration task, they examined detection and phonetic transcription of new words. This was not the same task they used in their initial study. <p> Since then, the amount of research on the detection and learning problems has increased. While this is encouraging, we feel that the new-word problem is still not getting the attention it deserves. The work by Suhm et al. <ref> [69] </ref> includes a characterization of the new-word problem that is lacking in some of the other prior research. This work included a study of the frequency and length characteristics of new words for a subset of the WSJ corpus. <p> Thus, we do not really have to worry about new function words. As we mentioned in Section 1.2.1, Suhm et al. <ref> [69] </ref> similarly examined new words in the wsj corpus. They found that 27% were names, which is comparable to our 33%. <p> In general, new words are slightly longer than in-vocabulary words, even when the in-vocabulary words are not weighted by word-frequency. On average, they are about 0.3 syllables (13%) and 0.6 phonemes (10%) longer. Suhm et al. <ref> [69] </ref> found almost no difference between the distributions of number of phonemes in a similar study on the wsj corpus. This discrepancy may be due to the fact that we examined a much greater quantity of data and used much larger vocabularies than Suhm et al. did in their study.
Reference: [70] <author> A. </author> <title> Viterbi, "Error bounds for convolutional codes and an asymptotic optimal decoding algorithm," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-13:260-269, </volume> <month> April </month> <year> 1967. </year>
Reference-contexts: In the lexical access search (Section 3.7) a class bigram is used to constrain word sequences. The reason for a bigram language model is for computational efficiency. Because the first stage of the search is a dynamic programming search similar to Viterbi decoding <ref> [70] </ref>, using more complex models is difficult because of longer language context. In a later recognition stage, a class n-gram model is used to re-sort N -best lists of complete-utterance hypotheses. In this re-sorting, SUMMIT typically uses a class 4-gram language model. <p> If the N - best word sequences are desired, a second pass, backward in time, is used. This second pass is an A* search, or possibly an A* word graph search as presented in Chapter 4. 3.7.1 Viterbi Forward Search SUMMIT uses a modified Viterbi search <ref> [70] </ref>, forward in time, to compute the single-best word sequence that covers an entire utterance. This dynamic programming search computes the best (partial) word sequence and its score from the beginning of the utterance to every lexical node-boundary pair. <p> As a result, researchers have employed efficient algorithms, such as the Viterbi dynamic-programming search algorithm <ref> [70] </ref>, to find the top-scoring word string.
Reference: [71] <author> G. Zavaliagkos, T. Anastasakos, G. Chou, F. Kubala, C. Lapre, J. Makhoul, L. Nguyen, R. Scwhartz, and Y. Zhao, </author> <title> "BBN hub system and results," </title> <booktitle> in Proc. ARPA Spoken Lang. Sys. Tech. Workshop, </booktitle> <address> Princeton, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Since the task vocabulary was larger than the system vocabulary in this condition, the systems did face some new words. However, the vocabulary filtering artificially reduced their frequency. For example, with the 20,000-word vocabulary 2.27% of the words in the development set were out-of-vocabulary <ref> [71] </ref>. If the vocabulary was increased to contain the 40,000 most-frequent words, the percentage of new words fell to only 0.17%.
Reference: [72] <author> V. Zue, J. Glass, D. Goddeau, D. Goodine, L. Hirschman, M. Phillips, J. Polifroni, and S. Seneff, </author> <title> "The MIT ATIS system: February 1992 progress report," </title> <booktitle> in Proc. DARPA Speech and Nat. Lang. Workshop, </booktitle> <pages> pp. 84-88, </pages> <address> Harriman, NY, </address> <month> February </month> <year> 1992. </year> <note> BIBLIOGRAPHY 173 </note>
Reference-contexts: In this chapter we briefly describe SUMMIT, the continuous-speech recognition system developed by the Spoken Language Systems Group of the MIT Laboratory of Computer Science. The SUMMIT speech recognition system <ref> [56, 66, 72, 74-77] </ref> is different from most other systems in that it is segment-based instead of frame-based. Most systems today utilize hidden Markov models (HMMs) to model acoustic features measured over a sequence of fixed-rate frames. 1 These frames are usually very short in duration, typically 10 ms.
Reference: [73] <author> V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, J. Polifroni, and S. Seneff, </author> <title> "Integration of speech recognition and natural language processing in the MIT VOYAGER system," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 713-716, </pages> <address> Toronto, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: In SUMMIT, this beam is so wide that this pruning introduces negligible search errors. 3.7.2 A* Backward Search SUMMIT does not compute the N -best word sequences in the first-stage Viterbi search, even though it is possible <ref> [13, 62, 64, 73] </ref>, because an A* search is more efficient in terms of both time and memory. SUMMIT's A* search is performed backwards in time from the end of the utterance and uses information computed during the forward Viterbi search [73]. <p> SUMMIT's A* search is performed backwards in time from the end of the utterance and uses information computed during the forward Viterbi search <ref> [73] </ref>. Briefly, the A* search is a best-first search that uses a heuristic evaluation function [47]. This evaluation function takes into account the actual score for a (partial) sequence and a heuristic estimate for the best completion of the sequence. If this heuris 3.8. <p> Based on this success, other more efficient N -best search strategies were developed, including other modifications of the Viterbi search at BBN [62] and MIT <ref> [73] </ref> as well as algorithms based on the A* search, such as the work of Kenny et al. [35], Soong and Huang [67], and Zue et al. [73]. <p> this success, other more efficient N -best search strategies were developed, including other modifications of the Viterbi search at BBN [62] and MIT <ref> [73] </ref> as well as algorithms based on the A* search, such as the work of Kenny et al. [35], Soong and Huang [67], and Zue et al. [73]. N -best algorithms have found widespread use in systems that combine speech recognition and natural language understanding, such as the systems at BBN [62] and MIT [73]. <p> algorithms based on the A* search, such as the work of Kenny et al. [35], Soong and Huang [67], and Zue et al. <ref> [73] </ref>. N -best algorithms have found widespread use in systems that combine speech recognition and natural language understanding, such as the systems at BBN [62] and MIT [73].
Reference: [74] <author> V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, and S. Seneff, </author> <title> "The VOYAGER speech understanding system: Preliminary development and evaluation," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 73-76, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference: [75] <author> V. Zue, J. Glass, D. Goodine, M. Phillips, and S. Seneff, </author> <title> "The SUMMIT speech recognition system: Phonological modelling and lexical acess," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <volume> vol. 1, </volume> <pages> pp. 49-52, </pages> <address> Albuquerque, NM, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: In SUMMIT, these lexical models are more sophisticated than strings of sub-word phonetic units: they are phonetic networks, or graphs <ref> [75] </ref>. SUMMIT uses phonetic networks in order 3.5. LEXICAL MODELS 65 d * d d y u w D D for the two words "did you." Solid arcs represent base-form pronunciations, dashed arcs are the result of applying the phonological rules, and dotted arcs indicate inter-word connections. <p> As the figure indicates, not all begin/end nodes can connect to all others. The inter-word phonological rules dictate which connections are sensible. The arcs in the lexical models, the lexical arcs, have scores or weights associated with them. These weights are trained using a corrective training algorithm <ref> [75] </ref> and are designed to favor pronunciations that help recognition performance. The weights are needed because the phonological rules tend to over-generate arcs, and this over-generation can increase word confusions. 66 CHAPTER 3. <p> A RECOGNIZER-BASED STUDY resulted in significant improvement in performance <ref> [75] </ref>. However, there is a possible explanation for why we found very little improvement with the arc weights for new words. Words that were frequent (e.g., "to," "from," "flights," and "the," the most frequent words in ATIS) were more likely to benefit from the corrective training of the arc weights.
Reference: [76] <author> V. Zue, J. Glass, M. Phillips, and S. Seneff, </author> <title> "Acoustic segmentation and phonetic classification in the SUMMIT system," </title> <booktitle> in Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pp. 389-392, </pages> <address> Glasgow, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: We used orthographic transcriptions of the utterances with spontaneous speech disfluencies removed. The voyager corpus consists of spontaneous speech utterances in English collected interactively for the MIT voyager urban navigation and exploration system <ref> [76] </ref>. The i-voyager corpus is similar, except that the utterances are in Italian [22]. Utterances were again collected using a human "wizard." Again, we removed spontaneous speech disfluencies from the orthographic transcriptions. The citron corpus consists of utterances collected by NYNEX from actual directory assistance telephone calls [12, 68].
Reference: [77] <author> V. Zue, S. Seneff, J. Polifroni, M. Phillips, C. Pao, D. Goddeau, J. Glass, and E. Brill, </author> <title> "The MIT ATIS system: December 1993 progress report," </title> <booktitle> in Proc. ARPA Spoken Lang. Sys. Tech. Workshop, </booktitle> <address> Princeton, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: The baseline system's large vocabulary was the vocabulary used by the SUMMIT system for the ARPA December 1993 ATIS-3 evaluation <ref> [77] </ref>. This vocabulary contained 2,461 words and was based on a vocabulary supplied by Carnegie Mellon University. The reduced vocabulary was to be built from ATIS-2 utterances, and the words for it were extracted from the baseline ATIS-3 vocabulary using the following procedure.
Reference: [78] <author> V. W. Zue, </author> <title> "Toward systems that understand spoken language," </title> <journal> IEEE Expert, </journal> <pages> pp. 51-59, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: With recent research effort in developing speech understanding 71 72 CHAPTER 4. WORD GRAPHS systems <ref> [78] </ref> it has become desirable either to integrate more complex language models into the search, or to have the speech recognition component provide multiple sentence hypotheses, which can then be filtered by the natural language component.
References-found: 78

