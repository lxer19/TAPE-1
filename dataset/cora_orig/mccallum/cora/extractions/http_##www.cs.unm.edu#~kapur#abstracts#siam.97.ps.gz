URL: http://www.cs.unm.edu/~kapur/abstracts/siam.97.ps.gz
Refering-URL: http://www.cs.unm.edu/~kapur/abstracts/siam.97.html
Root-URL: http://www.cs.unm.edu
Title: Solving Polynomial Systems Using a Branch and Prune Approach  
Author: P. Van Hentenryck D. McAllester D. Kapur 
Keyword: System of Equations, Global Methods, Interval and Finite Analysis  
Note: AMS subject Classification:  
Address: Box 1910 Providence, RI 02912  Technology Square, 545 Cambridge, USA  Albany, NY-12222  
Affiliation: Brown University  MIT AI Lab  SUNY at Albany Dep. of Computer Science  
Email: pvh@cs.brown.edu  dam@ai.mit.edu  kapur@cs.albany.edu  
Web: 65H10, 65G10  
Abstract: This paper presents Newton, a branch & prune algorithm to find all isolated solutions of a system of polynomial constraints. Newton can be characterized as a global search method which uses intervals for numerical correctness and for pruning the search space early. The pruning in Newton consists in enforcing at each node of the search tree a unique local consistency condition, called box-consistency, which approximates the notion of arc-consistency well-known in artificial intelligence. Box-consistency is parametrized by an interval extension of the constraint and can be instantiated to produce the Hansen-Segupta's narrowing operator (used in interval methods) as well as new operators which are more effective when the computation is far from a solution. Newton has been evaluated on a variety of benchmarks from kinematics, chemistry, combustion, economics, and mechanics. On these benchmarks, it outperforms the interval methods we are aware of and compares well with state-of-the-art continuation methods. Limitations of Newton (e.g., a sensitivity to the size of the initial intervals on some problems) are also discussed. Of particular interest is the mathematical and programming simplicity of the method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Alefeld and J. Herzberger. </author> <title> Introduction to Interval Computations. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1983. </year>
Reference-contexts: The second constraint can be used to reduce further the interval of x 2 by searching for the leftmost and rightmost zeros of <ref> [1; 1] </ref> 2 X 2 = 0 producing the interval [0,1] for x 2 . No more reduction is obtained by Newton and branching is needed to make progress. <p> More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 19, 20] </ref>). Our definitions are slightly non-standard. 3.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> It is important to stress that a real function (resp. relation) can be extended in many ways. For instance, the interval function is the most precise interval extension of addition (i.e., it returns the smallest possible interval containing all real results) while a function always returning <ref> [1; 1] </ref> would be the least accurate. In the following, we assume fixed interval extensions for the basic real operators +; ; fi and exponentiation (for instance, the interval extension of + is defined by ) and the basic real relations =; . <p> A tight extension of division to intervals can be best expressed if its result is allowed to be a union of intervals [10, 6, 12]. Assuming that c 0 d and c &lt; d, [a; b]=[c; d] is defined as follows: <ref> [bb=cc; 1] </ref> if b 0 and d = 0 [1; db=de] if b 0 and c = 0 [1; da=ce] if a 0 and d = 0 [ba=dc; 1] if a 0 and c = 0. When [c; d] = [0; 0], [a; b]=[c; d] = [1; 1]. <p> Assuming that c 0 d and c &lt; d, [a; b]=[c; d] is defined as follows: [bb=cc; 1] if b 0 and d = 0 <ref> [1; db=de] </ref> if b 0 and c = 0 [1; da=ce] if a 0 and d = 0 [ba=dc; 1] if a 0 and c = 0. When [c; d] = [0; 0], [a; b]=[c; d] = [1; 1]. The case where 0 =2 [c; d] is easy to define. <p> Assuming that c 0 d and c &lt; d, [a; b]=[c; d] is defined as follows: [bb=cc; 1] if b 0 and d = 0 [1; db=de] if b 0 and c = 0 <ref> [1; da=ce] </ref> if a 0 and d = 0 [ba=dc; 1] if a 0 and c = 0. When [c; d] = [0; 0], [a; b]=[c; d] = [1; 1]. The case where 0 =2 [c; d] is easy to define. <p> Assuming that c 0 d and c &lt; d, [a; b]=[c; d] is defined as follows: [bb=cc; 1] if b 0 and d = 0 [1; db=de] if b 0 and c = 0 [1; da=ce] if a 0 and d = 0 <ref> [ba=dc; 1] </ref> if a 0 and c = 0. When [c; d] = [0; 0], [a; b]=[c; d] = [1; 1]. The case where 0 =2 [c; d] is easy to define. <p> When [c; d] = [0; 0], [a; b]=[c; d] = <ref> [1; 1] </ref>. The case where 0 =2 [c; d] is easy to define. Note also that other operations such as addition and subtraction can also be extended to work with unions of intervals. <p> I 0 is used for this purpose, i.e., we compute I u (I c I n =I d ): This result may be more precise than (I 1 ] : : : ] I n ) " (I 0 m ): To illustrate the gain of precision, consider I = <ref> [5; 1] </ref>, I 1 = [1; 3] and I 2 = [3; 1]. The expression I u (I 1 [ I 2 ) evaluates to the interval [5; 3], while the expression I " (I 1 ] I 2 ) returns the interval [5; 1], since I 1 ] I 2 <p> this purpose, i.e., we compute I u (I c I n =I d ): This result may be more precise than (I 1 ] : : : ] I n ) " (I 0 m ): To illustrate the gain of precision, consider I = [5; 1], I 1 = <ref> [1; 3] </ref> and I 2 = [3; 1]. <p> u (I c I n =I d ): This result may be more precise than (I 1 ] : : : ] I n ) " (I 0 m ): To illustrate the gain of precision, consider I = [5; 1], I 1 = [1; 3] and I 2 = <ref> [3; 1] </ref>. <p> the gain of precision, consider I = <ref> [5; 1] </ref>, I 1 = [1; 3] and I 2 = [3; 1]. The expression I u (I 1 [ I 2 ) evaluates to the interval [5; 3], while the expression I " (I 1 ] I 2 ) returns the interval [5; 1], since I 1 ] I 2 = [1; 1]. 3.3 Constraint Representations It is well-known that different computer representations of a real function produce different results when evaluated with floating-point numbers on a computer. <p> The expression I u (I 1 [ I 2 ) evaluates to the interval [5; 3], while the expression I " (I 1 ] I 2 ) returns the interval [5; 1], since I 1 ] I 2 = <ref> [1; 1] </ref>. 3.3 Constraint Representations It is well-known that different computer representations of a real function produce different results when evaluated with floating-point numbers on a computer. As a consequence, the way constraints are written may have an impact on the behaviour on the algorithm. <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h <ref> [1; 1] </ref> ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; [2; 2] i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Example 4 Consider the constraint x 1 + x 2 x 1 = 0. The constraint is not arc-consistent wrt h <ref> [1; 1] </ref>; [1; 1]i since there is no value r 1 for x 1 which satisfies r 1 + 1 r 1 = 0. On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1] + [1; 1 + ] [1; 1])" <p> Example 4 Consider the constraint x 1 + x 2 x 1 = 0. The constraint is not arc-consistent wrt h <ref> [1; 1] </ref>; [1; 1]i since there is no value r 1 for x 1 which satisfies r 1 + 1 r 1 = 0. On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1] + [1; 1 + ] [1; 1])" [0; 0] <p> On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1] + <ref> [1; 1 + ] </ref> [1; 1])" [0; 0] and ([1; 1] + [1 ; 1] [1; 1]) " [0; 0] are non-empty. 4.2 Interval Extensions for Box Consistency Box-consistency strongly depends on the interval extensions chosen for the constraints and different interval extensions can produce very different (often incomparable) tradeoffs between <p> h <ref> [1; 1] </ref>; [1; 1]i since there is no value r 1 for x 1 which satisfies r 1 + 1 r 1 = 0. On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1] + [1; 1 + ] [1; 1])" [0; 0] and ([1; 1] + [1 ; 1] [1; 1]) " [0; 0] are non-empty. 4.2 Interval Extensions for Box Consistency Box-consistency strongly depends on the interval extensions chosen for the constraints and different interval extensions can produce very different (often incomparable) tradeoffs between pruning and computational complexity. <p> On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since ([1; 1] + [1; 1 + ] [1; 1])" [0; 0] and ([1; 1] + <ref> [1 ; 1] </ref> [1; 1]) " [0; 0] are non-empty. 4.2 Interval Extensions for Box Consistency Box-consistency strongly depends on the interval extensions chosen for the constraints and different interval extensions can produce very different (often incomparable) tradeoffs between pruning and computational complexity. <p> On the other hand, the interval constraint X 1 +X 2 X 1 = 0 is box-consistent, since (<ref> [1; 1] </ref> + [1; 1 + ] [1; 1])" [0; 0] and ([1; 1] + [1 ; 1] [1; 1]) " [0; 0] are non-empty. 4.2 Interval Extensions for Box Consistency Box-consistency strongly depends on the interval extensions chosen for the constraints and different interval extensions can produce very different (often incomparable) tradeoffs between pruning and computational complexity. <p> (X ) = b f (I 1 ; : : : ; I i1 ; X; I i+1 ; : : : ; I n ) d @f Example 7 Let c be the constraint x 2 1 + x 2 2 1 = 0 and ~ I be h <ref> [1; 1] </ref>; [1; 1]i. The function F and F 0 for i = 1 in the above definition are defined as follows: F (X ) = X 2 + [1; 1] 2 1: Box-consistency on the natural extension (and on the distributed extension as well) can be applied even if the <p> = b f (I 1 ; : : : ; I i1 ; X; I i+1 ; : : : ; I n ) d @f Example 7 Let c be the constraint x 2 1 + x 2 2 1 = 0 and ~ I be h <ref> [1; 1] </ref>; [1; 1]i. The function F and F 0 for i = 1 in the above definition are defined as follows: F (X ) = X 2 + [1; 1] 2 1: Box-consistency on the natural extension (and on the distributed extension as well) can be applied even if the function is <p> 7 Let c be the constraint x 2 1 + x 2 2 1 = 0 and ~ I be h <ref> [1; 1] </ref>; [1; 1]i. The function F and F 0 for i = 1 in the above definition are defined as follows: F (X ) = X 2 + [1; 1] 2 1: Box-consistency on the natural extension (and on the distributed extension as well) can be applied even if the function is not differentiable. <p> ) where high (I; x; n) = ( right (I ) x n if x 0 _ n is even left (I) x n otherwise 16 Example 8 Consider the function x 1 (x 1 + x 2 ) 4 and assume that x 1 and x 2 range over <ref> [0; 1] </ref>. The distributed interval extension is X 2 The function F obtained by projecting the distributed interval extension on variable X 1 is F (X ) = X 2 + [0; 1]X 4: The corresponding functions f l and f u are f u (x) = x 2 4 Their <p> x 1 (x 1 + x 2 ) 4 and assume that x 1 and x 2 range over <ref> [0; 1] </ref>. The distributed interval extension is X 2 The function F obtained by projecting the distributed interval extension on variable X 1 is F (X ) = X 2 + [0; 1]X 4: The corresponding functions f l and f u are f u (x) = x 2 4 Their natural interval extensions are, of course, F u (X ) = X 2 4 The narrowing operator can now be obtained by using interval Newton method on the above two functions. <p> 35 110 260 560 1200 2400 4480 fe-ne 81 1828 2943 5103 11993 20431 42125 fe-te 95 320 920 2720 8800 30400 111360 pr-con 0 0 0 0 0 0 0 proof yes yes yes yes yes yes yes Table 2: Newton on the Broyden Banded functions with initial intervals <ref> [1; 1] </ref> 5 10 20 40 80 160 320 CPU time 0.31 2.15 5.09 12.49 27.69 61.60 143.40 growth 6.93 2.36 2.45 2.21 2.22 2.32 branching 0 0 0 0 0 0 0 pr-con 0 0 0 0 0 0 0 proof yes yes yes yes yes yes yes Table 3: <p> Table 2 reports the results of our algorithm for various sizes assuming initial intervals <ref> [1; 1] </ref>. The results indicate that Newton solves the problem using only constraint propagation: no branching is needed. In addition, the growth of the computation times is very low and indicates that Newton is essentially linear and can thus solve very large instances of this problem. <p> Note that the Hansen-Segupta's operator alone does not produce any pruning initially and returns the initial intervals whether they be of the form [10 8 ; +10 8 ] or <ref> [1; 1] </ref>. This indicates that box-consistency on the natural and distributed interval extensions are particularly effective when far from a solution while box-consistency on the Taylor extension (and the Hansen-Segupta's operator) is effective when near a solution. <p> = x 14 0:46588640 0:21790395 x 13 x 3 x 10 0 = x 16 0:26516898 0:21037773 x 4 x 19 x 9 0 = x 18 0:56003141 0:18114505 x 6 x 13 x 8 0 = x 20 0:57001682 0:17949149 x 1 x 3 x 11 with initial intervals <ref> [1; 2] </ref>. Benchmark i3 has the same set of equations as i2 but has initial intervals [2; 2]. <p> 0:44166728 0:19950920 x 2 6 x 2 0 = x 2 8 x 2 10 7 0:42937161 0:21180486 x 2 5 x 2 0 = x 2 1 x 2 6 9 0:34504906 0:19612740 x 2 6 x 2 0 = x 2 4 x 2 1 and initial intervals <ref> [1; 1] </ref>. The number of solutions must be a multiple of 1024. <p> 7: Newton on the Traditional Interval Arithmetic Benchmarks i1 i2 i3 i4 i5 CPU time 14.28 1821.23 5640.80 445.28 33.58 branching 498 9031 36933 11263 1173 fe-tot 77380 6441640 19979025 2554066 154948 proof yes yes yes yes yes Table 8: HRB on the Traditional Interval Arithmetic Benchmarks and initial intervals <ref> [1; 1] </ref>. Newton solves all the problems with one solution without branching and solves the problem having 1024 solutions with 1023 branchings. Note also that box-consistency on the distributed extension solves benchmark i1 alone. The results once again confirm our observation on when the various extensions are useful. <p> Table 13 describes the results of Newton on for the initial intervals <ref> [1; 1] </ref> and [10 8 ; 10 8 ]. Newton behaves well on this example, since the continuation method of [35] takes about 57 seconds. Note once again that a substantial increase in the size of the initial intervals only induces a slowdown of about 2.5 for Newton.
Reference: [2] <author> F. Benhamou, D. McAllester, and P. Van Hentenryck. </author> <title> CLP(Intervals) Revisited. </title> <booktitle> In Proceedings of the International Symposium on Logic Programming (ILPS-94), </booktitle> <pages> pages 124-138, </pages> <address> Ithaca, NY, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Recall that we assume that all constraints are defined over variables x 1 ; : : : ; x n . 4 It is easy to extend the language to include functions such as sin, cos, e, : : : . 7 4.1 Box Consistency Box-consistency <ref> [2] </ref> is an approximation of arc-consistency, a notion well-known in artificial intelligence [16] which states a simple local condition on a constraint c and the set of possible values for each of its variables, say D 1 ; : : : ; D n . <p> Example 3 Let c be the constraint x 2 1 + x 2 2 = 1. c is arc-consistent wrt h [1; 1] ; [1; 1] i but is not arc-consistent wrt h [1; 1] ; <ref> [2; 2] </ref> i since, for instance, there is no value r 1 for x 1 in [1; 1] such that r 2 1 + 2 2 = 1. <p> Moreover, decomposing complex constraints into simple constraints entails a substantial loss in pruning, making this approach unpractical on many applications. See <ref> [2] </ref> for experimental results on this approach and their comparison with the approach presented in this paper. The notion of box-consistency introduced in [2] is a coarser approximation of arc-consistency which provides a much better trade-off between efficiency and pruning. <p> Moreover, decomposing complex constraints into simple constraints entails a substantial loss in pruning, making this approach unpractical on many applications. See <ref> [2] </ref> for experimental results on this approach and their comparison with the approach presented in this paper. The notion of box-consistency introduced in [2] is a coarser approximation of arc-consistency which provides a much better trade-off between efficiency and pruning. It consists in replacing the existential quantification in the above condition by the evaluation of an interval extension of the constraint on the intervals of the existential variables. <p> = x 4 0:19807914 0:15585316 x 7 x 1 x 6 0 = x 6 0:14654113 0:18922793 x 8 x 5 x 10 0 = x 8 0:07056438 0:17081208 x 1 x 7 x 6 0 = x 10 0:42651102 0:21466544 x 4 x 8 x 1 with initial intervals <ref> [2; 2] </ref>. <p> = x 14 0:46588640 0:21790395 x 13 x 3 x 10 0 = x 16 0:26516898 0:21037773 x 4 x 19 x 9 0 = x 18 0:56003141 0:18114505 x 6 x 13 x 8 0 = x 20 0:57001682 0:17949149 x 1 x 3 x 11 with initial intervals <ref> [1; 2] </ref>. Benchmark i3 has the same set of equations as i2 but has initial intervals [2; 2]. <p> Benchmark i3 has the same set of equations as i2 but has initial intervals <ref> [2; 2] </ref>. <p> simple constraints such as x 1 = x 2 + x 3 ; x 1 = x 2 x 3 ; x 1 = x 2 fi x 3 and complex constraints were decomposed in terms of these simple constraints. 30 As mentioned earlier, this approach is not very effective <ref> [2] </ref> and our main goal was to design new approximations of arc-consistency that could make use of existing interval methods.
Reference: [3] <author> F. Benhamou and W. </author> <title> Older. Applying Interval Arithmetic to Real, Integer and Boolean Constraints. </title> <journal> Journal of Logic Programming, </journal> <note> 1995. To appear. </note>
Reference-contexts: this purpose, i.e., we compute I u (I c I n =I d ): This result may be more precise than (I 1 ] : : : ] I n ) " (I 0 m ): To illustrate the gain of precision, consider I = [5; 1], I 1 = <ref> [1; 3] </ref> and I 2 = [3; 1]. <p> u (I c I n =I d ): This result may be more precise than (I 1 ] : : : ] I n ) " (I 0 m ): To illustrate the gain of precision, consider I = [5; 1], I 1 = [1; 3] and I 2 = <ref> [3; 1] </ref>. <p> The expression I u (I 1 <ref> [ I 2 ) evaluates to the interval [5; 3] </ref>, while the expression I " (I 1 ] I 2 ) returns the interval [5; 1], since I 1 ] I 2 = [1; 1]. 3.3 Constraint Representations It is well-known that different computer representations of a real function produce different <p> The expression I u (I 1 [ I 2 ) evaluates to the interval <ref> [5; 3] </ref>, while the expression I " (I 1 ] I 2 ) returns the interval [5; 1], since I 1 ] I 2 = [1; 1]. 3.3 Constraint Representations It is well-known that different computer representations of a real function produce different results when evaluated with floating-point numbers on a <p> j 9r 1 2 I 1 ; : : : ; 9r i1 2 I i1 ; : : : ; 9r i+1 2 I i+1 ; 9r n 2 I n : c (r 1 ; : : :; r n ) gg: This condition, used in systems like <ref> [26, 3] </ref>, is easily enforced on simple constraints such as x 1 = x 2 + x 3 ; x 1 = x 2 x 3 ; x 1 = x 2 fi x 3 but it is also computationally very expensive for complex constraints with multiple occurrences of the same <p> with the size of the initial intervals, showing a limitation of the method on this example. 7 Related Work and Discussion The research described in this paper originated in an attempt to improve the efficiency of constraint logic programming languages based on intervals such as BNR-Prolog [26] and CLP (BNR) <ref> [3] </ref>.
Reference: [4] <author> R. Hammer, M. Hocks, M. Kulisch, and D. Ratz. </author> <title> Numerical Toolbox for Verified Computing I Basic Numerical Problems, Theory, Algorithms, and PASCAL-XSC Programs. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1993. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> it to HRB is meaningful. 19 Benchmarks v d range Newton HRB CONT Broyden 10 3 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 <p> d range Newton HRB CONT Broyden 10 3 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 <p> 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 ] 353.06 4730.34 35.61 eco 5 54 [10 <p> 0 0 na-ne 3663 12616 46555 213949 1236532 na-tot 3767 12871 47060 214954 1239339 fe-te 884 3111 11211 42411 166415 fe-grow 3.62 3.41 4.27 5.40 pr-con 1 1 1 1 1 proof yes no no no no Table 4: Newton on the More-Cosnard nonlinear integral Equation with initial intervals in <ref> [4; 5] </ref> which have widths lower than 10 6 . Note that the Hansen-Segupta's operator alone does not produce any pruning initially and returns the initial intervals whether they be of the form [10 8 ; +10 8 ] or [1; 1]. <p> These functions come from the discretization of a nonlinear integral equation, giving a constraint system denser than the sparse constraint system for the Broyden banded functions. The variables x i were given initial domains <ref> [4; 5] </ref> as in [28] and the computation results are given in Table 4. Once again, it is interesting to note that Newton is completely deterministic on this problem, i.e., it does not do any branching. Newton is probably cubic in the number of variables for this problem. <p> in [10 8 ; 0] 5 10 20 40 80 CPU time 0.66 7.76 968.25 ? ? branching 5 24 508 ? ? fe-tot 3709 20194 1285764 ? ? proof yes no no ? ? Table 6: The HRB algorithm on the More-Cosnard nonlinear integral Equation with initial intervals in <ref> [4; 5] </ref> much precision due to the dependency problem (multiple occurrences of the same variable) and box-consistency on the Taylor interval extension is not helpful initially.
Reference: [5] <author> E. Hansen. </author> <title> Global Optimization Using Interval Analysis. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> I 0 is used for this purpose, i.e., we compute I u (I c I n =I d ): This result may be more precise than (I 1 ] : : : ] I n ) " (I 0 m ): To illustrate the gain of precision, consider I = <ref> [5; 1] </ref>, I 1 = [1; 3] and I 2 = [3; 1]. The expression I u (I 1 [ I 2 ) evaluates to the interval [5; 3], while the expression I " (I 1 ] I 2 ) returns the interval [5; 1], since I 1 ] I 2 <p> The expression I u (I 1 [ I 2 ) evaluates to the interval <ref> [5; 3] </ref>, while the expression I " (I 1 ] I 2 ) returns the interval [5; 1], since I 1 ] I 2 = [1; 1]. 3.3 Constraint Representations It is well-known that different computer representations of a real function produce different results when evaluated with floating-point numbers on a <p> the gain of precision, consider I = <ref> [5; 1] </ref>, I 1 = [1; 3] and I 2 = [3; 1]. The expression I u (I 1 [ I 2 ) evaluates to the interval [5; 3], while the expression I " (I 1 ] I 2 ) returns the interval [5; 1], since I 1 ] I 2 = [1; 1]. 3.3 Constraint Representations It is well-known that different computer representations of a real function produce different results when evaluated with floating-point numbers on a computer. <p> it to HRB is meaningful. 19 Benchmarks v d range Newton HRB CONT Broyden 10 3 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 <p> d range Newton HRB CONT Broyden 10 3 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 <p> 10 [-1,1] 1.65 18.23 Broyden 20 3 20 [-1,1] 4.25 ? Broyden 320 3 320 [-1,1] 113.71 ? Broyden 320 3 320 [10 8 ; 10 8 ] 143.40 ? More-Cosnard 20 3 20 <ref> [4; 5] </ref> 24.49 968.25 More-Cosnard 40 3 40 [4; 5] 192.81 ? More-Cosnard 80 3 80 [4; 5] 1752.64 ? More-Cosnard 80 3 80 [10 8 ; 0] 1735.09 ? i1 10 3 10 [-2,2] 0.06 14.28 i3 20 3 20 [-2,2] 0.31 5640.80 i5 10 11 10 [-1,1] 0.08 33.58 kin2 8 256 [10 8 ; 10 8 ] 353.06 4730.34 35.61 eco 5 54 [10 <p> 0 0 na-ne 3663 12616 46555 213949 1236532 na-tot 3767 12871 47060 214954 1239339 fe-te 884 3111 11211 42411 166415 fe-grow 3.62 3.41 4.27 5.40 pr-con 1 1 1 1 1 proof yes no no no no Table 4: Newton on the More-Cosnard nonlinear integral Equation with initial intervals in <ref> [4; 5] </ref> which have widths lower than 10 6 . Note that the Hansen-Segupta's operator alone does not produce any pruning initially and returns the initial intervals whether they be of the form [10 8 ; +10 8 ] or [1; 1]. <p> These functions come from the discretization of a nonlinear integral equation, giving a constraint system denser than the sparse constraint system for the Broyden banded functions. The variables x i were given initial domains <ref> [4; 5] </ref> as in [28] and the computation results are given in Table 4. Once again, it is interesting to note that Newton is completely deterministic on this problem, i.e., it does not do any branching. Newton is probably cubic in the number of variables for this problem. <p> in [10 8 ; 0] 5 10 20 40 80 CPU time 0.66 7.76 968.25 ? ? branching 5 24 508 ? ? fe-tot 3709 20194 1285764 ? ? proof yes no no ? ? Table 6: The HRB algorithm on the More-Cosnard nonlinear integral Equation with initial intervals in <ref> [4; 5] </ref> much precision due to the dependency problem (multiple occurrences of the same variable) and box-consistency on the Taylor interval extension is not helpful initially.
Reference: [6] <author> E.R. Hansen. </author> <title> Global Optimization Using Interval Analysis: the Multi-Dimensional Case. </title> <journal> Numer. Math, </journal> <volume> 34 </volume> <pages> 247-270, </pages> <year> 1980. </year> <month> 32 </month>
Reference-contexts: A tight extension of division to intervals can be best expressed if its result is allowed to be a union of intervals <ref> [10, 6, 12] </ref>.
Reference: [7] <author> E.R. Hansen and R.I. Greenberg. </author> <title> An Interval Newton Method. </title> <journal> Appl. Math. Comput., </journal> <volume> 12 </volume> <pages> 89-98, </pages> <year> 1983. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 19, 20] </ref>). Our definitions are slightly non-standard. 3.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> Note that the times 9 The width of [l; u] is u l. 10 Some interval methods such as <ref> [7] </ref> are more sophisticated than HRB but the sophistication aims at speeding up the computation near a solution. <p> We also indicate the number of preconditionings by pr-con and whether the algorithm can prove the existence of the solutions in the resulting intervals by proof. 6.1 Broyden Banded Functions This is a traditional benchmark of interval techniques and was used for instance in <ref> [7] </ref>. It consists in finding the zeros of the functions f i (x 1 ; : : : ; x n ) = x i (2 + 5x 2 P where J i = fj j j 6= i & max (1; i 5) j min (n; i + 1)g.
Reference: [8] <author> E.R. Hansen and S. Sengupta. </author> <title> Bounding Solutions of Systems of Equations Using Interval Analysis. </title> <journal> BIT, </journal> <volume> 21 </volume> <pages> 203-211, </pages> <year> 1981. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> Box-consistency is parametrized by an interval extension operator for the constraint and can be instantiated to produce various narrowing operators. In particular, box-consistency on the Taylor extension of the constraint produces a generalization of the Hansen-Segupta operator <ref> [8] </ref>, well-known in interval methods. In addition, box-consistency on the natural extension produces narrowing operators which are more effective when the algorithm is not near a solution. <p> It outperforms the interval methods we are aware of and compares well with state-of-the-art continuation methods on many problems. Interestingly, Newton solves the Broyden banded function problem <ref> [8] </ref> and More-Cosnard discretization of a nonlinear integral equation [22] for several hundred variables. * Simplicity and Uniformity: Newton is based on simple mathematical results and is easy to use and to implement. It is also based on a single concept: box-consistency. <p> More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 19, 20] </ref>). Our definitions are slightly non-standard. 3.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> ; : : : ; m n ) + i=1 @x i In the current version of our system, the partial derivatives are computed numerically using automatic differentiation [27]. 4.3 Conditioning It is interesting to note that box-consistency on the Taylor interval extension is closely related to the Hansen-Segupta's operator <ref> [8] </ref>, which is an improvement over Krawczyk's operator [15]. <p> If it is not, then the problem reduces once again to finding the leftmost and/or rightmost zeros. 6 Experimental Results This section reports experimental results of Newton on a variety of standard benchmarks. The benchmarks were taken from papers on numerical analysis [22], interval analysis <ref> [8, 11, 21] </ref>, and continuation methods [35, 24, 23, 17]. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [9] <author> E.R. Hansen and R.R. Smith. </author> <title> Interval Arithmetic in Matrix Computation: Part II. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 4 </volume> <pages> 1-9, </pages> <year> 1967. </year>
Reference-contexts: Hansen and Smith <ref> [9] </ref> also argued that these operators are more effective for a system ff 1 = 0; : : : ; f n = 0g wrt a box hI 1 ; : : : ; I n i when the interval Jacobian M ij = @x j is diagonally dominant, i.e., mig
Reference: [10] <author> R.J. Hanson. </author> <title> Interval Arithmetic as a Closed Arithmetic System on a Computer. </title> <type> Jet Propulsion Laboratory Report 197, </type> <year> 1968. </year>
Reference-contexts: A tight extension of division to intervals can be best expressed if its result is allowed to be a union of intervals <ref> [10, 6, 12] </ref>. <p> 29.88 5.87 eco 7 486 [10 8 ; 10 8 ] 127.65 ? 991.45 eco 9 4374 [10 8 ; 10 8 ] 8600.28 ? combustion 10 96 [10 8 ; 10 8 ] 9.94 ? 57.40 chemistry 5 108 [0; 10 8 ] 6.32 ? 56.55 neuro 6 1024 <ref> [10; 10] </ref> 0.91 28.84 5.02 neuro 6 1024 [1000; 1000] 172.71 ? 5.02 Table 1: Summary of the Experimental Results for the continuation method are on a DEC 5000/200. A space in a column means that the result is not available for the method. <p> They indicate that Newton is particularly effective on this problem, since it takes about 6 seconds and proves the existence of a solution in the final intervals. Note that the continuation method of [35] takes about 56 seconds on this problem. 29 <ref> [10; 10] </ref> [10 2 ; 10 2 ] [10 3 ; 10 3 ] [10 4 ; 10 4 ] CPU time 0.91 11.69 172.71 2007.51 growth 12.84 14.77 11.62 branching 52 663 9632 115377 na-ee 4290 57224 645951 6541038 na-tot 5100 63932 742839 7714494 fe-te 4104 48804 769056 9376632 pr-con <p> The continuation method of [35] solves this problem in about 6 seconds. The results of Newton are depicted in Table 15 for various initial intervals. Newton is fast when the initial intervals are small (i.e., <ref> [10; 10] </ref>).
Reference: [11] <author> H. Hong and V. Stahl. </author> <title> Safe Starting Regions by Fixed Points and Tightening. </title> <booktitle> Computing, </booktitle> <address> 53(3-4):323-335, </address> <year> 1994. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> +1:0000000000012430057] and to the second solution x 1 2 [+0:7861513777574231642; +0:7861513777574233864] Note that, in this case, Newton makes the smallest number of choices to isolate the solutions. 2 To conclude this motivating section, let us illustrate Newton on a larger example which describes the inverse kinematics of an elbow manipulator <ref> [11] </ref>: 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; : c 1 c 2 s 5 + c 1 c 3 s 5 + c 1 c 4 s 5 + s 1 c 5 = 1:9115 c 1 c 2 + c 1 c 3 + c <p> However, since the function is in distributed form, it is possible to do better by using an idea from <ref> [11] </ref>. <p> If it is not, then the problem reduces once again to finding the leftmost and/or rightmost zeros. 6 Experimental Results This section reports experimental results of Newton on a variety of standard benchmarks. The benchmarks were taken from papers on numerical analysis [22], interval analysis <ref> [8, 11, 21] </ref>, and continuation methods [35, 24, 23, 17]. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> Finally, Table 6 gives the results for the HRB algorithm on this problem. Once again, Newton outperforms the HRB method substantially. 6.3 Interval Arithmetic Benchmarks This section considers standard benchmarks from interval arithmetic papers <ref> [20, 11] </ref>. <p> Note also that box-consistency on the distributed extension solves benchmark i1 alone. The results once again confirm our observation on when the various extensions are useful. Closely related results were observed in <ref> [11] </ref> on these benchmarks (see the related work section for a more detailed comparison) but our algorithm is in general about 4 times faster (assuming similar machines) and does not do any branching on i5. Table 8 also describes the results for the traditional interval arithmetic method. <p> The excution times then dropped to less than 0.5 seconds. 6.4 Kinematics Applications We now describe the performance of Newton on two kinematics examples. Application kin1 comes from robotics and describes the inverse kinematics of an elbow manipulator <ref> [11] </ref>. <p> In both examples, the initial intervals were given as [10 8 ; 10 8 ]. The results of Newton on these two benchmarks are given in Table 10. Newton is fast on the first benchmark and does not branch much to obtain all solutions. The algorithm in <ref> [11] </ref> branches more (the reported figure is 257 branches but it is not really comparable due to the nature of the algorithm) and is about 16 times slower on comparable machines. We are not aware of the results of continuation methods on this problem. <p> It is interesting to note that the idea of using approximations of arc-consistency was also used independently by Hong and Stahl <ref> [11] </ref>, who were also exposed to research on Constraint Logic Programming. Their use of projections is however quite different from ours. The key idea is to work with a set of boxes and to use projections to split a box into several subboxes by isolating all zeros of a projection.
Reference: [12] <author> W.M. Kahan. </author> <title> A More Complete Interval Arithmetic. </title> <booktitle> Lecture Notes for a Summer Course at the University of Michigan, </booktitle> <year> 1968. </year>
Reference-contexts: A tight extension of division to intervals can be best expressed if its result is allowed to be a union of intervals <ref> [10, 6, 12] </ref>.
Reference: [13] <author> R.B. Kearfott. </author> <title> Preconditioners for the Interval Gauss-Seidel Method. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 27, </volume> <year> 1990. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> The resulting system is generally solved through Gauss-Seidel iterations, giving the Hansen-Segupta's operator. See also <ref> [13, 14] </ref> for an extensive coverage of conditioners. Newton exploits this idea to improve the effectiveness of box-consistency on the Taylor interval extension. The conditioning of Newton is abstracted by the following definition. <p> Note also that our current implementation does not use some of the novel techniques of the interval community such as the more advanced conditioners and splitting techniques of <ref> [13] </ref>. It is of course possible to include them easily, since the overall recursive structure of the implementations is essentially similar. Integrating these results would obviously be of benefit, since these techniques are complementary to ours.
Reference: [14] <author> R.B. Kearfott. </author> <title> A Review of Preconditioners for the Interval Gauss-Seidel Method. Interval Computations 1, </title> <booktitle> 1 </booktitle> <pages> 59-85, </pages> <year> 1991. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> The resulting system is generally solved through Gauss-Seidel iterations, giving the Hansen-Segupta's operator. See also <ref> [13, 14] </ref> for an extensive coverage of conditioners. Newton exploits this idea to improve the effectiveness of box-consistency on the Taylor interval extension. The conditioning of Newton is abstracted by the following definition.
Reference: [15] <author> R. Krawczyk. </author> <title> Newton-Algorithmen zur Bestimmung von Nullstellen mit Fehlerschranken. </title> <journal> Computing, </journal> <volume> 4 </volume> <pages> 187-201, </pages> <year> 1969. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> + i=1 @x i In the current version of our system, the partial derivatives are computed numerically using automatic differentiation [27]. 4.3 Conditioning It is interesting to note that box-consistency on the Taylor interval extension is closely related to the Hansen-Segupta's operator [8], which is an improvement over Krawczyk's operator <ref> [15] </ref>.
Reference: [16] <author> A.K. Mackworth. </author> <title> Consistency in Networks of Relations. </title> <journal> Artificial Intelligence, </journal> <volume> 8(1) </volume> <pages> 99-118, </pages> <year> 1977. </year>
Reference-contexts: The pruning in Newton is achieved by enforcing a unique local consistency condition, called box-consistency, at each node of the search tree. Box-consistency is an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [16, 18] </ref> and used to solve discrete combinatorial problems in several systems (e.g.,, [31, 32]). Box-consistency is parametrized by an interval extension operator for the constraint and can be instantiated to produce various narrowing operators. <p> The pruning step ensures that some local consistency conditions are satisfied. It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Newton is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [16, 18] </ref> and used in many systems (e.g., [32, 34, 30]) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied. <p> defined over variables x 1 ; : : : ; x n . 4 It is easy to extend the language to include functions such as sin, cos, e, : : : . 7 4.1 Box Consistency Box-consistency [2] is an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [16] </ref> which states a simple local condition on a constraint c and the set of possible values for each of its variables, say D 1 ; : : : ; D n .
Reference: [17] <author> K. Meintjes and A.P. Morgan. </author> <title> Chemical Equilibrium Systems as Numerical test Problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 </volume> <pages> 143-151, </pages> <year> 1990. </year>
Reference-contexts: The benchmarks were taken from papers on numerical analysis [22], interval analysis [8, 11, 21], and continuation methods <ref> [35, 24, 23, 17] </ref>. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> CPU time branching na-ee na-te na-tot fe-ee fe-te fe-tot pr-con proof 6.32 256 13725 3400 17125 34811 17425 52236 425 yes Table 14: Newton on the Chemestry Problem with Initial Intervals [0; 10 8 ] 6.7 Chemical Equilibrium Application This problem originates from <ref> [17] </ref> and describes a chemical equilibrium system.
Reference: [18] <author> U. Montanari. </author> <title> Networks of Constraints : Fundamental Properties and Applications to Picture Processing. </title> <journal> Information Science, </journal> <volume> 7(2) </volume> <pages> 95-132, </pages> <year> 1974. </year>
Reference-contexts: The pruning in Newton is achieved by enforcing a unique local consistency condition, called box-consistency, at each node of the search tree. Box-consistency is an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [16, 18] </ref> and used to solve discrete combinatorial problems in several systems (e.g.,, [31, 32]). Box-consistency is parametrized by an interval extension operator for the constraint and can be instantiated to produce various narrowing operators. <p> The pruning step ensures that some local consistency conditions are satisfied. It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Newton is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence <ref> [16, 18] </ref> and used in many systems (e.g., [32, 34, 30]) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied. <p> The research described here also provides a uniform framework to integrate these techniques in Constraint Logic Programming, to understand the importance of the various pruning operators and their relationships and to suggest further research directions. For instance, higher notions of consistency such as path-consistency <ref> [18] </ref> may be worth investigating for some applications. 11 The idea of sandwitching the interval function in between two real functions is described there. 31 8 Conclusion In this paper, we presented a branch & prune algorithm to find all isolated solutions to a system of polynomial constraints over the reals.
Reference: [19] <author> R.E. Moore. </author> <title> Interval Analysis. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1966. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 19, 20] </ref>). Our definitions are slightly non-standard. 3.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> This extension is an example of centered forms which are interval extensions introduced by Moore <ref> [19] </ref> and studied by many authors, since they have important properties. The Taylor interval extension of a constraint is parametrized by the intervals for the variables in the constraint.
Reference: [20] <author> R.E. Moore. </author> <title> Methods and Applications of Interval Analysis. </title> <publisher> SIAM Publ., </publisher> <year> 1979. </year>
Reference-contexts: More information on interval arithmetic can be found in many places (e.g., <ref> [1, 8, 7, 19, 20] </ref>). Our definitions are slightly non-standard. 3.1 Interval Arithmetic We consider &lt; 1 = &lt; [ f1; 1g the set of real numbers extended with the two infinity symbols and the natural extension of the relation &lt; to this set. <p> Finally, Table 6 gives the results for the HRB algorithm on this problem. Once again, Newton outperforms the HRB method substantially. 6.3 Interval Arithmetic Benchmarks This section considers standard benchmarks from interval arithmetic papers <ref> [20, 11] </ref>.
Reference: [21] <author> R.E. Moore and S.T. Jones. </author> <title> Safe Starting Regions for Iterative Methods. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 14 </volume> <pages> 1051-1065, </pages> <year> 1977. </year>
Reference-contexts: If it is not, then the problem reduces once again to finding the leftmost and/or rightmost zeros. 6 Experimental Results This section reports experimental results of Newton on a variety of standard benchmarks. The benchmarks were taken from papers on numerical analysis [22], interval analysis <ref> [8, 11, 21] </ref>, and continuation methods [35, 24, 23, 17]. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching.
Reference: [22] <author> J.J. More and M.Y. Cosnard. </author> <title> Numerical Solution of Nonlinear Equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 </volume> <pages> 64-85, </pages> <year> 1979. </year>
Reference-contexts: It outperforms the interval methods we are aware of and compares well with state-of-the-art continuation methods on many problems. Interestingly, Newton solves the Broyden banded function problem [8] and More-Cosnard discretization of a nonlinear integral equation <ref> [22] </ref> for several hundred variables. * Simplicity and Uniformity: Newton is based on simple mathematical results and is easy to use and to implement. It is also based on a single concept: box-consistency. The rest of this paper is organized as follows. Section 2 gives an overview of the approach. <p> If it is not, then the problem reduces once again to finding the leftmost and/or rightmost zeros. 6 Experimental Results This section reports experimental results of Newton on a variety of standard benchmarks. The benchmarks were taken from papers on numerical analysis <ref> [22] </ref>, interval analysis [8, 11, 21], and continuation methods [35, 24, 23, 17]. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> HRB takes 0.34 seconds on n = 5 with 18 branchings, about 18 seconds for n = 10 with about 300 branchings, and does not return after more than an hour on n = 20. 6.2 Discretization of a Nonlinear Integral Equation This example comes from <ref> [22] </ref> and is also a standard benchmark for nonlinear equation solving .
Reference: [23] <author> A.P. Morgan. </author> <title> Computing All Solutions To Polynomial Systems Using Homotopy Continuation. </title> <journal> Appl. Math. Comput., </journal> <volume> 24 </volume> <pages> 115-138, </pages> <year> 1987. </year>
Reference-contexts: The benchmarks were taken from papers on numerical analysis [22], interval analysis [8, 11, 21], and continuation methods <ref> [35, 24, 23, 17] </ref>. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> c 4 + c 1 c 2 + c 1 c 3 + c 1 c 2 = 4:0616 s 2 + s 3 + s 4 + s 2 + s 3 + s 2 = 3:9701 i + c 2 The second benchmark, denoted by kin2, is from <ref> [23] </ref> and describes the inverse position problem for a six-revolute-joint problem in mechanics.
Reference: [24] <author> A.P. Morgan. </author> <title> Solving Polynomial Systems Using Continuation for Scientific and Engineering Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1987. </year> <month> 33 </month>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29]) and continuation methods (e.g., <ref> [24, 35] </ref>). Continuation methods have been shown to be effective for problems for which the total degree is not too high, since the number of paths explored depends on the estimation of the number of solutions. Interval methods are generally robust but tend to be slow. <p> The benchmarks were taken from papers on numerical analysis [22], interval analysis [8, 11, 21], and continuation methods <ref> [35, 24, 23, 17] </ref>. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> 8 &gt; &gt; &gt; : x 14 = x 3 + x 5 + x 7 x 16 = 3 fl x 4 + 2 fl x 6 + x 8 to improve efficiently slightly in the first problem. 6.5 An Economics Modelling Application The following example is taken from <ref> [24] </ref>. It is a difficult economic modelling problem that can be scaled up to arbitrary dimensions. <p> Note also that Newton can establish the existence of solutions for these problems. Finally, it is worthwhile stating that the results were obtained for a computer representation where x n has been eliminated in a problem of dimension n. 6.6 Combustion Application This problem is also from Morgan's book <ref> [24] </ref> and represents a combustion problem for a tempara-ture of 3000 ffi .
Reference: [25] <author> A. Neumaier. </author> <title> Interval Methods for Systems of Equations. </title> <booktitle> PHI Series in Computer Science. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]). <p> The natural interval extension of the constraint x 1 (x 2 +x 3 ) = 0 is the interval constraint X 1 (X 2 + X 3 ) _=0: 5 It is interesting to note that this definition is also related to the theorem of Miranda <ref> [25] </ref>. <p> If hI 0 n i hI 1 ; : : : ; I n i then there exists a unique zero in hI 0 1 ; : : : ; I 0 n i. A proof of this result can be found in <ref> [25] </ref> where credit is given to Moore and Nickel.
Reference: [26] <author> W. Older and A. Vellino. </author> <title> Extending Prolog with Constraint Arithmetics on Real Intervals. </title> <booktitle> In Canadian Conference on Computer & Electrical Engineering, </booktitle> <address> Ottawa, </address> <year> 1990. </year>
Reference-contexts: j 9r 1 2 I 1 ; : : : ; 9r i1 2 I i1 ; : : : ; 9r i+1 2 I i+1 ; 9r n 2 I n : c (r 1 ; : : :; r n ) gg: This condition, used in systems like <ref> [26, 3] </ref>, is easily enforced on simple constraints such as x 1 = x 2 + x 3 ; x 1 = x 2 x 3 ; x 1 = x 2 fi x 3 but it is also computationally very expensive for complex constraints with multiple occurrences of the same <p> the algorithm increases linearly with the size of the initial intervals, showing a limitation of the method on this example. 7 Related Work and Discussion The research described in this paper originated in an attempt to improve the efficiency of constraint logic programming languages based on intervals such as BNR-Prolog <ref> [26] </ref> and CLP (BNR) [3].
Reference: [27] <author> L.B. Rall. </author> <title> Automatic Differentiation: Techniques and Applications. </title> <booktitle> Springer Lectures Notes in Computer Science, </booktitle> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: of c wrt ~ I , denoted by c t ( ~ I) , is the interval constraint b f (m 1 ; : : : ; m n ) + i=1 @x i In the current version of our system, the partial derivatives are computed numerically using automatic differentiation <ref> [27] </ref>. 4.3 Conditioning It is interesting to note that box-consistency on the Taylor interval extension is closely related to the Hansen-Segupta's operator [8], which is an improvement over Krawczyk's operator [15]. <p> Automatic Differentiation As mentioned, our algorithm takes a very simple approach to obtain partial derivatives, i.e., no effort is spent in factoring common expressions to reduce the dependency problem of interval arithmetic. The main reason comes from the fact that we are using automatic differentiation <ref> [27] </ref> to evaluate the derivatives together with the functions. This choice may be reconsidered in a future version of the system. Inequalities It is straightforward to generalize the above algorithms for inequalities. In general, it suffices to test if the inequality is satisfied at the end points of the interval.
Reference: [28] <author> H. Ratschek and J. Rokne. </author> <title> New Computer Methods for Global Optimization. </title> <publisher> Ellis Horwood Limited, </publisher> <address> Chichester, </address> <year> 1988. </year>
Reference-contexts: These functions come from the discretization of a nonlinear integral equation, giving a constraint system denser than the sparse constraint system for the Broyden banded functions. The variables x i were given initial domains [4; 5] as in <ref> [28] </ref> and the computation results are given in Table 4. Once again, it is interesting to note that Newton is completely deterministic on this problem, i.e., it does not do any branching. Newton is probably cubic in the number of variables for this problem.
Reference: [29] <author> S.M. Rump. </author> <title> Verification Methods for Dense and Sparse Systems of Equations. </title> <editor> In J. (Ed.) Herzberger, editor, </editor> <booktitle> Topics in Validated Computations, </booktitle> <pages> pages 217-231. </pages> <publisher> Elsevier, </publisher> <year> 1988. </year>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., <ref> [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29] </ref>) and continuation methods (e.g., [24, 35]).
Reference: [30] <author> J. Siskind and D. McAllester. </author> <title> Nondeterministic Lisp as a Substrate for Constraint Logic Programming. </title> <booktitle> In AAAI-93, </booktitle> <pages> pages 133-138, </pages> <year> 1993. </year>
Reference-contexts: It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Newton is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence [16, 18] and used in many systems (e.g., <ref> [32, 34, 30] </ref>) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied.
Reference: [31] <author> P. Van Hentenryck. </author> <title> A Logic Language for Combinatorial Optimization. </title> <journal> Annals of Operations Research, </journal> <volume> 21 </volume> <pages> 247-274, </pages> <year> 1989. </year>
Reference-contexts: The pruning in Newton is achieved by enforcing a unique local consistency condition, called box-consistency, at each node of the search tree. Box-consistency is an approximation of arc-consistency, a notion well-known in artificial intelligence [16, 18] and used to solve discrete combinatorial problems in several systems (e.g.,, <ref> [31, 32] </ref>). Box-consistency is parametrized by an interval extension operator for the constraint and can be instantiated to produce various narrowing operators. In particular, box-consistency on the Taylor extension of the constraint produces a generalization of the Hansen-Segupta operator [8], well-known in interval methods.
Reference: [32] <author> P. Van Hentenryck. </author> <title> Constraint Satisfaction in Logic Programming. Logic Programming Series, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The pruning in Newton is achieved by enforcing a unique local consistency condition, called box-consistency, at each node of the search tree. Box-consistency is an approximation of arc-consistency, a notion well-known in artificial intelligence [16, 18] and used to solve discrete combinatorial problems in several systems (e.g.,, <ref> [31, 32] </ref>). Box-consistency is parametrized by an interval extension operator for the constraint and can be instantiated to produce various narrowing operators. In particular, box-consistency on the Taylor extension of the constraint produces a generalization of the Hansen-Segupta operator [8], well-known in interval methods. <p> It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Newton is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence [16, 18] and used in many systems (e.g., <ref> [32, 34, 30] </ref>) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied.
Reference: [33] <author> P. Van Hentenryck, D. McAllester, and D. Kapur. </author> <title> Interval Methods Revisited. </title> <type> Technical Report No., </type> <institution> MIT AI Lab, MIT, </institution> <year> 1995. </year>
Reference-contexts: Section 5 discusses the implementation of the box-consistency. Section 6 describes the experimental results. Section 7 discusses related work and the development of the ideas presented here. Section 8 concludes the paper. A short version of this paper is available in <ref> [33] </ref>. 2 Overview of The Approach As mentioned, Newton is a global search algorithm which solves a problem by dividing it into subproblems which are solved recursively.
Reference: [34] <author> P. Van Hentenryck, V. Saraswat, and Y. Deville. </author> <title> The Design, Implementation, and Evaluation of the Constraint Language cc(FD). In Constraint Programming: Basics and Trends. </title> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: It consists of reducing the intervals associated with the variables so that every constraint appears to be locally consistent. The local consistency condition of Newton is called box-consistency, an approximation of arc-consistency, a notion well-known in artificial intelligence [16, 18] and used in many systems (e.g., <ref> [32, 34, 30] </ref>) to solve discrete combinatorial search problems. Informally speaking, a constraint is arc-consistent if for each value in the range of a variable there exist values in the ranges of the other variables such that the constraint is satisfied.
Reference: [35] <author> J Verschelde, P. Verlinden, and R. Cools. </author> <title> Homotopies Exploiting Newton Polytopes For Solving Sparse Polynomial Systems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 31(3) </volume> <pages> 915-930, </pages> <year> 1994. </year> <month> 34 </month>
Reference-contexts: Several interesting methods have been proposed in the past for this task, including two fundamentally different methods: interval methods (e.g., [4, 5, 7, 8, 11, 13, 14, 15, 19, 25, 29]) and continuation methods (e.g., <ref> [24, 35] </ref>). Continuation methods have been shown to be effective for problems for which the total degree is not too high, since the number of paths explored depends on the estimation of the number of solutions. Interval methods are generally robust but tend to be slow. <p> The benchmarks were taken from papers on numerical analysis [22], interval analysis [8, 11, 21], and continuation methods <ref> [35, 24, 23, 17] </ref>. We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. <p> We also compare Newton with a traditional interval method using the Hansen-Segupta's operator, range testing, and branching. This method uses the same implementation technology as Newton and is denoted by HRB in the following. 10 Finally, we compare Newton with a state-of-the-art continuation method <ref> [35] </ref>, denoted by CONT in the following. Note that all results given in this section were obtained by running Newton on a Sun Sparc 10 workstation to obtain all solutions. <p> We are not aware of the results of continuation methods on this problem. Newton is slower on the second application and takes about 6 minutes. The continuation method described in <ref> [35] </ref> requires about 30 seconds on a DEC 5000/200. This method exploits the fact that the Newton polytopes for the last 4 equations are the same. Note that HRB requires about 1630 and 4730 seconds on these examples. <p> Table 11 reports the results for various values of n with an initial interval of [100; 100]. It is interesting to compare those results with the continuation methods presented in <ref> [35] </ref>. [35] reports 27 4 5 6 7 8 9 CPU time 0.60 3.35 22.53 127.65 915.24 8600.28 growth 5.58 6.72 5.66 7.16 9.39 branching 102 500 1778 7527 38638 244263 na-ee 2689 15227 122662 606805 4413150 36325819 na-tot 3667 18523 136717 671437 4779586 38972835 fe-te 3288 15024 80110 429396 2825368 <p> Table 11 reports the results for various values of n with an initial interval of [100; 100]. It is interesting to compare those results with the continuation methods presented in <ref> [35] </ref>. [35] reports 27 4 5 6 7 8 9 CPU time 0.60 3.35 22.53 127.65 915.24 8600.28 growth 5.58 6.72 5.66 7.16 9.39 branching 102 500 1778 7527 38638 244263 na-ee 2689 15227 122662 606805 4413150 36325819 na-tot 3667 18523 136717 671437 4779586 38972835 fe-te 3288 15024 80110 429396 2825368 22672208 <p> Table 13 describes the results of Newton on for the initial intervals [1; 1] and [10 8 ; 10 8 ]. Newton behaves well on this example, since the continuation method of <ref> [35] </ref> takes about 57 seconds. Note once again that a substantial increase in the size of the initial intervals only induces a slowdown of about 2.5 for Newton. <p> The results are depicted in Table 14 for an initial interval [0; 10 8 ]. They indicate that Newton is particularly effective on this problem, since it takes about 6 seconds and proves the existence of a solution in the final intervals. Note that the continuation method of <ref> [35] </ref> takes about 56 seconds on this problem. 29 [10; 10] [10 2 ; 10 2 ] [10 3 ; 10 3 ] [10 4 ; 10 4 ] CPU time 0.91 11.69 172.71 2007.51 growth 12.84 14.77 11.62 branching 52 663 9632 115377 na-ee 4290 57224 645951 6541038 na-tot 5100 <p> The application is from neurophysiology <ref> [35] </ref> and consists of the following system of equations: 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : 1 + x 2 x 2 4 = 1 3 + x 6 x 3 x 5 x 3 2 = c 2 3 + x 6 x 2 x 5 x 2 <p> The continuation method of <ref> [35] </ref> solves this problem in about 6 seconds. The results of Newton are depicted in Table 15 for various initial intervals. Newton is fast when the initial intervals are small (i.e., [10; 10]).
References-found: 35

