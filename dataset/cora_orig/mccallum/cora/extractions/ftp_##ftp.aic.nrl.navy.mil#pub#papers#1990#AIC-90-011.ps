URL: ftp://ftp.aic.nrl.navy.mil/pub/papers/1990/AIC-90-011.ps
Refering-URL: http://www.aic.nrl.navy.mil/~schultz/papers.html
Root-URL: 
Email: (SCHULTZ@AIC.NRL.NAVY.MIL)  (RAMSEY@AIC.NRL.NAVY.MIL)  (GREF@AIC.NRL.NAVY.MIL)  
Phone: (202) 767-2684  
Title: Simulation-Assisted Learning by Competition: Effects of Noise Differences Between Training Model and Target Environment  
Author: ALAN C. SCHULTZ CONNIE LOGGIA RAMSEY JOHN J. GREFENSTETTE 
Address: (Code 5514),  DC 20375-5000, U.S.A.  
Affiliation: Navy Center for Applied Research in Artificial Intelligence  Naval Research Laboratory, Washington,  
Abstract: The problem of learning decision rules for sequential tasks is addressed, focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile. The learning method relies on the notion of competition and employs genetic algorithms to search the space of decision policies. Experiments are presented that address issues arising from differences between the simulation model on which learning occurs and the target environment on which the decision rules are ultimately tested. Specifically, either the model or the target environment may contain noise. These experiments examine the effect of learning tactical plans without noise and then testing the plans in a noisy environment, and the effect of learning plans in a noisy simulator and then testing the plans in a noise-free environment. Empirical results show that, while best result are obtained when the training model closely matches the target environment, using a training environment that is more noisy than the target environment is better than using using a training environment that has less noise than the target environment. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., R. S. Sutton and C. J. C. H. </author> <title> Watkins (1989). Learning and sequential decision making. </title> <type> COINS Technical Report, </type> <institution> University of Massachusetts, Amherst. </institution>
Reference-contexts: In fact, the paradigm is quite broad since it includes any problem solving task by defining the payoff to be positive for any goal state and null for non-goal states <ref> (Barto, Sutton & Watkins, 1989) </ref>. ____________________ _________ 1 If payoff is accumulated over an infinite period, the total payoff is usually defined to be a (finite) time-weighted sum (Barto et. al, 1989). The experiments described here reflect two important methodological assumptions: 1. <p> it includes any problem solving task by defining the payoff to be positive for any goal state and null for non-goal states (Barto, Sutton & Watkins, 1989). ____________________ _________ 1 If payoff is accumulated over an infinite period, the total payoff is usually defined to be a (finite) time-weighted sum <ref> (Barto et. al, 1989) </ref>. The experiments described here reflect two important methodological assumptions: 1. Our learning system is designed to continue learning indefinitely. 2.
Reference: <author> Booker, L. B. </author> <year> (1982). </year> <title> Intelligent behavior as as adaptation to the task environment. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer and Communications Sciences, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference: <author> Buchanan, B. G. J. Sullivan, T. P. Cheng and S. H. </author> <month> Clearwater </month> <year> (1988). </year> <title> Simulation-assisted inductive learning. </title> <booktitle> Proceedings Seventh National Conference on Artifi cial Intelligence. </booktitle> <pages> (pp. 552-557). </pages>
Reference: <author> Erickson, M. D. & J. M. </author> <title> Zytkow (1988). Utilizing experience for improving the tactical manager. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning. </booktitle> <address> Ann Arbor, MI. </address> <pages> (pp. 444-450). </pages>
Reference: <author> Goldberg, D. E. </author> <year> (1983). </year> <title> Computer-aided gas pipeline operation using genetic algorithms and machine learning, </title> <type> Doctoral dissertation, </type> <institution> Department Civil Engineering, University of Michigan, </institution> <address> Ann Arbor. </address>
Reference-contexts: This cycle repeats indefinitely. The objective is to find a set of decision rules that maximizes the expected total payoff. 1 Several sequential decision tasks have been investigated in the machine learning literature, including pole balancing (Selfridge, Sutton & Barto, 1985), gas pipeline control <ref> (Goldberg, 1983) </ref>, and the animat problem (Wilson, 1987). For many interesting problems, including the one considered here, payoff is delayed in the sense that non-null payoff occurs only at the end of an episode that may span several decision steps.
Reference: <author> Gordon, D. G & J. J. </author> <title> Grefenstette (1990). Explanations of empirically derived reactive plans. </title> <booktitle> Proceedings of the Seventh International Conference of Machine Learning, </booktitle> <address> Austin, Texas. </address>
Reference-contexts: Second, it is easier to transfer the knowledge learned to human operators. Third, it makes it possible to combine empirical methods such as genetic algorithms with analytic learning methods that explain the success of the empirically derived rules <ref> (Gordon & Grefenstette, 1990) </ref>. 4 Evaluation of the Method This section presents an empirical study of the performance of SAMUEL on the EM problem with respect to the differences between the simulation model in which the knowledge is learned and the target environment in which the learned knowledge will be used.
Reference: <author> Grefenstette, J. J. </author> <year> (1988). </year> <title> Credit assignment in rule discovery system based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 3(2/3), </volume> <pages> (pp. 225-245). </pages>
Reference-contexts: In addition to matching, CPS implements conflict resolution as a competition among rules based on rule strength and performs credit assignment based on payoff <ref> (Grefenstette, 1988) </ref>. The learning module uses a genetic ____________________ _________ 2 The current statement of the problem assumes a two-dimensional world. <p> The strength is an estimate of the rule's utility and is used for conflict resolution <ref> (Grefenstette, 1988) </ref>. The use of a high level language for rules offers several advantages over low level binary pattern languages typically adopted in genetic learning systems (Smith, 1980; Goldberg, 1983). First, it makes it easier to incorporate existing knowledge, whether acquired from experts or by symbolic learning programs.
Reference: <editor> Grefenstette, J. J. </editor> <booktitle> (1989). Incremental learning of control strategies with genetic algorithms Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher> <pages> (pp. 340-344). </pages>
Reference-contexts: The following sections present one approach to addressing these challenges. 3 SAMUEL on EM SAMUEL 4 is a system designed to explore competition-based learning for sequential decision tasks <ref> (Grefenstette, 1989) </ref>. SAMUEL consists of three major components: a problem specific module, a performance module, and a learning module. The problem specific module consists of the task environment simulation, or world model (in this case, the EM model), and its interfaces.
Reference: <author> Grefenstette, J. J., Connie Loggia Ramsey, and Alan C. </author> <title> Schultz (1990). Learning sequential decision rules using simulation models and competition. </title> <note> To appear in Machine Learning. </note>
Reference-contexts: Second, it is easier to transfer the knowledge learned to human operators. Third, it makes it possible to combine empirical methods such as genetic algorithms with analytic learning methods that explain the success of the empirically derived rules <ref> (Gordon & Grefenstette, 1990) </ref>. 4 Evaluation of the Method This section presents an empirical study of the performance of SAMUEL on the EM problem with respect to the differences between the simulation model in which the knowledge is learned and the target environment in which the learned knowledge will be used. <p> Preliminary results support the general conclusion reported here that it is far less risky to have a training model with overly general initial conditions than to have one with overly restricted initial conditions <ref> (Grefenstette et. al, 1990) </ref>. Current efforts are also aimed at augmenting the task environment to test SAMUEL's ability to learn tactical plans for more realistic scenarios. Multiple incoming threats will be considered, as well as multiple control variables (e.g., accelerations, directions, weapons, etc.).
Reference: <author> Holland J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. In R.S. </title>
Reference-contexts: More detailed descriptions of SAMUEL appear in (Grefenstette, 1989; Grefenstette, Ramsey & Schultz, 1990). The design of SAMUEL owes much to Smith's LS-1 system (Smith, 1980), and draws on some ideas from classifier systems <ref> (Holland, 1986) </ref>. In a departure from these earlier genetic learning systems, SAMUEL learns plans consisting of rules expressed in a high level rule language.
Reference: <editor> Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach, </booktitle> <volume> (Vol. </volume> <pages> 2). </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology for inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20(2), </volume> <pages> (pp. 111-161). </pages>
Reference-contexts: If the performance task is classification, and a large number of training examples are available, then ______________________ _______ To appear in the Proceedings of the Seventh International Conference on Machine Learning, June 1990. inductive learning techniques <ref> (Michalski, 1983) </ref> can be used to learn classification rules. If there exists an extensive domain theory and a source of expert behavior, then explanation-based methods may be applied (Mitchell et. al, 1985). For many interesting sequential decision tasks, there exists neither a database of examples nor a reliable domain theory.
Reference: <author> Mitchell, T. M., S. Mahadevan and L. </author> <title> Steinberg (1985). LEAP: A learning apprentice for VLSI design. </title> <booktitle> Proc. Ninth IJCAI, </booktitle> <pages> (pp. 573-580). </pages> <address> Los Angeles: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If there exists an extensive domain theory and a source of expert behavior, then explanation-based methods may be applied <ref> (Mitchell et. al, 1985) </ref>. For many interesting sequential decision tasks, there exists neither a database of examples nor a reliable domain theory.
Reference: <author> Selfridge, O., R. S. Sutton and A. G. </author> <title> Barto (1985). Training and tracking in robotics. </title> <booktitle> Proceedings of the Ninth International Conference on Artificial Intelligence. </booktitle> <address> Los Angeles, CA. </address> <month> August, </month> <year> 1985. </year>
Reference-contexts: This cycle repeats indefinitely. The objective is to find a set of decision rules that maximizes the expected total payoff. 1 Several sequential decision tasks have been investigated in the machine learning literature, including pole balancing <ref> (Selfridge, Sutton & Barto, 1985) </ref>, gas pipeline control (Goldberg, 1983), and the animat problem (Wilson, 1987). For many interesting problems, including the one considered here, payoff is delayed in the sense that non-null payoff occurs only at the end of an episode that may span several decision steps.
Reference: <author> Smith, S. F. </author> <year> (1980). </year> <title> A learning system based on genetic adaptive algorithms, </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, University of Pittsburgh. </institution>
Reference-contexts: As a result of these evaluations, genetic operators, such as crossover and mutation, produce plausible new plans from high performance parents. More detailed descriptions of SAMUEL appear in (Grefenstette, 1989; Grefenstette, Ramsey & Schultz, 1990). The design of SAMUEL owes much to Smith's LS-1 system <ref> (Smith, 1980) </ref>, and draws on some ideas from classifier systems (Holland, 1986). In a departure from these earlier genetic learning systems, SAMUEL learns plans consisting of rules expressed in a high level rule language.
Reference: <author> Wilson, S. W. </author> <year> (1985). </year> <title> Knowledge growth in an artificial animal. </title> <booktitle> Proceedings of the International Conference Genetic Algorithms and Their Applications (pp. </booktitle> <pages> 16-23). </pages> <address> Pittsburgh, PA. </address>
Reference: <author> Wilson, S. W. </author> <year> (1987). </year> <title> Classifier systems and the animat problem. </title> <journal> Machine Learning, </journal> <volume> 2(3), </volume> <pages> (pp. 199-228). </pages>
Reference-contexts: The objective is to find a set of decision rules that maximizes the expected total payoff. 1 Several sequential decision tasks have been investigated in the machine learning literature, including pole balancing (Selfridge, Sutton & Barto, 1985), gas pipeline control (Goldberg, 1983), and the animat problem <ref> (Wilson, 1987) </ref>. For many interesting problems, including the one considered here, payoff is delayed in the sense that non-null payoff occurs only at the end of an episode that may span several decision steps.
References-found: 17

