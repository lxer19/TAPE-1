URL: http://gn.www.media.mit.edu/groups/gn/publications/wecc_final.ps
Refering-URL: http://gn.www.media.mit.edu/groups/gn/publications.html
Root-URL: http://www.media.mit.edu
Email: yanhao-@media.mit.edu  
Phone: +1 617 253 4899  
Title: An Architecture for Embodied Conversational Characters  
Author: J. Cassell, T. Bickmore, M. Billinghurst, L. Campbell, K. Chang, H. Vilhjlmsson, H. Yan justine, bickmore, markb, elwin, tetrion, hannes, 
Address: E15-315 20 Ames St, Cambridge, Massachusetts  
Affiliation: Gesture and Narrative Language Group MIT Media Laboratory  
Abstract: In this paper we describe the computational and architectural requirements for systems which support real-time multimodal interaction with an embodied conversational character. We argue that the three primary design drivers are real-time multithreaded entrainment, processing of both interactional and propositional information, and an approach based on a functional understanding of human faceto-face conversation. We then present an architecture which meets these requirements and an initial conversational character that we have developed who is capable of increasingly sophisticated multimodal input and output in a limited application domain. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Azarbayejani, A., Wren, C., Pentland A. </author> <title> Real-time 3-D tracking of the human body. </title> <booktitle> In Proceedings of IMAGE'COM 96, </booktitle> <address> Bordeaux, France, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The implementation of the multimodal conversational model and the modules in the Rea architecture are discussed in the next sections. Input Sensors The input manager currently receives three types of input: Gesture Input: STIVE vision software <ref> (Azarbayejani, Wren and Pentland 1996) </ref> uses two video cameras to track flesh color and produce 3D position and NotPresent Present UserTurn ReaTurn Conclude Interrupt orientation of the head and hands at 10 to 15 updates per second.
Reference: <author> Ball, G., Ling, D., Kurlander, D., Miller, D., Pugh, D., Skelly, T., Stankosky, A., Thiel, D., Van Dantzich, M. and T. Wax. </author> <title> Lifelike computer characters: the persona project at Microsoft Research. In Software Agents, </title> <editor> J. M. Bradshaw (Ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference-contexts: Other researchers have built embodied multimodal interfaces that add dialogue and discourse knowledge to produce more natural conversational characters. For example, Peedy the parrot is an embodied agent that allows users to verbally command it to play different music tracks <ref> (Ball et al. 1997) </ref>. Peedy integrates a simple conversational dialogue manager with spoken language input, reactive 3D animation, and recorded speech output.
Reference: <author> Beskow, J. and McGlashan, S. </author> <title> Olga - A Conversational Agent with Gestures, </title> <booktitle> In Proceedings of the IJCAI'97 workshop on Animated Interface Agents - Making them Intelligent, </booktitle> <address> (Nagoya, Japan, August 1997), </address> <publisher> Morgan-Kaufmann Publishers, </publisher> <address> San Francisco, </address> <year> 1997. </year>
Reference-contexts: Another example is Olga, an embodied humanoid agent that allows the user to employ speech, keyboard and mouse commands to engage in a conversation about microwave ovens <ref> (Beskow and McGlashan 1997) </ref>. Olga has a distributed clientserver architecture with separate modules for language processing, interaction management, direct manipulation interface output animation, all communicating through a central server. Olga is event driven, and so only responds to user input and is unable to initiate output on its own.
Reference: <editor> Bolt, R.A. Putthat-there: </editor> <title> voice and gesture at the graphics interface. </title> <journal> Computer Graphics, </journal> <volume> 14(3), </volume> <year> 1980, </year> <pages> 262-270. </pages>
Reference-contexts: Related Work Embodied conversational agents are a specific type of multimodal interface, so in presenting our architecture we must first review other conversational systems and multimodal interfaces in general. One of the first multimodal systems was PutThat-There, developed by Bolt, Schmandt and their colleagues <ref> (Bolt 1980) </ref>. PutThat-There used speech recognition and a six-degree-of-freedom space sensing device to gather user input and allow the user to manipulate a wallsized information display. PutThat-There used a simple architecture that combined speech and deictic gesture input into a single command that was then resolved by the system.
Reference: <editor> Bolt, R.A. and Herranz, E. </editor> <booktitle> Two-handed gesture in multimodal natural dialog. In Proceedings of UIST `92, Fifth Annual Symposium on User Interface Software and Technology, </booktitle> <address> (Monterey, CA, </address> <month> November </month> <year> 1992). </year> <note> ACM Press, </note> <year> 1992, </year> <pages> 7-14. </pages>
Reference-contexts: Wahlster used a similar method, also depending on the linguistic input to guide the interpretation of the other modalities (Wahlster et al. 1991). Bolt and Herranz describe a system that allowed a user to manipulate graphics with two-handed semi-iconic gesture <ref> (Bolt and Herranz 1992) </ref>. Using a cutoff point and time stamping, motions could be selected that related to the intended movement mentioned in speech.
Reference: <author> Brooks, R.A. </author> <title> A Robust Layered Control System for a Mobile Robot. </title> <journal> IEEE Journal of Robotics and Automation. </journal> <volume> 2 (1), </volume> <year> 1986, </year> <pages> 14-23. </pages>
Reference-contexts: Nagao and Takeuchi (Nagao and Takeuchi 1994) suggest a different approach. Their conversational agent is based on the subsumption architecture created by Rodney Brooks <ref> (Brooks 1986) </ref>. In this case the agent is based on a horizontal decomposition of task-achieving behavior modules. The modules each compete with one another to see which behavior is active at a particular moment.
Reference: <author> Cassell, J., Pelachaud, C., Badler, N.I., Steedman, M., Achorn, B., Beckett, T., Douville, B., Prevost, S. and Stone, M. </author> <title> Animated conversation: rule-based generation of facial display, gesture and spoken intonation for multiple conversational agents. </title> <booktitle> Computer Graphics (SIGGRAPH 94 Proceedings), 1994, </booktitle> <volume> 28(4): </volume> <pages> 413-420. </pages>
Reference-contexts: The use of conversational functions such as turn taking, feedback, and repair mechanisms. A performance model that allows negotiation of the conversational process, and contributions of new propositions to the discourse. Our current work grows out of experience developing two prior systems"Animated Conversation" <ref> (Cassell et al. 1994) </ref> and Ymir (Thrisson 1996). Animated Conversation was the first system to automatically produce context-appropriate gestures, facial movements and intonational patterns for animated agents based on deep semantic representations of information, but did not provide for real-time multimodal interaction with a user. <p> However, Gandalf had limited ability to recognize and generate propositional information, such as providing correct intonation for speech emphasis on speech output, or a gesture cooccurring with speech. In contrast, Animated Conversation <ref> (Cassell et al. 1994) </ref> was a system that automatically generated context-appropriate gestures, facial movements and intonational patterns. In this case the domain was conversation between two artificial agents and the emphasis was on the production of nonverbal propositional behaviors that emphasized and reinforced the content of speech. <p> For the moment, Reas responses are generated from an Eliza-like engine (Weizenbaum 1966), but efforts are currently underway to implement an incremental natural language generation engine, along the lines of <ref> (Cassell 1994) </ref>. In order to carry on natural conversation of this sort, Rea uses a conversational model that supports multimodal input and output as constituents of conversational functions. That is, input and output is interpreted and generated based on the discourse functions it serves.
Reference: <author> Cassell, J. and Thrisson, K. </author> <title> The Power of a Nod and a Glance: Envelope vs. Emotional Feedback in Animated Conversational Agents. </title> <journal> Journal of Applied Artificial Intelligence, </journal> <note> in press. CLIPS Reference Manual Version 6.0. Technical Report, </note> <institution> Number JSC-25012, Software Technology Branch, Lyndon B. Johnson Space Center, Houston, TX, </institution> <year> 1994. </year>
Reference-contexts: The use of conversational functions such as turn taking, feedback, and repair mechanisms. A performance model that allows negotiation of the conversational process, and contributions of new propositions to the discourse. Our current work grows out of experience developing two prior systems"Animated Conversation" <ref> (Cassell et al. 1994) </ref> and Ymir (Thrisson 1996). Animated Conversation was the first system to automatically produce context-appropriate gestures, facial movements and intonational patterns for animated agents based on deep semantic representations of information, but did not provide for real-time multimodal interaction with a user. <p> However, Gandalf had limited ability to recognize and generate propositional information, such as providing correct intonation for speech emphasis on speech output, or a gesture cooccurring with speech. In contrast, Animated Conversation <ref> (Cassell et al. 1994) </ref> was a system that automatically generated context-appropriate gestures, facial movements and intonational patterns. In this case the domain was conversation between two artificial agents and the emphasis was on the production of nonverbal propositional behaviors that emphasized and reinforced the content of speech. <p> For the moment, Reas responses are generated from an Eliza-like engine (Weizenbaum 1966), but efforts are currently underway to implement an incremental natural language generation engine, along the lines of <ref> (Cassell 1994) </ref>. In order to carry on natural conversation of this sort, Rea uses a conversational model that supports multimodal input and output as constituents of conversational functions. That is, input and output is interpreted and generated based on the discourse functions it serves.
Reference: <author> Finin, T., Fritzson, R. </author> <title> KQML as an Agent Communication Language. </title> <booktitle> In The Proceedings of the Third International Conference on Information and Knowledge Management (CIKM'94), </booktitle> <publisher> ACM Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: In our implementation of this architecture, the modules communicate with each other using KQML <ref> (Finin and Fritzson 1994) </ref>, a speech-act based inter-agent communication protocol, which serves to make the system very modular and extensible. Detailed characteristics of the modules in the architecture are described next.
Reference: <author> Goodwin, C. </author> <title> Conversational Organization: Interaction Between Speakers and Hearers. </title> <address> New York, NY: </address> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: Conversational functions, such as turn-taking and feedback, rely on integration of information from all of these channels together. When we attempt to construct human conversation (Circles indicate gaze moving towards other, lines indicate fixation on other, squares are withdrawal of gaze from other, question mark shows rising intonation) (from <ref> (Goodwin 1981) </ref>, adapted from (Thrisson 1996)) D d 150 NOD NOD B GAZE SPEECH NODS GAZE SPEECH NODS NOD t 400 1.4 app (h)are (h)nt Yeah NOD NODNOD embodied conversational characters which can participate in this kind of interaction, we find that these features have significant ramifications on the design of
Reference: <author> Johnston, M., Cohen, P. R., McGee, D., Oviatt, S. L., Pittman, J. A. and Smith, I. </author> <title> Unification-based multimodal integration. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Madrid, Spain, </address> <year> 1997. </year>
Reference-contexts: The interface only responds to complete, well-formed input and there is no attempt to use nonverbal behavior as interactional information to control the pace of the user-computer interaction. These limitations were partially overcome by Johnston et al. <ref> (Johnston et al. 1997) </ref>, who described an approach to understanding of user input based on unification with strongly typed multimodal grammars. In his pen and speech interface, either gesture or voice could be used to produce input and either could drive the recognition process.
Reference: <author> Koons, D.B. Sparrell, C.J. and Thrisson, K.R. </author> <title> Integrating simultaneous input from speech, gaze and hand gestures. In Intelligent MultiMedia Interfaces M.T. </title> <editor> Maybury (Ed.), </editor> <publisher> AAAI Press/MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: When these words were encountered, the system analyzed the users pointing gestures to resolve diectic references. Koons continued this work by allowing users to maneuver objects around a two-dimensional map using spoken commands, deictic hand gestures, and eye gaze <ref> (Koons et al. 1993) </ref>. In his system, nested frames were employed to gather and combine information from the different modalities. As in PutThat-There, speech drove the analysis of gesture: if information was missing from speech, the system would search for the missing information in the gestures and/or gaze.
Reference: <author> Nagao, K. and Takeuchi, A. </author> <title> Social interaction: multimodal conversation with social agents. </title> <booktitle> Proceedings of the 12th National Conference on Artificial Intelligence (AAAI-94), </booktitle> <address> (Seattle, WA, </address> <month> August </month> <year> 1994), </year> <note> AAAI Press/MIT Press, </note> <year> 1994, </year> <journal> vol. </journal> <volume> 1, </volume> <pages> 22-28. </pages>
Reference-contexts: In addition, Olga does not support nonspeech audio or computer vision as input modalities. Both Olga and Peedy use a linear architecture in which data flows from user input to agent output, passing through all the internal modules in between. Nagao and Takeuchi <ref> (Nagao and Takeuchi 1994) </ref> suggest a different approach. Their conversational agent is based on the subsumption architecture created by Rodney Brooks (Brooks 1986). In this case the agent is based on a horizontal decomposition of task-achieving behavior modules.
Reference: <author> Reeves, B. and Nass, C. </author> <title> The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places. </title> <publisher> Cambridge University Press, </publisher> <year> 1996. </year>
Reference-contexts: The Social Nature of the Interaction. Whether or not computers look human, people attribute to them humanlike properties such as friendliness, or cooperativeness <ref> (Reeves and Nass 1996) </ref>. An embodied conversational interface can take advantage of this and prompt the user to naturally engage the computer in humanlike conversation. If the interface is well-designed to reply to such conversation, the interaction may be improved.
Reference: <author> Sparrell, C. J. </author> <title> Coverbal Iconic Gestures in Human-Computer Interaction. </title> <type> S.M. Thesis, </type> <institution> MIT Media Arts and Sciences Section, </institution> <year> 1993. </year> <title> Specification for a Standard VRML HumanoidVersion 1.0. http://ece.uwaterloo.ca/~h-anim/spec.html Steedman, M. Structure and intonation. </title> <booktitle> Language, 1991, </booktitle> <volume> 67(2), </volume> <pages> 190-296. </pages>
Reference: <author> Stone, M. </author> <title> Modality in Dialogue: Planning, Pragmatics, and Computation. </title> <type> PhD Thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1998. </year>
Reference-contexts: High on our list is the implementation of a response planner which can synthesize natural language utterances and gestures based on conversational goals and context. We are in the process of integrating the SPUD system <ref> (Stone 1998) </ref> to perform this function. We are also currently exploring the implications of mixed-initiative, multi-sentential dialog on the architecture.
Reference: <author> Thrisson, K. R. </author> <title> Communicative Humanoids: A Computational Model of Psychosocial Dialogue Skills. </title> <type> PhD Thesis, </type> <institution> MIT Media Laboratory, </institution> <year> 1996. </year>
Reference-contexts: The use of conversational functions such as turn taking, feedback, and repair mechanisms. A performance model that allows negotiation of the conversational process, and contributions of new propositions to the discourse. Our current work grows out of experience developing two prior systems"Animated Conversation" (Cassell et al. 1994) and Ymir <ref> (Thrisson 1996) </ref>. Animated Conversation was the first system to automatically produce context-appropriate gestures, facial movements and intonational patterns for animated agents based on deep semantic representations of information, but did not provide for real-time multimodal interaction with a user. <p> When we attempt to construct human conversation (Circles indicate gaze moving towards other, lines indicate fixation on other, squares are withdrawal of gaze from other, question mark shows rising intonation) (from (Goodwin 1981), adapted from <ref> (Thrisson 1996) </ref>) D d 150 NOD NOD B GAZE SPEECH NODS GAZE SPEECH NODS NOD t 400 1.4 app (h)are (h)nt Yeah NOD NODNOD embodied conversational characters which can participate in this kind of interaction, we find that these features have significant ramifications on the design of the characters control architecture. <p> The end result is that user input and agent output are decomposed according to task behaviors rather than conversational function. Our current approach derives from our previous work on the Ymir architecture <ref> (Thrisson 1996) </ref>. In this work the main emphasis was the development of a multilayer multimodal architecture that could support fluid faceto-face dialogue between a human and graphical agent. The agent, Gandalf, was capable of discussing a graphical model of the solar system in an educational application.
Reference: <author> Wahlster, W., Andr, E., Graf, W. and Rist, T. </author> <title> Designing illustrated texts. </title> <booktitle> In Proceedings of the 5th EACL (Berlin, </booktitle> <address> Germany, </address> <month> April </month> <year> 1991), 1991, </year> <pages> 8-14. </pages>
Reference-contexts: Time stamps united the actions in the different modalities into a coherent picture. Wahlster used a similar method, also depending on the linguistic input to guide the interpretation of the other modalities <ref> (Wahlster et al. 1991) </ref>. Bolt and Herranz describe a system that allowed a user to manipulate graphics with two-handed semi-iconic gesture (Bolt and Herranz 1992). Using a cutoff point and time stamping, motions could be selected that related to the intended movement mentioned in speech.
Reference: <author> Weizenbaum, J. </author> <title> Eliza -a computer program for the study of natural language communication between man and machine. </title> <journal> Communications of the ACM, 1966, </journal> <volume> 9, </volume> <pages> 26-45. </pages>
Reference-contexts: She is able to initiate conversational repair when she misunderstands what the user says, and can generate combined voice and gestural output. For the moment, Reas responses are generated from an Eliza-like engine <ref> (Weizenbaum 1966) </ref>, but efforts are currently underway to implement an incremental natural language generation engine, along the lines of (Cassell 1994). In order to carry on natural conversation of this sort, Rea uses a conversational model that supports multimodal input and output as constituents of conversational functions.
References-found: 19

