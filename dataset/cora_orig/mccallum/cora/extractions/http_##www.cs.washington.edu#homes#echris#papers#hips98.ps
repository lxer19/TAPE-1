URL: http://www.cs.washington.edu/homes/echris/papers/hips98.ps
Refering-URL: http://www.cs.washington.edu/homes/echris/
Root-URL: 
Email: zpl-info@cs.washington.edu lin@cs.utexas.edu  
Title: on High-Level Parallel Programming Models and Supportive Environments (HIPS '98) ZPL's WYSIWYG Performance Model  
Author: Bradford L. Chamberlain Sung-Eun Choi E Christopher Lewis Calvin Lin Lawrence Snyder W. Derrick Weathersby 
Address: Box 352350 Taylor Hall 2.124 Seattle, WA 98195-2350 Austin, TX 78712-1188  
Affiliation: Dept. of Computer Science and Eng. Department of Computer Sciences University of Washington The University of Texas at Austin  
Note: To appear in IEEE Workshop  
Abstract: ZPL is a parallel array language designed for high performance scientific and engineering computations. Unlike other parallel languages, ZPL is founded on a machine model (the CTA) that accurately abstracts contemporary MIMD parallel computers. This makes it possible to correlate ZPL programs with machine behavior. As a result, programmers can reason about how code will perform on a typical parallel machine and thereby make informed decisions between alternative programming solutions. This paper describes ZPL's performance model and its syntactic cues for conveying operation cost. The what-you-see-is-what-you-get (WYSIWYG) nature of ZPL operations is demonstrated on the IBM SP-2, Intel Paragon, SGI Power Challenge, and Cray T3E. Additionally, the model is used to evaluate two algorithms for matrix multiplication. Experiments show that the performance model correctly predicts the faster solution on all four platforms for a range of problem sizes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. S. Adve, J.-C. Wang, J. Mellor-Crummey, D. A. Reed, M. Anderson, and K. Kennedy. </author> <title> An integrated compilation and performance analysis environment for data parallel programs. </title> <booktitle> In Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: By defining a performance model to which all HPF compilers must adhere, this problem could have been alleviated. Most compilers compensate for HPF's lack of a performance model by providing tools that give source-level feedback about the compilation process and/or program execution. The dPablo toolkit <ref> [1] </ref> To appear in IEEE Workshop on High-Level Parallel Programming Models and Supportive Environments (HIPS '98) is one such example. The problem with this approach is that such tools are tightly coupled to a particular compiler's compilation model, and therefore do not aid in the creation of portably performing programs.
Reference: [2] <author> G. E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 39(3):8597, </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: However, coding at this per-processor level is tedious and error-prone, motivating the need for higher-level parallel programming languages. NESL is an example of a higher-level parallel language that includes a well-defined performance model <ref> [2] </ref>. It uses a work/depth scheme to calculate asymptotic bounds for the execution time of NESL programs on parallel computers.
Reference: [3] <author> L. F. Cannon. </author> <title> A Cellular Computer to Implement the Kalman Filter Algorithm. </title> <type> PhD thesis, </type> <institution> Montana State University, </institution> <year> 1969. </year>
Reference-contexts: In the next section we use this identical technique to evaluate matrix multiplication algorithms. 5.2. Matrix multiplication Although analyzing the performance of individual ZPL statements is instructive, the real test of the WYSIWYG performance model is in evaluating whole algorithms. In matrix-matrix multiplication: SUMMA [17] and Cannon's Algorithm <ref> [3] </ref>. SUMMA is considered to be the most scalable of portable parallel matrix multiplication algorithms. It iteratively floods a column of matrix A and a row of matrix B, accumulating their product in C.
Reference: [4] <author> M. D. Dikaiakos, C. Lin, D. Manoussaki, and D. E. Wood-ward. </author> <title> The portable parallel implementation of two novel mathematical biology algorithms in ZPL. </title> <booktitle> In Ninth International Conference on Supercomputing, </booktitle> <pages> pages 365374, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing [10, 9]. Applications from a variety of disciplines have been written using ZPL <ref> [4, 7, 14] </ref>, and the language was released for widespread use in July, 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge/Origin, clusters of workstations using PVM and MPI, and sequential workstations.
Reference: [5] <author> R. Friedman, J. Levesque, and G. Wagenbreth. </author> <title> Fortran Par-allelization Handbook, </title> <note> Preliminary Edition. Applied Parallel Research, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: One of the biggest causes of ambiguity in the performance of HPF programs is the fact that communication is completely hidden from the user, making it difficult to evaluate different implementation options <ref> [5] </ref>. As an example, Ngo compares matrix multiplication algorithms written in HPF, demonstrating that there is neither any source-level indication of how they will perform, nor any consistency in the relative performance of the algorithms [11].
Reference: [6] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification Version 2.0. </title> <month> January </month> <year> 1997. </year>
Reference-contexts: For example, the cost of in-terprocessor communication is considered negligible in the NESL model and is therefore ignored entirely. The most prevalent parallel language, High Performance Fortran <ref> [6] </ref>, suffers from the complete lack of a performance model. As a result, programmers must re-tune their programs for each compiler and platform that they use, neutralizing any notion of portable performance.
Reference: [7] <author> E. C. Lewis, C. Lin, L. Snyder, and G. Turkiyyah. </author> <title> A portable parallel n-body solver. </title> <editor> In D. Bailey, P. Bjorstad, J. Gilbert, M. Mascagni, R. Schreiber, H. Simon, V. Tor-czon, and L. Watson, editors, </editor> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 331336. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference-contexts: ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing [10, 9]. Applications from a variety of disciplines have been written using ZPL <ref> [4, 7, 14] </ref>, and the language was released for widespread use in July, 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge/Origin, clusters of workstations using PVM and MPI, and sequential workstations.
Reference: [8] <author> C. Lin. </author> <title> ZPL language reference manual. </title> <type> Technical Report 941006, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: In this section, we give a brief introduction to ZPL concepts that are required to understand this paper. More complete presentations of the language are available in the ZPL Programmer's Guide and Reference Manual <ref> [8, 16] </ref>. 3.1. Regions and arrays The region is ZPL's most fundamental concept. Regions are index sets through which a program's parallelism is expressed. In their most basic form, regions are simply dense rectangular sets of indices similar to those used to define arrays in traditional languages.
Reference: [9] <author> C. Lin and L. Snyder. </author> <title> SIMPLE performance results in ZPL. </title> <editor> In K. Pingali, U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 361375. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: ZPL has sequential semantics that allow programs to be written and debugged on sequential workstations and then ported to parallel architectures in a single recompilation. ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing <ref> [10, 9] </ref>. Applications from a variety of disciplines have been written using ZPL [4, 7, 14], and the language was released for widespread use in July, 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge/Origin, clusters of workstations using PVM and MPI, and sequential workstations.
Reference: [10] <author> C. Lin, L. Snyder, R. Anderson, B. Chamberlain, S. Choi, G. Forman, E. Lewis, and W. D. Weathersby. </author> <title> ZPL vs. HPF: A comparison of performance and programming style. </title> <type> Technical Report 951105, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1995. </year> <title> To appear in IEEE Workshop on High-Level Parallel Programming Models and Supportive Environments (HIPS '98) graph shows the execution times on three processor configurations. Each column of graphs repre sents a per-processor problem size, and each row represents a machine. </title>
Reference-contexts: ZPL has sequential semantics that allow programs to be written and debugged on sequential workstations and then ported to parallel architectures in a single recompilation. ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing <ref> [10, 9] </ref>. Applications from a variety of disciplines have been written using ZPL [4, 7, 14], and the language was released for widespread use in July, 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge/Origin, clusters of workstations using PVM and MPI, and sequential workstations.
Reference: [11] <author> T. A. Ngo. </author> <title> The Role of Performance Models in Parallel Pro gramming and Languages. </title> <type> PhD thesis, </type> <institution> University of Wash ington, Department of Computer Science and Engineering, </institution> <year> 1997. </year>
Reference-contexts: Because HPF's directives are optional and because the compiler is free to ignore them, it is difficult for a programmer to reason about an algorithm's performance without detailed knowledge of the compiler. Ngo has shown that this lack of a performance model leads to unpredictable, inconsistent, and poor performance <ref> [11] </ref>. In this paper we describe the performance model of the ZPL parallel array language. We describe the straightforward mapping of ZPL constructs to the Candidate Type Architecture (CTA), a parallel analog of the von Neumann machine model that accurately abstracts contemporary MIMD parallel computers [15]. <p> As an example, Ngo compares matrix multiplication algorithms written in HPF, demonstrating that there is neither any source-level indication of how they will perform, nor any consistency in the relative performance of the algorithms <ref> [11] </ref>. By defining a performance model to which all HPF compilers must adhere, this problem could have been alleviated. Most compilers compensate for HPF's lack of a performance model by providing tools that give source-level feedback about the compilation process and/or program execution. <p> Performing the equivalent experiment in HPF, Ngo demonstrated that not only is it virtually impossible to predict the relative performance of these algorithms by looking at the HPF source, but also that neither algorithm consistently outperforms the other across all compilers <ref> [11] </ref>. ZPL's WYSI-WYG performance model makes both source-level evaluation and portable performance a reality.
Reference: [12] <author> T. A. Ngo, L. Snyder, and B. L. Chamberlain. </author> <title> Portable per formance of data parallel languages. </title> <booktitle> In SC97: High Perfor mance Networking and Computing, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: As a result, programmers must re-tune their programs for each compiler and platform that they use, neutralizing any notion of portable performance. Ngo et al. demonstrate that this lack of a performance model results in erratic execution times when compiling HPF programs using different compilers on the IBM SP-2 <ref> [12] </ref>. One of the biggest causes of ambiguity in the performance of HPF programs is the fact that communication is completely hidden from the user, making it difficult to evaluate different implementation options [5].
Reference: [13] <author> R. W. Numrich and J. L. Steidel. </author> <title> Simple parallel extensions to Fortran 90. </title> <booktitle> In 8th SIAM Conference on Parallel Process ing for Scientific Computing, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: In contrast with HPF's hidden and unspecified communication model, F was developed to make communication explicit and highly visible to the programmer using a simple and natural syntax extension to Fortran 90 <ref> [13] </ref>. This results in a clearer performance model than HPF, but not without some cost. The user is forced to program at a local per-processor level, thereby forfeiting some of the benefits of higher-level languages, such as sequential semantics and deterministic execution.
Reference: [14] <author> W. Richardson, M. Bailey, and W. H. Sanders. </author> <title> Using ZPL to develop a parallel Chaos router simulator. </title> <booktitle> In 1996 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: ZPL generally outperforms HPF and has proven to be competitive with hand-coded C and message passing [10, 9]. Applications from a variety of disciplines have been written using ZPL <ref> [4, 7, 14] </ref>, and the language was released for widespread use in July, 1997. Supported platforms include the Cray T3D/T3E, Intel Paragon, IBM SP-2, SGI Power Challenge/Origin, clusters of workstations using PVM and MPI, and sequential workstations.
Reference: [15] <author> L. Snyder. </author> <title> Experimental validation of models of parallel computation. </title> <editor> In A. Hofmann and J. van Leeuwen, editors, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Special Volume 1000, </volume> <pages> pages 78100. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: In this paper we describe the performance model of the ZPL parallel array language. We describe the straightforward mapping of ZPL constructs to the Candidate Type Architecture (CTA), a parallel analog of the von Neumann machine model that accurately abstracts contemporary MIMD parallel computers <ref> [15] </ref>. This allows programmers to reason about the behavior of their ZPL programs on parallel machines. In addition, we demonstrate that ZPL's syntax inherently identifies operations that induce communication.
Reference: [16] <author> L. Snyder. </author> <title> The ZPL Programmer's Guide. </title> <note> MIT Press (in pressavailable at publication date at ftp://ftp.cs.washington.edu/pub/orca/docs/zpl guide.ps), </note> <year> 1998. </year>
Reference-contexts: In this section, we give a brief introduction to ZPL concepts that are required to understand this paper. More complete presentations of the language are available in the ZPL Programmer's Guide and Reference Manual <ref> [8, 16] </ref>. 3.1. Regions and arrays The region is ZPL's most fundamental concept. Regions are index sets through which a program's parallelism is expressed. In their most basic form, regions are simply dense rectangular sets of indices similar to those used to define arrays in traditional languages.
Reference: [17] <author> R. van de Geijn and J. Watts. SUMMA: </author> <title> Scalable universal matrix multiplication algorithm. </title> <type> Technical Report TR-95 13, </type> <institution> University of Texas, Austin, Texas, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: In the next section we use this identical technique to evaluate matrix multiplication algorithms. 5.2. Matrix multiplication Although analyzing the performance of individual ZPL statements is instructive, the real test of the WYSIWYG performance model is in evaluating whole algorithms. In matrix-matrix multiplication: SUMMA <ref> [17] </ref> and Cannon's Algorithm [3]. SUMMA is considered to be the most scalable of portable parallel matrix multiplication algorithms. It iteratively floods a column of matrix A and a row of matrix B, accumulating their product in C.
References-found: 17

