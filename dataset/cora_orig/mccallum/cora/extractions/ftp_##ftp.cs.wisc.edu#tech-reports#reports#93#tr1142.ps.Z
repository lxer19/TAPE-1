URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/93/tr1142.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Title: Mechanisms for Cooperative Shared Memory  
Author: David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, Steven K. Reinhardt 
Keyword: Shared-memory multiprocessors, memory systems, cache coherence, directory protocols, and hardware mechanisms.  
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: Appears in: Proceedings of the 20th Annual International Symposium on Computer Architecture,  
Email: wwt@cs.wisc.edu  
Date: May 1993.  
Abstract: This paper explores the complexity of implementing directory protocols by examining their mechanisms| primitive operations on directories, caches, and network interfaces. We compare the following protocols: Dir 1 B, Dir 4 B, Dir 4 NB, Dir n NB [2], Dir 1 SW [9] and an improved version of Dir 1 SW (Dir 1 SW + ). The comparison shows that the mechanisms and mechanism sequencing of Dir 1 SW and Dir 1 SW + are simpler than those for other protocols. We also compare protocol performance by running eight benchmarks on 32 processor systems. Simulations show that Dir 1 SW + 's performance is comparable to more complex directory protocols. The significant disparity in hardware complexity and the small difference in performance argue that Dir 1 SW + may be a more effective use of resources. The small performance difference is attributable to two factors: the low degree of sharing in the benchmarks and Check-In/Check-Out (CICO) directives [9]. fl This work is supported in part by NSF PYI Awards CCR-9157366 and MIPS-8957278, NSF Grant CCR-9101035, Univ. of Wisconsin Graduate School Grant, Wisconsin Alumni Research Foundation Fellowship and donations from A.T.&T. Bell Laboratories and Digital Equipment Corporation. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the Univ. of Wisconsin Graduate School. c fl 1993 IEEE. Abstracting is permitted with credit to the source. Libraries are permitted to photocopy beyond the limit of US copytight law, for private use of patrons, those articles in this volume that carry a code at the bottom of the first page, provided that the per-copy fee indicated in the code is paid through the Copyright Clearance Center, 27 Congress Street, Salem, MA 01970. For other copying, reprint, or republication permission, write to IEEE Copyrights Manager, IEEE Service Center, 445 Hoes Lane, PO Box 1331, Piscataway, NJ 088551. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Ku-biatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This suggests that moving protocol sequencing to software running on a node's main processor, an auxiliary processor (as in the Intel Paragon), or a processor in the network interface may be practical <ref> [1] </ref>. The obvious drawback of this approach is that a processor sequences a protocol slower than a hardware finite state machine. A secondary drawback is that slower directories increase directory contention. The data shows that increasing directory latency from 10 to 100 cycles degrades execution time by 15%.
Reference: [2] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Directory protocols are a technique used to implement cache coherence on large-scale shared-memory parallel computers <ref> [2] </ref>. Directory protocols logically associate a directory entry with each aligned block in main memory. This entry records that the block is idle (no cached copies), one writable copy exists, or one or more read-only copies exist. <p> To facilitate invalidations, each directory entry also contains logical pointers to some or all of the processor (s) that hold copies of the block. Agarwal et al. <ref> [2] </ref> use the notation Dir i B to denote protocols that explicitly record the i processors that share a block and rely on broadcasts to invalidate more than i processors. Dir i NB denotes protocols that avoid broadcast by preventing more than i processors from sharing a block. <p> On the other hand, for an exclusive-exclusive transition, the directory only sends a single invalidation, which greatly simplifies these considerations. The first contribution of this paper is to explore the complexity of Dir 1 B, Dir 4 B, Dir 4 NB, Dir n NB <ref> [2] </ref>, and Dir 1 SW [9] at the mechanism level of abstraction (Section 3). <p> Dir 1 SW's mechanisms can also be used to implement a protocol with higher performance. We call the best extended pro tocol Dir 1 SW + . However, this comparison is an academic exercise if simpler protocols perform poorly. Several papers have examined directory protocol performance. Agarwal et al. <ref> [2] </ref> presented event counts for four-processor VAX traces less than two million instructions long (half-million per processor). Weber and Gupta [17] used five benchmarks, more processors (up to 16), and longer traces (4 million instructions). <p> We compare the following protocols: Dir 1 B, Dir 4 B, Dir 4 NB, Dir n NB <ref> [2] </ref>, Dir 1 SW [9] and an improved version of Dir 1 SW (Dir 1 SW + ). The comparison shows that the mechanisms and mechanism sequencing of Dir 1 SW and Dir 1 SW + are simpler than those for other protocols.
Reference: [3] <author> John K. Bennett, John B. Carter, and Willy Zwanepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 168-176, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: time and hardware for shared-memory machines could be similar to message-passing com puters. * Protocols can adapt to dynamic program behavior since buffering and analyzing recent behavior is practical in software. * Protocols can be statically tailored by compilers, program libraries, or application programs to behave differently for different objects <ref> [3] </ref>.
Reference: [4] <author> David Chaiken, John Kubiatowics, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Dir 4 B and Dir 4 NB were inspired by empirical data suggesting that, in many sharing patterns, the number of readers is lower than four, regardless of the system size [17]. MIT Alewife's <ref> [4] </ref> LimitLESS directory contains four hardware pointers and uses software to record additional pointers. LimitLESS's software can implement Dir 4 B, Dir 4 NB, Dir n NB, and other alternatives. <p> A processor sends a Msg Put message on an explicit check in or a cache replacement of the block. Several state transitions in Figure 1 set a trap bit and trap to a software trap handler running on the directory processor (not the requesting processor), as in MIT LimitLESS <ref> [4] </ref>. The trap bit serializes traps from multiple references to a block. The software trap handler reads directory entries from the hardware and sends explicit messages to other processors to complete the request that trapped and then restarts the program that faulted. <p> This mechanism may, in turn, be implemented as an atomic sequence of primitive hardware operations that read, add one to, and write the counter. Policy and mechanisms can be implemented in either hardware or software. Most directory protocols implement both policy and mechanisms in hardware. However, both LimitLESS <ref> [4] </ref> and Dir 1 SW [9] implement policy with a combination of software and hardware. Previous work has concentrated on developing new protocols, that is, policies. This section focuses, instead, on the mechanisms required to implement these protocols. <p> Shared-memory machines, such as Stanford DASH and the Kendell Square KSR1, use these mechanisms to implement shared memory but hide the underlying mechanisms from the programmer. We believe that future shared-memory systems will expose the underlying message passing, as done in MIT Alewife <ref> [4] </ref>. Some statically-partitionable codes achieve maximum performance through explicit message passing. Agarwal, et al., have demonstrated that other codes achieve better performance with a combination of shared-memory and message-passing than by using one or the other alone [11].
Reference: [5] <author> David Lars Chaiken. </author> <title> Cache Coherence Protocols for Large-Scale Multiprocessors. </title> <type> Technical Report MIT/LCS/TR-489, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: This change may be due to longer traces and different synchronization assumptions. Lenoski et al. [13] presented speedup measurements from the Stanford DASH prototype running three applications. Since prototypes of other directory systems were unavailable, the paper could not compare DASH against alternatives. Chaiken <ref> [5] </ref> compared LimitLESS against Dir 4 NB and Dir n NB, using several applications on 16 and 64 processors with 7 to 30 million references per application.
Reference: [6] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 64-77, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: For example, update protocols could distribute widely-used data (e.g., the vector x in x := Ax + b) and help in synchronization (a barrier wakeup) <ref> [6] </ref>. * Protocols can support higher-level operations such as fetching an entire row of a matrix or a scatter-gather operator. * Collecting information for performance monitor ing is much easier. Regrettably, we leave evaluation of these ideas to future work. Our benchmarks were written for small scale systems.
Reference: [7] <author> Anoop Gupta and Wolf-Dietrich Weber. </author> <title> Cache Invalidation Patterns in Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: A major contribution of their paper is a classification of shared objects (into read-only, migratory, synchronization, mostly-read, and frequently read/written). In addition, for migratory data, they suggested flushing data from a cache. Their second paper <ref> [7] </ref> switched to the MIPS architecture, ran the applications to completion (up to 2-48 million references|instruction counts not given), and extended results to 32 processors. This change may be due to longer traces and different synchronization assumptions.
Reference: [8] <author> David B. Gustavson. </author> <title> The Scalable Coherent Interface and Related Standards Projects. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 10-22, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: We examine Dir n NB, Dir 4 B, Dir 4 NB, Dir 1 B, Dir 1 SW, and Dir 1 SW + . The Stanford DASH project and IEEE Scalable Coherent Interface (SCI) implement Dir n NB <ref> [13, 8] </ref>. DASH uses a bit vector pointing to a maximum of 16 clusters, while SCI uses a linked-list whose head is stored in the directory and other list elements are associated with blocks in a maximum of 64K processor caches.
Reference: [9] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 262-273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Dir 1 B, and our protocols, Dir 1 SW and Dir 1 SW + , record only one writer or reader. The limited state reduces implementation complexity, but can cause many broadcasts. Dir 1 SW <ref> [9] </ref> (reviewed in Section 2) 1 and Dir 1 SW + (introduced in Section 3.6.3) count the readers so they can return the directory to idle when all readers return the block, thereby avoiding an unnecessary broadcast. <p> Programmers or compilers can also produce more desirable sharing patterns by reasoning about the shared-memory communication in a program with the Check-In/Check-Out (CICO) programming model. Furthermore, CICO primitives also serve as memory system directives that improve performance. We review this approach|cooperative shared memory|in Section 2 <ref> [9] </ref>. Many directory protocols are complex and require considerable hardware, which reduces the attractiveness of shared-memory machines. A directory protocol policy describes its response to program events, such as loads and stores, and the interactions among directories and caches on different processors. <p> On the other hand, for an exclusive-exclusive transition, the directory only sends a single invalidation, which greatly simplifies these considerations. The first contribution of this paper is to explore the complexity of Dir 1 B, Dir 4 B, Dir 4 NB, Dir n NB [2], and Dir 1 SW <ref> [9] </ref> at the mechanism level of abstraction (Section 3). The mechanisms and mechanism sequencing of Dir 1 SW are significantly simpler than these other protocols because the shared-to-exclusive transition is not handled by hardware (MIT LimitLESS is more complex that Dir 1 SW, but much simpler than the other protocols). <p> His principal result was that LimitLESS's performance is comparable to Dir n NB and better than Dir 4 NB (even though he assumes read-only data is handled separately). Hill et al. <ref> [9] </ref> measured the performance of Dir 1 SW by recording event counts, but did not compare their results with other protocols. The second contribution of this paper is a comparison of directory protocol performance that extends previous work in three ways (Section 4). <p> After the principal results in Sections 3 and 4, Section 5 discusses the implication of technology trends and directions for future work, while Section 6 draws conclusions. 2 CICO and Dir 1 SW This section reviews cooperative shared memory, CICO and Dir 1 SW, originally presented in Hill et al. <ref> [9] </ref>. The Check-In/Check-Out (CICO) programming performance model allows a programmer both (1) to reason about the communications caused by shared-memory references and (2) to pass performance directives to the memory system. Neither the programming model or the directives are specific to Dir 1 SW. <p> Policy and mechanisms can be implemented in either hardware or software. Most directory protocols implement both policy and mechanisms in hardware. However, both LimitLESS [4] and Dir 1 SW <ref> [9] </ref> implement policy with a combination of software and hardware. Previous work has concentrated on developing new protocols, that is, policies. This section focuses, instead, on the mechanisms required to implement these protocols. <p> We compare the following protocols: Dir 1 B, Dir 4 B, Dir 4 NB, Dir n NB [2], Dir 1 SW <ref> [9] </ref> and an improved version of Dir 1 SW (Dir 1 SW + ). The comparison shows that the mechanisms and mechanism sequencing of Dir 1 SW and Dir 1 SW + are simpler than those for other protocols.
Reference: [10] <author> Raj Jain. </author> <title> The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: -2.02] NPT/CICO 1.94 4.48 [ -1.42, 10.38] RO1/CICO 0.08 0.90 [ -5.01, 6.80] NPT+RO1/CICO 0.00 0.01 [ -5.89, 5.92] Table 4: Analysis of Variance of Dir 1 SW Extensions These numbers were collected from a full factorial experiment using the benchmarks for replication and NPT, RO1, and CICO as factors <ref> [10, Chapter 18] </ref>. Column "Variation Due" lists the percent of performance variation in the 64 runs (8 fl 2 3 ) caused by benchmark, factors, and interactions between factors. The results show that the benchmark, NPT, and CICO are the most important factors.
Reference: [11] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatow-icz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <month> page ?, May </month> <year> 1993. </year>
Reference-contexts: Some statically-partitionable codes achieve maximum performance through explicit message passing. Agarwal, et al., have demonstrated that other codes achieve better performance with a combination of shared-memory and message-passing than by using one or the other alone <ref> [11] </ref>. Consequently, we assume base hardware includes support to explicitly send and receive messages. Messages contain a 4-bit message type and are sent to an explicitly-specified destination node p. The messages are large enough to contain at least one cache block and an address.
Reference: [12] <author> James R. Larus, Satish Chandra, and David A Wood. </author> <month> CICO: </month>
Reference-contexts: Neither the programming model or the directives are specific to Dir 1 SW. Elsewhere, we demonstrate that the annotations can be used to improve program performance by increasing cache reuse and reducing program sharing <ref> [12] </ref>. This paper examines the effect on directory protocol behavior of using CICO annotations as memory system directives. We do not discuss the cooperative prefetch mechanism. <p> Regrettably, we leave evaluation of these ideas to future work. Our benchmarks were written for small scale systems. Running these programs on more than 32 or 64 processors exposes bottlenecks and yields poor speedup. We plan to use the CICO programming model <ref> [12] </ref> to construct programs that manage communication more effectively and use these programs to evaluate these ideas. 6 Conclusions Shared memory offers many advantages, such as a uniform address space and referential transparency, that are difficult to replicate in today's massively-parallel, message-passing computers.
References-found: 12

