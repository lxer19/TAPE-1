URL: http://www.cs.caltech.edu/~heirich/jsuper.ps.Z
Refering-URL: http://www.cs.caltech.edu/~heirich/heirich.html
Root-URL: http://www.cs.caltech.edu
Email: heirich@sgi.com, arvo@cs.caltech.edu  
Title: A Competitive Analysis of Load Balancing Strategies for Parallel Ray Tracing  
Author: ALAN HEIRICH JAMES ARVO Editor: H. Arabnia 
Keyword: load balancing, ray tracing, rendering, graphics, image processing, diffusion.  
Address: 2011 N. Shoreline Blvd., Mountain View, CA 94043  Pasadena, CA 91125  
Affiliation: Silicon Graphics Computer Systems,  Department of Computer Science, California Institute of Technology,  
Note: 1-18 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: This paper examines the effectiveness of load balancing strategies for ray tracing on large parallel computer systems and cluster computers. Popular static load balancing strategies are shown to be inadequate for rendering complex images with contemporary ray tracing algorithms, and for rendering NTSC resolution images on 128 or more computers. Strategies based on image tiling are shown to be ineffective except on very small numbers of computers. A dynamic load balancing strategy, based on a diffusion model, is applied to a parallel Monte Carlo rendering system. The diffusive strategy is shown to remedy the defects of the static strategies. A hybrid strategy that combines static and dynamic approaches produces nearly optimal performance on a variety of images and computer systems. The theoretical results should be relevant to other rendering and image processing applications. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Anderson, T. E., Culler, D. E. & Patterson, D. A. </author> <year> 1995. </year> <title> A case for NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15: </volume> <pages> 54-64. </pages>
Reference: 2. <author> Baskett, F. & Hennessy, J. L. </author> <year> 1993. </year> <title> Microprocessors: from desktops to supercomputers. </title> <journal> Science, </journal> <volume> 261: </volume> <pages> 864-871. </pages>
Reference-contexts: percentage of imbalance that it produces, * (T T min )=T min : (2) It has come to be commonly accepted that "embarrassingly parallel" algorithms can be implemented so that they scale linearly with respect to the number of processors that they run on, up to large numbers of processors <ref> [2] </ref>. In the best cases a speedup of p can be achieved on p processors. In these cases overheads of even a few percent can come to dominate the efficiency of a computation.
Reference: 3. <author> Boden, N. J., Cohen, D., Felderman, R. E., Kulawik, A. E., Seitz, C. L., Seizovic, J. N. & Su, W. </author> <year> 1995. </year> <title> Myrinet: a gigabit per second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15: </volume> <pages> 29-36. </pages>
Reference: 4. <author> Cybenko, G. </author> <year> 1989. </year> <title> Dynamic load balancing for distributed memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 7: </volume> <pages> 279-301. </pages>
Reference-contexts: Diffusion has been discussed as a general solution to the problems of load balancing and mapping in distributed systems and in recent years several authors have proposed or demonstrated diffusive algorithms with these desirable properties <ref> [4, 6] </ref>. When these algorithms are constructed from finite difference approximations to differential equations they are computationally efficient and converge at rates that are independent of problem scale [8, 9]. 4.
Reference: 5. <author> Delaney, H. C. </author> <year> 1988. </year> <title> Ray tracing on the Connection Machine. Proceedings of SIGGRAPH (Atlanta, August), </title> <publisher> ACM Press, </publisher> <pages> pp. 659-664. </pages>
Reference-contexts: The most popular of these techniques, ray tracing and radiosity [7, 11] require massive amounts of floating point calculations, comparable to the requirements of the largest problems in computational science and engineering. It has long been realized that parallel computers can be an effective way to accelerate these computations <ref> [5, 10, 14] </ref>. The advent of commodity based cluster computing has made it possible to render photo-realistic images in reasonable time on modest budgets.
Reference: 6. <author> Dijkstra, E. W. & Scholten, C. S. </author> <year> 1980. </year> <title> Termination detection for diffusing computations. </title> <journal> Information Processing Letters, </journal> <volume> 11: </volume> <pages> 1-14. </pages>
Reference-contexts: Diffusion has been discussed as a general solution to the problems of load balancing and mapping in distributed systems and in recent years several authors have proposed or demonstrated diffusive algorithms with these desirable properties <ref> [4, 6] </ref>. When these algorithms are constructed from finite difference approximations to differential equations they are computationally efficient and converge at rates that are independent of problem scale [8, 9]. 4.
Reference: 7. <author> Hanrahan, P., Saltzman, D. & Aupperle, L. </author> <year> 1991. </year> <title> A rapid hierarchical radiosity algorithm. </title> <journal> Computer Graphics, </journal> <volume> 25: </volume> <pages> 197-206. </pages>
Reference-contexts: 1. Introduction High quality computer generated images can attain the clarity and realism of photographs. Techniques for photo-realistic image synthesis model the transport of light through geometrically complex scenes. The most popular of these techniques, ray tracing and radiosity <ref> [7, 11] </ref> require massive amounts of floating point calculations, comparable to the requirements of the largest problems in computational science and engineering. It has long been realized that parallel computers can be an effective way to accelerate these computations [5, 10, 14].
Reference: 8. <author> Heirich, A. & Taylor, S. </author> <year> 1995. </year> <title> A parabolic load balancing method. </title> <booktitle> Proceedings of the 24th International Conference on Parallel Processing (Milwaukee, August), </booktitle> <publisher> CRC Press, III, </publisher> <pages> pp. 192-202. </pages>
Reference-contexts: The paper concludes by measuring the online performance of a dynamic load balancing scheme based on a diffusion model. Such schemes, which can be implemented as a form of work-stealing, have previously been analyzed and shown to scale without limit <ref> [8, 9] </ref>. In this paper these predictions are tested empirically by measuring the performance of these schemes on substantial test problems. In general the dynamic scheme proved at least as effective as the best of the static schemes and was usually superior. <p> When these algorithms are constructed from finite difference approximations to differential equations they are computationally efficient and converge at rates that are independent of problem scale <ref> [8, 9] </ref>. 4. <p> Diffusion is one of the few load balancing paradigms that is provably scalable. Diffusion algorithms provide scalable solutions to a number of other dynamic problems in placement and partitioning, including the mapping problem <ref> [8, 9] </ref>. We have experimented with diffusive load balancing in the context of Monte Carlo image synthesis. The results were very favorable and appear in table 1. In most instances the diffusion algorithm produced a smaller imbalance than scatter decomposition, as evidenced by shorter elapsed rendering times.
Reference: 9. <author> Heirich, A. </author> <year> 1997. </year> <title> A scalable diffusion algorithm for dynamic mapping and load balancing on networks of arbitrary topology. </title> <note> To appear in The International Journal of Foundations of Computer Science. </note>
Reference-contexts: It considers static image partitioning strategies, both with and without randomization, and compares them to a dynamic scheme based on a diffusion model <ref> [9] </ref>. Using both o*ine simulation, and online performance measurements, the paper demonstrates that static strategies produce unacceptable levels of workload imbalance in rendering complex images. <p> The paper concludes by measuring the online performance of a dynamic load balancing scheme based on a diffusion model. Such schemes, which can be implemented as a form of work-stealing, have previously been analyzed and shown to scale without limit <ref> [8, 9] </ref>. In this paper these predictions are tested empirically by measuring the performance of these schemes on substantial test problems. In general the dynamic scheme proved at least as effective as the best of the static schemes and was usually superior. <p> When these algorithms are constructed from finite difference approximations to differential equations they are computationally efficient and converge at rates that are independent of problem scale <ref> [8, 9] </ref>. 4. <p> Diffusion is one of the few load balancing paradigms that is provably scalable. Diffusion algorithms provide scalable solutions to a number of other dynamic problems in placement and partitioning, including the mapping problem <ref> [8, 9] </ref>. We have experimented with diffusive load balancing in the context of Monte Carlo image synthesis. The results were very favorable and appear in table 1. In most instances the diffusion algorithm produced a smaller imbalance than scatter decomposition, as evidenced by shorter elapsed rendering times.
Reference: 10. <author> Heirich, A. & Arvo, J. </author> <year> 1997. </year> <title> Scalable Monte Carlo image synthesis. </title> <note> To appear in Parallel Computing. </note>
Reference-contexts: The most popular of these techniques, ray tracing and radiosity [7, 11] require massive amounts of floating point calculations, comparable to the requirements of the largest problems in computational science and engineering. It has long been realized that parallel computers can be an effective way to accelerate these computations <ref> [5, 10, 14] </ref>. The advent of commodity based cluster computing has made it possible to render photo-realistic images in reasonable time on modest budgets. <p> To take a single example, a parallel rendering system running on a $50,000 commodity cluster computer (the 3.2 GFlops NASA/Caltech Beowulf system) renders a complex image, which takes close to an hour on a high performance workstation, in under two minutes <ref> [10, 16, 17] </ref>. Further speedups can be obtained by larger clusters and high performance parallel computers. These speedups are most important in interactive settings where a human is in-the-loop waiting for images to be rendered. These settings occur in product design, architectural walkthroughs, and in previewing commercial animation.
Reference: 11. <author> Kajiya, J. T. </author> <year> 1986. </year> <title> The rendering equation. </title> <journal> Computer Graphics, </journal> <volume> 20: </volume> <pages> 143-150. </pages>
Reference-contexts: 1. Introduction High quality computer generated images can attain the clarity and realism of photographs. Techniques for photo-realistic image synthesis model the transport of light through geometrically complex scenes. The most popular of these techniques, ray tracing and radiosity <ref> [7, 11] </ref> require massive amounts of floating point calculations, comparable to the requirements of the largest problems in computational science and engineering. It has long been realized that parallel computers can be an effective way to accelerate these computations [5, 10, 14].
Reference: 12. <author> Karypis, G. & Kumar, V. </author> <year> 1995. </year> <title> Multilevel graph partitioning schemes. </title> <booktitle> Proceedings of the 24th International Conference on Parallel Processing (Milwaukee, August), </booktitle> <publisher> CRC Press, III, </publisher> <pages> pp. 113-122. </pages>
Reference-contexts: Dynamic load balancing strategies work on-line during the course of a calculation to redistribute work as the need arises. Several algorithms based on recursive partitioning have been proposed for static load balancing and some of these have also been applied to dynamic load balancing <ref> [12, 18] </ref>. Unfortunately these approaches are usually expensive because they always repartition an entire calculation. It would be desirable to have a dynamic load balancing strategy that worked locally, required no global communication, and expended computational work only in areas where it was needed.
Reference: 13. <author> Lindgren, B. W. </author> <year> 1962. </year> <title> Statistical Theory. </title> <address> New York: </address> <publisher> Macmillan. </publisher>
Reference-contexts: If the sample population is uniform then it is possible to quantify the probability that the sum of any individual set of n samples is below a given bound y <ref> [13] </ref>. In the general case we have P n X w i y = p Z (yn)= n 1 + 2 2n : (3) In this expression and 2 are the mean and variance of the sample population.
Reference: 14. <author> Priol, T. & Bouatouch, K. </author> <year> 1988. </year> <title> Experimenting with a parallel ray-tracing algorithm on a hypercube machine. </title> <booktitle> Proceedings of Eurographics (Nice, </booktitle> <address> September), </address> <publisher> North-Holland, </publisher> <pages> pp. 243-259. 11 </pages>
Reference-contexts: The most popular of these techniques, ray tracing and radiosity [7, 11] require massive amounts of floating point calculations, comparable to the requirements of the largest problems in computational science and engineering. It has long been realized that parallel computers can be an effective way to accelerate these computations <ref> [5, 10, 14] </ref>. The advent of commodity based cluster computing has made it possible to render photo-realistic images in reasonable time on modest budgets.
Reference: 15. <author> Salmon, J. </author> <year> 1988. </year> <title> A mathematical analysis of the scattered decomposition. </title> <booktitle> Proceedings of the Third Caltech Conference on Hypercube Computers and Applications (Pasadena, January), </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 239-240. </pages>
Reference-contexts: A more effective strategy assigns pixels pseudo-randomly to computers in an attempt to obtain a balanced workload. The most straightforward version of this strategy assigns pixels in an alternating sequence so that F (i) = (i mod p). This well-known strategy has been dubbed the "scatter decomposition" <ref> [15] </ref>. The effectiveness of various static load balancing strategies can be predicted using data obtained from program traces generated in the course of rendering complex images. Figure 1 shows black and white versions of three images that were used for the study reported here.
Reference: 16. <author> Sterling, T. L. </author> <year> 1996. </year> <title> The scientific workstation of the future may be a pile-of-pcs. </title> <journal> Communications of the ACM, </journal> <volume> 39: </volume> <pages> 11-12. </pages>
Reference-contexts: To take a single example, a parallel rendering system running on a $50,000 commodity cluster computer (the 3.2 GFlops NASA/Caltech Beowulf system) renders a complex image, which takes close to an hour on a high performance workstation, in under two minutes <ref> [10, 16, 17] </ref>. Further speedups can be obtained by larger clusters and high performance parallel computers. These speedups are most important in interactive settings where a human is in-the-loop waiting for images to be rendered. These settings occur in product design, architectural walkthroughs, and in previewing commercial animation.
Reference: 17. <author> Taubes, G. </author> <year> 1996. </year> <title> Do-it-yourself supercomputers. </title> <journal> Science, </journal> <volume> 274: </volume> <pages> 1840. </pages>
Reference-contexts: To take a single example, a parallel rendering system running on a $50,000 commodity cluster computer (the 3.2 GFlops NASA/Caltech Beowulf system) renders a complex image, which takes close to an hour on a high performance workstation, in under two minutes <ref> [10, 16, 17] </ref>. Further speedups can be obtained by larger clusters and high performance parallel computers. These speedups are most important in interactive settings where a human is in-the-loop waiting for images to be rendered. These settings occur in product design, architectural walkthroughs, and in previewing commercial animation.

References-found: 17

