URL: ftp://ftp.ai.mit.edu/pub/cbcl/nnsp97-svm.ps
Refering-URL: http://www.ai.mit.edu/people/girosi/home-page/svm.html
Root-URL: 
Title: An Improved Training Algorithm for Support Vector Machines  
Author: Edgar Osuna Robert Freund Federico Girosi 
Affiliation: O.R. Center  Cambridge,  
Address: Island, FL,  MA, 02139 Cambridge, MA, 02139 Cambridge, MA, 02139  
Note: (to appear in the Proc. of IEEE NNSP'97, Amelia  110,000 data points that generates 100,000 support vectors.  
Pubnum: C.B.C.L.  C.B.C.L. MIT E25-201 MIT E40-149A MIT E25-201  
Email: eosuna@ai.mit.edu rfreund@mit.edu girosi@ai.mit.edu  
Phone: tel: (617) 252 1723 tel: (617) 253 8997 tel: (617) 253 0548  
Date: 24-26 Sep., 1997)  
Abstract: We investigate the problem of training a Support Vector Machine (SVM) [1, 2, 7] on a very large date base (e.g. 50,000 data points) in the case in which the number of support vectors is also very large (e.g. 40,000). Training a SVM is equivalent to solving a linearly constrained quadratic programming (QP) problem in a number of variables equal to the number of data points. This optimization problem is known to be challenging when the number of data points exceeds few thousands. In previous work, done by us as well as by other researchers, the strategy used to solve the large scale QP problem takes advantage of the fact that the expected number of support vectors is small (&lt; 3; 000). Therefore, the existing algorithms cannot deal with more than a few thousand support vectors. In this paper we present a decomposition algorithm that is guaranteed to solve the QP problem and that does not make assumptions on the expected number of support vectors. In order to present the feasibility of our approach we consider a foreign exchange rate time series data base with 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B.E. Boser, I.M. Guyon, and V.N. Vapnik. </author> <title> A training algorithm for op timal margin classifier. </title> <booktitle> In Proc. 5th ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-152, </pages> <address> Pittsburgh, PA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction In this paper we consider the problem of training a Support Vector Machine (SVM), a pattern classification algorithm recently developed by V. Vapnik and his team at AT&T Bell Labs. <ref> [1, 2, 7] </ref>. SVM can be seen as a new way to train polynomial, neural network, or Radial Basis Functions classifiers, based on the idea of structural risk minimization rather than empirical risk minimization 1 . <p> V. Vapnik showed [7] that the hyperplane with this property is the one that leaves the maximum margin between the two classes <ref> [1] </ref>, where the margin is defined as the sum of the distances of the hyperplane from the closest point of the two classes. <p> This strategy will be stated in section 3.2. Using the results of sections 3.1 and 3.2 we will then formulate our decomposition algorithm in section 3.3. 3.1 Optimality Conditions The QP problem that we have to solve in order to train a SVM is the following <ref> [1, 2, 7] </ref>: Minimize W (fl) = fl T 1 + 1 2 fl T Dfl subject to fl T y = 0 () fl 0 () where (1) i = 1, D ij = y i y j K (x i ; x j ), , T = ( 1
Reference: [2] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 1-25, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction In this paper we consider the problem of training a Support Vector Machine (SVM), a pattern classification algorithm recently developed by V. Vapnik and his team at AT&T Bell Labs. <ref> [1, 2, 7] </ref>. SVM can be seen as a new way to train polynomial, neural network, or Radial Basis Functions classifiers, based on the idea of structural risk minimization rather than empirical risk minimization 1 . <p> A more detailed description of SVM can be found in [7] (chapter 5) and <ref> [2] </ref>. We start from the simple case of two linearly separable classes. <p> This strategy will be stated in section 3.2. Using the results of sections 3.1 and 3.2 we will then formulate our decomposition algorithm in section 3.3. 3.1 Optimality Conditions The QP problem that we have to solve in order to train a SVM is the following <ref> [1, 2, 7] </ref>: Minimize W (fl) = fl T 1 + 1 2 fl T Dfl subject to fl T y = 0 () fl 0 () where (1) i = 1, D ij = y i y j K (x i ; x j ), , T = ( 1 <p> Case: 0 &lt; i &lt; C From the first three equations of the KT conditions we have: (Dfl) i 1 + y i = 0 (3) Using the results in <ref> [2] </ref> and [7] one can show that this implies that = b. 2.
Reference: [3] <author> B. Murtagh and M. Saunders. </author> <title> Large-scale linearly constrained optimiza tion. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 41-72, </pages> <year> 1978. </year>
Reference-contexts: Since the objective function is bounded (W (fl) is convex quadratic and the feasible region is bounded), the algorithm must converge to the global optimal solution in a finite number of iterations. 4 Implementation and Results We have implemented the decomposition algorithm using MINOS 5.4 <ref> [3] </ref> as the solver of the sub-problems.
Reference: [4] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Support vector machines: Training and applications. A.I. </title> <type> Memo 1602, </type> <institution> MIT A. I. Lab., </institution> <year> 1997. </year>
Reference-contexts: This problem is challenging when the size of the data set becomes larger than a few thousands, which is often the case in practical applications. A number of techniques for SVM training have been proposed <ref> [7, 4, 5, 6] </ref>. However, many of these strategies take advantage of the following assumptions, or expectations: 1) The number of support vectors is small, with respect to the the number of data points; 2) the total number of support vectors does not exceed a few thousands (e.g. &lt; 3:000). <p> It should be noticed, however, that in the case in which the assumptions above are satisfied the algorithm does take advantage of them. The algorithm is similar in spirit to the algorithm that we proposed in <ref> [4] </ref> (that was limited to deal with few thousands support vectors): it is a decomposition algorithm, in which the original QP problem is replaced by a sequence of smaller problems that is proved to converge to the global optimum.
Reference: [5] <author> M. Schmidt. </author> <title> Identifying speakers with support vectors networks. </title> <booktitle> In Pro ceedings of Interface '96, </booktitle> <address> Sydney, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: This problem is challenging when the size of the data set becomes larger than a few thousands, which is often the case in practical applications. A number of techniques for SVM training have been proposed <ref> [7, 4, 5, 6] </ref>. However, many of these strategies take advantage of the following assumptions, or expectations: 1) The number of support vectors is small, with respect to the the number of data points; 2) the total number of support vectors does not exceed a few thousands (e.g. &lt; 3:000).
Reference: [6] <author> B. Scholkopf, C. Burges, and V. Vapnik. </author> <title> Extracting support data for a given task. In U.M. </title> <editor> Fayyad and R. Uthurusamy, editors, </editor> <booktitle> Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Menlo Park, CA, 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This problem is challenging when the size of the data set becomes larger than a few thousands, which is often the case in practical applications. A number of techniques for SVM training have been proposed <ref> [7, 4, 5, 6] </ref>. However, many of these strategies take advantage of the following assumptions, or expectations: 1) The number of support vectors is small, with respect to the the number of data points; 2) the total number of support vectors does not exceed a few thousands (e.g. &lt; 3:000).
Reference: [7] <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction In this paper we consider the problem of training a Support Vector Machine (SVM), a pattern classification algorithm recently developed by V. Vapnik and his team at AT&T Bell Labs. <ref> [1, 2, 7] </ref>. SVM can be seen as a new way to train polynomial, neural network, or Radial Basis Functions classifiers, based on the idea of structural risk minimization rather than empirical risk minimization 1 . <p> This problem is challenging when the size of the data set becomes larger than a few thousands, which is often the case in practical applications. A number of techniques for SVM training have been proposed <ref> [7, 4, 5, 6] </ref>. However, many of these strategies take advantage of the following assumptions, or expectations: 1) The number of support vectors is small, with respect to the the number of data points; 2) the total number of support vectors does not exceed a few thousands (e.g. &lt; 3:000). <p> A more detailed description of SVM can be found in <ref> [7] </ref> (chapter 5) and [2]. We start from the simple case of two linearly separable classes. <p> i=1 of labeled examples, where y i 2 f1; 1g, and we wish to select, among the infinite number of linear classifiers that separate the data, one that minimizes the generalization error, or at least an upper bound on it (this is the idea underlying the structural risk minimization principle <ref> [7] </ref>). V. Vapnik showed [7] that the hyperplane with this property is the one that leaves the maximum margin between the two classes [1], where the margin is defined as the sum of the distances of the hyperplane from the closest point of the two classes. <p> where y i 2 f1; 1g, and we wish to select, among the infinite number of linear classifiers that separate the data, one that minimizes the generalization error, or at least an upper bound on it (this is the idea underlying the structural risk minimization principle <ref> [7] </ref>). V. Vapnik showed [7] that the hyperplane with this property is the one that leaves the maximum margin between the two classes [1], where the margin is defined as the sum of the distances of the hyperplane from the closest point of the two classes. <p> This strategy will be stated in section 3.2. Using the results of sections 3.1 and 3.2 we will then formulate our decomposition algorithm in section 3.3. 3.1 Optimality Conditions The QP problem that we have to solve in order to train a SVM is the following <ref> [1, 2, 7] </ref>: Minimize W (fl) = fl T 1 + 1 2 fl T Dfl subject to fl T y = 0 () fl 0 () where (1) i = 1, D ij = y i y j K (x i ; x j ), , T = ( 1 <p> Case: 0 &lt; i &lt; C From the first three equations of the KT conditions we have: (Dfl) i 1 + y i = 0 (3) Using the results in [2] and <ref> [7] </ref> one can show that this implies that = b. 2.
Reference: [8] <author> X. Zhang. </author> <title> Non-linear predictive models for intra-day foreign exchange trading. </title> <type> Technical report, </type> <institution> PHZ Partners, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The training time on a SUN Sparc 20 with 128 Mb of RAM ranged from 3 hours for 10,000 support vectors to 48 hours for 40,000 support vectors. The results that we obtain are comparable to the results reported in <ref> [8] </ref> using a Neural Networks approach, where generalization errors around 53% were reported. The purpose of this experiment was not to benchmark SVM's on this specific problem, but to show that its use in a problem with as many as 100,000 support vectors is computationally tractable.
References-found: 8

