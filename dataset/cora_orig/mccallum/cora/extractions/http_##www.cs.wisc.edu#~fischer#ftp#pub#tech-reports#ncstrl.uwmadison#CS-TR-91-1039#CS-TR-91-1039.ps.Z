URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-91-1039/CS-TR-91-1039.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-91-1039/
Root-URL: http://www.cs.wisc.edu
Title: RACE CONDITION DETECTION FOR DEBUGGING SHARED-MEMORY PARALLEL PROGRAMS  
Author: by ROBERT HARRY BENSON NETZER 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1991  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adve, Sarita V., Mark D. Hill, Barton P. Miller, and Robert H. B. Netzer, </author> <title> ``Detecting Data Races on Weak Memory Systems,'' </title> <booktitle> 18th Int'l Symp. on Computer Architecture, </booktitle> <pages> pp. </pages> <address> 234-243 Toronto, Ontario, </address> <month> (May </month> <year> 1991). </year>
Reference-contexts: After execution is complete, the final phase constructs and analyzes the ordering graph to detect races. Once the instrumentation phase is complete, the last two phases can be repeated for any number of executions. Most post-mortem methods detect data races <ref> [1, 2, 16, 50] </ref>; only one method detects general races [23-25]. In contrast, on-the-fly methods use only two phases. As with post-mortem methods, the first phase instruments the program to collect information during execution.
Reference: [2] <author> Allen, Todd R. and David A. Padua, </author> <title> ``Debugging Fortran on a Shared Memory Machine,'' </title> <booktitle> 1987 Intl. Conf. on Parallel Processing, </booktitle> <pages> pp. </pages> <address> 721-727 St. Charles, IL, </address> <month> (August </month> <year> 1987). </year>
Reference-contexts: Static analysis has also been used to complement dynamic methods. Static analysis can sometimes rule out the possibility of races between some sections of the program, precluding the need for tracing these program 5 6 sections for dynamic analysis <ref> [2, 23-25, 75] </ref>. Static analysis can also compute input data that might manifest a race [63], allowing dynamic analysis to attempt to verify the existence of that race. 2.2. Dynamic Analysis Unlike static analysis, dynamic analysis detects the race conditions exhibited by a particular execution of the program. <p> After execution is complete, the final phase constructs and analyzes the ordering graph to detect races. Once the instrumentation phase is complete, the last two phases can be repeated for any number of executions. Most post-mortem methods detect data races <ref> [1, 2, 16, 50] </ref>; only one method detects general races [23-25]. In contrast, on-the-fly methods use only two phases. As with post-mortem methods, the first phase instruments the program to collect information during execution. <p> Below, we briefly outline these aspects of previous methods. Allen and Padua <ref> [2] </ref> present a post-mortem data race detection method for Fortran programs that introduce parallelism with the doall construct, and synchronize with test and testset primitives. <p> This example shows how we can guarantee that the temporal ordering in which a and d execute concurrently could have occurred. We finally mention that our event-control dependences are similar to the hides relation used by Allen and Pa-dua <ref> [2, 24, 25] </ref> and the semantic dependences defined by Podgurski and Clarke [60, 61]. <p> Bernstein's conditions state that atomic execution of a critical section is guaranteed if the shared variables it reads and modifies are not modified by any other concurrently executing section of code [9]. A violation of these conditions has typically been called a data race <ref> [2, 16, 17, 50, 54, 55] </ref>, access anomaly [20, 37, 51], or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals.
Reference: [3] <author> Appelbe, William F. and Charles E. McDowell, </author> <title> ``Anomaly Reporting A Tool for Debugging and Developing Parallel Numerical Algorithms (extended abstract),'' </title> <booktitle> 1st Intl. Conf. on Supercomputing Systems, </booktitle> <pages> pp. </pages> <month> 386-391 </month> <year> (1985). </year>
Reference-contexts: Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. This state space can either be constructed explicitly, by building a graph <ref> [3, 4, 46, 47, 52, 72, 74, 75] </ref>, or implicitly, by constructing a representation of the state space (such as a formal language or a petri-net)[5, 36, 67]. In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well.
Reference: [4] <author> Appelbe, William F. and Charles E. McDowell, </author> <title> ``Integrating Tools for Debugging and Developing Multitasking Programs,'' </title> <booktitle> SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 78-88 Madison, WI, </address> <month> (May </month> <year> 1988). </year> <note> Also appears in SIGPLAN Notices 24(1) (January 1989). </note>
Reference-contexts: Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. This state space can either be constructed explicitly, by building a graph <ref> [3, 4, 46, 47, 52, 72, 74, 75] </ref>, or implicitly, by constructing a representation of the state space (such as a formal language or a petri-net)[5, 36, 67]. In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well.
Reference: [5] <author> Apt, Krzysztof R., </author> <title> ``A Static Analysis of CSP Programs,'' </title> <booktitle> Workshop on Logics of Programs, </booktitle> <pages> pp. </pages> <month> 1-17 (June </month> <year> 1983). </year>
Reference: [6] <author> Balasundaram, Vasanth and Ken Kennedy, </author> <title> ``Compile-time Detection of Race Conditions in a Parallel Program,'' </title> <booktitle> 3rd Intl. Conf. on Supercomputing, </booktitle> <pages> pp. </pages> <address> 175-185 Crete, Greece, </address> <month> (June </month> <year> 1989). </year>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report).
Reference: [7] <author> Balasundaram, Vasanth and Ken Kennedy, </author> <title> ``A Technique for Summarizing Data Access and Its Use in Parallelism Enhancing Transformations,'' </title> <booktitle> SIGPLAN '89 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. </pages> <address> 41-53 Portland, OR, </address> <year> (1989). </year>
Reference-contexts: One approach is to write a compressed trace by using simple data compression schemes or methods for summarizing regular access patterns <ref> [7, 8, 12] </ref>. Another approach is to employ two passes. The first pass would record an incomplete, coarse trace containing only enough information to replay a portion of the execution [43, 44, 50].
Reference: [8] <author> Balasundaram, Vasanth, </author> <title> ``Interactive Parallelization of Numerical Scientific Programs,'' </title> <type> Ph.D. </type> <institution> Thesis; also Rice Univ. Computer Science Dept. </institution> <type> Tech. Rep. </type> <institution> TR89-95, </institution> <month> (July </month> <year> 1989). </year>
Reference-contexts: One approach is to write a compressed trace by using simple data compression schemes or methods for summarizing regular access patterns <ref> [7, 8, 12] </ref>. Another approach is to employ two passes. The first pass would record an incomplete, coarse trace containing only enough information to replay a portion of the execution [43, 44, 50].
Reference: [9] <author> Bernstein, A. J., </author> <title> ``Analysis of Programs for Parallel Processing,'' </title> <journal> IEEE Trans. on Electronic Computers EC-15(5) pp. </journal> <month> 757-763 (October </month> <year> 1966). </year>
Reference-contexts: Bernstein's conditions state that atomic execution of a critical section is guaranteed if the shared variables it reads and modifies are not modified by any other concurrently executing section of code <ref> [9] </ref>. A violation of these conditions has typically been called a data race [2, 16, 17, 50, 54, 55], access anomaly [20, 37, 51], or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals.
Reference: [10] <author> Bristow, Guy, </author> <title> ``The Static Detection of Synchronization Anomalies in HAL/S Programs,'' </title> <type> Ph.D. Thesis, </type> <note> also available as Comp Sci Tech Rep CU-CS-165-79, </note> <institution> Univ. Colorado at Boulder, </institution> <year> (1979). </year>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report).
Reference: [11] <author> Bristow, G., C. Drey, B. Edwards, and W. Riddle, </author> <title> ``Anomaly Detection in Concurrent Programs,'' </title> <booktitle> 4th Intl. Conf. on Software Engineering, </booktitle> <pages> pp. </pages> <month> 265-273 </month> <year> (1979). </year>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report).
Reference: [12] <author> Callahan, David, </author> <title> ``A Global Approach to Parallelism Detection,'' </title> <type> Ph.D. Thesis, </type> <institution> Rice University, </institution> <month> (July </month> <year> 1986). </year>
Reference-contexts: One approach is to write a compressed trace by using simple data compression schemes or methods for summarizing regular access patterns <ref> [7, 8, 12] </ref>. Another approach is to employ two passes. The first pass would record an incomplete, coarse trace containing only enough information to replay a portion of the execution [43, 44, 50].
Reference: [13] <author> Callahan, David and Jaspal Subhlok, </author> <title> ``Static Analysis of Low-Level Synchronization,'' </title> <booktitle> SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 100-111 Madison, WI, </address> <month> (May </month> <year> 1988). </year> <note> Also appears in SIGPLAN Notices 24(1) (January 1989). </note>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report). <p> As we will show, the execution can exhibit certain orderings iff B is satisfiable (or not satisfiable). For each variable, X i , construct the following three processes: hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 This type of synchronization uses only Post and Wait primitives <ref> [13, 14, 24] </ref>. All Waits on a synchronization variable block until a Post on the same variable is issued. Unlike semaphores, a Wait does not change the value of the variable; once a Post is issued, all past and subsequent Waits are allowed to proceed.
Reference: [14] <author> Callahan, David, Ken Kennedy, and Jaspal Subhlok, </author> <title> ``Analysis of Event Synchronization in A Parallel Programming Tool,'' </title> <booktitle> 2nd Symp. on Principle and Practice of Parallel Programming, </booktitle> <pages> pp. </pages> <address> 21-30 Seattle, WA, </address> <month> (March </month> <year> 1990). </year> <pages> 104 105 </pages>
Reference-contexts: As we will show, the execution can exhibit certain orderings iff B is satisfiable (or not satisfiable). For each variable, X i , construct the following three processes: hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 This type of synchronization uses only Post and Wait primitives <ref> [13, 14, 24] </ref>. All Waits on a synchronization variable block until a Post on the same variable is issued. Unlike semaphores, a Wait does not change the value of the variable; once a Post is issued, all past and subsequent Waits are allowed to proceed.
Reference: [15] <author> Carver, Richard H. and Kuo-Chung Tai, </author> <title> ``Reproducible Testing of Concurrent Programs Based on Shared Variables,'' </title> <booktitle> 6th Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> pp. </pages> <address> 428-432 Boston, MA, </address> <month> (May </month> <year> 1986). </year>
Reference-contexts: Program replay is one example of an area where race detection and debugging can cooperate. Naive replay schemes simply trace the value of every shared-memory access and restore that value during re-execution [59]. More efficient schemes record and reproduce only the temporal ordering <ref> [15, 44] </ref>, but these schemes fail in general to produce correct replay for executions containing actual data races.
Reference: [16] <author> Choi, Jong-Deok, Barton P. Miller, and Robert H. B. Netzer, </author> <title> ``Techniques for Debugging Parallel Programs with Flowback Analysis,'' </title> <journal> ACM Trans. on Programming Languages and Systems 13(4) pp. </journal> <month> 491-530 (Oc-tober </month> <year> 1991). </year>
Reference-contexts: After execution is complete, the final phase constructs and analyzes the ordering graph to detect races. Once the instrumentation phase is complete, the last two phases can be repeated for any number of executions. Most post-mortem methods detect data races <ref> [1, 2, 16, 50] </ref>; only one method detects general races [23-25]. In contrast, on-the-fly methods use only two phases. As with post-mortem methods, the first phase instruments the program to collect information during execution. <p> Bernstein's conditions state that atomic execution of a critical section is guaranteed if the shared variables it reads and modifies are not modified by any other concurrently executing section of code [9]. A violation of these conditions has typically been called a data race <ref> [2, 16, 17, 50, 54, 55] </ref>, access anomaly [20, 37, 51], or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals. <p> Locating unordered events requires analyzing G to decide whether a path connects two given nodes. Since this determination must be made many times, it should be as efficient as possible. We therefore preprocess the graph to compute a timestamp for each node <ref> [16, 27, 28, 33, 35, 45, 50] </ref>. Times-tamps provide a mechanism for quickly determining the ordering between two given events. Each timestamp is a vector (of length p) of event serial numbers 2 . <p> As with direct dependences, at most p transitive out-edges are ever added for any event, maintaining the O (np) space bound. 7.2.2. Computing Timestamps for Cyclic Graphs Previously, timestamps have only been used to represent connectivity in acyclic graphs <ref> [16, 33, 35, 50] </ref>, as was done in Chapter 6. However, because the augmented graphs may contain cycles, we now generalize the use of timestamps to cyclic graphs and show how to compute them for G E . <p> Another area where race detection and debugging might be integrated is the use of data and control dependences to understand the program behavior. For example, debugging using flowback analysis attempts to follow data and control dependences backwards through the execution to understand the causal relationship between events <ref> [16, 50] </ref>. Such an analysis uses static information obtained by semantic analysis of the program to first locate potential dependences, and dynamic information obtained by coarse execution traces to locate actual dependences.
Reference: [17] <author> Choi, Jong-Deok and Sang Lyul Min, </author> <title> ``Race Frontier: Reproducing Data Races in Parallel Program Debugging,'' </title> <booktitle> 3rd ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. </pages> <address> 145-154 Willi-amsburg, VA, </address> <month> (April </month> <year> 1991). </year>
Reference-contexts: The thrust of previous work has been the development of efficient encoding schemes. Several schemes have been proposed, but most buffer only enough information to guarantee detection of one race involving each shared variable <ref> [17, 19-22, 37, 49, 64, 65] </ref>, leaving some races undetected (however, Choi and Min [17] propose a method, discussed further in Chapter 8, for re-executing the program to reproduce the undetected races). <p> The thrust of previous work has been the development of efficient encoding schemes. Several schemes have been proposed, but most buffer only enough information to guarantee detection of one race involving each shared variable [17, 19-22, 37, 49, 64, 65], leaving some races undetected (however, Choi and Min <ref> [17] </ref> propose a method, discussed further in Chapter 8, for re-executing the program to reproduce the undetected races). <p> On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> The testset (x) primitive increments the value of the synchronization variable x. An edge is drawn in the ordering graph from a doall node to the first node representing hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 Some methods store the inverse of the READ and WRITE sets <ref> [17, 19, 37, 56] </ref>. 9 each iteration of the loop. Similarly, an edge is drawn from the last node of each loop iteration to the end doall node corresponding to the end of the loop. <p> Steele [69] and Nudler and Rudolph [56] also present on-the-fly schemes for programs that use only fork/join. They conceptually construct the ordering graph in the same way as all other methods, and differ in the techniques used to encode the graph in memory. Choi and Min <ref> [17, 51] </ref> present techniques that address both the execution-time overhead of on-the-fly race detection and the problem of undetected races. They outline a hardware-based scheme for reducing the amount of execution-time overhead by using state information maintained by (a modified version of) the underlying cache coherence protocol [51]. <p> To address the problem of undetected races, they also show how to guarantee deterministic re-execution of the program up to the point of the first detected race, allowing additional instrumentation to be added that locates the originally undetected races <ref> [17] </ref>. Their work has some similarities to ours; a more detailed discussion appears in Chapter 8. The general race detection method described by Emrath, Ghosh, and Padua [23-25] considers programs that use fork/join and event style synchronization (using the Post, Wait, and Clear primitives). <p> Since on-the-fly methods serialize all accesses to a given memory location, they have information in addition to the ordering graph about the order in which individual shared-memory accesses occurred. As suggested by Choi and Min <ref> [17] </ref>, these methods could stop the program after the first race is detected (even though earlier races may remain undetected). However, obtaining this extra information incurs overhead at each shared-memory access and introduces central bottlenecks into the execution. <p> Bernstein's conditions state that atomic execution of a critical section is guaranteed if the shared variables it reads and modifies are not modified by any other concurrently executing section of code [9]. A violation of these conditions has typically been called a data race <ref> [2, 16, 17, 50, 54, 55] </ref>, access anomaly [20, 37, 51], or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals. <p> In such a case, augmenting the graph with transitive edges can be avoided entirely. 8.3. Related Work We now discuss the only other work that addresses the issue of locating first data races. Choi and Min <ref> [17] </ref> consider an on-the-fly approach for detecting data races in which they locate a subset of the races called the Race Frontier. Below we describe their work and contrast it with ours.
Reference: [18] <author> Dijkstra, E. W., </author> <title> ``Solution of a Problem in Concurrent Programming Control,'' </title> <booktitle> Communications of the ACM 8(9) p. </booktitle> <month> 569 (September </month> <year> 1965). </year>
Reference-contexts: Second, even if the programmer expects the program to be nondeterministic, the program may lack the proper synchronization required for it to behave as expected. In such programs, explicit synchronization is often added to implement critical sections. Critical sections are portions of code intended to execute atomically <ref> [18] </ref>. Atomic execution is guaranteed if the shared variables read or modified by the critical section are not modified by any other concurrently executing section of code. Without proper synchronization, these shared variables may be modified by other processes as the code executes, violating the expected atomicity. <p> A Data Race Example One purpose of adding explicit synchronization to shared-memory parallel programs is to implement critical sections. Critical sections are any blocks of code intended to execute as if they were atomic <ref> [18] </ref>, regardless of the synchronization constructs used to enforce this atomicity.
Reference: [19] <author> Dinning, Anne and Edith Schonberg, </author> <title> ``The Task Recycling Technique for Detecting Access Anomalies On-The-Fly,'' </title> <type> IBM Tech. Rep. RC 15385, </type> <month> (January </month> <year> 1990). </year>
Reference-contexts: On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> The testset (x) primitive increments the value of the synchronization variable x. An edge is drawn in the ordering graph from a doall node to the first node representing hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 Some methods store the inverse of the READ and WRITE sets <ref> [17, 19, 37, 56] </ref>. 9 each iteration of the loop. Similarly, an edge is drawn from the last node of each loop iteration to the end doall node corresponding to the end of the loop. <p> To draw edges between nodes representing semaphore operations, they pair the i th V operation on each semaphore with the i th P operation on the same semaphore. An edge is then drawn from each V node to the P node with which it is paired. Dinning and Schonberg <ref> [19, 20, 64, 65] </ref> describe on-the-fly techniques for detecting data races in executions of programs using fork/join and arbitrary synchronization (such as barriers, events, Ada rendezvous, and semaphores). <p> They outline a hardware-based scheme for reducing the amount of execution-time overhead by using state information maintained by (a modified version of) the underlying cache coherence protocol [51]. The resulting race detection method is essentially the same as that of Dinning and Schon-berg <ref> [19, 20] </ref>. To address the problem of undetected races, they also show how to guarantee deterministic re-execution of the program up to the point of the first detected race, allowing additional instrumentation to be added that locates the originally undetected races [17]. <p> The fact that previous work has not precisely defined the notion of a race (or a race artifact) exacerbates the race-artifact problem. Without a formal statement of the race-detection problem, developing correct techniques for dealing with race artifacts is complicated. Previous work has only characterized races informally <ref> [19, 24, 25] </ref> or not at all. For example, data races have only been defined as occurring when two blocks of code access common shared variables and ``can potentially execute concurrently''[19]. <p> Determine the connectivity between two nodes hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 This space bound can be optimized for executions that create and destroy processes on-the-fly, by re-using process numbers <ref> [19, 33] </ref>. For simplicity, we assume that a fixed number of processes exist during execution. 56 Algorithm 6.1 computes timestamps. The timestamp of a given node, n, is the maximum over the timestamps of all nodes that have edges into n.
Reference: [20] <author> Dinning, Anne and Edith Schonberg, </author> <title> ``An Empirical Comparison of Monitoring Algorithms for Access Anomaly Detection,'' </title> <booktitle> 2nd ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. </pages> <address> 1-10 Seattle, WA, </address> <month> (March </month> <year> 1990). </year>
Reference-contexts: On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> To draw edges between nodes representing semaphore operations, they pair the i th V operation on each semaphore with the i th P operation on the same semaphore. An edge is then drawn from each V node to the P node with which it is paired. Dinning and Schonberg <ref> [19, 20, 64, 65] </ref> describe on-the-fly techniques for detecting data races in executions of programs using fork/join and arbitrary synchronization (such as barriers, events, Ada rendezvous, and semaphores). <p> They outline a hardware-based scheme for reducing the amount of execution-time overhead by using state information maintained by (a modified version of) the underlying cache coherence protocol [51]. The resulting race detection method is essentially the same as that of Dinning and Schon-berg <ref> [19, 20] </ref>. To address the problem of undetected races, they also show how to guarantee deterministic re-execution of the program up to the point of the first detected race, allowing additional instrumentation to be added that locates the originally undetected races [17]. <p> A violation of these conditions has typically been called a data race [2, 16, 17, 50, 54, 55], access anomaly <ref> [20, 37, 51] </ref>, or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals. Bank tellers make either deposits or withdrawals from the given bank account (we are assuming a small bank that has only a single customer).
Reference: [21] <author> Dinning, Anne, </author> <title> ``Detecting Nondeterminism in Shared Memory Parallel Programs,'' </title> <type> Ph.D. </type> <institution> Thesis; also Dept. of Computer Science Tech. </institution> <type> Rep. 526, </type> <address> New York University, </address> <month> (November </month> <year> 1990). </year>
Reference: [22] <author> Dinning, Anne and Edith Schonberg, </author> <title> ``Detecting Access Anomalies in Programs with Critical Sections,'' </title> <booktitle> ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 85-96 Santa Cruz, CA, </address> <month> (May </month> <year> 1991). </year>
Reference-contexts: On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> For asynchronous coordination, an edge is constructed from the node representing the sender to the node representing the receiver. They do not specify how to pair senders with receivers. Dinning and Schonberg <ref> [22] </ref> also extend their techniques to uncover some races that are hidden by critical sections.
Reference: [23] <author> Emrath, Perry A. and David A. Padua, </author> <title> ``Automatic Detection Of Nondeterminacy in Parallel Programs,'' </title> <booktitle> SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 89-99 Madison, WI, </address> <month> (May </month> <year> 1988). </year> <note> Also appears in SIGPLAN Notices 24(1) (January 1989). </note>
Reference-contexts: Nondeterminism is generally introduced when the order of two accesses to the same resource is not enforced by the program's synchronization. The existence of two such unordered accesses has typically been called a race condition or race <ref> [23, 24] </ref>. For a more consistent terminology, we propose the term general race. As an example of programs for which general races are considered bugs, consider parallel programs that are constructed from sequential programs by parallelizing loops. <p> Characterizing General Races Intuitively, a general race exists in P when a and b have a data conflict and their access order is not ``guaranteed'' by the execution's synchronization <ref> [23, 24] </ref>. We formally characterize this situation as follows: a general race exists if a and b are issued in a different order in some feasible program execution than they are in P. As with data races, two variations (feasible and apparent) of a general race exist. <p> In contrast, detecting general races requires analyzing the entire execution's synchronization to determine which orderings are not guaranteed. In the next chapter, we prove that, in general, computing the guaranteed orderings is an NP-hard problem. We finally mention that Emrath and Padua <ref> [23] </ref> have also attempted to characterize different types of races, but their work has a different scope. They only address programs intended to be deterministic, and consider four levels of nondeterminacy of a program (on a given input).
Reference: [24] <author> Emrath, Perry A., Sanjoy Ghosh, and David A. Padua, </author> <title> ``Event Synchronization Analysis for Debugging Parallel Programs,'' </title> <booktitle> Supercomputing '89, </booktitle> <pages> pp. </pages> <address> 580-588 Reno, NV, </address> <month> (November </month> <year> 1989). </year>
Reference-contexts: The key to general race detection is determining the event orderings that are guaranteed to be exhibited by any execution of the program on the given input. The general race detection method by Emrath, Ghosh, and Padua <ref> [24, 25] </ref> computes these guaranteed orderings by analyzing the execution's explicit synchronization and transforming the ordering graph. An edge in the transformed graph indicates that the synchronization semantics would force the node at the tail of the edge to precede the node at the head in any execution. <p> The fact that previous work has not precisely defined the notion of a race (or a race artifact) exacerbates the race-artifact problem. Without a formal statement of the race-detection problem, developing correct techniques for dealing with race artifacts is complicated. Previous work has only characterized races informally <ref> [19, 24, 25] </ref> or not at all. For example, data races have only been defined as occurring when two blocks of code access common shared variables and ``can potentially execute concurrently''[19]. <p> This example shows how we can guarantee that the temporal ordering in which a and d execute concurrently could have occurred. We finally mention that our event-control dependences are similar to the hides relation used by Allen and Pa-dua <ref> [2, 24, 25] </ref> and the semantic dependences defined by Podgurski and Clarke [60, 61]. <p> Nondeterminism is generally introduced when the order of two accesses to the same resource is not enforced by the program's synchronization. The existence of two such unordered accesses has typically been called a race condition or race <ref> [23, 24] </ref>. For a more consistent terminology, we propose the term general race. As an example of programs for which general races are considered bugs, consider parallel programs that are constructed from sequential programs by parallelizing loops. <p> Characterizing General Races Intuitively, a general race exists in P when a and b have a data conflict and their access order is not ``guaranteed'' by the execution's synchronization <ref> [23, 24] </ref>. We formally characterize this situation as follows: a general race exists if a and b are issued in a different order in some feasible program execution than they are in P. As with data races, two variations (feasible and apparent) of a general race exist. <p> As we will show, the execution can exhibit certain orderings iff B is satisfiable (or not satisfiable). For each variable, X i , construct the following three processes: hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 This type of synchronization uses only Post and Wait primitives <ref> [13, 14, 24] </ref>. All Waits on a synchronization variable block until a Post on the same variable is issued. Unlike semaphores, a Wait does not change the value of the variable; once a Post is issued, all past and subsequent Waits are allowed to proceed. <p> Since critical sections implement mutual exclusion, exact and exhaustive apparent data race detection is always NP-hard. Even though exactly determining the existence of general races and data races is NP-hard for executions using powerful synchronization, approximations can be efficiently computed. Emrath, Ghosh, and Padua <ref> [24, 25] </ref> present an algorithm for computing a superset of the CHB SYNC relation for executions that use an extended form of Post/Wait style synchronization that includes Clear operations 7 .
Reference: [25] <author> Emrath, Perry A., Sanjoy Ghosh, and David A. Padua, </author> <title> ``Detecting Non-Determinacy in Parallel Programs,'' </title> <journal> IEEE Software 9(1) pp. </journal> <month> 69-77 (January </month> <year> 1992). </year>
Reference-contexts: The key to general race detection is determining the event orderings that are guaranteed to be exhibited by any execution of the program on the given input. The general race detection method by Emrath, Ghosh, and Padua <ref> [24, 25] </ref> computes these guaranteed orderings by analyzing the execution's explicit synchronization and transforming the ordering graph. An edge in the transformed graph indicates that the synchronization semantics would force the node at the tail of the edge to precede the node at the head in any execution. <p> The fact that previous work has not precisely defined the notion of a race (or a race artifact) exacerbates the race-artifact problem. Without a formal statement of the race-detection problem, developing correct techniques for dealing with race artifacts is complicated. Previous work has only characterized races informally <ref> [19, 24, 25] </ref> or not at all. For example, data races have only been defined as occurring when two blocks of code access common shared variables and ``can potentially execute concurrently''[19]. <p> This example shows how we can guarantee that the temporal ordering in which a and d execute concurrently could have occurred. We finally mention that our event-control dependences are similar to the hides relation used by Allen and Pa-dua <ref> [2, 24, 25] </ref> and the semantic dependences defined by Podgurski and Clarke [60, 61]. <p> Since critical sections implement mutual exclusion, exact and exhaustive apparent data race detection is always NP-hard. Even though exactly determining the existence of general races and data races is NP-hard for executions using powerful synchronization, approximations can be efficiently computed. Emrath, Ghosh, and Padua <ref> [24, 25] </ref> present an algorithm for computing a superset of the CHB SYNC relation for executions that use an extended form of Post/Wait style synchronization that includes Clear operations 7 .
Reference: [26] <author> Ferrante, J., Karl J. Ottenstein, and Joe D. Warren, </author> <title> ``The Program Dependence Graph and its Use in Optimization,'' </title> <journal> ACM Trans. on Programming Languages and Systems 9(3) pp. </journal> <month> 319-349 </month> <year> (1987). </year>
Reference-contexts: This definition of data dependence is somewhat nonstandard since we consider transitive dependences involving flow-, anti-, and output-dependences <ref> [26, 38] </ref>, and do not expli citly state the variables involved. 3.1.2. Axioms A program execution is simply a notation for describing the execution of a shared-memory parallel program. Following Lamport, our model also contains several axioms that describe properties every program execution must possess.
Reference: [27] <author> Fidge, C. J., </author> <title> ``Partial Orders for Parallel Debugging,'' </title> <booktitle> SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 183-194 Madison, WI, </address> <month> (May </month> <year> 1988). </year> <note> Also appears in SIGPLAN Notices 24(1) (Janu-ary 1989). </note>
Reference-contexts: Locating unordered events requires analyzing G to decide whether a path connects two given nodes. Since this determination must be made many times, it should be as efficient as possible. We therefore preprocess the graph to compute a timestamp for each node <ref> [16, 27, 28, 33, 35, 45, 50] </ref>. Times-tamps provide a mechanism for quickly determining the ordering between two given events. Each timestamp is a vector (of length p) of event serial numbers 2 .
Reference: [28] <author> Fowler, Jerry and Willy Zwaenepoel, </author> <title> ``Causal Distributed Breakpoints,'' </title> <booktitle> 10th Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> pp. </pages> <address> 134-141 Paris, France, </address> <month> (June </month> <year> 1990). </year>
Reference-contexts: Locating unordered events requires analyzing G to decide whether a path connects two given nodes. Since this determination must be made many times, it should be as efficient as possible. We therefore preprocess the graph to compute a timestamp for each node <ref> [16, 27, 28, 33, 35, 45, 50] </ref>. Times-tamps provide a mechanism for quickly determining the ordering between two given events. Each timestamp is a vector (of length p) of event serial numbers 2 .
Reference: [29] <author> Garey, Michael R. and David S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Co. </publisher> <year> (1979). </year> <month> 106 </month>
Reference-contexts: Furthermore, we present a proof only for the happened-before relations ( MHB SYNC and SYNC ); proofs for the other relations are analogous. We give a reduc tion 3 from 3CNFSAT <ref> [29] </ref> such that any Boolean formula is satisfiable (or not satisfiable) iff b CHB SYNC a (or a SYNC b) for two events, a and b, defined in the reduction.
Reference: [30] <author> Gottlieb, Allan, B. D. Lubachevsky, and Larry Rudolph, </author> <title> ``Basic Techniques for the Efficient Coorindation of Very Large Numbers of Cooperating Sequential Processors,'' </title> <journal> ACM Trans. on Programming Languages and Systems 5(2) pp. </journal> <month> 164-189 (April </month> <year> 1983). </year>
Reference-contexts: consisting of 50 arcs. shpath implements a parallel version of Dijkstra's shortest path algorithm, and was run on graphs with 50 nodes that were assigned random inter-node costs. tycho is a cache simulator, and was run with an address trace consisting of 1024 memory references. parq implements a parallel queue <ref> [30] </ref>; child processes loop, queueing and dequeueing records from the queue. Each child queued and dequeued 300 records. workq implements the shared work queue example of Figure 2.1.
Reference: [31] <author> Griswold, Victor Jon, </author> <title> ``Determining Interior Vertices of Graph Intervals,'' </title> <institution> Dept. of Computer Science Tech. </institution> <type> Rep. </type> <institution> WUCS-90-40, Washington Univ, </institution> <month> (December </month> <year> 1990). </year>
Reference-contexts: If ordering can be performed during apparent race detection, portions of the trace that cannot possibly contain first races need not be analyzed. Second, to reduce the space overhead of post-mortem analysis, alternatives to using timestamps should be investigated. Work on the incremental maintenance of ordering graphs <ref> [31, 32] </ref>, which avoids timestamps, might provide some insights. Third, to reduce the time overhead, the analysis itself might be parallelized. One obvious opportunity is to parallelize the detection of apparent races, but validation and ordering would also benefit. 10.2.2.
Reference: [32] <author> Griswold, Victor Jon, </author> <title> ``Core Algorithms for Autonomous Monitoring of Distributed Systems,'' </title> <booktitle> ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 36-45 Santa Cruz, CA, </address> <month> (May </month> <year> 1991). </year>
Reference-contexts: If ordering can be performed during apparent race detection, portions of the trace that cannot possibly contain first races need not be analyzed. Second, to reduce the space overhead of post-mortem analysis, alternatives to using timestamps should be investigated. Work on the incremental maintenance of ordering graphs <ref> [31, 32] </ref>, which avoids timestamps, might provide some insights. Third, to reduce the time overhead, the analysis itself might be parallelized. One obvious opportunity is to parallelize the detection of apparent races, but validation and ordering would also benefit. 10.2.2.
Reference: [33] <author> Haban, Dieter and Wolfgang Weigel, </author> <title> ``Global Events and Global Breakpoints in Distributed Systems,'' </title> <booktitle> 21st Hawaii Int'l Conf. on System Sciences, </booktitle> <volume> Vol II, </volume> <pages> pp. </pages> <month> 166-175 </month> <year> (1989). </year>
Reference-contexts: Locating unordered events requires analyzing G to decide whether a path connects two given nodes. Since this determination must be made many times, it should be as efficient as possible. We therefore preprocess the graph to compute a timestamp for each node <ref> [16, 27, 28, 33, 35, 45, 50] </ref>. Times-tamps provide a mechanism for quickly determining the ordering between two given events. Each timestamp is a vector (of length p) of event serial numbers 2 . <p> Determine the connectivity between two nodes hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 This space bound can be optimized for executions that create and destroy processes on-the-fly, by re-using process numbers <ref> [19, 33] </ref>. For simplicity, we assume that a fixed number of processes exist during execution. 56 Algorithm 6.1 computes timestamps. The timestamp of a given node, n, is the maximum over the timestamps of all nodes that have edges into n. <p> As with direct dependences, at most p transitive out-edges are ever added for any event, maintaining the O (np) space bound. 7.2.2. Computing Timestamps for Cyclic Graphs Previously, timestamps have only been used to represent connectivity in acyclic graphs <ref> [16, 33, 35, 50] </ref>, as was done in Chapter 6. However, because the augmented graphs may contain cycles, we now generalize the use of timestamps to cyclic graphs and show how to compute them for G E .
Reference: [34] <author> Habermann, A. Nico, </author> <title> ``Synchronization of Communicating Processes,'' </title> <journal> Communications of the ACM 12(3) pp. </journal> <month> 171-176 (March </month> <year> 1972). </year>
Reference-contexts: To describe program executions that use general semaphores, we distinguish between P events and V events. The set of all P and V events on semaphore S is denoted by E P (S) and E V (S) , respectively. We assume that in any program execution the semaphore invariant <ref> [34] </ref> is always maintained. For general semaphores, the semaphore invariant is maintained iff at each point in the execution, the number of V operations that have either completed or have begun executing is greater than or equal to the number of P operations that have completed.
Reference: [35] <author> Helmbold, David P., Charles E. McDowell, and Jian-Zhong Wang, </author> <title> ``Analyzing Traces with Anonymous Synchronization,'' </title> <booktitle> 1990 Intl. Conf. on Parallel Processing, </booktitle> <pages> pp. </pages> <address> 70-77 St. Charles, IL, </address> <month> (August </month> <year> 1990). </year>
Reference-contexts: However, the graph constructed by their approximation algorithm sometimes omits some guaranteed orderings, possibly causing race artifacts to be reported. In Chapter 5 we prove that the problem of exactly computing the guaranteed orderings is NP-hard. 11 Helmbold, McDowell, and Wang <ref> [35] </ref> also present algorithms for computing guaranteed orderings for programs that use general semaphores. As above, these algorithms transform the ordering graph. They achieve polynomial running time by computing safe orderings, which are conservative approximations of the guaranteed order-ings. <p> Emrath, Ghosh, and Padua [24, 25] present an algorithm for computing a superset of the CHB SYNC relation for executions that use an extended form of Post/Wait style synchronization that includes Clear operations 7 . Helmbold, McDowell, and Wang <ref> [35] </ref> present algorithms for computing a subset of MHB SYNC and a superset of CCW SYNC for programs that use semaphores. <p> Locating unordered events requires analyzing G to decide whether a path connects two given nodes. Since this determination must be made many times, it should be as efficient as possible. We therefore preprocess the graph to compute a timestamp for each node <ref> [16, 27, 28, 33, 35, 45, 50] </ref>. Times-tamps provide a mechanism for quickly determining the ordering between two given events. Each timestamp is a vector (of length p) of event serial numbers 2 . <p> As with direct dependences, at most p transitive out-edges are ever added for any event, maintaining the O (np) space bound. 7.2.2. Computing Timestamps for Cyclic Graphs Previously, timestamps have only been used to represent connectivity in acyclic graphs <ref> [16, 33, 35, 50] </ref>, as was done in Chapter 6. However, because the augmented graphs may contain cycles, we now generalize the use of timestamps to cyclic graphs and show how to compute them for G E . <p> Following previous approaches <ref> [35, 50] </ref>, we pair each unlock operation with the subse quent lock on the same variable, and mutually pair all related barriers; we then have a T b if a is paired with b.
Reference: [36] <author> Herzog, Otthein, </author> <title> ``Static Analysis of Concurrent Processes for Dynamic Properties Using Petri Nets,'' </title> <booktitle> Intl. Symp. on Semantics of Concurrent Computation, </booktitle> <pages> pp. 66-90 (July 2-4, </pages> <year> 1979). </year>
Reference: [37] <author> Hood, Robert, Ken Kennedy, and John Mellor-Crummey, </author> <title> ``Parallel Program Debugging with On-the-fly Anomaly Detection,'' </title> <booktitle> Supercomputing '90, </booktitle> <pages> pp. </pages> <address> 74-81 New York, NY, </address> <month> (November </month> <year> 1990). </year>
Reference-contexts: The thrust of previous work has been the development of efficient encoding schemes. Several schemes have been proposed, but most buffer only enough information to guarantee detection of one race involving each shared variable <ref> [17, 19-22, 37, 49, 64, 65] </ref>, leaving some races undetected (however, Choi and Min [17] propose a method, discussed further in Chapter 8, for re-executing the program to reproduce the undetected races). <p> On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> The testset (x) primitive increments the value of the synchronization variable x. An edge is drawn in the ordering graph from a doall node to the first node representing hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 Some methods store the inverse of the READ and WRITE sets <ref> [17, 19, 37, 56] </ref>. 9 each iteration of the loop. Similarly, an edge is drawn from the last node of each loop iteration to the end doall node corresponding to the end of the loop. <p> However, when it is not possible for two critical sections to have executed in any order, race artifacts can be reported. They also present a static analysis, based on program slicing, for determining which parts of the program may cause such artifacts. Hood, Kennedy, and Mellor-Crummey <ref> [37] </ref> describe an on-the-fly technique for detecting data races in PCF Fortran programs that use parallel do loops with send and wait primitives. Edges are added to the graph in the same 10 way as the method by Dinning and Schonberg. <p> A violation of these conditions has typically been called a data race [2, 16, 17, 50, 54, 55], access anomaly <ref> [20, 37, 51] </ref>, or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals. Bank tellers make either deposits or withdrawals from the given bank account (we are assuming a small bank that has only a single customer).
Reference: [38] <author> Kuck, D. J., R. H. Kuhn, B. Leasure, D. A. Padua, and M. Wolfe, </author> <title> ``Dependence Graphs and Compiler Optimizations,'' </title> <booktitle> 8th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pp. </pages> <address> 207-218 Williamsburg, VA, </address> <month> (January </month> <year> 1981). </year>
Reference-contexts: This definition of data dependence is somewhat nonstandard since we consider transitive dependences involving flow-, anti-, and output-dependences <ref> [26, 38] </ref>, and do not expli citly state the variables involved. 3.1.2. Axioms A program execution is simply a notation for describing the execution of a shared-memory parallel program. Following Lamport, our model also contains several axioms that describe properties every program execution must possess.
Reference: [39] <author> Lamport, Leslie, </author> <title> ``How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs,'' </title> <journal> IEEE Trans. on Computers C-28(9) pp. </journal> <month> 690-691 (September </month> <year> 1979). </year>
Reference-contexts: Representing all of these behaviors is necessary for reasoning about race conditions and techniques for race detection. 3.1. Actual Behavior The first part of our model is simply a notation for representing the actual behavior of a shared-memory parallel program executing on a sequentially consistent multi-processor <ref> [39] </ref>. This part of the model contains objects that represent a program execution (such as which statements are executed and in what order) and axioms that characterize properties those objects must possess. <p> We use Lamport's theory, but restrict it to the class of shared-memory parallel programs that execute on multi-processors with sequentially consistent memory systems <ref> [39] </ref>. Sequential consistency ensures that shared-memory accesses behave as if they were all performed atomically and in some linear order (that is consistent with the order specified by the program).
Reference: [40] <author> Lamport, Leslie, </author> <title> ``Interprocess Communication,'' </title> <type> SRI Technical Report, </type> <month> (March </month> <year> 1985). </year>
Reference: [41] <author> Lamport, Leslie, </author> <title> ``The Mutual Exclusion Problem: Part I A Theory of Interprocess Communication,'' </title> <journal> Journal of the ACM 33(2) pp. </journal> <month> 313-326 (April </month> <year> 1986). </year>
Reference-contexts: Below, we first outline the basic model, and then describe the axioms for programs that use fork/join and general semaphores. Other types of synchronization are easily accommodated by including the appropriate axioms. 3.1.1. Basic Model The first part of our model is based on Lamport's theory of concurrent systems <ref> [41, 42] </ref>, which provides a formalism for reasoning about concurrent systems that does not assume the existence of atomic operations. In Lamport's formalism, a concurrent system execution is modeled as a collection of operation executions, which represent instances of operations performed by the system.
Reference: [42] <author> Lamport, Leslie, </author> <title> ``On Interprocess Communication, Part I: Basic Formalism,'' </title> <booktitle> Distributed Computing, </booktitle> <pages> pp. </pages> <month> 77-85 </month> <year> (1986). </year>
Reference-contexts: Below, we first outline the basic model, and then describe the axioms for programs that use fork/join and general semaphores. Other types of synchronization are easily accommodated by including the appropriate axioms. 3.1.1. Basic Model The first part of our model is based on Lamport's theory of concurrent systems <ref> [41, 42] </ref>, which provides a formalism for reasoning about concurrent systems that does not assume the existence of atomic operations. In Lamport's formalism, a concurrent system execution is modeled as a collection of operation executions, which represent instances of operations performed by the system.
Reference: [43] <author> Larus, James R., </author> <title> ``Abstract Execution: A Technique for Efficiently Tracing Programs,'' </title> <journal> Software Practice and Experience 20(12) pp. </journal> <month> 1241-1258 (December </month> <year> 1990). </year>
Reference-contexts: Another approach is to employ two passes. The first pass would record an incomplete, coarse trace containing only enough information to replay a portion of the execution <ref> [43, 44, 50] </ref>. The second pass would then re-execute parts of the execution to record the more detailed READ and WRITE sets.
Reference: [44] <author> LeBlanc, Thomas J. and John M. Mellor-Crummey, </author> <title> ``Debugging Parallel Programs with Instant Replay,'' </title> <journal> IEEE Trans. on Computers C-36(4) pp. </journal> <month> 471-482 (April </month> <year> 1987). </year>
Reference-contexts: Another approach is to employ two passes. The first pass would record an incomplete, coarse trace containing only enough information to replay a portion of the execution <ref> [43, 44, 50] </ref>. The second pass would then re-execute parts of the execution to record the more detailed READ and WRITE sets. <p> Program replay is one example of an area where race detection and debugging can cooperate. Naive replay schemes simply trace the value of every shared-memory access and restore that value during re-execution [59]. More efficient schemes record and reproduce only the temporal ordering <ref> [15, 44] </ref>, but these schemes fail in general to produce correct replay for executions containing actual data races.
Reference: [45] <author> Mattern, F., </author> <title> ``Virtual Time and Global States of Distributed Systems,'' pp. 215-226 in Parallel and Distributed Algorithms, </title> <editor> ed. Michel Cosnard, </editor> <publisher> North Holland (1989). </publisher> <pages> 107 </pages>
Reference-contexts: Locating unordered events requires analyzing G to decide whether a path connects two given nodes. Since this determination must be made many times, it should be as efficient as possible. We therefore preprocess the graph to compute a timestamp for each node <ref> [16, 27, 28, 33, 35, 45, 50] </ref>. Times-tamps provide a mechanism for quickly determining the ordering between two given events. Each timestamp is a vector (of length p) of event serial numbers 2 .
Reference: [46] <author> McDowell, Charles E. and William F. Appelbe, </author> <title> ``Minimizing the Complexity of Static Analysis of Parallel Programs,'' </title> <booktitle> 20th Annual Hawaii Intl. Conf. on System Sciences, </booktitle> <pages> pp. </pages> <month> 171-176 </month> <year> (1987). </year>
Reference-contexts: Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. This state space can either be constructed explicitly, by building a graph <ref> [3, 4, 46, 47, 52, 72, 74, 75] </ref>, or implicitly, by constructing a representation of the state space (such as a formal language or a petri-net)[5, 36, 67]. In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well.
Reference: [47] <author> McDowell, Charles E. and David P. Helmbold, </author> <title> ``Computing Reachable States of Parallel Programs,'' </title> <booktitle> ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> Santa Cruz, CA, </address> <month> (May </month> <year> 1991). </year>
Reference-contexts: Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. This state space can either be constructed explicitly, by building a graph <ref> [3, 4, 46, 47, 52, 72, 74, 75] </ref>, or implicitly, by constructing a representation of the state space (such as a formal language or a petri-net)[5, 36, 67]. In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well.
Reference: [48] <author> Mellor-Crummey, John M., </author> <title> ``Debugging and Analysis of Large-Scale Parallel Programs,'' </title> <type> Ph.D. Thesis, </type> <institution> also available as Computer Science Dept. </institution> <type> Tech. Rep. 312, </type> <institution> Univ. of Rochester, </institution> <month> (September </month> <year> 1989). </year>
Reference-contexts: This result can be proven by showing that the program's input and the single-access dependences uniquely determine these values <ref> [48] </ref>. Any temporal ordering that could still allow these dependences to occur (and that would not violate the semantics of the synchronization operations) is an ordering that the execution could have exhibited. <p> We will use the result mentioned above that any single-access program execution possessing the same (single-access) shared-data dependences as those that actually occurred represents an execution the program could exhibit <ref> [48] </ref>. This theorem extends the result to higher-level program executions. Since computation events in higher-level program executions can consist of more than one shared-memory access, there may be more than one single-access program execution for which P, the actual program execution, is a higher-level view.
Reference: [49] <author> Mellor-Crummey, John M., </author> <title> ``On-the-Fly Detection of Data Races for Programs with Nested Fork-Join Parallelism,'' </title> <booktitle> Supercomputing '91, </booktitle> <pages> pp. </pages> <address> 24-33 Albuquerque, NM, </address> <month> (November </month> <year> 1991). </year>
Reference-contexts: The thrust of previous work has been the development of efficient encoding schemes. Several schemes have been proposed, but most buffer only enough information to guarantee detection of one race involving each shared variable <ref> [17, 19-22, 37, 49, 64, 65] </ref>, leaving some races undetected (however, Choi and Min [17] propose a method, discussed further in Chapter 8, for re-executing the program to reproduce the undetected races). <p> However, the semantics of PCF Fortran dictate that exactly one send and one wait operation must be issued for each synchronization variable. Under this restriction, pairing send operations with wait operations is straightforward. In addition, Mellor-Crummey <ref> [49] </ref> presents a technique called Offset-Span Labeling for programs that use only fork/join. Offset-Span labeling is a technique for encoding the ordering graph that incurs lower space overhead than other methods. Steele [69] and Nudler and Rudolph [56] also present on-the-fly schemes for programs that use only fork/join.
Reference: [50] <author> Miller, Barton P. and Jong-Deok Choi, </author> <title> ``A Mechanism for Efficient Debugging of Parallel Programs,'' </title> <booktitle> SIG-PLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. </pages> <address> 135-144 Atlanta, GA, </address> <month> (June </month> <year> 1988). </year> <note> Also appears in SIGPLAN Notices 23(7) (July 1988). </note>
Reference-contexts: Data-hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 Some methods do not actually construct a node to represent a computation event but rather represent the event by an edge connecting the two surrounding synchronization events <ref> [50, 65] </ref>. 7 conflicting events are easily located by analyzing the READ and WRITE sets. Events that had the potential of executing concurrently can be located by analyzing the ordering graph. All previous methods search for pairs of events that are unordered by the graph. <p> After execution is complete, the final phase constructs and analyzes the ordering graph to detect races. Once the instrumentation phase is complete, the last two phases can be repeated for any number of executions. Most post-mortem methods detect data races <ref> [1, 2, 16, 50] </ref>; only one method detects general races [23-25]. In contrast, on-the-fly methods use only two phases. As with post-mortem methods, the first phase instruments the program to collect information during execution. <p> Our work on data race ordering also considers how events potentially affect one another, and we contrast it with Allen and Padua's work in Chapter 8. Miller and Choi <ref> [50] </ref> describe a parallel program debugger that incorporates post-mortem data race detection. They consider C programs that use fork/join and counting semaphores. <p> Bernstein's conditions state that atomic execution of a critical section is guaranteed if the shared variables it reads and modifies are not modified by any other concurrently executing section of code [9]. A violation of these conditions has typically been called a data race <ref> [2, 16, 17, 50, 54, 55] </ref>, access anomaly [20, 37, 51], or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals. <p> Locating unordered events requires analyzing G to decide whether a path connects two given nodes. Since this determination must be made many times, it should be as efficient as possible. We therefore preprocess the graph to compute a timestamp for each node <ref> [16, 27, 28, 33, 35, 45, 50] </ref>. Times-tamps provide a mechanism for quickly determining the ordering between two given events. Each timestamp is a vector (of length p) of event serial numbers 2 . <p> As with direct dependences, at most p transitive out-edges are ever added for any event, maintaining the O (np) space bound. 7.2.2. Computing Timestamps for Cyclic Graphs Previously, timestamps have only been used to represent connectivity in acyclic graphs <ref> [16, 33, 35, 50] </ref>, as was done in Chapter 6. However, because the augmented graphs may contain cycles, we now generalize the use of timestamps to cyclic graphs and show how to compute them for G E . <p> Following previous approaches <ref> [35, 50] </ref>, we pair each unlock operation with the subse quent lock on the same variable, and mutually pair all related barriers; we then have a T b if a is paired with b. <p> Another approach is to employ two passes. The first pass would record an incomplete, coarse trace containing only enough information to replay a portion of the execution <ref> [43, 44, 50] </ref>. The second pass would then re-execute parts of the execution to record the more detailed READ and WRITE sets. <p> Another area where race detection and debugging might be integrated is the use of data and control dependences to understand the program behavior. For example, debugging using flowback analysis attempts to follow data and control dependences backwards through the execution to understand the causal relationship between events <ref> [16, 50] </ref>. Such an analysis uses static information obtained by semantic analysis of the program to first locate potential dependences, and dynamic information obtained by coarse execution traces to locate actual dependences.
Reference: [51] <author> Min, Sang Lyul and Jong-Deok Choi, </author> <title> ``An Efficient Cache-based Access Anomaly Detection Scheme,'' </title> <booktitle> 4th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. </pages> <address> 235-244 Palo Alto, CA, </address> <month> (April </month> <year> 1991). </year>
Reference-contexts: On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> Steele [69] and Nudler and Rudolph [56] also present on-the-fly schemes for programs that use only fork/join. They conceptually construct the ordering graph in the same way as all other methods, and differ in the techniques used to encode the graph in memory. Choi and Min <ref> [17, 51] </ref> present techniques that address both the execution-time overhead of on-the-fly race detection and the problem of undetected races. They outline a hardware-based scheme for reducing the amount of execution-time overhead by using state information maintained by (a modified version of) the underlying cache coherence protocol [51]. <p> They outline a hardware-based scheme for reducing the amount of execution-time overhead by using state information maintained by (a modified version of) the underlying cache coherence protocol <ref> [51] </ref>. The resulting race detection method is essentially the same as that of Dinning and Schon-berg [19, 20]. <p> A violation of these conditions has typically been called a data race [2, 16, 17, 50, 54, 55], access anomaly <ref> [20, 37, 51] </ref>, or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals. Bank tellers make either deposits or withdrawals from the given bank account (we are assuming a small bank that has only a single customer).
Reference: [52] <author> Morgan, E. Timothy and Rami R. Razouk, </author> <title> ``Interactive State-Space Analysis of Concurrent Systems,'' </title> <journal> IEEE Trans on Software Engineering SE-13(10) pp. </journal> <month> 1080-1091 (October, </month> <year> 1987). </year>
Reference-contexts: Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. This state space can either be constructed explicitly, by building a graph <ref> [3, 4, 46, 47, 52, 72, 74, 75] </ref>, or implicitly, by constructing a representation of the state space (such as a formal language or a petri-net)[5, 36, 67]. In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well.
Reference: [53] <author> Netzer, Robert H. B. and Barton P. Miller, </author> <title> ``On the Complexity of Event Ordering for Shared-Memory Parallel Program Executions,'' </title> <booktitle> 1990 Intl. Conf. on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II-93-II-97 St. Charles, IL, </address> <month> (August </month> <year> 1990). </year>
Reference-contexts: Even though Post and Wait alone are insufficient to implement two-process mutual exclusion, the addition of Clear operations allows mutual exclusion to be implement ed <ref> [53] </ref>, making exact race detection NP-hard. 49 CHB SYNC ) or the apparent data races (by using the approximation to CCW SYNC ). Conservative approximations are useful for debugging since the absence of race reports indicates that the execution was race-free.
Reference: [54] <author> Netzer, Robert H. B. and Barton P. Miller, </author> <title> ``Improving the Accuracy of Data Race Detection,'' </title> <booktitle> 3rd ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. </pages> <address> 133-144 Williamsburg, VA, </address> <month> (April </month> <year> 1991). </year>
Reference-contexts: Bernstein's conditions state that atomic execution of a critical section is guaranteed if the shared variables it reads and modifies are not modified by any other concurrently executing section of code [9]. A violation of these conditions has typically been called a data race <ref> [2, 16, 17, 50, 54, 55] </ref>, access anomaly [20, 37, 51], or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals.
Reference: [55] <author> Netzer, Robert H. B. and Barton P. Miller, </author> <title> ``Detecting Data Races in Parallel Program Executions,'' pp. </title> <booktitle> 109-129 in Advances in Languages and Compilers for Parallel Processing, </booktitle> <editor> ed. A. Nicolau, D. Gelernter, T. Gross, and D. Padua, </editor> <publisher> MIT Press (1991). </publisher>
Reference-contexts: Bernstein's conditions state that atomic execution of a critical section is guaranteed if the shared variables it reads and modifies are not modified by any other concurrently executing section of code [9]. A violation of these conditions has typically been called a data race <ref> [2, 16, 17, 50, 54, 55] </ref>, access anomaly [20, 37, 51], or harmful shared-memory access [56]. We prefer the term data race. processes that process commands from bank teller terminals.
Reference: [56] <author> Nudler, Itzhak and Larry Rudolph, </author> <title> ``Tools for the Efficient Development of Efficient Parallel Programs,'' </title> <booktitle> 1st Israeli Conf. on Computer System Engineering, </booktitle> <year> (1988). </year>
Reference-contexts: On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> The testset (x) primitive increments the value of the synchronization variable x. An edge is drawn in the ordering graph from a doall node to the first node representing hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 2 Some methods store the inverse of the READ and WRITE sets <ref> [17, 19, 37, 56] </ref>. 9 each iteration of the loop. Similarly, an edge is drawn from the last node of each loop iteration to the end doall node corresponding to the end of the loop. <p> In addition, Mellor-Crummey [49] presents a technique called Offset-Span Labeling for programs that use only fork/join. Offset-Span labeling is a technique for encoding the ordering graph that incurs lower space overhead than other methods. Steele [69] and Nudler and Rudolph <ref> [56] </ref> also present on-the-fly schemes for programs that use only fork/join. They conceptually construct the ordering graph in the same way as all other methods, and differ in the techniques used to encode the graph in memory. <p> A violation of these conditions has typically been called a data race [2, 16, 17, 50, 54, 55], access anomaly [20, 37, 51], or harmful shared-memory access <ref> [56] </ref>. We prefer the term data race. processes that process commands from bank teller terminals. Bank tellers make either deposits or withdrawals from the given bank account (we are assuming a small bank that has only a single customer). Figure 4.1 (a) shows a correct version of the program.
Reference: [57] <author> Olender, Kurt M. and Leon J. Osterweil, ``Cecil: </author> <title> A Sequencing Constraint Language for Automatic Static Analysis Generation,'' </title> <journal> IEEE Trans. on Software Engineering 16(3) pp. </journal> <month> 268-280 (March </month> <year> 1990). </year>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report).
Reference: [58] <author> Osterweil, Leon, </author> <title> ``Integrating the Testing, Analysis and Debugging of Programs,'' </title> <booktitle> Software Validation, </booktitle> <pages> pp. </pages> <publisher> 73-93 Elsevier Science Publishers, </publisher> <year> (1984). </year>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report).
Reference: [59] <author> Pan, Douglas Z. and Mark A. Linton, </author> <title> ``Supporting Reverse Execution of Parallel Programs,'' </title> <booktitle> SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. </pages> <address> 124-129 Madison, WI, </address> <month> (May </month> <year> 1988). </year> <note> Also appears in SIGPLAN Notices 24(1) (January 1989). 108 </note>
Reference-contexts: Previous work on the efficient debugging of shared-memory parallel programs has not considered race detection in depth. Program replay is one example of an area where race detection and debugging can cooperate. Naive replay schemes simply trace the value of every shared-memory access and restore that value during re-execution <ref> [59] </ref>. More efficient schemes record and reproduce only the temporal ordering [15, 44], but these schemes fail in general to produce correct replay for executions containing actual data races.
Reference: [60] <author> Podgurski, Andy and Lori A. Clarke, </author> <title> ``A Formal Model of Program Dependences and Its Implications for Software Testing, Debugging, and Maintenance,'' </title> <journal> IEEE Trans. on Software Engineering 16(9) pp. </journal> <month> 965-979 (September </month> <year> 1990). </year>
Reference-contexts: We finally mention that our event-control dependences are similar to the hides relation used by Allen and Pa-dua [2, 24, 25] and the semantic dependences defined by Podgurski and Clarke <ref> [60, 61] </ref>. The hides relation is defined to show when the data computed by an event in one data race may have been used by an event in another race to determine either control flow or the shared locations accessed.
Reference: [61] <author> Podgurski, H. Andy, </author> <title> ``The Significance of Program Dependences for Software Testing, Debugging, and Maintenance,'' </title> <type> Ph.D. Thesis, </type> <institution> Univ. of Massachusetts, </institution> <month> (September </month> <year> 1989). </year>
Reference-contexts: We finally mention that our event-control dependences are similar to the hides relation used by Allen and Pa-dua [2, 24, 25] and the semantic dependences defined by Podgurski and Clarke <ref> [60, 61] </ref>. The hides relation is defined to show when the data computed by an event in one data race may have been used by an event in another race to determine either control flow or the shared locations accessed.
Reference: [62] <author> Reif, John H., </author> <title> ``Data Flow Analysis of Communicating Processes,'' </title> <booktitle> 6th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pp. </pages> <month> 257-268 </month> <year> (1979). </year>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report).
Reference: [63] <author> Schatz, Emmi and Barbara G. Ryder, </author> <title> ``Directed Tracing to Detect Race Conditions,'' </title> <type> LCSR Tech. Rep. 155, </type> <institution> Rutgers Univ., </institution> <month> (October </month> <year> 1990). </year>
Reference-contexts: Static analysis can sometimes rule out the possibility of races between some sections of the program, precluding the need for tracing these program 5 6 sections for dynamic analysis [2, 23-25, 75]. Static analysis can also compute input data that might manifest a race <ref> [63] </ref>, allowing dynamic analysis to attempt to verify the existence of that race. 2.2. Dynamic Analysis Unlike static analysis, dynamic analysis detects the race conditions exhibited by a particular execution of the program. Dynamic analysis is especially useful for debugging since precise information about manifestations of particular bugs is available.
Reference: [64] <author> Schonberg, Edith, </author> <title> ``On-The-Fly Detection of Access Anomalies,'' Ultracomputer Note #149, </title> <month> (October </month> <year> 1988). </year>
Reference-contexts: The thrust of previous work has been the development of efficient encoding schemes. Several schemes have been proposed, but most buffer only enough information to guarantee detection of one race involving each shared variable <ref> [17, 19-22, 37, 49, 64, 65] </ref>, leaving some races undetected (however, Choi and Min [17] propose a method, discussed further in Chapter 8, for re-executing the program to reproduce the undetected races). <p> On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> To draw edges between nodes representing semaphore operations, they pair the i th V operation on each semaphore with the i th P operation on the same semaphore. An edge is then drawn from each V node to the P node with which it is paired. Dinning and Schonberg <ref> [19, 20, 64, 65] </ref> describe on-the-fly techniques for detecting data races in executions of programs using fork/join and arbitrary synchronization (such as barriers, events, Ada rendezvous, and semaphores).
Reference: [65] <author> Schonberg, Edith, </author> <title> ``On-The-Fly Detection of Access Anomalies,'' </title> <booktitle> SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. </pages> <address> 285-297 Portland, OR, </address> <month> (July </month> <year> 1989). </year> <note> Also appears in SIGPLAN Notices 24(7) (July 1989). </note>
Reference-contexts: Data-hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 Some methods do not actually construct a node to represent a computation event but rather represent the event by an edge connecting the two surrounding synchronization events <ref> [50, 65] </ref>. 7 conflicting events are easily located by analyzing the READ and WRITE sets. Events that had the potential of executing concurrently can be located by analyzing the ordering graph. All previous methods search for pairs of events that are unordered by the graph. <p> The thrust of previous work has been the development of efficient encoding schemes. Several schemes have been proposed, but most buffer only enough information to guarantee detection of one race involving each shared variable <ref> [17, 19-22, 37, 49, 64, 65] </ref>, leaving some races undetected (however, Choi and Min [17] propose a method, discussed further in Chapter 8, for re-executing the program to reproduce the undetected races). <p> On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> To draw edges between nodes representing semaphore operations, they pair the i th V operation on each semaphore with the i th P operation on the same semaphore. An edge is then drawn from each V node to the P node with which it is paired. Dinning and Schonberg <ref> [19, 20, 64, 65] </ref> describe on-the-fly techniques for detecting data races in executions of programs using fork/join and arbitrary synchronization (such as barriers, events, Ada rendezvous, and semaphores).
Reference: [66] <author> Sedgewick, Robert, </author> <title> Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA (1983). </address>
Reference-contexts: The topological sort can be performed in time O (n +e ), where n is the number of nodes <ref> [66] </ref>, and is dominated by the rest of the computation. In the worst case, the number of edges in the graph is of order n; all existing race detection methods add edges only between pairs of synchronization operations on the same variable, or between fork/join nodes and their children. <p> its nodes that is a global-time model defining a temporal hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 A strongly connected component is a maximal cycle; there is path from every node in the component to every other node, but no path from a node in one component to a node in another component and back <ref> [66] </ref>. 61 ordering relation T such that a / T b. By the definition of G, T obeys axioms (A4)-(A6) (since G con tains no more edges than G D ACTUAL). <p> Compute a topological order for G E hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 73 computes the topological order by a generalization of the standard depth-first-search approach to topological sorting <ref> [66] </ref>. When the depth-first search this algorithm performs visits a node in a component, it avoids visiting any other node in the component by marking them all visited (line 12), and then follows out-edges from all nodes in the component (lines 13-14). <p> The resulting topological order contains one representative node from each component. The running time of Algorithm 7.6 is O (n + e), the same as for standard topological sorting. Each node is visited (and each edge is followed) exactly once, and locating all the strongly connected components <ref> [66] </ref> only requires O (n + e) time. Since Algorithm 7.5 operates in the same way as for Algorithm 6.1, its running time is also the same, O (ep).
Reference: [67] <author> Shatz, S. M. and W. K. Cheng, </author> <title> ``An Approach to Automated Static Analysis of Distributed Software,'' </title> <booktitle> 1st Intl. Conf. on Supercomputing Systems, </booktitle> <pages> pp. </pages> <month> 377-385 </month> <year> (1985). </year>
Reference: [68] <author> Silberschatz, Abraham, James L. Peterson, and Peter B. Galvin, </author> <title> Operating System Concepts, </title> <publisher> 3rd ed., Addison-Wesley (1991). </publisher>
Reference-contexts: For this proof, we perform the reduction by constructing this program without using explicit synchronization at all, but instead implement two-process mutual exclusion using only shared variables (using Peterson's trick <ref> [68] </ref>, for example). For a given execution, P, of this program, the set F DIFF contains all other executions that perform a prefix of the same events, regardless of how their shared-data dependences differ from those of P.
Reference: [69] <author> Steele, Guy L., </author> <title> ``Making Asynchronous Parallelism Safe for the World,'' </title> <booktitle> 17th Annual ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pp. </pages> <address> 218-231 San Francisco, CA, </address> <month> (January </month> <year> 1990). </year>
Reference-contexts: On-the-fly methods have primarily been used to detect data races <ref> [17, 19, 20, 22, 37, 51, 56, 64, 65, 69] </ref>, although they can be used to detect general races for programs that use no synchronization other than fork/join. <p> Under this restriction, pairing send operations with wait operations is straightforward. In addition, Mellor-Crummey [49] presents a technique called Offset-Span Labeling for programs that use only fork/join. Offset-Span labeling is a technique for encoding the ordering graph that incurs lower space overhead than other methods. Steele <ref> [69] </ref> and Nudler and Rudolph [56] also present on-the-fly schemes for programs that use only fork/join. They conceptually construct the ordering graph in the same way as all other methods, and differ in the techniques used to encode the graph in memory.
Reference: [70] <author> Taylor, Richard N. and Leon J. Osterweil, </author> <title> ``A Facility for Verification, Testing, and Documentation of Concurrent Process Software,'' </title> <booktitle> IEEE COMPSAC '78, </booktitle> <pages> pp. </pages> <month> 36-41 </month> <year> (1978). </year>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report).
Reference: [71] <author> Taylor, Richard N. and Leon J. Osterweil, </author> <title> ``Anomaly Detection in Concurrent Software by Static Data Flow Analysis,'' </title> <journal> IEEE Trans on Software Engineering SE-6(3) pp. </journal> <month> 265-277 (May </month> <year> 1980). </year>
Reference-contexts: In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. Second, other static analysis methods perform a data-flow analysis of the program to discover potential event orderings <ref> [6, 10, 11, 13, 57, 58, 62, 70, 71] </ref>. These methods have polynomial time and space complexity, but are less accurate than the state-space methods, sometimes reporting races that the program could never exhibit (and that the state-space methods would never report).
Reference: [72] <author> Taylor, Richard N., </author> <title> ``Static Analysis of the Synchronization Structure of Concurrent Programs,'' </title> <type> Ph.D. Thesis, </type> <institution> Univ. of Colorado at Boulder, </institution> <year> (1980). </year>
Reference-contexts: Taylor showed that, for Ada programs containing no branches or conditionally executed code, the problem of computing the ordering information is NP-complete <ref> [72, 73] </ref>. Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. <p> Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. This state space can either be constructed explicitly, by building a graph <ref> [3, 4, 46, 47, 52, 72, 74, 75] </ref>, or implicitly, by constructing a representation of the state space (such as a formal language or a petri-net)[5, 36, 67]. In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. <p> Although some implementations also provide a Clear primitive, which resets the synchronization variable, we are considering the use of only Post and Wait. 3 This reduction was motivated by the ones Taylor <ref> [72, 73] </ref> constructed to prove that certain static analysis problems are NP-complete. 43 P (mutex i ) P (mutex i ) V (mutex i ) V (X i ) V (X i ) P (Pass 2) . . V (mutex i ) . .
Reference: [73] <author> Taylor, Richard N., </author> <title> ``Complexity of Analyzing the Synchronization Structure of Concurrent Programs,'' </title> <note> Acta Informatica 19 pp. </note> <month> 57-84 </month> <year> (1983). </year>
Reference-contexts: Taylor showed that, for Ada programs containing no branches or conditionally executed code, the problem of computing the ordering information is NP-complete <ref> [72, 73] </ref>. Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. <p> Although some implementations also provide a Clear primitive, which resets the synchronization variable, we are considering the use of only Post and Wait. 3 This reduction was motivated by the ones Taylor <ref> [72, 73] </ref> constructed to prove that certain static analysis problems are NP-complete. 43 P (mutex i ) P (mutex i ) V (mutex i ) V (X i ) V (X i ) P (Pass 2) . . V (mutex i ) . .
Reference: [74] <author> Taylor, Richard N., </author> <title> ``A General-Purpose Algorithm for Analyzing Concurrent Programs,'' </title> <journal> Communications of the ACM 26(5) pp. </journal> <month> 362-376 (May </month> <year> 1983). </year>
Reference-contexts: Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. This state space can either be constructed explicitly, by building a graph <ref> [3, 4, 46, 47, 52, 72, 74, 75] </ref>, or implicitly, by constructing a representation of the state space (such as a formal language or a petri-net)[5, 36, 67]. In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well.
Reference: [75] <author> Taylor, Richard N., </author> <title> ``Analysis of Concurrent Software by Cooperative Application of Static and Dynamic Techniques,'' </title> <booktitle> Software Validation, </booktitle> <pages> pp. </pages> <publisher> 127-137 Elsevier Science Publishers, </publisher> <year> (1984). </year> <month> 109 </month>
Reference-contexts: Given this NP-completeness result, two different approaches to static analysis have been developed. First, some methods traverse the space of all possible states that the program may enter. This state space can either be constructed explicitly, by building a graph <ref> [3, 4, 46, 47, 52, 72, 74, 75] </ref>, or implicitly, by constructing a representation of the state space (such as a formal language or a petri-net)[5, 36, 67]. In the general case, these methods have exponential time complexity, and in some cases, exponential space complexity as well. <p> Static analysis has also been used to complement dynamic methods. Static analysis can sometimes rule out the possibility of races between some sections of the program, precluding the need for tracing these program 5 6 sections for dynamic analysis <ref> [2, 23-25, 75] </ref>. Static analysis can also compute input data that might manifest a race [63], allowing dynamic analysis to attempt to verify the existence of that race. 2.2. Dynamic Analysis Unlike static analysis, dynamic analysis detects the race conditions exhibited by a particular execution of the program.
Reference: [76] <author> Wolfe, Michael J., </author> <title> ``Optimizing Supercompilers for Supercomputers,'' </title> <type> Ph.D. Thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> (1982). </year>
Reference-contexts: Typically, the parallelized version is intended to have the same semantics as the sequential version. Preserving these semantics can be accomplished by adding synchronization to the program that ensures all of the data dependences ever exhibited by the sequential version are also exhibited by the parallel version <ref> [76] </ref>. Such programs exhibit no general races, since preserving these dependences requires that all operations on any specific location are performed in some specific order (independent of external timing variations). 4.2.2.

References-found: 76

