URL: http://www.is.cs.cmu.edu/papers/multimodal/93.chi.ps.gz
Refering-URL: http://www.cs.cmu.edu:80/afs/cs.cmu.edu/user/tue/WWW/resume.html
Root-URL: 
Phone: 10 8 50  
Title: set of manually-classified gestures using a modified backprop-agation algorithm [3]. The output neuron with the
Author: bard W. Waibel A., Hanazawa T., Hinton G., Shikano K., and Lang K. . Ward, W. . Zeppenfeld, T. and Waibel, A. 
Date: (May 1989),  (Mar. 1992).  
Note: 34  REFERENCES  In Proc. CHI89  ACM Press, pp. 241-245. 3.  In Proc. ICASSP91 (May 1991), pp.  
Pubnum: ICASSP92  
Abstract: Our gesture recognizer achieves 98.9% recognition rate on the training data set (640 samples) and 98.8% on an independent test set (160 samples). SPEECH PROCESSOR The speech processing subsystem of our multi-modal text editor consists of a word spotter and a semantic-fragment parser. Word Spotter This initial version of the text editor requires only a small vocabulary, hence a word spotter was deemed more appropriate than a full speech recognition system. Instead of trying to recognize all parts of an input utterance, the word spotter only signals occurrences of predefined keywords within the utterance. The word spotter used in our system was developed at Carnegie Mellon by Zeppenfeld, based on the Multi-State Time Delay Neural Network (MS-TDNN), an extension of the standard TDNN architecture. More details on architecture, implementation, and performance evaluation of the word spotter can be found in [5]. For our editing task, the word spotter was trained on a single-speaker speech database that includes about 45 instances of each of 11 keywords: delete, move, transpose, paste, split, character, word, line, sentence, paragraph, and selection. The word spotter achieves a recognition performance of 95.9% on the training data set. Semantic-Fragment Parser The output of the word spotter is a text string consisting of keywords occurring in the input utterance. This can be regarded as a machine-transcribed version of the input in which only essential words are retained. For instance, Please delete this word for me produces delete word. This simplified version is then parsed using a semantic-fragment grammar. The parser, developed by Ward [4], matches fragments of the input text against predefined templates to find semantically useful parts of the text. It then creates a frame consisting of slots representing various components of a plausible semantic interpretation, and fills in any slot it can using semantic fragments found in the input sentence. In the case of our text editor, the grammar defines two slots: action and scope. For the above example, the sentence delete word will cause the action slot to be filled with delete, and the scope slot to be filled with word. 8 We based the interpretation of multi-modal inputs on frames. As explained above, a frame consists of slots representing parts of an interpretation. In our case, there are three slots named action, source-scope, and destination-scope (the destination is used only for the move command). Within each scope slot are subslots named type and unit. The possible scope types are: point (specified by coordinates), box (specified by coordinates of opposite corners), and selection (i.e. currently highlighted text). The unit subslot specifies the unit of text to be operated on, e.g. character or word. Consider an example in which a user draws a circle and says Please delete this word. The gesture-processing subsystem recognizes the circle and fills in the coordinates of the box scope specified by the circle in the gesture frame. The word spotter produces delete word, which causes the parser to fill the action slot with delete and the unit subslot of source-scope with word. The frame merger then produces a unified frame in which action=delete, source-scope has unit=word and type=- box with coordinates as specified by the drawn circle. From this the command interpreter constructs an editing command to delete the word circled by the user. One important advantage of this frame-based approach is its exibility, which will facilitate the integration of more than two modalities. All we have to do is define a general frame for interpretation and specify the ways in which slots can be filled in by each input modality. In a general implementation, it is possible that the slots may be filled in different ways, and performing a search to find the best merging would be superior. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Guyon I., Albrecht P., LeCun Y., Denker J., and Hub bard W. </author> <title> Design of a Neural Network Character Recog nizer for a Touch Terminal. </title> <journal> Pattern Recognition, </journal> <year> 1990. </year>
Reference: 2. <author> Hauptmann, </author> <title> A.G. Speech and Gestures for Graphic Image Manipulation. </title> <booktitle> In Proc. </booktitle> <address> CHI89 (May 1989), </address> <publisher> ACM Press, </publisher> <pages> pp. 241-245. </pages>
Reference: 3. <author> Waibel A., Hanazawa T., Hinton G., Shikano K., and Lang K. </author> <title> Phoneme Recognition Using Time-Delay Neu ral Networks. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <month> Mar. </month> <year> 1989. </year>
Reference: 4. <author> Ward, W. </author> <title> Understanding Spontaneous Speech: </title> <booktitle> the Phoenix System. In Proc. </booktitle> <month> ICASSP91 (May </month> <year> 1991), </year> <pages> pp. 365-367. </pages>

References-found: 4

