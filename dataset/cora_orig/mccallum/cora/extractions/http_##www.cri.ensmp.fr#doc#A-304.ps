URL: http://www.cri.ensmp.fr/doc/A-304.ps
Refering-URL: http://www.cri.ensmp.fr/rapports.html
Root-URL: 
Phone: 3  
Title: Automatic Data Mapping of Signal Processing Applications  
Author: Corinne Ancourt Denis Barthou Christophe Guettier Fran~cois Irigoin Bertrand Jeannet Jean Jourdan Juliette Mattioli 
Keyword: parallelizing compiler, scheduling, constraint logic programming  
Address: Paris/CRI 77305 Fontainebleau, France 2 &lt;Denis.Barthou@prism.uvsq.fr&gt; PRISM, UVSQ, 45, avenue des Etats-Unis 78035 Versailles, France  beville, F-91404 ORSAY, France  
Affiliation: Ecole des Mines de  &lt;guettier,jeannet,jourdan,juliette.@thomson-lcr.fr&gt; LCR, Thomson-CSF, Domaine de Cor  
Note: 1: Introduction 1 &lt;ancourt,irigoin@cri.ensmp.fr&gt;  
Abstract: This paper presents a technique to map automatically a complete digital signal processing (DSP) application onto a parallel machine with distributed memory. Unlike other applications where coarse or medium grain scheduling techniques can be used, DSP applications integrate several thousand of tasks and hence necessitate fine grain considerations. Moreover finding an effective mapping imperatively require to take into account both architectural resources constraints and real time constraints. The main contribution of this paper is to show how it is possible to handle and to solve data partitioning, and fine-grain scheduling under the above operational constraints using Concurrent Constraints Logic Programming languages (CCLP). Our concurrent resolution technique undertaking linear and non linear constraints takes advantage of the special features of signal processing applications and provides a solution equivalent to a manual solution for the representative Panoramic Analysis (PA) application. The post World War II era has resulted in the trend of using Digital Signal Processing (DSP) technologies for both military and civilian applications. The growing requirements for sophisticated algorithms, especially those used for 3-D applicative domains, lead to process in real time large multi-dimensional arrays of data. These applications are executed on parallel computers, that offer enough computing power [25]. The mapping of DSP applications onto parallel machines raises new problems. The real time and target machine constraints are imperative. The solution must fit the available hardware: the local memory, the number of processors, the processor communications. The application latency must meet the real time requirements. This necessitates fine-grain optimizations. Combining both kinds of constraints is still out of the scope of automation and requires deep human skills. This paper presents a new technique to map automatically DSP application, represented by a sequence of loop nests, onto a SPMD distributed memory machine. This technique is based on formalizations of the architectural, applicative and mapping models by constraints. The result is (1) a fine grain affine schedule of computations, (2) their distribution onto processors and (3) a memory allocation. Computations are distributed in a block-cyclic 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: These times have to be compared with human being inquiries to comprehend and map the application. 6: Related Work Mapping applications onto parallel machines addresses issues such as scheduling [10], parallelization <ref> [1] </ref>, loop transformations [29, 4, 38], parallel languages [35, 28, 3], integer linear programming and machine architecture.
Reference: [2] <author> J.M. Anderson and M.S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In SIGPLAN Conf on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <address> Albuquerque, NM, June 1993. </address> <publisher> ACM Press. </publisher>
Reference-contexts: A lot of work has been done to optimize a few criteria such as data and/or computation distribution [40, 21, 34, 7, 43], parallelism detection, minimization of communications <ref> [18, 2, 41, 5] </ref>, processor network usage. This section focuses on the most relevant work. Although manual loop transformation techniques are attractive and give good results, it is not possible to find automatically the transformation set to apply for obtaining the optimal schedule [33, 9].
Reference: [3] <author> Fran~coise Andre, J.-L. Pazat, and Henry Thomas. </author> <title> Pandore: a system to manage data distribution. </title> <booktitle> In Int. Conf. on Supercomputing, </booktitle> <pages> pages 380-388, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: These times have to be compared with human being inquiries to comprehend and map the application. 6: Related Work Mapping applications onto parallel machines addresses issues such as scheduling [10], parallelization [1], loop transformations [29, 4, 38], parallel languages <ref> [35, 28, 3] </ref>, integer linear programming and machine architecture. A lot of work has been done to optimize a few criteria such as data and/or computation distribution [40, 21, 34, 7, 43], parallelism detection, minimization of communications [18, 2, 41, 5], processor network usage.
Reference: [4] <author> U. Banerjee. </author> <title> Unimodular transformations of do loops. </title> <type> Technical Report CSRD Rpt. No. 1036, </type> <institution> Uni--versity of Illinois, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: These times have to be compared with human being inquiries to comprehend and map the application. 6: Related Work Mapping applications onto parallel machines addresses issues such as scheduling [10], parallelization [1], loop transformations <ref> [29, 4, 38] </ref>, parallel languages [35, 28, 3], integer linear programming and machine architecture. A lot of work has been done to optimize a few criteria such as data and/or computation distribution [40, 21, 34, 7, 43], parallelism detection, minimization of communications [18, 2, 41, 5], processor network usage.
Reference: [5] <author> D. Bau, I. Kodukula, K. Pingali, and P. Stodghill. </author> <title> Solving alignment using elementary linear algebra. </title> <booktitle> In Proc. of the seventh Annual Workshop on Languages and Compilers for Parallelism, </booktitle> <pages> pages 4.1-4.15, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: A lot of work has been done to optimize a few criteria such as data and/or computation distribution [40, 21, 34, 7, 43], parallelism detection, minimization of communications <ref> [18, 2, 41, 5] </ref>, processor network usage. This section focuses on the most relevant work. Although manual loop transformation techniques are attractive and give good results, it is not possible to find automatically the transformation set to apply for obtaining the optimal schedule [33, 9].
Reference: [6] <author> S. S. Bhattacharyya, S. Sriram, and E. A. Lee. </author> <title> Latency-constrained resynchronisation for multiprocessor dsp implementation. </title> <booktitle> In Proceedings of ASAP'96, </booktitle> <year> 1996. </year>
Reference-contexts: Mapping statically DSP application with specific signal requirements [27, 49] have been widely investigated. The representative Ptolemy framework [39, 47, 44] brings some solution but at a coarse grain level. Most of the resolution schemes are based on dedicated algorithms <ref> [6] </ref>.
Reference: [7] <author> E. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In Proc. of the International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: A lot of work has been done to optimize a few criteria such as data and/or computation distribution <ref> [40, 21, 34, 7, 43] </ref>, parallelism detection, minimization of communications [18, 2, 41, 5], processor network usage. This section focuses on the most relevant work.
Reference: [8] <author> M. </author> <type> Bouvet. </type> <institution> Traitements des Signaux Pour les Systemes Sonars. Masson. </institution>
Reference-contexts: Each loop nest includes a procedure call (called macro-instruction) that reads one or several multidimensional data arrays and updates one different array. Array accesses are affine functions of loop indices with eventual modulo. Figure 1 presents a global view of PA application <ref> [8] </ref>. Figure 2 details the first PA loop nest. Parallelism. Since the application is in a single-assignment form, each loop nest is full parallel. Furthermore, the loops are perfectly nested. Macro-instructions can be seen as black boxes where computational dependencies are encapsulated.
Reference: [9] <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: This section focuses on the most relevant work. Although manual loop transformation techniques are attractive and give good results, it is not possible to find automatically the transformation set to apply for obtaining the optimal schedule <ref> [33, 9] </ref>. However restructuring the application such that the parallelism and data locality are maximized is yet a relevant objective. Many studies [9, 41, 48] present interesting approaches. Thereafter, the compiler is in charge of mapping physically the optimized application of the target machine. <p> However restructuring the application such that the parallelism and data locality are maximized is yet a relevant objective. Many studies <ref> [9, 41, 48] </ref> present interesting approaches. Thereafter, the compiler is in charge of mapping physically the optimized application of the target machine. Compared to our approach, there is no real time and architectural constraints (number of processors and memory resources) to take into account during the parallelization phase.
Reference: [10] <author> P. Clauss, C. Mongenet, and G.-R. Perrin. </author> <title> Synthesis of size-optimal torodal arrays for the algebraic path problem: A new contribution. Parallel Computing, </title> <journal> North-Holand, </journal> <volume> 18 </volume> <pages> 185-194, </pages> <year> 1992. </year>
Reference-contexts: These times have to be compared with human being inquiries to comprehend and map the application. 6: Related Work Mapping applications onto parallel machines addresses issues such as scheduling <ref> [10] </ref>, parallelization [1], loop transformations [29, 4, 38], parallel languages [35, 28, 3], integer linear programming and machine architecture.
Reference: [11] <author> P. Codognet, F. Fages, J.Jourdan, R. Lissajoux, and T. Sola. </author> <title> On the design of meta(f) and its application to air traffic control. </title> <booktitle> In Proc. </booktitle> <address> ICLP'92, Washington DC, USA, </address> <year> 1992. </year>
Reference-contexts: From an operational standpoint, they are based on constraint solving, constraint entailment and arithmetic reasoning. Going in deeper details on CCLP is out of the scope of this paper but we have used these new capabilities to extend our CCLP languages Meta (F) <ref> [11] </ref> in order to solve efficiently polynomial constraints over finite domain variables.
Reference: [12] <author> Beatrice Creusillet. </author> <title> Array Region Analyses and Applications. </title> <type> PhD thesis, </type> <institution> Ecole des Mines de Paris, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: The computational recurrence extraction from the application puts forward a cyclic schedule of a finite amount of computations. Then, classical parallelization techniques can be used. DSP applications manipulate array references that can be represented by Read and Write regions <ref> [50, 12] </ref>. Read and Write regions represent, with affine constraints [13], the set of array elements read and written by the macro-instruction. Figure 2 gives the FFT read and write regions.
Reference: [13] <author> Beatrice Creusillet and Fran~cois Irigoin. </author> <title> Interprocedural array region analyses. </title> <journal> International Journal of Parallel Programming (special issue on LCPC), </journal> <volume> 24(6) </volume> <pages> 513-546, </pages> <year> 1996. </year>
Reference-contexts: The computational recurrence extraction from the application puts forward a cyclic schedule of a finite amount of computations. Then, classical parallelization techniques can be used. DSP applications manipulate array references that can be represented by Read and Write regions [50, 12]. Read and Write regions represent, with affine constraints <ref> [13] </ref>, the set of array elements read and written by the macro-instruction. Figure 2 gives the FFT read and write regions.
Reference: [14] <author> A. Darte and Y. Robert. </author> <title> Constructive methods for scheduling uniform loop nests. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(8):814, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: Thereafter, the compiler is in charge of mapping physically the optimized application of the target machine. Compared to our approach, there is no real time and architectural constraints (number of processors and memory resources) to take into account during the parallelization phase. Similar techniques are used in systolic arrays <ref> [16, 17, 14] </ref> and parallelization [23, 22, 26] communities to compute affine schedules. In the systolic community, these techniques are applied on a single loop nest with complex internal dependencies. The other approaches dealing with complete applications, do not have the same architectural and application constraints.
Reference: [15] <author> Alain Darte, Leonid Khachiyan, and Yves Ropbert. </author> <title> Linear scheduling is nearly optimal. </title> <booktitle> In Parallel Processing Letters, </booktitle> <pages> pages 73-81, </pages> <year> 1991. </year>
Reference-contexts: The resulting schedule can be viewed as a succession of loop transformations. In general, it is not possible to find automatically the transformation set to apply such that the final schedule is optimal. So, the affine scheduling approach, used in systolic arrays and parallelization techniques <ref> [23, 22, 15, 16, 17] </ref>, is chosen and applied to our context. The partitioning model states that computations having to be scheduled (called computational block) are the set of L pipelined local iterations mapped onto p at cycle c.
Reference: [16] <author> Alain Darte and Yves Robert. </author> <title> Affine-by-statement scheduling of uniform loop nests over parametric domains. </title> <type> Technical Report 92-16, </type> <institution> LIP-IMAG, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The resulting schedule can be viewed as a succession of loop transformations. In general, it is not possible to find automatically the transformation set to apply such that the final schedule is optimal. So, the affine scheduling approach, used in systolic arrays and parallelization techniques <ref> [23, 22, 15, 16, 17] </ref>, is chosen and applied to our context. The partitioning model states that computations having to be scheduled (called computational block) are the set of L pipelined local iterations mapped onto p at cycle c. <p> Thereafter, the compiler is in charge of mapping physically the optimized application of the target machine. Compared to our approach, there is no real time and architectural constraints (number of processors and memory resources) to take into account during the parallelization phase. Similar techniques are used in systolic arrays <ref> [16, 17, 14] </ref> and parallelization [23, 22, 26] communities to compute affine schedules. In the systolic community, these techniques are applied on a single loop nest with complex internal dependencies. The other approaches dealing with complete applications, do not have the same architectural and application constraints.
Reference: [17] <author> Alain Darte and Yves Robert. </author> <title> Mapping uniform loop nests onto distributed memory architectures. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 679-710, </pages> <year> 1994. </year>
Reference-contexts: The resulting schedule can be viewed as a succession of loop transformations. In general, it is not possible to find automatically the transformation set to apply such that the final schedule is optimal. So, the affine scheduling approach, used in systolic arrays and parallelization techniques <ref> [23, 22, 15, 16, 17] </ref>, is chosen and applied to our context. The partitioning model states that computations having to be scheduled (called computational block) are the set of L pipelined local iterations mapped onto p at cycle c. <p> Thereafter, the compiler is in charge of mapping physically the optimized application of the target machine. Compared to our approach, there is no real time and architectural constraints (number of processors and memory resources) to take into account during the parallelization phase. Similar techniques are used in systolic arrays <ref> [16, 17, 14] </ref> and parallelization [23, 22, 26] communities to compute affine schedules. In the systolic community, these techniques are applied on a single loop nest with complex internal dependencies. The other approaches dealing with complete applications, do not have the same architectural and application constraints.
Reference: [18] <author> C. G. Diderich and M. Gengler. </author> <title> Solving the constant-degree parallelism alignment problem. In Eu-ropar'96. </title> <institution> Laboratoire d'Informatique du Parallelisme, </institution> <month> August 96. </month>
Reference-contexts: A lot of work has been done to optimize a few criteria such as data and/or computation distribution [40, 21, 34, 7, 43], parallelism detection, minimization of communications <ref> [18, 2, 41, 5] </ref>, processor network usage. This section focuses on the most relevant work. Although manual loop transformation techniques are attractive and give good results, it is not possible to find automatically the transformation set to apply for obtaining the optimal schedule [33, 9].
Reference: [19] <author> M. Dincbas, P. Van Hentenryck, H. Simonis, A. Aggoun, T.Graf, and F. Berthier. </author> <title> The constraint logic programming language chip. </title> <booktitle> In International Conference on Fifth Generation Computer System, </booktitle> <address> Tokyo, Japan, </address> <month> December </month> <year> 1988. </year>
Reference-contexts: Solving directly both constraints is still out of the scope of any general algorithms and necessitates the combination of integer programming and search [24]. Following the same idea of combining constraints solving and nondeterminism, our technique uses a new approach: the CCLP <ref> [19, 53] </ref> approach. Unlike conventional constraint solvers based on black box algorithms, CCLP languages use an incomplete constraint solvers over a finite domains algebra.
Reference: [20] <author> M. Dincbas, H. Simonis, P. Van Hentenryck, A. Aggoun, T. Graf, and F. Berthier. </author> <title> The constraint logic programming language chip. </title> <booktitle> In fifth Generation Computer Systems conference, </booktitle> <address> Tokyo, Japan, </address> <month> Dec. </month> <year> 1988. </year>
Reference-contexts: These domains include linear rational arithmetics, boolean algebra, Presburger arithmetics and finite domains <ref> [20] </ref>. More recently the introduction of the notion of constraint entailment, stemming from the Ask & Tell paradigm of concurrent programming [46], enhanced the CCLP framework with synchronism mechanisms.
Reference: [21] <author> P. Feautrier. </author> <title> Toward automatic distribution. </title> <journal> Parallel Processing Letters, </journal> <volume> 4(3) </volume> <pages> 233-244, </pages> <year> 1994. </year>
Reference-contexts: A lot of work has been done to optimize a few criteria such as data and/or computation distribution <ref> [40, 21, 34, 7, 43] </ref>, parallelism detection, minimization of communications [18, 2, 41, 5], processor network usage. This section focuses on the most relevant work.
Reference: [22] <author> Paul Feautrier. </author> <title> Some efficient solution to the affine scheduling problem, II, multidimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(6) </volume> <pages> 389-420, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: The resulting schedule can be viewed as a succession of loop transformations. In general, it is not possible to find automatically the transformation set to apply such that the final schedule is optimal. So, the affine scheduling approach, used in systolic arrays and parallelization techniques <ref> [23, 22, 15, 16, 17] </ref>, is chosen and applied to our context. The partitioning model states that computations having to be scheduled (called computational block) are the set of L pipelined local iterations mapped onto p at cycle c. <p> Compared to our approach, there is no real time and architectural constraints (number of processors and memory resources) to take into account during the parallelization phase. Similar techniques are used in systolic arrays [16, 17, 14] and parallelization <ref> [23, 22, 26] </ref> communities to compute affine schedules. In the systolic community, these techniques are applied on a single loop nest with complex internal dependencies. The other approaches dealing with complete applications, do not have the same architectural and application constraints.
Reference: [23] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, I, one dimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(5) </volume> <pages> 313-348, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The resulting schedule can be viewed as a succession of loop transformations. In general, it is not possible to find automatically the transformation set to apply such that the final schedule is optimal. So, the affine scheduling approach, used in systolic arrays and parallelization techniques <ref> [23, 22, 15, 16, 17] </ref>, is chosen and applied to our context. The partitioning model states that computations having to be scheduled (called computational block) are the set of L pipelined local iterations mapped onto p at cycle c. <p> Compared to our approach, there is no real time and architectural constraints (number of processors and memory resources) to take into account during the parallelization phase. Similar techniques are used in systolic arrays [16, 17, 14] and parallelization <ref> [23, 22, 26] </ref> communities to compute affine schedules. In the systolic community, these techniques are applied on a single loop nest with complex internal dependencies. The other approaches dealing with complete applications, do not have the same architectural and application constraints.
Reference: [24] <author> Paul Feautrier. </author> <title> Fine-grain scheduling under resource constraints. </title> <booktitle> In 7th Workshop on Language and Compiler for Parallel Computers, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: While data dependence constraints can be translated into linear in-equations and then solved by classical linear programming algorithms, resource constraints require non linear expressions. Solving directly both constraints is still out of the scope of any general algorithms and necessitates the combination of integer programming and search <ref> [24] </ref>. Following the same idea of combining constraints solving and nondeterminism, our technique uses a new approach: the CCLP [19, 53] approach. Unlike conventional constraint solvers based on black box algorithms, CCLP languages use an incomplete constraint solvers over a finite domains algebra.
Reference: [25] <author> David Foxwell and Mark Hewish. </author> <title> High-performance asw at an affordable price. </title> <journal> Jane' IDR Review, </journal> <pages> pages 39-43, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: The growing requirements for sophisticated algorithms, especially those used for 3-D applicative domains, lead to process in real time large multi-dimensional arrays of data. These applications are executed on parallel computers, that offer enough computing power <ref> [25] </ref>. The mapping of DSP applications onto parallel machines raises new problems. The real time and target machine constraints are imperative. The solution must fit the available hardware: the local memory, the number of processors, the processor communications. The application latency must meet the real time requirements.
Reference: [26] <author> R. Govindarajan, E. R. Altman, and G. R. Gao. </author> <title> A framework for ressource-constrained rate-optimal software pipelining. </title> <journal> IEEE Transactions On Parallel And Distributed Systems, </journal> <volume> 7(11) </volume> <pages> 1133-1149, </pages> <month> Nov </month> <year> 1996. </year>
Reference-contexts: Compared to our approach, there is no real time and architectural constraints (number of processors and memory resources) to take into account during the parallelization phase. Similar techniques are used in systolic arrays [16, 17, 14] and parallelization <ref> [23, 22, 26] </ref> communities to compute affine schedules. In the systolic community, these techniques are applied on a single loop nest with complex internal dependencies. The other approaches dealing with complete applications, do not have the same architectural and application constraints.
Reference: [27] <author> Ching-Chih Han, Kwei-Jay Lin, and Chao-Ju Hou. </author> <title> Distance constrained scheduling and its applications to real-time systems. </title> <journal> IEEE Transactions On Computers, </journal> <volume> 45(7) </volume> <pages> 814-825, </pages> <month> Jul </month> <year> 1996. </year>
Reference-contexts: DSP application features are taken into account in [45]. This approach is based on task fusion, but for a sequential result. Mapping statically DSP application with specific signal requirements <ref> [27, 49] </ref> have been widely investigated. The representative Ptolemy framework [39, 47, 44] brings some solution but at a coarse grain level. Most of the resolution schemes are based on dedicated algorithms [6].
Reference: [28] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the fortran d programming system. </title> <booktitle> In Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: These times have to be compared with human being inquiries to comprehend and map the application. 6: Related Work Mapping applications onto parallel machines addresses issues such as scheduling [10], parallelization [1], loop transformations [29, 4, 38], parallel languages <ref> [35, 28, 3] </ref>, integer linear programming and machine architecture. A lot of work has been done to optimize a few criteria such as data and/or computation distribution [40, 21, 34, 7, 43], parallelism detection, minimization of communications [18, 2, 41, 5], processor network usage.
Reference: [29] <author> F. Irigoin. Partitionnement de boucles imbriquees, </author> <title> une technique d'optimisation pour les programmes scientifiques. </title> <type> PhD thesis, </type> <institution> Universite Pierre et Marie Curie, </institution> <month> juin </month> <year> 1987. </year>
Reference-contexts: These times have to be compared with human being inquiries to comprehend and map the application. 6: Related Work Mapping applications onto parallel machines addresses issues such as scheduling [10], parallelization [1], loop transformations <ref> [29, 4, 38] </ref>, parallel languages [35, 28, 3], integer linear programming and machine architecture. A lot of work has been done to optimize a few criteria such as data and/or computation distribution [40, 21, 34, 7, 43], parallelism detection, minimization of communications [18, 2, 41, 5], processor network usage.
References-found: 29

