URL: http://www.ai.sri.com/~luong/research/publications/ijcv-caliblines.ps.Z
Refering-URL: http://www.ai.sri.com/~luong/research/publications/publications.html
Root-URL: 
Email: vthierry@sophia.inria.fr  
Title: Motion of points and lines in the uncalibrated case. International Motion of points and lines
Author: T. Vieville, Q. Luong, and O. Faugeras. Thierry Vieville, Olivier Faugeras and Quang-Tuan Luong 
Keyword: Motion, Auto-calibration, Points and Lines  
Address: BP93, 06902 Valbonne, France  
Affiliation: INRIA, Sophia,  
Note: Journal of Computer Vision, 1994. To appear.  
Abstract: In the present paper we address the problem of computing structure and motion, given a set point and/or line correspondences, in a monocular image sequence, when the camera is not calibrated. Considering point correspondences first, we analyse how to parameterize the retinal correspondences, in function of the chosen geometry : Euclidean, affine or projective geometry. The simplest of these parameterizations is called the F Qs-representation and is a composite projective representation. The main result is that considering N + 1 views in such a monocular image sequence, the retinal correspondences are parameterized by 11 N 4 parameters in the general projective case. Moreover, 3 other parameters are required to work in the affine case and 5 additional parameters in the Euclidean case. These 8 parameters are "calibration" parameters and must be calculated considering at least 8 external informations or constraints. The method being constructive, all these representations are made explicit. Then, considering line correspondences, we show how the the same parameterizations can be used when we analyse the motion of lines, in the uncalibrated case. The case of three views is extensively studied and a geometrical interpretation is proposed, introducing the notion of trifocal geometry which generalizes the well known epipolar geometry. It is also discussed how to introduce line correspondences, in a framework based on point correspondences, using the same equations. Finally, considering the F Qs-representation, one implementation is proposed as a "motion module", taking retinal correspondences as input, and providing and estimation of the 11 N 4 retinal motion parameters. As discussed in this paper, this module can also estimate the 3D depth of the points up to an affine and projective transformation, defined by the 8 parameters identified in the first section. Experimental results are provided. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Bar Shalom and T. E. Fortmann. </author> <title> Tracking and Data Association. </title> <address> Academic-Press, Boston, </address> <year> 1988. </year>
Reference-contexts: N parameters less), while there is another global scale indetermination in this equation (37), when considering the quantities ( jjSjj jjsjj ; jjqjj), as detailed in the previous paragraph. These quantities are therefore defined by [9 N ] + [3 (N 1)] [N ] <ref> [1] </ref> = 11 N 4 parameters. Now this representation is sufficient since : * All S-matrices and s-vectors can be generated from fS i1;i ; s i1;i g i=1::N (i.e. <p> We consider : (a) The F -matrix defined between view 0 and 1. 14 (b) Qs-representations between other pairs of consecutive frames. (c) The relative scale factor between the F -matrix and other Q-matrices. We obtain a representation of [7] + [11 (N 1) 1] + <ref> [1] </ref>, that is 11 N 1 parameters, as expected. <p> it has been demonstrated that an alternative to the non-linear statistical estimation of a quantity is to perform a submersion of the parameter space in a Euclidean space, in order to get linear equations, then find the best estimate in the Euclidean space under the constraints defining this parameter space <ref> [1] </ref>. This mechanism is equivalent to first find an unconstrained estimate, then reproject on the parameter space, as done here. We thus have an application of the general schema given in [1], but it is in any case more heavy to implement than when a direct parametrization is available, as developed <p> get linear equations, then find the best estimate in the Euclidean space under the constraints defining this parameter space <ref> [1] </ref>. This mechanism is equivalent to first find an unconstrained estimate, then reproject on the parameter space, as done here. We thus have an application of the general schema given in [1], but it is in any case more heavy to implement than when a direct parametrization is available, as developed in the previous section. 3.5 Conclusion on lines and points motion We have now established that the same parameterization can be used to estimate the motion of lines and/or points, in
Reference: [2] <author> J. Crowley, P. Bobet, and C. Schmid. </author> <title> Autocalibration by direct observations of objects. </title> <journal> Image and Vision Computing, </journal> <volume> 11, </volume> <year> 1993. </year>
Reference-contexts: We thus would like to treat the problem in the following manner : Paradigm Projection Correspondences Rigid Displacement 3D Structure Motion Without Calibration output input output output This corresponds also to what is called "auto-calibration" <ref> [36, 8, 2] </ref> since we have introduced the calibra tion parameters as unknowns in the state of the system.
Reference: [3] <author> R. Deriche and O. D. Faugeras. </author> <title> Tracking Line Segments. </title> <booktitle> In Proceedings of the 1st ECCV, Antibes, </booktitle> <pages> pages 259-269. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: The facility in a motion paradigm is that we can assume the disparity between two frames to be small, leading to easy solutions for the correspondence problem. These correspondences can efficiently been established (Token-Tracking) <ref> [3, 32] </ref>. This problem is thus, given correspondences between points or lines, to recover, the motion, structure and calibration of the system. <p> We have verified the stability of the algorithm also considering large errors of 5 pixels because this corresponds to the error amplitudes when false matchings occur in an image sequence <ref> [3, 32] </ref>. 34 The precision of the estimate is measured considering the distance between the 11 components of the expected and estimated Qs-representations in the parameter space.
Reference: [4] <author> R. Deriche and G. Giraudon. </author> <title> Accurate corner detection : an analytical study. </title> <booktitle> In Proceedings of the 3rd ICCV, </booktitle> <address> Osaka, </address> <pages> pages 66-71, </pages> <year> 1990. </year>
Reference-contexts: We have considered that the precision on the retinal location of the points could have an uncertainty from 0:1 pixel (corresponding to accurate subpixel corner estimators for instance <ref> [4, 11] </ref>) up to 1 or 2 pixels for standard feature detectors.
Reference: [5] <author> R. Enciso, T. Vieville, and O. Faugeras. </author> <title> Approximation du changement de focale et de mise au point par une transformation affine a trois parametres. </title> <type> Technical Report 2071, </type> <institution> INRIA, </institution> <year> 1993. </year>
Reference-contexts: Moreover the algebraic complexity of the problem is very low, due to the important fact that all equations are linear, as visible in equation (9). Such constraints can be stated considering information about the physical sensor: for standard CCD sensors, for instance, <ref> [5] </ref> we can assume that fl = 0 and/or that ff u ff v is constant. Moreover, variations of some calibration parameters can be predicted during variation of zoom or focus [41, 42].
Reference: [6] <author> O. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig ? In 2nd ECCV, </title> <address> Genoa, </address> <year> 1992. </year>
Reference-contexts: The corresponding coefficients are functions of the retinal location. In such a case we reconstruct the depth map, not only up to affine transformation of the retinal plane, but also up to projective transform of the depth. These transformations have been already made explicit <ref> [6] </ref>, in the case of a stereo rig. Therefore, we can design the following strategy : * compute a precise 3D reconstruction, up to an affine/projective transformation, without calibration, and then * estimate the previous transformation from the knowledge of the calibration parameters.
Reference: [7] <author> O. Faugeras. </author> <title> Three-dimensional Computer Vision: a geometric viewpoint. </title> <publisher> MIT Press, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: Moreover, we can easily calculate its depth considering the point location in the first two views (structure from motion), as it is going to be made explicit for the next representation, and is well known in the Euclidean case (see for instance <ref> [7] </ref>). As a consequence, with this parametrization, considering the point location in the first two views, we can recover its location the N 1 other views. It corresponds to a Euclidean parameterization of the retinal motion. <p> Let us explain why. We represent the rotations using the exponential of a skew-symmetric matrix, that is : R ij = e where r is a vector aligned with the rotation axis and which magnitude is equal to the angle of the rotation (see for instance <ref> [7] </ref>). We can write : Q ij = A R ij A 1 = A e = e K~ ij with ij = Ar ij det (A) . This relation is only valid if K i = K j = K. <p> We consider : (a) The F -matrix defined between view 0 and 1. 14 (b) Qs-representations between other pairs of consecutive frames. (c) The relative scale factor between the F -matrix and other Q-matrices. We obtain a representation of <ref> [7] </ref> + [11 (N 1) 1] + [1], that is 11 N 1 parameters, as expected. <p> It is straightforward to verify that N i is orthogonal to L i and does not depend on M (see for instance <ref> [7] </ref>). Moreover, the magnitude of N i , jjN i jj, is equal to the distance from D to the origin C i . <p> is undefined, we can integrate this constraint very easily by defining an information matrix of the form : i = ( k i n k T which interpretation is that the variance of the measure in the direction where the retinal correspondence is undefined is infinite (notion of generalized inverse) <ref> [7] </ref>. A 2D-correspondence between a point m i on a 2D-curve and another point m j on the corresponding 2D-curve after a retinal displacement can be integrated in our framework. Along a curve, the tangential displacement is generally ambiguous (aperture problem) [7], except for points with a high curvature (corners, junctions). <p> correspondence is undefined is infinite (notion of generalized inverse) <ref> [7] </ref>. A 2D-correspondence between a point m i on a 2D-curve and another point m j on the corresponding 2D-curve after a retinal displacement can be integrated in our framework. Along a curve, the tangential displacement is generally ambiguous (aperture problem) [7], except for points with a high curvature (corners, junctions). <p> matrix P i and 3 fi 1 vector p i which defines the projection of a 3D point M = (X; Y; Z) T , given in an absolute frame of reference onto a retinal point m i = (u i ; v i ; 1) T using the relation <ref> [7] </ref> : Z i m i = P i M + p i = [P i jp i ] B @ Y 1 C A If we write R i and t i for the rotation matrix and for the translation vector from the absolute frame of reference to the retinal
Reference: [8] <author> O. Faugeras, Q. T. Luong, and S. Maybank. </author> <title> Camera self-calibration : Theory and experiment. </title> <booktitle> In 2nd ECCV, </booktitle> <address> Genoa, </address> <year> 1992. </year>
Reference-contexts: Most of the earlier or recent studies in the field assume that the calibration of the system is known (a far from been exhaustive bibliography being [19, 18, 24, 9]), except for <ref> [8, 34, 33, 36] </ref>. <p> We thus would like to treat the problem in the following manner : Paradigm Projection Correspondences Rigid Displacement 3D Structure Motion Without Calibration output input output output This corresponds also to what is called "auto-calibration" <ref> [36, 8, 2] </ref> since we have introduced the calibra tion parameters as unknowns in the state of the system. <p> In any cases, it appears that we can recover the intrinsic parameters with the Qs-representation in an apparently simpler way, than using the projective representation as usually proposed <ref> [23, 8] </ref>. Where does it comes from ? A very deep reason. As explained before, the Q-matrix corresponds to the collineation of the plane at infinity. From the section on the planar case, we see that if we know the collineation, we have the equation of this plane. <p> Previous studies on the computation of motion from point or line correspondences, when no calibration, do not integrate all these constraints <ref> [8, 13] </ref> although the existence of these constraints has been already established and used for point correspondences [22]. The generalization to a sequence of views is now done and at the implementation level, we can avoid this level of complexity and build the algorithms on local correspondences only.
Reference: [9] <author> O. D. Faugeras, F. Lustman, and G. Toscani. </author> <title> Motion and structure from point and line matches. </title> <booktitle> In Proceedings of the First International Conference on Computer Vision, London, </booktitle> <pages> pages 25-34, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Most of the earlier or recent studies in the field assume that the calibration of the system is known (a far from been exhaustive bibliography being <ref> [19, 18, 24, 9] </ref>), except for [8, 34, 33, 36]. <p> In this section we are going to study the case of three views in detail, for several reasons : 1. It is known that for lines, when calibration is given, 3 views are needed <ref> [18, 9] </ref>, while we now know that at least 3 views are also needed considering point correspondences when calibration is not given. 2. The geometric approach will focus on the three views problem. 3.
Reference: [10] <author> P. E. Gill and W. Murray. </author> <title> Algorithms for the solution of non-linear least squares problem. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 15 </volume> <pages> 977-992, </pages> <year> 1978. </year>
Reference-contexts: This version of the implementation seems to be a good compromise between several constraints, not all being easy to formalize. Fast implementation of the algorithm. In the proposed implementation, each minimization is performed using a comprehensive algorithm for finding the minimum of a sum of squares <ref> [10] </ref>. No derivatives are required, which is an advantage (less computations, adaptive estimation of either the Gauss-Newton di rections or the Newton directions depending on the numerical stability of the algorithm, etc...) with respect to methods requiring the analytic gradient. <p> However, we perform here the minimization with respect to 7 parameters instead of 11 and it is known <ref> [10] </ref> that the smaller the number of parameters, the more stable the algorithm. This might explain why this minimization loop is less sensitive to high errors. Limitations in the convergence of the algorithm, simulation. .
Reference: [11] <author> A. Guiducci. </author> <title> Corner characterization by differential geometry techniques. </title> <journal> Pattern Recognition Letters, </journal> <volume> 8 </volume> <pages> 311-318, </pages> <year> 1988. </year>
Reference-contexts: We have considered that the precision on the retinal location of the points could have an uncertainty from 0:1 pixel (corresponding to accurate subpixel corner estimators for instance <ref> [4, 11] </ref>) up to 1 or 2 pixels for standard feature detectors.
Reference: [12] <author> R. I. </author> <title> Hartley. Camera calibration using line correspondences. </title> <booktitle> In Proc. DARPA Image Understanding Workshop, </booktitle> <pages> pages 361-366, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: The generalization to the case where points and lines are given has not been made (except <ref> [13, 12] </ref> for some results), and the problem of considering non-constant intrinsic parameters has also not yet been addressed. This is however an important challenge. In particular, in the case of active vision, the extrinsic parameters and the intrinsic parameters of the visual sensor are modified dynamically.
Reference: [13] <author> R. I. Hartley and R. Gupta. </author> <title> Computing matched-epipolar projections. </title> <booktitle> In Proceedings of the CVPR'93 conference, </booktitle> <pages> pages 549-555, </pages> <year> 1993. </year>
Reference-contexts: The generalization to the case where points and lines are given has not been made (except <ref> [13, 12] </ref> for some results), and the problem of considering non-constant intrinsic parameters has also not yet been addressed. This is however an important challenge. In particular, in the case of active vision, the extrinsic parameters and the intrinsic parameters of the visual sensor are modified dynamically. <p> It is known that given more than 8 points in 2 views, there exists invariants <ref> [13] </ref>. The previous count allows us to conjecture that given more than 7 points in 3, 4 or 5 views or more than 6 points in 6 views or more, there exists invariants. <p> q 2 ij , i.e. explicit the columns of the Q-matrix this tensor can be seen as the concatenation of three matrices T 1 , T 2 , T 3 with : T i = q i 21 s 01 q i T This equation has been simultaneously found by <ref> [13] </ref>. <p> Moreover, the fundamental matrix is directly related to these vectors since, from equation (43) : F 01 = s 01 ^ q 1 1 f 1 s 01 ^ q 2 2 f 2 s 01 ^ q 3 3 f 3 ! This equation is due to <ref> [13] </ref> 8 . * (6) let f i 21 ; i = 1; 2; 3 be the 3 vectors of the kernel of T i , i.e. <p> Previous studies on the computation of motion from point or line correspondences, when no calibration, do not integrate all these constraints <ref> [8, 13] </ref> although the existence of these constraints has been already established and used for point correspondences [22]. The generalization to a sequence of views is now done and at the implementation level, we can avoid this level of complexity and build the algorithms on local correspondences only.
Reference: [14] <author> J. Heel. </author> <title> Temporally integrated surface reconstruction. </title> <booktitle> In Proceedings of the 3rd ICCV, </booktitle> <address> Osaka, </address> <year> 1990. </year>
Reference-contexts: Obviously, when the retinal disparity has been canceled, all information about the motion has been extracted. * It is always possible and sometimes more efficient <ref> [39, 14] </ref> to compute motion and structure at the same time, instead of eliminating structure parameters to estimate motion and then structure from motion. We will follow this track here. Using retinal disparities as measurement error.
Reference: [15] <author> T. Huang and A. Netravali. </author> <title> Linear and polynomial methods in motion estimation. </title> <editor> In L. Auslander, T. Kailath, and S. Mitter, editors, </editor> <booktitle> Signal Processing, Part I: Signal Processing Theory. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: If not, we only reconstruct the depth map, up to an affine transformation of the image, given by the matrix A, and this corresponds to an affine transformation of the 3D space. More generally, considering equation (3) and following the same method as in <ref> [15] </ref>, when calibration was taken into account, we can make the following count of equations.
Reference: [16] <editor> L. Kanal and J. Lemmer. </editor> <booktitle> Uncertainty in Artificial Intelligence. </booktitle> <publisher> North-Holland Press, </publisher> <address> Amsterdam, </address> <year> 1988. </year>
Reference-contexts: Ideally, i.e. if the model integrates all information about the retinal disparities, except the noise, the standard deviation must correspond to the input noise. If not, we have a bias in the estimation process of amplitude b and we can estimate this bias using the following approximate formula <ref> [31, 16] </ref> : 2 R = 2 We have obtained : Noise Level Standard deviation of Estimated bias the residual error (pixel) N = 0 R = 3 10 12 b = 3 10 12 N = 0:2 R = 0:209 b = 0:061 N = 1:0 R = 1:117 b
Reference: [17] <author> J. Lavest, G. Rives, and M. Dhome. </author> <title> 3D reconstruction by zooming. </title> <booktitle> In Intelligent Autonomous System, </booktitle> <address> Pittsburg, </address> <year> 1993. </year>
Reference-contexts: It must be noted, that the pinhole model can still be used for a zoom lens if the object-to-image distance is not considered as fixed <ref> [17] </ref>. This is the case here, since we will adapt the camera metric for different object locations.
Reference: [18] <author> Y. Liu and T. S. Huang. </author> <title> Estimation of rigid body motion using straight line correspondences. </title> <booktitle> Computer Vision, Graphics and Image Processing, </booktitle> <pages> pages 35-57, </pages> <year> 1988. </year>
Reference-contexts: Most of the earlier or recent studies in the field assume that the calibration of the system is known (a far from been exhaustive bibliography being <ref> [19, 18, 24, 9] </ref>), except for [8, 34, 33, 36]. <p> In this section we are going to study the case of three views in detail, for several reasons : 1. It is known that for lines, when calibration is given, 3 views are needed <ref> [18, 9] </ref>, while we now know that at least 3 views are also needed considering point correspondences when calibration is not given. 2. The geometric approach will focus on the three views problem. 3. <p> can easily be decomposed in the two equivalent scalar equations : jn 1 ; Q T 21 n 2 j = 0 ; 01 n 0 jj 01 n 0 ) jjn 1 ^ Q T (s T (36) which correspond to the two equations found by Liu and Huang <ref> [18] </ref> in the case where calibration is given. They are now generalized. <p> We can express the fact these three lines are concurrent through : jd 1 ; S T 01 d 0 j = 0 This equation is nothing else than the first equation of Liu and Huang <ref> [18] </ref> on line motion, but given now in a general case, as in equation (36). <p> By doing this we entirely defines d 1 . This equation is nothing else than the second equation of Liu and Huang <ref> [18] </ref> on line motion, but given now in a general case. In the projective case, this corresponds to a trilinearity which relates the intersections of the lines d 1 , d 0 0 2 with the trifocal line t 1 . <p> And we obtain also q 012 up to a scale factor since we have : S 0 = 0 fi 012 ; S 2 = 0 fi 012 We first must notice that this "linear" algorithm does not require more lines than in the case where calibration is given <ref> [18] </ref>. However, as in [18], we have no guaranty that the estimation of T will verify the expected constraints, given a set of uncertain measurements. But the way we have enforced T to verify the constraints is not innocent. <p> also q 012 up to a scale factor since we have : S 0 = 0 fi 012 ; S 2 = 0 fi 012 We first must notice that this "linear" algorithm does not require more lines than in the case where calibration is given <ref> [18] </ref>. However, as in [18], we have no guaranty that the estimation of T will verify the expected constraints, given a set of uncertain measurements. But the way we have enforced T to verify the constraints is not innocent.
Reference: [19] <author> H. C. Longuet-Higgins. </author> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <year> 1981. </year>
Reference-contexts: Most of the earlier or recent studies in the field assume that the calibration of the system is known (a far from been exhaustive bibliography being <ref> [19, 18, 24, 9] </ref>), except for [8, 34, 33, 36]. <p> i and Z j in equation (3) we obtain : m T | -z - m j = 0 (11) The matrix F ij = ~ s ij Q ij is the Fundamental matrix and is also called the "essential matrix in the uncalibrated case" considering the original Longuet-Higgins equation <ref> [19] </ref>, now generalized. In fact, if we consider that the only information available is related to the retinal correspondences between points, without any knowledge about the depths Z i , equation (11) is the only equation that can be derived, as known from a long time in the calibrated case [19] <p> <ref> [19] </ref>, now generalized. In fact, if we consider that the only information available is related to the retinal correspondences between points, without any knowledge about the depths Z i , equation (11) is the only equation that can be derived, as known from a long time in the calibrated case [19] and recently confirmed in the uncalibrated case [22, 23]. However, note that, in an image sequence, such equations are available between each pair of consecutive or non-consecutive frames, although all F -matrices are not expected to be independent. We are going to clarify this problem now.
Reference: [20] <author> Q. Luong, R. Deriche, O. Faugeras, and T. Papadopoulo. </author> <title> On determining the fundamental matrix: analysis of different methods and experimental results. </title> <type> Technical Report RR-1894, </type> <institution> INRIA, Sophia, France, </institution> <year> 1993. </year>
Reference-contexts: Thank's to several other developments in the field such as <ref> [20, 39, 32, 24, 37] </ref> we do not have to discuss again how to implement such a module in great details but simply can base our work on previous experiences. <p> certain uncertainty, often represented by a covariance matrix, and estimation criteria must weight their estimates using this uncertainty. * It is always more robust and reliable to have a criterion based on a retinal measurement error (i.e. a retinal disparity or a image related quantity), even in the uncalibrated case <ref> [20] </ref>, because this quantity corresponds to the physical measure. <p> The corresponding criterion can be derived exactly as the previous 12 Each correspondence provides 2 equations (eventually 1 vanishes, if (fl k i ) 1 is of rank 1) for the disparity and 1 for the depth prediction. 30 one, and we have, very briefly, from <ref> [20] </ref> : J 0 ( 0 ) = k=1 ~ k ~ k ~ k P P 1 V 1 1 1 0 = 1 F 10 ( 0 )m 0 p V 1 0 @~ k @m 1 (fl k @~ k @m 1 (F 10 ( 0 )m 0 <p> (F 10 ( 0 ) T m 1 ) T U (F 10 ( 0 ) T m 1 ) i )(F 10 ( 0 ) T m 1 ) with U = 0 1 0 0 0 0 0 A following the method developed and discussed by previous authors <ref> [20, 22] </ref>. <p> Please refer to <ref> [20] </ref> for a discussion. In any case, note that this estimation cannot be done using line correspondences (we use 2 views) but point correspondences only. This appears in the fact that we use the covariance matrix (fl k i ) and not its inverse which is undefined for normal correspondences. <p> This validates our mechanism of integration of line correspondences. Precision of the boot-strapping phase. We have studied the precision of the boot-strapping phase, which corresponds to the estimation of a fundamental matrix as in <ref> [20] </ref>. Starting from scratch, the minimization is obtained in about 30 iterations and 1:5sec CPU-time in the same conditions as before.
Reference: [21] <author> Q.-T. Luong and T. Vieville. </author> <title> Canonic representations for the geometries of multiple projective views. </title> <booktitle> In 3rd E.C.C.V., </booktitle> <address> Stockholm, </address> <year> 1994. </year>
Reference-contexts: See <ref> [21] </ref> for a geometric interpretation. In other words, the transpose of the Q-matrix is decomposed into its restriction to the vectorial line generated by s ij and its restriction to s ? ij , represented by S ij . <p> the epipolar line defined by its correspondent m j which yields the fundamental equation : m i 2 F ij m j Please note that S ij map any epipolar line in frame i to the corresponding epipolar line in frame j, in coherence to what has been found by <ref> [21] </ref>. Trifocal correspondences for a 3D line. Let us consider a 3D-line D, as shown also in figure 7.
Reference: [22] <author> T. Luong. </author> <title> Matrice Fondamentale et Calibration Visuelle sur l'Environnement. </title> <type> PhD thesis, </type> <institution> Universite de Paris-Sud, Orsay, </institution> <year> 1992. </year> <type> PhD thesis. </type>
Reference-contexts: that the only information available is related to the retinal correspondences between points, without any knowledge about the depths Z i , equation (11) is the only equation that can be derived, as known from a long time in the calibrated case [19] and recently confirmed in the uncalibrated case <ref> [22, 23] </ref>. However, note that, in an image sequence, such equations are available between each pair of consecutive or non-consecutive frames, although all F -matrices are not expected to be independent. We are going to clarify this problem now. <p> We are going to clarify this problem now. There is also a deep relationship between the Qs-representation and the fundamental matrix defined by <ref> [22, 23] </ref>, but if we want to understand this relationship, we need the following decomposition of a Q-matrix, given the corresponding s-vector : Q ij = S ij + s ij r T ij with s T ij S ij = 0 , r ij = ij s ij ~ s <p> : jjs ij jj 2 = 1 and jjS ij jj 2 = 1, in addition to the 3 constraints given by s T ij S ij = 0, we have a (3 1) svector + (9 3 1) Smatrix = 7 parameters representation of F ij , as expected <ref> [22] </ref> : a F -matrix is defined by 7 parameters (The matrix has 9 coefficients but is defined only up to 1 scale factor and is subject to 1 constraint det (F ij ) = 0). However, in our case we have entirely defined the F -matrix using (11). <p> Let us consider a sequence of N + 1 views and P t correspondences between points, in order to solve the system, we must have : [2 N 1] P t 11 N 4 therefore at least 7 points for 2 views as already known <ref> [22] </ref>, and 6 points for 3 views as in [28], but also 6 points for more than 3 views. Thus, we need always more than 5 point correspondences. This representation is schematized in figure 5. Q Qs Q Qs Q Qs Q Qs Q Qs Q Qs etc... <p> using equation (19), we have : m T F 10 1 ~ s 12 Q 12 | -z - m 2 = 0 ; m T 10 [ ~ s 12 ~ s 10 ] Q 12 F 02 Please note that these three equations are independent, as already shown <ref> [22] </ref>. <p> In the case of points, considering two views, the similar algorithm is to estimate the nine components of the fundamental matrix as given is equation (3) under the constraints that F ij is defined up to a scale factor, while det (F ij ) = 0 <ref> [22] </ref>. 7 The Eddington (or Levi-Civita) symbol is defined by the following relations * ijk = 0 if i = j or i = k or j = k 1 if i &lt; k &lt; j or k &lt; j &lt; i or j &lt; i &lt; k It is an <p> Previous studies on the computation of motion from point or line correspondences, when no calibration, do not integrate all these constraints [8, 13] although the existence of these constraints has been already established and used for point correspondences <ref> [22] </ref>. The generalization to a sequence of views is now done and at the implementation level, we can avoid this level of complexity and build the algorithms on local correspondences only. <p> (F 10 ( 0 ) T m 1 ) T U (F 10 ( 0 ) T m 1 ) i )(F 10 ( 0 ) T m 1 ) with U = 0 1 0 0 0 0 0 A following the method developed and discussed by previous authors <ref> [20, 22] </ref>. <p> Therefore we have chosen a sequence of views containing a calibration grid, for which all these parameters can be easily computed as often used in the past <ref> [29, 22] </ref>. A typical image sequence is shown in figure 11, for three consecutive images. The order of magnitude of the disparity between two frames is 2 to 4 pixels, corresponding to rotations of the object of about 5 deg. This corresponds to an acquisition at video rate.
Reference: [23] <author> S. Maybank and O. Faugeras. </author> <title> A theory of self-calibration of a moving camera. </title> <journal> The International Journal of Computer Vision, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: This K-matrix defines, in fact, the retinal image of the absolute conic at infinity which equation is 0 = m T K 1 m, that is the angular relations between each optical ray, or in other words, the intrinsic calibration or the Euclidean geometry of the system <ref> [23] </ref>. Then, if we know the matrices K i and K j we can calculate A i and A j , then R ij = A 1 i Q ij A j and i s ij . We thus obtain intrinsic and extrinsic (motion) parameters. <p> In any cases, it appears that we can recover the intrinsic parameters with the Qs-representation in an apparently simpler way, than using the projective representation as usually proposed <ref> [23, 8] </ref>. Where does it comes from ? A very deep reason. As explained before, the Q-matrix corresponds to the collineation of the plane at infinity. From the section on the planar case, we see that if we know the collineation, we have the equation of this plane. <p> that the only information available is related to the retinal correspondences between points, without any knowledge about the depths Z i , equation (11) is the only equation that can be derived, as known from a long time in the calibrated case [19] and recently confirmed in the uncalibrated case <ref> [22, 23] </ref>. However, note that, in an image sequence, such equations are available between each pair of consecutive or non-consecutive frames, although all F -matrices are not expected to be independent. We are going to clarify this problem now. <p> We are going to clarify this problem now. There is also a deep relationship between the Qs-representation and the fundamental matrix defined by <ref> [22, 23] </ref>, but if we want to understand this relationship, we need the following decomposition of a Q-matrix, given the corresponding s-vector : Q ij = S ij + s ij r T ij with s T ij S ij = 0 , r ij = ij s ij ~ s
Reference: [24] <author> A. Mitiche, S. Seida, and J. K. Aggarwal. </author> <title> Interpretation of structure and motion using straight line correspondences. </title> <booktitle> In Proceedings of the 8th ICPR, </booktitle> <pages> pages 1110-1112, </pages> <address> Paris, France, October 1986. </address> <publisher> IEEE Computer Society Press, Alamitos, </publisher> <address> California. </address>
Reference-contexts: Most of the earlier or recent studies in the field assume that the calibration of the system is known (a far from been exhaustive bibliography being <ref> [19, 18, 24, 9] </ref>), except for [8, 34, 33, 36]. <p> Thank's to several other developments in the field such as <ref> [20, 39, 32, 24, 37] </ref> we do not have to discuss again how to implement such a module in great details but simply can base our work on previous experiences.
Reference: [25] <author> J. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Boston, </address> <year> 1992. </year> <month> 37 </month>
Reference-contexts: to solve equation (3) : N + 1 3 4 5 6 ... 1 5 7 6 2 One application of this method is that for P &gt; P min there exists some internal relations between the coordinates of the points, neither dependent on motion nor on depth, thus invariant <ref> [25] </ref>. It is known that given more than 8 points in 2 views, there exists invariants [13]. The previous count allows us to conjecture that given more than 7 points in 3, 4 or 5 views or more than 6 points in 6 views or more, there exists invariants.
Reference: [26] <author> N. Navab, O. D. Faugeras, and T. Vieville. </author> <title> The critical sets of lines for camera displacement estimation:a mixed euclidean--projective and constructive approach. </title> <booktitle> In Proc. Fourth Int'l Conf. Comput. Vision, </booktitle> <pages> pages 713-723, </pages> <address> Berlin, Germany, </address> <month> May </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: Combining equations (22) and (2) yields after a few algebra (see for instance <ref> [26] </ref>) : L i = R ij L j ; N i = R ij N j + t ij ^ L i (27) This equation defines the 3D correspondence, for a line, between two frames. <p> In the Euclidean case, the geometrical interpretation of the second equation is that the distance from the line to the origin, computed by triangulation using view 0 and 1 is the same as the distance from the line to the origin, computed by triangulation using view 2 and 1 <ref> [26] </ref>, but this interpretation is no more valid because the related concept is only Euclidean.
Reference: [27] <author> W. Press, B. Flannery, S. Teukolsky, and W. Vetterling. </author> <title> Numerical recipes, </title> <booktitle> the art of scientific computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, U.S.A., </address> <year> 1988. </year>
Reference-contexts: This decomposition, called singular value decomposition, is unique and can be efficiently estimated numerically <ref> [27] </ref>. The reader can easily check that the matrix M fl = U 1 0 0 0 0 0 V T verifies det (M fl ) = 0 and minimizes jjM M fl jj considering the norm jjMjj = sup jjujj=1 jjM ujj.
Reference: [28] <author> L. Quan. </author> <title> Invariants of 6 points form 3 uncalibrated images. </title> <booktitle> In 3rd E.C.C.V., </booktitle> <address> Stockholm, </address> <year> 1994. </year>
Reference-contexts: sequence of N + 1 views and P t correspondences between points, in order to solve the system, we must have : [2 N 1] P t 11 N 4 therefore at least 7 points for 2 views as already known [22], and 6 points for 3 views as in <ref> [28] </ref>, but also 6 points for more than 3 views. Thus, we need always more than 5 point correspondences. This representation is schematized in figure 5. Q Qs Q Qs Q Qs Q Qs Q Qs Q Qs etc...
Reference: [29] <author> L. Robert. </author> <title> Perception Stereoscopique de Courbes et de Surfaces Tridimensionnelles, Application a la Robotique Mobile. </title> <type> PhD thesis, </type> <institution> Ecole Polytechnique, Palaiseau. France, </institution> <year> 1992. </year> <type> PhD thesis. </type>
Reference-contexts: This means that we can perform "trinocular stereo, when the system is weakly calibrated" <ref> [29, 30] </ref>, that is without recovering explicitly the Euclidean parameters of the system. These results, considering point correspondences were already known. The corresponding results consid ering lines are not, and we derive them now. Motion equations for lines. <p> Therefore, m 1 is entirely defined from the correspondence m 0 and m 2 , using (33), since it is an homogeneous quantity, as obtained in <ref> [29] </ref>. 19 This shows that we cannot recover all parameters of the Q-matrices, but only S-matrices, s-vectors and q-vectors and leads to the following result : Proposition 4 Considering correspondences between lines the retinal motion is parameterized using the Ssq-representation, i.e 18 parameters for 3 views, as for points and only <p> Therefore we have chosen a sequence of views containing a calibration grid, for which all these parameters can be easily computed as often used in the past <ref> [29, 22] </ref>. A typical image sequence is shown in figure 11, for three consecutive images. The order of magnitude of the disparity between two frames is 2 to 4 pixels, corresponding to rotations of the object of about 5 deg. This corresponds to an acquisition at video rate.
Reference: [30] <author> L. Robert and O. Faugeras. </author> <title> Relative 3d positionning and 3d convex hull computation from a weakly calibrated stereo pair. </title> <editor> In H. Nagel, editor, </editor> <booktitle> 4th I.C.C.V., </booktitle> <address> Berlin. </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1993. </year>
Reference-contexts: This means that we can perform "trinocular stereo, when the system is weakly calibrated" <ref> [29, 30] </ref>, that is without recovering explicitly the Euclidean parameters of the system. These results, considering point correspondences were already known. The corresponding results consid ering lines are not, and we derive them now. Motion equations for lines. <p> This shows that correspondences can be verified without recovering the Euclidean parameters, as in the case of points <ref> [30] </ref>.
Reference: [31] <author> P. A. Ruymgaart and T. T. Soong. </author> <title> Mathematics of Kalman-Bucy filtering. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1985. </year>
Reference-contexts: Ideally, i.e. if the model integrates all information about the retinal disparities, except the noise, the standard deviation must correspond to the input noise. If not, we have a bias in the estimation process of amplitude b and we can estimate this bias using the following approximate formula <ref> [31, 16] </ref> : 2 R = 2 We have obtained : Noise Level Standard deviation of Estimated bias the residual error (pixel) N = 0 R = 3 10 12 b = 3 10 12 N = 0:2 R = 0:209 b = 0:061 N = 1:0 R = 1:117 b
Reference: [32] <author> M. Stephens, R. Blisset, D. Charnley, E. Sparks, and J. Pike. </author> <title> Outdoor vehicle navigation using passive 3d vision. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 556-562. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year>
Reference-contexts: The facility in a motion paradigm is that we can assume the disparity between two frames to be small, leading to easy solutions for the correspondence problem. These correspondences can efficiently been established (Token-Tracking) <ref> [3, 32] </ref>. This problem is thus, given correspondences between points or lines, to recover, the motion, structure and calibration of the system. <p> Thank's to several other developments in the field such as <ref> [20, 39, 32, 24, 37] </ref> we do not have to discuss again how to implement such a module in great details but simply can base our work on previous experiences. <p> We have verified the stability of the algorithm also considering large errors of 5 pixels because this corresponds to the error amplitudes when false matchings occur in an image sequence <ref> [3, 32] </ref>. 34 The precision of the estimate is measured considering the distance between the 11 components of the expected and estimated Qs-representations in the parameter space.
Reference: [33] <author> N. A. Thacker. </author> <title> On-line calibration of a 4-dof robot head for stereo vision. </title> <booktitle> In British Machine Vision Association meeting on Active Vision, </booktitle> <address> London, </address> <year> 1992. </year>
Reference-contexts: Most of the earlier or recent studies in the field assume that the calibration of the system is known (a far from been exhaustive bibliography being [19, 18, 24, 9]), except for <ref> [8, 34, 33, 36] </ref>.
Reference: [34] <author> H. Trivedi. </author> <title> Semi-analytic method for estimating stereo camera geometry from matched points. </title> <journal> Image and Vision Computing, </journal> <volume> 9, </volume> <year> 1991. </year>
Reference-contexts: Most of the earlier or recent studies in the field assume that the calibration of the system is known (a far from been exhaustive bibliography being [19, 18, 24, 9]), except for <ref> [8, 34, 33, 36] </ref>.
Reference: [35] <author> R. Y. Tsai. </author> <title> Synopsis of recent progress on camera calibration for 3D machine vision. </title> <journal> Robotics Review, </journal> <volume> 1 </volume> <pages> 147-159, </pages> <year> 1989. </year>
Reference-contexts: This corresponds, if fl = 0, to the usual equations u = ff u X=Z + u 0 , v = ff v Y =Z + v 0 . See <ref> [35] </ref> for a recent review. In particular det (A i ) = ff u ff v = k f 2 is proportional to the inverse of the square of the focal length.
Reference: [36] <author> T. Vieville. </author> <title> Autocalibration of visual sensor parameters on a robotic head. </title> <journal> Image and Vision Computing, </journal> <volume> 12, </volume> <year> 1994. </year>
Reference-contexts: Most of the earlier or recent studies in the field assume that the calibration of the system is known (a far from been exhaustive bibliography being [19, 18, 24, 9]), except for <ref> [8, 34, 33, 36] </ref>. <p> We thus would like to treat the problem in the following manner : Paradigm Projection Correspondences Rigid Displacement 3D Structure Motion Without Calibration output input output output This corresponds also to what is called "auto-calibration" <ref> [36, 8, 2] </ref> since we have introduced the calibra tion parameters as unknowns in the state of the system. <p> Reciprocally, with two such rotations, if we can compute the corresponding Q ij matrices, and thus the corresponding L ij matrices, we can recover K from the two equations L ij = K ~ ij . This is coherent to what has been observed by <ref> [36] </ref>.
Reference: [37] <author> T. Vieville, P. Facao, and E. Clergue. </author> <title> Building a depth and kinematic 3D-map from visual and inertial sensors using the vertical cue. </title> <editor> In H. Nagel, editor, </editor> <booktitle> 4th I.C.C.V., </booktitle> <address> Berlin. </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1993. </year>
Reference-contexts: From the section on the planar case, we see that if we know the collineation, we have the equation of this plane. But knowing the plane at infinity, means knowing the affine geometry of the scene <ref> [37] </ref>. In other words, the Qs-representation is an intermediate representation of the motion in which the affine geometry of the scene has been made explicit. A projective representation would have involved F -matrices only, as defined in the next section. <p> Finally, let us point out that it is possible, for a rather large class of real scenes, to infer the collineation of the plane at infinity, i.e. the Q-matrix, considering points "at the horizon", that is with a negligible depth <ref> [37] </ref>. This segmentation process, although not rigorous is quite efficient and is discussed elsewhere [40]. 8 Structure from motion using the Qs-representation. In order to increase our understanding of the potentialities of this representation, let us explicit a very useful, but oversimple result. <p> Thank's to several other developments in the field such as <ref> [20, 39, 32, 24, 37] </ref> we do not have to discuss again how to implement such a module in great details but simply can base our work on previous experiences. <p> We reduce this equation on line correspondences to equations involving points correspondences, using Prop. 3, as developed now. Let us choose two points m 0 and m 1 on this 2D-line. In practice, we are going to detect line-segments in an image. It has been shown <ref> [37] </ref> that we better choose the center of the segment m 0 and the direction of the segment, or point at infinity m 1 = m 1 . This will minimize the covariance of the measures, as demonstrated in [38]. However, any other pair of points could be used.
Reference: [38] <author> T. Vieville, P. Facao, and E. Clergue. </author> <title> Computation of ego-motion using the vertical cue. </title> <booktitle> Machine Vision and Applications, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: It has been shown [37] that we better choose the center of the segment m 0 and the direction of the segment, or point at infinity m 1 = m 1 . This will minimize the covariance of the measures, as demonstrated in <ref> [38] </ref>. However, any other pair of points could be used. Each point is in correspondence with a point for which we cannot determine the exact location but only its location up to a displacement on the line, i.e. a "sliding point".
Reference: [39] <author> T. Vieville and O. Faugeras. </author> <title> Feed forward recovery of motion and structure from a sequence of 2D-lines matches. </title> <editor> In S. Tsuji, A. Kak, and J.-O. Eklundh, editors, </editor> <booktitle> Third International Conference on Computer Vision, </booktitle> <address> Osaka, </address> <pages> pages 517-522. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990. </year>
Reference-contexts: Let us explain how. It is easily seen from equation (30) that it is not possible to recover L = A 1 j l j and the distance from the line to the origin jjN j jj which would have characterized the Euclidean parameters of the line <ref> [39] </ref> from the equations, without knowing the A-matrix. We, in fact, recover l j only up to a scale factor. <p> They are now generalized. In the Euclidean case, the geometrical interpretation of the first equation is that the three normals of the three planes P 0 , P 1 and P 2 (see figure 6) are coplanar <ref> [39] </ref>, and using the Qs-interpretation we can easily verify, that this interpretation is still valid. <p> Thank's to several other developments in the field such as <ref> [20, 39, 32, 24, 37] </ref> we do not have to discuss again how to implement such a module in great details but simply can base our work on previous experiences. <p> Obviously, when the retinal disparity has been canceled, all information about the motion has been extracted. * It is always possible and sometimes more efficient <ref> [39, 14] </ref> to compute motion and structure at the same time, instead of eliminating structure parameters to estimate motion and then structure from motion. We will follow this track here. Using retinal disparities as measurement error. <p> Such a minimization is again very easy to perform. Obviously, this decomposition dramatically simplifies the amount of calculations. In order to accelerate the convergence of the estimation process, we use a very common strategy (see for instance <ref> [39] </ref>) in which we decompose the estimation process into two phases, a (1) boot-strapping phase and several (2) steady-state phases : * During the boot-strapping phase, we attempt to obtain a initial estimate of a subset of the parameters in order to start the minimization as close as possible to the
Reference: [40] <author> T. Vieville, C. Zeller, and L. Robert. </author> <title> Using collineations to compute motion and structure in an uncalibrated image sequence, </title> <note> 1994. Accepted after review. </note>
Reference-contexts: This segmentation process, although not rigorous is quite efficient and is discussed elsewhere <ref> [40] </ref>. 8 Structure from motion using the Qs-representation. In order to increase our understanding of the potentialities of this representation, let us explicit a very useful, but oversimple result. <p> We can summarize this result as follow : Proposition 3 The retinal motion of a line is entirely determined by the normal displacement of two points of this line. This result is very important for implementations as already stress in <ref> [40] </ref> for other estimations, and will be used in the implementation proposed in this paper. 3.2 Motion of lines in three views Let us now study the problem of computing the motion parameters S ij , s ij and q ijk , considering points and/or lines correspondences.
Reference: [41] <author> R. Willson. </author> <title> Modeling and Calibration of Automated Zoom Lenses. </title> <type> PhD thesis, </type> <institution> Department of Electical and Computer Engineering, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: Such constraints can be stated considering information about the physical sensor: for standard CCD sensors, for instance, [5] we can assume that fl = 0 and/or that ff u ff v is constant. Moreover, variations of some calibration parameters can be predicted during variation of zoom or focus <ref> [41, 42] </ref>. In practice, these equations are to be solved using a statistical framework and a model of the evolution of the parameters, but this is out of the scope of the present paper.
Reference: [42] <author> R. Willson and S. Shafer. </author> <booktitle> What is the center of the image ? In IEEE Proc CVPR'93, </booktitle> <address> New-York, </address> <month> June, </month> <pages> pages 670-671, </pages> <year> 1993. </year>
Reference-contexts: Such constraints can be stated considering information about the physical sensor: for standard CCD sensors, for instance, [5] we can assume that fl = 0 and/or that ff u ff v is constant. Moreover, variations of some calibration parameters can be predicted during variation of zoom or focus <ref> [41, 42] </ref>. In practice, these equations are to be solved using a statistical framework and a model of the evolution of the parameters, but this is out of the scope of the present paper.
References-found: 42

