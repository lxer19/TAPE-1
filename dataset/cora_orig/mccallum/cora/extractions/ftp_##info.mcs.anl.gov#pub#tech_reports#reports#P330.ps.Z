URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P330.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: LINE SEARCH ALGORITHMS WITH GUARANTEED SUFFICIENT DECREASE  
Phone: 60439  
Author: Jorge J. More and David J. Thuente 
Note: Work supported by the Applied Mathematical Sciences subprogram of the Office of Energy Research, U.S. Department of Energy, under Contract W-31-109-Eng-38. Permanent address:  
Date: October 1992  
Address: 9700 South Cass Avenue Argonne, Illinois  Preprint MCS-P330-1092  Fort Wayne, Indiana 46805  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  Department of Mathematical Sciences, Indiana-Purdue University,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Al-Baali, </author> <title> Descent property and global convergence of the Fletcher-Reeves method with inexact line searches, </title> <journal> IMA J. Numer. Anal., </journal> <volume> 5 (1985), </volume> <pages> pp. 121-124. </pages>
Reference-contexts: See, for example, Dennis and Schnabel [4] and Fletcher [6] for gradient-related methods; Powell [13] and Byrd, Nocedal, and Yuan [3] for quasi-Newton methods; and Al-Baali <ref> [1] </ref>, Liu and Nocedal [10], and Gilbert and Nocedal [7] for conjugate gradient methods. In most practical situations it is important to impose additional requirements on ff. <p> Search Algorithm. Set I 0 = <ref> [0; 1] </ref>. For k = 0; 1; : : : Choose a safeguarded ff k 2 I k " [ff min ; ff max ]. Test for convergence. Update the interval I k .
Reference: [2] <author> M. Al-Baali and R. Fletcher, </author> <title> An efficient line search for nonlinear least squares, </title> <journal> J. Optim. Theory Appl., </journal> <volume> 48 (1984), </volume> <pages> pp. 359-377. 21 </pages>
Reference-contexts: Fletcher [5] suggested that it is possible to compute a sequence of nested intervals that contain points that satisfy (1.1) and (1.2), but he did not prove any result along these lines. This suggestion led to the algorithms developed by Al-Baali and Fletcher <ref> [2] </ref> and More and Sorensen [11]. In this paper we provide a convergence analysis, implementation details, and numerical results for the algorithm of More and Sorensen [11]. The search algorithm for T () is defined in Section 2. <p> Of course, in this case there is no need to update I because ff t belongs to T (). Al-Baali and Fletcher <ref> [2] </ref> present two updating schemes. The aim of scheme S1 is to identify a point that satisfies (1.1) and OE 0 (ff) jOE 0 (0), while scheme S2 seeks a point that satisfies (1.1) and (1.2). <p> Theorem 2.3 thus justifies our claim that, except for pathological cases, the search algorithm terminates in a finite number of iterations. Closely related results have been established by Al-Baali and Fletcher <ref> [2] </ref> and More and Sorensen [11]. <p> A difficulty with setting j &lt; is that, even if T () is not empty, there may not be an ff 0 that satisfies (1.1) and (1.2). We illustrate this point with a minor modification of an example of Al-Baali and Fletcher <ref> [2] </ref>. Define OE (ff) = &gt; &lt; 1 1 2 (oe 1) oeff; 1 ff; (3:1) where j &lt; oe &lt; .
Reference: [3] <author> R. H. Byrd, J. Nocedal, and Y. Yuan, </author> <title> Global convergence of a class of quasi--Newton methods on convex problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 24 (1987), </volume> <pages> pp. 1171-1190. </pages>
Reference-contexts: See, for example, Dennis and Schnabel [4] and Fletcher [6] for gradient-related methods; Powell [13] and Byrd, Nocedal, and Yuan <ref> [3] </ref> for quasi-Newton methods; and Al-Baali [1], Liu and Nocedal [10], and Gilbert and Nocedal [7] for conjugate gradient methods. In most practical situations it is important to impose additional requirements on ff. <p> Moreover, if &lt; 1 2 , then T () = 1 ; 2 ( oe) : Thus T () is a nonempty interval with ff = 1 in the interior. In Figure 3.1 we have set oe = 0:1 and = 0:25 and thus T () = <ref> [ 5 6 ; 3] </ref>.
Reference: [4] <author> J. E. Dennis and R. E. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: The curvature condition (1.2) is particularly important in a quasi-Newton method because it guarantees that a positive definite quasi-Newton update is possible. See, for example, Dennis and Schnabel <ref> [4] </ref> and Fletcher [6]. Work supported in part by the Applied Mathematical Sciences subprogram of the Office of Energy Research of the U.S. <p> See, for example, Dennis and Schnabel <ref> [4] </ref> and Fletcher [6] for gradient-related methods; Powell [13] and Byrd, Nocedal, and Yuan [3] for quasi-Newton methods; and Al-Baali [1], Liu and Nocedal [10], and Gilbert and Nocedal [7] for conjugate gradient methods. In most practical situations it is important to impose additional requirements on ff. <p> This is done by choosing ff + h minfffi max ff t ; ff max g; ff max i for some factor ffi max &gt; 1. In our implementation we use ff + t = minfff t + ffi (ff t ff l ); ff max g; ffi 2 <ref> [1:1; 4] </ref>; and thus an induction argument shows that (2.2) holds with ffi max = 1:1.
Reference: [5] <author> R. Fletcher, </author> <title> Practical Methods of Optimization Volume 1: Unconstrained Optimization, </title> <publisher> John Wiley & Sons, </publisher> <year> 1980. </year> <title> [6] , Practical Methods of Optimization, </title> <publisher> John Wiley & Sons, </publisher> <editor> Second ed., </editor> <year> 1987. </year>
Reference-contexts: Their algorithm, however, is not guaranteed to produce a point that satisfies (1.1) and (1.2). Fletcher <ref> [5] </ref> suggested that it is possible to compute a sequence of nested intervals that contain points that satisfy (1.1) and (1.2), but he did not prove any result along these lines. This suggestion led to the algorithms developed by Al-Baali and Fletcher [2] and More and Sorensen [11].
Reference: [7] <author> J. C. Gilbert and J. Nocedal, </author> <title> Global convergence properties of conjugate gradient methods for optimization, </title> <journal> SIAM J. Optimization, </journal> <volume> 2 (1992), </volume> <pages> pp. 21-42. </pages>
Reference-contexts: The search algorithm described in this paper has been used by several authors, for example, Liu and Nocedal [10], O'Leary [12], Schlick and Fogelson [14, 15], and Gilbert and Nocedal <ref> [7] </ref>. This paper describes this search procedure and the associated convergence theory. In a line search method we are given a continuously differentiable function f : IR n ! IR and a descent direction p for f at a given point x 2 IR n . <p> See, for example, Dennis and Schnabel [4] and Fletcher [6] for gradient-related methods; Powell [13] and Byrd, Nocedal, and Yuan [3] for quasi-Newton methods; and Al-Baali [1], Liu and Nocedal [10], and Gilbert and Nocedal <ref> [7] </ref> for conjugate gradient methods. In most practical situations it is important to impose additional requirements on ff.
Reference: [8] <author> P. E. Gill and W. Murray, </author> <title> Safeguarded steplength algorithms for optimization using descent methods, </title> <type> Report NAC 37, </type> <institution> National Physical Laboratory, Teddington, </institution> <address> England, </address> <year> 1974. </year>
Reference-contexts: Another advantage of phrasing the results in terms of T () is that T () is usually not empty. For example, T () is not empty when OE is bounded below. Several authors have done related work on the solution of (1.1) and (1.2). For example, Gill and Murray <ref> [8] </ref> attacked (1.1) and (1.2) by using a univariate minimization algorithm for OE to find a solution ff fl to (1.2). If ff fl did not satisfy (1.1), then ff fl was repeatedly halved in order to obtain a solution fi fl to (1.1).
Reference: [9] <author> P. E. Gill, W. Murray, M. A. Saunders, and M. H. Wright, </author> <title> Two steplength algorithms for numerical optimization, </title> <type> Report SOL 79-25, </type> <institution> Stanford University, Systems Optimization Laboratory, Stanford, California, </institution> <year> 1979. </year>
Reference-contexts: In particular, the sufficient decrease condition (1.1) can rule out many of the points that satisfy (1.2), and then the algorithm is not guaranteed to converge. Gill, Murray, Saunders, and Wright <ref> [9] </ref> proposed an interesting variation on (1.1) and (1.2) when they argued that if there is no solution to (1.1) and (1.2), then it was necessary 2 to compute a point such that OE (ff) = OE (0) + OE 0 (0)ff: (1:6) If (1.6) has a solution, then their algorithm
Reference: [10] <author> D. C. Liu and J. Nocedal, </author> <title> On the limited memory BFGS method for large scale optimization, </title> <journal> Math. Programming, </journal> <volume> 45 (1989), </volume> <pages> pp. 503-528. </pages>
Reference-contexts: The search algorithm described in this paper has been used by several authors, for example, Liu and Nocedal <ref> [10] </ref>, O'Leary [12], Schlick and Fogelson [14, 15], and Gilbert and Nocedal [7]. This paper describes this search procedure and the associated convergence theory. <p> See, for example, Dennis and Schnabel [4] and Fletcher [6] for gradient-related methods; Powell [13] and Byrd, Nocedal, and Yuan [3] for quasi-Newton methods; and Al-Baali [1], Liu and Nocedal <ref> [10] </ref>, and Gilbert and Nocedal [7] for conjugate gradient methods. In most practical situations it is important to impose additional requirements on ff.
Reference: [11] <author> J. J. Mor e and D. C. Sorensen, </author> <title> Newton's method, in Studies in Numerical Analysis, </title> <editor> G. H. Golub, ed., </editor> <booktitle> The Mathematical Association of America, </booktitle> <year> 1984, </year> <pages> pp. 29-82. </pages>
Reference-contexts: Fletcher [5] suggested that it is possible to compute a sequence of nested intervals that contain points that satisfy (1.1) and (1.2), but he did not prove any result along these lines. This suggestion led to the algorithms developed by Al-Baali and Fletcher [2] and More and Sorensen <ref> [11] </ref>. In this paper we provide a convergence analysis, implementation details, and numerical results for the algorithm of More and Sorensen [11]. The search algorithm for T () is defined in Section 2. <p> This suggestion led to the algorithms developed by Al-Baali and Fletcher [2] and More and Sorensen <ref> [11] </ref>. In this paper we provide a convergence analysis, implementation details, and numerical results for the algorithm of More and Sorensen [11]. The search algorithm for T () is defined in Section 2. <p> Theorem 2.3 thus justifies our claim that, except for pathological cases, the search algorithm terminates in a finite number of iterations. Closely related results have been established by Al-Baali and Fletcher [2] and More and Sorensen <ref> [11] </ref>.
Reference: [12] <author> D. P. O'Leary, </author> <title> Robust regression computation using iteratively reweighted least squares, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11 (1990), </volume> <pages> pp. 466-480. </pages>
Reference-contexts: The search algorithm described in this paper has been used by several authors, for example, Liu and Nocedal [10], O'Leary <ref> [12] </ref>, Schlick and Fogelson [14, 15], and Gilbert and Nocedal [7]. This paper describes this search procedure and the associated convergence theory.
Reference: [13] <author> M. J. D. Powell, </author> <title> Some global convergence properties of a variable metric method without line searches, in Nonlinear Programming, </title> <editor> R. W. Cottle and C. E. Lemke, eds., </editor> <volume> vol. </volume> <booktitle> IX of SIAM-AMS Proceedings, </booktitle> <publisher> American Mathematical Society, </publisher> <year> 1976, </year> <pages> pp. 53-72. </pages>
Reference-contexts: See, for example, Dennis and Schnabel [4] and Fletcher [6] for gradient-related methods; Powell <ref> [13] </ref> and Byrd, Nocedal, and Yuan [3] for quasi-Newton methods; and Al-Baali [1], Liu and Nocedal [10], and Gilbert and Nocedal [7] for conjugate gradient methods. In most practical situations it is important to impose additional requirements on ff.
Reference: [14] <author> T. Schlick and A. Fogelson, </author> <title> TNPACK A truncated Newton minimization package for large-scale problemns: I. Algorithms and usage, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 18 (1992), </volume> <pages> pp. </pages> <month> 46-70. </month> <title> [15] , TNPACK A truncated Newton minimization package for large-scale problemns: II. Implementations examples, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 18 (1992), </volume> <pages> pp. 71-111. </pages>
Reference-contexts: The search algorithm described in this paper has been used by several authors, for example, Liu and Nocedal [10], O'Leary [12], Schlick and Fogelson <ref> [14, 15] </ref>, and Gilbert and Nocedal [7]. This paper describes this search procedure and the associated convergence theory.
Reference: [16] <author> D. F. Shanno and K. H. Phua, </author> <title> Minimization of unconstrained multivariate functions, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 2 (1976), </volume> <pages> pp. </pages> <month> 87-94. </month> <title> 22 [17] , Remark on Algorithm 500, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 6 (1980), </volume> <pages> pp. 618-622. </pages>
Reference-contexts: Of course, fi fl did not necessarily satisfy (1.2); but if was sufficiently small, then it was argued that this was an unlikely event. In a similar vein, we mention that the search algorithm of Shanno and Phua <ref> [16, 17] </ref> is not guaranteed to work in all cases. In particular, the sufficient decrease condition (1.1) can rule out many of the points that satisfy (1.2), and then the algorithm is not guaranteed to converge.
Reference: [18] <author> H. Yanai, M. Ozawa, and S. Kaneko, </author> <title> Interpolation methods in one dimensional optimization, </title> <journal> Computing, </journal> <volume> 27 (1981), </volume> <pages> pp. 155-163. 23 </pages>
Reference-contexts: A plot of this function with these parameter settings appears in Figure 5.3. The other three functions in the test set are from the paper of Yanai, Ozawa, and Kaneko <ref> [18] </ref>.
References-found: 15

