URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3323/3323.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Compiling Real-Time Programs with Timing Constraint Refinement and Structural Code Motion  
Author: Richard Gerber and Seongsoo Hong 
Keyword: Real-time, programming languages, compiler optimization, code scheduling, single-static assignment, timing analysis, trace scheduling, code motion.  
Note: July  This research is supported in part by ONR grant N00014-94-10228, NSF grant CCR-9209333, and an NSF Young Investigator Award CCR-9357850. A preliminary abstract of this material appeared in the Proceedings of the ACM SIGPLAN 93 Conference on Programming Language Design and Implementation (June 1993).  
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Email: rich@cs.umd.edu sshong@cs.umd.edu  
Phone: (301) 405-2710  
Date: 1994  
Abstract: University of Maryland Technical Report UMD CS-TR-3323, UMIACS-TR-94-90 Abstract We present a programming language called TCEL (Time-Constrained Event Language), whose semantics is based on time-constrained relationships between observable events. Such a semantics infers only those timing constraints necessary to achieve real-time correctness, without over-constraining the system. Moreover, an optimizing compiler can exploit this looser semantics to help tune the code, so that its worst-case execution time is consistent with its real-time requirements. In this paper we describe such a transformation system, which works in two phases. First the TCEL source code is translated into an intermediate representation. Then an instruction-scheduling algorithm rearranges selected unobservable operations, and synthesizes tasks guaranteed to respect the original event-based constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Moreover, since we do not move code out of loops but only within their bodies we can treat a HFG as an acyclic graph, and ignore all back edges. Of course our hierarchical structure assumes that the program's (flat) flow graph reducible <ref> [1] </ref>.
Reference: [2] <author> A. Aiken and A. Nicolau. </author> <title> A development environment for horizontal microcode. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 584-594, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next subsection we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 5, 6, 8, 22, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling [6] or Percolation Scheduling <ref> [2, 5] </ref>. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility.
Reference: [3] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> 9 </volume> <pages> 319-345, </pages> <month> July </month> <year> 1987. </year> <month> 30 </month>
Reference-contexts: And the situation gets much more complicated when the program possesses a branching structure, i.e., when actual execution paths are determined at runtime. Thus we take a greedy approximation approach, which works in several phases. First the TCEL source is translated into a single-static-assignment (SSA) representation <ref> [3] </ref>, whose naming conventions help isolate the "worst-case" execution paths. Next the code is decomposed into several blocks, and equations are generated to constrain their start and finish-times. Finally, a variant of trace-scheduling [6] is used to relocate the unobservable code, and hopefully attain feasibility. <p> Thus we take the following alternative approach, in which feasible tasks are synthesized in a two-step process section decomposition (Section 6) and code scheduling (Section 7). Section Decomposition. First the code is translated into its single-static-assignment (SSA) form <ref> [3] </ref>. <p> tuning process, and is usually preferable to measures like hand-optimizing the code, redesigning subsystems, or re-implementing components in silicon. 29 A Appendix: The Static Single Assignment Form A program is defined to be in SSA form if each use of a variable is reached by exactly one assignment to it <ref> [3] </ref>.
Reference: [4] <author> B. Dasarathy. </author> <title> Timing constraints of real-time systems: Constructs for expressing them, method for validating them. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(1) </volume> <pages> 80-86, </pages> <month> Jan-uary </month> <year> 1985. </year>
Reference-contexts: Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." For example, in Real-Time Logic [14], events are instantaneous and require no resources while actions consume nonzero time. Similar distinctions exist in RTRL <ref> [4] </ref>, Timed IO Automata [21], ACSR [16], and in almost every formal approach to real-time. It therefore seemed natural to extend this common technique to a "full-blown" real-time programming language, in which the "events" correspond to actual IO operations within C code.
Reference: [5] <author> K. Ebcioglu and A. Nicolau. </author> <title> A global resource-constrained parallelization technique. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 154-163. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next subsection we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 5, 6, 8, 22, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling [6] or Percolation Scheduling <ref> [2, 5] </ref>. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility.
Reference: [6] <author> J. A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computer, </journal> <volume> 30 </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: First the TCEL source is translated into a single-static-assignment (SSA) representation [3], whose naming conventions help isolate the "worst-case" execution paths. Next the code is decomposed into several blocks, and equations are generated to constrain their start and finish-times. Finally, a variant of trace-scheduling <ref> [6] </ref> is used to relocate the unobservable code, and hopefully attain feasibility. The remainder of this paper is organized as follows. In the following section we survey related work in programming languages, semantics and optimization methods for real-time systems. <p> Then, the surrounding node is handled. If this level is found inconsistent, the inner nodes are "opened up" once again, and more aggressive optimization is carried out. The actual transformations are similar to those used in Trace Scheduling <ref> [6] </ref>. As the name implies, the Trace Scheduling algorithm works on specific traces: it selects a path (or trace) from a given code block, and then selects instructions on that path to move. <p> 2:82ms along the worst-case execution time path.) In the next subsection we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 5, 6, 8, 22, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput. <p> Thus the objective is to keep each unit busy, and to achieve better overall throughput. Our problem context has an entirely different goal, and it cannot be solved by directly applying well-known techniques such as Trace Scheduling <ref> [6] </ref> or Percolation Scheduling [2, 5]. We are concerned not with enhancing average-case performance, but instead with ensuring feasibility. <p> In turn it processes section S3, which may now contain newly moved code. To perform greedy code motion, we have adapted a technique from the approach to Trace Scheduling in <ref> [6] </ref>, and we use it as a component of the code scheduling algorithm. In our approach, nodes lying on paths that exceed their section's duration constraint are considered for code motion. We distinguish such paths as critical traces.
Reference: [7] <author> M. R. Garey and D. S. Johnson. </author> <title> Computer and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: Clause (3) means that the new code is feasible. This problem is NP-hard, due to the existence of immovable operations and data dependences. Theorem 5.1 Feasible code synthesis is NP-hard. Proof: The proof follows by a straightforward transformation from "Partition [SP12]" <ref> [7] </ref> to feasible code synthesis. Consider an instance (A; s) of Partition, where A = fa 1 ; a 2 ; : : : ; a n g is a set of elements, and where s : A 7! NN is the cost of each element.
Reference: [8] <author> F. Gasperoni. </author> <title> Compilation techniques for VLIW architectures. </title> <type> Technical Report RC 14915(#66741), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next subsection we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 5, 6, 8, 22, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput.
Reference: [9] <author> R. Gerber and S. Hong. </author> <title> Semantics-based compiler transformations for enhanced schedulability. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 232-242. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [9, 10, 12, 23, 20] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. <p> Unlike the semantics for TCEL, however, the execution times of non-time-critical statements are not explicitly decoupled from timing constraints imposed on the events. Thus, the applicability of some transformations may be unnecessarily restricted. Finally, in <ref> [9] </ref> we describe how we use TCEL's event-based semantics to specialize tasks for multi-threaded applications. Specifically, a program-slicing tool splits a task into a deadline-sensitive sub-thread, and a thread which can be postponed past its deadline. <p> With our scheduling engine as its foundation, a graphical interface would allow a programmer to selectively apply the transformations and also remain informed of the results. Pushing Forward. We have recently turned our attention to a more aggressive goal inter-task transformations to achieve schedulability. In <ref> [9] </ref> we explore a technique that helps auto-tune an unschedulable task set into a schedulable one, by isolating the time-critical threads, and then ensuring that they can be run under a fixed-priority dispatcher.
Reference: [10] <author> P. Gopinath and R. Gupta. </author> <title> Applying compiler techniques to scheduling in real-time systems. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 247-256. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1990. </year>
Reference-contexts: But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [9, 10, 12, 23, 20] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. <p> There have been other compiler-based approaches to real-time programming [9, 10, 12, 23, 20]. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. In <ref> [10] </ref> a compiler classifies application program on the basis of its predictability and monotonicity, and creates partitions which have a higher degree of adaptability. The objective is to produce a transformed program possessing a smaller variance in its execution time.
Reference: [11] <author> M. G. Harmon, T. P. Baker, and D. B. Whalley. </author> <title> A retargetable technique for predicting execution time. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 68-77. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1992. </year>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [11, 18, 24, 25, 29] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [12] <author> S. Hong and R. Gerber. </author> <title> Scheduling with compiler transformations: the TCEL approach. </title> <booktitle> In Proceedings IEEE Workshop on Real-Time Operating Systems and Software, </booktitle> <pages> pages 80-84. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1993. </year> <journal> IEEE RTTC Real-Time Newsletter, </journal> 9(1/2):80-84. 
Reference-contexts: But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [9, 10, 12, 23, 20] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs.
Reference: [13] <author> Y. Ishikawa, H. Tokuda, and C. W. Mercer. </author> <title> Object-oriented real-time language design: Constructs for timing constraints. </title> <booktitle> In Proceedings of OOPSLA-90, </booktitle> <pages> pages 289-298, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the requirements side of the design equation; examples are <ref> [13, 17, 19, 23, 27] </ref>. These languages provide programmers with a convenient means of postulating timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime.
Reference: [14] <author> F. Jahanian and Al Mok. </author> <title> Safety analysis of timing properties in real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(9) </volume> <pages> 890-904, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: That is, when reasoning about a real-time concurrent system it is often useful to consider only "events of interest," and to abstract away local-state information. Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." For example, in Real-Time Logic <ref> [14] </ref>, events are instantaneous and require no resources while actions consume nonzero time. Similar distinctions exist in RTRL [4], Timed IO Automata [21], ACSR [16], and in almost every formal approach to real-time.
Reference: [15] <author> K. B. Kenny and K.-J. Lin. </author> <title> Building flexible real-time systems using the Flex language. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 70-78, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Similarly, let S4.start and S4.finish represent the start and finish times of section S4. Using these variables we can represent the section decomposition of a TCEL construct in a manner similar to that found in the Flex language <ref> [15] </ref>. Recall the flight controller program from Figure 3. Figure 5 illustrates its constituent sections. The constraint-expression for S6 corresponds to the program's outer, periodic loop. As the program is in SSA form, -functions appear at confluence points where different values of the same variable in the original program merge.
Reference: [16] <author> I. Lee, P. Bremond-Gregoire, and R. Gerber. </author> <title> A Process Algebraic Apprach to the Specification and Analysis of Resource-Bound Real-Time Systems. </title> <journal> IEEE Proceedings, </journal> <volume> 82(1), </volume> <month> January </month> <year> 1994. </year> <month> 31 </month>
Reference-contexts: Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." For example, in Real-Time Logic [14], events are instantaneous and require no resources while actions consume nonzero time. Similar distinctions exist in RTRL [4], Timed IO Automata [21], ACSR <ref> [16] </ref>, and in almost every formal approach to real-time. It therefore seemed natural to extend this common technique to a "full-blown" real-time programming language, in which the "events" correspond to actual IO operations within C code.
Reference: [17] <author> I. Lee and V. Gehlot. </author> <title> Language constructs for real-time programming. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 57-66. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1985. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the requirements side of the design equation; examples are <ref> [13, 17, 19, 23, 27] </ref>. These languages provide programmers with a convenient means of postulating timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime. <p> Most other real-time languages do not make such a distinction, and instead place constraints on the boundaries of code blocks. Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [19, 17, 27] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives. <p> Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in [19, 17, 27]), or they are postulated in a separate interface, and then passed to the scheduler as directives. A common language-based approach (first presented in <ref> [17] </ref>) is to provide constructs such as "within t do f: : : g," "at t do f: : : g" and "after t do f: : : g." An alternative, taken in [19], is to set up linear constraint expressions on the the start times and deadlines of code blocks. <p> Both constructs are syntactic descendents of the temporal scope, first introduced in <ref> [17] </ref>. However, as we have stated, our semantics is quite different, in that it relies on constrained relationships between observable events. We first elaborate the "do" construct which establishes several types of relative timing constraints.
Reference: [18] <author> S. Lim, Y. Bae, C. Jang, B. Rhee, S. Min, C. Park, H. Shin, K. Park, and C. Kim. </author> <title> An accurate worst case timing analysis for risc processors. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [11, 18, 24, 25, 29] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [19] <author> K. J. Lin and S. Natarajan. </author> <title> Expressing and maintaining timing constraints in FLEX. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1988. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the requirements side of the design equation; examples are <ref> [13, 17, 19, 23, 27] </ref>. These languages provide programmers with a convenient means of postulating timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime. <p> Most other real-time languages do not make such a distinction, and instead place constraints on the boundaries of code blocks. Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [19, 17, 27] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives. <p> A common language-based approach (first presented in [17]) is to provide constructs such as "within t do f: : : g," "at t do f: : : g" and "after t do f: : : g." An alternative, taken in <ref> [19] </ref>, is to set up linear constraint expressions on the the start times and deadlines of code blocks. We have borrowed from both approaches: in the TCEL source we use the higher-level constructs, while in our intermediate code we make use of the constraint representation.
Reference: [20] <author> T. Marlowe and S. Masticola. </author> <title> Safe optimization for hard real-time programming. </title> <booktitle> In Second International Conference on Systems Integration, </booktitle> <pages> pages 438-446, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [9, 10, 12, 23, 20] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. <p> This is tangentially similar to our application of "speculative transformations," since both break control-dependences that are predicated on inputs. The principal objective in [28], however, is quite different, in that "shadow threads" are forked off to execute on available resources. 3 In <ref> [20] </ref> time-critical statements (or events) are assumed in the underlying programming language, and used for developing the notion of safe real-time code transformations. Based on this notion of safety, a large number of conventional code transformations are examined, and then classified for application in real-time programming.
Reference: [21] <author> M. Merritt, F. Modungo, and M. Tuttle. </author> <title> Time-Constrained Automata. </title> <booktitle> In CONCUR '91, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: Indeed, almost all formal models ease this process by making some distinction between an "event" and a corresponding "action." For example, in Real-Time Logic [14], events are instantaneous and require no resources while actions consume nonzero time. Similar distinctions exist in RTRL [4], Timed IO Automata <ref> [21] </ref>, ACSR [16], and in almost every formal approach to real-time. It therefore seemed natural to extend this common technique to a "full-blown" real-time programming language, in which the "events" correspond to actual IO operations within C code.
Reference: [22] <author> A. Nicolau. </author> <title> Parallelism, Memory Anti-aliasing and Correctness Issues for a Trace Scheduling Compiler. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> June </month> <year> 1984. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next subsection we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 5, 6, 8, 22, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput.
Reference: [23] <author> V. Nirkhe. </author> <title> Application of Partial Evaluation to Hard Real-Time Programming. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Maryland at College Park, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the requirements side of the design equation; examples are <ref> [13, 17, 19, 23, 27] </ref>. These languages provide programmers with a convenient means of postulating timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime. <p> But in TCEL the semantics is quite different, as it establishes constraints between the observable events within the code, and not on the code's textual boundaries. There have been other compiler-based approaches to real-time programming <ref> [9, 10, 12, 23, 20] </ref>. These approaches, while addressing different problems associated with real-time programming, share a common goal, namely enhancing the predictability and schedulability of programs. <p> In [10] a compiler classifies application program on the basis of its predictability and monotonicity, and creates partitions which have a higher degree of adaptability. The objective is to produce a transformed program possessing a smaller variance in its execution time. In <ref> [23] </ref> a partial evaluator is applied to a source program, which produces residual code that is both more optimized and more deterministic. In [28] an approach to speculative execution is postulated for distributed real-time systems.
Reference: [24] <author> C. Park and A. C. Shaw. </author> <title> Experimenting with a program timing tool based on source-level timing schema. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 72-81. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1990. </year>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [11, 18, 24, 25, 29] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [25] <author> A. C. Shaw. </author> <title> Reasoning about time in higher level language software. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 875-889, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [11, 18, 24, 25, 29] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
Reference: [26] <author> M. Smith, M. Horowitz, and M. Lam. </author> <title> Efficient superscalar performance through boosting. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 248-259. </pages> <publisher> ACM Press, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: 2:82ms along the worst-case execution time path.) In the next subsection we discuss our code-scheduling techniques which handle cases such as this, in which the duration constraints fail to hold. 7 Code Scheduling The code scheduling algorithm is inspired by a common compiler strategy used for VLIW and superscalar architectures <ref> [2, 5, 6, 8, 22, 26] </ref>. In such domains, an optimizing compiler exploits a program's inherent fine-grained parallelism, and "packs" its computations into as many functional units as possible. Thus the objective is to keep each unit busy, and to achieve better overall throughput.
Reference: [27] <author> V. Wolfe, S. Davidson, and I. Lee. RTC: </author> <title> Language support for real-time concurrency. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 43-52. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1991. </year>
Reference-contexts: This typically involves multiple, painful phases of instrumentation and hand-optimization. Additional measures may include re-coding key subsystems in assembly language, off-loading functions in programmable logic, or perhaps redesigning the system altogether. Several programming languages help manage the requirements side of the design equation; examples are <ref> [13, 17, 19, 23, 27] </ref>. These languages provide programmers with a convenient means of postulating timing constraints within a program's text. The constraints are, in turn, conveyed to the real-time scheduler as a directive, or perhaps replaced by kernel calls to be invoked at runtime. <p> Most other real-time languages do not make such a distinction, and instead place constraints on the boundaries of code blocks. Two paradigms are used in these languages: either constraints are expressed directly in the program itself (as in <ref> [19, 17, 27] </ref>), or they are postulated in a separate interface, and then passed to the scheduler as directives.
Reference: [28] <author> M. Younis, T. Marlowe, and A. Stoyenko. </author> <title> Compiler transformations for speculative execution in a real-time system. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: The objective is to produce a transformed program possessing a smaller variance in its execution time. In [23] a partial evaluator is applied to a source program, which produces residual code that is both more optimized and more deterministic. In <ref> [28] </ref> an approach to speculative execution is postulated for distributed real-time systems. This is tangentially similar to our application of "speculative transformations," since both break control-dependences that are predicated on inputs. The principal objective in [28], however, is quite different, in that "shadow threads" are forked off to execute on available <p> In <ref> [28] </ref> an approach to speculative execution is postulated for distributed real-time systems. This is tangentially similar to our application of "speculative transformations," since both break control-dependences that are predicated on inputs. The principal objective in [28], however, is quite different, in that "shadow threads" are forked off to execute on available resources. 3 In [20] time-critical statements (or events) are assumed in the underlying programming language, and used for developing the notion of safe real-time code transformations.
Reference: [29] <author> N. Zhang, A. Burns, and M. Nicholson. </author> <title> Pipelined processors and worst case execution times. </title> <journal> The Journal of Real-Time Systems, </journal> <volume> 5(4), </volume> <month> October </month> <year> 1993. </year> <month> 32 </month>
Reference-contexts: (P, &m)" and "input (Q, &x)," as well as a 20ms deadline between the events generated by "input (P, &m)" and "output (R, y)." Meanwhile, the bracketed "20ms" denotes that the unobservable statement S requires a maximum of 20ms to execute, a bound obtained by a timing analysis tool (e.g., <ref> [11, 18, 24, 25, 29] </ref>). Consequently, the program possesses an inherent conflict, since S requires 20ms to execute while it is only allowed 10ms. We address this problem by an approach we call feasible code synthesis.
References-found: 29

