URL: ftp://theory.lcs.mit.edu/pub/cilk/randall-phdthesis.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~cilk/abstracts/randall-phdthesis.html
Root-URL: 
Title: Cilk: Efficient Multithreaded Computing  
Author: by Keith H. Randall Arthur C. Smith 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science and Engineering at the  All rights reserved. Author  Certified by Charles E. Leiserson Professor of Computer Science and Engineering Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: June 1998  May 21, 1998  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1998.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering | a new definition. </title> <booktitle> In Pro ceedings of the 17th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <pages> pages 2-14, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: didactic example. 20 #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;cilk.h&gt; cilk int fib (int n) if (n&lt;2) return n; else int x, y; x = spawn fib (n-1); y = spawn fib (n-2); sync; return (x+y); -cilk int main (int argc, char *argv []) int n, result; n = atoi (argv <ref> [1] </ref>); result = spawn fib (n); sync; printf ("Result: %d"n", result); return 0; (using a very bad algorithm). be invoked only via spawn statements. 2 These restrictions ensure that parallel code is invoked with parallel spawn calls and serial code is invoked with regular C calls. 2.3 Inlets Ordinarily, when a <p> [63] and various forms of processor consistency [47] are too expensive to implement in a distributed setting. (Even modern SMP's do not typically implement sequential consistency.) Relaxed models, such as Gao and Sarkar's location consistency [43] (not the same as Frigo's location consistency [40]) and various forms of release consistency <ref> [1, 33, 44] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, 150 such as the acquisition or release of a lock.
Reference: [2] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing systems, </booktitle> <pages> pages 274-281, </pages> <address> Arlington, Texas, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Causal memory <ref> [2] </ref> ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B.
Reference: [3] <author> Andrew W. Appel and Zhong Shao. </author> <title> Empirical and analytic study of stack versus heap cost for languages with closures. </title> <journal> Journal of Functional Programming, </journal> <volume> 6(1) </volume> <pages> 47-74, </pages> <year> 1996. </year>
Reference-contexts: On the downside, heap allocation can potentially waste more memory than stack allocation due to fragmentation. For a careful analysis of the relative merits of stack and heap based allocation that supports 40 heap allocation, see the paper by Appel and Shao <ref> [3] </ref>. For an equally careful analysis that supports stack allocation, see [73].
Reference: [4] <author> Nimar S. Arora, Robert D. Blumofe, and C. Greg Plaxton. </author> <title> Thread scheduling for multiprogrammed multiprocessors. </title> <booktitle> In Proceedings of the Tenth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <address> Puerto Vallarta, Mexico, </address> <month> June </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: For example, even if a worker is suspended by the operating system during the execution of pop, the infrequency of locking in the THE protocol means that a thief can usually complete a steal operation on the worker's deque. Recent work by Arora et al. <ref> [4] </ref> has shown that a completely nonblocking work-stealing scheduler can be implemented. Using these ideas, Lisiecki and Medina [68] have modified the Cilk-5 scheduler to make it completely nonblocking. Their experience is that the THE protocol greatly simplifies a nonblocking implementation.
Reference: [5] <author> Joshua E. Barnes. </author> <title> A hierarchical O(N log N ) N -body code. Available on the Internet from ftp://hubble.ifa.hawaii.edu/pub/barnes/treecode/. </title>
Reference-contexts: To back up this claim, Figure 1-2 presents some comparisions among three versions of Barnes-Hut (the last application listed in Figure 1-1), an application which simulates the motion of galaxies under the influence of gravity. The three versions are the serial C version obtained from Barnes's web page <ref> [5] </ref>, the Cilk parallelization of that code, and the SPLASH-2 parallelization [101]. SPLASH-2 is a standard parallel library similar to POSIX threads developed at Stan 11 ford. <p> Barnes-Hut is part of the SPLASH-2 benchmark suite from Stanford [101], a set of standard parallel benchmark applications. We currently have a version of the Barnes-Hut algorithm coded in Cilk, derived from a C version of the algorithm obtained from Barnes's home page <ref> [5] </ref>. This C code was also the basis for the SPLASH-2 benchmark program.
Reference: [6] <author> Philippe Bekaert, Frank Suykens de Laet, and Philip Dutre. Renderpark, </author> <year> 1997. </year> <note> Available on the Internet from http://www.cs.kuleuven.ac./cwis/ research/graphics/RENDERPARK/. </note>
Reference-contexts: This algorithm is analogous to the typical way a hash table is accessed in parallel. For this program, we have k = L = 1. rad: A 3-dimensional radiosity renderer running on a "maze" scene. The original 75-source-file C code was developed in Belgium by Bekaert et. al. <ref> [6] </ref>. We used Cilk to parallelize its scene geometry calculations. Each surface in the scene has its own lock, as does each "patch" of the surface.
Reference: [7] <author> Monica Beltrametti, Kenneth Bobey, and John R. Zorbas. </author> <title> The control mech anism for the Myrias parallel computer system. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: The notion that independent tasks may have incoherent views of each others' memory is not new to Cilk. The BLAZE [70] language incorporated a memory semantics similar to that of dag consistency into a PASCAL-like language. The Myrias <ref> [7] </ref> computer was designed to support a relaxed memory semantics similar to dag consistency, with many of the mechanisms implemented in hardware. Loosely-Coherent Memory [64] allows for a range of consistency protocols and uses compiler support to direct their use. <p> By leveraging this high-level knowledge, Backer in conjunction with Cilk's work-stealing scheduler is able to execute Cilk programs with the performance bounds shown here. The BLAZE parallel language [70] and the Myrias parallel computer <ref> [7] </ref> define a high-level relaxed consistency model much like dag consistency, but we do not know of any efficient implementation of either of these systems. After an extensive literature search, we are aware of no other distributed shared memory with analytical performance bounds for any nontrivial algorithms.
Reference: [8] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Digest of Papers from the Thirty-Eighth IEEE Computer Society International Conference (Spring COMPCON), </booktitle> <pages> pages 528-537, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Most DSM's implement one of these relaxed consistency models [20, 59, 61, 90], though some implement a fixed collection of consistency models <ref> [8] </ref>, while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [64, 86]. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [9] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 39(3), </volume> <month> March </month> <year> 1996. </year> <month> 170 </month>
Reference-contexts: For example, the computation generated by the execution of fib (3) from the program in Figure 2-1 generates the thread dag shown in Figure 2-6. Any computation can be measured in terms of its "work" and "critical-path length" <ref> [9, 15, 16, 60] </ref>. Consider the computation that results when a given Cilk program is used to solve a given input problem.
Reference: [10] <author> Guy E. Blelloch, Phillip B. Gibbons, and Yossi Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 1-12, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The critical path is T 1 = fi (lg 2 n), so using our performance model, the 4 In recent work, Blelloch, Gibbons, and Matias <ref> [10] </ref> have shown that "series-parallel" dag computations can be scheduled to achieve substantially better space bounds than we report here. For example, they give a bound of S P (n) = O (n 2 + P lg 2 n) for matrix multiplication.
Reference: [11] <author> Robert D. Blumofe. </author> <title> Executing Multithreaded Programs Efficiently. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The original Cilk-1 release <ref> [11, 14, 58] </ref> featured a provably efficient, randomized, "work-stealing" scheduler [11, 16], but the language was clumsy, because parallelism was exposed "by hand" using explicit continuation passing. <p> The original Cilk-1 release [11, 14, 58] featured a provably efficient, randomized, "work-stealing" scheduler <ref> [11, 16] </ref>, but the language was clumsy, because parallelism was exposed "by hand" using explicit continuation passing. The Cilk-2 language provided better linguistic support by allowing the user to write code using natural "spawn" and "sync" keywords, which the compiler then converted to the Cilk-1 continuation-passing form automatically. <p> The theoretical analysis presented in <ref> [11, 16] </ref> cites two fundamental lower bounds as to how fast a Cilk program can run. For a computation with T 1 work, the lower bound T P T 1 =P must hold, because at most P units of work can be executed in a single step. <p> In addition, the lower bound T P T 1 must hold, since a finite number of processors cannot execute faster than an infinite number. 5 Cilk's randomized work-stealing scheduler <ref> [11, 16] </ref> executes a Cilk computation that does not use locks 6 on P processors in expected time T P = T 1 =P + O (T 1 ) ; (2.1) assuming an ideal parallel computer. <p> As shown in Section 2.8, Cilk's randomized work-stealing scheduler executes a Cilk computation on P processors in expected time T P = T 1 =P + O (T 1 ). Importantly, all communication costs due to Cilk's scheduler are borne by the critical-path term <ref> [11, 16] </ref>, as are most of the other scheduling costs. <p> In an analysis of the Cilk scheduler, Blumofe and Leiserson have shown that the space used by a P -processor execution is at most S 1 P in the worst case <ref> [11, 16] </ref>. We improve this characterization of the space requirements, and we provide a much stronger upper bound on the space requirements of regular divide-and-conquer Cilk programs. <p> In addition, we bound the number of page faults. The exposition of the proofs in this section makes heavy use of results and techniques from <ref> [11, 16] </ref>. In the following analysis, we consider the computation that results when a given Cilk program is executed to solve a given input problem. <p> A busy-leaves scheduler is a scheduler with the property that at all times during the execution, if a procedure has no living children, then that procedure has a processor working on it. Cilk's work-stealing scheduler is a busy-leaves scheduler <ref> [11, 16] </ref>. We shall proceed through a series of lemmas that provide an exact characterization of the space used by "simple" Cilk programs when executed by a busy-leaves scheduler. <p> A simple Cilk program is a program in which each procedure's control consists of allocating memory, spawning children, waiting for the children to complete, deallocating memory, and returning, in that order. We shall then specialize this characterization to provide space bounds for regular divide-and-conquer Cilk programs. Previous work <ref> [11, 16] </ref> has shown that a busy-leaves scheduler can efficiently execute a Cilk program on P processors using no more space than P times the space required to execute the program on a single processor. <p> worst-case space bound of S P (n) = fi (P (n=P 1=3 ) 2 ) = fi (n 2 P 1=3 ). 4 Note that this space bound is better than the O (n 2 P ) bound obtained by just using the O (S 1 P ) bound from <ref> [11, 16] </ref>. We have already computed the computational work and critical path length of the blockedmul algorithm in Section 4.1.1. Using these values we can compute the total work and estimate the total running time T P (C; n). <p> After an extensive literature search, we are aware of no other distributed shared memory with analytical performance bounds for any nontrivial algorithms. We are also currently working on supporting dag-consistent shared memory in our Cilk-NOW runtime system <ref> [11] </ref> which executes Cilk programs in an adaptively parallel and fault-tolerant manner on networks of workstations. We expect that the "well-structured" nature of Cilk computations will allow the runtime system to maintain dag consistency efficiently, even in the presence of processor faults.
Reference: [12] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> Dag-consistent distributed shared memory. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium (IPPS), </booktitle> <pages> pages 132-141, </pages> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: consistency [63]: The result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations 1 The contents of this chapter are joint work with Robert Blumofe, Matteo Frigo, Christopher Joerg, and Charles Leiserson and appeared at IPPS'96 <ref> [12] </ref>. 107 of each individual processor appear in this sequence in the order specified by its program.
Reference: [13] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> An analysis of dag-consistent distributed shared-memory algorithms. </title> <booktitle> In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 297-308, </pages> <address> Padua, Italy, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: matrices incurs F 1 (C; n) = O (n 3 =m 3=2 C) faults and uses S P (n) = O (n 2 P 1=3 ) space. 1 The contents of this chapter are joint work with Robert Blumofe, Matteo Frigo, Christopher Joerg, and Charles Leiserson and appeared at SPAA'96 <ref> [13] </ref>. 126 7.1 Introduction To analyze the performance of Cilk programs which use a shared virtual address space implemented by Backer, we must take into account all of the protocol actions required by Backer.
Reference: [14] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1) </volume> <pages> 55-69, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The original Cilk-1 release <ref> [11, 14, 58] </ref> featured a provably efficient, randomized, "work-stealing" scheduler [11, 16], but the language was clumsy, because parallelism was exposed "by hand" using explicit continuation passing. <p> This definition means, however, that T 1 does not necessarily correspond to the running time of the nondeterministic program on 1 processor. 5 This abstract model of execution time ignores real-life details, such as memory-hierarchy effects, but is nonetheless quite accurate <ref> [14] </ref>. 6 The Cilk computation model provides no guarantees for scheduling performance when a program contains lock statements. <p> The graph also shows the speedup predicted by the formula T P = T 1 =P + T 1 . We also attempted to measure the critical-path overhead c 1 . We used the synthetic knary benchmark <ref> [14] </ref> to synthesize computations artificially with a wide range of work and critical-path lengths. Figure 3-7 shows the outcome from many such experiments. The figure plots the measured speedup T 1 =T P for each run against the machine size P for that run. <p> In order to plot different computations on the same graph, we normalized the machine size and the speedup by dividing these values by the average parallelism P = T 1 =T 1 , as was done in <ref> [14] </ref>. For each run, the horizontal position of the plotted datum is the inverse of the slackness P=P , and thus, the normalized machine size is 1:0 when the number of processors is equal to the average parallelism.
Reference: [15] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 207-216, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: For example, the computation generated by the execution of fib (3) from the program in Figure 2-1 generates the thread dag shown in Figure 2-6. Any computation can be measured in terms of its "work" and "critical-path length" <ref> [9, 15, 16, 60] </ref>. Consider the computation that results when a given Cilk program is used to solve a given input problem. <p> We view the abstract execution machine for Cilk as a (sequentially consistent [63]) shared memory together with a collection of interpreters, each with some private state. (See <ref> [15, 28, 51] </ref> for examples of multithreaded implementations similar to this model.) Interpreters are dynamically created during execution by each spawn statement. The ith such child of an interpreter is given a unique interpreter name by appending i to its parent's name.
Reference: [16] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multithreaded com putations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mex-ico, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Backer, the protocol that implements dag consistency, is a simple protocol that nevertheless provides good performance on the Connection Machine CM5 for a variety of Cilk applications. 15 * A proof that Backer, together with the work-stealing scheduler from <ref> [16] </ref>, gives provably good performance. By weakening the memory consistency model to dag consistency, we are able to use Backer, a memory consistency protocol that we can analyze theoretically. <p> The original Cilk-1 release [11, 14, 58] featured a provably efficient, randomized, "work-stealing" scheduler <ref> [11, 16] </ref>, but the language was clumsy, because parallelism was exposed "by hand" using explicit continuation passing. The Cilk-2 language provided better linguistic support by allowing the user to write code using natural "spawn" and "sync" keywords, which the compiler then converted to the Cilk-1 continuation-passing form automatically. <p> For example, the computation generated by the execution of fib (3) from the program in Figure 2-1 generates the thread dag shown in Figure 2-6. Any computation can be measured in terms of its "work" and "critical-path length" <ref> [9, 15, 16, 60] </ref>. Consider the computation that results when a given Cilk program is used to solve a given input problem. <p> The theoretical analysis presented in <ref> [11, 16] </ref> cites two fundamental lower bounds as to how fast a Cilk program can run. For a computation with T 1 work, the lower bound T P T 1 =P must hold, because at most P units of work can be executed in a single step. <p> In addition, the lower bound T P T 1 must hold, since a finite number of processors cannot execute faster than an infinite number. 5 Cilk's randomized work-stealing scheduler <ref> [11, 16] </ref> executes a Cilk computation that does not use locks 6 on P processors in expected time T P = T 1 =P + O (T 1 ) ; (2.1) assuming an ideal parallel computer. <p> As shown in Section 2.8, Cilk's randomized work-stealing scheduler executes a Cilk computation on P processors in expected time T P = T 1 =P + O (T 1 ). Importantly, all communication costs due to Cilk's scheduler are borne by the critical-path term <ref> [11, 16] </ref>, as are most of the other scheduling costs. <p> In an analysis of the Cilk scheduler, Blumofe and Leiserson have shown that the space used by a P -processor execution is at most S 1 P in the worst case <ref> [11, 16] </ref>. We improve this characterization of the space requirements, and we provide a much stronger upper bound on the space requirements of regular divide-and-conquer Cilk programs. <p> In addition, we bound the number of page faults. The exposition of the proofs in this section makes heavy use of results and techniques from <ref> [11, 16] </ref>. In the following analysis, we consider the computation that results when a given Cilk program is executed to solve a given input problem. <p> We shall show that with probability at least 1 *, an execution contains only O (T 1 + lg (1=*)) rounds. To bound the number of rounds, we shall use a delay-sequence argument. We define a modified dag D 0 exactly as in <ref> [16] </ref>. (The dag D 0 is for the purposes of analysis only and has no effect on the computation.) The critical-path length of D 0 is at most 2T 1 . <p> Since there are at least 4P steal requests during the first 22CP events, the probability is at least 1=2 that any task that is critical at the beginning of a round is the target of a steal request <ref> [16, Lemma 10] </ref>, if it is not executed locally by the processor on which it resides. Any task takes at most 3mC + 1 4mC time to execute, since we are ignoring the effects of congestion for the moment. <p> We want to show that with probability at least 1 *, the total number of rounds is O (T 1 + lg (1=*)). Consider a possible delay sequence. Recall from <ref> [16] </ref> that a delay sequence of size R is a maximal path U in the augmented dag D 0 of length at most 2T 1 , along with a partition of R which represents the number of rounds during which each task of the path in D 0 is critical. <p> Then for any * &gt; 0, the execution time is O (T 1 (C)=P + mCT 1 + m lg P + mC lg (1=*)) with probability at least 1 *. Moreover, the expected execution time is O (T 1 (C)=P + mCT 1 ). 135 Proof: As in <ref> [16] </ref>, we shall use an accounting argument to bound the running time. During the execution, at each time step, each processor puts a dollar into one of 5 buckets according to its activity at that time step. <p> A busy-leaves scheduler is a scheduler with the property that at all times during the execution, if a procedure has no living children, then that procedure has a processor working on it. Cilk's work-stealing scheduler is a busy-leaves scheduler <ref> [11, 16] </ref>. We shall proceed through a series of lemmas that provide an exact characterization of the space used by "simple" Cilk programs when executed by a busy-leaves scheduler. <p> A simple Cilk program is a program in which each procedure's control consists of allocating memory, spawning children, waiting for the children to complete, deallocating memory, and returning, in that order. We shall then specialize this characterization to provide space bounds for regular divide-and-conquer Cilk programs. Previous work <ref> [11, 16] </ref> has shown that a busy-leaves scheduler can efficiently execute a Cilk program on P processors using no more space than P times the space required to execute the program on a single processor. <p> worst-case space bound of S P (n) = fi (P (n=P 1=3 ) 2 ) = fi (n 2 P 1=3 ). 4 Note that this space bound is better than the O (n 2 P ) bound obtained by just using the O (S 1 P ) bound from <ref> [11, 16] </ref>. We have already computed the computational work and critical path length of the blockedmul algorithm in Section 4.1.1. Using these values we can compute the total work and estimate the total running time T P (C; n).
Reference: [17] <author> Ronald F. Boisvert, Roldan Pozo, Karin Remington, Richard Barrett, and Jack J. Dongarra. </author> <title> The matrix market: A web resource for test matrix collections. </title> <editor> In Ronald F. Boisvert, editor, </editor> <booktitle> Quality of Numerical Software, Assessment and Enhancement, </booktitle> <pages> pages 125-137. </pages> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1997. </year> <note> Web address http://math.nist.gov/MatrixMarket. </note>
Reference-contexts: Figure 3-5 shows the results of an experiment for the matrix bcsstk32 obtained from Matrix Market <ref> [17] </ref>. We can see that with an average parallelism of 420, the Cholesky algorithm run on this particular problem instance should scale well. 61 well. These experiments were run with the sparse matrix bcsstk29 obtained from Matrix Market [17], reordered using MATLAB's column minimum degree permutation. <p> results of an experiment for the matrix bcsstk32 obtained from Matrix Market <ref> [17] </ref>. We can see that with an average parallelism of 420, the Cholesky algorithm run on this particular problem instance should scale well. 61 well. These experiments were run with the sparse matrix bcsstk29 obtained from Matrix Market [17], reordered using MATLAB's column minimum degree permutation. The matrix has 13; 992 rows with 316; 740 nonzero elements. Despite the irregular nature of our parallel sparse matrix computation, the Cilk runtime system is able to schedule the computation efficiently on our 8 processor UltraSPARC SMP.
Reference: [18] <author> G. A. Boughton. </author> <title> Arctic routing chip. </title> <booktitle> In Proceedings of Hot Interconnects II, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Distributed Cilk runs on multiple platforms and networks. There are currently implementations for Sun Ultra 5000 SMP's, Digital Alpha 4100 SMP's, and Penium Pro SMP's. Currently supported networks include UDP over Ethernet, Digital's Memory Channel, and MIT's Arctic network <ref> [18, 56] </ref>. This section gives some preliminary performance numbers for our Alpha 4100 SMP implementation running with the Memory Channel interconnect. Importantly, no changes were made to the source code of the program to run it on distributed Cilk.
Reference: [19] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <month> April </month> <year> 1974. </year> <month> 171 </month>
Reference-contexts: This equation resembles "Brent's theorem" <ref> [19, 48] </ref> and is optimal to within a constant factor, since T 1 =P and T 1 are both lower bounds. We call the first term on the right-hand side of Equation (2.1) the work term and the second term the critical-path term.
Reference: [20] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [67] and others <ref> [20, 39, 61] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently. <p> Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [20, 59, 61, 90] </ref>, though some implement a fixed collection of consistency models [8], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [64, 86].
Reference: [21] <author> David Chaiken and Anant Agarwal. </author> <title> Software-extended coherent shared mem ory: Performance and cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <pages> pages 314-324, </pages> <address> Chicago, Illi-nois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [67] and others [20, 39, 61], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [21, 62, 86] </ref> require some degree of hardware support [91] to manage shared memory efficiently.
Reference: [22] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <pages> pages 147-158, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [22, 59, 88, 90, 97] </ref> require either an object-oriented programming model or explicit user management of objects. 124 The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel.
Reference: [23] <author> Guang-Ien Cheng, Mingdong Feng, Charles E. Leiserson, Keith H. Randall, and Andrew F. Stark. </author> <title> Detecting data races in Cilk programs that use locks. </title> <booktitle> In Proceedings of the Tenth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <address> Puerto Vallarta, Mexico, </address> <month> June </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: We present an algorithm, Brelly, 1 The contents of this chapter are joint work with Guang-Ien Cheng, Mingdong Feng, Charles Leiserson, and Andrew Stark and will appear at SPAA'98 <ref> [23] </ref>. 73 which detects violations of the umbrella discipline in O (kT ff (V; V )) time using O (kV ) space.
Reference: [24] <institution> Cilk-5.1 (Beta 1) Reference Manual. </institution> <note> Available on the Internet from http://theory.lcs.mit.edu/~cilk. </note>
Reference-contexts: Chapter 8 we describe the latest distributed version of Cilk for clusters of SMP's, and outline MultiBacker, our multilevel consistency protocol. 17 Chapter 2 The Cilk language This chapter presents a overview of the Cilk extensions to C as supported by Cilk-5. (For a complete description, consult the Cilk-5 manual <ref> [24] </ref>.) The key features of the language are the specification of parallelism and synchronization, through the spawn and sync keywords, and the specification of nondeterminism, using inlet and abort. <p> With shared memory provided in hardware, much of the explicit shared memory support could be made implicit, simplifying the language and enabling sharing of all memory, including global and stack variables. The Cilk language implemented by our latest Cilk-5 release <ref> [24] </ref> still uses a theoretically efficient scheduler, but the language has been simplified considerably. It employs call/return semantics for parallelism and features a linguistically simple "in-let" mechanism for nondeterministic control. Cilk-5 is designed to run efficiently on contemporary symmetric multiprocessors (SMP's), which feature hardware support for shared memory. <p> Unlike in Cilk-1, where the Cilk scheduler was an identifiable piece of code, in Cilk-5 both the compiler and runtime system bear the responsibility for scheduling. Cilk-5's compiler cilk2c is a source-to-source translator <ref> [74, 24] </ref> which converts the 1 The contents of this chapter are joint work with Matteo Frigo and Charles Leiserson and will appear at PLDI'98 [41]. 31 with our runtime system library to create an executable. Cilk constructs into regular C code. <p> We compiled our programs with gcc 2.7.2 at optimization level -O3. For a full description of these programs, see the Cilk 5.1 manual <ref> [24] </ref>. The table shows the work of each Cilk program T 1 , the critical path T 1 , and the two derived quantities P and c 1 .
Reference: [25] <author> Edward G. Coffman, Jr. and Peter J. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: Indeed, at the point when one execution has just taken its Cth page fault, each cache contains exactly the last C distinct pages referenced <ref> [25] </ref>. We can now count the number of page faults during the execution E. The fault behavior of E is the same as the fault behavior of E 0 except for the subcomputation T and the subcomputation, call it U , from which it stole. <p> We shall further assume that pages in the cache are maintained using the popular LRU (least-recently-used) <ref> [25] </ref> heuristic. In addition to servicing page faults, Backer must reconcile pages between the processor page caches and the backing store so that the semantics of the execution obey the assumptions of dag consistency.
Reference: [26] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: A spawn statement can spawn only Cilk procedures, not C functions, and Cilk procedures can 1 This program uses an inefficient algorithm which runs in exponential time. Although logarithmic-time methods are known <ref> [26, p. 850] </ref>, this program nevertheless provides a good didactic example. 20 #include &lt;stdlib.h&gt; #include &lt;stdio.h&gt; #include &lt;cilk.h&gt; cilk int fib (int n) if (n&lt;2) return n; else int x, y; x = spawn fib (n-1); y = spawn fib (n-2); sync; return (x+y); -cilk int main (int argc, char *argv <p> As the program never holds more than one lock at a time, we have k = L = 1. bucket: A bucket sort <ref> [26, Section 9.4] </ref>. Parallel threads acquire the lock associated with a bucket before adding elements to it. This algorithm is analogous to the typical way a hash table is accessed in parallel. <p> Consequently, we obtain the recurrence F 1 (C; n) &gt; &lt; aF 1 (C; n=b) + p (n) if n &gt; n C ; (7.1) We can solve this recurrence using standard techniques <ref> [26, Section 4.4] </ref>. We iterate the recurrence, stopping as soon as we reach the first value of the iteration count k such that n=b k n C holds, or equivalently when k = dlog b (n=n C )e holds. <p> The solution to this recurrence <ref> [26, Section 4.4] </ref> is * S 1 (n) = fi (lg k+1 n), if s (n) = fi (lg k n) for some constant k 0, and * S 1 (n) = fi (s (n)), if s (n) = (n * ) for some constant * &gt; 0 and in addition
Reference: [27] <author> Daved E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishna murthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Ore-gon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The dag-consistent shared memory performs at 5 megaflops per processor as long as the work per processor is sufficiently large. This performance compares fairly well with other matrix multiplication codes on the CM5 (that do not use the CM5's vector units). For example, an implementation coded in Split-C <ref> [27] </ref> attains just over 6 megaflops per processor on 64 processors using a static data layout, a static thread schedule, and an optimized assembly-language inner loop. In contrast, Cilk's dag-consistent shared memory is mapped across the processors dynamically, and the Cilk threads performing the computation are scheduled dynamically at runtime.
Reference: [28] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Cilk provides an inlet feature for this purpose, which was inspired in part by the inlet feature of TAM <ref> [28] </ref>. An inlet is essentially a C function internal to a Cilk procedure. In the normal syntax of Cilk, the spawning of a procedure must occur as a separate statement and not in an expression. <p> Cilk is a faithful extension of C, however, supporting the simplifying notion of a C elision and allowing Cilk to exploit the C compiler technology more readily. TAM <ref> [28] </ref> and Lazy Threads [46] also analyze many of the same overhead issues in a more general, "nonstrict" language setting, where the individual performances of a whole host of mechanisms are required for applications to obtain good overall performance. <p> We view the abstract execution machine for Cilk as a (sequentially consistent [63]) shared memory together with a collection of interpreters, each with some private state. (See <ref> [15, 28, 51] </ref> for examples of multithreaded implementations similar to this model.) Interpreters are dynamically created during execution by each spawn statement. The ith such child of an interpreter is given a unique interpreter name by appending i to its parent's name.
Reference: [29] <author> E. W. Dijkstra. </author> <title> Solution of a problem in concurrent programming control. </title> <journal> Communications of the ACM, </journal> <volume> 8(9):569, </volume> <month> September </month> <year> 1965. </year>
Reference-contexts: The slow clone is executed in the infrequent case that parallel semantics and its concomitant bookkeeping are required. All communication due to scheduling occurs in the slow clone and contributes to critical-path overhead, but not to work overhead. The work-first principle also inspired a Dijkstra-like <ref> [29] </ref>, shared-memory, mutual-exclusion protocol as part of the runtime load-balancing scheduler. Cilk's scheduler uses a "work-stealing" algorithm in which idle processors, called thieves, "steal" threads from busy processors, called victims. Cilk's scheduler guarantees that the cost of stealing contributes only to critical-path overhead, and not to work overhead. <p> We also feel that the current overheads are sufficiently low that other problems, notably minimizing overheads for data synchronization, deserve more attention. 3.3 Implemention of work-stealing In this section, we describe Cilk-5's work-stealing mechanism, which is based on a Dijkstra-like <ref> [29] </ref>, shared-memory, mutual-exclusion protocol called the "THE" protocol. In accordance with the work-first principle, this protocol has been designed to minimize work overhead. For example, on a 167-megahertz UltraSPARC I, the fib program with the THE protocol runs about 25% faster than with hardware locking primitives. <p> This solution has the same fundamental problem as the interrupt and polling mechanisms just described, however. Whenever a worker pops a frame, it pays the heavy price to grab a lock, which contributes to work overhead. Consequently, we adopted a solution that employs Dijkstra's protocol for mutual exclusion <ref> [29] </ref>, which assumes only that reads and writes are atomic. Because our protocol uses three atomic shared variables T, H, and E, we call it the THE protocol.
Reference: [30] <author> Anne Dinning and Edith Schonberg. </author> <title> An empirical comparison of monitoring al gorithms for access anomaly detection. </title> <booktitle> In Proceedings of the Second ACM SIG-PLAN Symposium on Principles & Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 1-10. </pages> <publisher> ACM Press, </publisher> <year> 1990. </year> <month> 172 </month>
Reference-contexts: Some dynamic race detectors perform a post-mortem analysis based on program execution traces [34, 53, 72, 79], while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler <ref> [30, 31, 37, 38, 71, 83] </ref>, by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84]. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks.
Reference: [31] <author> Anne Dinning and Edith Schonberg. </author> <title> Detecting access anomalies in programs with critical sections. </title> <booktitle> In Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 85-96. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: Some dynamic race detectors perform a post-mortem analysis based on program execution traces [34, 53, 72, 79], while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler <ref> [30, 31, 37, 38, 71, 83] </ref>, by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84]. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks. <p> Tighter, more complicated bounds on All-Sets will be given in Section 5.2. In previous work, Dinning and Schonberg's "lock-covers" algorithm <ref> [31] </ref> also detects all data races in a computation. The All-Sets algorithm improves the lock-covers algorithm by generalizing the data structures and techniques from the original Nondeterminator to produce better time and space bounds. <p> For abelian programs, therefore, All-Sets and Brelly can verify the determinacy of the program on a given input. Our results on abelian programs formalize and generalize the claims of Dinning and Schonberg <ref> [31, 32] </ref>, who argue that for "internally deterministic" programs, checking a single computation suffices to detect all races in the program. The remainder of this chapter is organized as follows. Section 5.2 presents the All-Sets algorithm, and Section 5.3 presents the Brelly algorithm. <p> False reports are not a problem when the program being debugged is abelian, but programmers would like to know whether an ostensibly abelian program is actually abelian. Dinning and Schonberg give a conservative compile-time algorithm to check if a program is "internally deterministic" <ref> [31] </ref>, and we have given thought to how the abelian property might likewise be conservatively checked. The parallelizing compiler techniques of Rinard and Diniz [87] may be applicable. We are currently investigating versions of All-Sets and Brelly that correctly detect races even when parallelism is allowed within critical sections.
Reference: [32] <author> Anne Carolyn Dinning. </author> <title> Detecting Nondeterminism in Shared Memory Parallel Programs. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, </institution> <address> New York University, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: For abelian programs, therefore, All-Sets and Brelly can verify the determinacy of the program on a given input. Our results on abelian programs formalize and generalize the claims of Dinning and Schonberg <ref> [31, 32] </ref>, who argue that for "internally deterministic" programs, checking a single computation suffices to detect all races in the program. The remainder of this chapter is organized as follows. Section 5.2 presents the All-Sets algorithm, and Section 5.3 presents the Brelly algorithm.
Reference: [33] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Unfortunately, they have generally found that Lamport's model is difficult to implement efficiently, and hence relaxed models of shared-memory consistency have been developed <ref> [33, 43, 44] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are "processor centric" in the sense that they define consistency in terms of actions by physical processors. <p> [63] and various forms of processor consistency [47] are too expensive to implement in a distributed setting. (Even modern SMP's do not typically implement sequential consistency.) Relaxed models, such as Gao and Sarkar's location consistency [43] (not the same as Frigo's location consistency [40]) and various forms of release consistency <ref> [1, 33, 44] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, 150 such as the acquisition or release of a lock.
Reference: [34] <author> Perry A. Emrath, Sanjoy Ghosh, and David A. Padua. </author> <title> Event synchronization analysis for debugging parallel programs. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 580-588, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Since static debuggers cannot fully understand the semantics of programs, however, most race detectors are dynamic tools in which potential races are detected at runtime by executing the program on a given input. Some dynamic race detectors perform a post-mortem analysis based on program execution traces <ref> [34, 53, 72, 79] </ref>, while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler [30, 31, 37, 38, 71, 83], by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84].
Reference: [35] <author> Andrew Erlichson, Neal Nuckolls, Greg Chesson, and John Hennessy. </author> <title> Soft FLASH: Analyzing the performance of clustered distributed shared memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 210-220, </pages> <address> Cambridge, Massachusetts, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: The SMP has a single unified cache for all processors on the SMP. The processors of the SMP operate on the cache just as if they were executing as timeslices of a single processor. Most distributed shared memory protocols use this model for their SMP caches <ref> [61, 35] </ref>, because it does not cache redundant pages and allows hardware sharing within an SMP. Unified Backer does have a drawback, however. In particular, when one processor requires a consistency operation to be performed on a page, it must be performed with respect to all processors. <p> For instance, if a processor needs to invalidate a page because of a consistency operation, it must be invalidated by all processors on the SMP. Erlichson et al <ref> [35] </ref> call this the TLB synchronization problem, because invalidations are done by removing entries from a page table, and cached page table entries in the TLB must be removed as well. Invalidating TLB entries on multiple processors requires some form of interprocessor synchronization and can be expensive.
Reference: [36] <author> Marc Feeley. </author> <title> Polling efficiently on stock hardware. </title> <booktitle> In Proceedings of the 1993 ACM SIGPLAN Conference on Functional Programming and Computer Architecture, </booktitle> <pages> pages 179-187, </pages> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: As an alternative to sending interrupts, thieves could post steal requests, and workers could periodically poll for them. Once again, however, a cost accrues to the work overhead, this time for polling. Techniques are known that can limit the overhead of polling <ref> [36] </ref>, but they require the support of a sophisticated compiler. The work-first principle suggests that it is reasonable to put substantial effort into minimizing work overhead in the work-stealing protocol.
Reference: [37] <author> Mingdong Feng and Charles E. Leiserson. </author> <title> Efficient detection of determinacy races in Cilk programs. </title> <booktitle> In Proceedings of the Ninth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 1-11, </pages> <address> Newport, Rhode Island, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Some dynamic race detectors perform a post-mortem analysis based on program execution traces [34, 53, 72, 79], while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler <ref> [30, 31, 37, 38, 71, 83] </ref>, by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84]. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks. <p> On-the-fly debuggers directly instrument memory accesses via the compiler [30, 31, 37, 38, 71, 83], by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84]. The race-detection algorithms in this chapter are based on the Nondeterminator <ref> [37] </ref>, which finds "determinacy races" in Cilk programs that do not use locks. The Nondeterminator executes a Cilk program serially on a given input, maintaining an efficient "SP-bags" data structure to keep track of the logical series/parallel relationships between threads. <p> Similarly, we do not add the locker he; Hi to lockers [l ] if we record in line 9 that another stronger locker he 0 ; H 0 i is already in lockers [l ]. Before proving the correctness of All-Sets, we restate two important lemmas from <ref> [37] </ref>. Lemma 1 Suppose that three threads e 1 , e 2 , and e 3 execute in order in a serial, depth-first execution of a Cilk program, and suppose that e 1 e 2 and e 1 k e 3 . <p> To determine whether the currently executing thread is in series or parallel with previously executed threads, Brelly uses the SP-bags data structure from <ref> [37] </ref>. for future reference. If accessor [l ] e, we say the access is a serial access, and the algorithm performs lines 2-5, setting locks [l ] H and accessor [l ] e, as well as updating nonlocker [h] and alive [h] appropriately for each h 2 H. <p> Moreover, all writes and any one given read must also lie on a single path. Consequently, by Definition 1, every read of an object sees exactly one write to that object, and the execution is deterministic. This determinism guarantee can be verified by the Nondeterminator <ref> [37] </ref>, which checks for determinacy races. 2 We now describe the Backer coherence algorithm for maintaining dag-consistent shared memory. 3 In this algorithm, versions of shared-memory objects can reside simultaneously in any of the processors' local caches or the global backing store.
Reference: [38] <author> Yaacov Fenster. </author> <title> Detecting parallel access anomalies. </title> <type> Master's thesis, </type> <institution> Hebrew University, </institution> <month> March </month> <year> 1998. </year>
Reference-contexts: Some dynamic race detectors perform a post-mortem analysis based on program execution traces [34, 53, 72, 79], while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler <ref> [30, 31, 37, 38, 71, 83] </ref>, by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84]. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks.
Reference: [39] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. Andrews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201-213, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [67] and others <ref> [20, 39, 61] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently.
Reference: [40] <author> Matteo Frigo. </author> <title> The weakest reasonable memory model. </title> <type> Master's thesis, </type> <institution> Mas sachusetts Institute of Technology, </institution> <year> 1997. </year>
Reference-contexts: For a full discussion of constructibility in memory models and the relation of dag consistency to other memory models, see <ref> [40, 42] </ref>. 115 cessor can run with the speed of a serial algorithm with no overheads. <p> motivated by the fact that sequential consistency [63] and various forms of processor consistency [47] are too expensive to implement in a distributed setting. (Even modern SMP's do not typically implement sequential consistency.) Relaxed models, such as Gao and Sarkar's location consistency [43] (not the same as Frigo's location consistency <ref> [40] </ref>) and various forms of release consistency [1, 33, 44], ensure consistency (to varying degrees) only when explicit synchronization operations occur, 150 such as the acquisition or release of a lock.
Reference: [41] <author> Matteo Frigo, Charles E. Leiserson, and Keith H. Randall. </author> <booktitle> The implementa tion of the Cilk-5 multithreaded language. In Proceedings of the 1998 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1998. </year> <note> To appear. 173 </note>
Reference-contexts: Cilk-5's compiler cilk2c is a source-to-source translator [74, 24] which converts the 1 The contents of this chapter are joint work with Matteo Frigo and Charles Leiserson and will appear at PLDI'98 <ref> [41] </ref>. 31 with our runtime system library to create an executable. Cilk constructs into regular C code.
Reference: [42] <author> Matteo Frigo and Victor Luchangco. </author> <title> Computation-centric memory models. </title> <booktitle> In Proceedings of the Tenth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <address> Puerto Vallarta, Mexico, </address> <month> June </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: For a full discussion of constructibility in memory models and the relation of dag consistency to other memory models, see <ref> [40, 42] </ref>. 115 cessor can run with the speed of a serial algorithm with no overheads.
Reference: [43] <author> Guang R. Gao and Vivek Sarkar. </author> <title> Location consistency: Stepping beyond the memory coherence barrier. </title> <booktitle> In Proceedings of the 24th International Conference on Parallel Processing, </booktitle> <address> Aconomowoc, Wisconsin, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Unfortunately, they have generally found that Lamport's model is difficult to implement efficiently, and hence relaxed models of shared-memory consistency have been developed <ref> [33, 43, 44] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are "processor centric" in the sense that they define consistency in terms of actions by physical processors. <p> Intuitively, a read can "see" a write in the dag-consistency model only if there is some serial execution order consistent with the dag in which the read sees the write. Unlike sequential consistency, but similar to certain processor-centric models <ref> [43, 47] </ref>, dag consistency allows different reads to return values that are based on different serial orders, but the values returned must respect the dependencies in the dag. The mechanisms to support dag-consistent distributed shared memory on the Connection Machine CM5 are implemented in software. <p> Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [63] and various forms of processor consistency [47] are too expensive to implement in a distributed setting. (Even modern SMP's do not typically implement sequential consistency.) Relaxed models, such as Gao and Sarkar's location consistency <ref> [43] </ref> (not the same as Frigo's location consistency [40]) and various forms of release consistency [1, 33, 44], ensure consistency (to varying degrees) only when explicit synchronization operations occur, 150 such as the acquisition or release of a lock.
Reference: [44] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Unfortunately, they have generally found that Lamport's model is difficult to implement efficiently, and hence relaxed models of shared-memory consistency have been developed <ref> [33, 43, 44] </ref> that compromise on semantics for a faster implementation. By and large, all of these consistency models have had one thing in common: they are "processor centric" in the sense that they define consistency in terms of actions by physical processors. <p> [63] and various forms of processor consistency [47] are too expensive to implement in a distributed setting. (Even modern SMP's do not typically implement sequential consistency.) Relaxed models, such as Gao and Sarkar's location consistency [43] (not the same as Frigo's location consistency [40]) and various forms of release consistency <ref> [1, 33, 44] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, 150 such as the acquisition or release of a lock.
Reference: [45] <author> Andrew V. Goldberg and Robert E. Tarjan. </author> <title> A new approach to the maximum flow problem. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 136-146, </pages> <address> Berkeley, California, </address> <month> 28-30 May </month> <year> 1986. </year>
Reference-contexts: In order to compare our experimental 93 results with the theoretical bounds, we characterize our four test programs in terms of the parameters k and L: 4 maxflow: A maximum-flow code based on Goldberg's push-relabel method <ref> [45] </ref>. Each vertex in the graph contains a lock. Parallel threads perform simple operations asynchronously on graph edges and vertices.
Reference: [46] <author> S. C. Goldstein, K. E. Schauser, and D. E. Culler. </author> <title> Lazy threads: Implementing a fast parallel call. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37(1) </volume> <pages> 5-20, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: We believe that Cilk-5 work overhead is nearly as low as possible, given our goal of generating portable C output from our compiler. Other researchers have been able to reduce overheads even more, however, at the expense of portability. For example, lazy threads <ref> [46] </ref> obtains efficiency at the expense of implementing its own calling conventions, stack layouts, etc. Although we could in principle incorporate such machine-dependent techniques into our compiler, we feel that Cilk-5 strikes a good balance between performance and portability. <p> Cilk is a faithful extension of C, however, supporting the simplifying notion of a C elision and allowing Cilk to exploit the C compiler technology more readily. TAM [28] and Lazy Threads <ref> [46] </ref> also analyze many of the same overhead issues in a more general, "nonstrict" language setting, where the individual performances of a whole host of mechanisms are required for applications to obtain good overall performance.
Reference: [47] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 61, </type> <institution> IEEE Scalable Coherent Interface (SCI) Working Group, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Intuitively, a read can "see" a write in the dag-consistency model only if there is some serial execution order consistent with the dag in which the read sees the write. Unlike sequential consistency, but similar to certain processor-centric models <ref> [43, 47] </ref>, dag consistency allows different reads to return values that are based on different serial orders, but the values returned must respect the dependencies in the dag. The mechanisms to support dag-consistent distributed shared memory on the Connection Machine CM5 are implemented in software. <p> Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [63] and various forms of processor consistency <ref> [47] </ref> are too expensive to implement in a distributed setting. (Even modern SMP's do not typically implement sequential consistency.) Relaxed models, such as Gao and Sarkar's location consistency [43] (not the same as Frigo's location consistency [40]) and various forms of release consistency [1, 33, 44], ensure consistency (to varying degrees)
Reference: [48] <author> R. L. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 17(2) </volume> <pages> 416-429, </pages> <month> March </month> <year> 1969. </year>
Reference-contexts: This equation resembles "Brent's theorem" <ref> [19, 48] </ref> and is optimal to within a constant factor, since T 1 =P and T 1 are both lower bounds. We call the first term on the right-hand side of Equation (2.1) the work term and the second term the critical-path term.
Reference: [49] <author> Dirk Grunwald. </author> <title> Heaps o' stacks: Time and space efficient threads without oper ating system support. </title> <type> Technical Report CU-CS-750-94, </type> <institution> University of Colorado, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The generated C code has the same general structure as the C elision, with a few additional statements. In lines 4-5, an activation frame is allocated for fib and initialized. The Cilk runtime system uses activation frames to represent procedure instances. Using techniques similar to <ref> [49, 50] </ref>, our inlined allocator typically takes only a few cycles. The frame is initialized in line 5 by storing a pointer to a static structure, called a signature, describing fib. The first spawn in fib is translated into lines 12-17.
Reference: [50] <author> Dirk Grunwald and Richard Neves. </author> <title> Whole-program optimization for time and space efficient threads. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 50-59, </pages> <address> Cambridge, Massachusetts, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: The generated C code has the same general structure as the C elision, with a few additional statements. In lines 4-5, an activation frame is allocated for fib and initialized. The Cilk runtime system uses activation frames to represent procedure instances. Using techniques similar to <ref> [49, 50] </ref>, our inlined allocator typically takes only a few cycles. The frame is initialized in line 5 by storing a pointer to a static structure, called a signature, describing fib. The first spawn in fib is translated into lines 12-17.
Reference: [51] <author> Michael Halbherr, Yuli Zhou, and Chris F. Joerg. </author> <title> MIMD-style parallel pro gramming with continuation-passing threads. </title> <booktitle> In Proceedings of the 2nd International Workshop on Massive Parallelism: Hardware, Software, and Applications, </booktitle> <address> Capri, Italy, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: We view the abstract execution machine for Cilk as a (sequentially consistent [63]) shared memory together with a collection of interpreters, each with some private state. (See <ref> [15, 28, 51] </ref> for examples of multithreaded implementations similar to this model.) Interpreters are dynamically created during execution by each spawn statement. The ith such child of an interpreter is given a unique interpreter name by appending i to its parent's name.
Reference: [52] <author> E. A. Hauck and B. A. Dent. </author> <title> Burroughs' B6500/B7500 stack mechanism. </title> <booktitle> Pro ceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 245-251, </pages> <year> 1968. </year>
Reference-contexts: We implement a heap allocator along the lines of C's malloc and free, but many times a simpler allocation model suffices. Cilk provides stack-like allocation in what is called a cactus-stack <ref> [52, 77, 94] </ref> to handle these simple allocations. From the point of view of a single Cilk procedure, a cactus stack behaves much like 26 procedure sees its own stack allocations and the stack allocated by its ancestors. The stack grows downwards.
Reference: [53] <author> David P. Helmbold, Charles E. McDowell, and Jian-Zhong Wang. </author> <title> Analyzing traces with anonymous synchronization. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages II70-II77, </pages> <month> August </month> <year> 1990. </year> <month> 174 </month>
Reference-contexts: Since static debuggers cannot fully understand the semantics of programs, however, most race detectors are dynamic tools in which potential races are detected at runtime by executing the program on a given input. Some dynamic race detectors perform a post-mortem analysis based on program execution traces <ref> [34, 53, 72, 79] </ref>, while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler [30, 31, 37, 38, 71, 83], by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84].
Reference: [54] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: a Quanti tative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: If the runtime system had to operate on every word independently, however, the system would be terribly inefficient. Since extra fetches and reconciles do not adversely affect the Backer coherence algorithm, we implemented the familiar strategy of grouping objects into pages <ref> [54, Section 8.2] </ref>, each of which is fetched or reconciled as a unit. Assuming that spatial locality exists when objects are accessed, grouping objects helps amortize the runtime system overhead.
Reference: [55] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: a Quanti tative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <note> second edition, </note> <year> 1996. </year>
Reference-contexts: Cilk guarantees that critical sections locked by the same lock act atomically with respect to each other. Acquiring and releasing a Cilk lock has the memory consistency semantics of release consistency <ref> [55, p. 716] </ref>. Locks must be initialized using the function Cilk lock init. An example procedure that uses locks is shown in Figure 2-3. This program computes a simple histogram of the elements in the array elements.
Reference: [56] <author> James C. Hoe. StarT-X: </author> <title> A one-man-year exercise in network interface engineer ing. </title> <booktitle> In Proceedings of Hot Interconnects VI, </booktitle> <month> August </month> <year> 1998. </year>
Reference-contexts: Distributed Cilk runs on multiple platforms and networks. There are currently implementations for Sun Ultra 5000 SMP's, Digital Alpha 4100 SMP's, and Penium Pro SMP's. Currently supported networks include UDP over Ethernet, Digital's Memory Channel, and MIT's Arctic network <ref> [18, 56] </ref>. This section gives some preliminary performance numbers for our Alpha 4100 SMP implementation running with the Memory Channel interconnect. Importantly, no changes were made to the source code of the program to run it on distributed Cilk.
Reference: [57] <author> Richard C. Holt. </author> <title> Some deadlock properties of computer systems. </title> <journal> Computing Surveys, </journal> <volume> 4(3) </volume> <pages> 179-196, </pages> <month> September </month> <year> 1972. </year>
Reference-contexts: A locking discipline is a programming methodology that dictates a restriction on the use of locks. For example, many programs adopt the discipline of acquiring locks in a fixed order so as to avoid deadlock <ref> [57] </ref>. Similarly, the "umbrella" locking discipline precludes data races. It requires that each location be protected by the same lock within every parallel subcomputation of the computation.
Reference: [58] <author> Christopher F. Joerg. </author> <title> The Cilk System for Parallel Multithreaded Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Mass-achusetts Institute of Technology, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: The original Cilk-1 release <ref> [11, 14, 58] </ref> featured a provably efficient, randomized, "work-stealing" scheduler [11, 16], but the language was clumsy, because parallelism was exposed "by hand" using explicit continuation passing. <p> Each processor's cache contains objects recently used by the threads that have executed 2 The Nondeterminator-2 is not required in this case because locks are not part of the definition of dag consistency. 3 See <ref> [58] </ref> for details of a "lazier" coherence algorithm than Backer based on climbing the spawn tree. 113 on that processor, and the backing store provides default global storage for each object.
Reference: [59] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Irregular applications like Barnes-Hut and Strassen's algorithm provide a good test of Cilk's ability to schedule computations dynamically. We achieve a speedup of 9 on an 8192-particle Barnes-Hut simulation using 32 processors, which is competitive with other software implementations of distributed shared memory <ref> [59] </ref> on the CM5. Strassen's algorithm runs as fast as regular matrix multiplication on a small number of processors for 2048 fi 2048 matrices. 109 The remainder of this chapter is organized as follows. Section 6.2 gives a motivating example of dag consistency using the blockedmul matrix multiplication algorithm. <p> In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [22, 59, 88, 90, 97] </ref> require either an object-oriented programming model or explicit user management of objects. 124 The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel. <p> Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [20, 59, 61, 90] </ref>, though some implement a fixed collection of consistency models [8], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [64, 86].
Reference: [60] <author> Richard M. Karp and Vijaya Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science|Volume A: Algorithms and Complexity, chapter 17, </booktitle> <pages> pages 869-941. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: For example, the computation generated by the execution of fib (3) from the program in Figure 2-1 generates the thread dag shown in Figure 2-6. Any computation can be measured in terms of its "work" and "critical-path length" <ref> [9, 15, 16, 60] </ref>. Consider the computation that results when a given Cilk program is used to solve a given input problem.
Reference: [61] <author> Pete Keleher, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. Tread Marks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In USENIX Winter 1994 Conference Proceedings, </booktitle> <pages> pages 115-132, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: We describe the grouping of shared memory into pages and describe the "diff" mechanism <ref> [61] </ref> for managing dirty bits. Finally, we discuss minor anomalies in atomicity that can occur when the size of the concrete objects supported by the shared-memory system is different from the abstract objects that the programmer manipulates. The Cilk-3 system on the CM5 supports concrete shared-memory objects of 32-bit words. <p> Current microprocessors do not provide hardware support for maintaining user-level dirty bits at the granularity of words. Rather than using dirty bits explicitly, Cilk uses a diff mechanism as is used in the Treadmarks system <ref> [61] </ref>. The diff mechanism computes the dirty bit for an object by comparing that object's value with its value in a twin copy made at fetch time. Our implementation makes this twin copy only for pages loaded in read/write mode, thereby avoiding the overhead of copying for read-only 116 pages. <p> Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [67] and others <ref> [20, 39, 61] </ref>, Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently. <p> Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [20, 59, 61, 90] </ref>, though some implement a fixed collection of consistency models [8], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [64, 86]. <p> The SMP has a single unified cache for all processors on the SMP. The processors of the SMP operate on the cache just as if they were executing as timeslices of a single processor. Most distributed shared memory protocols use this model for their SMP caches <ref> [61, 35] </ref>, because it does not cache redundant pages and allows hardware sharing within an SMP. Unified Backer does have a drawback, however. In particular, when one processor requires a consistency operation to be performed on a page, it must be performed with respect to all processors.
Reference: [62] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford Flash multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <pages> pages 302-313, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [67] and others [20, 39, 61], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [21, 62, 86] </ref> require some degree of hardware support [91] to manage shared memory efficiently.
Reference: [63] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The exception mechanism is used to implement Cilk's abort statement. Interestingly, this extension does not introduce any additional work overhead. The pseudocode of the simplified THE protocol is shown in Figure 3-3. Assume that shared memory is sequentially consistent <ref> [63] </ref>. 5 The code assumes that the ready deque is implemented as an array of frames. The head and tail of the deque are determined by two indices T and H, which are stored in shared memory and are visible to all processors. <p> In order to study the determinacy of abelian programs, we first give a formal multithreaded machine model that more precisely describes an actual execution of a Cilk program. We view the abstract execution machine for Cilk as a (sequentially consistent <ref> [63] </ref>) shared memory together with a collection of interpreters, each with some private state. (See [15, 28, 51] for examples of multithreaded implementations similar to this model.) Interpreters are dynamically created during execution by each spawn statement. <p> We present empirical evidence that this warm-up overhead is actually much smaller in practice than the theoretical bound. 6.1 Introduction Why do we care about weak memory consistency models? Architects of shared memory for parallel computers have attempted to support Lamport's strong model of sequential consistency <ref> [63] </ref>: The result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations 1 The contents of this chapter are joint work with Robert Blumofe, Matteo Frigo, Christopher Joerg, and Charles Leiserson and appeared at IPPS'96 [12]. 107 <p> Relaxed shared-memory consistency models are motivated by the fact that sequential consistency <ref> [63] </ref> and various forms of processor consistency [47] are too expensive to implement in a distributed setting. (Even modern SMP's do not typically implement sequential consistency.) Relaxed models, such as Gao and Sarkar's location consistency [43] (not the same as Frigo's location consistency [40]) and various forms of release consistency [1,
Reference: [64] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory sys tem support for parallel language implementation. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 208-218, </pages> <address> San Jose, California, </address> <month> Octo-ber </month> <year> 1994. </year> <month> 175 </month>
Reference-contexts: The BLAZE [70] language incorporated a memory semantics similar to that of dag consistency into a PASCAL-like language. The Myrias [7] computer was designed to support a relaxed memory semantics similar to dag consistency, with many of the mechanisms implemented in hardware. Loosely-Coherent Memory <ref> [64] </ref> allows for a range of consistency protocols and uses compiler support to direct their use. Compared with these systems, Cilk provides a multithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory. <p> Most DSM's implement one of these relaxed consistency models [20, 59, 61, 90], though some implement a fixed collection of consistency models [8], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies <ref> [64, 86] </ref>. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [65] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays Trees Hypercubes. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, Cali-fornia, </address> <year> 1992. </year>
Reference-contexts: This algorithm can be parallelized to obtain computational work T 1 (n) = fi (n 3 ) and critical-path length T 1 (n) = fi (lg n) <ref> [65] </ref>. If the matrix B does not fit into the cache, that is, mC &lt; n 2 , then in the serial execution, every access to a block of B causes a page fault.
Reference: [66] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: in Cilk, communication to enforce dependencies can be amortized against steals, so such communication does not happen often if the computation has enough parallel slackness. 6.4 Implementation This section describes our implementation of dag-consistent shared memory for the Cilk-3 runtime system running on the Connection Machine Model CM5 parallel supercomputer <ref> [66] </ref>. We describe the grouping of shared memory into pages and describe the "diff" mechanism [61] for managing dirty bits.
Reference: [67] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Compared with these systems, Cilk provides a multithreaded programming model based on directed acyclic graphs, which leads to a more flexible linguistic expression of operations on shared memory. Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy <ref> [67] </ref> and others [20, 39, 61], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently.
Reference: [68] <author> Phillip Lisiecki and Alberto Medina. </author> <type> Personal communication. </type>
Reference-contexts: Recent work by Arora et al. [4] has shown that a completely nonblocking work-stealing scheduler can be implemented. Using these ideas, Lisiecki and Medina <ref> [68] </ref> have modified the Cilk-5 scheduler to make it completely nonblocking. Their experience is that the THE protocol greatly simplifies a nonblocking implementation. The simplified THE protocol can be extended to support the signaling of exceptions to a worker.
Reference: [69] <author> Victor Luchangco. </author> <title> Precedence-based memory models. </title> <booktitle> In Eleventh International Workshop on Distributed Algorithms (WDAG97), number 1320 in Lecture Notes in Computer Science, </booktitle> <pages> pages 215-229. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1997. </year>
Reference-contexts: Consequently, the pro 4 For a rigorous proof that Backer maintains dag consistency, see <ref> [69] </ref>. In fact, Backer maintains a stronger memory model, called location consistency , which is the weakest memory model stronger than dag consistency that is constructible (exactly implementable by an on-line algorithm).
Reference: [70] <author> Piyush Mehrotra and John Van Rosendale. </author> <title> The BLAZE language: A parallel language for scientific programming. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 339-361, </pages> <year> 1987. </year>
Reference-contexts: To conclude, we briefly outline work in this area and offer some ideas for future work. The notion that independent tasks may have incoherent views of each others' memory is not new to Cilk. The BLAZE <ref> [70] </ref> language incorporated a memory semantics similar to that of dag consistency into a PASCAL-like language. The Myrias [7] computer was designed to support a relaxed memory semantics similar to dag consistency, with many of the mechanisms implemented in hardware. <p> By leveraging this high-level knowledge, Backer in conjunction with Cilk's work-stealing scheduler is able to execute Cilk programs with the performance bounds shown here. The BLAZE parallel language <ref> [70] </ref> and the Myrias parallel computer [7] define a high-level relaxed consistency model much like dag consistency, but we do not know of any efficient implementation of either of these systems.
Reference: [71] <author> John Mellor-Crummey. </author> <title> On-the-fly detection of data races for programs with nested fork-join parallelism. </title> <booktitle> In Proceedings of Supercomputing'91, </booktitle> <pages> pages 24-33. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Some dynamic race detectors perform a post-mortem analysis based on program execution traces [34, 53, 72, 79], while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler <ref> [30, 31, 37, 38, 71, 83] </ref>, by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84]. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks.
Reference: [72] <author> Barton P. Miller and Jong-Deok Choi. </author> <title> A mechanism for efficient debugging of parallel programs. </title> <booktitle> In Proceedings of the 1988 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 135-144, </pages> <address> Atlanta, Georgia, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Since static debuggers cannot fully understand the semantics of programs, however, most race detectors are dynamic tools in which potential races are detected at runtime by executing the program on a given input. Some dynamic race detectors perform a post-mortem analysis based on program execution traces <ref> [34, 53, 72, 79] </ref>, while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler [30, 31, 37, 38, 71, 83], by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84].
Reference: [73] <author> James S. Miller and Guillermo J. Rozas. </author> <title> Garbage collection is fast, but a stack is faster. </title> <type> Technical Report Memo 1462, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: For a careful analysis of the relative merits of stack and heap based allocation that supports 40 heap allocation, see the paper by Appel and Shao [3]. For an equally careful analysis that supports stack allocation, see <ref> [73] </ref>. Thus, although the work-first principle gives a general understanding of where overheads should be borne, our experience with Cilk-4 showed that large enough critical-path overheads can tip the scales to the point where the assumptions underlying the principle no longer hold.
Reference: [74] <author> Robert C. Miller. </author> <title> A type-checking preprocessor for Cilk 2, a multithreaded C language. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Unlike in Cilk-1, where the Cilk scheduler was an identifiable piece of code, in Cilk-5 both the compiler and runtime system bear the responsibility for scheduling. Cilk-5's compiler cilk2c is a source-to-source translator <ref> [74, 24] </ref> which converts the 1 The contents of this chapter are joint work with Matteo Frigo and Charles Leiserson and will appear at PLDI'98 [41]. 31 with our runtime system library to create an executable. Cilk constructs into regular C code.
Reference: [75] <author> Sang Lyul Min and Jong-Deok Choi. </author> <title> An efficient cache-based access anomaly detection scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (AS-PLOS), </booktitle> <pages> pages 235-244, </pages> <address> Palo Alto, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: On-the-fly debuggers directly instrument memory accesses via the compiler [30, 31, 37, 38, 71, 83], by binary rewriting [89], or by augmenting the machine's cache coherence protocol <ref> [75, 84] </ref>. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks.
Reference: [76] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead, Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year> <month> 176 </month>
Reference-contexts: Lastly, we describe how the runtime system links together the actions of the fast and slow clones to produce a complete Cilk implementation. As in lazy task creation <ref> [76] </ref>, in Cilk-5 each processor, called a worker , maintains a ready deque (doubly-ended queue) of ready procedures (technically, procedure instances). Each deque has two ends, a head and a tail , from which procedures can be added or removed. <p> Mohr et al. <ref> [76] </ref> introduced lazy task creation in their implementation of the Mul-T language. Lazy task creation is similar in many ways to our lazy scheduling techniques. Mohr et al. report a work overhead of around 2 when comparing with serial T, the Scheme dialect on which Mul-T is based.
Reference: [77] <author> Joel Moses. </author> <title> The function of FUNCTION in LISP or why the FUNARG problem should be called the envronment problem. </title> <type> Technical Report memo AI-199, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> June </month> <year> 1970. </year>
Reference-contexts: We implement a heap allocator along the lines of C's malloc and free, but many times a simpler allocation model suffices. Cilk provides stack-like allocation in what is called a cactus-stack <ref> [52, 77, 94] </ref> to handle these simple allocations. From the point of view of a single Cilk procedure, a cactus stack behaves much like 26 procedure sees its own stack allocations and the stack allocated by its ancestors. The stack grows downwards. <p> Similarly, when procedure P 3 is spawned by P 2 , the cactus stack branches again. Cactus stacks have many of the same limitations as ordinary procedure stacks <ref> [77] </ref>. For instance, a child procedure cannot return to its parent a pointer to an object that it has allocated from the cactus stack. Similarly, sibling procedures cannot share 27 threads of each procedure instance are ordered by horizontal continue edges.
Reference: [78] <author> Greg Nelson, K. Rustan M. Leino, James B. Saxe, and Raymie Stata. </author> <title> Ex tended static checking home page, </title> <note> 1996. Available on the Internet from http://www.research.digital.com/SRC/esc/Esc.html. </note>
Reference-contexts: Since a data race is usually a bug, automatic data-race detection has been studied extensively. Static race detectors <ref> [78] </ref> can sometimes determine whether a program 74 int x; cilk void foo3 () -Cilk_lockvar A, B; Cilk_lock (&B); x++; cilk void foo1 () - Cilk_unlock (&B); Cilk_lock (&A); -Cilk_lock (&B); x += 5; cilk int main () -Cilk_unlock (&B); Cilk_lock_init (&A); Cilk_unlock (&A); Cilk_lock_init (&B); - x = 0; spawn
Reference: [79] <author> Robert H. B. Netzer and Sanjoy Ghosh. </author> <title> Efficient race condition detection for shared-memory programs with post/wait synchronization. </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Since static debuggers cannot fully understand the semantics of programs, however, most race detectors are dynamic tools in which potential races are detected at runtime by executing the program on a given input. Some dynamic race detectors perform a post-mortem analysis based on program execution traces <ref> [34, 53, 72, 79] </ref>, while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler [30, 31, 37, 38, 71, 83], by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84].
Reference: [80] <author> Robert H. B. Netzer and Barton P. Miller. </author> <title> On the complexity of event ordering for shared-memory parallel program executions. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, pages II: </booktitle> <pages> 93-97, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: the measures of coding effort used here are arguably imprecise, they nevertheless suggest that less effort is required to parallelize the original C code using Cilk than using a thread library like SPLASH-2, and the resulting parallelization obtains better end-to-end speedup. 1.3 Debugging Parallel codes are notoriously hard to debug <ref> [80] </ref>. Much of this difficulty arises from either intentional or unintentional nondeterministic behavior of parallel programs. <p> Section 5.5 defines the notion of abelian programs and proves that data-race free abelian programs produce determinate results. Section 5.6 offers some concluding remarks. 2 Even in simple models, finding feasible data races is NP-hard <ref> [80] </ref>. 79 to show only the accesses to shared location x.
Reference: [81] <author> Robert H. B. Netzer and Barton P. Miller. </author> <title> What are race conditions? ACM Letters on Programming Languages and Systems, </title> <booktitle> 1(1) </booktitle> <pages> 74-88, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For example, the data-race free program in Figure 5-2 can be verified by All-Sets to be data-race free, but Brelly will detect an umbrella discipline violation. Most dynamic race detectors, like All-Sets and Brelly, attempt to find, in the terminology of Netzer and Miller <ref> [81] </ref>, apparent data races|those that appear to occur in a computation according to the parallel control constructs|rather than feasible data races|those that can actually occur during program execution. The 78 distinction arises, because operations in critical sections may affect program control depending on the way threads are scheduled.
Reference: [82] <author> Rishiyur Sivaswami Nikhil. </author> <booktitle> Parallel Symbolic Computing in Cid. In Proc. Wk shp. on Parallel Symbolic Computing, </booktitle> <address> Beaune, France, </address> <publisher> Springer-Verlag LNCS 1068, </publisher> <pages> pages 217-242, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Mohr et al. report a work overhead of around 2 when comparing with serial T, the Scheme dialect on which Mul-T is based. Our research confirms the intuition behind their methods and shows that work overheads of close to 1 are achievable. The Cid language <ref> [82] </ref> is like Cilk in that it uses C as a base language and has a simple preprocessing compiler to convert parallel Cid constructs to C. Cid is designed to work in a distributed memory environment, and so it employs latency-hiding mechanisms which Cilk-5 could avoid.
Reference: [83] <author> Itzhak Nudler and Larry Rudolph. </author> <title> Tools for the efficient development of efficient parallel programs. </title> <booktitle> In Proceedings of the First Israeli Conference on Computer Systems Engineering, </booktitle> <month> May </month> <year> 1986. </year>
Reference-contexts: Some dynamic race detectors perform a post-mortem analysis based on program execution traces [34, 53, 72, 79], while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler <ref> [30, 31, 37, 38, 71, 83] </ref>, by binary rewriting [89], or by augmenting the machine's cache coherence protocol [75, 84]. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks.
Reference: [84] <author> Dejan Perkovic and Peter Keleher. </author> <title> Online data-race detection via coherency guarantees. </title> <booktitle> In Proceedings of the Second USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <address> Seattle, Washington, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: On-the-fly debuggers directly instrument memory accesses via the compiler [30, 31, 37, 38, 71, 83], by binary rewriting [89], or by augmenting the machine's cache coherence protocol <ref> [75, 84] </ref>. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks. <p> In previous work, Dinning and Schonberg's "lock-covers" algorithm [31] also detects all data races in a computation. The All-Sets algorithm improves the lock-covers algorithm by generalizing the data structures and techniques from the original Nondeterminator to produce better time and space bounds. Perkovic and Keleher <ref> [84] </ref> offer an on-the-fly race-detection algorithm that "piggybacks" on a cache-coherence protocol for lazy release consistency.
Reference: [85] <author> Keith H. Randall. </author> <title> Solving Rubik's cube. </title> <booktitle> In Proceedings of the 1998 MIT Stu dent Workshop on High-Performance Computing in Science and Engineering, </booktitle> <month> January </month> <year> 1998. </year>
Reference-contexts: From these numbers, we conclude that Cilk provides both a programming environment which enables parallel programming at a higher level of abstraction, and performance competitive with implementations that use no such abstractions. 64 4.4 Rubik's cube Rubik's cube has fascinated mathematicians and laypeople alike since its introduction by Erno Rubik <ref> [93, 85] </ref>. Despite the fact that a large amount of research has been done on Rubik's cube, some important questions remain unanswered.
Reference: [86] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Ty phoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <pages> pages 325-336, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Cilk's implementation of dag consistency borrows heavily on the experiences from previous implementations of distributed shared memory. Like Ivy [67] and others [20, 39, 61], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines <ref> [21, 62, 86] </ref> require some degree of hardware support [91] to manage shared memory efficiently. <p> Most DSM's implement one of these relaxed consistency models [20, 59, 61, 90], though some implement a fixed collection of consistency models [8], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies <ref> [64, 86] </ref>. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [87] <author> Martin C. Rinard and Pedro C. Diniz. </author> <title> Commutativity analysis: A new anal ysis framework for parallelizing compilers. </title> <booktitle> In Proceedings of the 1996 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 54-67, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Dinning and Schonberg give a conservative compile-time algorithm to check if a program is "internally deterministic" [31], and we have given thought to how the abelian property might likewise be conservatively checked. The parallelizing compiler techniques of Rinard and Diniz <ref> [87] </ref> may be applicable. We are currently investigating versions of All-Sets and Brelly that correctly detect races even when parallelism is allowed within critical sections.
Reference: [88] <author> Martin C. Rinard, Daniel J. Scales, and Monica S. Lam. </author> <title> Jade: A high-level machine-independent language for parallel programming. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year> <month> 177 </month>
Reference-contexts: In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [22, 59, 88, 90, 97] </ref> require either an object-oriented programming model or explicit user management of objects. 124 The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel.
Reference: [89] <author> Stefan Savage, Michael Burrows, Greg Nelson, Patric Sobalvarro, and Thomas Anderson. Eraser: </author> <title> A dynamic race detector for multi-threaded programs. </title> <booktitle> In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: Some dynamic race detectors perform a post-mortem analysis based on program execution traces [34, 53, 72, 79], while others perform an "on-the-fly" analysis during program execution. On-the-fly debuggers directly instrument memory accesses via the compiler [30, 31, 37, 38, 71, 83], by binary rewriting <ref> [89] </ref>, or by augmenting the machine's cache coherence protocol [75, 84]. The race-detection algorithms in this chapter are based on the Nondeterminator [37], which finds "determinacy races" in Cilk programs that do not use locks. <p> If a program obeys the umbrella discipline, a data race cannot occur, because parallel accesses are always protected by the same lock. The Brelly algorithm detects violations of the umbrella locking discipline. Savage et al. <ref> [89] </ref> originally suggested that efficient debugging tools can be developed by requiring programs to obey a locking discipline. Their Eraser tool enforces a simple discipline in which any shared variable is protected by a single lock throughout the course of the program execution.
Reference: [90] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [22, 59, 88, 90, 97] </ref> require either an object-oriented programming model or explicit user management of objects. 124 The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel. <p> Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [20, 59, 61, 90] </ref>, though some implement a fixed collection of consistency models [8], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [64, 86].
Reference: [91] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASP-LOS), </booktitle> <pages> pages 297-306, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Like Ivy [67] and others [20, 39, 61], Cilk's implementation uses fixed-sized pages to cut down on the overhead of managing shared objects. In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support <ref> [91] </ref> to manage shared memory efficiently.
Reference: [92] <author> Jaswinder Pal Singh. </author> <type> Personal communication. </type>
Reference-contexts: SPLASH-2 is a standard parallel library similar to POSIX threads developed at Stan 11 ford. The SPLASH-2 Barnes-Hut code, derived from the serial C code <ref> [92] </ref>, is part of the SPLASH-2 parallel benchmark suite. original C code using both Cilk and SPLASH-2. One simple measure of effort is the number of lines that were added to the code.
Reference: [93] <author> David Singmaster. </author> <title> Notes on Rubik's Magic Cube. </title> <publisher> Enslow Publishers, </publisher> <address> Hillside, New Jersey, </address> <year> 1980. </year>
Reference-contexts: From these numbers, we conclude that Cilk provides both a programming environment which enables parallel programming at a higher level of abstraction, and performance competitive with implementations that use no such abstractions. 64 4.4 Rubik's cube Rubik's cube has fascinated mathematicians and laypeople alike since its introduction by Erno Rubik <ref> [93, 85] </ref>. Despite the fact that a large amount of research has been done on Rubik's cube, some important questions remain unanswered.
Reference: [94] <author> Per Stenstrom. </author> <title> VLSI support for a cactus stack oriented memory organization. </title> <booktitle> Proceedings of the Twenty-First Annual Hawaii International Conference on System Sciences, </booktitle> <volume> volume 1, </volume> <pages> pages 211-220, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: We implement a heap allocator along the lines of C's malloc and free, but many times a simpler allocation model suffices. Cilk provides stack-like allocation in what is called a cactus-stack <ref> [52, 77, 94] </ref> to handle these simple allocations. From the point of view of a single Cilk procedure, a cactus stack behaves much like 26 procedure sees its own stack allocations and the stack allocated by its ancestors. The stack grows downwards. <p> Because of the cactus stack semantics of the Cilk stack (see Section 2.7), however, Cilk-4 had to manage the virtual-memory map on each processor explicitly, as was done in <ref> [94] </ref>. The work overhead in Cilk-4 for frame allocation was little more than that of incrementing the stack pointer, but whenever the stack pointer overflowed a page, an expensive user-level interrupt ensued, during which Cilk-4 would modify the memory map.
Reference: [95] <author> Robert Stets, Sandhya Dwarkadas, Nikolaos Hardavellas, Galen Hunt, Leonidas Kontothanassis, Srinivasan Parthasarathy, and Michael Scott. Cashmere-2L: </author> <title> Software coherent shared memory on a clustered remote-write network. </title> <booktitle> In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Saint-Malo, France, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: Because some processors may have a page mapped invalid while others have it mapped valid, a page fetch cannot simply copy a page into the cache, because pages with dirty entries cannot be overwritten. Instead, we use a procedure called two-way diffing , described in <ref> [95] </ref> as "outgoing" and "incoming" diffs. Just as with the Backer algorithm described in Section 6.4, an outgoing diff compares the working copy of a page with its twin, and sends any data that have been written since the twin was created to the backing store.
Reference: [96] <author> Volker Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numerische Mathematik, </journal> <volume> 14(3) </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: The average parallelism is still quite large, however, as can be seen from Figure 3-5, where a 1024 fi 1024 notempmul multiply has a parallelism of 1970. Thus, the use of notempmul is generally a win except on very large machines. We also experimented with Strassen's algorithm <ref> [96] </ref>, a subcubic algorithm for matrix multiplication. The algorithm is significantly more complicated than that shown in Figure 4-1, but still required only an evening to code in Cilk. On one processor, Strassen is competitive with our other matrix multiplication codes for large matrices.
Reference: [97] <author> Andrew S. Tanenbaum, Henri E. Bal, and M. Frans Kaashoek. </author> <title> Programming a distributed system using shared objects. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 5-12, </pages> <address> Spokane, Washington, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In contrast, systems that use cache lines [21, 62, 86] require some degree of hardware support [91] to manage shared memory efficiently. As another alternative, systems that use arbitrary-sized objects or regions <ref> [22, 59, 88, 90, 97] </ref> require either an object-oriented programming model or explicit user management of objects. 124 The idea of dag-consistent shared memory can be extended to the domain of file I/O to allow multiple threads to read and write the same file in parallel.
Reference: [98] <author> Robert Endre Tarjan. </author> <title> Applications of path compression on balanced trees. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 26(4) </volume> <pages> 690-715, </pages> <month> October </month> <year> 1979. </year>
Reference-contexts: This execution order mirrors that of normal C programs: every sub-computation that is spawned executes completely before the procedure that spawned it continues. While executing the program, SP-bags maintains an SP-bags data structure based on Tarjan's nearly linear-time least-common-ancestors algorithm <ref> [98] </ref>. 3 The Nondeterminator-2 can still be used with programs for which this assumption does not hold, but the race detector prints a warning, and some races may be missed.
Reference: [99] <author> Leslie G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: First, we assume that Cilk's scheduler operates in practice according 33 to the theoretical model presented in Section 2.8. Second, we assume that in the common case, ample "parallel slackness" <ref> [99] </ref> exists, that is, the average parallelism of a Cilk program exceeds the number of processors on which we run it by a sufficient margin. Third, we assume (as is indeed the case) that every Cilk program has a C elision against which its one-processor performance can be measured. <p> Define the average parallelism as P = T 1 =T 1 , which corresponds to the maximum possible speedup that the application can obtain. Define also the parallel slackness <ref> [99] </ref> to be the ratio P =P . The assumption of parallel slackness is that P =P c 1 , which means that the number P of processors is much smaller than the average parallelism P .
Reference: [100] <author> David S. Wise. </author> <title> Representing matrices as quadtrees for parallel processors. </title> <journal> Information Processing Letters, </journal> <volume> 20(4) </volume> <pages> 195-199, </pages> <month> July </month> <year> 1985. </year> <month> 178 </month>
Reference-contexts: For our parallel sparse Cholesky factorization algorithm, we chose to change the matrix representation to allow the problem to be parallelized more efficiently. We represent a matrix as a quadtree <ref> [100] </ref>, which is a recursive data structure shown in Figure 4-5. A matrix M is represented by pointers to the four submatrices that make up the quadrants of M . Sparsity is encoded by representing submatrices with 60 all zero elements as null pointers.
Reference: [101] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The splash-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The three versions are the serial C version obtained from Barnes's web page [5], the Cilk parallelization of that code, and the SPLASH-2 parallelization <ref> [101] </ref>. SPLASH-2 is a standard parallel library similar to POSIX threads developed at Stan 11 ford. The SPLASH-2 Barnes-Hut code, derived from the serial C code [92], is part of the SPLASH-2 parallel benchmark suite. original C code using both Cilk and SPLASH-2. <p> Thus, our algorithm is only recommended for matrices which are well-conditioned. 4.3 Barnes-Hut Barnes-Hut is an algorithm for simulating the motion of particles under the influence of gravity. Barnes-Hut is part of the SPLASH-2 benchmark suite from Stanford <ref> [101] </ref>, a set of standard parallel benchmark applications. We currently have a version of the Barnes-Hut algorithm coded in Cilk, derived from a C version of the algorithm obtained from Barnes's home page [5]. This C code was also the basis for the SPLASH-2 benchmark program.
Reference: [102] <author> Matthew J. Zekauskas, Wayne A. Sawdon, and Brian N. Bershad. </author> <title> Software write detection for a distributed shared memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year> <month> 179 </month>
Reference-contexts: Our implementation makes this twin copy only for pages loaded in read/write mode, thereby avoiding the overhead of copying for read-only 116 pages. The diff mechanism imposes extra overhead on each reconcile, but it imposes no extra overhead on each access <ref> [102] </ref>. Dag consistency can suffer from atomicity anomalies when abstract objects that the programmer is reading and writing are larger than the concrete objects supported by the shared-memory system. For example, suppose the programmer is treating two 4-byte concrete objects as one 8-byte abstract object.
References-found: 102

