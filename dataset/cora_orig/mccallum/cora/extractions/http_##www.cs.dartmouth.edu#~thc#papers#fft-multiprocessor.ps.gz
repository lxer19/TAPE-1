URL: http://www.cs.dartmouth.edu/~thc/papers/fft-multiprocessor.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~thc/papers.html
Root-URL: http://www.cs.dartmouth.edu
Title: Multiprocessor Out-of-Core FFTs with Distributed Memory and Parallel Disks (Extended Abstract)  
Author: Thomas H. Cormen Jake Wegmann David M. Nicol 
Affiliation: Dartmouth College Department of Computer Science  
Note: PCSTR97-303  
Abstract: Dartmouth College Computer Science Technical Report Abstract This paper extends an earlier out-of-core Fast Fourier Transform (FFT) method for a uniprocessor with the Parallel Disk Model (PDM) to use multiple processors. Four out-of-core multiprocessor methods are examined. Operationally, these methods differ in the size of "mini-butterfly" computed in memory and how the data are organized on the disks and in the distributed memory of the multiprocessor. The methods also perform differing amounts of I/O and communication. Two of them have the remarkable property that even though they are computing the FFT on a multiprocessor, all interprocessor communication occurs outside the mini-butterfly computations. Performance results on a small workstation cluster indicate that except for unusual combinations of problem size and memory size, the methods that do not perform interprocessor communication during the mini-butterfly computations require approximately 86% of the time of those that do. Moreover, the faster methods are much easier to implement.
Abstract-found: 1
Intro-found: 1
Reference: [Bai90] <author> David H. Bailey. </author> <title> FFTs in external or hierarchical memory. </title> <journal> The Journal of Supercomputing, </journal> <volume> 4 </volume> <pages> 23-35, </pages> <year> 1990. </year>
Reference-contexts: The project currently requires FFTs with 10 gigapoints, and it desires FFTs with up to 64 gigapoints. Although the literature contains some related work, the approach in this paper is unique. There have been a few papers on out-of-core FFTs on uniprocessors <ref> [Bai90, Bre69, CN96] </ref>. There are also some papers on in-core FFTs on multiprocessors [Cal96, JJK92, Swa87, Zhu90]; each of these papers assumes some interconnection network topology. The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson [SW95].
Reference: [Bre69] <author> Norman M. Brenner. </author> <title> Fast Fourier transform of externally stored data. </title> <journal> IEEE Transactions on Audio and Electroacoustics, </journal> <volume> AU-17(2):128-132, </volume> <month> June </month> <year> 1969. </year>
Reference-contexts: The project currently requires FFTs with 10 gigapoints, and it desires FFTs with up to 64 gigapoints. Although the literature contains some related work, the approach in this paper is unique. There have been a few papers on out-of-core FFTs on uniprocessors <ref> [Bai90, Bre69, CN96] </ref>. There are also some papers on in-core FFTs on multiprocessors [Cal96, JJK92, Swa87, Zhu90]; each of these papers assumes some interconnection network topology. The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson [SW95].
Reference: [Cal96] <author> C. Calvin. </author> <title> Implementation of parallel FFT algorithms on distributed memory machines with a minimum overhead of communication. </title> <journal> Parallel Computing, </journal> <volume> 22 </volume> <pages> 1255-1279, </pages> <year> 1996. </year>
Reference-contexts: Although the literature contains some related work, the approach in this paper is unique. There have been a few papers on out-of-core FFTs on uniprocessors [Bai90, Bre69, CN96]. There are also some papers on in-core FFTs on multiprocessors <ref> [Cal96, JJK92, Swa87, Zhu90] </ref>; each of these papers assumes some interconnection network topology. The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson [SW95].
Reference: [CGK + 88] <author> Peter Chen, Garth Gibson, Randy H. Katz, David A. Patterson, and Martin Schulze. </author> <title> Two papers on RAIDs. </title> <type> Technical Report UCB/CSD 88/479, </type> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: least significant bits, the bit fields are * lg (N=BD) = n (b + d) bits containing the number of the stripe (since each stripe has BD records, there are N=BD stripes), 2 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 3 * lg D = d bits containing the disk number; of these, the most significant lg P = p contain the processor number, * lg B = b bits containing the record's offset within its block.
Reference: [CH96] <author> Thomas H. Cormen and Melissa Hirschl. </author> <title> Early experiences in evaluating the Parallel Disk Model with the ViC* implementation. </title> <type> Technical Report PCS-TR96-293, </type> <institution> Dart-mouth College Department of Computer Science, </institution> <month> August </month> <year> 1996. </year> <note> To appear in Parallel Computing. </note>
Reference-contexts: The platform we use is a cluster of IBM RS6000 workstations with a FDDI network. Disk I/O operations are performed by calls to the ViC* API <ref> [CH96] </ref>, which is implemented as a set of wrappers on top of the Galley File System [NK96a, NK96b]. The full paper will also include data for an IBM SP-2, also running Galley. The remainder of this paper is organized as follows. <p> The M record memory is distributed among the P processors so that each processor holds M=P records. The implementation of the PDM we use is the ViC* API <ref> [CH96] </ref>, in which D P and each processor P i communicates only with the D=P disks D iD=P ; D iD=P +1 ; : : : ; D (i+1)D=P 1 . (If D &lt; P in a given physical configuration, the ViC* implementation provides the illusion that D = P by <p> Both of these permutations belong to the class of BMMC (bit-matrix-multiply/complement) permutations. We employ the BMMC algorithm for the PDM given in [CSW94] to perform these permutations optimally under the PDM. In particular, we use the BMMC implementation described in <ref> [CH96] </ref>, which is carefully optimized for communication and computational efficiency. The BMMC subroutine is the only part of the out-of-core FFT algorithm that requires independent I/O; it uses independent writes. All other I/O in the FFT algorithm is striped. <p> The platform is "Fleet," a set of eight IBM RS6000 workstations connected by a FDDI network. Each node runs AIX 4.1. Interprocessor communication is performed via the MPI calls MPI_Sendrecv () and MPI_Sendrecv_replace (). Parallel I/O calls are through the ViC* API <ref> [CH96] </ref>, which in turn makes calls to the Galley File System [NK96a, NK96b]. Galley uses separate I/O processes (IOPs) to manage parallel I/O calls. The ViC* API treats each IOP like a disk. On Fleet, it is fastest to run the IOPs on separate nodes from the computational processes.
Reference: [CLR90] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Then lg N = 3 stages of butterfly operations are performed, and the results (y 0 ; y 1 ; : : : ; y N1 ) emerge from the right. This figure is taken from <ref> [CLR90, p. 796] </ref>. we perform lg N= lg F superlevels. Each superlevel consists of N=F mini-butterflies on F values, followed by a (lg F )-bit right-rotation permutation on the entire array. 5 At the end of each superlevel is a (lg F )-bit right-rotation permutation.
Reference: [CN96] <author> Thomas H. Cormen and David M. Nicol. </author> <title> Performing out-of-core FFTs on parallel disk systems. </title> <type> Technical Report PCS-TR96-294, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> August </month> <year> 1996. </year> <note> To appear in Parallel Computing. </note>
Reference-contexts: 1 Introduction This paper extends earlier work <ref> [CN96] </ref> in performing out-of-core Fast Fourier Transforms (FFTs) on parallel disk systems. Whereas the implementation described in [CN96] performs out-of-core FFTs on a uniprocessor with parallel disks, the present paper examines four related ways to perform out-of-core FFTs on a multiprocessor with a distributed memory and parallel disks. <p> 1 Introduction This paper extends earlier work <ref> [CN96] </ref> in performing out-of-core Fast Fourier Transforms (FFTs) on parallel disk systems. Whereas the implementation described in [CN96] performs out-of-core FFTs on a uniprocessor with parallel disks, the present paper examines four related ways to perform out-of-core FFTs on a multiprocessor with a distributed memory and parallel disks. The study in [CN96] showed that an FFT algorithm explicitly designed for out-of-core problems on the Parallel Disk Model (PDM) <p> Whereas the implementation described in <ref> [CN96] </ref> performs out-of-core FFTs on a uniprocessor with parallel disks, the present paper examines four related ways to perform out-of-core FFTs on a multiprocessor with a distributed memory and parallel disks. The study in [CN96] showed that an FFT algorithm explicitly designed for out-of-core problems on the Parallel Disk Model (PDM) [VS94] can significantly outperform traditional in-core FFT fl Contact author. Send correspondence to Dartmouth College Department of Computer Science, 6211 Sudikoff Laboratory, Hanover, NH 03755-3510 or to thc@cs.dartmouth.edu. <p> That study demonstrated rather convincingly that out-of-core FFT computations should use explicit disk I/O. In the present paper, we adapt the FFT method used in <ref> [CN96] </ref> for multiple processors with a distributed-memory architecture. Conceptually, the method adapts easily. There are some design choices to be made, however, and this paper considers two of them. Section 4 examines these choices in more detail, but briefly they are described by the following parameters: 1. <p> The project currently requires FFTs with 10 gigapoints, and it desires FFTs with up to 64 gigapoints. Although the literature contains some related work, the approach in this paper is unique. There have been a few papers on out-of-core FFTs on uniprocessors <ref> [Bai90, Bre69, CN96] </ref>. There are also some papers on in-core FFTs on multiprocessors [Cal96, JJK92, Swa87, Zhu90]; each of these papers assumes some interconnection network topology. The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson [SW95]. <p> The full paper will also include data for an IBM SP-2, also running Galley. The remainder of this paper is organized as follows. Section 2 defines the Parallel Disk Model, and Section 3 summarizes the out-of-core uniprocessor algorithm for the PDM from <ref> [CN96] </ref>. Section 4 describes the modifications to the uniprocessor algorithm for a multiprocessor, detailing the effective memory size and band size parameters. Section 5 discusses the effects of these modifications on I/O and communication complexity. Section 6 compares the performance of the four methods on the network of RS6000 workstations. <p> an I/O complexity of fi N lg min (B;N=B) , which appears to be the analogue of the fi (N lg N ) bound seen for so many sequential algorithms on the standard RAM model. 3 The uniprocessor out-of-core FFT algorithm This section summarizes the uniprocessor out-of-core FFT algorithm from <ref> [CN96] </ref>. We will modify the uniprocessor algorithm in Section 4 to devise multiprocessor versions. Traditional FFTs The out-of-core algorithm is based on a redrawing of the butterfly graph, so we start by reviewing the traditional approach of computing FFTs in-core by computing the butterfly graph. <p> All other I/O in the FFT algorithm is striped. For a uniprocessor, we always choose the effective memory size to be M . The I/O complexity of this algorithm is then fi BD lg (M=B) parallel I/Os, which is asymptotically optimal. See <ref> [CN96] </ref> for details. Other implementation details General values of N and F . If lg F does not divide lg N, then there are dlg N= lg F e superlevels and we compensate in the last one.
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. Revised version to appear in SIAM Journal on Computing. 15 </note>
Reference-contexts: Computationally, we have added the work of performing the bit-reversal and (lg F )-bit right-rotation permutations. Both of these permutations belong to the class of BMMC (bit-matrix-multiply/complement) permutations. We employ the BMMC algorithm for the PDM given in <ref> [CSW94] </ref> to perform these permutations optimally under the PDM. In particular, we use the BMMC implementation described in [CH96], which is carefully optimized for communication and computational efficiency. The BMMC subroutine is the only part of the out-of-core FFT algorithm that requires independent I/O; it uses independent writes.
Reference: [Gib92] <author> Garth A. Gibson. </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year> <note> Also available as Technical Report UCB/CSD 91/613, </note> <institution> Computer Science Division (EECS), University of California, Berkeley, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: least significant bits, the bit fields are * lg (N=BD) = n (b + d) bits containing the number of the stripe (since each stripe has BD records, there are N=BD stripes), 2 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 3 * lg D = d bits containing the disk number; of these, the most significant lg P = p contain the processor number, * lg B = b bits containing the record's offset within its block.
Reference: [GLS94] <author> William Gropp, Ewing Lusk, and Anthony Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: They use a CM-5 with a Scalable Disk Array [TMC92], which appears to the programmer as one large disk. The implementation in the present paper uses a PDM interface to access multiple disks independently and MPI <ref> [GLS94, SOHL + 96] </ref> for interprocessor communication; there are no assumptions about the interconnection network topology. The platform we use is a cluster of IBM RS6000 workstations with a FDDI network.
Reference: [JJK92] <author> S. Lennart Johnsson, Michel Jacquemin, and Robert L. Krawitz. </author> <title> Communication efficient multi-processor FFT. </title> <journal> Journal of Computational Physics, </journal> <volume> 102 </volume> <pages> 381-397, </pages> <year> 1992. </year>
Reference-contexts: Although the literature contains some related work, the approach in this paper is unique. There have been a few papers on out-of-core FFTs on uniprocessors [Bai90, Bre69, CN96]. There are also some papers on in-core FFTs on multiprocessors <ref> [Cal96, JJK92, Swa87, Zhu90] </ref>; each of these papers assumes some interconnection network topology. The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson [SW95].
Reference: [NK96a] <author> Nils Nieuwejaar and David Kotz. </author> <title> The Galley parallel file system. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <pages> pages 374-381, </pages> <address> Philadelphia, PA, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The platform we use is a cluster of IBM RS6000 workstations with a FDDI network. Disk I/O operations are performed by calls to the ViC* API [CH96], which is implemented as a set of wrappers on top of the Galley File System <ref> [NK96a, NK96b] </ref>. The full paper will also include data for an IBM SP-2, also running Galley. The remainder of this paper is organized as follows. Section 2 defines the Parallel Disk Model, and Section 3 summarizes the out-of-core uniprocessor algorithm for the PDM from [CN96]. <p> Each node runs AIX 4.1. Interprocessor communication is performed via the MPI calls MPI_Sendrecv () and MPI_Sendrecv_replace (). Parallel I/O calls are through the ViC* API [CH96], which in turn makes calls to the Galley File System <ref> [NK96a, NK96b] </ref>. Galley uses separate I/O processes (IOPs) to manage parallel I/O calls. The ViC* API treats each IOP like a disk. On Fleet, it is fastest to run the IOPs on separate nodes from the computational processes. Consequently, we report results for P = 4 and D = 4.
Reference: [NK96b] <author> Nils Nieuwejaar and David Kotz. </author> <title> Performance of the Galley parallel file system. </title> <booktitle> In Proceedings of the Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 83-94, </pages> <address> Philadelphia, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The platform we use is a cluster of IBM RS6000 workstations with a FDDI network. Disk I/O operations are performed by calls to the ViC* API [CH96], which is implemented as a set of wrappers on top of the Galley File System <ref> [NK96a, NK96b] </ref>. The full paper will also include data for an IBM SP-2, also running Galley. The remainder of this paper is organized as follows. Section 2 defines the Parallel Disk Model, and Section 3 summarizes the out-of-core uniprocessor algorithm for the PDM from [CN96]. <p> Each node runs AIX 4.1. Interprocessor communication is performed via the MPI calls MPI_Sendrecv () and MPI_Sendrecv_replace (). Parallel I/O calls are through the ViC* API [CH96], which in turn makes calls to the Galley File System <ref> [NK96a, NK96b] </ref>. Galley uses separate I/O processes (IOPs) to manage parallel I/O calls. The ViC* API treats each IOP like a disk. On Fleet, it is fastest to run the IOPs on separate nodes from the computational processes. Consequently, we report results for P = 4 and D = 4.
Reference: [PGK88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM International Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: least significant bits, the bit fields are * lg (N=BD) = n (b + d) bits containing the number of the stripe (since each stripe has BD records, there are N=BD stripes), 2 A block might consist of several sectors of a physical device or, in the case of RAID <ref> [CGK + 88, Gib92, PGK88] </ref>, sectors from several physical devices. 3 * lg D = d bits containing the disk number; of these, the most significant lg P = p contain the processor number, * lg B = b bits containing the record's offset within its block.
Reference: [Sni81] <author> M. Snir. </author> <title> I/O limitations on multi-chip VLSI systems. </title> <booktitle> In Proceedings of the 19th Allerton Conference on Communication, Control and Computation, </booktitle> <pages> pages 224-233, </pages> <year> 1981. </year>
Reference-contexts: Each butterfly operation has a third input, known as a twiddle factor. The twiddle factor for a butterfly operation in stage s and the jth butterfly within a group (0 j &lt; 2 s1 ) is ! j Redrawing the butterfly graph for out-of-core FFTs devised by Snir <ref> [Sni81] </ref> and is implicitly used in the FFT algorithm for the PDM devised by Vitter and Shriver [VS94]. We describe the out-of-core algorithm in terms of an effective memory size F , which is a power of 2 in the range 1 F M .
Reference: [SOHL + 96] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Don-garra. </author> <title> MPI: The Complete Reference. </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: They use a CM-5 with a Scalable Disk Array [TMC92], which appears to the programmer as one large disk. The implementation in the present paper uses a PDM interface to access multiple disks independently and MPI <ref> [GLS94, SOHL + 96] </ref> for interprocessor communication; there are no assumptions about the interconnection network topology. The platform we use is a cluster of IBM RS6000 workstations with a FDDI network.
Reference: [SW95] <author> Roland Sweet and John Wilson. </author> <title> Development of out-of-core fast Fourier transform software for the Connection Machine. </title> <note> URL http://www-math.cudenver.edu/~jwilson/ final report/final report.html, </note> <month> December </month> <year> 1995. </year>
Reference-contexts: There are also some papers on in-core FFTs on multiprocessors [Cal96, JJK92, Swa87, Zhu90]; each of these papers assumes some interconnection network topology. The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson <ref> [SW95] </ref>. They use a CM-5 with a Scalable Disk Array [TMC92], which appears to the programmer as one large disk.
Reference: [Swa87] <author> Paul N. Swarztrauber. </author> <title> Multiprocessor FFTs. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 197-210, </pages> <year> 1987. </year>
Reference-contexts: Although the literature contains some related work, the approach in this paper is unique. There have been a few papers on out-of-core FFTs on uniprocessors [Bai90, Bre69, CN96]. There are also some papers on in-core FFTs on multiprocessors <ref> [Cal96, JJK92, Swa87, Zhu90] </ref>; each of these papers assumes some interconnection network topology. The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson [SW95].
Reference: [TMC92] <institution> CM-5 scalable disk array. Thinking Machines Corporation glossy, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson [SW95]. They use a CM-5 with a Scalable Disk Array <ref> [TMC92] </ref>, which appears to the programmer as one large disk. The implementation in the present paper uses a PDM interface to access multiple disks independently and MPI [GLS94, SOHL + 96] for interprocessor communication; there are no assumptions about the interconnection network topology.
Reference: [VS94] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and September 1994. 
Reference-contexts: The study in [CN96] showed that an FFT algorithm explicitly designed for out-of-core problems on the Parallel Disk Model (PDM) <ref> [VS94] </ref> can significantly outperform traditional in-core FFT fl Contact author. Send correspondence to Dartmouth College Department of Computer Science, 6211 Sudikoff Laboratory, Hanover, NH 03755-3510 or to thc@cs.dartmouth.edu. <p> The number of stripes is N=BD = 4. Numbers indicate record indices. 2 The Parallel Disk Model This section describes the Parallel Disk Model <ref> [VS94] </ref>. It is the underlying model for both the uniprocessor algorithm in Section 3 and the multiprocessor algorithm in Section 4. <p> for a butterfly operation in stage s and the jth butterfly within a group (0 j &lt; 2 s1 ) is ! j Redrawing the butterfly graph for out-of-core FFTs devised by Snir [Sni81] and is implicitly used in the FFT algorithm for the PDM devised by Vitter and Shriver <ref> [VS94] </ref>. We describe the out-of-core algorithm in terms of an effective memory size F , which is a power of 2 in the range 1 F M . Assume for the moment that lg F divides lg N . As before, we start with a bit-reversal permutation.
Reference: [Zhu90] <author> J. P. Zhu. </author> <title> An efficient FFT algorithm on multiprocessors with distributed memory. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <volume> volume I, </volume> <pages> pages 358-363, </pages> <month> April </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: Although the literature contains some related work, the approach in this paper is unique. There have been a few papers on out-of-core FFTs on uniprocessors [Bai90, Bre69, CN96]. There are also some papers on in-core FFTs on multiprocessors <ref> [Cal96, JJK92, Swa87, Zhu90] </ref>; each of these papers assumes some interconnection network topology. The only previous out-of-core implementation for a multiprocessor of which we are aware is by Sweet and Wilson [SW95].
References-found: 21

