URL: http://ltt-www.lcs.mit.edu/ltt-www/People/jeremy/thesis/MIT-LCS-TR-678.ps
Refering-URL: http://ltt-www.lcs.mit.edu/ltt-www/People/jeremy/thesis/
Root-URL: 
Title: Identifying and Merging Related Bibliographic Records  
Author: by Jeremy A. Hylton Jerome H. Saltzer 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degrees of Master of Engineering in Electrical Engineering and Computer Science and Bachelor of Science in Computer Science and Engineering at the  c Jeremy A. Hylton, 1996. All rights reserved. The author hereby grants to MIT permission to reproduce and distribute publicly paper and electronic copies of this thesis document in whole or in part, and to grant others the right to do so. Author  Certified by  Professor of Computer Science and Engineering, Emeritus Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Theses  
Date: June 1996  February 13, 1996  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 1 </institution>
Reference-contexts: I also used the results of two other algorithms to gauge the relative success of the author-title clustering algorithm presented here. The bibmerge program <ref> [1] </ref> creates title-date clusters; if two records have the same title and date, they are placed in the same cluster. Bibmerge has only limited utility as a benchmark, because it uses 56 a very simple detection scheme that is not tolerant of formatting and typographical errors.
Reference: [1] <author> Alf-Christian Achilles. bibmerge. </author> <note> [Program available via WWW], 1994. URL &lt;http://liinwww.ira.uka.de/bibliography/tools/bibmerge&gt; (version 28 Feb. </note> <year> 1995). </year>
Reference-contexts: I also used the results of two other algorithms to gauge the relative success of the author-title clustering algorithm presented here. The bibmerge program <ref> [1] </ref> creates title-date clusters; if two records have the same title and date, they are placed in the same cluster. Bibmerge has only limited utility as a benchmark, because it uses 56 a very simple detection scheme that is not tolerant of formatting and typographical errors.
Reference: [2] <institution> Alf-Christian Achilles. A collection of computer science bibliographies. </institution> <note> [WWW document], 1995. URL &lt;http://liinwww.ira.uka.de/bibliography/index.html&gt; (visited 10 Feb. </note> <year> 1995). </year>
Reference-contexts: The primary source is Alf-Christian Achilles' collection of 450,000 Bibtex records, titled "A Collection of Computer Science Bibliographies" <ref> [2] </ref>. This work organizes several hundred individual collections of varying size and quality. The second source is the CS-TR records produced by the five participants in the CS-TR project; the collections is composed of approximately 6,000 technical report records from Berkeley, Carnegie Mellon, Cornell, M.I.T., and Stanford.
Reference: [3] <author> Eytan Adar and Jeremy Hylton. </author> <title> On-the-fly hyperlink creation for pages images. </title> <editor> In Shipman et al. </editor> <volume> [34], </volume> <pages> pages 173-176. </pages>
Reference-contexts: Some leverage on the problem can be gained by looking for bibliographic records that are "similar" to the citation, using the structured information contained in bibliographic records to try and understand the unstructerd citation. Eytan Adar and I <ref> [3] </ref> proposed one scheme for linking the two. 7.1.5 Enabling librarianship and human input The automatic processes for identifying and merging bibligraphic records work quite well in general, but human intervention would be helpful for correcting the errors that do occur.
Reference: [4] <author> Deborah Lines Andersen, Thomas J. Galvin, and Mark D. Giguere, </author> <title> editors. Navigating the Networks: </title> <booktitle> Proceedings of the ASIS Mid-Year Meeting, Medford, NJ, 1994. American Society for Information Science (ASIS), Learned Information. </booktitle>
Reference: [5] <author> Eva Bertha. </author> <title> Inter- and intrabibliographical relationships: A concept for a hypercatalog. In Helal [16], pages 211-223. 1 Several documents cited in this thesis are available only in electronic form, via the World-Wide Web. It is difficult, however, to provide long-lasting citations for these documents. I have chosen to include the current URLs for the Web page and either the date of the most recent update to the page or the date of the last time I checked that the page was available (if the page was not dated). </title> <type> 93 </type>
Reference-contexts: The OCLC Online Union Catalog merged several million bibliographic records and developed one of the first duplicate detection systems [18]. More recently, the library community has begun to re-evaluate its cataloging standards. Several papers <ref> [5, 15, 40, 48] </ref> suggest that catalogers should focus more on describing "works"|particular, identifiable intellectual works|rather than "documents"|the particular physical versions of a work. <p> Two majors themes run through several recent papers <ref> [5, 15, 37, 48] </ref>: * Library catalogs should make is easier for users to understand relationships between different entries.
Reference: [6] <author> C. Mic Bowman, Peter B. Danzig, Darren R. Hardy, Udi Manber, and Michael F. Schwartz. </author> <title> The harvest information discovery and access system. </title> <note> In Committee [10]. [WWW document] URL &lt;ftp://ftp.cs.colorado.edu/pub/cs/techreports/schwartz/Harvest.Conf.ps.Z&gt; (visited 10 Feb. </note> <year> 1995). </year>
Reference-contexts: The Unified Computer Science Technical Report Index (UCSTRI) [43] automatically collects information about technical reports distributed on the Internet and provides an index of that information with links to the original report. The Harvest system <ref> [6] </ref> is a more general information discovery system that combines tools for building local, content-specific indexes and sharing them to build indexes that span many sites; these tools include support for replicate and caching. The Harvest implementors developed a sample index of computer science technical reports.
Reference: [7] <author> C. Mic Bowman, Peter B. Danzig, Udi Manber, and Michael F. Schwartz. </author> <title> Scalable internet resources discovery: Research problems and approaches. </title> <journal> Communications of the ACM, </journal> <volume> 37(8) </volume> <pages> 98-107, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: The Harvest implementors developed a sample index of computer science technical reports. Harvest was designed to illustrate the principles for scalable access to Internet resources described in Bowman, et al. <ref> [7] </ref>. A third system is the Networked Computer Science Technical Report Library (NCSTRL) [12], which uses the Dienst protocol [21]. Dienst provides a repository for storing documents, a distributed indexing scheme, and a user interface for the documents in a repository.
Reference: [8] <author> Michael K. Buckland, Mark H. Butler, Barbara A. Norhard, and Christian Plaunt. </author> <title> Union records and dossiers: Extended bibliographic information objects. </title> <editor> In Andersen et al. </editor> <volume> [4], </volume> <pages> pages 42-57. </pages>
Reference-contexts: The heterogeneity provides considerable leverage on the problems of extracting the best possible information from records and providing links between closely related records (an observation made by Buckland, et al. <ref> [8] </ref>). By identifying related records, a union record can be created that combines the best information from each of the source records. One of the primary contributions of this thesis is an algorithm for identifying related bibliographic records. <p> A different union record is created for each different type of document included in the cluster; thus, a particular cluster may have union records for a journal article, a technical report, and a conference paper. An information dossier <ref> [8] </ref> is a collection of information objects, e.g. bibliographic records, related to some way to one another. Specifically, I use the term to describe the source records that form an author-title cluster and the union records generated for the cluster.
Reference: [9] <editor> Michael J. Carey and Donovan A. Schneider, editors. </editor> <booktitle> Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1995. </year> <note> Also published as SIGMOD Record 24(2), </note> <month> June </month> <year> 1995. </year>
Reference: [10] <editor> International World Wide Web Conference Committee, editor. </editor> <booktitle> Proceedings of the 2nd International Conference on the World-Wide Web, </booktitle> <address> Chicago, </address> <month> December </month> <year> 1994. </year> <title> [WWW document, labeled "These documents are no longer being supported."] </title> <address> URL &lt;http://www.ncsa.uiuc.edu/SDG/IT94/IT94Info.html&gt;. </address>
Reference: [11] <author> Walt Crawford. </author> <title> MARC for library use. </title> <editor> G. K. </editor> <publisher> Hall, </publisher> <address> Boston, </address> <note> second edition, </note> <year> 1989. </year>
Reference-contexts: While it is not used by the system presented here, the MARC record makes an interesting point of comparison. The MARC record is a highly structured format; its use emphasizes precise labels for fields and detailed descriptions of the items being cataloged. Crawford <ref> [11] </ref> provides an overview of MARC and its use in libraries; he observes that all MARC records share five characteristics: * Each record has a title or identifying name. * Each object is produced, published, or released at a specific time, by a specific person or group. * Each object is
Reference: [12] <author> James R. Davis. </author> <note> Creating a networked computer science technical report library. D-Lib Magazine [Online journal], September 1995. URL &lt;http://www.dlib.org/dlib/september95/09davis.html&gt;. 94 </note>
Reference-contexts: The Harvest implementors developed a sample index of computer science technical reports. Harvest was designed to illustrate the principles for scalable access to Internet resources described in Bowman, et al. [7]. A third system is the Networked Computer Science Technical Report Library (NCSTRL) <ref> [12] </ref>, which uses the Dienst protocol [21]. Dienst provides a repository for storing documents, a distributed indexing scheme, and a user interface for the documents in a repository. All three systems rely, in vary degrees, on publishers for making documents available and providing bibliographic information. The publisher-centric model is limiting.
Reference: [13] <author> Ahmed K. Elmagarmid and Calton Pu. </author> <title> Introduction to the special issue on heterogeneous databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(3) </volume> <pages> 175-178, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: how people actually use libraries in their discussion of future digital libraries. 1.3.3 Database systems Heterogeneous databases differ from more conventional database systems because they included distributed components that do not all share the same database model; 20 the component databases may have different data models, query languages, or sche--mas <ref> [13] </ref>. One of the problems that arises in multidatabase systems is the integration of the underlying schemas to provide users with a standard interface.
Reference: [14] <author> M. C. Harrison. </author> <title> Implementation of the substring test by hashing. </title> <journal> Communications of the ACM, </journal> <volume> 14(12) </volume> <pages> 777-779, </pages> <month> December </month> <year> 1971. </year>
Reference-contexts: The second round of the OCLC scheme hashes the entire title into a 109-bit key. The key is built by converting the trigrams that do not span words to integers and applying a hash function to the sequence of integers. The IUCS scheme builds a Harrison key <ref> [14] </ref> from trigrams. The Hamming distance between two keys can be compared, allowing tolerance of typographical errors and other small variations between two strings.
Reference: [15] <author> Michael Heaney. </author> <title> Object-oriented cataloging. </title> <journal> Information Technology and Libraries, </journal> <volume> 14(3) </volume> <pages> 135-153, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: The OCLC Online Union Catalog merged several million bibliographic records and developed one of the first duplicate detection systems [18]. More recently, the library community has begun to re-evaluate its cataloging standards. Several papers <ref> [5, 15, 40, 48] </ref> suggest that catalogers should focus more on describing "works"|particular, identifiable intellectual works|rather than "documents"|the particular physical versions of a work. <p> Two majors themes run through several recent papers <ref> [5, 15, 37, 48] </ref>: * Library catalogs should make is easier for users to understand relationships between different entries. <p> OCLC cataloging rules require a new bibliographic record be created for each copy of a work that has different date of impression and different text. Heaney <ref> [15] </ref> cites a message from Bob Strauss to the Autocat mailing list that describes the problem; Strauss observes that between the original publication of Ed Krol's Whole Internet Catalog in 1992 and his search on Dec. 2, 1993, nine different versions have been cataloged in the OCLC union catalog (see Table <p> Heaney makes the same case in his argument for an object-oriented cataloging standard <ref> [15] </ref>. 7.1.3 Identifying other bibliographic relationships The clustering algorithm identifies author-title clusters, in part because identifying equivalence and derivative bibliographic relationships has the most advantage for users and in part because they can be reliably identified in the presence of mixed-quality records.
Reference: [16] <author> Ahmed H. Helal, </author> <title> editor. Opportunity 2000: understanding and serving users in an electronic library. </title> <address> Essen University Library, </address> <year> 1993. </year>
Reference: [17] <author> Mauricio A. Hernandez and Salvatore J. Stolfo. </author> <title> The merge/purge problem for large databases. </title> <booktitle> In Carey and Schneider [9], </booktitle> <pages> pages 127-138. </pages> <note> Also published as SIGMOD Record 24(2), </note> <month> June </month> <year> 1995. </year>
Reference-contexts: Papakonstantinou, et al. [30] present a more thorough discussion of the differences between the integration of databases and the integration of information systems like bibliographic record collections. The merge/purge problem described by Hernandez and Stolfo <ref> [17] </ref> implements a duplicate detection system for mailing lists that copes with variations and errors in the underlying data by making multiple passes over the data, each time using a different key to compare the records. <p> Subsequent work reflects the general approach mapped out in these two seminal studies. The database merge/purge problem, described by Hernandez and Stolfo <ref> [17] </ref>, is very similar to the duplication detection problem. In the merge/purge problem, 50 several different databases with similar records must be merged and the duplicates purged; an example is joining several mailing lists, which may contain inconsistent or incorrect addresses. <p> It is not affected by word errors or by cataloging variations that affect which words are included in the title. 3.3.2 The database merge/purge problem The approach to the merge/purge problem used by Hernandez and Stolfo <ref> [17] </ref> is similar to the one used for duplicate detection. Their algorithm use keys to partition their data into sets small enough that they are willing to apply computationally intensive comparisons.
Reference: [18] <author> Thomas B. Hickey and David J. Rypka. </author> <title> Automatic detection of duplicate monographic records. </title> <journal> Journal of Library Automation, </journal> <volume> 12(2) </volume> <pages> 125-142, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: The OCLC Online Union Catalog merged several million bibliographic records and developed one of the first duplicate detection systems <ref> [18] </ref>. More recently, the library community has begun to re-evaluate its cataloging standards. Several papers [5, 15, 40, 48] suggest that catalogers should focus more on describing "works"|particular, identifiable intellectual works|rather than "documents"|the particular physical versions of a work. <p> Two early duplicate detection projects are surveyed below|the IUCS scheme [47] and the original OCLC On-Line Union Catalog <ref> [18] </ref>; O'Neill, et. al [28] presents some characteristics of duplicate records in the OCLC catalog. Toney [42] describes a more recent effort, and provides a good overview of the design space for library duplicate detection systems. <p> The USBC is similar to the OCLC and IUCS first rounds keys, but Toney [42] notes that it is more dependent on clean data. Quantitative results of the QUALCAT system were not provided. In the OCLC study, Hickey and Rypka <ref> [18] </ref> observe three major causes of failure to match duplicates. A difference in the first 34 characters of the title string that the 53 initial date-title keys is drawn from causes 12 percent of the missed matches. The two other causes are differences in place of publication and pagination.
Reference: [19] <author> Robert E. Kahn. </author> <title> An introduction to the cs-tr project. </title> <note> [WWW document], December 1995. URL &lt;http://www.cnri.reston.va.us/home/cstr.html&gt; (version 11 Dec. </note> <year> 1995). </year>
Reference-contexts: Because other bibliographic formats, like Refer or Tib, are less common than Bibtex records, the current implementation supports only one other bibliographic format, the CS-TR format developed as part of the Computer Science Technical Report Project (CS-TR) <ref> [19] </ref> and defined by RFC 1807 [23]. The two formats differ in both syntax and semantics, so using both formats interchangeably requires a common format that both can be converted into.
Reference: [20] <author> Karen Kukich. </author> <title> Techniques for automatically correcting words in text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Misspelling and typographical errors are common to most text databases and the literature about them is substantial. O'Neill and Vizine-Goetz's overview of quality control in text databases [29] provides a thorough, though somewhat dated discussion of the sources of errors and some techniques for correcting them; Kukich <ref> [20] </ref> surveys techniques for automatically detecting and correcting spelling errors. While entry errors are an unintentional source of variability, differing cataloging standards are often intentional. Many bibliographies apply a consistent set of cataloging rules, but the standards are quite different from one bibliography to another.
Reference: [21] <author> Carl Lagoze and James R. Davis. Dienst: </author> <title> An architecture for distributed digital libraries. </title> <journal> Communications of the ACM, </journal> <volume> 38(4):47, </volume> <month> April </month> <year> 1995. </year> <month> 95 </month>
Reference-contexts: The Harvest implementors developed a sample index of computer science technical reports. Harvest was designed to illustrate the principles for scalable access to Internet resources described in Bowman, et al. [7]. A third system is the Networked Computer Science Technical Report Library (NCSTRL) [12], which uses the Dienst protocol <ref> [21] </ref>. Dienst provides a repository for storing documents, a distributed indexing scheme, and a user interface for the documents in a repository. All three systems rely, in vary degrees, on publishers for making documents available and providing bibliographic information. The publisher-centric model is limiting.
Reference: [22] <author> Leslie Lamport. </author> <title> Latex: a document preparation system. </title> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1994. </year>
Reference-contexts: The format not only dictates what information can be recorded, but also tends to affect the practice of recording. Many freely available bibliographic records use the Bibtex format <ref> [22] </ref>, which is used to produce citations lists in the LaTeX document preparation system. <p> In a list of authors, some bibliographies include only the first author (only sometimes indicating that there are other authors) and others include all the names. Unusual names frequently cause problems. In a sample of 1,000 records, 14 uses of "Jr." were found, but none follow the specified format <ref> [22] </ref>. The correct placement of "Jr." within the author string is: "Steele, Jr., Guy L." Abbreviations are another source of problems caused by differences in cataloging.
Reference: [23] <author> Rebecca Lasher and Danny Cohen. </author> <title> A format for bibliographic records, </title> <month> June </month> <year> 1995. </year> <institution> Internet Engineering Task Force, </institution> <note> RFC 1807. </note>
Reference-contexts: Because other bibliographic formats, like Refer or Tib, are less common than Bibtex records, the current implementation supports only one other bibliographic format, the CS-TR format developed as part of the Computer Science Technical Report Project (CS-TR) [19] and defined by RFC 1807 <ref> [23] </ref>. The two formats differ in both syntax and semantics, so using both formats interchangeably requires a common format that both can be converted into. The common format involves some information loss, when one format captures more information about a particular field than the other format is capable of expressing.
Reference: [24] <author> David M. Levy and Catherine C. Marshall. </author> <title> Going digital: A look at assumptions underlying digital libraries. </title> <journal> Communications of the ACM, </journal> <volume> 38(4) </volume> <pages> 77-84, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: For example, Shakespeare's play Hamlet is a clearly identifiable work; it has been published in many editions, each a "document." This thesis makes use of this distinction when it labels as duplicates records for different documents that instantiate a particular work. Levy and Marshall <ref> [24] </ref> raise similar questions about how people actually use libraries in their discussion of future digital libraries. 1.3.3 Database systems Heterogeneous databases differ from more conventional database systems because they included distributed components that do not all share the same database model; 20 the component databases may have different data models,
Reference: [25] <author> Clifford Lynch and Hector Garcia-Molina, </author> <title> editors. Interoperability, Scaling and the Digital Libraries Research Agenda. </title> <institution> HPCC/IITA Working Group, </institution> <month> August </month> <year> 1995. </year> <note> A Report on the May 18-19, 1995 IITA Digital Libraries Workship. </note>
Reference-contexts: The digital library is, of course, not a single system, but a loose federation of many systems and services; nonetheless, in many cases it should appear to operate as a single system. Providing interoperability among these systems remains a major research topic <ref> [25] </ref>. One kind of interoperability that is sometimes overlooked is the interoperation between physical and digital objects in the library. Most documents exist today only on paper, and digital libraries must provide access to both paper and digital documents to be able to satisfy most users' needs.
Reference: [26] <author> Clifford A. Lynch, Avra Michelson, Craig Summerhill, and Cecilia Preston. </author> <title> The nature of the nidr challenge. </title> <type> Technical report, </type> <note> Coalition for Networked Information, 1995. URL &lt;http://www.cni.org/projects/nidr/www/toc.html&gt; (visited 12 Feb. </note> <year> 1995). </year>
Reference-contexts: This section gives an overview of some related areas of research. 1.3.1 Networked information discovery and retrieval Networked information discovery and retrieval (NIDR) is a broad category encompassing nearly any kind of information access using a large scale computer network <ref> [26] </ref>.
Reference: [27] <author> Keith D. MacLaury. </author> <title> Automatic merging of monographic data bases-use of fixed-length keys derived from title strings. </title> <journal> Journal of Library Automation, </journal> <volume> 12(2) </volume> <pages> 143-155, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: The Hamming distance between two keys can be compared, allowing tolerance of typographical errors and other small variations between two strings. The IUCS scheme uses a short, fixed-length title-date key for its first round, taking eight characters from the beginning and end of titles plus some date information. (MacLaury <ref> [27] </ref> described the choice of title characters.) Records that have the same title-date key are compared using three more keys: the first five characters of the author field, a 72-bit Harrison key of the title field, and the highest number taken from the various page fields. 52 The OCLC scheme is
Reference: [28] <author> Edward T. O'Neill, Sally A. Rogers, and W. Michael Oskins. </author> <title> Characteristics of duplicate records in oclc's online union catalog. Library Resources and Technical Services, </title> <booktitle> 37(1) </booktitle> <pages> 59-71, </pages> <year> 1993. </year>
Reference-contexts: Two early duplicate detection projects are surveyed below|the IUCS scheme [47] and the original OCLC On-Line Union Catalog [18]; O'Neill, et. al <ref> [28] </ref> presents some characteristics of duplicate records in the OCLC catalog. Toney [42] describes a more recent effort, and provides a good overview of the design space for library duplicate detection systems.
Reference: [29] <author> Edward T. O'Neill and Diane Vizine-Goetz. </author> <title> Quality control in online databases. </title> <booktitle> In Williams [46], </booktitle> <pages> pages 125-156. 96 </pages>
Reference-contexts: Misspelling and typographical errors are common to most text databases and the literature about them is substantial. O'Neill and Vizine-Goetz's overview of quality control in text databases <ref> [29] </ref> provides a thorough, though somewhat dated discussion of the sources of errors and some techniques for correcting them; Kukich [20] surveys techniques for automatically detecting and correcting spelling errors. While entry errors are an unintentional source of variability, differing cataloging standards are often intentional.
Reference: [30] <author> Yannis Papakonstantinou, Hector Garcia-Molina, and Jennifer Widom. </author> <title> Object exchange across heterogeneous information sources. </title> <booktitle> In Yu and Chen [50], </booktitle> <pages> pages 251-260. </pages>
Reference-contexts: Duplicate detection is closely related to integration of heterogeneous databases, but is complicated by the fact that bibliographic formats impose little structure on the data they contain; the wide variations in quality and accuracy that typify collections of Bibtex records further complicate the problem. Papakonstantinou, et al. <ref> [30] </ref> present a more thorough discussion of the differences between the integration of databases and the integration of information systems like bibliographic record collections.
Reference: [31] <author> M. J. Ridley. </author> <title> An expert system for quality control and duplicate detection in bibliographic databases. </title> <booktitle> Program, </booktitle> <volume> 26(1) </volume> <pages> 1-18, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: The QUALCAT expert system <ref> [31] </ref> is an interesting alternative to the key-based comparisons. A team of catalogers developed a set of rules that describe whether a certain combination of fields values makes it more or less likely that two records are duplicates.
Reference: [32] <author> Gerard Salton, James Allan, Chris Buckley, and Amit Singhal. </author> <title> Automatic analysis, theme generation, and summarization of machine-readable text. </title> <journal> Science, </journal> <volume> 264 </volume> <pages> 1421-1426, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Including the full-text of a document (including the citations) enables many other applications as well. Users can perform queries across the entire text of a document, which creates more opportunities for discovery relavant documents. Abstracts and summaries can be automatically generated for the documents <ref> [32] </ref>, which can help the user quickly establish the relevance of a document to the current search. Citation indexes and graphs can be created that show how often and how widely a particular paper or conference proceedings is cited.
Reference: [33] <author> Gerard Salton and Michael J. McGill. </author> <title> Introduction to Modern Information Retrieval, </title> <booktitle> chapter 6.2 Vector Similarity Functions, </booktitle> <pages> pages 201-204. </pages> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: There were some problems, however, and it seems likely that it could be improved. Many similarity functions for string and document vectors are discussed in the information retrieval literature; Salton <ref> [33] </ref> offers a brief overview. 3.2.3 Performance of algorithm The first round, which identifies a pool of potential matches, greatly reduces the num ber of comparisons that must be made to create author-title clusters.
Reference: [34] <author> Frank M. Shipman, III, Richard Furuta, and David M. Levy, </author> <title> editors. </title> <booktitle> Proceedings of Digital Libraries '95, </booktitle> <institution> Department of Computer Science, Texas A&M Universty, College Station, </institution> <address> TX 77843, </address> <month> June </month> <year> 1995. </year> <note> Hypermedia Research Laboratory. </note>
Reference: [35] <author> Narayanan Shivakumar and Hector Garcia-Molina. </author> <title> Scam: A copy detection mechanism for digital documents. </title> <editor> In Shipman et al. </editor> <volume> [34]. </volume>
Reference-contexts: The recent information retrieval literature contains several reports from Stanford on duplicate detection in a Usenet awareness service [49] and several schemes for copy detection in a digital library <ref> [35, 36] </ref>.
Reference: [36] <author> Narayanan Shivakumar and Hector Garcia-Molina. </author> <title> The scam approach to copy detection in digital libraries. </title> <journal> D-Lib Magazine [Online journal], </journal> <month> November </month> <year> 1995. </year> <note> URL &lt;http://www.dlib.org/dlib/november95/scam/11shivakumar.html&gt;. </note>
Reference-contexts: The recent information retrieval literature contains several reports from Stanford on duplicate detection in a Usenet awareness service [49] and several schemes for copy detection in a digital library <ref> [35, 36] </ref>.
Reference: [37] <author> Richard P. Smiraglia and Gregory H. Leazer. </author> <title> Toward the bibliographic control of works: Derivative bibliographic relationships in the online union catalog. </title> <journal> In OCLC Research Bulletin, </journal> <pages> pages 56-59. </pages> <year> 1994. </year> <month> 97 </month>
Reference-contexts: Two majors themes run through several recent papers <ref> [5, 15, 37, 48] </ref>: * Library catalogs should make is easier for users to understand relationships between different entries. <p> The emphasis on the physical object over the work is inadequate in several ways. The user's interest in the work rather than the document has already been noted. Smiraglia and Leazer <ref> [37] </ref> note that anecdotal evidence supports this claim and that catalog usage studies show that the bibliographic fields used to differentiate between variant editions are seldom used.
Reference: [38] <author> Guy L. Steele, Jr. </author> <title> Common LISP, </title> <booktitle> chapter 6.3 Equality Predicates, </booktitle> <pages> pages 103-110. </pages> <publisher> Digital Press, </publisher> <year> 1990. </year>
Reference-contexts: Using the term equivalent requires some care; it sounds simple enough, but equivalence depends entirely on the context in which some equivalence test is applied. (Consider, for example, the four different equality tests in Common Lisp <ref> [38] </ref>.) The records in an author-title cluster are equivalent for the purpose of identifying a work, 29 but are probably not equivalent for the purpose of locating the work in a library or retrieving it across the network.
Reference: [39] <author> Elaine Svenonius, </author> <title> editor. The Conceptual Foundationgs of Descriptive Cataloging. </title> <publisher> Academic Press, </publisher> <address> San Diego, Calif., </address> <year> 1989. </year>
Reference: [40] <author> Barbara B. Tillett. </author> <title> Bibliographic structures: The evolution of catalog entries, references, </title> <booktitle> and tracings. In Svenonius [39], </booktitle> <pages> pages 149-166. </pages>
Reference-contexts: The OCLC Online Union Catalog merged several million bibliographic records and developed one of the first duplicate detection systems [18]. More recently, the library community has begun to re-evaluate its cataloging standards. Several papers <ref> [5, 15, 40, 48] </ref> suggest that catalogers should focus more on describing "works"|particular, identifiable intellectual works|rather than "documents"|the particular physical versions of a work.
Reference: [41] <author> Barbara B. Tillett. </author> <title> A taxonomy of bibliographic relationships. Library Resources & Technical Services, </title> <booktitle> 35(2) </booktitle> <pages> 150-158, </pages> <year> 1991. </year>
Reference-contexts: as #1?) 5 51 July 1993, minor corr. 6 136 "May 1993" 7 116 [Corr ed] (1993) 8 19 [Corr ed] 9 132 "Feb. 1993; minor corr" total 2638 Table 2.1: Versions of the Whole Internet Catalog cataloged in the OCLC union catalog 2.1.2 Taxonomy of relationships between records Tillett <ref> [41] </ref> and others have developed taxonomies of bibliographic relationships. Till-ett's taxonomy provides a useful vocabulary for discussing the different kinds of documents that describe the same work, as well as relationships between different works.
Reference: [42] <author> Stephen R. Toney. </author> <title> Cleanup and deduplication of an international bibliographic database. </title> <journal> Information Technology and Libraries, </journal> <volume> 11(1) </volume> <pages> 19-28, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Two early duplicate detection projects are surveyed below|the IUCS scheme [47] and the original OCLC On-Line Union Catalog [18]; O'Neill, et. al [28] presents some characteristics of duplicate records in the OCLC catalog. Toney <ref> [42] </ref> describes a more recent effort, and provides a good overview of the design space for library duplicate detection systems. The OCLC and IUCS projects were completed in the late 70s, when the library community had just begun sharing bibliographic records among institutions and building large online union catalogs. <p> The expert system was used to compare records during the second round of QUAL-CAT testing. The first round was a fairly typical key-based filter using Universal Standard Bibliographic Code (USBC). The USBC is similar to the OCLC and IUCS first rounds keys, but Toney <ref> [42] </ref> notes that it is more dependent on clean data. Quantitative results of the QUALCAT system were not provided. In the OCLC study, Hickey and Rypka [18] observe three major causes of failure to match duplicates.
Reference: [43] <author> Marc Van Heyningen. </author> <title> The unified computer science technical report index: Lessons in indexing diverse resources. </title> <note> In Committee [10]. [WWW document] URL &lt;http://www.cs.indiana.edu/ucstri/paper/paper.html&gt; (visited 10 Feb, </note> <year> 1995). </year>
Reference-contexts: These systems work primarily with technical reports, because they are often freely available and organized for Internet access by the publishing organization. The Unified Computer Science Technical Report Index (UCSTRI) <ref> [43] </ref> automatically collects information about technical reports distributed on the Internet and provides an index of that information with links to the original report.
Reference: [44] <author> Stuart Weibel. </author> <title> Automated cataloging: Implications for libraries and patrons. </title> <editor> In F. W. Lancaster and Linda C. Smith, editors, </editor> <booktitle> Artificial Intelligence and Expert Systems: Will They Change the Library?, </booktitle> <pages> pages 67-80. </pages> <institution> University of Illinois, </institution> <year> 1992. </year>
Reference-contexts: The identification of basic bibliographic information| the author and title of the work, the pages it appears on, etc.|is a largely clerical process. (Fully automated cataloging is an active area for research, but little progress has been made <ref> [44] </ref>.) Instead, cataloging should focus information that is more difficult to obtain|whether two authors with similar names are in fact the same person or whether two papers with similar but different titles actually represent the same work.
Reference: [45] <author> Gio Weiderhold. </author> <title> Mediators in the architecture of future information systems. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 38-49, </pages> <month> mar </month> <year> 1992. </year>
Reference-contexts: The merge/purge problem described by Hernandez and Stolfo [17] implements a duplicate detection system for mailing lists that copes with variations and errors in the underlying data by making multiple passes over the data, each time using a different key to compare the records. Mediators <ref> [45] </ref> are a different approach to the problem of integrating information from multiple sources. Mediators are part of a model of the networked information environment that includes database access as its lowest level and users and information gathering applications at the top level.
Reference: [46] <author> Martha E. Williams, </author> <title> editor. </title> <journal> Annual Review of Information Science and Technology, </journal> <volume> volume 23. </volume> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1988. </year> <month> 98 </month>
Reference: [47] <author> Martha E. Williams and Keith D. MacLaury. </author> <title> Automatic merging of monographic data bases-identification of duplicate records in multiple files: The IUCS scheme. </title> <journal> Journal of Library Automation, </journal> <volume> 12(2) </volume> <pages> 156-168, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: Two early duplicate detection projects are surveyed below|the IUCS scheme <ref> [47] </ref> and the original OCLC On-Line Union Catalog [18]; O'Neill, et. al [28] presents some characteristics of duplicate records in the OCLC catalog. Toney [42] describes a more recent effort, and provides a good overview of the design space for library duplicate detection systems.
Reference: [48] <author> Patrick Wilson. </author> <title> The second objective. </title> <booktitle> In Svenonius [39], </booktitle> <pages> pages 5-16. </pages>
Reference-contexts: The OCLC Online Union Catalog merged several million bibliographic records and developed one of the first duplicate detection systems [18]. More recently, the library community has begun to re-evaluate its cataloging standards. Several papers <ref> [5, 15, 40, 48] </ref> suggest that catalogers should focus more on describing "works"|particular, identifiable intellectual works|rather than "documents"|the particular physical versions of a work. <p> Two majors themes run through several recent papers <ref> [5, 15, 37, 48] </ref>: * Library catalogs should make is easier for users to understand relationships between different entries. <p> The most recent theoretical framework for descriptive cataloging was formulated in the 1950s by Seymour Lubetzky. According to Wilson <ref> [48] </ref>, Lubetzky suggested the library catalog serves two functions: the finding function and the collocation function. The finding function. If a patron knows the author, the title, or the subject of the book, the catalog should enable him or her to determine whether the library has the book.
Reference: [49] <author> Tak W. Yan and Hector Garcia-Molina. </author> <title> Duplicate detection in information dissemination. </title> <booktitle> In Proceedings of 21st International Very Large Database Conference (VLDB), </booktitle> <month> September </month> <year> 1995. </year> <institution> Zurich, Switzerland. </institution>
Reference-contexts: The stories are duplicates because their content overlaps substantially and not because of some external feature like their title or date of publication. Yan and Garcia-Molina <ref> [49] </ref> provide a more detailed discussion of duplicate detection in this context. 22 Chapter 2 Cataloging and the Computer Science Collection This chapter introduces the conceptual framework for creating, using, and relating bibliographic records. <p> The recent information retrieval literature contains several reports from Stanford on duplicate detection in a Usenet awareness service <ref> [49] </ref> and several schemes for copy detection in a digital library [35, 36].
Reference: [50] <author> P. S. Yu and A. L. P. Chen, </author> <title> editors. </title> <booktitle> Proceedings of the 11th International Conference on Data Engineering, </booktitle> <address> Taipei, Taiwan, March 1995. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 99 </pages>
References-found: 51

