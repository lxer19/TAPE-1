URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/uwem/WWW/work/papers/95.icassp.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/uwem/WWW/work/papers.html
Root-URL: 
Phone: 2  
Title: TOWARD MOVEMENT-INVARIANT AUTOMATIC LIP-READING AND SPEECH RECOGNITION  
Author: Paul Duchnowski Martin Hunke ; Dietrich B usching Uwe Meier Alex Waibel ; 
Address: Germany  Pittsburgh PA, USA  
Affiliation: Interactive Systems Laboratories 1 University of Karlsruhe, Karlsruhe,  Carnegie Mellon University,  
Abstract: We present the development of a modular system for flexible human-computer interaction via speech. The speech recognition component integrates acoustic and visual information (automatic lip-reading) improving overall recognition, especially in noisy environments. The image of the lips, constituting the visual input, is automatically extracted from the camera picture of the speaker's face by the lip locator module. Finally, the speaker's face is automatically acquired and followed by the face tracker sub-system. Integration of the three functions results in the first bi-modal speech recognizer allowing the speaker reasonable freedom of movement within a possibly noisy room while continuing to communicate with the computer via voice. Compared to audio-alone recognition, the combined system achieves a 20 to 50 percent error rate reduction for various signal/noise conditions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Bregler, H. Hild, S. Manke, and A. Waibel. </author> <title> Improving Con nected Letter Recognition by Lipreading. </title> <booktitle> Proc. </booktitle> <address> ICASSP'93. </address>
Reference-contexts: Lip-reading plays an important role in communication by the hearing-impaired and by individuals listening in difficult acoustic environments [10]. Several studies have demonstrated the utility of augmenting automatic speech recognition (ASR) with visual information (eg. [6, 7, 9]). Our own work in this area has been previously reported in <ref> [1, 3] </ref>. However, a major limitation of virtually all the systems was the method by which visual data was acquired. This included such invasive techniques as head-mounted cameras, reflective markers placed on the speaker's lips, and manual extraction of relevant face image sections, effectively precluding practical applications. <p> The weights in the parallel networks are trained by backpropagation. There are 15 hidden units in both sub-nets. The combination weights (so called entropy weights, see <ref> [1] </ref>) are computed dynamically during recognition to reflect the estimated reliability of each modality. We have also investigated alternative methods of combining the audio and visual information at the input and hidden layer levels of the network.
Reference: [2] <author> D. B usching. Automatische Lokalisierung der Lippenregion in Videobildern von Gesichtern. </author> <type> Masters Thesis, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, </institution> <year> 1994. </year>
Reference-contexts: Also, using other features such as the outline of the face and the relative location of the eyes was considered advantageous in pinpointing the lips. Accordingly, a system consisting of two neural networks was designed <ref> [2] </ref>. The first network gives a coarse estimate of the position of the mouth. The second locates the two corners of the mouth within a window around the position that was estimated by the first network.
Reference: [3] <author> P. Duchnowski, U. Meier, and A. Waibel. </author> <title> See Me, Hear Me: Integrating Automatic Speech Recognition and Lipreading. </title> <note> to appear in Proc. ICSLP 94. </note>
Reference-contexts: Lip-reading plays an important role in communication by the hearing-impaired and by individuals listening in difficult acoustic environments [10]. Several studies have demonstrated the utility of augmenting automatic speech recognition (ASR) with visual information (eg. [6, 7, 9]). Our own work in this area has been previously reported in <ref> [1, 3] </ref>. However, a major limitation of virtually all the systems was the method by which visual data was acquired. This included such invasive techniques as head-mounted cameras, reflective markers placed on the speaker's lips, and manual extraction of relevant face image sections, effectively precluding practical applications. <p> This included such invasive techniques as head-mounted cameras, reflective markers placed on the speaker's lips, and manual extraction of relevant face image sections, effectively precluding practical applications. In our system as described in <ref> [3] </ref> the process was continuous, automatic, and without special markers but required the speaker to position himself such that his lips appeared within a window shown on a workstation screen. The goal of present research is to free the user from all such interference. <p> We have also investigated alternative methods of combining the audio and visual information at the input and hidden layer levels of the network. Initial results suggesting an advantage of hidden layer combination can be found in <ref> [3] </ref>. This approach is possibly more reflective of the way humans integrate audio-visual input [10] but complicates somewhat the training process. We are also currently experimenting with guiding the combination by an explicit estimate of the acoustic signal-to-noise ratio (SNR). 4.2. <p> We are also currently experimenting with guiding the combination by an explicit estimate of the acoustic signal-to-noise ratio (SNR). 4.2. Results In experiments with an earlier version of the automatic lip-reader <ref> [3] </ref> we established that the gray-level and LDA image representations deliver generally best results. Therefore, we have concentrated on these parameters for the movement-invariant system. We have trained the recognizer on visual/acoustic data from 200/1500 letter sequences from a single speaker and tested on 30 sequences.
Reference: [4] <author> H. Hild and A. Waibel. </author> <title> Connected Letter Recognition with a Multi-State Time Delay Neural Network. </title> <booktitle> Neural Information Processing Systems (NIPS-5), </booktitle> <year> 1993. </year>
Reference-contexts: Representations 2-4 were chosen with the goal of preserving the relevant information in the lip image while substantially decreasing the parameter count. at the phoneme/viseme level). In the basic system a modular Multi-State Time Delay Neural Network (MS-TDNN) <ref> [4] </ref> performs the recognition.
Reference: [5] <author> H.M. Hunke. </author> <title> Locating and Tracking of Human Faces with Neu ral Networks. </title> <type> Technical Report CMU-CS-94-155, </type> <institution> Carnegie Mellon Univ., </institution> <year> 1994. </year>
Reference-contexts: In this paper we present an overview of each of these three components, their combination within the overall scheme, and the performance of the integrated system. 2. FACE TRACKING The task of the face tracking system, described in detail in <ref> [5] </ref>, is to support the lip-locating/reading system with a stable image of the speaker's face. The face-tracker can locate faces in arbitrary environments.
Reference: [6] <author> K. Mase and A. Pentland. </author> <title> Automatic Lipreading by Optical Flow Analysis. </title> <journal> Systems and Computers in Japan, </journal> <volume> 22(6), </volume> <year> 1991, </year> <pages> pp. 67-76. </pages>
Reference-contexts: Lip-reading plays an important role in communication by the hearing-impaired and by individuals listening in difficult acoustic environments [10]. Several studies have demonstrated the utility of augmenting automatic speech recognition (ASR) with visual information (eg. <ref> [6, 7, 9] </ref>). Our own work in this area has been previously reported in [1, 3]. However, a major limitation of virtually all the systems was the method by which visual data was acquired.
Reference: [7] <author> E.D. Petajan. </author> <title> Automatic lipreading to enhance speech recogni tion. </title> <booktitle> in Proc. IEEE Communications Society Global Telecom. Conf., </booktitle> <address> Atlanta GA, </address> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: Lip-reading plays an important role in communication by the hearing-impaired and by individuals listening in difficult acoustic environments [10]. Several studies have demonstrated the utility of augmenting automatic speech recognition (ASR) with visual information (eg. <ref> [6, 7, 9] </ref>). Our own work in this area has been previously reported in [1, 3]. However, a major limitation of virtually all the systems was the method by which visual data was acquired.
Reference: [8] <author> D.A. Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <type> Technical Report CMU-CS-92-115, </type> <institution> Carnegie Mel-lon Univ., </institution> <year> 1992. </year>
Reference-contexts: Two neural networks are used for finding the face coordinates. Their structure is similar to that used in <ref> [8] </ref> and is shown schemat ically in Figure 2. The input retina receives data from the motion and color analysis and determines the position of the face with the first network.
Reference: [9] <author> D.G. Stork, G. Wolff, and E. Levine. </author> <title> Neural network lipreading system for improved speech recognition. </title> <booktitle> in Proc. </booktitle> <address> IJCNN'92. </address>
Reference-contexts: Lip-reading plays an important role in communication by the hearing-impaired and by individuals listening in difficult acoustic environments [10]. Several studies have demonstrated the utility of augmenting automatic speech recognition (ASR) with visual information (eg. <ref> [6, 7, 9] </ref>). Our own work in this area has been previously reported in [1, 3]. However, a major limitation of virtually all the systems was the method by which visual data was acquired.
Reference: [10] <author> Q. Summerfield. </author> <title> Audio-visual Speech Perception, Lipread ing, and Artificial Stimulation. in Hearing Science and Hearing Disorders, M.E. </title> <editor> Lutman and M.P. Haggard eds., </editor> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference-contexts: This paper focuses on the development of a speech recognition system incorporating automatic lip-reading while allowing the user reasonable freedom of movement within a room. Lip-reading plays an important role in communication by the hearing-impaired and by individuals listening in difficult acoustic environments <ref> [10] </ref>. Several studies have demonstrated the utility of augmenting automatic speech recognition (ASR) with visual information (eg. [6, 7, 9]). Our own work in this area has been previously reported in [1, 3]. <p> Initial results suggesting an advantage of hidden layer combination can be found in [3]. This approach is possibly more reflective of the way humans integrate audio-visual input <ref> [10] </ref> but complicates somewhat the training process. We are also currently experimenting with guiding the combination by an explicit estimate of the acoustic signal-to-noise ratio (SNR). 4.2.
Reference: [11] <author> A. Waibel, M.T. Vo, P. Duchnowski, and S. Manke. </author> <title> Multi modal Interfaces. </title> <note> to appear in Artificial Intelligence Review Journal, special issue, 1994. 4 </note>
Reference-contexts: A survey of multiple projects in these areas undertaken in our labs at the University of Karlsruhe and Carnegie Mellon University can be found in <ref> [11] </ref>. This paper focuses on the development of a speech recognition system incorporating automatic lip-reading while allowing the user reasonable freedom of movement within a room. Lip-reading plays an important role in communication by the hearing-impaired and by individuals listening in difficult acoustic environments [10].
References-found: 11

