URL: http://www.cs.rice.edu/~adve/Papers/wppte94.dpipeline_perf.ps.gz
Refering-URL: http://www.cs.rice.edu/~adve/Papers/
Root-URL: 
Title: Compiler Support for Analysis and Tuning Data Parallel Programs  
Author: Vikram S. Adve Charles Koelbel John M. Mellor-Crummey 
Abstract: Data parallel languages such as High-Performance Fortran (HPF) and Fortran D simplify the task of parallel programming by enabling users to express parallel algorithms at a high level. Compilers for these languages are responsible for realizing parallelism and inserting all interprocessor communication. For this reason, these compilers have detailed knowledge of the the relationship between its parallelism and communication in the program. Here, we explore how this knowledge can be exploited to support the process of tuning programs for high performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Ball and J. R. Larus, </author> <title> Optimally profiling and tracing programs, </title> <booktitle> in POPL92, </booktitle> <month> Jan </month> <year> 1992, </year> <pages> pp. 59-70. </pages>
Reference-contexts: Some previous performance tools have attempted to use program information to reduce the volume of information measured at runtime. Sequential profiling tools such as QPT <ref> [1] </ref> use control-flow analysis to reduce the volume of profiling or tracing data.
Reference: [2] <author> S. Bokhari, </author> <title> Communication overhead on the Intel iPSC/860 Hypercube, </title> <type> ICASE Report 10, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> May </month> <year> 1990. </year>
Reference: [3] <author> P. Brezany, M. Gerndt, V. Sipkova, and H. Zima, </author> <title> SUPERB support for irregular scientific computations, </title> <booktitle> in Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> Apr. </month> <year> 1992. </year>
Reference: [4] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng, </author> <title> Generating local addresses and communication sets for data-parallel programs, </title> <booktitle> in Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference: [5] <author> J. Dongarra, J. Bunch, C. Moler, and G. Stewart, </author> <title> LINPACK User's Guide, </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: Dgefa is a Linpack subroutine and a principal computational kernel in the Linpackd benchmark developed by Dongarra et al <ref> [5] </ref>. It performs LU decomposition using Gaussian elimination with partial pivoting.
Reference: [6] <author> T. Fahringer and H. Zima, </author> <title> A static parameter based performance prediction tool for parallel programs, </title> <booktitle> in Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: One such project is the Vienna Fortran Compilation System, which integrates a static performance prediction tool called PPPT into the compilation process <ref> [6] </ref>. In particular, PPPT uses sequential profiling to obtain iteration counts and branch frequencies and combines this information with static analysis of a parallelized program to predict a number of parallel performance metrics.
Reference: [7] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley, PICL: </author> <title> A portable instrumented communication library, C reference manual, </title> <type> Technical Report ORNL/TM-11130, </type> <institution> Oak Ridge National Laboratories, Oak Ridge, TN, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: The most common strategy for collecting dynamic information about message-passing programs involves tracing each communication event. Such a tracing methodology is embodied in packages such as the PICL communication primitives <ref> [7] </ref>. However, for understanding and tuning the performance of programs written in data parallel languages, this level of detail often appears unnecessary.
Reference: [8] <author> M. T. Heath and J. E. </author> <title> Finger, Visualizing performance of parallel programs, </title> <journal> IEEE Software, </journal> <year> (1991), </year> <pages> pp. 29-39. </pages>
Reference-contexts: are beyond the scope of any data-parallel compiler we are aware of.) In the next subsection, we discuss the various compiler enhancements suggested by the applications studied above. 5 Related Work Many previous performance tools for parallel systems support performance monitoring of parallel programs with explicit parallelism, communication and synchronization <ref> [17, 20, 8] </ref>, while some more recent tools support performance debugging of data-parallel programs at the language level [22, 14]. However, there is relatively little work that on integrating performance analysis and compilation, partly because of the lack of availability of sophisticated parallelizing compilers to the performance evaluation community.
Reference: [9] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran language specification, </title> <booktitle> Scientific Programming, 2 (1993), </booktitle> <pages> pp. 1-170. </pages>
Reference-contexts: 1 Introduction Data parallel languages such as High-Performance Fortran (HPF) <ref> [9, 15] </ref> and Fortran D [11] support an abstract model of parallel programming. The principal advantage of these abstract languages is that they insulate programmers from the intricacies of concurrent programming and managing distributed data.
Reference: [10] <author> S. Hiranandani, K. Kennedy, and C. Tseng, </author> <title> Compiler support for machine-independent parallel programming in Fortran D, in Languages, Compilers, and Run-Time Environments for 9 Distributed Memory Machines, </title> <editor> J. Saltz and P. Mehrotra, eds., </editor> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year> <title> [11] , Compiling Fortran D for MIMD distributed-memory machines, </title> <journal> Communications of the ACM, </journal> <volume> 35 (1992), </volume> <pages> pp. </pages> <month> 66-80. </month> <title> [12] , Preliminary experiences with the Fortran D compiler, </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: This optimization has been implemented for the restricted case of FORALL statements in the Kali compiler [16] and previously suggested in <ref> [10] </ref>. The reduction in waiting time for message receives in the case of Red-Black SOR is small but significant since it reduces overhead idle time by nearly half, as shown by the measurements of the send and receive overhead given in Table 6.
Reference: [13] <author> J. K. Hollingsworth and B. P. Miller, </author> <title> Dynamic control of performance monitoring on large scale parallel systems, </title> <booktitle> in International Conference on Supercomputing, </booktitle> <address> Tokyo, </address> <month> Jul </month> <year> 1993. </year>
Reference-contexts: Some previous performance tools have attempted to use program information to reduce the volume of information measured at runtime. Sequential profiling tools such as QPT [1] use control-flow analysis to reduce the volume of profiling or tracing data. Dynamic parallel instrumentation in the W 3 Search Model <ref> [13] </ref> reduces the instrumentation data volume by using sampled performance information to selectively insert instrumentation in interesting parts of a program at runtime, in order to answer specific performance queries.
Reference: [14] <author> R. B. Irvin and B. P. Miller, </author> <title> A performance tool for high-level parallel languages, </title> <type> Technical Report 1204, </type> <institution> WISCONSIN, </institution> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: the various compiler enhancements suggested by the applications studied above. 5 Related Work Many previous performance tools for parallel systems support performance monitoring of parallel programs with explicit parallelism, communication and synchronization [17, 20, 8], while some more recent tools support performance debugging of data-parallel programs at the language level <ref> [22, 14] </ref>. However, there is relatively little work that on integrating performance analysis and compilation, partly because of the lack of availability of sophisticated parallelizing compilers to the performance evaluation community.
Reference: [15] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel, </author> <title> The High Performance Fortran Handbook, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Data parallel languages such as High-Performance Fortran (HPF) <ref> [9, 15] </ref> and Fortran D [11] support an abstract model of parallel programming. The principal advantage of these abstract languages is that they insulate programmers from the intricacies of concurrent programming and managing distributed data.
Reference: [16] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale, </author> <title> Supporting shared data structures on distributed memory machines, </title> <booktitle> in Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: This optimization has been implemented for the restricted case of FORALL statements in the Kali compiler <ref> [16] </ref> and previously suggested in [10]. The reduction in waiting time for message receives in the case of Red-Black SOR is small but significant since it reduces overhead idle time by nearly half, as shown by the measurements of the send and receive overhead given in Table 6.
Reference: [17] <author> B. P. Miller, M. Clark, J. K. Hollingsworth, S. Kierstead, S. Lim, and T. Torzewski, Ips-2: </author> <title> The second generation of a parallel program measurement system, </title> <journal> TOPDS, </journal> <month> 1 </month> <year> (1990). </year>
Reference-contexts: are beyond the scope of any data-parallel compiler we are aware of.) In the next subsection, we discuss the various compiler enhancements suggested by the applications studied above. 5 Related Work Many previous performance tools for parallel systems support performance monitoring of parallel programs with explicit parallelism, communication and synchronization <ref> [17, 20, 8] </ref>, while some more recent tools support performance debugging of data-parallel programs at the language level [22, 14]. However, there is relatively little work that on integrating performance analysis and compilation, partly because of the lack of availability of sophisticated parallelizing compilers to the performance evaluation community.
Reference: [18] <author> J. Ramanujam, </author> <title> Compile-time Techniques for Parallel Execution of Loops on Distributed Memory Multiprocessors, </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, Ohio State University, Columbus, OH, </institution> <year> 1990. </year>
Reference-contexts: By deriving an expression for the optimal granularity in terms of system and program parameters, the compiler could estimate or measure the various parameters to optimize the execution time of the pipeline. Previous authors <ref> [21, 12, 18, 19, ?] </ref> have recognized this trade-off and presented models for choosing the pipeline granularity.
Reference: [19] <author> J. Ramanujam and P. Sadayappan, </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines, </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: By deriving an expression for the optimal granularity in terms of system and program parameters, the compiler could estimate or measure the various parameters to optimize the execution time of the pipeline. Previous authors <ref> [21, 12, 18, 19, ?] </ref> have recognized this trade-off and presented models for choosing the pipeline granularity.
Reference: [20] <author> D. A. Reed, R. A. Aydt, T. M. Madhyastha, R. J. Noe, K. A. Shields, and B. W. Schwartz, </author> <title> An overview of the pablo performance analysis environment, </title> <type> technical report, </type> <institution> UIUCCS, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: We traced the communication operations in the two versions of the program using the Pablo instrumentation library <ref> [20] </ref>. Table 5 shows the distribution of idle time by cause in the two versions of the program for a 1024 fi 1024 matrix on 32 processors. <p> are beyond the scope of any data-parallel compiler we are aware of.) In the next subsection, we discuss the various compiler enhancements suggested by the applications studied above. 5 Related Work Many previous performance tools for parallel systems support performance monitoring of parallel programs with explicit parallelism, communication and synchronization <ref> [17, 20, 8] </ref>, while some more recent tools support performance debugging of data-parallel programs at the language level [22, 14]. However, there is relatively little work that on integrating performance analysis and compilation, partly because of the lack of availability of sophisticated parallelizing compilers to the performance evaluation community.
Reference: [21] <author> A. Rogers and K. Pingali, </author> <title> Process decomposition through locality of reference, </title> <booktitle> in Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: By deriving an expression for the optimal granularity in terms of system and program parameters, the compiler could estimate or measure the various parameters to optimize the execution time of the pipeline. Previous authors <ref> [21, 12, 18, 19, ?] </ref> have recognized this trade-off and presented models for choosing the pipeline granularity.
Reference: [22] <author> M. </author> <title> Thinking Machines Corp., Cambridge, Prism reference manual, </title> <type> tech. rep., </type> <year> 1992. </year>
Reference-contexts: the various compiler enhancements suggested by the applications studied above. 5 Related Work Many previous performance tools for parallel systems support performance monitoring of parallel programs with explicit parallelism, communication and synchronization [17, 20, 8], while some more recent tools support performance debugging of data-parallel programs at the language level <ref> [22, 14] </ref>. However, there is relatively little work that on integrating performance analysis and compilation, partly because of the lack of availability of sophisticated parallelizing compilers to the performance evaluation community.

References-found: 20

