URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/tcm/www/tcm_papers/jpdc.ps.Z
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/tcm/www/Papers.html
Root-URL: 
Title: Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors  
Author: Todd Mowry and Anoop Gupta 
Date: June 1991.  
Note: To appear in the Journal of Parallel and Distributed Computing,  
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: The large latency of memory accesses is a major obstacle in obtaining high processor utilization in large scale shared-memory multiprocessors. Although the provision of coherent caches in many recent machines has alleviated the problem somewhat, cache misses still occur frequently enough that they significantly lower performance. In this paper we evaluate the effectiveness of non-binding software-controlled prefetching, as proposed in the Stanford DASH Multiprocessor, to address this problem. The prefetches are non-binding in the sense that the prefetched data is brought to a cache close to the processor, but is still available to the cache coherence protocol to keep it consistent. Prefetching is software-controlled since the program must explicitly issue prefetch instructions. The paper presents results from detailed simulation studies done in the context of the Stanford DASH multiprocessor. Our results show that for applications with regular data access patterns|we evaluate a particle-based simulator used in aeronautics and an LU-decomposition application|prefetching can be very effective. It was easy to augment the applications to do prefetching and it increased their performance by 100-150% when we prefetched directly into the processor's cache. However, for applications with complex data usage patterns, prefetching was less successful. After much effort, the performance of a distributed-time logic simulation application that made extensive use of pointers and linked lists could be increased only by 30%. The paper also evaluates the effects of various hardware optimizations such as separate prefetch issue buffers, prefetching with exclusive ownership, lockup-free caches, and weaker memory consistency models on the performance of prefetching.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita Adve and Mark Hill. </author> <title> Weak ordering anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A number of different solutions have been proposed. For example, many recent multiprocessors [2, 13, 17] provide caches to help reduce the latency seen by the processor. More recently, weaker memory consistency models <ref> [1, 4, 6, 7] </ref> have been proposed that allow buffering and pipelining of memory references to hide latency. Still another technique is the use of processors with multiple hardware contexts [2, 10, 11, 26].
Reference: [2] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> April: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Techniques that can cope with the large latency of memory accesses are essential for achieving high processor utilization in large scale shared-memory multiprocessors. A number of different solutions have been proposed. For example, many recent multiprocessors <ref> [2, 13, 17] </ref> provide caches to help reduce the latency seen by the processor. More recently, weaker memory consistency models [1, 4, 6, 7] have been proposed that allow buffering and pipelining of memory references to hide latency. <p> More recently, weaker memory consistency models [1, 4, 6, 7] have been proposed that allow buffering and pipelining of memory references to hide latency. Still another technique is the use of processors with multiple hardware contexts <ref> [2, 10, 11, 26] </ref>. These processors tolerate latency by switching from one context to another when they encounter a high latency memory access. The various techniques that have been proposed are not mutually exclusive, but are complementary and offset the limitations of one another.
Reference: [3] <author> James Archibald and Jean-Loup Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <year> 1986. </year>
Reference-contexts: When multiple prefetches are issued back-to-back, the latency of all but the first prefetched reference can be hidden due to the pipelining of the memory accesses. Prefetching offers another benefit in multiprocessors that use an ownership-based cache coherence protocol <ref> [3] </ref>. By selectively prefetching data that are to be modified directly with ownership, it is possible to significantly reduce the write latencies and the ensuing network traffic for obtaining ownership.
Reference: [4] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: A number of different solutions have been proposed. For example, many recent multiprocessors [2, 13, 17] provide caches to help reduce the latency seen by the processor. More recently, weaker memory consistency models <ref> [1, 4, 6, 7] </ref> have been proposed that allow buffering and pipelining of memory references to hide latency. Still another technique is the use of processors with multiple hardware contexts [2, 10, 11, 26].
Reference: [5] <author> Susan J. Eggers and Randy H. Katz. </author> <title> Evaluating the performance of four snooping cache coherency protocols. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-15, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Large cache lines also successfully utilize the block transfer capabilities of modern memory systems. Although the utility of multiword cache lines is almost universally accepted nowadays [23, 21], recent data <ref> [5, 15, 25] </ref> show that cache lines should be kept small for multiprocessors. The primary reason is that the spatial locality is considerably reduced in the process of parallelizing applications for multiprocessors, and large cache lines can result in a significant increase in memory system traffic.
Reference: [6] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: A number of different solutions have been proposed. For example, many recent multiprocessors [2, 13, 17] provide caches to help reduce the latency seen by the processor. More recently, weaker memory consistency models <ref> [1, 4, 6, 7] </ref> have been proposed that allow buffering and pipelining of memory references to hide latency. Still another technique is the use of processors with multiple hardware contexts [2, 10, 11, 26]. <p> pf3 nopf pf3 SC RC SC RC SC RC MP3D LU PTHOR sync ops writes reads busy Comparing the no-prefetching case under the two models, the main performance increase for RC comes from the reduced stall time waiting for the write buffer, since the reads can bypass the pending writes <ref> [6] </ref>. The latency experienced once the read is past the write buffer (shown by the reads section of the bars in the figure) is similar for both models, as is to be expected. 7 When prefetching is added, the latencies decrease considerably. <p> Similar observations have been made in <ref> [6] </ref>. In summary, lockup-free caches along with release consistency effectively eliminate almost all stall time due to writes, even in the case where no prefetches are issued. <p> For the LFC architecture, Figure 14 shows that read-exclusive prefetching offers very little benefit in reducing write buffer stall times. This is to be expected, since this time has already been eliminated by the lockup-free caches <ref> [6] </ref>. However, read-exclusive prefetching can still improve performance by reducing the traffic associated with cache coherency. This benefit occurs in situations where a line is read before it is written.
Reference: [7] <author> Kourosh Gharachorloo, Dan Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A number of different solutions have been proposed. For example, many recent multiprocessors [2, 13, 17] provide caches to help reduce the latency seen by the processor. More recently, weaker memory consistency models <ref> [1, 4, 6, 7] </ref> have been proposed that allow buffering and pipelining of memory references to hide latency. Still another technique is the use of processors with multiple hardware contexts [2, 10, 11, 26]. <p> The primary data cache interfaces to a 256 Kbyte secondary write-back cache. The interface consists of a read buffer and a write buffer. Both the primary and secondary caches are direct-mapped and support 16 byte lines. The DASH architecture uses the release consistency model <ref> [7] </ref> for memory consistency. The latency of a memory access in DASH depends on where in the memory hierarchy the access is serviced. (the simulations done in this paper do model contention, however). The following naming convention is used for describing the memory hierarchy. <p> While conceptually intuitive, the model imposes several restrictions on the buffering and pipelining of memory accesses. One of the least strict models is the release consistency model (RC) <ref> [7] </ref>. It requires synchronization accesses in the program to be identified, but allows significant overlap of memory accesses. In this section we explore the impact that the memory consistency model has on the benefits of prefetching. The BASE architecture, which we have been using so far, uses RC.
Reference: [8] <author> Stephen R. Goldschmidt and Helen Davis. </author> <title> Tango introduction and tutorial. </title> <type> Technical Report CSL-TR-90-410, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: The latency parameters for the simulated architecture are derived from the DASH prototype (see Figure 3). The architecture simulator is tightly coupled to the Tango reference generator <ref> [8] </ref> to assure a correct interleaving of accesses. For example, a process doing a read operation is blocked until that read completes, where the latency of the read is 5 determined by the architecture simulator. Operating system references are not modeled.
Reference: [9] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-Directed Data Prefetching in Multiprocessors with Memory Hierarchies. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <year> 1990. </year>
Reference-contexts: Prefetching in DASH is non-binding in the sense that prefetched data remains visible to the cache coherence protocol [18] to keep it consistent until the processor actually reads the value through a binding access (e.g. a register load operation). In contrast, with binding prefetching <ref> [9, 14] </ref> the value of a later reference is bound (e.g. a processor register is loaded) at the time the prefetch completes. <p> to earlier studies where data prefetching was controlled by the hardware, for example through instruction look-ahead in [16], software control allows the prefetching to be done selectively (thus reducing overhead) and extends the possible interval between the issue of prefetch and the actual use of that data (thus increasing effectiveness) <ref> [9, 21] </ref>. The disadvantage, of course, is that programmer or software intervention is required. The benefits due to prefetching come from several sources. The most obvious benefit occurs when a prefetch is issued early enough that the line is already in the cache by the time it is referenced. <p> We did not want to be constrained by the limits of existing compiler technology to automatically add prefetching. For example, existing compiler technology <ref> [9, 21] </ref> would not have handled the linked-lists and pointers in PTHOR. Also, since we did not have a prefetch-ing compiler available to us, the prefetch statements were added manually to the programs. Prefetches were introduced at the source level through macro statements. <p> It would not be legal to prefetch either of these data structures if binding prefetching <ref> [9, 16] </ref> is used, since there is always a small chance that they will be modified between the time they are prefetched and the time they are used. 11 | 0 | 20 | 40 | 60 | 80 | 100 | 120 Normalized Execution Time prefetches 1.2 2.9 10.2 10.2 <p> This ability to move up prefetches by large amounts is especially important in large scale multiprocessors where latencies can be quite large. Lee also found the effectiveness of his prefetching scheme to be seriously limited by branches and synchronization instructions. Gornish, Granston, and Veidenbaum <ref> [9] </ref> evaluate a binding, software-controlled prefetching scheme. They present prefetching algorithms that focus on array references and attempt to find the earliest point where prefetches can be issued without violating memory coherence. Since prefetching is binding, all control and data dependencies have to be carefully considered.
Reference: [10] <author> Robert H. Halstead, Jr. and Tetsuya Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: More recently, weaker memory consistency models [1, 4, 6, 7] have been proposed that allow buffering and pipelining of memory references to hide latency. Still another technique is the use of processors with multiple hardware contexts <ref> [2, 10, 11, 26] </ref>. These processors tolerate latency by switching from one context to another when they encounter a high latency memory access. The various techniques that have been proposed are not mutually exclusive, but are complementary and offset the limitations of one another.
Reference: [11] <editor> J. S. Kowalik, editor. </editor> <title> Parallel MIMD Computation : The HEP Supercomputer and Its Applications. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: More recently, weaker memory consistency models [1, 4, 6, 7] have been proposed that allow buffering and pipelining of memory references to hide latency. Still another technique is the use of processors with multiple hardware contexts <ref> [2, 10, 11, 26] </ref>. These processors tolerate latency by switching from one context to another when they encounter a high latency memory access. The various techniques that have been proposed are not mutually exclusive, but are complementary and offset the limitations of one another.
Reference: [12] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-85, </pages> <year> 1981. </year>
Reference-contexts: The two columns that need some explaining are the prefetch read latency and the overall read miss latency. The former refers to the latency seen by a read 4 Recall that the BASE architecture allows reads to bypass pending writes. However, since the secondary cache is not lockup-free <ref> [12, 22] </ref>, it cannot allow both a read and a write operation to access cache at the same time. <p> To examine the effects of removing write buffer waiting time, we evaluate a version of the BASE architecture which has lockup-free caches <ref> [12, 22] </ref>. A lockup-free cache permits multiple accesses to be outstanding, thus allowing a read miss to proceed without waiting for writes to complete. The results of our experiments are presented in Figure 12, where the LFC architecture is the BASE architecture with lockup-free caches.
Reference: [13] <author> David J. Kuck, Edward S. Davidson, Duncan H. Lawrie, and Ahmed H. Sameh. </author> <title> Experimental Parallel Computing Architectures: </title> <booktitle> Volume 1 Special Topics in Supercomputing, chapter Parallel Supercomputing Today and the Cedar Approach, </booktitle> <pages> pages 1-23. </pages> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction Techniques that can cope with the large latency of memory accesses are essential for achieving high processor utilization in large scale shared-memory multiprocessors. A number of different solutions have been proposed. For example, many recent multiprocessors <ref> [2, 13, 17] </ref> provide caches to help reduce the latency seen by the processor. More recently, weaker memory consistency models [1, 4, 6, 7] have been proposed that allow buffering and pipelining of memory references to hide latency.
Reference: [14] <author> Roland L. Lee, Pen-Chung Yew, and Duncan H. Lawrie. </author> <title> Data prefetching in shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 28-31, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Prefetching in DASH is non-binding in the sense that prefetched data remains visible to the cache coherence protocol [18] to keep it consistent until the processor actually reads the value through a binding access (e.g. a register load operation). In contrast, with binding prefetching <ref> [9, 14] </ref> the value of a later reference is bound (e.g. a processor register is loaded) at the time the prefetch completes. <p> For example, a binding prefetch cannot be issued if there is a synchronization reference 1 between the prefetch and the subsequent binding reference. Binding prefetching studies done in <ref> [14] </ref> reported significant performance loss due to such interactions. Non-binding prefetching imposes no such restrictions on when a prefetch can be issued; the coherence protocol ensures that the value fetched by the final binding-read will be correct. <p> Also, unless prefetching or non-blocking loads are used, no overlap between cache miss service and computation is possible. Consequently, we do not expect prefetching due to large cache line sizes to play a significant role in solving the latency problem in parallel computers. Lee <ref> [14, 16] </ref> evaluates a binding, hardware-controlled prefetching scheme on an architecture with software-based cache coherence. Instruction lookahead is used at runtime to prefetch data that has been explicitly marked as cacheable at compile time.
Reference: [15] <author> Roland L. Lee, Pen-Chung Yew, and Duncan H. Lawrie. </author> <title> Multiprocessor cache design considerations. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 253-262, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Large cache lines also successfully utilize the block transfer capabilities of modern memory systems. Although the utility of multiword cache lines is almost universally accepted nowadays [23, 21], recent data <ref> [5, 15, 25] </ref> show that cache lines should be kept small for multiprocessors. The primary reason is that the spatial locality is considerably reduced in the process of parallelizing applications for multiprocessors, and large cache lines can result in a significant increase in memory system traffic.
Reference: [16] <author> Roland Lun Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Prefetches can be introduced either explicitly by the programmer, or automatically by the compiler, or perhaps dynamically by the runtime system of a programming language. In contrast to earlier studies where data prefetching was controlled by the hardware, for example through instruction look-ahead in <ref> [16] </ref>, software control allows the prefetching to be done selectively (thus reducing overhead) and extends the possible interval between the issue of prefetch and the actual use of that data (thus increasing effectiveness) [9, 21]. The disadvantage, of course, is that programmer or software intervention is required. <p> It would not be legal to prefetch either of these data structures if binding prefetching <ref> [9, 16] </ref> is used, since there is always a small chance that they will be modified between the time they are prefetched and the time they are used. 11 | 0 | 20 | 40 | 60 | 80 | 100 | 120 Normalized Execution Time prefetches 1.2 2.9 10.2 10.2 <p> Also, unless prefetching or non-blocking loads are used, no overlap between cache miss service and computation is possible. Consequently, we do not expect prefetching due to large cache line sizes to play a significant role in solving the latency problem in parallel computers. Lee <ref> [14, 16] </ref> evaluates a binding, hardware-controlled prefetching scheme on an architecture with software-based cache coherence. Instruction lookahead is used at runtime to prefetch data that has been explicitly marked as cacheable at compile time.
Reference: [17] <author> Dan Lenoski, Kourosh Gharachorloo, James Laudon, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> Design of Scalable Shared-Memory Multiprocessors: The DASH Approach. </title> <booktitle> In Proceedings of COMPCON'90, </booktitle> <pages> pages 62-67, </pages> <year> 1990. </year> <month> 24 </month>
Reference-contexts: 1 Introduction Techniques that can cope with the large latency of memory accesses are essential for achieving high processor utilization in large scale shared-memory multiprocessors. A number of different solutions have been proposed. For example, many recent multiprocessors <ref> [2, 13, 17] </ref> provide caches to help reduce the latency seen by the processor. More recently, weaker memory consistency models [1, 4, 6, 7] have been proposed that allow buffering and pipelining of memory references to hide latency. <p> In this paper, we evaluate the effectiveness of another powerful latency hiding technique, namely non-binding software-controlled prefetching. Our study is done in the context of the Stanford DASH multiprocessor <ref> [17] </ref>, a large scale shared-memory machine that provides for coherent caches, a weaker memory consistency model, and support for software directed prefetching. A prefetch operation in DASH is an explicit non-blocking request to the memory system that brings the prefetched location into a cache close to the processor.
Reference: [18] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory--based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A prefetch operation in DASH is an explicit non-blocking request to the memory system that brings the prefetched location into a cache close to the processor. Prefetching in DASH is non-binding in the sense that prefetched data remains visible to the cache coherence protocol <ref> [18] </ref> to keep it consistent until the processor actually reads the value through a binding access (e.g. a register load operation). In contrast, with binding prefetching [9, 14] the value of a later reference is bound (e.g. a processor register is loaded) at the time the prefetch completes. <p> This section presents the architectural assumptions that we make, the benchmark applications, and the simulation environment used to get performance results. 2.1 Architectural Assumptions For this study, we have chosen an architecture that resembles the DASH multiprocessor <ref> [18] </ref>, a large scale cache-coherent machine currently being built at Stanford. Figure 1 shows the high-level organization of the architecture. <p> The processor request is satisfied as soon as the reply to the original prefetch request arrives. DASH provides three different types of prefetch operations <ref> [18] </ref>. The two main types that we use in this study are read prefetches and read-exclusive prefetches. A read prefetch brings data into the RAC in a read-shared 1 In the DASH prototype currently being built, the processor's own caches are not checked for the presence of the prefetched location.
Reference: [19] <editor> Ewing Lusk, Ross Overbeek, et al. </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: This information will be useful in later sections for understanding the performance results. The selected applications are representative of algorithms used in an engineering computing environment. All of the applications are written in C. The Argonne National Laboratory macro package <ref> [19] </ref> is used to provide synchronization and sharing primitives. The three applications we studied are MP3D, LU, and PTHOR. Table 1 shows some general statistics for the benchmarks when using 16 processors (as is the case throughout this study). Table 1: General statistics for the benchmarks.
Reference: [20] <author> Jeffrey D. McDonald and Donald Baganoff. </author> <title> Vectorization of a particle simulation method for hypersonic rarified flow. </title> <booktitle> In AIAA Thermodynamics, Plasmadynamics and Lasers Conference, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: The results presented in this paper are based on our experience with three parallel applications to which we explicitly added prefetching at the source level. The applications we studied are a particle-based simulator used in aeronautics <ref> [20] </ref>, an LU-decomposition program, and a digital logic simulation program [24]. Our results show that for applications with regular data access patterns, as found in the first two applications, it is easy to add prefetching. <p> Instructions Total Shared Shared Shared Shared Executed Data Refs Data Refs Reads Writes Data Size Application (x 1000) (x 1000) (x 1000) (x 1000) (x 1000) Locks Barriers (KBytes) MP3D 5240 1737 1592 1084 507 0 448 524 PTHOR 18,997 8588 4577 3946 468 79,114 2208 2856 MP3D <ref> [20] </ref> is a 3-dimensional particle simulator. It is used to study the pressure and temperature profiles created as an object flies at high speed through the upper atmosphere.
Reference: [21] <author> Allan K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: to earlier studies where data prefetching was controlled by the hardware, for example through instruction look-ahead in [16], software control allows the prefetching to be done selectively (thus reducing overhead) and extends the possible interval between the issue of prefetch and the actual use of that data (thus increasing effectiveness) <ref> [9, 21] </ref>. The disadvantage, of course, is that programmer or software intervention is required. The benefits due to prefetching come from several sources. The most obvious benefit occurs when a prefetch is issued early enough that the line is already in the cache by the time it is referenced. <p> We did not want to be constrained by the limits of existing compiler technology to automatically add prefetching. For example, existing compiler technology <ref> [9, 21] </ref> would not have handled the linked-lists and pointers in PTHOR. Also, since we did not have a prefetch-ing compiler available to us, the prefetch statements were added manually to the programs. Prefetches were introduced at the source level through macro statements. <p> Such prefetching relies on the spatial locality in program accesses and has the advantage that it requires no software intervention. Large cache lines also successfully utilize the block transfer capabilities of modern memory systems. Although the utility of multiword cache lines is almost universally accepted nowadays <ref> [23, 21] </ref>, recent data [5, 15, 25] show that cache lines should be kept small for multiprocessors. The primary reason is that the spatial locality is considerably reduced in the process of parallelizing applications for multiprocessors, and large cache lines can result in a significant increase in memory system traffic. <p> The reason is that different cache lines within the block may be present in a modified state in caches of different processors. Porterfield <ref> [21] </ref> evaluates both hardware-controlled and software-controlled non-binding prefetching schemes for a uniprocessor. The evaluation is done using several scientific applications from RiCEPS, the Rice Computer Evaluation Program Suite. Hardware prefetching is simulated by using long cache lines (1 word, 16 word, and 32 word cache lines).
Reference: [22] <author> Christoph Scheurich and Michel Dubois. </author> <title> Concurrent miss resolution in multiprocessor caches. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages I: 118-125, </pages> <year> 1988. </year>
Reference-contexts: The two columns that need some explaining are the prefetch read latency and the overall read miss latency. The former refers to the latency seen by a read 4 Recall that the BASE architecture allows reads to bypass pending writes. However, since the secondary cache is not lockup-free <ref> [12, 22] </ref>, it cannot allow both a read and a write operation to access cache at the same time. <p> To examine the effects of removing write buffer waiting time, we evaluate a version of the BASE architecture which has lockup-free caches <ref> [12, 22] </ref>. A lockup-free cache permits multiple accesses to be outstanding, thus allowing a read miss to proceed without waiting for writes to complete. The results of our experiments are presented in Figure 12, where the LFC architecture is the BASE architecture with lockup-free caches.
Reference: [23] <author> Alan Jay Smith. </author> <title> Cache memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Such prefetching relies on the spatial locality in program accesses and has the advantage that it requires no software intervention. Large cache lines also successfully utilize the block transfer capabilities of modern memory systems. Although the utility of multiword cache lines is almost universally accepted nowadays <ref> [23, 21] </ref>, recent data [5, 15, 25] show that cache lines should be kept small for multiprocessors. The primary reason is that the spatial locality is considerably reduced in the process of parallelizing applications for multiprocessors, and large cache lines can result in a significant increase in memory system traffic.
Reference: [24] <author> Larry Soule and Anoop Gupta. </author> <title> Parallel Distributed-Time Logic Simulation. </title> <journal> IEEE Design and Test of Computers, </journal> <volume> 6(6) </volume> <pages> 32-48, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The results presented in this paper are based on our experience with three parallel applications to which we explicitly added prefetching at the source level. The applications we studied are a particle-based simulator used in aeronautics [20], an LU-decomposition program, and a digital logic simulation program <ref> [24] </ref>. Our results show that for applications with regular data access patterns, as found in the first two applications, it is easy to add prefetching. <p> Each processor waits until a column has been produced, and then that column is used to modify all columns that the processor owns. Once a processor completes a column, it releases any processors waiting for that column. For our experiments we performed LU-decomposition on a 200x200 matrix. PTHOR <ref> [24] </ref> is a parallel logic simulator based on the Chandy-Misra simulation algorithm. Unlike centralized-time algorithms, this algorithm does not rely on a single global time during simulation. The primary data structures associated with the simulator are the logic elements (e.g.
Reference: [25] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> Measurement, analysis, and improvement of the cache behavior of shared data in cache coherent multiprocessors. </title> <type> Technical Report CSL-TR-90-412, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: Large cache lines also successfully utilize the block transfer capabilities of modern memory systems. Although the utility of multiword cache lines is almost universally accepted nowadays [23, 21], recent data <ref> [5, 15, 25] </ref> show that cache lines should be kept small for multiprocessors. The primary reason is that the spatial locality is considerably reduced in the process of parallelizing applications for multiprocessors, and large cache lines can result in a significant increase in memory system traffic.
Reference: [26] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <month> June </month> <year> 1989. </year> <month> 25 </month>
Reference-contexts: More recently, weaker memory consistency models [1, 4, 6, 7] have been proposed that allow buffering and pipelining of memory references to hide latency. Still another technique is the use of processors with multiple hardware contexts <ref> [2, 10, 11, 26] </ref>. These processors tolerate latency by switching from one context to another when they encounter a high latency memory access. The various techniques that have been proposed are not mutually exclusive, but are complementary and offset the limitations of one another.
References-found: 26

