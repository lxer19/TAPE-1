URL: http://www.csl.sri.com/~livio/tech.01.ps.Z
Refering-URL: http://www.csl.sri.com/~livio/papers.html
Root-URL: 
Email: meseguer-@csl.sri.com  
Phone: Phone: 415-859-2969 Fax: 415-859-2844  
Title: Execution-Driven Distributed Simulation of Parallel Architectures  
Author: Livio Ricciulli, Patrick Lincoln, and Jos Meseguer 
Keyword: Performance evaluation, Distributed simulation of computer architectures, Execution-driven simulation, Distributed shared memory computers, Relaxed memory consistency.  
Address: Menlo Park, California 94025, USA  mail: -livio, lincoln,  
Affiliation: Computer Science Laboratory SRI International  E  
Abstract: A new methodology for the asynchronous discrete event-driven simulation of parallel computers is proposed. This methodology integrates sequential and distributed simulation in a unified paradigm and is applicable to the simulation of all classes of parallel computer architectures. In our own simulation work we accelerated simulations by more than an order of magnitude with parallel execution speedup efficiencies in the order of 60-70%. When simulating in parallel, our approach has the important benefit of testing the robustness of a simulated design by not hiding the asynchronous nature of the system being studied (our simulation model preserves the nondeterministic behavior of certain parallel executions). Unlike other distributed simulation methodologies, our execution model does not rely on a global view of virtual time to maintain coherent distributed event causality relations. The simulator correctly executes a given parallel application that observes a particular synchronization model of choice without a notion of virtual time. We then estimate a global virtual time in which the execution could have been carried out. We give a detailed description of our simulation methodology, the computational models that it can implement, and the conditions for its correctness. We also give some preliminary performance results obtained by implementing our parallelization technique to simulate a massively parallel machine on a CM-5 computer and on a heterogeneous network of workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve, M. D. Hill, </author> <title> "Weak Ordering A New Definition", </title> <booktitle> Proc. 17th Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Instead of emulating a serialization of global memory access, the hardware guarantees a fixed set of coherence properties that are understood by the software as a synchronization model. If the software then observes the constraints of the given synchronization model, then the architecture produces executions that are sequentially consistent <ref> [1] </ref>. Such relaxed consistency models allow much more parallelism than previous ones because global memory access operations do not always need to be ordered but in some cases are allowed to propagate asynchronously without introducing overhead to enforce sequential causality.
Reference: [2] <author> W. C. Athas, C. L. Seitz, </author> <title> "Multicomputers: Message Passing Concurrent Computers", </title> <journal> IEEE Computer Magazine, </journal> <month> August </month> <year> 1988. </year>
Reference-contexts: Our simulation methodology is general enough to efficiently simulate any parallel MIMD or MIMD/SIMD architecture in a distributed manner. In the case of MIMD multicomputers our methodology applies to both distributed memory machines with asynchronous message passing <ref> [2] </ref> and Distributed Shared Memory (DSM) multicomputers. For the sake of concreteness and because of their intrinsic interest, this paper focuses on the simulation of DSM machines. Because parallel computers have explicit synchronization requirements, their simulation can be decoupled in two distinct parts: 1.
Reference: [3] <author> J. Boyle, R. Butler, T. Disz, B. Glickfeld, E. Lusk, R. Overbeek, J. Patterson, R. Stevens, </author> <title> "Portable Programs for Parallel Processors", </title> <publisher> Holt, Reinhart and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: This method could synchronize, in a distributed manner, the local virtual clocks of any system with synchronization primitives based on lock and clear operations (e.g., using the ANL macros <ref> [3] </ref>). In a distributed implementation each resource has associated with it a time Tr that indicates the local virtual time of the last LP to release that resource.
Reference: [4] <author> K. M. Chandy, J. Misra, </author> <title> "Asynchronous Distributed Simulation via a Sequence of Parallel Computations", </title> <journal> Communications of the ACM, </journal> <month> April </month> <year> 1981. </year>
Reference-contexts: When a simulation's observable behavior of interest is the relative ordering of all the simulated events of the distributed entities, the simulation must enforce such ordering with simulation techniques of the style proposed by Chandy and Misra <ref> [4] </ref> or Jefferson [8]. Instead, if the relevant observable behavior of a system is not the ordering of events itself but its final state and its overall performance, we can greatly simplify the simulation methodology.
Reference: [5] <author> A. Costa, A. De Gloria, P. Fababoschi, M. Olivieri, </author> <title> "An evaluation system for Distributed-time VHDL Simulation", </title> <booktitle> Proc. of the 8th Workshop on Parallel and Distributed Simulation 1994, </booktitle> <address> Edinburgh, Scotland, UK. </address>
Reference-contexts: We believe, though, that our approach is directly applicable to other parallel machines not only for RTL simulations, but also for higher-level symbolic simulations [10] and for lower-level simulations of parallel computer designs directly specified in VHDL <ref> [5] </ref>. This latter kind would have the enormous benefit of avoiding the possibility of translation errors by integrating architectural studies directly into the digital design process.
Reference: [6] <author> M. Dubois, C. Scheurich, F. Briggs, </author> " <title> Memory Access Buffering in Multiprocessors", </title> <booktitle> Proc. 13th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1986. </year>
Reference: [7] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, J. Hennessy, </author> <title> "Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors", </title> <booktitle> Proc. 17th Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Our methodology correctly simulates the execution of a parallel program that obeys a particular synchronization model, provided that the simulator correctly implements that model in a distributed manner. For example, a program written under the Release Consistency model <ref> [7] </ref> executes correctly on a simulator of the hardware mechanisms required by this memory consistency paradigm and therefore yields a sequentially consistent simulation. <p> Condition 2 essentially requires the simulator to be correct. The above requirements do not impose any particular computational model or architecture for the applicability of our methodology. Because of its simplicity and potential for exploiting application parallelism, we have used Release Consistency <ref> [7] </ref> as the synchronization model of the particular simulation experiments we have performed. For this model we can translate the correctness conditions given in [7] to further specify the correctness conditions 1 and 2 mentioned above: 1. <p> Because of its simplicity and potential for exploiting application parallelism, we have used Release Consistency <ref> [7] </ref> as the synchronization model of the particular simulation experiments we have performed. For this model we can translate the correctness conditions given in [7] to further specify the correctness conditions 1 and 2 mentioned above: 1. <p> C Virtual Processor Virtual Network T7 B Virtual Processor Virtual Network T3 Virtual Processor Virtual Network T1 T10 T4 T9 supports the effectiveness of our approach. 4.1 The Simulated Applications As in the Release Consistency programming model <ref> [7] </ref>, we do not enforce a total ordering of the synchronization operations because this would place a very strong burden on the simulator, thus reducing its efficiency.
Reference: [8] <author> D. Jefferson, </author> <title> "Virtual Time", </title> <journal> ACM Trans. Programming Languages and Systems, </journal> <month> July </month> <year> 1985. </year>
Reference-contexts: When a simulation's observable behavior of interest is the relative ordering of all the simulated events of the distributed entities, the simulation must enforce such ordering with simulation techniques of the style proposed by Chandy and Misra [4] or Jefferson <ref> [8] </ref>. Instead, if the relevant observable behavior of a system is not the ordering of events itself but its final state and its overall performance, we can greatly simplify the simulation methodology. <p> In our methodology the clock synchronization solution is simple, because the simulated application will block the execution of an LP and therefore enforce an ordering of events that will never need to be undone. Unlike Time Warp <ref> [8] </ref>, we only modify the virtual clocks without having to restore previous states. For the purpose of clock synchronization we view a parallel execution as a series of resource requests, acquisitions, and releases.
Reference: [9] <author> L. Lamport, </author> <title> "How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs", </title> <journal> IEEE Trans. on Conputers C-28(9):241-248, </journal> <month> September </month> <year> 1979. </year>
Reference-contexts: In the context of shared memory parallel programming a variety of relaxed memory consistency models [1,6,8] have been proposed. These models define conditions under which shared memory parallel programs appear to the programmer as having sequentially consistent executions as defined by Lamport <ref> [9] </ref>, but allow the hardware to schedule coherence messages more freely. Instead of emulating a serialization of global memory access, the hardware guarantees a fixed set of coherence properties that are understood by the software as a synchronization model.
Reference: [10] <author> S. Narain, R. Chadha, </author> <title> "Symbolic Discrete-Event Simulation", Discrete-Event Systems, Manufacturing Systems and Communication Networks, </title> <editor> Editors: P.R. Kumar and P. Varaiya, </editor> <publisher> LNCS, Springer Verlag 1994. </publisher>
Reference-contexts: We have thus far experimented using our approach to simulate one particular parallel architecture at the Register Transfer Level (RTL). We believe, though, that our approach is directly applicable to other parallel machines not only for RTL simulations, but also for higher-level symbolic simulations <ref> [10] </ref> and for lower-level simulations of parallel computer designs directly specified in VHDL [5]. This latter kind would have the enormous benefit of avoiding the possibility of translation errors by integrating architectural studies directly into the digital design process.
Reference: [11] <author> H. Schwetman, "Csim: </author> <title> A C-based, Processoriented Simulation Language", </title> <type> MCC Technical Report. </type>
Reference-contexts: The RRM system we simulated consists of 64 SIMD processors, each with its own separate controller executing in MIMD mode. The simulator holds a very detailed description of all the hardware down to the register level; it uses the libraries provided by the general-purpose simulation package Csim <ref> [11] </ref>. This package is an extension of the C language; it allows very efficient processoriented event-driven simulations. Each device of each node is a separate process that interfaces with other processes through synchronization lines (events) and hardware queues (mailboxes).
Reference: [12] <author> D. Shasha, M. Snir, </author> " <title> Efficient and Correct Execution of Parallel Programs that Share Memory", </title> <journal> ACM Trans. Programming Languages and Systems, </journal> <month> April </month> <year> 1988. </year>
Reference-contexts: Condition 1 basically requires that any two conflicting events (two operations with the same address, of which at least one is a write <ref> [12] </ref>) must be ordered by a synchronization operation between them. Condition 2.1 specifies that an LP must block (generate null events) until it acquires needed synchronization resources. Condition 2.2 specifies that all memory operations must be committed before a release can cause an LP to resume execution.
Reference: [13] <author> C. Weems, E. Riseman, A. Hanson, </author> <title> "The DARPA Image Understanding Benchmark for Parallel Computers". </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 11(1), </volume> <year> 1991. </year>
Reference-contexts: We have evaluated our methodology on the simulation of three applications: sorting, hardware gate-level simulation, and the (D)ARPA image understanding benchmark for parallel computers <ref> [13] </ref>. All these applications follow the Single Program Multiple Data (SPMD) paradigm and were coded in assembly language. 4.2 Comparison of Sequential vs. Parallel Tables I and II report the performance of our parallel simulator on a CM-5 and on a heterogeneous network of workstations.
Reference: [14] <author> B. Crothers, </author> <title> "Multithreading Gets Lost on P100 Systems", </title> <type> Infoworld, </type> <institution> cover page, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: This latter kind would have the enormous benefit of avoiding the possibility of translation errors by integrating architectural studies directly into the digital design process. Potentially, very costly errors, such as Intel's Pen-tium/90 problems in a multiprocessing environment <ref> [14] </ref>, given enough computational power, could have been discovered by using this kind of methodology. Our methodology is also quite general with regard to the physical platforms on which it can efficiently run and and allows seamless integration of sequential and parallel simulation.
Reference: [15] <author> P. Lincoln, J. Meseguer, L. Ricciulli, </author> <title> "The Rewrite Rule Machine Node Architecture and Its Performance", </title> <booktitle> Proc. Conpar 94 - VAPP VI, </booktitle> <address> Lintz, Austria, </address> <publisher> LNCS 854. </publisher>
Reference-contexts: explain the particular choices we have made in our own architecture simulation work and explain how these techniques also allow to effiently simulate a complex interconnection network in a very parallel way. 3.1 The simulated architecture We have applied our ideas to the simulation of the Rewrite Rule Machine (RRM) <ref> [15] </ref>; a novel MIMD/SIMD parallel architecture currently being designed and simulated at SRI International. The RRM system we simulated consists of 64 SIMD processors, each with its own separate controller executing in MIMD mode.
References-found: 15

