URL: http://www.research.att.com/~singer/papers/psa.ps.gz
Refering-URL: http://www.research.att.com/~singer/pub.html
Root-URL: 
Email: danar@cs.huji.ac.il  singer@cs.huji.ac.il NAFTALI TISHBY tishby@cs.huji.ac.il  
Title: The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length  
Author: DANA RON YORAM SINGER 
Address: Jerusalem 91904, Israel  
Affiliation: Institute of Computer Science, Hebrew University,  
Abstract: We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata (PSA). Though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. The learning algorithm is motivated by applications in human-machine interaction. Here we present two applications of the algorithm. In the first one we apply the algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second application we construct a simple stochastic model for E.coli DNA. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> N. Abe and M. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference-contexts: One might hope that the problem can be overcome by improving the algorithm used or by finding a new approach. Unfortunately, there is strong evidence that the problem cannot be solved efficiently. Abe and Warmuth <ref> [1] </ref> study the problem of training HMMs. The HMM training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by HMMs. They prove that HMMs are not trainable in time polynomial in the alphabet size, unless RP = NP. <p> Probabilistic Finite Automata A Probabilistic Finite Automaton (PFA) M is a 5-tuple (Q; ; t; fl; ), where Q is a finite set of states, is a finite alphabet, t : Qfi ! Q is the transition function, fl : Qfi ! <ref> [0; 1] </ref> is the next symbol probability function, and : Q ! [0; 1] is the initial probability distribution over the starting states. <p> Probabilistic Finite Automaton (PFA) M is a 5-tuple (Q; ; t; fl; ), where Q is a finite set of states, is a finite alphabet, t : Qfi ! Q is the transition function, fl : Qfi ! <ref> [0; 1] </ref> is the next symbol probability function, and : Q ! [0; 1] is the initial probability distribution over the starting states. <p> The nodes of the tree are labeled by pairs (s; fl s ) 8 DANA RON, YORAM SINGER, NAFTALI TISHBY where s is the string associated with the walk starting from that node and ending in the root of the tree, and fl s : ! <ref> [0; 1] </ref> is the next symbol probability function related with s. We require that for every string s labeling a node in the tree, P 2 fl s () = 1.
Reference: 2. <author> L. E. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of markov chains. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: A detailed tutorial on the theory of HMMs as well as selected applications in speech recognition is given by Rabiner [22]. A commonly used procedure for learning an HMM from a given sample is a maximum likelihood parameter estimation procedure that is based on the Baum-Welch method [3], <ref> [2] </ref> (which is a special case of the EM (Expectation-Maximization) algorithm [7]).
Reference: 3. <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> A maximization technique occuring in the statistical analysis of probabilistic functions of markov chains. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 41(1) </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: A detailed tutorial on the theory of HMMs as well as selected applications in speech recognition is given by Rabiner [22]. A commonly used procedure for learning an HMM from a given sample is a maximum likelihood parameter estimation procedure that is based on the Baum-Welch method <ref> [3] </ref>, [2] (which is a special case of the EM (Expectation-Maximization) algorithm [7]).
Reference: 4. <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: Note that the sum (10c) can be computed efficiently in a recursive manner. Moreover, the maximization of Equation (10a) can be performed efficiently by using a dynamic programming (DP) scheme <ref> [4] </ref>. This scheme requires O (jQj fi t) operations. If jQj is large, then approximation schemes to the optimal DP, such as the stack decoding algorithm [13] can be employed. Using similar methods it is also possible to correct errors when insertions and deletions of symbols occur as well.
Reference: 5. <author> A. Blumer. </author> <title> Applications of DAWGs to data compression. </title> <editor> In A. Capocelli, editor, </editor> <title> Sequences: Combinatorics, compression, security, </title> <booktitle> and transmition, </booktitle> <pages> pages 303-311. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Laird and Saul [19] describe a prediction algorithm which is similar in spirit to our algorithm and is based on the Markov tree or Directed Acyclic Word Graph approach which is used for data compression <ref> [5] </ref>. They do not analyze the correctnes of the algorithm formally, but present several applications of the algorithm. 1.2. Overview of the Paper The paper is organized as follows.
Reference: 6. <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: In this definition we chose the Kullback-Leibler (KL) divergence as a distance measure between distributions. Similar definitions can be considered for other distance measures such as the variation and the quadratic distances. Note that the KL-divergence bounds the variation distance as follows <ref> [6] </ref>: D KL [P 1 jjP 2 ] 1 2 1 . Since the L 1 norm bounds the L 2 norm, the last bound holds for the quadratic distance as well.
Reference: 7. <author> A.P. Dempster, N.M Laird, and D.B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Stat. Soc., </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: A commonly used procedure for learning an HMM from a given sample is a maximum likelihood parameter estimation procedure that is based on the Baum-Welch method [3], [2] (which is a special case of the EM (Expectation-Maximization) algorithm <ref> [7] </ref>). However, this algorithm is guaranteed to converge only to a local maximum, and thus we are not assured that the hypothesis it outputs can serve 4 DANA RON, YORAM SINGER, NAFTALI TISHBY as a good approximation for the target distribution.
Reference: 8. <author> J.A. Fill. </author> <title> Eigenvalue bounds on convergence to stationary for nonreversible Markov chains, with an application to exclusion process. </title> <journal> Annals of Applied Probability, </journal> <volume> 1 </volume> <pages> 62-87, </pages> <year> 1991. </year>
Reference-contexts: We show that this convergence rate can be bounded using the expansion properties of a weighted graph related to U M [20] or more generally, using algebraic properties of U M , namely, its second largest eigenvalue <ref> [8] </ref>. 4. Emulation of PSAs by PSTs In this section we show that for every PSA there exists an equivalent PST which is not much larger. This allows us to consider the PST equivalent to our target PSA, whenever it is convenient.
Reference: 9. <author> Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 315-324, </pages> <year> 1993. </year>
Reference-contexts: Hoffgen [12] studies families of distributions related to the ones studied in this paper, but his algorithms depend exponentially and not polynomially on the order, or memory length, of the distributions. Freund et. al. <ref> [9] </ref> point out that their result for learning typical deterministic finite automata from random walks without membership queries, can be extended to learning typical PFAs. Unfortunately, there is strong evidence indicating that the problem of learning general PFAs is hard.
Reference: 10. <author> D. Gillman and M. Sipser. </author> <title> inference and minimization of hidden markov chains. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 147-158, </pages> <year> 1994. </year>
Reference-contexts: The HMM training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by HMMs. They prove that HMMs are not trainable in time polynomial in the alphabet size, unless RP = NP. Gillman and Sipser <ref> [10] </ref> study the problem of exactly inferring an (ergodic) HMM over a binary alphabet when the inference algorithm can query a probability oracle for the long-term probability of any binary string. They prove that inference is hard: any algorithm for inference must make exponentially many oracle calls.
Reference: 11. <author> G. I. </author> <title> Good. Statistics of language: Introduction. </title> <editor> In A. R. Meetham and R.A. Hudson, editors, </editor> <booktitle> Encyclopedia of Linguistics, Information and Control, </booktitle> <pages> pages 567-581. </pages> <publisher> Pergamon Press, Oxford, </publisher> <address> England, </address> <year> 1969. </year>
Reference-contexts: They prove that inference is hard: any algorithm for inference must make exponentially many oracle calls. Their method is information theoretic and does not depend on separation assumptions for any complexity classes. Natural simpler alternatives, which are often used as well, are order L Markov chains [29], <ref> [11] </ref>, also known as n-gram models.
Reference: 12. <author> K.-U. Hoffgen. </author> <title> Learning and robust learning of product distributions. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 97-106, </pages> <year> 1993. </year>
Reference-contexts: As noted earlier, the size of an order L Markov chain is exponential in L and hence, if we want to capture more than very short term memory dependencies in the sequences, of substantial length in the sequences, then these models are clearly not practical. Hoffgen <ref> [12] </ref> studies families of distributions related to the ones studied in this paper, but his algorithms depend exponentially and not polynomially on the order, or memory length, of the distributions.
Reference: 13. <author> F. Jelinek. </author> <title> A fast sequential decoding algorithm using a stack. </title> <journal> IBM J. Res. Develop., </journal> <volume> 13 </volume> <pages> 675-685, </pages> <year> 1969. </year>
Reference-contexts: Moreover, the maximization of Equation (10a) can be performed efficiently by using a dynamic programming (DP) scheme [4]. This scheme requires O (jQj fi t) operations. If jQj is large, then approximation schemes to the optimal DP, such as the stack decoding algorithm <ref> [13] </ref> can be employed. Using similar methods it is also possible to correct errors when insertions and deletions of symbols occur as well. We tested the algorithm by taking a text from Jenesis and corrupting it in two ways. First, we altered every letter (including blanks) with probability 0:2.
Reference: 14. <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. </title> <type> Technical report, </type> <institution> IBM T.J. Watson Research Center, </institution> <year> 1985. </year>
Reference-contexts: 1. Introduction Statistical modeling of complex sequences is a fundamental goal of machine learning due to its wide variety of natural applications. The most noticeable examples of such applications are statistical models in human communication such as natural language, handwriting and speech <ref> [14] </ref>, [21], and statistical models of biological sequences such as DNA and proteins [17]. These kinds of complex sequences clearly do not have any simple underlying statistical source since they are generated by natural sources.
Reference: 15. <author> M. Kearns, Y.Mansour, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie. </author> <title> On the learnabil-ity of discrete distributions. </title> <booktitle> In The 25th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1994. </year>
Reference-contexts: Despite an intractability result concerning the learnabil-ity of distributions generated by Probabilistic Finite Automata <ref> [15] </ref> (described in Section 1.1), our restricted model can be learned in a PAC-like sense efficiently. This has not been shown so far for any of the more popular sequence modeling algorithms. We present two applications of the learning algorithm. <p> Freund et. al. [9] point out that their result for learning typical deterministic finite automata from random walks without membership queries, can be extended to learning typical PFAs. Unfortunately, there is strong evidence indicating that the problem of learning general PFAs is hard. Kearns et. al. <ref> [15] </ref> show that PFAs are not efficiently learnable under the assumption that there is no efficient algorithm for learning noisy parity functions in the PAC model. <p> The Learning Model The learning model described in this paper is motivated by the PAC model for learning boolean concepts from labeled examples and is similar in spirit to that introduced in <ref> [15] </ref>. We start by defining an *-good hypothesis PST with respect to a given PSA. Definition. Let M be a PSA and let T be a PST. Let P M and P T be the two probability distributions they generate respectively.
Reference: 16. <author> P. Krishnan and J. S. Vitter. </author> <title> Optimal prediction for prefetching in the worst case. </title> <type> Technical Report CS-1993-26, </type> <institution> Duke University, </institution> <year> 1993. </year>
Reference-contexts: However, in case the source generating the examples is a PST, they are able to show that this PST convergence only in the limit of infinite sequence length to that source. Vitter and Krishnan [31], <ref> [16] </ref> adapt a version of the Ziv-Lempel data compression algorithm [34] to get a page prefetching algorithm, where the sequence of page accesses is assumed to be generated by a PFA.
Reference: 17. <author> A. Krogh, S.I. Mian, and D. Haussler. </author> <title> A hidden markov model that finds genes in E. coli DNA. </title> <type> Technical Report UCSC-CRL-93-16, </type> <institution> University of California at Santa-Cruz, </institution> <year> 1993. </year>
Reference-contexts: The most noticeable examples of such applications are statistical models in human communication such as natural language, handwriting and speech [14], [21], and statistical models of biological sequences such as DNA and proteins <ref> [17] </ref>. These kinds of complex sequences clearly do not have any simple underlying statistical source since they are generated by natural sources. However, they typically exhibit the following statistical property, which we refer to as the short memory property. <p> The PSAs built are rather small compared to the HMM model described in <ref> [17] </ref>: the PSA that models the coding regions has 65 states and the PSA that models the intergenic regions has 81 states. We tested the performance of the models by calculating the log-likelihood of the two models obtained on test data drawn from intergenic regions. <p> The latter property combined with the results mentioned above indicate that the PSA model might be used when performing tasks such as DNA gene locating. However, we should stress that we have done only a preliminary step in this direction and the results obtained in <ref> [17] </ref> as part of a complete parsing system are better. intergenic regions and a PSA trained on data taken from coding regions. The test data was taken from intergenic regions. In 90% of the cases the likelihood of the first PSA was higher.
Reference: 18. <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(6) </volume> <pages> 1331-1348, </pages> <year> 1993. </year>
Reference-contexts: This way we avoid exponential grow-up in the number of strings tested. A similar type of branch-and-bound technique (with various bounding criteria) is applied in many algorithms which use trees as data structures (cf. <ref> [18] </ref>). The set of strings tested at each step, denoted by S, can be viewed as a kind of potential frontier of the growing tree T , which is of bounded size.
Reference: 19. <author> P. Laird and R. Saul. </author> <title> Discrete sequence prediction and its applications. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 43-68, </pages> <year> 1994. </year>
Reference-contexts: This is true for almost all page access sequences (in the limit of the sequence length). Laird and Saul <ref> [19] </ref> describe a prediction algorithm which is similar in spirit to our algorithm and is based on the Markov tree or Directed Acyclic Word Graph approach which is used for data compression [5].
Reference: 20. <author> M. Mihail. </author> <title> Conductance and convergence of Markov chains A combinatorial treatment of expanders. </title> <booktitle> In Proceedings 30th Annual Conference on Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: When given one sample string, the given string must be long enough so as to ensure convergence of the probability of visiting a state to the stationary probability. We show that this convergence rate can be bounded using the expansion properties of a weighted graph related to U M <ref> [20] </ref> or more generally, using algebraic properties of U M , namely, its second largest eigenvalue [8]. 4. Emulation of PSAs by PSTs In this section we show that for every PSA there exists an equivalent PST which is not much larger.
Reference: 21. <author> A. Nadas. </author> <title> Estimation of probabilities in the language model of the IBM speech recognition system. </title> <journal> IEEE Trans. on ASSP, </journal> <volume> 32(4) </volume> <pages> 859-861, </pages> <year> 1984. </year>
Reference-contexts: 1. Introduction Statistical modeling of complex sequences is a fundamental goal of machine learning due to its wide variety of natural applications. The most noticeable examples of such applications are statistical models in human communication such as natural language, handwriting and speech [14], <ref> [21] </ref>, and statistical models of biological sequences such as DNA and proteins [17]. These kinds of complex sequences clearly do not have any simple underlying statistical source since they are generated by natural sources.
Reference: 22. <author> L.R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <year> 1989. </year> <type> 24 DANA RON, YORAM SINGER, </type> <institution> NAFTALI TISHBY </institution>
Reference-contexts: Related Work The most powerful (and perhaps most popular) model used in modeling natural sequences is the Hidden Markov Model (HMM). A detailed tutorial on the theory of HMMs as well as selected applications in speech recognition is given by Rabiner <ref> [22] </ref>. A commonly used procedure for learning an HMM from a given sample is a maximum likelihood parameter estimation procedure that is based on the Baum-Welch method [3], [2] (which is a special case of the EM (Expectation-Maximization) algorithm [7]).
Reference: 23. <author> J. Rissanen. </author> <title> A universal data compression system. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 29(5) </volume> <pages> 656-664, </pages> <year> 1983. </year>
Reference-contexts: Kearns et. al. [15] show that PFAs are not efficiently learnable under the assumption that there is no efficient algorithm for learning noisy parity functions in the PAC model. The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in <ref> [23] </ref> and have been used for other tasks such as universal data compression [23], [24], [32], [33]. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [33]. <p> The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in <ref> [23] </ref> and have been used for other tasks such as universal data compression [23], [24], [32], [33]. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [33].
Reference: 24. <author> J. Rissanen. </author> <title> Complexity of strings in the class of Markov sources. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 32(4) </volume> <pages> 526-532, </pages> <year> 1986. </year>
Reference-contexts: The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in [23] and have been used for other tasks such as universal data compression [23], <ref> [24] </ref>, [32], [33]. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [33].
Reference: 25. <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The power of amnesia. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Thus the running time of the algorithm is of the order of the size of the sample times L. 7. Applications A slightly modified version of our learning algorithm was applied and tested on various problems such as: correcting corrupted text, predicting DNA bases <ref> [25] </ref>, and LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH 19 part-of-speech disambiguation resolving [28]. We are still exploring other possible applications of the algorithm. Here we demonstrate how the algorithm can be used to correct corrupted text and how to build a simple model for DNA strands. 7.1.
Reference: 26. <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> On the learnability and usage of acyclic probabilistic finite automata. </title> <booktitle> In Proc. of the 8th Annual Conf. on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: In the second application we construct a simple stochastic model for E.coli DNA. Combined with a learning algorithm for a different subclass of probabilistic automata <ref> [26] </ref>, the algorithm presented here is part of a complete cursive handwriting recognition system [30]. 1.1. Related Work The most powerful (and perhaps most popular) model used in modeling natural sequences is the Hidden Markov Model (HMM).
Reference: 27. <author> K.E. Rudd. </author> <title> Maps, genes, sequences, and computers: An Escherichia coli case study. </title> <journal> ASM News, </journal> <volume> 59 </volume> <pages> 335-341, </pages> <year> 1993. </year>
Reference-contexts: DNA strands are composed of sequences of protein coding genes and fillers between those regions named intergenic regions. Locating the coding genes is necessary, prior to any further DNA analysis. Using manually segmented data of E. coli <ref> [27] </ref> we built two different PSAs, one for the coding regions and one for the intergenic regions.
Reference: 28. <author> H. Schutze and Y. Singer. </author> <title> Part-of-Speech tagging using a variable memory Markov model. </title> <booktitle> In Proceedings of ACL 32'nd, </booktitle> <year> 1994. </year>
Reference-contexts: Applications A slightly modified version of our learning algorithm was applied and tested on various problems such as: correcting corrupted text, predicting DNA bases [25], and LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH 19 part-of-speech disambiguation resolving <ref> [28] </ref>. We are still exploring other possible applications of the algorithm. Here we demonstrate how the algorithm can be used to correct corrupted text and how to build a simple model for DNA strands. 7.1.
Reference: 29. <author> C.E. Shannon. </author> <title> Prediction and entropy of printed english. </title> <journal> Bell Sys. Tech. Jour., </journal> <volume> 30(1) </volume> <pages> 50-64, </pages> <year> 1951. </year>
Reference-contexts: This observation lead Shannon, in his seminal paper <ref> [29] </ref>, to suggest modeling such sequences by Markov chains of order L &gt; 1, where the order is the memory length of the model. Alternatively, such sequences may be modeled by Hidden Markov Models (HMMs) which are more complex distribution generators and hence may capture additional properties of natural sequences. <p> They prove that inference is hard: any algorithm for inference must make exponentially many oracle calls. Their method is information theoretic and does not depend on separation assumptions for any complexity classes. Natural simpler alternatives, which are often used as well, are order L Markov chains <ref> [29] </ref>, [11], also known as n-gram models.
Reference: 30. <author> Y. Singer and N. Tishby. </author> <title> An adaptive cursive handwriting recognition system. </title> <type> Technical Report CS-TR-22, </type> <institution> Hebrew University, </institution> <year> 1995. </year>
Reference-contexts: In the second application we construct a simple stochastic model for E.coli DNA. Combined with a learning algorithm for a different subclass of probabilistic automata [26], the algorithm presented here is part of a complete cursive handwriting recognition system <ref> [30] </ref>. 1.1. Related Work The most powerful (and perhaps most popular) model used in modeling natural sequences is the Hidden Markov Model (HMM). A detailed tutorial on the theory of HMMs as well as selected applications in speech recognition is given by Rabiner [22].
Reference: 31. <author> J. S. Vitter and P. Krishnan. </author> <title> Optimal prefetching via data compression. </title> <booktitle> In Proceedings of the Thirty-Second Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 121-130, </pages> <year> 1991. </year>
Reference-contexts: However, in case the source generating the examples is a PST, they are able to show that this PST convergence only in the limit of infinite sequence length to that source. Vitter and Krishnan <ref> [31] </ref>, [16] adapt a version of the Ziv-Lempel data compression algorithm [34] to get a page prefetching algorithm, where the sequence of page accesses is assumed to be generated by a PFA.
Reference: 32. <author> M.J. Weinberger, A. Lempel, and J. Ziv. </author> <title> A sequential algorithm for the universal coding of finite-memory sources. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 38 </volume> <pages> 1002-1014, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in [23] and have been used for other tasks such as universal data compression [23], [24], <ref> [32] </ref>, [33]. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [33].
Reference: 33. <author> F.M.J. Willems, Y.M. Shtarkov, and T.J. Tjalkens. </author> <title> The context tree weighting method: Basic properties. </title> <journal> IEEE Trans. Inform. Theory, </journal> <note> 1993. Submitted for publication. </note>
Reference-contexts: The machines used as our hypothesis representation, namely Probabilistic Suffix Trees (PSTs), were introduced (in a slightly different form) in [23] and have been used for other tasks such as universal data compression [23], [24], [32], <ref> [33] </ref>. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [33]. This paper describes an efficient sequential procedure for universal data compression for PSTs by using a larger model class. <p> slightly different form) in [23] and have been used for other tasks such as universal data compression [23], [24], [32], <ref> [33] </ref>. Perhaps the strongest among these results (which has been brought to our attention after the completion of this work) and which is most tightly related to our result is [33]. This paper describes an efficient sequential procedure for universal data compression for PSTs by using a larger model class. This algorithm can be viewed as a distribution learning algorithm but the hypothesis it produces is not a PST or a PSA and hence cannot be used for many applications.
Reference: 34. <author> J. Ziv and A. Lempel. </author> <title> Compression of individual sequences via variable-rate coding. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 24 </volume> <pages> 530-536, </pages> <month> Sept. </month> <year> 1978. </year>
Reference-contexts: However, in case the source generating the examples is a PST, they are able to show that this PST convergence only in the limit of infinite sequence length to that source. Vitter and Krishnan [31], [16] adapt a version of the Ziv-Lempel data compression algorithm <ref> [34] </ref> to get a page prefetching algorithm, where the sequence of page accesses is assumed to be generated by a PFA.
References-found: 34

