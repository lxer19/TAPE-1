URL: http://www.research.att.com/~schapire/papers/FreundSc96.ps.Z
Refering-URL: http://www.research.att.com/~yoav/talks/boostforstat.html
Root-URL: 
Email: fyoav, schapireg@research.att.com  
Title: Experiments with a New Boosting Algorithm  
Author: Yoav Freund Robert E. Schapire 
Address: 600 Mountain Avenue Murray Hill, NJ 07974-0636  
Affiliation: AT&T Laboratories  
Note: Machine Learning: Proceedings of the Thirteenth International Conference, 1996.  
Abstract: In an earlier paper, we introduced a new boosting algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a pseudo-loss which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's bagging method when used to aggregate various classifiers (including decision trees and single attribute-value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Leo Breiman. </author> <title> Bagging predictors. </title> <type> Technical Report 421, </type> <institution> Department of Statistics, University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: This paper describes two distinct sets of experiments. In the first set of experiments, described in Section 3, we compared boosting to bagging, a method described by Breiman <ref> [1] </ref> which works in the same general fashion (i.e., by repeatedly rerunning a given weak learning algorithm, and combining the computed classifiers), but which constructs each distribution in a simpler manner. (Details given below.) We compared boosting with bagging because both methods work by combining many classifiers. <p> The second property is that the learning algorithm be sensitive to changes in the training examples so that significantly different hypotheses are generated for different training sets. In this sense, boosting is similar to Breiman's bagging <ref> [1] </ref> which performs best when the weak learner exhibits such unstable behavior. However, unlike bagging, boosting tries actively to force the weak learning algorithm to change its hypotheses by changing the distribution over the training examples as a function of the errors made by previously generated hypotheses. <p> Do for t = 1; 2; : : : ; T 1. Call WeakLearn, providing it with mislabel distribution D t . 2. Get back a hypothesis h t : X fi Y ! <ref> [0; 1] </ref>. 3. Calculate the pseudo-loss of h t : * t = 1 X D t (i; y) 1 h t (x i ; y i ) + h t (x i ; y) : 5. <p> In this case, rather than choosing between 7 and 9, the hypothesis may output the set f7; 9g indicating that both labels are plausible. We also allow the weak learner to indicate a degree of plausibility. Thus, each weak hypothesis outputs a vector <ref> [0; 1] </ref> k , where the components with values close to 1 or 0 correspond to those labels considered to be plausible or implausible, respectively. <p> On each round t of boosting, AdaBoost.M2 (Figure 2) supplies the weak learner with a mislabel distribution D t . In response, the weak learner computes a hypothesis h t of the form h t : X fi Y ! <ref> [0; 1] </ref>. There is no restriction on P y h t (x; y). In particular, the prediction vector does not have to define a probability distribution. <p> If using ordinary error (AdaBoost.M1), these predictions p 0 , p 1 , p ? would be simple classifications; for pseudo-loss, the predictions would be vectors in <ref> [0; 1] </ref> k (where k is the number of classes). The algorithm FindAttrTest searches exhaustively for the classifier of the form given above with minimum error or pseudo-loss with respect to the distribution provided by the booster. <p> Furthermore, we did not expect pseudo-loss to be helpful when using a weak learning algorithm as strong as C4.5, since such an algorithm will usually be able to find a hypothesis with error less than 1=2. 3.2 BAGGING We compared boosting to Breiman's <ref> [1] </ref> bootstrap aggregating or bagging method for training and combining multiple copies of a learning algorithm. <p> Using the more sophisticated tangent distance [21] is in our future plans. Each weak hypothesis is defined by a subset P of the training examples, and a mapping : P ! <ref> [0; 1] </ref> k . Given a new test point x, such a weak hypothesis predicts the vector (x 0 ) where x 0 2 P is the closest point to x.
Reference: [2] <author> Leo Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Unpublished manuscript, </type> <year> 1996. </year>
Reference-contexts: However, unlike bagging, boosting may also reduce the bias of the learning algorithm, as discussed above. (See Kong and Di-etterich [17] for further discussion of the bias and variance reducing effects of voting multiple hypotheses, as well as Breiman's <ref> [2] </ref> very recent work comparing boosting and bagging in terms of their effects on bias and variance.) In our first set of experiments, we compare boosting and bagging, and try to use that comparison to separate between the bias and variance reducing effects of boosting. Previous work.
Reference: [3] <author> William Cohen. </author> <title> Fast effective rule induction. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 115-123, </pages> <year> 1995. </year>
Reference-contexts: searches for very simple prediction rules which test on a single attribute (similar to Holte's very simple classification rules [14]); (2) an algorithm that searches for a single good decision rule that tests on a conjunction of attribute tests (similar in flavor to the rule-formation part of Cohen's RIPPER algorithm <ref> [3] </ref> and Furnkranz and Widmer's IREP algorithm [11]); and (3) Quinlan's C4.5 decision-tree algorithm [18]. We tested these algorithms on a collection of 27 benchmark learning problems taken from the UCI repository.
Reference: [4] <author> Tom Dietterich, Michael Kearns, and Yishay Mansour. </author> <title> Applying the weak learning framework to understand and improve C4.5. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <year> 1996. </year>
Reference-contexts: Kearns and Mansour [16] argue that C4.5 can itself be viewed as a kind of boosting algorithm, so a comparison of AdaBoost and C4.5 can be seen as a comparison of two competing boosting algorithms. See Dietterich, Kearns and Mansour's paper <ref> [4] </ref> for more detail on this point. In the second set of experiments, we test the performance of boosting on a nearest neighbor classifier for handwritten digit recognition.
Reference: [5] <author> Harris Drucker and Corinna Cortes. </author> <title> Boosting decision trees. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <year> 1996. </year>
Reference-contexts: Drucker, Schapire and Simard [8, 7] performed the first experiments using a boosting algorithm. They used Schapire's [20] original boosting algorithm combined with a neural net for an OCR problem. Follow-up comparisons to other ensemble methods were done by Drucker et al. [6]. More recently, Drucker and Cortes <ref> [5] </ref> used AdaBoost with a decision-tree algorithm for an OCR task. Jackson and Craven [15] used AdaBoost to learn classifiers represented by sparse perceptrons, and tested the algorithm on a set of benchmarks.
Reference: [6] <author> Harris Drucker, Corinna Cortes, L. D. Jackel, Yann LeCun, and Vladimir Vap-nik. </author> <title> Boosting and other ensemble methods. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1289-1301, </pages> <year> 1994. </year>
Reference-contexts: Previous work. Drucker, Schapire and Simard [8, 7] performed the first experiments using a boosting algorithm. They used Schapire's [20] original boosting algorithm combined with a neural net for an OCR problem. Follow-up comparisons to other ensemble methods were done by Drucker et al. <ref> [6] </ref>. More recently, Drucker and Cortes [5] used AdaBoost with a decision-tree algorithm for an OCR task. Jackson and Craven [15] used AdaBoost to learn classifiers represented by sparse perceptrons, and tested the algorithm on a set of benchmarks.
Reference: [7] <author> Harris Drucker, Robert Schapire, and Patrice Simard. </author> <title> Boosting performance in neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 705-719, </pages> <year> 1993. </year>
Reference-contexts: Previous work. Drucker, Schapire and Simard <ref> [8, 7] </ref> performed the first experiments using a boosting algorithm. They used Schapire's [20] original boosting algorithm combined with a neural net for an OCR problem. Follow-up comparisons to other ensemble methods were done by Drucker et al. [6].
Reference: [8] <author> Harris Drucker, Robert Schapire, and Patrice Simard. </author> <title> Improving performance in neural networks using a boosting algorithm. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 42-49, </pages> <year> 1993. </year>
Reference-contexts: Previous work. Drucker, Schapire and Simard <ref> [8, 7] </ref> performed the first experiments using a boosting algorithm. They used Schapire's [20] original boosting algorithm combined with a neural net for an OCR problem. Follow-up comparisons to other ensemble methods were done by Drucker et al. [6].
Reference: [9] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <volume> 121(2) </volume> <pages> 256-285, </pages> <year> 1995. </year>
Reference-contexts: Boosting works by repeatedly running a given weak 1 learning algorithm on various distributions over the training data, and then combining the classifiers produced by the weak learner into a single composite classifier. The first provably effective boosting algorithms were presented by Schapire [20] and Freund <ref> [9] </ref>. More recently, we described and analyzed AdaBoost, and we argued that this new boosting algorithm has certain properties which make it more practical and easier to implement than its predecessors [10]. This algorithm, which we used in all our experiments, is described in detail in Section 2.
Reference: [10] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoreticgeneralizationof online learning and an application to boosting. Unpublishedmanuscript available electronically (on our web pages, or by email request). An extended abstract appeared in Computational Learning Theory: </title> <booktitle> Second European Conference, </booktitle> <volume> EuroCOLT '95, </volume> <pages> pages 23-37, </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The first provably effective boosting algorithms were presented by Schapire [20] and Freund [9]. More recently, we described and analyzed AdaBoost, and we argued that this new boosting algorithm has certain properties which make it more practical and easier to implement than its predecessors <ref> [10] </ref>. This algorithm, which we used in all our experiments, is described in detail in Section 2. Home page: http://www.research.att.com/orgs/ssr/people/uid. <p> Output the final hypothesis: h fin (x) = arg max X log fi t 2 THE BOOSTING ALGORITHM In this section, we describe our boosting algorithm, called AdaBoost. See our earlier paper <ref> [10] </ref> for more details about the algorithm and its theoretical properties. We describe two versions of the algorithm which we denote AdaBoost.M1 and AdaBoost.M2. <p> This does not necessarily imply that the test error is small. However, if the weak hypotheses are simple and T not too large, then the difference between the training and test errors can also be theoretically bounded (see our earlier paper <ref> [10] </ref> for more on this subject). The experiments in this paper indicate that the theoretical bound on the training error is often weak, but generally correct qualitatively. However, the test error tends to be much better than the theory would suggest, indicating a clear defect in our theoretical understanding. <p> by the formula * t = 1 X D t (i; y) 1 h t (x i ; y i ) + h t (x i ; y) : Space limitations prevent us from giving a complete derivation of this formula which is explained in detail in our earlier paper <ref> [10] </ref>. It can be verified though that the pseudo-loss is minimized when correct labels y i are assigned the value 1 and incorrect labels y 6= y i assigned the value 0. Further, note that pseudo-loss 1=2 is trivially achieved by any constant-valued hypothesis h t .
Reference: [11] <author> Johannes Furnkranz and Gerhard Widmer. </author> <title> Incremental reduced error pruning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 70-77, </pages> <year> 1994. </year>
Reference-contexts: test on a single attribute (similar to Holte's very simple classification rules [14]); (2) an algorithm that searches for a single good decision rule that tests on a conjunction of attribute tests (similar in flavor to the rule-formation part of Cohen's RIPPER algorithm [3] and Furnkranz and Widmer's IREP algorithm <ref> [11] </ref>); and (3) Quinlan's C4.5 decision-tree algorithm [18]. We tested these algorithms on a collection of 27 benchmark learning problems taken from the UCI repository.
Reference: [12] <author> Geoffrey W. Gates. </author> <title> The reduced nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <pages> pages 431-433, </pages> <year> 1972. </year>
Reference-contexts: Speed-up is achieved by reducing the number of prototypes in the hypothesis (and thus the required number of distance calculations) without increasing the error rate. It is a similar approach to that of nearest-neighbor editing <ref> [12, 13] </ref> in which one tries to find the minimal set of prototypes that is sufficient to label all the training set correctly. The dataset comes from the US Postal Service (USPS) and consists of 9709 training examples and 2007 test examples.
Reference: [13] <author> Peter E. Hart. </author> <title> The condensed nearest neighbor rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-14:515-516, </volume> <month> May </month> <year> 1968. </year>
Reference-contexts: We show that the boosting al-gorithm is an effective way for finding a small subset of prototypes that performs almost as well as the complete set. We also show that it compares favorably to the standard method of Condensed Nearest Neighbor <ref> [13] </ref> in terms of its test error. There seem to be two separate reasons for the improvement in performance that is achieved by boosting. <p> Speed-up is achieved by reducing the number of prototypes in the hypothesis (and thus the required number of distance calculations) without increasing the error rate. It is a similar approach to that of nearest-neighbor editing <ref> [12, 13] </ref> in which one tries to find the minimal set of prototypes that is sufficient to label all the training set correctly. The dataset comes from the US Postal Service (USPS) and consists of 9709 training examples and 2007 test examples. <p> We compared the performance of the boosting algorithm to that of the strawman hypothesis that uses the same number of prototypes. We also compared our performance to that of the condensed nearest neighbor rule (CNN) <ref> [13] </ref>, a greedy method for finding a small set of prototypes which correctly classify the entire training set. 4.1 RESULTS AND DISCUSSION The results of our experiments are summarized in Table 3 and Figure 7.
Reference: [14] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-91, </pages> <year> 1993. </year>
Reference-contexts: In our experiments, we compared boosting to bagging using a number of different weak learning algorithms of varying levels of sophistication. These include: (1) an algorithm that searches for very simple prediction rules which test on a single attribute (similar to Holte's very simple classification rules <ref> [14] </ref>); (2) an algorithm that searches for a single good decision rule that tests on a conjunction of attribute tests (similar in flavor to the rule-formation part of Cohen's RIPPER algorithm [3] and Furnkranz and Widmer's IREP algorithm [11]); and (3) Quinlan's C4.5 decision-tree algorithm [18].
Reference: [15] <author> Jeffrey C. Jackson and Mark W. Craven. </author> <title> Learning sparse perceptrons. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <year> 1996. </year>
Reference-contexts: They used Schapire's [20] original boosting algorithm combined with a neural net for an OCR problem. Follow-up comparisons to other ensemble methods were done by Drucker et al. [6]. More recently, Drucker and Cortes [5] used AdaBoost with a decision-tree algorithm for an OCR task. Jackson and Craven <ref> [15] </ref> used AdaBoost to learn classifiers represented by sparse perceptrons, and tested the algorithm on a set of benchmarks. Finally, Quinlan [19] recently conducted an independent comparison of boosting and bagging combined with C4.5 on a collection of UCI benchmarks.
Reference: [16] <author> Michael Kearns and Yishay Mansour. </author> <title> On the boosting ability of top-down decision tree learning algorithms. </title> <booktitle> In Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1996. </year>
Reference-contexts: When combined with C4.5, boosting still seems to outperform bagging slightly, but the results are less compelling. We also found that boosting can be used with very simple rules (algorithm (1)) to construct classifiers that are quite good relative, say, to C4.5. Kearns and Mansour <ref> [16] </ref> argue that C4.5 can itself be viewed as a kind of boosting algorithm, so a comparison of AdaBoost and C4.5 can be seen as a comparison of two competing boosting algorithms. See Dietterich, Kearns and Mansour's paper [4] for more detail on this point.
Reference: [17] <author> Eun Bae Kong and Thomas G. Dietterich. </author> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proceedings of the Twelfth InternationalConference on Machine Learning, </booktitle> <pages> pages 313-321, </pages> <year> 1995. </year>
Reference-contexts: Thus, like bagging, boosting may have the effect of producing a combined hypothesis whose variance is significantly lower than those produced by the weak learner. However, unlike bagging, boosting may also reduce the bias of the learning algorithm, as discussed above. (See Kong and Di-etterich <ref> [17] </ref> for further discussion of the bias and variance reducing effects of voting multiple hypotheses, as well as Breiman's [2] very recent work comparing boosting and bagging in terms of their effects on bias and variance.) In our first set of experiments, we compare boosting and bagging, and try to use
Reference: [18] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Holte's very simple classification rules [14]); (2) an algorithm that searches for a single good decision rule that tests on a conjunction of attribute tests (similar in flavor to the rule-formation part of Cohen's RIPPER algorithm [3] and Furnkranz and Widmer's IREP algorithm [11]); and (3) Quinlan's C4.5 decision-tree algorithm <ref> [18] </ref>. We tested these algorithms on a collection of 27 benchmark learning problems taken from the UCI repository. The main conclusion of our experiments is that boosting performs significantly and uniformly better than bagging when the weak learning algorithm generates fairly simple classifiers (algorithms (1) and (2) above). <p> In the second phase, the list is pruned by selecting the prefix of the list with minimum error (or pseudo-loss) on the pruning set. The third weak learner is Quinlan's C4.5 decision-tree algorithm <ref> [18] </ref>. We used all the default options with pruning turned on. Since C4.5 expects an unweighted training sample, we used resampling. Also, we did not attempt to use AdaBoost.M2 since C4.5 is designed to minimize error, not pseudo-loss.
Reference: [19] <author> J. Ross Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In Proceedings, Fourteenth National Conference on Artificial Intelligence, </booktitle> <year> 1996. </year>
Reference-contexts: More recently, Drucker and Cortes [5] used AdaBoost with a decision-tree algorithm for an OCR task. Jackson and Craven [15] used AdaBoost to learn classifiers represented by sparse perceptrons, and tested the algorithm on a set of benchmarks. Finally, Quinlan <ref> [19] </ref> recently conducted an independent comparison of boosting and bagging combined with C4.5 on a collection of UCI benchmarks. <p> Naturally, one needs to consider whether the improvement in error is worth the additional computation time. Although we used 100 rounds of boosting, Quinlan <ref> [19] </ref> got good results using only 10 rounds. Boosting may have other applications, besides reducing the error of a classifier. For instance, we saw in Section 4 that boosting can be used to find a small set of prototypes for a nearest neighbor classifier.
Reference: [20] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: Boosting works by repeatedly running a given weak 1 learning algorithm on various distributions over the training data, and then combining the classifiers produced by the weak learner into a single composite classifier. The first provably effective boosting algorithms were presented by Schapire <ref> [20] </ref> and Freund [9]. More recently, we described and analyzed AdaBoost, and we argued that this new boosting algorithm has certain properties which make it more practical and easier to implement than its predecessors [10]. <p> Previous work. Drucker, Schapire and Simard [8, 7] performed the first experiments using a boosting algorithm. They used Schapire's <ref> [20] </ref> original boosting algorithm combined with a neural net for an OCR problem. Follow-up comparisons to other ensemble methods were done by Drucker et al. [6]. More recently, Drucker and Cortes [5] used AdaBoost with a decision-tree algorithm for an OCR task.
Reference: [21] <author> Patrice Simard, Yann Le Cun, and John Denker. </author> <title> Efficient pattern recognition using a new transformation distance. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 50-58, </pages> <year> 1993. </year> <month> 9 </month>
Reference-contexts: This is a very naive metric, but it gives reasonably good performance. A nearest-neighbor classifier which uses all the training examples as prototypes achieves a test error of 5:7% (2:3% on randomly partitioned data). Using the more sophisticated tangent distance <ref> [21] </ref> is in our future plans. Each weak hypothesis is defined by a subset P of the training examples, and a mapping : P ! [0; 1] k .
References-found: 21

