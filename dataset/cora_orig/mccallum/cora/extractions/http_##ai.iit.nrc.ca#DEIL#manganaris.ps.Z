URL: http://ai.iit.nrc.ca/DEIL/manganaris.ps.Z
Refering-URL: http://ai.iit.nrc.ca/DEIL/abstracts.html
Root-URL: 
Title: Bayesian Induction of Features in Temporal Domains  
Author: Stefanos Manganaris 
Keyword: Bayesian feature induction, deterministic time series, supervised concept learning, minimum message length, discriminative/generative classifiers.  
Note: This research has been supported in part by a grant from NASA Ames (NAG 2-834).  
Address: Box 1679, Station B Nashville, TN 37235, U.S.A.  
Affiliation: Dept. of Computer Science Vanderbilt University  
Email: E-mail: stefanos@vuse.vanderbilt.edu.  
Phone: Tel. (615)343-4111, (615)343-8006 (fax).  
Date: March 31, 1995  
Abstract: Most concept induction algorithms process concept instances described in terms of properties that remain constant over time. In temporal domains, instances are best described in terms of properties whose values vary with time. Data engineering is called upon in temporal domains to transform the raw data into an appropriate form for concept induction. I investigate a method for inducing features suitable for classifying finite, univariate, time series that are governed by unknown deterministic processes contaminated by noise. In a supervised setting, I induce piecewise polynomials of appropriate complexity to characterize the data in each class, using Bayesian model induction principles. In this study, I evaluate the proposed method empirically in a semi-deterministic domain: the waveform classification problem, originally presented in the CART book. I compared the classification accuracy of the proposed algorithm to the accuracy attained by C4.5 under various noise levels. Feature induction improved the classification accuracy in noisy situations, but degraded it when there was no noise. The results demonstrate the value of the proposed method in the presence of noise, and reveal a weakness shared by all classifiers using generative rather than discriminative models: sensitivity to model inaccuracies. 
Abstract-found: 1
Intro-found: 1
Reference: [BFOS84] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year> <title> 10 Feature Induction in Temporal Domains </title>
Reference-contexts: The proposed algorithm has been implemented as part of an experimental system called Calchas. Empirical Evaluation The waveform recognition problem was introduced by Breiman et al. in <ref> [BFOS84] </ref>. In this domain there are three classes. Instances are generated by combining two of 6 Feature Induction in Temporal Domains 0 4 1 3 5 7 9 11 13 15 17 19 21 t h2 instances. <p> With the percentage of correctly classified instances as the dependent measure of performance, I compared Calchas to C4.5 under a range of noise levels. C4.5 was directly applied to the data, by treating each sample point as a feature. I chose C4.5 rather than CART, which was used in <ref> [BFOS84] </ref>, because it was readily available; both C4.5 and CART are decision tree inducers. Both systems were trained on 300 instances, created at random with equal priors for each class. The accuracy of each system was estimated using a disjoint set of 5; 000 instances, created similarly. <p> The accuracy of each system was estimated using a disjoint set of 5; 000 instances, created similarly. For this test data, and under noise with variance equal to one, the optimal Bayes rate was given in <ref> [BFOS84] </ref> to be 86%. Feature Induction in Temporal Domains 7 I ran each system with data under noise with variance 0, 0:5, 1, 2, 4, and 8, and estimated the mean classification accuracies and standard error of the means in five runs (Fig. 3). <p> On the original waveform data (noise 2 = 1), C4.5 performed similarly to the 72% accuracy reported for CART in <ref> [BFOS84] </ref>; Calchas improved this accuracy to 77% (p = 0:01). Discussion It has been shown elsewhere that in domains where instances are governed by a few deterministic processes, piecewise polynomial models perform better than decision trees [Man95].
Reference: [Che90] <author> Peter Cheeseman. </author> <title> On finding the most probable model. </title> <editor> In J. Shrager and P. Langley, editors, </editor> <title> Computational Models of Discovery and Theory Formation, </title> <booktitle> chapter 3, </booktitle> <pages> pages 73-95. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Bayes' rule expresses the posterior probability of a model in terms of the product of its likelihood and its prior probability, balancing, in effect, the complexity of the model and its ability to explain the data. To assign priors, I use the minimum message length principle <ref> [Che90, Ris87, WF87] </ref>. The negative logarithm with base two of the prior probability of a model is computed as the length in bits of its shortest possible encoding. Inducing a k-model requires finding the value of k and the k-partition that yield the most probable k-model.
Reference: [Eli75] <author> Peter Elias. </author> <title> Universal codeword sets and representations of the integers. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-21(2):194-203, </volume> <month> March </month> <year> 1975. </year>
Reference-contexts: A positive integer i is coded with log 2 (m + 1) bits, when m is a known upper bound for i. When there is no bound in the state of prior knowledge, I use a universal prior <ref> [Ris83, Eli75] </ref>. For the noise variance, I do not use a uniform prior; instead, I use an encoding scheme that penalizes large deviations from the expected variance provided as prior knowledge.
Reference: [HW73] <author> M. Hollander and D. Wolfe. </author> <title> Nonparametric Statistical Methods. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The statistical significance of the results was evaluated by the Wilcoxon rank sum test (a nonparametric test for comparing the medians of two independent samples of cardinal or ordinal data <ref> [HW73] </ref>); p-values are shown at each point. On the original waveform data (noise 2 = 1), C4.5 performed similarly to the 72% accuracy reported for CART in [BFOS84]; Calchas improved this accuracy to 77% (p = 0:01).
Reference: [MA94] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. Machine-readable data repository. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science., </institution> <year> 1994. </year>
Reference-contexts: Note, the waveform domain is semi-deterministic: there is an infinite number of deterministic processes associated with each class (indexed by the scaling random variable u). The original waveform data (with noise 2 = 1) are available at the UCI repository of machine learning databases <ref> [MA94] </ref>. I used the waveform domain to evaluate empirically the performance of Calchas. With the percentage of correctly classified instances as the dependent measure of performance, I compared Calchas to C4.5 under a range of noise levels.
Reference: [Man95] <author> Stefanos Manganaris. </author> <title> Learning to classify sensor data. </title> <type> Technical Report CS-95-10, </type> <institution> Vanderbilt University, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Discussion It has been shown elsewhere that in domains where instances are governed by a few deterministic processes, piecewise polynomial models perform better than decision trees <ref> [Man95] </ref>. The models provide a representational inductive bias that can improve the classification performance under noise. In the absence of noise, piecewise polynomials still perform well in such domains, because an accurate model can be induced for each process.
Reference: [Ped89] <author> Edwin P.D. Pednault. </author> <title> Some experiments in applying inductive inference principles to surface reconstruction. </title> <booktitle> In Proc. of the Eleventh International Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 1603-1609, </pages> <year> 1989. </year>
Reference-contexts: Continuity is not enforced across intervals. As for k-models, "goodness" is judged by the posterior probability. This part of the algorithm is adopted from an algorithm presented by Pednault in <ref> [Ped89] </ref> for surface reconstruction in computer vision. The number of possible partitions for an interval grows exponentially with the length of the time series.
Reference: [PTVF92] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cam-bridge University Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: The optimal precision trades message length for likelihood, and vice-versa, to maximize the posterior probability. To estimate the optimal precision I use a standard golden-section search algorithm <ref> [PTVF92] </ref>. To compute the length of the optimal encoding of a k-model, I first encode the number of disjuncts.
Reference: [Ris83] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: A positive integer i is coded with log 2 (m + 1) bits, when m is a known upper bound for i. When there is no bound in the state of prior knowledge, I use a universal prior <ref> [Ris83, Eli75] </ref>. For the noise variance, I do not use a uniform prior; instead, I use an encoding scheme that penalizes large deviations from the expected variance provided as prior knowledge.
Reference: [Ris87] <author> Jorma Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 49(3) </volume> <pages> 223-239, </pages> <year> 1987. </year>
Reference-contexts: Bayes' rule expresses the posterior probability of a model in terms of the product of its likelihood and its prior probability, balancing, in effect, the complexity of the model and its ability to explain the data. To assign priors, I use the minimum message length principle <ref> [Che90, Ris87, WF87] </ref>. The negative logarithm with base two of the prior probability of a model is computed as the length in bits of its shortest possible encoding. Inducing a k-model requires finding the value of k and the k-partition that yield the most probable k-model.
Reference: [SM92] <author> Padhraic Smyth and Jeff Mellstrom. </author> <title> Detecting novel classes with applications to fault diagnosis. </title> <editor> In Derek Sleeman and Peter Edwards, editors, </editor> <booktitle> Proc. of the Ninth Intl. Conf. on Machine Learning, </booktitle> <pages> pages 416-425, </pages> <year> 1992. </year>
Reference-contexts: The exact circumstances under which the performance gain due to the models exceeds the losses due to inaccuracies depend on the domain. In the waveform problem, the comparison results with C4.5 indicate that the gains outweigh the losses when the noise variance exceeds 0:5. Smyth and Mellstrom in <ref> [SM92] </ref> distinguish between classifiers that use generative and discriminative models. Generative models focus on the predictability of features given a class; they provide descriptions of how data can be generated for the Feature Induction in Temporal Domains 9 class.
Reference: [WF87] <author> C. S. Wallace and P. R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 49(3) </volume> <pages> 252-265, </pages> <year> 1987. </year>
Reference-contexts: Bayes' rule expresses the posterior probability of a model in terms of the product of its likelihood and its prior probability, balancing, in effect, the complexity of the model and its ability to explain the data. To assign priors, I use the minimum message length principle <ref> [Che90, Ris87, WF87] </ref>. The negative logarithm with base two of the prior probability of a model is computed as the length in bits of its shortest possible encoding. Inducing a k-model requires finding the value of k and the k-partition that yield the most probable k-model.
References-found: 12

