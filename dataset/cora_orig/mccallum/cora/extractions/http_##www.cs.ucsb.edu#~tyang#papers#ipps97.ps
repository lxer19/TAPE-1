URL: http://www.cs.ucsb.edu/~tyang/papers/ipps97.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/SWEB.html
Root-URL: http://www.cs.ucsb.edu
Email: thanasisg@cs.ucsb.edu  
Title: Dynamic Processor Scheduling with Client Resources for Fast Multi-resolution WWW Image Browsing  
Author: Daniel Andresen, Tao Yang, David Watson, and Athanassios Poulakidas fdandrese, tyang, david, 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: WWW-based Internet information service has grown enormously during the last few years, and major performance bottlenecks have been caused by WWW server and Internet bandwidth inadequacies. Utilizing client-site computing power and also multi-processor support at the server site can substantially improve the system response time. In this paper, we examine the use of scheduling techniques in monitoring and adapting to workload variation at client and server sites for supporting fast WWW image browsing. We provide both analytic and experimental results to identify the impact of system loads and network bandwidth on response times and demonstrate the effectiveness of our scheduling strategy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.Andresen, L.Carver, R.Dolin, C.Fischer, J.Frew, M.Goodchild, O.Ibarra, R.Kothuri, M.Larsgaard, B.Manjunath, D.Nebert, J.Simpson, T.Smith, T.Yang, Q.Zheng, </author> <title> The WWW Prototype of the Alexandria Digital Library, </title> <booktitle> Proceedings of ISDL'95: International Symposium on Digital Libraries, </booktitle> <address> Japan August 22 - 25, </address> <year> 1995. </year>
Reference-contexts: Taking advantage of multiprocessor support with client resources can lead to significantly improved user interfaces and response times. An application of the server-client load shifting is in digital image browsing. The current collections of the Alexandria Digital Library (ADL) project at UCSB <ref> [1] </ref> involve geographically-referenced materials, such as maps, satellite images, digitized aerial photographs, and associated metadata. The image size is large, from 10-100MB. In such a case, it is obviously impractical to send all the images matching a query in their entirety. <p> With current network speeds, it is quite infeasible to consider sending the full contents of an image file to users for the browsing purposes. The ADL has adopted progressive multi-resolution image delivery and subregion browsing as strategies to reduce Internet traffic when accessing map images <ref> [1] </ref>. This approach is based on the idea that users often browse large images via a thumbnail (coarse resolution), and desire to rapidly view higher-resolution versions and subregions of those images already being viewed. We briefly describe the techniques of wavelet image data retrieval and transformation for multi-resolution browsing.
Reference: [2] <author> D.Andresen, T.Yang, </author> <title> 'Adaptive Scheduling with Client Resources to Improve WWW Server Scalability', </title> <institution> Dept. of Computer Science Tech Rpt. TRCS96-27 , U.C. Santa Barbara, </institution> <year> 1996. </year>
Reference-contexts: A detailed analysis can be found in <ref> [2] </ref>. <p> As the server load increases steadily, the response time improvement ratio increases dramatically. We also note a significant increase in the number of requests per second (RPS) a server system can complete by using client resources. The detailed results are in <ref> [2] </ref>. RPS=0.1 0.5 1 1.5 2 p=5 96% 183% 191% 1838% 7893% Table 1: Response time improvement ratio with and without client resource. Load balancing with hotspots. Hotspots are a typical problem with DNS rotation, where a single server node exhibits a higher load than its peers [13, 14]. <p> Our current work is to generalize this work to support other applications on the WWW for adaptive server/client scheduling <ref> [2] </ref>. Acknowledgments This work was supported in part by funding from NSF IRI94-11330, NSF CCR-9409695, NSF CDA-9529418 and a grant from NRaD. We would like to thank Omer Egecioglu, Oscar Ibarra, Terry Smith, Cong Fu, Norbert Strubel, and the ADL image processing team for many valuable discussions and suggestions.
Reference: [3] <author> D.Andresen, T.Yang, V.Holmedahl, O.Ibarra, SWEB: </author> <title> Towards a Scalable World Wide Web Server on Multicomputers, </title> <booktitle> Proc. of 10th IEEE International Symp. on Parallel Processing (IPPS'96), </booktitle> <month> April, </month> <year> 1996, </year> <booktitle> Hawaii. </booktitle> <pages> pp. 850-856. </pages>
Reference-contexts: The main performance bottlenecks are server computing capability and Internet bandwidth. We examine these two issues under the scope of WWW-based digital library (DL) systems [12]. While multi-processor support for a server is critical for a popular WWW site <ref> [3] </ref>, transferring part of the server's workload to the client is also possible since the current Web browsers have achieved the ability to download executable content. Taking advantage of multiprocessor support with client resources can lead to significantly improved user interfaces and response times. <p> Our scheduling strategy for redirecting a request to an appropriate server node is to find a processor that minimizes the overall response time. In <ref> [3] </ref>, we have proposed server. a prediction model for approximating the processing time of a given request on a processor. In this model, we consider several factors that affect the response time. These include loads on CPU, disk, and network resources. <p> best partition is at D2, and it is advisable to send over the compressed data for the client to process. 6 Experimental Results We have implemented a prototype of our dynamic scheduling scheme with client resources on a Meiko CS-2 distributed memory parallel machine based on our previous SWEB work <ref> [3] </ref>. The Meiko CS-2 is essentially a workstation cluster with a fast network interconnect. Each node has a scalar processing unit (a 40Mhz SuperSparc chip) with 32MB of RAM running SUN Solaris 2.3. <p> The overhead of our scheduling and load monitoring is quite small for all experiments. Analyzing a request takes about the range of 2-4ms, and load monitoring takes about 0.1% of CPU resources. These results are very similar to those reported in <ref> [3] </ref>. the number of servers and RPS change. Performance improvement using server and client resources. We examine the performance of the multiprocessor server with client resources. We run a test for a period of 30 seconds, at each second R requests are launched from clients.
Reference: [4] <author> D. Andresen, T. Yang, O. Egecioglu, O.H. Ibarra, T.R. </author> <title> Smith,Scalability Issues for High Performance Digital Libraries on the World Wide Web, </title> <booktitle> Proc. of the 3rd Forum on Research and Tech. Advances in Digital Libraries (ADL96), </booktitle> <pages> pp. 139-148, </pages> <month> May, </month> <year> 1996. </year>
Reference: [5] <author> M. Arlitt, C. Williamson, </author> <title> Web Server Workload Characterization: The Search for Invariants, </title> <booktitle> Proc. SIGMETRICS Conference, </booktitle> <address> Philadelphia, PA, </address> <month> May, </month> <year> 1996. </year>
Reference-contexts: While our scheme works for processing a sequence of requests, we study the response time in simultaneously processing a fixed number of requests. Our result reflects the scheduling performance for responding to a burst of requests, which occurs frequently in many WWW sites <ref> [5, 9] </ref>. In the following analysis, we assume that the system is homogeneous in the sense that all nodes have the same CPU speed and the initial load, and each node has a local disk with the same bandwidth.
Reference: [6] <author> F. Berman, R. Wolski, S. Figueira, J. Schopf, G. Shao, </author> <title> Application-Level Scheduling on Distributed Heterogeneous Networks, </title> <booktitle> Proc. of Supercomputing '96, </booktitle> <year> 1996. </year>
Reference-contexts: We have demonstrated the effectiveness of our scheme in adapting to different client-server capabilities for minimizing response times. Shifting computation from a server to its clients essentially scatters the workload around the world. This relates the global computing and application-level scheduling projects <ref> [6, 7, 10] </ref>. Those projects deal with an integration of different machines as one virtual machine. Our experience in using bandwidth and load information for scheduling could be useful to this research.
Reference: [7] <author> H. Casanova, J. Dongarra, NetSolve: </author> <title> A network server for solving computation science problems, </title> <booktitle> Proc. of Supercomputing'96, </booktitle> <address> ACM/IEEE, </address> <month> Nov., </month> <year> 1996. </year>
Reference-contexts: We have demonstrated the effectiveness of our scheme in adapting to different client-server capabilities for minimizing response times. Shifting computation from a server to its clients essentially scatters the workload around the world. This relates the global computing and application-level scheduling projects <ref> [6, 7, 10] </ref>. Those projects deal with an integration of different machines as one virtual machine. Our experience in using bandwidth and load information for scheduling could be useful to this research.
Reference: [8] <author> E.C.K. Chui, </author> <title> Wavelets: A Tutorial in Theory and Applications, </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: We model such a process as follows. subregion (I 1 ) = Inv W (subregion (I 2 ); subregion (C 1 ); subregion (C 2 ); subregion (C 3 )): A detailed definition of forward and inverse wavelet functions can be found in <ref> [8] </ref>. The time complexity of wavelet transforms is proportional to the image size. The wavelet transform can be applied recursively, namely the thumbnail I 2 can be decomposed further to produce smaller thumbnails.
Reference: [9] <author> M. Crovella, A. Bestavros, </author> <title> Self-Similarity in World Wide Web Traffic Evidence and Possible Causes, </title> <booktitle> Proc. </booktitle> <address> SIGMETRICS96, Philadelphia, </address> <month> May, </month> <year> 1996. </year>
Reference-contexts: While our scheme works for processing a sequence of requests, we study the response time in simultaneously processing a fixed number of requests. Our result reflects the scheduling performance for responding to a burst of requests, which occurs frequently in many WWW sites <ref> [5, 9] </ref>. In the following analysis, we assume that the system is homogeneous in the sense that all nodes have the same CPU speed and the initial load, and each node has a local disk with the same bandwidth.
Reference: [10] <author> K. Dincer, and G. C. Fox, </author> <title> Building a world-wide virtual machine based on Web and HPCC technologies. </title> <booktitle> Proc. of Supercomputing'96, </booktitle> <address> ACM/IEEE, </address> <month> November, </month> <year> 1996. </year>
Reference-contexts: We have demonstrated the effectiveness of our scheme in adapting to different client-server capabilities for minimizing response times. Shifting computation from a server to its clients essentially scatters the workload around the world. This relates the global computing and application-level scheduling projects <ref> [6, 7, 10] </ref>. Those projects deal with an integration of different machines as one virtual machine. Our experience in using bandwidth and load information for scheduling could be useful to this research.
Reference: [11] <author> A. Fox, E. Brewer, </author> <title> Reducing WWW Latency and Bandwidth Requirements by Real-Time Distillation, </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> Volume 28, issues 711, </volume> <editor> p. </editor> <volume> 1445. </volume> <month> May, </month> <year> 1996. </year>
Reference-contexts: This relates the global computing and application-level scheduling projects [6, 7, 10]. Those projects deal with an integration of different machines as one virtual machine. Our experience in using bandwidth and load information for scheduling could be useful to this research. Addressing client configuration variation is discussed in <ref> [11] </ref> for filtering multi-media data in order to reduce network bandwidth requirements but does not consider the use of client resources for integrated computing. Our current work is to generalize this work to support other applications on the WWW for adaptive server/client scheduling [2].
Reference: [12] <author> E. Fox, Akscyn, R., Furuta, R. and Leggett, J. </author> <title> (Eds), </title> <journal> Special issue on digital libraries, CACM, </journal> <month> April </month> <year> 1995. </year>
Reference-contexts: The main performance bottlenecks are server computing capability and Internet bandwidth. We examine these two issues under the scope of WWW-based digital library (DL) systems <ref> [12] </ref>. While multi-processor support for a server is critical for a popular WWW site [3], transferring part of the server's workload to the client is also possible since the current Web browsers have achieved the ability to download executable content.
Reference: [13] <author> E.D. Katz, M. Butler, R. McGrath, </author> <title> A Scalable HTTP Server: the NCSA Prototype, </title> <journal> Computer Networks and ISDN Systems. </journal> <volume> vol. 27, </volume> <year> 1994, </year> <pages> pp. 155-164. </pages>
Reference-contexts: The wavelet transform can be applied recursively, namely the thumbnail I 2 can be decomposed further to produce smaller thumbnails. The actual wavelet implementation for data access and image reconstruction is a combination of Java and Computational Gateway Interface (CGI) programs <ref> [13] </ref>. Namely, an HTTP request activates a CGI program at the server, which computes the server's data using the predefined pro cedure. <p> The user requests are first evenly routed to processors via DNS rotation <ref> [13] </ref>. The server nodes in the system communicate with each other and redirect requests to the proper node by actively monitoring the usages of CPU, I/O channels, and the interconnection network. <p> RPS=0.1 0.5 1 1.5 2 p=5 96% 183% 191% 1838% 7893% Table 1: Response time improvement ratio with and without client resource. Load balancing with hotspots. Hotspots are a typical problem with DNS rotation, where a single server node exhibits a higher load than its peers <ref> [13, 14] </ref>. We examine how our system deals with hotspots. We directed a fixed number of requests to a subset of nodes in our server cluster, giving a wide range of load disparities. Without our scheduler, those nodes would have to process all of those requests.
Reference: [14] <author> D. Mosedale, W. Foss, R. McCool, </author> <title> Administering Very High Volume Internet Services, </title> <booktitle> 1995 LISA IX, </booktitle> <address> Monterey, CA, </address> <month> September, </month> <year> 1995. </year>
Reference-contexts: RPS=0.1 0.5 1 1.5 2 p=5 96% 183% 191% 1838% 7893% Table 1: Response time improvement ratio with and without client resource. Load balancing with hotspots. Hotspots are a typical problem with DNS rotation, where a single server node exhibits a higher load than its peers <ref> [13, 14] </ref>. We examine how our system deals with hotspots. We directed a fixed number of requests to a subset of nodes in our server cluster, giving a wide range of load disparities. Without our scheduler, those nodes would have to process all of those requests.
Reference: [15] <author> A. Poulakidas, A. Srinivasan, O. Egecioglu, O. Ibarra, and T. Yang, </author> <title> Experimental Studies on a Compact Storage Scheme for Wavelet-based Multires-olution Subregion Retrieval, </title> <booktitle> Proceedings of NASA 1996 Combined Industry, Space and Earth Science Data Compression Workshop, </booktitle> <address> Utah, </address> <month> April, </month> <year> 1996. </year>
Reference-contexts: The computation involved in multi-resolution image construction can be partially executed at a server and at a client also. Based on an implementation in <ref> [15] </ref>, we model the computation and communication involved using a chain of subtasks depicted in Figure 2: 1) Fetching compressed wavelet data and extracting the subregion. The wavelet image data is stored in a combined quad-tree/Huffman encoded form on a disk. These compressed files must be fetched.
References-found: 15

