URL: ftp://ftp.cs.colorado.edu/users/mozer/papers/multiscale.ps
Refering-URL: http://www.cs.colorado.edu/~mozer/papers/multiscale.html
Root-URL: http://www.cs.colorado.edu
Title: Induction of Multiscale Temporal Structure  
Author: Michael C. Mozer 
Address: Boulder, CO 80309-0430  
Affiliation: Department of Computer Science Institute of Cognitive Science University of Colorado  
Abstract: Learning structure in temporally-extended sequences is a difficult computational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to find structure in sequences, in practice they are not sufficiently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical structure that occurs locally in time|e.g., relations among notes within a musical phrase|but not structure that occurs over longer time periods|e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with different time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard Many patterns in the world are intrinsically temporal, e.g., speech, music, the unfolding of events. Recurrent neural net architectures have been devised to accommodate time-varying sequences. For example, the architecture shown in Figure 1 can map a sequence of inputs to a sequence of outputs. Learning structure in temporally-extended sequences is a difficult computational problem because the input pattern may not contain all the task-relevant information at any instant. Thus, back propagation.
Abstract-found: 1
Intro-found: 1
Reference: <author> Hinton, G. E. </author> <year> (1988). </year> <title> Representing part-whole hierarchies in connectionist networks. </title> <booktitle> Proceedings of the Eighth Annual Conference of the Cognitive Science Society. </booktitle>
Reference-contexts: 1 BUILDING A REDUCED DESCRIPTION The basic idea behind my work involves building a reduced description <ref> (Hinton, 1988) </ref> of the sequence that makes global aspects more explicit or more readily detectable. The challenge of this approach is to devise an appropriate reduced description.
Reference: <author> Jordan, M. I. </author> <year> (1987). </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 531-546). </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> McClelland, J. L. </author> <year> (1979). </year> <title> On the time relations of mental processes: An examination of systems of processes in cascade. </title> <journal> Psychological Review, </journal> <volume> 86, </volume> <pages> 287-330. </pages>
Reference: <author> Miyata, Y., & Burr, D. </author> <year> (1990). </year> <title> Hierarchical recurrent networks for learning musical structure. </title> <type> Unpublished Manuscript. </type>
Reference: <author> Mozer, M. C. </author> <year> (1989). </year> <title> A focused back-propagation algorithm for temporal pattern recognition. </title> <journal> Complex Systems, </journal> <volume> 3, </volume> <pages> 349-381. </pages>
Reference: <author> Mozer, M. C., & Soukup, T. </author> <year> (1991). </year> <title> CONCERT: A connectionist composer of erudite tunes. </title> <editor> In R. P. Lippmann, J. Moody, & D. S. Touretzky (Eds.), </editor> <booktitle> Advances in neural information processing systems 3 (pp. </booktitle> <pages> 789-796). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One might dynamically adjust t as a sequence is presented based on external criteria. In Section 5, I discuss one such criterion. 4 MUSIC COMPOSITION I have used music composition as a domain for testing and evaluating different approaches to learning multiscale temporal structure. In previous work <ref> (Mozer & Soukup, 1991) </ref>, we designed a sequential prediction network, called concert, that learns to reproduce a set of pieces of a particular musical style. concert also learns structural regularities of the musical style, and can be used to compose new pieces in the same style. concert was trained on a
Reference: <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 263-269. </pages>
Reference: <author> Pineda, F. </author> <year> (1987). </year> <title> Generalization of back propagation to recurrent neural networks. </title> <journal> Physical Review Letters, </journal> <volume> 19, </volume> <pages> 2229-2232. </pages>
Reference: <author> Rohwer, R. </author> <year> (1990). </year> <title> The 'moving targets' training algorithm. </title> <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in neural information processing systems 2 (pp. </booktitle> <pages> 558-565). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. </booktitle> <volume> Volume I: </volume> <pages> Foundations (pp. 318-362). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press/Bradford Books. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1991). </year> <title> Neural sequence chunkers (Report FKI-148-91). </title> <institution> Munich, Ger-many: Technische Universitaet Muenchen, Institut fuer Informatik. </institution>
Reference: <author> Williams, R. J., & Zipser, D. </author> <year> (1989). </year> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 270-280. </pages>
References-found: 12

