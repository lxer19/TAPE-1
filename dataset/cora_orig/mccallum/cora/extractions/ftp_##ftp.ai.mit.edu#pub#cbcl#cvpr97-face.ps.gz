URL: ftp://ftp.ai.mit.edu/pub/cbcl/cvpr97-face.ps.gz
Refering-URL: http://www.ai.mit.edu/people/eosuna/publications.html
Root-URL: 
Title: Training Support Vector Machines: an Application to Face Detection  
Author: Edgar Osuna Robert Freund Federico Girosi 
Address: Cambridge, MA, 02139, U.S.A.  
Affiliation: Center for Biological and Computational Learning and Operations Research Center Massachusetts Institute of Technology  
Web: Rico.)  
Note: (To appear in the Proceedings of CVPR'97, June 17-19, 1997, Puerto  
Abstract: We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs.) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> G. Burel and D. Carel. </author> <title> Detection and localization of faces on digital images. </title> <journal> Pattern Recognition Letters, </journal> <volume> 15 </volume> <pages> 963-967, </pages> <year> 1994. </year>
Reference-contexts: This techniques include Neural Networks <ref> [2, 9, 11] </ref>, detection of face features and use of geometrical constraints [13], density estimation of the training data [6], labeled graphs [5] and clustering and distribution-based modeling [10].
Reference: [3] <author> C.J.C. Burges. </author> <title> Simplified support vector decision rules. </title> <booktitle> In International Conference on Machine Learning, </booktitle> <pages> pages 71-77. </pages> <year> 1996. </year>
Reference-contexts: In this paper we concentrate on the Support Vector Machine (SVM), a pattern classification algorithm recently developed by V. Vapnik and his team at AT&T Bell Labs. <ref> [1, 3, 4, 12] </ref>. SVM can be seen as a new way to train polynomial, neural network, or Radial Basis Functions classifiers. <p> Table 2 shows a comparison between the 2 systems. At run-time the SVM system is approximately 30 times faster than the system of Sung and Poggio. One reason for that is the use of a technique introduced by C. Burges <ref> [3] </ref> that allows to replace a large numbers of support vectors with a much smaller number of points (which are not necessarily data points), and therefore to speed up the run time considerably. In figure 5 we report the result of our system on some test images.
Reference: [4] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 1-25, </pages> <year> 1995. </year>
Reference-contexts: In this paper we concentrate on the Support Vector Machine (SVM), a pattern classification algorithm recently developed by V. Vapnik and his team at AT&T Bell Labs. <ref> [1, 3, 4, 12] </ref>. SVM can be seen as a new way to train polynomial, neural network, or Radial Basis Functions classifiers. <p> A more detailed description of SVM can be found in [12] (chapter 5) and <ref> [4] </ref>. We start from the simple case of two linearly separable classes. <p> Case: 0 &lt; i &lt; C From the first three equations of the KT condi tions we have: (Dfl) i 1 + y i = 0 (4) Using the results in <ref> [4] </ref> and [12] one can easily show that when is strictly between 0 and C the following equality holds: y i ( j=1 Noticing that (Dfl) i = j=1 ` X j y j K (x i ; x j ) and combining this expression with (5) and (4) we immediately
Reference: [5] <author> N. Kruger, M. Potzsch, and C. v.d. Malsburg. </author> <title> Determination of face position and pose with learned representation based on labled graphs. </title> <type> Technical Report 96-03, </type> <institution> Ruhr-Universitat, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: This techniques include Neural Networks [2, 9, 11], detection of face features and use of geometrical constraints [13], density estimation of the training data [6], labeled graphs <ref> [5] </ref> and clustering and distribution-based modeling [10]. Out of all these previous works, the results of Sung and Poggio [10], and Rowley et al. [9] reflect systems with very high detection rates and low false positive rates.
Reference: [6] <author> B. Moghaddam and A. Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <type> Technical Report 326, </type> <institution> MIT Media Laboratory, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: This techniques include Neural Networks [2, 9, 11], detection of face features and use of geometrical constraints [13], density estimation of the training data <ref> [6] </ref>, labeled graphs [5] and clustering and distribution-based modeling [10]. Out of all these previous works, the results of Sung and Poggio [10], and Rowley et al. [9] reflect systems with very high detection rates and low false positive rates.
Reference: [7] <author> B. Murtagh and M. Saunders. </author> <title> Large-scale linearly constrained optimization. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 41-72, </pages> <year> 1978. </year>
Reference-contexts: Notice that the size of the margin has decreased, and the shape of the decision surface has changed. 2.4 Implementation and Results We have implemented the decomposition algorithm using MINOS 5.4 as the solver of the sub-problems. For information on MINOS 5.4 see <ref> [7] </ref>. The computational results that we present in this section have been obtained using real data from our Face Detection System, which is described in Section 3.
Reference: [8] <author> F. Riesz and B. Sz.-Nagy. </author> <title> Functional Analysis. </title> <publisher> Ungar, </publisher> <address> New York, </address> <year> 1955. </year>
Reference-contexts: With this choice the scalar product in the feature space becomes particularly simple because: z T (x)z (y) = i=1 where the last equality comes from the Mercer-Hilbert-Schmidt theorem for positive definite func tions (see <ref> [8] </ref>, pp. 242-246). The QP problem that has to be solved now is exactly the same as in eq. (1), with the exception that the matrix D has now elements D ij = y i y j K (x i ; x j ).
Reference: [9] <author> H. Rowley, S. Baluja, and T. Kanade. </author> <title> Human face detection in visual scenes. </title> <type> Technical Report CMU-CS-95-158R, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: This techniques include Neural Networks <ref> [2, 9, 11] </ref>, detection of face features and use of geometrical constraints [13], density estimation of the training data [6], labeled graphs [5] and clustering and distribution-based modeling [10]. <p> Out of all these previous works, the results of Sung and Poggio [10], and Rowley et al. <ref> [9] </ref> reflect systems with very high detection rates and low false positive rates. Sung and Poggio use clustering and distance met-rics to model the distribution of the face and non-face manifold, and a Neural Network to classify a new pattern given the measurements.
Reference: [10] <author> K. Sung and T. Poggio. </author> <title> Example-based Learning for View-based Human Face Detection. A.I. </title> <type> Memo 1521, </type> <institution> MIT A.I. Lab., </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: This techniques include Neural Networks [2, 9, 11], detection of face features and use of geometrical constraints [13], density estimation of the training data [6], labeled graphs [5] and clustering and distribution-based modeling <ref> [10] </ref>. Out of all these previous works, the results of Sung and Poggio [10], and Rowley et al. [9] reflect systems with very high detection rates and low false positive rates. <p> This techniques include Neural Networks [2, 9, 11], detection of face features and use of geometrical constraints [13], density estimation of the training data [6], labeled graphs [5] and clustering and distribution-based modeling <ref> [10] </ref>. Out of all these previous works, the results of Sung and Poggio [10], and Rowley et al. [9] reflect systems with very high detection rates and low false positive rates. Sung and Poggio use clustering and distance met-rics to model the distribution of the face and non-face manifold, and a Neural Network to classify a new pattern given the measurements. <p> Images of landscapes, trees, buildings, rocks, etc., are good sources of false positives due to the many different textured patterns they contain. This bootstrapping step, which was successfully used by Sung and Poggio <ref> [10] </ref> is very important in the context of a face detector that learns from examples because: * Although negative examples are abundant, negative examples that are useful from a learning point of view are very difficult to characterize and define. * The two classes, face and non-face are not equally complex <p> After training the SVM, we incorporate it as the classifier in a run-time system very similar to the one used by Sung and Poggio <ref> [10] </ref> that performs the following operations: * Re-scale the input image several times. * Cut 19fi19 window patterns out of the scaled image. * Preprocess the window using masking, light correction and histogram equalization. * Classify the pattern using the SVM. * If the pattern is a face, draw a rectangle <p> The set A, contained 313 high-quality images with one face per image. The set B, contained 23 images of mixed quality, with a total of 155 faces. Both sets were tested using our system and the one by Sung and Poggio <ref> [10] </ref>. In order to give true meaning to the number of false positives obtained, it is important to state that set A involved 4,669,960 pattern windows, while set B 5,383,682. Table 2 shows a comparison between the 2 systems.
Reference: [11] <author> R. Vaillant, C. Monrocq, and Y. Le Cun. </author> <title> Original approach for the localisation of objects in images. </title> <booktitle> IEEE Proc. Vis. Image Signal Process., </booktitle> <volume> 141(4), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: This techniques include Neural Networks <ref> [2, 9, 11] </ref>, detection of face features and use of geometrical constraints [13], density estimation of the training data [6], labeled graphs [5] and clustering and distribution-based modeling [10].
Reference: [12] <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: In this paper we concentrate on the Support Vector Machine (SVM), a pattern classification algorithm recently developed by V. Vapnik and his team at AT&T Bell Labs. <ref> [1, 3, 4, 12] </ref>. SVM can be seen as a new way to train polynomial, neural network, or Radial Basis Functions classifiers. <p> In section 3 we present our application to the face detection problem, and in section 4 we summarize our results. 1.1 Support Vector Machines In this section we briefly sketch the SVM algorithm and its motivation. A more detailed description of SVM can be found in <ref> [12] </ref> (chapter 5) and [4]. We start from the simple case of two linearly separable classes. <p> Case: 0 &lt; i &lt; C From the first three equations of the KT condi tions we have: (Dfl) i 1 + y i = 0 (4) Using the results in [4] and <ref> [12] </ref> one can easily show that when is strictly between 0 and C the following equality holds: y i ( j=1 Noticing that (Dfl) i = j=1 ` X j y j K (x i ; x j ) and combining this expression with (5) and (4) we immediately obtain that
Reference: [13] <author> G. Yang and T. Huang. </author> <title> Human face detection in a complex background. </title> <journal> Pattern Recognition, </journal> <volume> 27 </volume> <pages> 53-63, </pages> <year> 1994. </year>
Reference-contexts: This techniques include Neural Networks [2, 9, 11], detection of face features and use of geometrical constraints <ref> [13] </ref>, density estimation of the training data [6], labeled graphs [5] and clustering and distribution-based modeling [10]. Out of all these previous works, the results of Sung and Poggio [10], and Rowley et al. [9] reflect systems with very high detection rates and low false positive rates.
References-found: 12

