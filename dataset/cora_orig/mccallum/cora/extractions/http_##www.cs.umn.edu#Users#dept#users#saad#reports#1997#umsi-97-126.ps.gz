URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1997/umsi-97-126.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1997/
Root-URL: http://www.cs.umn.edu
Title: BILUM: Block Versions of Multi-Elimination and Multi-Level ILU Preconditioner for General Sparse Linear Systems  
Author: Yousef Saad and Jun Zhang 
Keyword: Reynolds number independent and near grid independent. Key words: Incomplete LU factorization, ILUM, multi-level preconditioner, GMRES, multi-elimination incomplete LU factorization.  
Note: AMS subject classifications: 65F10, 65N06.  
Date: August 27, 1997  
Address: 200 Union Street S.E., Minneapolis, MN 55455  
Affiliation: Department of Computer Science and Engineering, University of Minnesota,  
Abstract: We introduce block versions of the multi-elimination incomplete LU (ILUM) factorization preconditioning technique for solving general sparse unstructured linear systems. These preconditioners have a multi-level structure and exhibit properties that are typically enjoyed by multigrid methods. Several heuristic strategies for forming blocks of independent set are introduced and their relative merits are discussed. Advantages of block ILUM over point ILUM include increased robustness and efficiency. We compare several versions of the block ILUM, point ILUM and the dual-threshold-based ILUT preconditioners. In particular, the ILUM preconditioned Krylov subspace solver is tested for some convection-diffusion problems to show convergence that is near 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. C. Anderson and Y. Saad, </author> <title> Solving sparse triangular systems on parallel computers, </title> <journal> Internat. J. High Speed Comput. </journal> <volume> 1, </volume> <month> 73-96 </month> <year> (1989). </year>
Reference-contexts: The implementation of ILU (0) and other ILU preconditioners on high performance computers can be optimized by a technique called "level scheduling" or "wavefront ordering" <ref> [1] </ref>. However, the parallelism that can be extracted by level-scheduling techniques is limited. The more accurate ILU factorizations are generally more robust than ILU (0) but their level of parallelism is generally worse.
Reference: [2] <author> O. Axelsson and B. Polman, </author> <title> On approximate factorization methods for block matrices suitable for vector and parallel processors, </title> <journal> Linear Algebra Appl. </journal> <volume> 77, </volume> <month> 867-889 </month> <year> (1989). </year>
Reference: [3] <author> O. Axelsson, V. Eijkout, B. Polman and P. Vassilevski, </author> <title> Incomplete block-matrix factorization iterative methods for convection-diffusion problems, </title> <journal> BIT, </journal> <volume> 29, </volume> <month> 867-889 </month> <year> (1989). </year>
Reference: [4] <author> E. F. F. Botta, A. van der Ploeg and F. W. Wubs, </author> <title> A fast linear-system solver for large unstructured problems on a shared-memory computer, in Proceedings of the Conference on Algebraic Multilevel Methods with Applications, </title> <editor> O. Axelsson and B. Polman, </editor> <booktitle> eds., </booktitle> <pages> pp. 105-116, </pages> <year> 1996. </year>
Reference-contexts: One reason for studying this type of preconditioners is that they have a far better parallelism than the traditional incomplete LU factorization type preconditioners. Similar preconditioners have been designed and tested to show the property of near grid-independent convergence in <ref> [4] </ref>. There are two main reasons to seek block generalizations of ILUM. First, the diagonal elements in D may be close to zero at each reduction step. The resulting reduced systems may be poor and this may lead to inaccurate LU factors. <p> This approximate solution is usually too crude to approximate the solution of Eq. (1), but it may instead be used to precondition the original linear system. For detailed descriptions, see [34]. Our preconditioner is constructed differently from that of <ref> [4, 5] </ref>. Specifically, we construct D j (and D 1 j ) and F j exactly whenever this is possible, since these matrices do not have any fill-in (except for D 1 j ) during the reduction process.
Reference: [5] <author> E. F. F. Botta and W. Wubs, MRILU: </author> <title> it's the preconditioning that counts, </title> <type> Technical Report W-9703, </type> <institution> Department of Mathematics, University of Groningen, </institution> <address> The Netherlands, </address> <year> 1997. </year>
Reference-contexts: Such techniques were incorporated in ILU-type preconditioners and tested by Botta and Wubs <ref> [5] </ref>. In their approach for solving convection-diffusion equations, they select block pivots (defined from pairs of indices) on the basis of the dominant flow direction (similar to the well-known line iteration methods). These pairs are maintained during the whole process. <p> This approximate solution is usually too crude to approximate the solution of Eq. (1), but it may instead be used to precondition the original linear system. For detailed descriptions, see [34]. Our preconditioner is constructed differently from that of <ref> [4, 5] </ref>. Specifically, we construct D j (and D 1 j ) and F j exactly whenever this is possible, since these matrices do not have any fill-in (except for D 1 j ) during the reduction process.
Reference: [6] <author> J. R. Bunch and B. N. Parlett, </author> <title> Direct methods for solving symmetric indefinite systems of linear equations, </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 8, </volume> <month> 639-655 </month> <year> (1971). </year>
Reference-contexts: The idea of using pre-selected blocks to deal with convection-diffusion problems, such as the line relaxation methods, has been known for many years. 2 The use of pre-selected small blocks for diagonal pivoting method was introduced in <ref> [6] </ref> and the definition of a pivot was extended to 2 fi 2 blocks when choosing a 1 fi 1 pivot on the diagonal is no longer stable due to large off-diagonal elements. Such techniques were incorporated in ILU-type preconditioners and tested by Botta and Wubs [5].
Reference: [7] <author> A. Chapman, Y. Saad, and L. Wigton, </author> <title> High-order ILU preconditioners for CFD problems, </title> <type> Technical Report UMSI 96/14, </type> <institution> Minnesota Supercomputer Institute, </institution> <year> 1996. </year>
Reference-contexts: If explicit inversion is employed, it is best to utilize a thresholded pseudo-inverse employing a singular value decomposition as is often done in block ILU factorizations, for details, see <ref> [7, 8] </ref>. The above reduction process can be continued recursively with the matrix A j being replaced by A j+1 , until we reach a Schur complement matrix A nlev which is small enough to be solved by a direct or a preconditioned iterative method.
Reference: [8] <author> E. Chow and Y. Saad, </author> <title> Experimental study of ILU preconditioners for indefinite matrices, </title> <journal> J. Comput. Appl. Math. </journal> <note> (to appear). </note>
Reference-contexts: If explicit inversion is employed, it is best to utilize a thresholded pseudo-inverse employing a singular value decomposition as is often done in block ILU factorizations, for details, see <ref> [7, 8] </ref>. The above reduction process can be continued recursively with the matrix A j being replaced by A j+1 , until we reach a Schur complement matrix A nlev which is small enough to be solved by a direct or a preconditioned iterative method.
Reference: [9] <author> P. Concus, G. H. Golub and G. Meurant, </author> <title> Block preconditioning for the conjugate gradient method, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 6, </volume> <month> 187-209 </month> <year> (1983). </year> <month> 24 </month>
Reference: [10] <author> E. F. D'Azevedo, F. A. Forsyth and W. P. Tang, </author> <title> Ordering methods for preconditioned conjugate gradient methods applied to unstructured grid problems, </title> <note> SIAM J. Matrix Anal. Appl. 13 944-961 (1992). </note>
Reference-contexts: To improve the efficiency and robustness of ILU factorizations, many alternatives which allow higher amounts of fill-in have been developed <ref> [10, 11, 21, 27, 37, 41] </ref>. The implementation of ILU (0) and other ILU preconditioners on high performance computers can be optimized by a technique called "level scheduling" or "wavefront ordering" [1]. However, the parallelism that can be extracted by level-scheduling techniques is limited.
Reference: [11] <author> E. F. D'Azevedo, F. A. Forsyth and W. P. Tang, </author> <title> Towards a cost effective ILU pre-conditioner with high level fill, </title> <type> BIT 31, </type> <month> 442-463 </month> <year> (1992). </year>
Reference-contexts: To improve the efficiency and robustness of ILU factorizations, many alternatives which allow higher amounts of fill-in have been developed <ref> [10, 11, 21, 27, 37, 41] </ref>. The implementation of ILU (0) and other ILU preconditioners on high performance computers can be optimized by a technique called "level scheduling" or "wavefront ordering" [1]. However, the parallelism that can be extracted by level-scheduling techniques is limited.
Reference: [12] <author> P. M. de Zeeuw, </author> <title> Matrix-dependent prolongations and restrictions in a blackbox multi-grid solver, </title> <journal> J. Comput. Appl. Math. </journal> <volume> 3, </volume> <month> 1-27 </month> <year> (1990). </year>
Reference-contexts: High Reynolds number flow problems are particularly hard to solve with a standard multigrid approach [23, 40]. Techniques based on algebraic multigrid or matrix-dependent grid transfer operators have been used for systems arising from upwind-type discretization schemes <ref> [12, 28] </ref>. There is strong interest in the multigrid community to develop multi-level algorithms which converge independently of the Reynolds number. <p> Our tests with convection-diffusion problems showed convergence that is nearly independent of the Reynolds number. 23 Convergence was also found to be nearly independent of the mesh-size of the underlying system. The results compared favorably with similar problems tested with the geometric or matrix-dependent multigrid methods <ref> [12, 28, 39] </ref>. Our algorithms, however, are purely algebraic and do not make any assumptions on properties of the linear system.
Reference: [13] <author> I. S. Duff and J. Reid, </author> <title> The multifrontal solution of unsymmetric sets of linear equations, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 5, </volume> <month> 633-641 </month> <year> (1984). </year>
Reference-contexts: This technique called ILU with Multi-elimination (ILUM) is somewhat related to multifrontal elimination, a standard method used in the parallel solution of sparse linear systems by direct methods <ref> [13, 14] </ref>. Block versions of ILUM preconditioner are attractive because block ILU precondi-tioners usually perform better than their point counterparts. In particular, the point ILUM factorization may have difficulties when the diagonal elements of the resulting U factor are small.
Reference: [14] <author> I. S. Duff, A. M. Erisman and J. K. Reid, </author> <title> Direct Methods for Sparse Matrices, </title> <publisher> Claren-don Press, Oxford, </publisher> <address> UK, </address> <year> 1986. </year>
Reference-contexts: This technique called ILU with Multi-elimination (ILUM) is somewhat related to multifrontal elimination, a standard method used in the parallel solution of sparse linear systems by direct methods <ref> [13, 14] </ref>. Block versions of ILUM preconditioner are attractive because block ILU precondi-tioners usually perform better than their point counterparts. In particular, the point ILUM factorization may have difficulties when the diagonal elements of the resulting U factor are small.
Reference: [15] <author> I. S. Duff, R. G. Grimes and J. G. Lewis, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. Math. </journal> <volume> Software 15, </volume> <month> 1-14 </month> <year> (1989). </year>
Reference-contexts: In contrast with the test results obtained for the 5-point matrices, BILUM (2) was eventually faster than ILUM (1). This suggests that matrices with denser couplings may be better solved with BILUM with larger blocks. Fig. 6 shows a more dramatic comparison with the Harwell-Boeing matrix SAYLR4 <ref> [15, 17] </ref>. In this case, ILUM (1) and BILUM (2) were much more faster than ILUT both in terms of iteration counts and CPU times. The performance of the two ILUM algorithms were comparable and BILUM (2) took only slightly fewer iterations than ILUM (1) did. <p> GMRES (10) was used for the outer iterations. 6.2 Matrices from the Harwell-Boeing Collection We compare the performance of different BILUM preconditioners with 5 matrices from the Harwell-Boeing collection <ref> [15, 17] </ref>, together with our 5-POINT and 9-POINT matrices. (4 out of the 5 Harwell-Boeing matrices were tested with ILUM in [34] using different independent set ordering strategy.) We used t = 10 3 and p = 10 for the reduction process and the last reduced system.
Reference: [16] <author> I.S. Duff and G. A. Meurant, </author> <title> The effect of reordering on preconditioned conjugate gradients, </title> <type> BIT 29, </type> <month> 635-657 </month> <year> (1989). </year>
Reference-contexts: One notable drawback of this approach is that the efficiency of the preconditioning deteriorates and as a result the number of iterations to achieve convergence may increase substantially, when compared with that required for the original system <ref> [16, 18, 19] </ref>. Recently, a technique based on exploiting the idea of successive independent sets (a simple form of multicoloring) [25] has been used to develop preconditioners that are akin to multi-level preconditioners [33, 34].
Reference: [17] <author> I. S. Duff, R. G. Grimes and J. G. Lewis, </author> <title> User's Guide for the Harwell-Boeing Sparse Matrix Collection, </title> <type> Technical Report TR/PA/92/86, </type> <institution> CERFACS, Toulouse, France, </institution> <year> 1992. </year>
Reference-contexts: In contrast with the test results obtained for the 5-point matrices, BILUM (2) was eventually faster than ILUM (1). This suggests that matrices with denser couplings may be better solved with BILUM with larger blocks. Fig. 6 shows a more dramatic comparison with the Harwell-Boeing matrix SAYLR4 <ref> [15, 17] </ref>. In this case, ILUM (1) and BILUM (2) were much more faster than ILUT both in terms of iteration counts and CPU times. The performance of the two ILUM algorithms were comparable and BILUM (2) took only slightly fewer iterations than ILUM (1) did. <p> GMRES (10) was used for the outer iterations. 6.2 Matrices from the Harwell-Boeing Collection We compare the performance of different BILUM preconditioners with 5 matrices from the Harwell-Boeing collection <ref> [15, 17] </ref>, together with our 5-POINT and 9-POINT matrices. (4 out of the 5 Harwell-Boeing matrices were tested with ILUM in [34] using different independent set ordering strategy.) We used t = 10 3 and p = 10 for the reduction process and the last reduced system.
Reference: [18] <author> L. C. Dutto, </author> <title> The effect of reordering on the preconditioned GMRES algorithm for solving the compressible Navier-Stokes equations, </title> <journal> Internat. J. Numer. Methods Engrg. </journal> <volume> 36, </volume> <month> 457-497 </month> <year> (1993). </year>
Reference-contexts: One notable drawback of this approach is that the efficiency of the preconditioning deteriorates and as a result the number of iterations to achieve convergence may increase substantially, when compared with that required for the original system <ref> [16, 18, 19] </ref>. Recently, a technique based on exploiting the idea of successive independent sets (a simple form of multicoloring) [25] has been used to develop preconditioners that are akin to multi-level preconditioners [33, 34].
Reference: [19] <author> H. C. Elman and E. Agron, </author> <title> Ordering techniques for the preconditioned conjugate gradient method on parallel computers, </title> <journal> Comput. Phys. Comm. </journal> <volume> 53, </volume> <month> 253-269 </month> <year> (1989). </year>
Reference-contexts: One notable drawback of this approach is that the efficiency of the preconditioning deteriorates and as a result the number of iterations to achieve convergence may increase substantially, when compared with that required for the original system <ref> [16, 18, 19] </ref>. Recently, a technique based on exploiting the idea of successive independent sets (a simple form of multicoloring) [25] has been used to develop preconditioners that are akin to multi-level preconditioners [33, 34].
Reference: [20] <author> J. A. George and J. W. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N. J., </address> <year> 1981. </year>
Reference-contexts: 6= ; of the vertex set V which are disjoint exclusive, i.e., such that B j " B i = ;; if j 6= i: Recall that a quotient graph, is a graph whose vertices are the subsets B i ; i = 1; : : : ; m, see <ref> [20, p. 105] </ref>. <p> The set of these vertices is usually denoted by adj (B) <ref> [20] </ref> and therefore, pdeg (B) = jadj (B)j The main parameter which determines the size of V S is the ratio pdeg (B)=jBj. As the blocks become smaller these ratios become smaller, provided the aspect ratio of each subgraph remains moderate.
Reference: [21] <author> K. Gallivan, A. Sameh and Z. Zlatev, </author> <title> A parallel hybrid sparse linear system solver, </title> <journal> Comput. Systems Engrg. </journal> <volume> 1, </volume> <month> 183-195 </month> <year> (1990). </year>
Reference-contexts: To improve the efficiency and robustness of ILU factorizations, many alternatives which allow higher amounts of fill-in have been developed <ref> [10, 11, 21, 27, 37, 41] </ref>. The implementation of ILU (0) and other ILU preconditioners on high performance computers can be optimized by a technique called "level scheduling" or "wavefront ordering" [1]. However, the parallelism that can be extracted by level-scheduling techniques is limited.
Reference: [22] <author> M. M. Gupta, R. P. Manohar and J. W. Stephenson, </author> <title> A single cell high order scheme for the convection-diffusion equation with variable coefficients, </title> <journal> Internat. J. Numer. </journal> <note> Methods Fluids 4, </note> <month> 641-651 </month> <year> (1984). </year>
Reference-contexts: Dirichlet boundary values were removed and artificial left-hand side was used. Here Re roughly reflects the Reynolds number. Eq. (10) is frequently encountered in computational fluid dynamics to model transport problems. Eq. (10) is discretized by the standard 5-point upwind finite difference scheme and the fourth-order 9-point compact scheme <ref> [22] </ref> with various (uniform) meshsize h and Re. The resulting matrices are conveniently called 5-point or 9-point matrices. In particular, we will refer to a 5-POINT matrix 12 discretized by the 5-point upwind scheme using Re = 1 and h = 1=201.
Reference: [23] <author> M. M. Gupta, J. Kouatchou and J. Zhang, </author> <title> A compact multigrid solver for convection-diffusion equations, </title> <journal> J. Comput. Phys. </journal> <volume> 132, </volume> <month> 123-129 </month> <year> (1997). </year>
Reference-contexts: Consequently, a 9-point matrix is generally more difficult to solve by iterative methods than a 5-point counterpart with the same discretization parameters. High Reynolds number flow problems are particularly hard to solve with a standard multigrid approach <ref> [23, 40] </ref>. Techniques based on algebraic multigrid or matrix-dependent grid transfer operators have been used for systems arising from upwind-type discretization schemes [12, 28]. There is strong interest in the multigrid community to develop multi-level algorithms which converge independently of the Reynolds number.
Reference: [24] <author> O. G. Johnson, C. A. Micchelli and G. Paul, </author> <title> Polynomial preconditionings for conjugate gradient calculations, </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 20, </volume> <month> 362-376 </month> <year> (1983). </year> <month> 25 </month>
Reference-contexts: Different alternatives have been considered in the past to improve the degree of parallelism, see for example [35, 36, 30] for references. A particularly interesting group of methods in this category introduce parallelism by exploiting "graph coloring," or multicoloring <ref> [24, 33] </ref>. The unknowns of the problem are colored in such a way that no two unknowns of the same color are coupled by an equation.
Reference: [25] <author> R. Leuze, </author> <title> Independent set orderings for parallel matrix factorizations by Gaussian elimination, </title> <journal> Parallel Comput. </journal> <volume> 10, </volume> <month> 177-191 </month> <year> (1989). </year>
Reference-contexts: Recently, a technique based on exploiting the idea of successive independent sets (a simple form of multicoloring) <ref> [25] </ref> has been used to develop preconditioners that are akin to multi-level preconditioners [33, 34]. This technique called ILU with Multi-elimination (ILUM) is somewhat related to multifrontal elimination, a standard method used in the parallel solution of sparse linear systems by direct methods [13, 14].
Reference: [26] <author> J. A. Meijerink and H. A. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M -matrix, </title> <journal> Math. Comp. </journal> <volume> 31, </volume> <month> 148-162 </month> <year> (1977). </year>
Reference-contexts: It is generally recognized that the key to solving a general sparse linear system efficiently by iterative methods is a high quality preconditioner. With a suitable preconditioner, the accelerator plays a secondary role. The incomplete LU (ILU) factorization with no fill-in, or ILU (0) <ref> [26] </ref>, is one of the best known preconditioners. A disadvantage of ILU (0) is that it is a rather crude approximation of A and therefore it is unreliable when used to solve large problems arising from certain applications, such as computational fluid dynamics.
Reference: [27] <author> O. Orterby and Z. Zlatev, </author> <title> Direct Methods for Sparse Matrices, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: To improve the efficiency and robustness of ILU factorizations, many alternatives which allow higher amounts of fill-in have been developed <ref> [10, 11, 21, 27, 37, 41] </ref>. The implementation of ILU (0) and other ILU preconditioners on high performance computers can be optimized by a technique called "level scheduling" or "wavefront ordering" [1]. However, the parallelism that can be extracted by level-scheduling techniques is limited.
Reference: [28] <author> A. Reusken, </author> <title> Fourier analysis of a robust multigrid method for convection-diffusion equation, </title> <journal> Numer. Math. </journal> <volume> 71, </volume> <month> 365-398 </month> <year> (1995). </year>
Reference-contexts: High Reynolds number flow problems are particularly hard to solve with a standard multigrid approach [23, 40]. Techniques based on algebraic multigrid or matrix-dependent grid transfer operators have been used for systems arising from upwind-type discretization schemes <ref> [12, 28] </ref>. There is strong interest in the multigrid community to develop multi-level algorithms which converge independently of the Reynolds number. <p> Our tests with convection-diffusion problems showed convergence that is nearly independent of the Reynolds number. 23 Convergence was also found to be nearly independent of the mesh-size of the underlying system. The results compared favorably with similar problems tested with the geometric or matrix-dependent multigrid methods <ref> [12, 28, 39] </ref>. Our algorithms, however, are purely algebraic and do not make any assumptions on properties of the linear system.
Reference: [29] <author> Y. Saad and M. H. Schultz, </author> <title> GMRES: a generalized minimal residual method for solving nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 7, </volume> <month> 856-869 </month> <year> (1986). </year>
Reference: [30] <author> Y. Saad, </author> <title> Krylov subspace methods on supercomputers, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 10, </volume> <month> 1200-1232 </month> <year> (1989). </year>
Reference-contexts: However, the parallelism that can be extracted by level-scheduling techniques is limited. The more accurate ILU factorizations are generally more robust than ILU (0) but their level of parallelism is generally worse. Different alternatives have been considered in the past to improve the degree of parallelism, see for example <ref> [35, 36, 30] </ref> for references. A particularly interesting group of methods in this category introduce parallelism by exploiting "graph coloring," or multicoloring [24, 33]. The unknowns of the problem are colored in such a way that no two unknowns of the same color are coupled by an equation.
Reference: [31] <author> Y. Saad, </author> <title> A flexible inner-outer preconditioned GMRES algorithm, </title> <journal> SIAM J. Sci. Com-put. </journal> <volume> 14, </volume> <month> 461-469 </month> <year> (1993). </year>
Reference: [32] <author> Y. Saad, ILUT: </author> <title> a dual threshold incomplete ILU preconditioner, </title> <journal> Numer. Linear Algebra Appl. </journal> <volume> 1, </volume> <month> 387-402 </month> <year> (1994). </year>
Reference-contexts: In all cases, the last reduced system was solved approximately by a GMRES (10) algorithm preconditioned by a dual threshold ILUT (p,t ) preconditioner <ref> [32] </ref>, which will be referred to as the inner iteration. The inner iteration was terminated when either the number of iterations exceeded 10 or the (inner iteration) residual in 2-norm was reduced by a factor of 10 2 , whichever condition is satisfied first.
Reference: [33] <author> Y. Saad, </author> <title> Highly parallel preconditioners for general sparse matrices, In Recent Advances in Iterative Methods, IMA Volumes in Mathematics and Its Applications, </title> <editor> G. Golub, M. Luskin, and A. Greenbaum, eds., </editor> <volume> Vol. 60, </volume> <publisher> Springer Verlag, </publisher> <address> New York, </address> <pages> pp. 165-199, </pages> <year> 1994. </year>
Reference-contexts: Different alternatives have been considered in the past to improve the degree of parallelism, see for example [35, 36, 30] for references. A particularly interesting group of methods in this category introduce parallelism by exploiting "graph coloring," or multicoloring <ref> [24, 33] </ref>. The unknowns of the problem are colored in such a way that no two unknowns of the same color are coupled by an equation. <p> Recently, a technique based on exploiting the idea of successive independent sets (a simple form of multicoloring) [25] has been used to develop preconditioners that are akin to multi-level preconditioners <ref> [33, 34] </ref>. This technique called ILU with Multi-elimination (ILUM) is somewhat related to multifrontal elimination, a standard method used in the parallel solution of sparse linear systems by direct methods [13, 14]. Block versions of ILUM preconditioner are attractive because block ILU precondi-tioners usually perform better than their point counterparts.
Reference: [34] <author> Y. Saad, ILUM: </author> <title> a multi-elimination ILU preconditioner for general sparse matrices, </title> <journal> SIAM J. Sci. Comput. </journal> <volume> 17, </volume> <month> 830-847 </month> <year> (1996). </year>
Reference-contexts: Recently, a technique based on exploiting the idea of successive independent sets (a simple form of multicoloring) [25] has been used to develop preconditioners that are akin to multi-level preconditioners <ref> [33, 34] </ref>. This technique called ILU with Multi-elimination (ILUM) is somewhat related to multifrontal elimination, a standard method used in the parallel solution of sparse linear systems by direct methods [13, 14]. Block versions of ILUM preconditioner are attractive because block ILU precondi-tioners usually perform better than their point counterparts. <p> BILUM is aimed at alleviating the above shortcomings of ILUM. Block versions of ILUM have already been suggested (but not implemented) along with point ILUM in <ref> [34] </ref>. <p> The process may be combined with a dropping strategy to reduce fill-in and it is repeated a few times until the final reduced system is easily solvable. In <ref> [34] </ref>, several heuristic algorithms have been suggested to find Maximal Independent Sets. <p> The blocking strategies referred to in this paper are related to this graph, or mesh, and are therefore in addition to the original blocking that may be naturally available. Heuristic algorithms for finding point independent sets have been discussed in <ref> [34] </ref> and been successfully utilized with ILUM to develop parallelizable preconditioners for solving general sparse matrices. Based on the strategies of [34], we introduce some heuristic algorithms for finding block independent set from a given matrix. <p> Heuristic algorithms for finding point independent sets have been discussed in <ref> [34] </ref> and been successfully utilized with ILUM to develop parallelizable preconditioners for solving general sparse matrices. Based on the strategies of [34], we introduce some heuristic algorithms for finding block independent set from a given matrix. <p> If ~ is the maximum degree of all the vertices that constitute S 1 , then a lower bound for the size of S 1 is given by <ref> [34] </ref> jS 1 j 1 + ~ This suggests that it may be a good idea to visit the nodes with the smallest degrees first [34] to form independent sets of size 1. <p> the maximum degree of all the vertices that constitute S 1 , then a lower bound for the size of S 1 is given by <ref> [34] </ref> jS 1 j 1 + ~ This suggests that it may be a good idea to visit the nodes with the smallest degrees first [34] to form independent sets of size 1. This observation also suggests pairing the current node with the vertex among its nearest neighbors which has the smallest degree, to form independent sets of size 2. Algorithm 3.3 Minimal degree algorithm for blocking elements. 1. Set m = 0. 3. <p> is best to add these singletons to the set of independent subsets, listing them last. (In the experiments reported in this paper this strategy was not implemented.) 4 Block ILUM Factorization The main lines of a block version of ILUM are similar to those of the scalar version developed in <ref> [34] </ref>. Here we only recall the main steps. <p> A general practice for avoiding this undesirable result in developing preconditioners is to neglect some of the fill-ins created by using a simple dropping strategies as we form the reduced system <ref> [34, 35] </ref>. For example, we may drop any fill-in element created when ever its absolute value is less than a given tolerance t times the average of the absolute 7 values of the row in question. <p> This approximate solution is usually too crude to approximate the solution of Eq. (1), but it may instead be used to precondition the original linear system. For detailed descriptions, see <ref> [34] </ref>. Our preconditioner is constructed differently from that of [4, 5]. Specifically, we construct D j (and D 1 j ) and F j exactly whenever this is possible, since these matrices do not have any fill-in (except for D 1 j ) during the reduction process. <p> Fig. 1 is an illustration of the structure of the processed matrix after two levels of reduction with 4 blocks of size 2 at each level. The forward-backward solution process is similar to the one described in <ref> [34] </ref> with the diagonal matrix D j being replaced by the block diagonal matrix. In our current imple mentation, we stored the permutation matrices for each level and did the permutation and inverse permutation explicitly at each preconditioning step. A slightly different approach was used in [34] for implementing ILUM, where <p> the one described in <ref> [34] </ref> with the diagonal matrix D j being replaced by the block diagonal matrix. In our current imple mentation, we stored the permutation matrices for each level and did the permutation and inverse permutation explicitly at each preconditioning step. A slightly different approach was used in [34] for implementing ILUM, where the matrix is permuted explicitly. 5 Size of the Independent Set In addition to being potentially robust by avoiding instabilities caused by small diagonals in the ILUM factorization, BILUM may also have the advantage of yielding larger independent sets. <p> Let j be the degree of vertex j. Let be the maximum degree of each node in V , i.e., = max f j g; It has been shown in <ref> [34] </ref> that jV S 1 j 1 + 8 with 4 blocks of size 2 at each level. This is a rough lower bound. <p> GMRES (10) was used for the outer iterations. 6.2 Matrices from the Harwell-Boeing Collection We compare the performance of different BILUM preconditioners with 5 matrices from the Harwell-Boeing collection [15, 17], together with our 5-POINT and 9-POINT matrices. (4 out of the 5 Harwell-Boeing matrices were tested with ILUM in <ref> [34] </ref> using different independent set ordering strategy.) We used t = 10 3 and p = 10 for the reduction process and the last reduced system. GMRES (10) was used for the outer iterations. Various levels of reduction were tested.
Reference: [35] <author> Y. Saad, </author> <title> Iterative Methods for Sparse Linear Systems, </title> <publisher> PWS Pub. Co., </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: Efficient iterative solvers for solving such large problems consist of a combination of an accelerator and a good preconditioner <ref> [35] </ref>. It is generally recognized that the key to solving a general sparse linear system efficiently by iterative methods is a high quality preconditioner. With a suitable preconditioner, the accelerator plays a secondary role. <p> However, the parallelism that can be extracted by level-scheduling techniques is limited. The more accurate ILU factorizations are generally more robust than ILU (0) but their level of parallelism is generally worse. Different alternatives have been considered in the past to improve the degree of parallelism, see for example <ref> [35, 36, 30] </ref> for references. A particularly interesting group of methods in this category introduce parallelism by exploiting "graph coloring," or multicoloring [24, 33]. The unknowns of the problem are colored in such a way that no two unknowns of the same color are coupled by an equation. <p> A general practice for avoiding this undesirable result in developing preconditioners is to neglect some of the fill-ins created by using a simple dropping strategies as we form the reduced system <ref> [34, 35] </ref>. For example, we may drop any fill-in element created when ever its absolute value is less than a given tolerance t times the average of the absolute 7 values of the row in question.
Reference: [36] <author> H. A. van der Vorst, </author> <title> A vectorizable version of some ICCG methods, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 3, </volume> <month> 350-356 </month> <year> (1982). </year>
Reference-contexts: However, the parallelism that can be extracted by level-scheduling techniques is limited. The more accurate ILU factorizations are generally more robust than ILU (0) but their level of parallelism is generally worse. Different alternatives have been considered in the past to improve the degree of parallelism, see for example <ref> [35, 36, 30] </ref> for references. A particularly interesting group of methods in this category introduce parallelism by exploiting "graph coloring," or multicoloring [24, 33]. The unknowns of the problem are colored in such a way that no two unknowns of the same color are coupled by an equation.
Reference: [37] <author> D. M. Young, R. G. Melvin, F. T. Johnson, J.B. Bussoletti, L. B. Wigton and S. S. Samant, </author> <title> Application of sparse matrix solvers as effective preconditioners, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 10, </volume> <month> 1186-1199 </month> <year> (1989). </year>
Reference-contexts: To improve the efficiency and robustness of ILU factorizations, many alternatives which allow higher amounts of fill-in have been developed <ref> [10, 11, 21, 27, 37, 41] </ref>. The implementation of ILU (0) and other ILU preconditioners on high performance computers can be optimized by a technique called "level scheduling" or "wavefront ordering" [1]. However, the parallelism that can be extracted by level-scheduling techniques is limited.
Reference: [38] <author> J. Zhang, </author> <title> On convergence of iterative methods for a fourth-order discretization scheme, </title> <journal> Appl. Math. Lett. </journal> <volume> 10, </volume> <month> 49-55 </month> <year> (1997). </year>
Reference-contexts: This 9-POINT matrix has 40; 000 unknowns and 357; 604 nonzeros. The 5-point matrices are always weakly diagonally dominant as Re increases, due to the large amount of artificial viscosity added. For constant coefficients, it can be shown <ref> [38] </ref> that the 9-point matrices are weakly diagonally dominant only when the cell Reynolds number (hRe=2) is less than 1 and they lose diagonal dominance quickly as Re increases.
Reference: [39] <author> J. Zhang, </author> <title> Accelerated multigrid high accuracy solution of the convection-diffusion equations with high Reynolds number, Numer. Methods for PDEs, </title> <type> 13, </type> <month> 77-92 </month> <year> (1997). </year>
Reference-contexts: Our tests with convection-diffusion problems showed convergence that is nearly independent of the Reynolds number. 23 Convergence was also found to be nearly independent of the mesh-size of the underlying system. The results compared favorably with similar problems tested with the geometric or matrix-dependent multigrid methods <ref> [12, 28, 39] </ref>. Our algorithms, however, are purely algebraic and do not make any assumptions on properties of the linear system.
Reference: [40] <author> J. Zhang, </author> <title> On convergence and performance of iterative methods with fourth-order compact schemes, </title> <journal> Numer. </journal> <note> Methods for PDEs (to appear). 26 </note>
Reference-contexts: Consequently, a 9-point matrix is generally more difficult to solve by iterative methods than a 5-point counterpart with the same discretization parameters. High Reynolds number flow problems are particularly hard to solve with a standard multigrid approach <ref> [23, 40] </ref>. Techniques based on algebraic multigrid or matrix-dependent grid transfer operators have been used for systems arising from upwind-type discretization schemes [12, 28]. There is strong interest in the multigrid community to develop multi-level algorithms which converge independently of the Reynolds number.
Reference: [41] <author> Z. Zlatev, </author> <title> Use of iterative refinement in the solution of sparse linear systems, </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 19, </volume> <month> 381-399 </month> <year> (1982). </year> <month> 27 </month>
Reference-contexts: To improve the efficiency and robustness of ILU factorizations, many alternatives which allow higher amounts of fill-in have been developed <ref> [10, 11, 21, 27, 37, 41] </ref>. The implementation of ILU (0) and other ILU preconditioners on high performance computers can be optimized by a technique called "level scheduling" or "wavefront ordering" [1]. However, the parallelism that can be extracted by level-scheduling techniques is limited.
References-found: 41

