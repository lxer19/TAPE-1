URL: http://ranger.uta.edu/~piotr/papers/RMM-Coord.ps
Refering-URL: http://www-cse.uta.edu/~piotr/www/piotr.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: piotr@cse.uta.edu  durfee@caen.engin.umich.edu  
Title: Rational Interaction in Multiagent Environments: Coordination  
Author: Piotr J. Gmytrasiewicz and Edmund H. Durfee 
Note: 0 This research was supported, in part, by the Department of Energy under contract DG-FG-86NE37969, by the National Science Foundation under grant IRI-9015423, by the PYI award IRI-9158473, and by ONR grant N00014-95-1-0775.  
Date: July 30, 1997  
Address: TX 76019-0015  Ann Arbor, Michigan 48109  
Affiliation: Department of Computer Science and Engineering University of Texas at Arlington, Arlington,  Department of Electrical Engineering and Computer Science University of Michigan  
Abstract: We adopt the decision-theoretic principle of expected utility maximization as a paradigm for designing autonomous rational agents, and present a method of representing and processing a finite amount of an agent's modeling knowledge to arrive at the rational choice of coordinated action. The representation that we endow an agent with captures the agent's knowledge about the environment and about the other agents, including its knowledge about their states of knowledge, which can include what they know about the other agents, and so on. This reciprocity leads to a recursive nesting of models. Our framework puts forth a representation for these recursive models and, under the assumption that the nesting of models is finite, uses dynamic programming to solve this representation for the agent's rational choice of action. Using a decision-theoretic approach, our work addresses concerns of agent decision-making about coordinated action in unpredictable situations, without imposing upon agents pre-designed prescriptions, or protocols, about standard rules of interaction. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> James F. Allen. </author> <title> Two views of intention. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 a a a 2 0 2 3 3 p = 0.9 [0.5, 0.5] R 2 2 2 1 2 3 a 0 4 0 2 4 0 <ref> [1, 0, 0] </ref> R 2 2 2 1 2 3 a 0 4 0 2 4 0 [0.9, 0.1, 0] R 2 2 2 1 2 3 a 0 4 0 2 4 0 [0, 0, 1] 2 The no-information model No-Info 2 in the left branch in Figure 2 expresses <p> [0.5, 0.5] R 2 2 2 1 2 3 a 0 4 0 2 4 0 [1, 0, 0] R 2 2 2 1 2 3 a 0 4 0 2 4 0 [0.9, 0.1, 0] R 2 2 2 1 2 3 a 0 4 0 2 4 0 <ref> [0, 0, 1] </ref> 2 The no-information model No-Info 2 in the left branch in Figure 2 expresses the fact that R 1 knows that if R 2 cannot see P2 then R 2 has no information based on which it could predict R 1 's behavior. <p> For example, if R 2 models R 1 's expected behavior using the probability distribution <ref> [1; 0; 0] </ref> over the actions a 1 1 , 2 , and a 1 3 , then, given R 2 's payoff matrix, the expected utilities of R 2 's alternatives a 2 1 , a 2 10 The principle of indifference is applied here to the probability itself. <p> R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 a a a 2 0 2 3 1 3 p = 0.9 <ref> [0, 1, 0] </ref> [0, 0, 1] p = 0.1 (1/16) x and a 2 3 , according to Equation 5 are 0, 5, and 2, respectively, and the action a 2 2 is preferred for R 2 . <p> 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 a a a 2 0 2 3 1 3 p = 0.9 [0, 1, 0] <ref> [0, 0, 1] </ref> p = 0.1 (1/16) x and a 2 3 , according to Equation 5 are 0, 5, and 2, respectively, and the action a 2 2 is preferred for R 2 . <p> Another distribution, say, [0:9; 0:1; 0] also favors a 2 2 and thus belongs to the same equivalence class as <ref> [1; 0; 0] </ref>. The distribution [0:1; 0:9; 0], on the other hand, makes the action a 2 3 preferable for R 2 , and belongs to a different equivalence class. <p> 2 's alternatives that maximize its expected payoff in this model has only one element (Amax (R 1 ;1) 3 g) the probability distribution of the intentions of agent R 2 (Equations 10 and 11) is: p (R 1 ;1) (R 1 ;1) 1 (R 1 ;1) 3 ] = <ref> [0; 1] </ref>. Thus, R 1 knows that if R 2 cannot see point P2 it will remain stationary. <p> models of R 2 's behavior can be combined into the overall intentional probabilities as a probabilistic mixture of R 2 's intentions in each of the alternative models (Equation 6): p R 2 = [p a 2 ; p a 2 ; p a 2 ] = :90625 fi <ref> [0; 0; 1] </ref> + 0:09375 fi [0; 1; 0] = [0; 0:09375; 0:90625]: The expected utilities of R 1 's alternative actions in its own decision-making situation (top matrix in Figure 4) can now be computed (Equation 5) as: u R 1 1 u R 1 2 u R 1 3 <p> can be combined into the overall intentional probabilities as a probabilistic mixture of R 2 's intentions in each of the alternative models (Equation 6): p R 2 = [p a 2 ; p a 2 ; p a 2 ] = :90625 fi [0; 0; 1] + 0:09375 fi <ref> [0; 1; 0] </ref> = [0; 0:09375; 0:90625]: The expected utilities of R 1 's alternative actions in its own decision-making situation (top matrix in Figure 4) can now be computed (Equation 5) as: u R 1 1 u R 1 2 u R 1 3 Thus, the best choice for R <p> In his philosophical investigations into the nature of intentions Bratman [8] distinguishes between mere plans, say as behavioral alternatives, and mental states of agents when they "have a plan in mind" which is relevant for having an intention (see also <ref> [1] </ref>). Our approach of viewing intentions as the results of rational deliberations over alternatives for action, given an agent's beliefs and preferences, is clearly very similar. Closely related is also the concept of practical rationality in [59].
Reference: [2] <author> Robert J. Aumann. </author> <title> Agreeing to disagree. </title> <journal> Annals of Statistics, </journal> <volume> 4(6) </volume> <pages> 1236-1239, </pages> <year> 1976. </year>
Reference-contexts: A well-known particular case of infinitely nested knowledge is based on the notion of common knowledge <ref> [2] </ref>. A proposition, say p, is common knowledge if and only if everyone knows p, and everyone knows that everyone knows p, and everyone knows that everyone knows that everyone knows p, and so on ad infinitum.
Reference: [3] <author> Robert J. Aumann and Adam Brandenburger. </author> <title> Epistemic conditions for Nash equilibrium. </title> <note> Accepted for Publication in Econometrica, </note> <year> 1995. </year>
Reference-contexts: state of R 1 's knowledge that would result in R 1 's rational action to be pursuing observation from P1 and expecting R 2 to observe from P2, which happens to be the other equilibrium point that could be derived if the agents were assumed to have common knowledge <ref> [3] </ref> about P2. <p> agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept [5, 25, 39, 62, 70], and an alternative has been proposed <ref> [3, 7, 39, 60] </ref>, called a decision-theoretic approach to game theory. <p> the equilibrium 15 accounts for the inability of predicting which particular equilibrium is the right one and will actually be realized, if there happens to be more than one candidate. 16 Our definition of the recursive model structure above is closely related to interactive belief systems considered in game theory <ref> [3, 36, 48] </ref>. Our structures are somewhat more expressive, since they also include the sub-intentional and no-information models.
Reference: [4] <author> Afzal Ballim and Yoric Wilks. </author> <title> Artificial Believers. </title> <publisher> Earlbaum Associates, Inc., </publisher> <year> 1991. </year> <month> 22 </month>
Reference-contexts: Further, they do not elaborate on any decision mechanism that could use their representation (presumably relying on centrally designed protocols). Another related work on nested belief with an extensive formalism is one by Ballim and Wilkes <ref> [4] </ref>. While it concentrates on mechanisms for belief ascription and revision, primarily in the context of communication, it does not address the issues of decision making.
Reference: [5] <author> Ken Binmore. </author> <booktitle> Essays on Foundations of Game Theory. </booktitle> <publisher> Pitman, </publisher> <year> 1982. </year>
Reference-contexts: times, 13 common knowledge is not achievable in finite time unless agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept <ref> [5, 25, 39, 62, 70] </ref>, and an alternative has been proposed [3, 7, 39, 60], called a decision-theoretic approach to game theory. <p> In contrast, the usual game-theoretic approach is to analyze and solve the decision problems of all players together, like a system of simultaneous equations in several unknowns. Binmore <ref> [5] </ref> and Brandenburger [7] both point out that unjustifiability of common knowledge leads directly to the situation in which one has to explicitly model the decision-making of the agents involved given their state of knowledge, which is exactly our approach in RMM.
Reference: [6] <author> Ken Binmore. </author> <title> Rationality in the centipede. </title> <booktitle> In Proceedings of the Conference on Theoretical Aspects of Reasoning about Knowladge, </booktitle> <pages> pages 150-159. </pages> <publisher> Morgan Kaufman, </publisher> <month> March </month> <year> 1994. </year>
Reference-contexts: well as many types of messages that the agents can exchange, and he has developed a 15 Binmore compares it to trying to decide which of the roots of the quadratic equation is the "right" solution without reference to the context in which the quadratic equation has arisen. 16 Binmore <ref> [6] </ref>, as well as others in game theory [40, 41, 13, 14] and related fields [68], suggest the evolutionary approach to the equilibrating process.
Reference: [7] <author> Adam Brandenburger. </author> <title> Knowledge and equilibrium in games. </title> <journal> Journal of Economic Perspectives, </journal> <volume> 6 </volume> <pages> 83-101, </pages> <year> 1992. </year>
Reference-contexts: agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept [5, 25, 39, 62, 70], and an alternative has been proposed <ref> [3, 7, 39, 60] </ref>, called a decision-theoretic approach to game theory. <p> In contrast, the usual game-theoretic approach is to analyze and solve the decision problems of all players together, like a system of simultaneous equations in several unknowns. Binmore [5] and Brandenburger <ref> [7] </ref> both point out that unjustifiability of common knowledge leads directly to the situation in which one has to explicitly model the decision-making of the agents involved given their state of knowledge, which is exactly our approach in RMM.
Reference: [8] <author> Michael E. Bratman. </author> <title> What is intention? In P. </title> <editor> R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Apart from game theory we should mention related work in artificial intelligence. In his philosophical investigations into the nature of intentions Bratman <ref> [8] </ref> distinguishes between mere plans, say as behavioral alternatives, and mental states of agents when they "have a plan in mind" which is relevant for having an intention (see also [1]).
Reference: [9] <author> J. R. Busemeyer and J. T. Townsend. </author> <title> Decision field theory: A dymanic cognitive approach to decision making in an uncertain environment. </title> <journal> Psychological Review, </journal> <pages> pages 432-457, </pages> <year> 1993. </year>
Reference-contexts: all of the rest. 9 Formally, we can construct the set of R j 's options that maximize its utility: 9 An alternative view, that an action with twice the utility should be twice as likely, could be considered and seems particularly useful to model human behavior, as described in <ref> [9] </ref>. 12 Amax (R i ;ff) j j (R i ;ff) (R i ;ff);R j k (R i ;ff);R j k 0 Then, the probabilities are assigned according to the following: p a j = &lt; 1 jAmax (R i ;ff) j j j k 2 Amax (R i ;ff) 0
Reference: [10] <author> David Carmel and Shaul Markovitch. </author> <title> Learning models of intelligent agents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 62-67, </pages> <address> Portland, OR, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: console controller board's components lead to its overall behavior [34]), and the physical stance, which predicts behavior using the description of the state of what is being modeled along with knowledge of its dynamics (like in the qualitative model of a bouncing ball [23], or finite state automata models in <ref> [10] </ref>).
Reference: [11] <author> Peter Cheeseman. </author> <title> In defense of probability. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1002-1009, </pages> <address> Los Angeles, California, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: However, in order for a fact to be epsilon or eventual common knowledge, other facts have to be common knowledge within the, so called, view interpretation. See [33] for details. 16 much of AI (see <ref> [11, 52, 57] </ref> and the references therein). Its distinguishing feature seems best summarized by Myerson ([50], Section 3.6): The decision-analytic approach to player i's decision problem is to try to predict the behavior of the players other than i first, and then to solve i's decision problem last.
Reference: [12] <author> H. Chernoff and L. E. Moses. </author> <title> Elementary Decision Theory. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: We, as well as other authors [16, 19, 22, 31, 38], view this paradigm as very promising for the design of autonomous intelligent agents, since it can be shown (see, for example <ref> [12, 24] </ref>) that it results in an optimal choice of a course of an agent's action, given its beliefs about the world and its preferences. <p> Our ability to represent agents' preferences over actions as payoffs follows directly from the axioms of utility theory, which postulate that ordinal preferences among actions in the current situation can be represented as cardinal, numeric values (see <ref> [12, 19] </ref> for details).
Reference: [13] <author> In-Koo Choo and Akihiko Matsui. </author> <title> Induction and bounded rationality in repeated games. </title> <type> Technical report, </type> <note> CARESS Working Paper 92-16, </note> <institution> University of Pennsylvania, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: agents can exchange, and he has developed a 15 Binmore compares it to trying to decide which of the roots of the quadratic equation is the "right" solution without reference to the context in which the quadratic equation has arisen. 16 Binmore [6], as well as others in game theory <ref> [40, 41, 13, 14] </ref> and related fields [68], suggest the evolutionary approach to the equilibrating process. The centerpiece of these techniques lies in methods of belief revision, which we see as an interesting area for investigation in the context of RMM in the future. 17 preliminary version of an interpreter.
Reference: [14] <author> In-Koo Choo and Akihiko Matsuri. </author> <title> Learning and the ramsey policy. </title> <type> Technical report, </type> <note> CARESS Working Paper 92-18, </note> <institution> University of Pennsylvania, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: agents can exchange, and he has developed a 15 Binmore compares it to trying to decide which of the roots of the quadratic equation is the "right" solution without reference to the context in which the quadratic equation has arisen. 16 Binmore [6], as well as others in game theory <ref> [40, 41, 13, 14] </ref> and related fields [68], suggest the evolutionary approach to the equilibrating process. The centerpiece of these techniques lies in methods of belief revision, which we see as an interesting area for investigation in the context of RMM in the future. 17 preliminary version of an interpreter.
Reference: [15] <author> Ronald Christensen. </author> <title> Multivariate Statistical Modeling. Entropy Limited, </title> <year> 1983. </year>
Reference-contexts: The no-information model No-Info 1 in the right branch means that R 1 has no information about how it is modeled by R 2 . Therefore, all of the conjectures that R 2 may have about R 1 's behavior are possible and, according to the principle of indifference <ref> [15, 52] </ref>, equally likely. This can be represented as the branch on the first level of the recursive structure splitting into infinite sub-branches, each of which terminates with a different, legal probability distribution describing R 2 's conjecture about R 1 's behavior. <p> See, for example, the discussion in <ref> [15] </ref> Section 1.G. 13 R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 a a a 2 0 2 3 1 3
Reference: [16] <author> L. S. Coles, A. M. Robb, P. L. Sinclar, M. H. Smith, and R. R. Sobek. </author> <title> Decision analysis for an experimental robot with unreliable sensors. </title> <booktitle> In Proceedings of the Fourth International Joint Conference on Artificial Intelligence, </booktitle> <address> Stanford, California, </address> <month> August </month> <year> 1975. </year>
Reference-contexts: In our work, we use a decision-theoretic paradigm of rationality, according to which all of an agent's undertakings in its environment are guided by its drive to maximize its expected utility. We, as well as other authors <ref> [16, 19, 22, 31, 38] </ref>, view this paradigm as very promising for the design of autonomous intelligent agents, since it can be shown (see, for example [12, 24]) that it results in an optimal choice of a course of an agent's action, given its beliefs about the world and its preferences.
Reference: [17] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: For lack of space, we briefly list some of the most intuitive methods (see [26] for more details). First, the dynamic programming solution of the recursive model structure takes advantage of the property of overlapping subproblems (see <ref> [17] </ref>, section 16.2), which avoids repeated redundant solutions of similar branches in the recursive model structure. The extend to which problems do overlap, is, of course, case dependent.
Reference: [18] <author> D. Dennett. </author> <title> Intentional systems. </title> <editor> In D. Dennett, editor, Brainstorms. </editor> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: If R 1 thinks that R 2 will attempt to maximize its own expected utility, then R 1 can adopt the intentional stance toward R 2 <ref> [18] </ref>, treat R 2 as rational, and model R 2 's decision-making situation using payoff matrices. R 2 's payoff matrix, if it knows about both observation points, arrived at analogously to R 1 's matrix above, has the form depicted in the right branch in Figure 2. <p> The sub-intentional model is a model which does not include the ascription of beliefs and preferences, and does not use rationality to derive behavior, 8 as in the model of the bush in the preceding section. Besides the intentional stance, Dennett <ref> [18] </ref> enumerates two sub-intentional stances: The design stance, which predicts behavior using functionality (such as how the functions of a console controller board's components lead to its overall behavior [34]), and the physical stance, which predicts behavior using the description of the state of what is being modeled along with knowledge <p> In other words, in representing the content of its KB about its 8 According to Dennett <ref> [18] </ref>, such a sub-intentional agent does not even satisfy the basic requirement of agenthood. <p> Closely related is also the concept of practical rationality in [59]. Another strand of philosophical work that we follow, as we have mentioned before, is Dennett's formulation of the intentional stance <ref> [18] </ref>, and his idea of the ladder of agenthood (see [51] for a succinct discussion), the first five levels of which we see as actually embodied in RMM. Shoham's agent-oriented programming (AOP) [67] takes more of a programming-language perspective.
Reference: [19] <author> Jon Doyle. </author> <title> Rationality and its role in reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 8 </volume> <pages> 376-409, </pages> <year> 1992. </year>
Reference-contexts: In our work, we use a decision-theoretic paradigm of rationality, according to which all of an agent's undertakings in its environment are guided by its drive to maximize its expected utility. We, as well as other authors <ref> [16, 19, 22, 31, 38] </ref>, view this paradigm as very promising for the design of autonomous intelligent agents, since it can be shown (see, for example [12, 24]) that it results in an optimal choice of a course of an agent's action, given its beliefs about the world and its preferences. <p> Our ability to represent agents' preferences over actions as payoffs follows directly from the axioms of utility theory, which postulate that ordinal preferences among actions in the current situation can be represented as cardinal, numeric values (see <ref> [12, 19] </ref> for details).
Reference: [20] <author> Ronald R. Fagin, Joseph Y. Halpern, Yoram Moses, and Moshe Y. Vardi. </author> <title> Reasoning About Knowledge. </title> <publisher> MIT Press, </publisher> <year> 1995. </year> <month> 23 </month>
Reference-contexts: However, while Shoham has proposed it as an extension, decision-theoretic rationality has not yet been included in AOP. The issue of nested knowledge has also been investigated in the area of distributed systems [21] (see also <ref> [20] </ref>). In [21] Fagin et. al. present an extensive model-theoretic treatment of nested knowledge which includes a no-information extension (like the no-information model in RMM) to handle the situation where an agent runs out of knowledge at a finite level of nesting; however, no sub-intentional modeling is envisioned.
Reference: [21] <author> Ronald R. Fagin, Joseph Y. Halpern, and Moshe Y. Vardi. </author> <title> A model-theoretic analysis of knowledge. </title> <journal> Journal of the ACM, </journal> (2):382-428, April 1991. 
Reference-contexts: However, while Shoham has proposed it as an extension, decision-theoretic rationality has not yet been included in AOP. The issue of nested knowledge has also been investigated in the area of distributed systems <ref> [21] </ref> (see also [20]). In [21] Fagin et. al. present an extensive model-theoretic treatment of nested knowledge which includes a no-information extension (like the no-information model in RMM) to handle the situation where an agent runs out of knowledge at a finite level of nesting; however, no sub-intentional modeling is envisioned. <p> However, while Shoham has proposed it as an extension, decision-theoretic rationality has not yet been included in AOP. The issue of nested knowledge has also been investigated in the area of distributed systems <ref> [21] </ref> (see also [20]). In [21] Fagin et. al. present an extensive model-theoretic treatment of nested knowledge which includes a no-information extension (like the no-information model in RMM) to handle the situation where an agent runs out of knowledge at a finite level of nesting; however, no sub-intentional modeling is envisioned.
Reference: [22] <author> Jerome A. Feldman and Robert F. Sproull. </author> <booktitle> Decision theory and Artificial Intelligence II: The hungry monkey. Cognitive Science, </booktitle> <volume> 1(2) </volume> <pages> 158-192, </pages> <month> April </month> <year> 1977. </year>
Reference-contexts: In our work, we use a decision-theoretic paradigm of rationality, according to which all of an agent's undertakings in its environment are guided by its drive to maximize its expected utility. We, as well as other authors <ref> [16, 19, 22, 31, 38] </ref>, view this paradigm as very promising for the design of autonomous intelligent agents, since it can be shown (see, for example [12, 24]) that it results in an optimal choice of a course of an agent's action, given its beliefs about the world and its preferences.
Reference: [23] <author> K. Forbus. </author> <title> Spatial and qualitative aspects of reasoning about motion. </title> <booktitle> In Proceedings of the First Annual National Conference on Artificial Intelligence, </booktitle> <address> Stanford, California, </address> <month> July </month> <year> 1980. </year>
Reference-contexts: (such as how the functions of a console controller board's components lead to its overall behavior [34]), and the physical stance, which predicts behavior using the description of the state of what is being modeled along with knowledge of its dynamics (like in the qualitative model of a bouncing ball <ref> [23] </ref>, or finite state automata models in [10]).
Reference: [24] <author> Peter Gardenfors and Nils-Eric Sahlin. </author> <title> Decision, probability, and utility. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: We, as well as other authors [16, 19, 22, 31, 38], view this paradigm as very promising for the design of autonomous intelligent agents, since it can be shown (see, for example <ref> [12, 24] </ref>) that it results in an optimal choice of a course of an agent's action, given its beliefs about the world and its preferences.
Reference: [25] <author> John Geanakoplos. </author> <title> Common knowledge. </title> <booktitle> In Proceedings of the Conference on Theoretical Aspects of Reasoning about Knowladge, </booktitle> <pages> pages 255-315. </pages> <publisher> Morgan Kaufman, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: times, 13 common knowledge is not achievable in finite time unless agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept <ref> [5, 25, 39, 62, 70] </ref>, and an alternative has been proposed [3, 7, 39, 60], called a decision-theoretic approach to game theory.
Reference: [26] <author> Piotr Gmytrasiewicz. </author> <title> A Decision-Theoretic Model of Coordination and Communication in Autonomous Systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Luckily, an exhaustive evaluation of the full-blown RMM hierarchy can be simplified in a number of ways. For lack of space, we briefly list some of the most intuitive methods (see <ref> [26] </ref> for more details). First, the dynamic programming solution of the recursive model structure takes advantage of the property of overlapping subproblems (see [17], section 16.2), which avoids repeated redundant solutions of similar branches in the recursive model structure. <p> As it turns out, the payoff matrices lend themselves to an efficient assessment of the strength of interaction between agents by analyzing variability of the payoff values. For details of these and other simplification methods, see <ref> [26, 71] </ref>, and related work in [58, 66]. 7 Application Domains and Experiments RMM fills a niche among multiagent reasoning techniques based on pre-established protocols in many realistic domains for two main reasons.
Reference: [27] <author> Piotr J. Gmytrasiewicz. </author> <title> An approach to user modeling in decision support systems. </title> <booktitle> In Proceedings of the Fifth International Conference on User Modeling, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Other examples include telecommunications networks, flexible manufacturing systems, and financial markets. In our work, we looked more closely at applying RMM to coordinate autonomous manufacturing units [30], and applications to coordination and intelligent communication in human-computer interaction <ref> [27] </ref>. Finally, we have implemented RMM in three examples of multiagent domains. Our aim has been to assess the reasonableness of the behavior resulting from our approach in a number of circumstances, and to assess its robustness and performance in mixed environments composed of RMM and human-controlled agents.
Reference: [28] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Logic of knowledge and belief for recursive modeling: Preliminary report. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 628-634, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Further, multiple research issues arise as to how decision-theoretic reasoning about actions of other agents can be integrated with other forms of reasoning, for example with nested models of the other agents' deduction (see, for instance, <ref> [28, 49] </ref>). Another direction, and an application area, for RMM is for studying rational communicative behavior among agents involved in interactions. We address this issue in the companion paper.
Reference: [29] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> Combining decision theory and hierarchical planning for a time-dependent robotic application. </title> <booktitle> In Proceedings of the Seventh IEEE Conference on AI Applications, </booktitle> <pages> pages 282-288, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Other methods include equipping probabilistic or classical planners with multiattribute utility evaluation modules, as in the work reported in [31, 35], and in our early system called the Rational Reasoning System (RRS) <ref> [29] </ref>, which combined hierarchical planning with a utility evaluation to generate the payoff matrices in a nuclear power plant environment. Still another method was used in the air-defense domain we report on in Section 6.1.
Reference: [30] <author> Piotr J. Gmytrasiewicz, H. H. Huang, and Frank L. Lewis. </author> <title> Combining operations research and agent-oriented techniques for agile manufacturing system design. </title> <booktitle> In The Proceedings of the IASTED International Conference on Robotics and Manufacturing, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Other examples include telecommunications networks, flexible manufacturing systems, and financial markets. In our work, we looked more closely at applying RMM to coordinate autonomous manufacturing units <ref> [30] </ref>, and applications to coordination and intelligent communication in human-computer interaction [27]. Finally, we have implemented RMM in three examples of multiagent domains.
Reference: [31] <author> Peter Haddawy and Steven Hanks. </author> <title> Issues in decision-theoretic planning: Symbolic goals and numeric utilities. </title> <booktitle> In Proceedings of the 1990 DARPA Workshop on Innovative Approaches to Planning, Scheduling, and Control, </booktitle> <pages> pages 48-58, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: In our work, we use a decision-theoretic paradigm of rationality, according to which all of an agent's undertakings in its environment are guided by its drive to maximize its expected utility. We, as well as other authors <ref> [16, 19, 22, 31, 38] </ref>, view this paradigm as very promising for the design of autonomous intelligent agents, since it can be shown (see, for example [12, 24]) that it results in an optimal choice of a course of an agent's action, given its beliefs about the world and its preferences. <p> Other methods include equipping probabilistic or classical planners with multiattribute utility evaluation modules, as in the work reported in <ref> [31, 35] </ref>, and in our early system called the Rational Reasoning System (RRS) [29], which combined hierarchical planning with a utility evaluation to generate the payoff matrices in a nuclear power plant environment. Still another method was used in the air-defense domain we report on in Section 6.1.
Reference: [32] <author> Joseph Y. Halpern. </author> <title> Reasoning about only knowing with many agents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 655-661, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: This amounts to the agent's being able to detect the lack of statements in its knowledge base that describe beliefs nested deeper than the given level. 6 Our representation here is related to the problem of "only knowing". See <ref> [32, 45] </ref> for a discussion and related references. 7 R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 a a a 2
Reference: [33] <author> Joseph Y. Halpern and Yoram Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <journal> Journal of the ACM, </journal> <volume> 37(3) </volume> <pages> 549-587, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: A proposition, say p, is common knowledge if and only if everyone knows p, and everyone knows that everyone knows p, and everyone knows that everyone knows that everyone knows p, and so on ad infinitum. However, in their pioneering paper <ref> [33] </ref>, Halpern and Moses show that, in situations in which agents use realistic communication channels which can lose messages or which have uncertain transmission times, 13 common knowledge is not achievable in finite time unless agents are willing to "jump to conclusions," and assume that they know more than they really <p> However, in order for a fact to be epsilon or eventual common knowledge, other facts have to be common knowledge within the, so called, view interpretation. See <ref> [33] </ref> for details. 16 much of AI (see [11, 52, 57] and the references therein).
Reference: [34] <author> Walter C. Hamscher. </author> <title> Modeling digital circuits for troubleshooting. </title> <journal> Artificial Intelligence, </journal> <volume> 51:1991, </volume> <year> 1986. </year>
Reference-contexts: Besides the intentional stance, Dennett [18] enumerates two sub-intentional stances: The design stance, which predicts behavior using functionality (such as how the functions of a console controller board's components lead to its overall behavior <ref> [34] </ref>), and the physical stance, which predicts behavior using the description of the state of what is being modeled along with knowledge of its dynamics (like in the qualitative model of a bouncing ball [23], or finite state automata models in [10]).
Reference: [35] <author> Steven Hanks and R. James Firby. </author> <title> Issues in architectures for planning and execution. </title> <booktitle> In Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> pages 59-70, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Other methods include equipping probabilistic or classical planners with multiattribute utility evaluation modules, as in the work reported in <ref> [31, 35] </ref>, and in our early system called the Rational Reasoning System (RRS) [29], which combined hierarchical planning with a utility evaluation to generate the payoff matrices in a nuclear power plant environment. Still another method was used in the air-defense domain we report on in Section 6.1.
Reference: [36] <author> John C. Harsanyi. </author> <title> Games with incomplete information played by 'bayesian' players. </title> <journal> Management Science, </journal> <volume> 14(3) </volume> <pages> 159-182, </pages> <month> November </month> <year> 1967. </year>
Reference-contexts: The No-Info 1 model, terminating 4 Our use of this term coincides with the notion of agent's type introduced by Harsanyi in <ref> [36] </ref>. 6 a 1 2 1 a 2 a 2 2 2 3 a 2 a 2 a 3 a 1a 1 a 3 1 a 3 R 2 R 2 R 2 2 4 0 1 5 1 2 4 0 0 4 0 2 0 No-Info No-Info 1 2 <p> the equilibrium 15 accounts for the inability of predicting which particular equilibrium is the right one and will actually be realized, if there happens to be more than one candidate. 16 Our definition of the recursive model structure above is closely related to interactive belief systems considered in game theory <ref> [3, 36, 48] </ref>. Our structures are somewhat more expressive, since they also include the sub-intentional and no-information models.
Reference: [37] <author> Marcus J. Huber, Edmund H. Durfee, and Michael P. Wellman. </author> <title> The automated mapping of plans for plan recognition. </title> <booktitle> In Proceedings of 1994 Conference on Uncertainty in Artificial Intelligence, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Further, any informative conjecture, i.e., a probability distribution over others' actions, can be treated as a sub-intentional model, if it has been arrived at without the use of intentionality ascription. For example, a conjecture as to another's actions may be derived from plan recognition, from past actions (as in <ref> [37] </ref>), or from information related by a third agent, and it can be given a probabilistic weight according to the assessment of its faithfulness within the RMM framework.
Reference: [38] <author> W. Jacobs and M. Kiefer. </author> <title> Robot decisions based on maximizing utility. </title> <booktitle> In Proceedings of the Third International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 402-411, </pages> <month> August </month> <year> 1973. </year>
Reference-contexts: In our work, we use a decision-theoretic paradigm of rationality, according to which all of an agent's undertakings in its environment are guided by its drive to maximize its expected utility. We, as well as other authors <ref> [16, 19, 22, 31, 38] </ref>, view this paradigm as very promising for the design of autonomous intelligent agents, since it can be shown (see, for example [12, 24]) that it results in an optimal choice of a course of an agent's action, given its beliefs about the world and its preferences.
Reference: [39] <author> Joseph B. Kadane and Patrick D. Larkey. </author> <title> Subjective probability and the theory of games. </title> <journal> Management Science, </journal> <volume> 28(2) </volume> <pages> 113-120, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: times, 13 common knowledge is not achievable in finite time unless agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept <ref> [5, 25, 39, 62, 70] </ref>, and an alternative has been proposed [3, 7, 39, 60], called a decision-theoretic approach to game theory. <p> agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept [5, 25, 39, 62, 70], and an alternative has been proposed <ref> [3, 7, 39, 60] </ref>, called a decision-theoretic approach to game theory.
Reference: [40] <author> Michihiro Kandori, George J. Mailath, and Rafael Rob. </author> <title> Learning, mutation and long run equilibria in games. </title> <type> Technical report, </type> <institution> CARESS Working Paper 91-01R, University of Pennsylvania, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: agents can exchange, and he has developed a 15 Binmore compares it to trying to decide which of the roots of the quadratic equation is the "right" solution without reference to the context in which the quadratic equation has arisen. 16 Binmore [6], as well as others in game theory <ref> [40, 41, 13, 14] </ref> and related fields [68], suggest the evolutionary approach to the equilibrating process. The centerpiece of these techniques lies in methods of belief revision, which we see as an interesting area for investigation in the context of RMM in the future. 17 preliminary version of an interpreter.
Reference: [41] <author> Michihiro Kandori and Rafael Rob. </author> <title> Evolution of equilibria in the long run: A general theory and applications. </title> <type> Technical report, </type> <institution> CARESS Working Paper 91-01R, University of Pennsylvania, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: agents can exchange, and he has developed a 15 Binmore compares it to trying to decide which of the roots of the quadratic equation is the "right" solution without reference to the context in which the quadratic equation has arisen. 16 Binmore [6], as well as others in game theory <ref> [40, 41, 13, 14] </ref> and related fields [68], suggest the evolutionary approach to the equilibrating process. The centerpiece of these techniques lies in methods of belief revision, which we see as an interesting area for investigation in the context of RMM in the future. 17 preliminary version of an interpreter.
Reference: [42] <author> Daphne Koller and Avi Pfeffer. </author> <title> Generating and solving imperfect information games. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1185-1192, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: This work uses the traditional game-theoretic concept of equilibrium to develop a family of rules of interaction, or protocols, that would guarantee the properties of the system as a whole that are desirable by the designer, like stability, fairness and global efficiency. Other work by Koller <ref> [42] </ref> on games with imperfect information, and Wellman's WALRAS system [74, 73] also follow the more traditional lines and global view of equilibrium analysis. 6 Complexity One look at the branching nested representations proposed in this paper is enough to suggest that complexity may become an issue.
Reference: [43] <author> Richard E. Korf. </author> <title> Multi-agent decision trees. </title> <booktitle> In Proceedings of the 1989 Distributed AI Workshop, </booktitle> <pages> pages 293-306, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Korf's work on multi-agent decision trees also considers issues in nested beliefs, where the beliefs that agents have about how each other evaluate game situations can vary <ref> [43] </ref>. The applications of game-theoretic techniques to the problem of interactions in multiagent domains have also received attention in the Distributed AI literature, for example in [63, 64, 65].
Reference: [44] <author> John Laird, Paul Rosenbloom, and Allen Newell. </author> <title> Universal Subgoaling and Chunking: The automatic generation and learning of goal hierarchies. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1986. </year>
Reference-contexts: By storing this as a rule of behavior that can be recalled when appropriate in the future (see related work on chunking <ref> [44] </ref>), RMM can provide the basis for the accrual of rational heuristics. Further, multiple research issues arise as to how decision-theoretic reasoning about actions of other agents can be integrated with other forms of reasoning, for example with nested models of the other agents' deduction (see, for instance, [28, 49]).
Reference: [45] <author> Gerhard Lakemeyer. </author> <title> All they know about. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: This amounts to the agent's being able to detect the lack of statements in its knowledge base that describe beliefs nested deeper than the given level. 6 Our representation here is related to the problem of "only knowing". See <ref> [32, 45] </ref> for a discussion and related references. 7 R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 2 2 2 a 4 2 2 1 1 1 R 2 a a a 2
Reference: [46] <author> Ran Levy and Jeffrey S. Rosenschein. </author> <title> A game theoretic approach to the pursuit problem. </title> <booktitle> In Working Papaers of the Eleventh International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 195-213, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Our RMM implementation of the predators' decision-making uses the evaluation of expected utility of alternative positions of the agents, resulting from their alternative moves, including the factors of how close the agents are to the prey, and how well the prey is surrounded and blocked off, as discussed in <ref> [46] </ref>. The expected utilities of alternative moves 17 These are the, so called, open systems. 19 were then assembled into payoff matrices and used by the RMM agents in recursive model structures ending on the fifth level with no information models.
Reference: [47] <author> J. E. Mardsen and A. J. Tromba. </author> <title> Vector Calculus. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1981. </year> <month> 25 </month>
Reference-contexts: All of these legal 3-vectors form a triangular part of a plane in the three dimensional space spanned by the axes of x 1 , x 2 , and x 3 . The area of this triangle can be computed <ref> [47] </ref> as p 1 R 1x 1 R dx 2 dx 1 = 3 2 . The part of the area of the legal 3-vectors that favor the action a 2 2 , can be computed (again, see [47]) as p 0:25 R 0:75 R dx 2 dx 1 + 0:25 <p> The area of this triangle can be computed <ref> [47] </ref> as p 1 R 1x 1 R dx 2 dx 1 = 3 2 . The part of the area of the legal 3-vectors that favor the action a 2 2 , can be computed (again, see [47]) as p 0:25 R 0:75 R dx 2 dx 1 + 0:25 0 p 32 , and the area that favors a 2 3 can be computed as p 1 R 1x 2 R dx 1 dx 2 = 3 32 . 11 Thus, two equivalence classes among the 3-vectors
Reference: [48] <author> Mertens and Zamir. </author> <title> Formulation of bayesian analysis for games with incomplete infor-mation. </title> <journal> International Journal of Game Theory, </journal> <volume> 14 </volume> <pages> 1-29, </pages> <year> 1985. </year>
Reference-contexts: the equilibrium 15 accounts for the inability of predicting which particular equilibrium is the right one and will actually be realized, if there happens to be more than one candidate. 16 Our definition of the recursive model structure above is closely related to interactive belief systems considered in game theory <ref> [3, 36, 48] </ref>. Our structures are somewhat more expressive, since they also include the sub-intentional and no-information models.
Reference: [49] <author> Y. Moses, D. Dolev, and J. Y. Halpern. </author> <title> Cheating husbands and other stories: a case study in common knowledge. </title> <type> Technical report, </type> <institution> IBM, Almaden Research Center, </institution> <year> 1983. </year>
Reference-contexts: Further, multiple research issues arise as to how decision-theoretic reasoning about actions of other agents can be integrated with other forms of reasoning, for example with nested models of the other agents' deduction (see, for instance, <ref> [28, 49] </ref>). Another direction, and an application area, for RMM is for studying rational communicative behavior among agents involved in interactions. We address this issue in the companion paper.
Reference: [50] <author> Roger B. Myerson. </author> <title> Game Theory: Analysis of Conflict. </title> <publisher> Harvard University Press, </publisher> <year> 1991. </year>
Reference: [51] <author> Ajit Narayanan. </author> <title> On Being a Machine. </title> <publisher> Ellis Horwood, </publisher> <year> 1988. </year>
Reference-contexts: Closely related is also the concept of practical rationality in [59]. Another strand of philosophical work that we follow, as we have mentioned before, is Dennett's formulation of the intentional stance [18], and his idea of the ladder of agenthood (see <ref> [51] </ref> for a succinct discussion), the first five levels of which we see as actually embodied in RMM. Shoham's agent-oriented programming (AOP) [67] takes more of a programming-language perspective.
Reference: [52] <author> Richard E. </author> <title> Neapolitan. Probabilistic Reasoning in Expert Systems. </title> <publisher> John Wiley and Sons, </publisher> <year> 1990. </year>
Reference-contexts: The no-information model No-Info 1 in the right branch means that R 1 has no information about how it is modeled by R 2 . Therefore, all of the conjectures that R 2 may have about R 1 's behavior are possible and, according to the principle of indifference <ref> [15, 52] </ref>, equally likely. This can be represented as the branch on the first level of the recursive structure splitting into infinite sub-branches, each of which terminates with a different, legal probability distribution describing R 2 's conjecture about R 1 's behavior. <p> Again, this translates into all of the legal 2-vector distributions, now emanating from the model on the second level, being possible and equally likely. It can be shown, for example using the principle of interval constraints (see <ref> [52] </ref> for definition), that the set of all of these legal distributions can be equivalently represented by a uniform distribution over R 1 's possible actions, a 1 1 and a 1 3 , themselves: [0:5; 0:5]. <p> Thus, for a no-information model, No-Info , located on level l, we have: 1 l 1. The no-information models assign uniform probabilities to all of the alternative distributions over the actions of the other agents and contain no information <ref> [52] </ref> beyond the currently considered level of nesting, representing the limits of knowledge reached at a particular stage of recursive modeling. <p> This model is a shorthand for all legal distributions being possible and equally likely. 10 As could be expected, it can be shown, for example using the principle of interval constraints (see <ref> [52] </ref> for definition), that it can be equivalently represented by a uniform distribution over the other agents' possible actions at this level, yielding the probabilities p (R i ;ff) k jA j j specified in this model. <p> However, in order for a fact to be epsilon or eventual common knowledge, other facts have to be common knowledge within the, so called, view interpretation. See [33] for details. 16 much of AI (see <ref> [11, 52, 57] </ref> and the references therein). Its distinguishing feature seems best summarized by Myerson ([50], Section 3.6): The decision-analytic approach to player i's decision problem is to try to predict the behavior of the players other than i first, and then to solve i's decision problem last.
Reference: [53] <author> Allen Newell. </author> <title> The knowledge level. </title> <journal> AI Magazine, </journal> <volume> 2(2) </volume> <pages> 1-20, </pages> <month> Summer </month> <year> 1981. </year>
Reference-contexts: While this kind of agent description adheres to the knowledge-level view (articulated by Newell <ref> [53] </ref>) that is a cornerstone of artificial intelligence, operationalizing it is a complex design process.
Reference: [54] <author> Nils J. Nilsson. </author> <booktitle> Probloem-Solving Methods in Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1971. </year>
Reference-contexts: Thus, to be able to rationally choose its action in a multiagent situation, an agent has to be able to represent and solve the recursive modeling problem. A related problem is the familiar minimax method for searching game trees <ref> [54] </ref>, which assumes turn taking on the part of the players during the course of the game. Another related area of research is the wide field of game theory, but we postpone a more detailed look at related work (Section 5) until after we have described our approach in full.
Reference: [55] <author> Sanguk Noh and Piotr J. Gmytrasiewicz. </author> <title> Agent modeling in antiair defense. </title> <booktitle> In Proceedings of the Sixth International Conference on User Modeling, </booktitle> <pages> pages 389-400, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: For the sake of brevity, we refer interested readers to <ref> [55, 56] </ref> and to the above URL for details. 20 7.3 Cooperative Assembly Domain We simulated a cooperative assembly tasks, characteristic of many space and manufacturing applications, using the blocks world in which the agents were to assemble the blocks into simple given configurations.
Reference: [56] <author> Sanguk Noh and Piotr J. Gmytrasiewicz. </author> <title> Multiagent coordination in antiair defense: A case study. </title> <booktitle> In Multi-Agent Rationality Eighth European Workshop on Modeling Autonomous Agents in a Multi-Agent World, MAAMAW'97, Lecture Notes in Artificial Intelligence., </booktitle> <pages> pages 4-16, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: For the sake of brevity, we refer interested readers to <ref> [55, 56] </ref> and to the above URL for details. 20 7.3 Cooperative Assembly Domain We simulated a cooperative assembly tasks, characteristic of many space and manufacturing applications, using the blocks world in which the agents were to assemble the blocks into simple given configurations.
Reference: [57] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufman, </publisher> <year> 1988. </year>
Reference-contexts: However, in order for a fact to be epsilon or eventual common knowledge, other facts have to be common knowledge within the, so called, view interpretation. See [33] for details. 16 much of AI (see <ref> [11, 52, 57] </ref> and the references therein). Its distinguishing feature seems best summarized by Myerson ([50], Section 3.6): The decision-analytic approach to player i's decision problem is to try to predict the behavior of the players other than i first, and then to solve i's decision problem last.
Reference: [58] <author> Kim L. Poh and Eric J. Horvitz. </author> <title> Reasoning about the value of decision-model refinement: Methods and application. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty on Uncertainty in Artificial Intelligence (UAI-93), </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: As it turns out, the payoff matrices lend themselves to an efficient assessment of the strength of interaction between agents by analyzing variability of the payoff values. For details of these and other simplification methods, see [26, 71], and related work in <ref> [58, 66] </ref>. 7 Application Domains and Experiments RMM fills a niche among multiagent reasoning techniques based on pre-established protocols in many realistic domains for two main reasons.
Reference: [59] <author> J. L. Pollock. </author> <title> How to Build a Person: A Prolegomenon. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Our approach of viewing intentions as the results of rational deliberations over alternatives for action, given an agent's beliefs and preferences, is clearly very similar. Closely related is also the concept of practical rationality in <ref> [59] </ref>. Another strand of philosophical work that we follow, as we have mentioned before, is Dennett's formulation of the intentional stance [18], and his idea of the ladder of agenthood (see [51] for a succinct discussion), the first five levels of which we see as actually embodied in RMM.
Reference: [60] <author> H. Raiffa. </author> <title> The Art and Science of Negotiation. </title> <publisher> Harvard University Press, </publisher> <year> 1982. </year>
Reference-contexts: agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept [5, 25, 39, 62, 70], and an alternative has been proposed <ref> [3, 7, 39, 60] </ref>, called a decision-theoretic approach to game theory.
Reference: [61] <author> Eric Rasmusen. Games and Information. B. Blackwel, </author> <year> 1989. </year>
Reference-contexts: A payoff matrix represents the decision-making situation an agent finds itself in when it must choose an action to take in its multiagent environment. Following the definition used in game theory <ref> [61] </ref>, we define the payoff matrix, P R i , of an agent R i as a triple P R i = (R; A; U ). R is a set of agents in the environment, labeled R 1 through R n (n 1).
Reference: [62] <author> Philip J. Reny. </author> <title> Extensive games and common knowledge. </title> <booktitle> In Proceedings of the Conference on Theoretical Aspects of Reasoning about Knowladge, </booktitle> <pages> page 395. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1988. </year>
Reference-contexts: times, 13 common knowledge is not achievable in finite time unless agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept <ref> [5, 25, 39, 62, 70] </ref>, and an alternative has been proposed [3, 7, 39, 60], called a decision-theoretic approach to game theory.
Reference: [63] <author> Jeffrey S. Rosenschein and John S. Breese. </author> <title> Communication-free interactions among rational agents: A probablistic approach. </title> <editor> In Les Gasser and Michael N. Huhns, editors, </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes in Artificial Intelligence, </booktitle> <pages> pages 99-118. </pages> <publisher> Pitman, </publisher> <year> 1989. </year> <month> 26 </month>
Reference-contexts: The applications of game-theoretic techniques to the problem of interactions in multiagent domains have also received attention in the Distributed AI literature, for example in <ref> [63, 64, 65] </ref>. This work uses the traditional game-theoretic concept of equilibrium to develop a family of rules of interaction, or protocols, that would guarantee the properties of the system as a whole that are desirable by the designer, like stability, fairness and global efficiency.
Reference: [64] <author> Jeffrey S. Rosenschein and Michael R. Genesereth. </author> <title> Deals among rational agents. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 91-99, </pages> <address> Los Angeles, California, </address> <month> August </month> <year> 1985. </year> <note> (Also published in Readings in Distributed Artificial Intelligence, </note> <editor> Alan H. Bond and Les Gasser, editors, </editor> <address> pages 227-234, </address> <publisher> Morgan Kaufmann, 1988.). </publisher>
Reference-contexts: The applications of game-theoretic techniques to the problem of interactions in multiagent domains have also received attention in the Distributed AI literature, for example in <ref> [63, 64, 65] </ref>. This work uses the traditional game-theoretic concept of equilibrium to develop a family of rules of interaction, or protocols, that would guarantee the properties of the system as a whole that are desirable by the designer, like stability, fairness and global efficiency.
Reference: [65] <author> Jeffrey S. Rosenschein and Gilag Zlotkin. </author> <title> Rules of Encounter. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Thus, research into coordination techniques has often led to prescriptions for task-sharing protocols, such as the Contract Net [69], for rules of interaction such as social laws [67], for negotiation conventions <ref> [65] </ref>, and so on. The emphasis in this work has been to provide the agents with ready-to-use knowledge that guides their interactions, so that their coordination achieves certain properties desirable from the designer's point of view, for example conflict avoidance, stability, fairness, or load balancing. <p> The applications of game-theoretic techniques to the problem of interactions in multiagent domains have also received attention in the Distributed AI literature, for example in <ref> [63, 64, 65] </ref>. This work uses the traditional game-theoretic concept of equilibrium to develop a family of rules of interaction, or protocols, that would guarantee the properties of the system as a whole that are desirable by the designer, like stability, fairness and global efficiency.
Reference: [66] <author> S. Russell and E. Wefald. </author> <title> On optimal game tree search using rational meta-reasoning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 334-340, </pages> <address> Detroit, Michigan, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: As it turns out, the payoff matrices lend themselves to an efficient assessment of the strength of interaction between agents by analyzing variability of the payoff values. For details of these and other simplification methods, see [26, 71], and related work in <ref> [58, 66] </ref>. 7 Application Domains and Experiments RMM fills a niche among multiagent reasoning techniques based on pre-established protocols in many realistic domains for two main reasons.
Reference: [67] <author> Yoav Shoham. </author> <title> Agent-oriented programming. </title> <journal> Artificial Intelligence, </journal> <volume> 60(1) </volume> <pages> 51-92, </pages> <year> 1993. </year>
Reference-contexts: Thus, research into coordination techniques has often led to prescriptions for task-sharing protocols, such as the Contract Net [69], for rules of interaction such as social laws <ref> [67] </ref>, for negotiation conventions [65], and so on. The emphasis in this work has been to provide the agents with ready-to-use knowledge that guides their interactions, so that their coordination achieves certain properties desirable from the designer's point of view, for example conflict avoidance, stability, fairness, or load balancing. <p> Shoham's agent-oriented programming (AOP) <ref> [67] </ref> takes more of a programming-language perspective.
Reference: [68] <author> John Maynard Smith. </author> <title> Evolution and the theory of games. </title> <publisher> Cambrigde University Press, </publisher> <year> 1982. </year>
Reference-contexts: a 15 Binmore compares it to trying to decide which of the roots of the quadratic equation is the "right" solution without reference to the context in which the quadratic equation has arisen. 16 Binmore [6], as well as others in game theory [40, 41, 13, 14] and related fields <ref> [68] </ref>, suggest the evolutionary approach to the equilibrating process. The centerpiece of these techniques lies in methods of belief revision, which we see as an interesting area for investigation in the context of RMM in the future. 17 preliminary version of an interpreter.
Reference: [69] <author> Reid G. Smith. </author> <title> The contract net protocol: High-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(12):1104-1113, </volume> <month> December </month> <year> 1980. </year>
Reference-contexts: Thus, research into coordination techniques has often led to prescriptions for task-sharing protocols, such as the Contract Net <ref> [69] </ref>, for rules of interaction such as social laws [67], for negotiation conventions [65], and so on.
Reference: [70] <author> Tommy C. Tan and Sergio R.D.C. Werlang. </author> <title> A guide to knowledge and games. </title> <booktitle> In Proceedings of the Second Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <year> 1988. </year>
Reference-contexts: times, 13 common knowledge is not achievable in finite time unless agents are willing to "jump to conclusions," and assume that they know more than they really do. 14 In other related work in game theory, researchers have begun to investigate the assumptions and limitations of the classical equilibrium concept <ref> [5, 25, 39, 62, 70] </ref>, and an alternative has been proposed [3, 7, 39, 60], called a decision-theoretic approach to game theory.
Reference: [71] <author> Jose Vidal and Edmund Durfee. </author> <title> Recursive agent modeling using limited rationality. </title> <booktitle> In To appear in Proceedings of the First International Conference on Multiagent Systems, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: As it turns out, the payoff matrices lend themselves to an efficient assessment of the strength of interaction between agents by analyzing variability of the payoff values. For details of these and other simplification methods, see <ref> [26, 71] </ref>, and related work in [58, 66]. 7 Application Domains and Experiments RMM fills a niche among multiagent reasoning techniques based on pre-established protocols in many realistic domains for two main reasons.
Reference: [72] <author> Michael P. Wellman. </author> <title> The preferential semantics for goals. </title> <booktitle> In AAAI91, </booktitle> <pages> pages 698-703, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Intuitively, any purposeful agent has reason to prefer some actions (that further its purposes in the current situation) to others <ref> [72] </ref>. Our ability to represent agents' preferences over actions as payoffs follows directly from the axioms of utility theory, which postulate that ordinal preferences among actions in the current situation can be represented as cardinal, numeric values (see [12, 19] for details).
Reference: [73] <author> Michael P. Wellman. </author> <title> A market-oriented programming environment and its applications to distributed multicommodity flow problems. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 1-22, </pages> <year> 1993. </year>
Reference-contexts: Other work by Koller [42] on games with imperfect information, and Wellman's WALRAS system <ref> [74, 73] </ref> also follow the more traditional lines and global view of equilibrium analysis. 6 Complexity One look at the branching nested representations proposed in this paper is enough to suggest that complexity may become an issue.
Reference: [74] <author> Michael P. Wellman. </author> <title> A computational market model for distributed configurational design. </title> <booktitle> AI EDAM, </booktitle> <year> 1995. </year> <month> 27 </month>
Reference-contexts: Other work by Koller [42] on games with imperfect information, and Wellman's WALRAS system <ref> [74, 73] </ref> also follow the more traditional lines and global view of equilibrium analysis. 6 Complexity One look at the branching nested representations proposed in this paper is enough to suggest that complexity may become an issue.
References-found: 74

