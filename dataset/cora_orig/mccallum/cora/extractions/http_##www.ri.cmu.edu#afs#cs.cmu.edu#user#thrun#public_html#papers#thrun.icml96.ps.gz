URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.icml96.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/thrun/public_html/papers/thrun.icml96.html
Root-URL: 
Title: Discovering Structure in Multiple Learning Tasks: The TC Algorithm  
Author: Sebastian Thrun Joseph O'Sullivan 
Date: 1996  
Note: or ~josullvn/ In: Proceedings of the Thirteenth International Conference on Machine Learning L. Saitta (ed.), Morgan Kaufmann,  
Web: http://www.cs.cmu.edu/~thrun/  
Address: Pittsburgh, PA 15213-3891  San Mateo, CA,  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: Recently, there has been an increased interest in lifelong machine learning methods, that transfer knowledge across multiple learning tasks. Such methods have repeatedly been found to outperform conventional, single-task learning algorithms when the learning tasks are appropriately related. To increase robustness of such approaches, methods are desirable that can reason about the relatedness of individual learning tasks, in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading. This paper describes the task-clustering (TC) algorithm. TC clusters learning tasks into classes of mutually related tasks. When facing a new learning task, TC first determines the most related task cluster, then exploits information selectively from this task cluster only. An empirical study carried out in a mobile robot domain shows that TC outperforms its non-selective counterpart in situations where only a small number of tasks is relevant.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. S. Abu-Mostafa. </author> <title> A method for learning from hints. </title> <editor> In S. J. Hanson, J. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 7380, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., <ref> [1, 4, 8, 19, 21, 22, 24] </ref>, * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning
Reference: [2] <author> W.-K. Ahn and W. F. Brewer. </author> <title> Psychological studies of explanation-based learning. </title> <editor> In G. DeJong, editor, </editor> <title> Investigating Explanation-Based Learning. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston/Dordrecht/London, </address> <year> 1993. </year>
Reference-contexts: Motivated by the observation that humans encounter more than just a single learning task during their lifetime, and that they successfully improve their ability to learn <ref> [2, 17] </ref>, several researchers have proposed algorithms that are able to acquire domain-specific knowledge and re-use it in future learning tasks.
Reference: [3] <author> C. A. Atkeson. </author> <title> Using locally weighted regression for robot learning. </title> <booktitle> In Proceedings of the 1991 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 958962, </pages> <address> Sacramento, CA, </address> <year> 1991. </year>
Reference-contexts: Obviously, d determines the generalization properties of nearest neighbor. 2.2 ADJUSTING THE DISTANCE METRIC TC transfers knowledge across learning tasks by adjusting d for some tasks, then re-using it in others. Following ideas presented elsewhere <ref> [3, 4, 10, 11, 15, 16, 28] </ref>, this is done by minimizing the distance between training examples that belong to the same class, while maximizing the distance between training examples with opposite class labels: E n (d) = x;y ffi xy dist d (x ; y ) ! min where ffi
Reference: [4] <author> J. Baxter. </author> <title> Learning Internal Representations. </title> <type> PhD thesis, </type> <institution> Flinders University, Australia, </institution> <year> 1995. </year>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., <ref> [1, 4, 8, 19, 21, 22, 24] </ref>, * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning <p> date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., <ref> [4, 16, 29] </ref>, * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. <p> Obviously, d determines the generalization properties of nearest neighbor. 2.2 ADJUSTING THE DISTANCE METRIC TC transfers knowledge across learning tasks by adjusting d for some tasks, then re-using it in others. Following ideas presented elsewhere <ref> [3, 4, 10, 11, 15, 16, 28] </ref>, this is done by minimizing the distance between training examples that belong to the same class, while maximizing the distance between training examples with opposite class labels: E n (d) = x;y ffi xy dist d (x ; y ) ! min where ffi
Reference: [5] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo No. 1431, </volume> <year> 1993. </year>
Reference-contexts: For example, in the context of face recognition, methods have been developed that improve the recognition accuracy significantly when learning to recognize a face, by transferring face-specific invariances learned in other, previous face recognition tasks <ref> [5, 13] </ref>. Similar results in the context of object recognition, robot navigation and chess are reported in [26]. fl The first author is also affiliated with the University of Bonn, Germany, where part of the research was carried out. <p> [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., <ref> [5, 13, 26, 28] </ref>, * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task.
Reference: [6] <author> C.E. Brodley. </author> <title> Recursive Automatic Algorithm Selection for Inductive Learning. </title> <type> PhD thesis, </type> <institution> University of Mas-sachusetts, </institution> <address> Amherst, MA 01003, </address> <year> 1994. </year>
Reference-contexts: for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., <ref> [6, 20, 25, 30] </ref>, and * learning domain models, e.g., [18, 26]. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task.
Reference: [7] <author> J. Buhmann. </author> <title> Data clustering and learning. </title> <editor> In M. Arbib, editor, </editor> <booktitle> Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 278282. </pages> <publisher> Bradfort Books/MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: The first six datasets were constructed with a particular person somewhere in front of 1 Notice that maximizing J defined over a pairwise matrix (C) is a well-understood combinatorial data clustering problem for which various algorithms exist (see for example <ref> [7] </ref>). equipped with a cam era and 24 sonar sen sors. the robot. Different persons wore different clothes, so that their recognition involved spotting certain colors. <p> Clearly, the complexity of exhaustive search prohibits global optimization for large values of N and T . However, we do not view this as a principal limitation of the TC algorithm, since heuristic and/or stochastic optimization methods are certainly applicable <ref> [7] </ref>. If learning tasks arrive one after another, task clusters may also be learned incrementally, by determining cluster membership when a task arrives. Little is known concerning how much the results presented here depend on the fact that the partitioning always represents the global minimum of J .
Reference: [8] <author> R. Caruana. </author> <title> Multitask learning: A knowledge-based of source of inductive bias. </title> <editor> In P. E. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 4148, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., <ref> [1, 4, 8, 19, 21, 22, 24] </ref>, * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning
Reference: [9] <author> R. Franke. </author> <title> Scattered data interpolation: Tests of some methods. </title> <booktitle> Mathematics of Computation, </booktitle> <address> 38(157):181200, </address> <year> 1982. </year>
Reference-contexts: TC will be introduced in five steps, the first two of which have been adopted from recent literature. 2.1 NEAREST NEIGHBOR GENERALIZATION At the underlying function approximation level, the TC algorithm uses nearest neighbor for generalization (see e.g. <ref> [9, 23] </ref>).
Reference: [10] <author> J. H. Friedman. </author> <title> Flexible metric nearest neighbor classification. </title> <year> 1994. </year>
Reference-contexts: Obviously, d determines the generalization properties of nearest neighbor. 2.2 ADJUSTING THE DISTANCE METRIC TC transfers knowledge across learning tasks by adjusting d for some tasks, then re-using it in others. Following ideas presented elsewhere <ref> [3, 4, 10, 11, 15, 16, 28] </ref>, this is done by minimizing the distance between training examples that belong to the same class, while maximizing the distance between training examples with opposite class labels: E n (d) = x;y ffi xy dist d (x ; y ) ! min where ffi
Reference: [11] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminant adaptive nearest neighbor classification. </title> <note> Submitted for publication, </note> <year> 1994. </year>
Reference-contexts: Obviously, d determines the generalization properties of nearest neighbor. 2.2 ADJUSTING THE DISTANCE METRIC TC transfers knowledge across learning tasks by adjusting d for some tasks, then re-using it in others. Following ideas presented elsewhere <ref> [3, 4, 10, 11, 15, 16, 28] </ref>, this is done by minimizing the distance between training examples that belong to the same class, while maximizing the distance between training examples with opposite class labels: E n (d) = x;y ffi xy dist d (x ; y ) ! min where ffi
Reference: [12] <author> H. Hild and A. Waibel. </author> <title> Multi-speaker/speaker-independent architectures for the multi-state time delay neural network. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pages II 255258. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., <ref> [12, 26, 29] </ref>, * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26].
Reference: [13] <author> M. Lando and S. Edelman. </author> <title> Generalizing from a single view in face recognition. </title> <type> Technical Report CS-TR 95-02, </type> <institution> Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel, </institution> <year> 1995. </year>
Reference-contexts: For example, in the context of face recognition, methods have been developed that improve the recognition accuracy significantly when learning to recognize a face, by transferring face-specific invariances learned in other, previous face recognition tasks <ref> [5, 13] </ref>. Similar results in the context of object recognition, robot navigation and chess are reported in [26]. fl The first author is also affiliated with the University of Bonn, Germany, where part of the research was carried out. <p> [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., <ref> [5, 13, 26, 28] </ref>, * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task.
Reference: [14] <author> H. Murase and S. Nayar. </author> <title> Visual learning and recognition of 3-d objects from appearance. </title> <journal> International Journal of Computer Vision, </journal> <volume> 14:524, </volume> <year> 1994. </year>
Reference: [15] <author> B. Mel. Seemore: </author> <title> A view-based approach to 3-d object recognition using multiple visual cues. </title> <editor> In M.C. Mozer D.S. Touretzky and M.E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Obviously, d determines the generalization properties of nearest neighbor. 2.2 ADJUSTING THE DISTANCE METRIC TC transfers knowledge across learning tasks by adjusting d for some tasks, then re-using it in others. Following ideas presented elsewhere <ref> [3, 4, 10, 11, 15, 16, 28] </ref>, this is done by minimizing the distance between training examples that belong to the same class, while maximizing the distance between training examples with opposite class labels: E n (d) = x;y ffi xy dist d (x ; y ) ! min where ffi
Reference: [16] <author> A. W. Moore, D. J. Hill, and M. P. Johnson. </author> <title> An Empirical Investigation of Brute Force to choose Features, Smoothers and Function Approximators. </title> <editor> In S. Hanson, S. Judd, and T. Petsche, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> Volume 3. </volume> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., <ref> [4, 16, 29] </ref>, * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. <p> Obviously, d determines the generalization properties of nearest neighbor. 2.2 ADJUSTING THE DISTANCE METRIC TC transfers knowledge across learning tasks by adjusting d for some tasks, then re-using it in others. Following ideas presented elsewhere <ref> [3, 4, 10, 11, 15, 16, 28] </ref>, this is done by minimizing the distance between training examples that belong to the same class, while maximizing the distance between training examples with opposite class labels: E n (d) = x;y ffi xy dist d (x ; y ) ! min where ffi
Reference: [17] <author> Y. Moses, S. Ullman, and S. Edelman. </author> <title> Generalization across changes in illumination and viewing position in upright and inverted faces. </title> <type> Technical Report CS-TR 93-14, </type> <institution> Department of Applied Mathematics and Computer Science, The Weiz-mann Institute of Science, Rehovot 76100, Israel, </institution> <year> 1993. </year>
Reference-contexts: Motivated by the observation that humans encounter more than just a single learning task during their lifetime, and that they successfully improve their ability to learn <ref> [2, 17] </ref>, several researchers have proposed algorithms that are able to acquire domain-specific knowledge and re-use it in future learning tasks.
Reference: [18] <author> J. O'Sullivan, T. M. Mitchell, and S. Thrun. </author> <title> Explanation-based neural network learning from mobile robot perception. </title> <editor> In K. Ikeuchi and M. Veloso, editors, </editor> <title> Symbolic Visual Learning. </title> <publisher> Oxford University Press, </publisher> <year> 1996. </year>
Reference-contexts: 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., <ref> [18, 26] </ref>. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task. However, all of them weigh previous learning tasks equally strongly when transferring knowledgethus, they may fail when only a small subset of learning tasks is related appropriately.
Reference: [19] <author> L. Y. Pratt. </author> <title> Transferring Previously Learned BackPropagation Neural Networks to New Learning Tasks. </title> <type> PhD thesis, </type> <institution> Rutgers University, Department of Computer Science, </institution> <address> New Brunswick, NJ 08904, </address> <year> 1993. </year>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., <ref> [1, 4, 8, 19, 21, 22, 24] </ref>, * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning
Reference: [20] <author> L. Rendell, R. Seshu, and D. Tcheng. </author> <title> Layered concept-learning and dynamically-variable bias management. </title> <booktitle> In Proceedings of IJCAI-87, </booktitle> <pages> pages 308314, </pages> <year> 1987. </year>
Reference-contexts: for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., <ref> [6, 20, 25, 30] </ref>, and * learning domain models, e.g., [18, 26]. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task.
Reference: [21] <author> N. E. Sharkey and A. J. C. Sharkey. </author> <title> Adaptive generalization and the transfer of knowledge. </title> <booktitle> In Proceedings of the Second Irish Neural Networks Conference, </booktitle> <address> Belfast, </address> <year> 1992. </year>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., <ref> [1, 4, 8, 19, 21, 22, 24] </ref>, * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning
Reference: [22] <author> D. Silver and R. Mercer. </author> <title> Toward a model of consolidation: The retention and transfer of neural net task knowledge. </title> <booktitle> In Proceedings of the INNS World Congress on Neural Networks, pages 164169, Volume III, </booktitle> <address> Washington, DC, </address> <year> 1995. </year>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., <ref> [1, 4, 8, 19, 21, 22, 24] </ref>, * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning
Reference: [23] <author> C. Stanfill and D. Waltz. </author> <title> Towards memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12):12131228, </volume> <year> 1986. </year>
Reference-contexts: TC will be introduced in five steps, the first two of which have been adopted from recent literature. 2.1 NEAREST NEIGHBOR GENERALIZATION At the underlying function approximation level, the TC algorithm uses nearest neighbor for generalization (see e.g. <ref> [9, 23] </ref>).
Reference: [24] <author> S. C. Suddarth and A. Holden. </author> <title> Symbolic neural systems and the use of hints for developing complex systems. </title> <journal> International Journal of Machine Studies, </journal> <volume> 35, </volume> <year> 1991. </year>
Reference-contexts: To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., <ref> [1, 4, 8, 19, 21, 22, 24] </ref>, * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning
Reference: [25] <author> R. S. Sutton. </author> <title> Adapting bias by gradient descent: An incremental version of delta-bar-delta. </title> <booktitle> In Proceeding of Tenth National Conference on Artificial Intelligence AAAI-92, </booktitle> <pages> pages 171176, </pages> <address> Menlo Park, CA, July 1992. </address> <publisher> AAAI, AAAI Press/The MIT Press. </publisher>
Reference-contexts: for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., <ref> [6, 20, 25, 30] </ref>, and * learning domain models, e.g., [18, 26]. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task.
Reference: [26] <author> S. Thrun. </author> <title> Explanation-Based Neural Network Learning: A Lifelong Learning Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: Similar results in the context of object recognition, robot navigation and chess are reported in <ref> [26] </ref>. fl The first author is also affiliated with the University of Bonn, Germany, where part of the research was carried out. Technically speaking, the problem of learning from mul tiple tasks can be stated as follows. <p> To utilize this data, mechanisms are required that can acquire and re-use domain-specific knowledge in order to guide the generalization in a knowledgeable way. To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see <ref> [26, 27] </ref> for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, <p> transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., <ref> [12, 26, 29] </ref>, * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. <p> [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., <ref> [5, 13, 26, 28] </ref>, * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task. <p> 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., <ref> [18, 26] </ref>. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task. However, all of them weigh previous learning tasks equally strongly when transferring knowledgethus, they may fail when only a small subset of learning tasks is related appropriately. <p> However, all of them weigh previous learning tasks equally strongly when transferring knowledgethus, they may fail when only a small subset of learning tasks is related appropriately. For example, the approaches to object recognition described in <ref> [26] </ref> generalize better if previous learning tasks involve the recognition of other objects; howeverif the learner faced previously unrelated learning tasks (such as stock market prediction), these approaches will most likely fail due to their non-selective nature of their transfer mechanisms. <p> These results are well in tune with other results obtained in robot perception, robot control and game playing domains <ref> [26] </ref>, which illustrate that a lifelong learner can generalize more accurately from less data if it transfers knowledge acquired in previous learning tasks. A key assumption made in the TC approach is the existence of groups of tasks so that all tasks are related within each group. <p> The reader may notice that the general scheme underlying the TC approach may be applicable to other approaches that transfer knowledge across multiple learning tasks, such as those surveyed in <ref> [26] </ref> (see also Section 1). Many of these approaches can learn and transfer more than just a global weighting vector.
Reference: [27] <author> S. Thrun. </author> <title> Is learning the n-th thing any easier than learning the first? In Advances in Neural Information Processing Systems 8, </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: To utilize this data, mechanisms are required that can acquire and re-use domain-specific knowledge in order to guide the generalization in a knowledgeable way. To date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see <ref> [26, 27] </ref> for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5,
Reference: [28] <author> S. Thrun and T. M. Mitchell. </author> <title> Learning one more thing. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <year> 1995. </year> <note> Also appeared as CMU Technical Report CMU-CS-94-184, </note> <year> 1994. </year>
Reference-contexts: [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., <ref> [5, 13, 26, 28] </ref>, * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task. <p> Obviously, d determines the generalization properties of nearest neighbor. 2.2 ADJUSTING THE DISTANCE METRIC TC transfers knowledge across learning tasks by adjusting d for some tasks, then re-using it in others. Following ideas presented elsewhere <ref> [3, 4, 10, 11, 15, 16, 28] </ref>, this is done by minimizing the distance between training examples that belong to the same class, while maximizing the distance between training examples with opposite class labels: E n (d) = x;y ffi xy dist d (x ; y ) ! min where ffi
Reference: [29] <author> S. Thrun and J. O'Sullivan. </author> <title> Clustering learning tasks and the selective cross-task transfer of knowledge. </title> <type> Technical Report CMU-CS-95-209, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA 15213, </address> <year> 1995. </year>
Reference-contexts: date, there is available a variety of strategies for the transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., <ref> [4, 16, 29] </ref>, * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. <p> transfer of domain-specific knowledge across multiple learning tasks (see [26, 27] for a more detailed survey and comparison): * learning internal representations for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., <ref> [12, 26, 29] </ref>, * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., [6, 20, 25, 30], and * learning domain models, e.g., [18, 26]. <p> The clustering strategy enables TC to handle multiple classes of tasks, each of which may exhibit different characteristics. To elucidate TC in practice, this paper reports results of a series of experiments carried out in a mobile robot domain <ref> [29] </ref>. The three key results of this empirical study are: 1. The sample complexity can be reduced significantly when domain-specific knowledge is transferred from pre vious learning tasks. 2. TC reliably succeeds in partitioning the task space into a (surprisingly) meaningful hierarchy of related tasks. 3. <p> Relatively speaking, this is only 73.46% of the average error that is being observed in the non-selective approach (which is 19.7%, cf. thick curve in Figure 4a), and it is also considerably close to the best possible distance metric (see <ref> [29] </ref>). The relative improvement in the sample complexity is even more significant: The sample complexity in the test set is only 58.7% when TC transfers knowledge selectively, when compared to the non-selective counterpart. <p> Compared to the equally-weighted distance metric, the relative generalization accuracy of TC is 73.1% and the relative sample complexity is 74.3%. Not shown here are results obtained in task family T 1 , in which case TC performs approximately as well as its non-selective counterpart (see <ref> [29] </ref>). 4 DISCUSSION This paper considers situations in which a learner faces an entire collection of learning tasks.
Reference: [30] <author> P. E. Utgoff. </author> <title> Machine Learning of Inductive Bias. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1986. </year>
Reference-contexts: for artificial neural net works, e.g., [1, 4, 8, 19, 21, 22, 24], * learning distance metrics, e.g., [4, 16, 29], * learning to re-represent the data, e.g., [12, 26, 29], * learning invariances in classification, e.g., [5, 13, 26, 28], * learning algorithmic parameters and choosing algo rithms, e.g., <ref> [6, 20, 25, 30] </ref>, and * learning domain models, e.g., [18, 26]. Many of these approaches have been demonstrated empirically to reduce the sample complexity when learning more than one task.
References-found: 30

