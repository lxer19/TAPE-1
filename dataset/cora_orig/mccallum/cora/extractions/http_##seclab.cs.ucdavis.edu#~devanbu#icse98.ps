URL: http://seclab.cs.ucdavis.edu/~devanbu/icse98.ps
Refering-URL: http://seclab.cs.ucdavis.edu/papers.html
Root-URL: http://www.cs.ucdavis.edu
Email: devanbu@cs.ucdavis.edu  pwfong@cs.sfu.ca  stubblebine@research.att.com  
Phone: +1-530-752-7324  +1-604-291-4277  +1-973-360-8354  
Title: Techniques for Trusted Software Engineering  
Author: Premkumar T. Devanbu Philip W-L Fong Stuart G. Stubblebine 
Keyword: Safety, security, mobile code, cryptography, analysis, verification.  
Address: Davis, CA 95616 USA  Burnaby, Canada V5A 1S6  180 Park Ave, Florham Park, NJ07932, USA  
Affiliation: Dept of Computer Science, University of California,  School of Computing Science Simon Fraser University  AT&T Laboratories Research  
Abstract: How do we decide if it is safe to run a given piece of software on our machine? Software used to arrive in shrink-wrapped packages from known vendors. But increasingly, software of unknown provenance arrives over the internet as applets or agents. Running such software risks serious harm to the hosting machine. Risks include serious damage to the system and loss of private information. Decisions about hosting such software are preferably made with good knowledge of the software product itself, and of the software process used to build it. We use the term Trusted Software Engineering to describe tools and techniques for constructing safe software artifacts in a manner designed to inspire trust in potential hosts. Existing approaches have considered issues such as schedule, cost and efficiency; we argue that the traditionally software engineering issues of configuration management and intellectual property protection are also of vital concern. Existing approaches (e.g., Java) to this problem have used static type checking, run-time environments, formal proofs and/or cryptographic signatures; we propose the use of trusted hardware in combination with a key management infrastructure as an additional, complementary technique for trusted software engineering, which offers some attractive features 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> ActiveX Consortium. </institution> <note> http:/www.activex.org. </note>
Reference-contexts: We describe and evaluate current approaches to establishing the safety of software products, two of which have their roots in the WWW. These are: Java, ActiveX <ref> [1, 13] </ref> and Proof Carrying Code (PCC)[19]. Each approach makes different tradeoff; we now discuss them in detail. 2.1 JAVA Java is a strongly-typed, object-oriented language [15], which in combination with a well defined run-time environment [16] provides a safe environment for hosting mobile code.
Reference: [2] <editor> Chrysalis, </editor> <publisher> Inc. </publisher> <address> http:/www.chrysalis-its.com. </address>
Reference-contexts: two logical functions: 1) updating the trusted software tool that runs at the vendor's site within the trusted hardware device, and the associated private keys, 2) updating certificates for the keys used at the host's site for signature verification. 3.1 TRUSTED HARDWARE Several manufacturers offer physically secure co-processors in PCMCIA <ref> [5, 2] </ref> and PCI [3] form factors. These devices contain a CPU, volatile and non-volatile memory, built-in cryptographic [18] facilities (symmetric & public key algorithms, random number generation, etc), private keys, and certificates.
Reference: [3] <institution> IBM PCI Secure Co-processor. </institution> <address> http:/www.ibm .com/Security/cryptocards. </address>
Reference-contexts: updating the trusted software tool that runs at the vendor's site within the trusted hardware device, and the associated private keys, 2) updating certificates for the keys used at the host's site for signature verification. 3.1 TRUSTED HARDWARE Several manufacturers offer physically secure co-processors in PCMCIA [5, 2] and PCI <ref> [3] </ref> form factors. These devices contain a CPU, volatile and non-volatile memory, built-in cryptographic [18] facilities (symmetric & public key algorithms, random number generation, etc), private keys, and certificates.
Reference: [4] <institution> Marimba Inc. </institution> <note> http:/www.marimba.com. </note>
Reference-contexts: These certificates do not have to be distributed with the code C in it; they could be distributed independently, using push technologies such as a Marimba <ref> [4] </ref> channel. Now, suppose, that a software bug is discovered in the current version of the verifier software.
Reference: [5] <institution> Spyrus Product Guide, Spyrus, Inc. </institution> <note> (See also: http://www.spyrus.com). </note>
Reference-contexts: two logical functions: 1) updating the trusted software tool that runs at the vendor's site within the trusted hardware device, and the associated private keys, 2) updating certificates for the keys used at the host's site for signature verification. 3.1 TRUSTED HARDWARE Several manufacturers offer physically secure co-processors in PCMCIA <ref> [5, 2] </ref> and PCI [3] form factors. These devices contain a CPU, volatile and non-volatile memory, built-in cryptographic [18] facilities (symmetric & public key algorithms, random number generation, etc), private keys, and certificates.
Reference: [6] <institution> Fips140-1 security requirements for cryptographic modules. </institution> <type> Technical report, NIST, </type> <year> 1994. </year> <note> http://csrc.ncsl.nist.gov/fips /fips1401.htm. </note>
Reference-contexts: Physical security is a critical requirement in the intended application of such devices (highly security-critical financial and defense uses) and is regulated by national and international standards <ref> [6] </ref>. Since these are general purpose machines, one can conduct arbitrary computations on them, and generate outputs signed with a secret private key.
Reference: [7] <author> R. Anderson and M. Kuhn. </author> <title> Tamper resistance a cautionary note. </title> <booktitle> In Second Usenix Electronic Commerce Workshop. USENIX Association, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: We also explore the criteria under which this approach is applicable. We conclude with a brief discussion of our future plans. 6.1 HARDWARE COMPROMISE The security of physical devices and the technology to circumvent protection mechanisms is continually evolving. T H devices have been compromised <ref> [7] </ref>. PCMCIA cards and PCI cards (which contain batteries, and can erase secret memory when intrusion is detected) are less vulnerable to attack than smart cards. However, as time evolves, devices once thought secure may become vulnerable. <p> It shall respond with its signature (using its private key) on the challenge data. The mean period-icity can be adjusted to discourage attempts to remove the T H device and penetrate it off-line. This is a kind of periodic inspection (similar to ones used in arms control surveillance regimens <ref> [7] </ref>) by electronic means. If challenges are unanswered, the key associated with that T H device could be revoked via the configuration manager (Figure 2), and the vendor V may be required to produce the T H for inspection.
Reference: [8] <author> M. Blum, W. Evans, P. Gemmell, S. Kannan, and M. Noar. </author> <title> Checking the correctness of memories. </title> <journal> Al-gorithmica, </journal> 12(2/3):225-244, 1994. Originally appeared in FOCS <volume> 91. </volume>
Reference-contexts: Thus, we place large data structures such as stacks, queues, and tables in P, and check operations using a small amount of memory in T H. This approach draws upon memory-checking techniques developed in the theory community <ref> [8] </ref>; however, those approaches use very strong information theoretic considerations, which allow the P unlimited computing power to mount an attack on T H. In particular P can completely simulate T H. Because of these restrictive assumptions, their approaches lead to unattractive implementations. <p> Thus the stack invariants are preserved. The approach described here uses only a constant number of bits in the T H device, irrespective of the size of the stack; existing methods use a logarithmic number of bits, which is exactly the information theoretic bound <ref> [8] </ref>. For our application, with limited adversaries, this is adequate. If information-theoretic security are desired, our security could be increased by using a counter in the T H device, and inserting signed counts into the stack. Our implementation is also simpler; each stack operation executes in constant time, whereas [8] requires <p> bound <ref> [8] </ref>. For our application, with limited adversaries, this is adequate. If information-theoretic security are desired, our security could be increased by using a counter in the T H device, and inserting signed counts into the stack. Our implementation is also simpler; each stack operation executes in constant time, whereas [8] requires O (log (stack size)) operations (amortized) for each stack push and pop. 5 IMPLEMENTATION: JAVA BYTECODE CERTIFICATION To demonstrate our approach, we are implementing a Java byte code verifier suitable for embedding in a T H device. Our approach has necessitated a redesign of the Java bytecode verifier.
Reference: [9] <author> Y. Chu, J. Feigenbaum, B. LaMacchia, P. Resnick, and M. Strauss. Referee: </author> <title> Trust management for web applications. </title> <booktitle> In Proceedings of the Sixth International World-Wide Web Conference, </booktitle> <pages> pages 227-238, </pages> <year> 1997. </year>
Reference-contexts: However, this may not be a perfect solution; plugging the host's security "hole" may require very strong verification which may reduce functionality. Other configuration management strategies can be used in conjunction with more complex host-side safety requirements. In combination with a flexible trust policy management infrastructure <ref> [9] </ref>, this approach offers a high degree of flexibility. In the most general case, one can envision a situation where the host H specifies a set of safety requirements, and describes the configuration (version information) of his safety environment.
Reference: [10] <author> P. Devanbu and S. Stubblebine. </author> <title> Automated soft-ware verification with trusted hardware. </title> <booktitle> In Twelfth International Conference on Automated Software Engineering, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: After describing trusted hardware systems, we explore the role they could play in trusted software engineering. The paper concludes after a description of implementation considerations and possible difficulties with this approach. An outline of the ideas discussed in this paper were presented earlier in a position paper <ref> [10] </ref>. In this paper, we describe full details including a key management infrastructure, details of our approaches to dealing with resource limitation, and describe an implementation for Java [15] bytecode verification. 2 APPROACHES TO SAFETY In this section, we describe some existing approaches to safety and trust in software.
Reference: [11] <author> P. Devanbu and S. G. Stubblebine. </author> <title> Cryptographic verification of test coverage claims. </title> <booktitle> In Proceedings of The Fifth ACM/SIGSOFT Symposium on the foundations of software engineering, </booktitle> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: Engineering concerns such as cost, efficiency, delay, etc., are of vital importance; in addition, the vendor (V) can be expected to be deeply concerned about disclosure of valuable intellectual property. In an earlier paper <ref> [11] </ref> we explored techniques for the process side of trusted software engineering: the concern there was to find ways in which V could convince (quickly, and at low cost) a host (H) that V's testing practices were rigorous, without disclosing too much information.
Reference: [12] <author> P. Devanbu and S. G. Stubblebine. </author> <title> Stack and queue integrity on hostile platforms. </title> <booktitle> In Proceedings of IEEE Symposium on Security and Privacy, </booktitle> <address> Oak-land, California, </address> <month> May </month> <year> 1998. </year>
Reference-contexts: Under these conditions, P cannot simulate T H. More efficient implementations of memory-checking protocols are possible, which offer acceptably low probabilities of memory compromise. A full discussion of this approach and the security of the approach is presented in <ref> [12] </ref>; we have developed schemes for handling implementations of stacks, queues, and associative arrays implemented as binary trees. For brevity, we only present our implementation of stacks. In Figure 3, the stack is shown just after the push of an item N .
Reference: [13] <author> E. Felten. </author> <title> Princeton safe internet programming java/activex faq, </title> <note> 1997. http://www.CS .Princeton.EDU/sip/java-vs-activex.html. </note>
Reference-contexts: We describe and evaluate current approaches to establishing the safety of software products, two of which have their roots in the WWW. These are: Java, ActiveX <ref> [1, 13] </ref> and Proof Carrying Code (PCC)[19]. Each approach makes different tradeoff; we now discuss them in detail. 2.1 JAVA Java is a strongly-typed, object-oriented language [15], which in combination with a well defined run-time environment [16] provides a safe environment for hosting mobile code. <p> The security and safety of the application is left unspecified. This is potentially risky. To quote the Princeton Safe Internet Programming group <ref> [13] </ref>: "ActiveX security relies entirely on human judgment.
Reference: [14] <author> P. W. Fong. </author> <title> Modular verification of dynamically-loaded mobile code. </title> <note> Working Paper, August 97. </note>
Reference-contexts: We then sign C together with the commitments and obligations induced by C; the signature certifies both that C has been verified, and the correctness of the linking information. Full details are omitted due to space constraints, and can be found in <ref> [14] </ref>. A suitably modified JVM can make use of this signed information, avoid verification, and speed up the linking process. This part of the work is still ongoing. As in [21], we have adopted a "cleanroom" approach to our implementation of the verifier.
Reference: [15] <author> J. Gosling, B. Joy, and G. Steele. </author> <title> The Java T M language specification. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Mass., USA, </address> <year> 1996. </year>
Reference-contexts: Thus, hosting companies need ways of developing confidence that the CGIs have certain safety properties (e.g., they don't delete files, write to operating system tables, use up too much CPU time/Memory, etc.). The traditional model also breaks down in the context of technologies such as Java <ref> [15, 17] </ref>, particularly with applets and mobile code. The simple act of browsing a web page can cause software to be installed and run on a hosting machine. <p> An outline of the ideas discussed in this paper were presented earlier in a position paper [10]. In this paper, we describe full details including a key management infrastructure, details of our approaches to dealing with resource limitation, and describe an implementation for Java <ref> [15] </ref> bytecode verification. 2 APPROACHES TO SAFETY In this section, we describe some existing approaches to safety and trust in software. Figure 1 represents current approaches to safety. There is a vendor, V, who produces a piece of code (perhaps mobile code) . <p> These are: Java, ActiveX [1, 13] and Proof Carrying Code (PCC)[19]. Each approach makes different tradeoff; we now discuss them in detail. 2.1 JAVA Java is a strongly-typed, object-oriented language <ref> [15] </ref>, which in combination with a well defined run-time environment [16] provides a safe environment for hosting mobile code. Java source language programs are compiled into Java bytecodes, an equivalent binary representation, which are interpreted by the Java Virtual Machine (JVM).
Reference: [16] <author> T. Lindholm and F. Yellin. </author> <title> The Java T M Virtual Machine specification. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Mass., USA, </address> <year> 1996. </year>
Reference-contexts: These are: Java, ActiveX [1, 13] and Proof Carrying Code (PCC)[19]. Each approach makes different tradeoff; we now discuss them in detail. 2.1 JAVA Java is a strongly-typed, object-oriented language [15], which in combination with a well defined run-time environment <ref> [16] </ref> provides a safe environment for hosting mobile code. Java source language programs are compiled into Java bytecodes, an equivalent binary representation, which are interpreted by the Java Virtual Machine (JVM). <p> This version is under continuous refinement. Our approach of verifying and signing each class file separately creates some special implementation issues, which we now discuss. Recall that in Java, each class file contains the implementation of one Java class. The verification process, as described in <ref> [16] </ref> comprises several passes. The first 2 passes ensure that the class file is laid out correctly. The magic number, symbol table entries, instruction sizes and arguments etc. are all checked. All branch statements are examined for target validity. The third pass actually does typechecking. <p> As in [21], we have adopted a "cleanroom" approach to our implementation of the verifier. There is no available formal description of the bytecode verifier; so we have tried to align our implementation closely with the description given in the JVM book <ref> [16] </ref>. For each part of the JVM description, there is an allied, clearly identities portion of the source code in our implementation. As in the clean room approach, we use statistical testing, with millions of test cases generated by random mutations of legal applets [21].
Reference: [17] <author> G. McGraw and E. Felten. </author> <title> Java Security: Hostile Applets, Holes & Antidotes. </title> <publisher> John Wiley & Sons, </publisher> <year> 1997. </year>
Reference-contexts: Thus, hosting companies need ways of developing confidence that the CGIs have certain safety properties (e.g., they don't delete files, write to operating system tables, use up too much CPU time/Memory, etc.). The traditional model also breaks down in the context of technologies such as Java <ref> [15, 17] </ref>, particularly with applets and mobile code. The simple act of browsing a web page can cause software to be installed and run on a hosting machine. <p> Avoiding type confusion is critical (for details, see <ref> [17] </ref>). Java source language programs are type checked by the Java compiler prior to being compiled into byte-code applets. Since browsers execute applets received from untrusted web servers, the associated JVMs have to recheck the applets for type safety prior to executing them. <p> By the same token, certain weaknesses in the Java virtual machine itself, such as one that allowed the creation of rogue classloaders ( <ref> [17] </ref>, pp 77-82) can be fixed by distributing more restrictive versions of the bytecode verifier to vendors, and doing the appropriate key management on the host side. Since Java bytecodes are essentially source code [22], applets contain all the information available in source code.
Reference: [18] <author> A. J. Menezes, P. C. van Oorschot, Scott, and A. Vanstone. </author> <title> Handbook of Applied Cryptography. </title> <publisher> CRC Press, </publisher> <year> 1996. </year>
Reference-contexts: These devices contain a CPU, volatile and non-volatile memory, built-in cryptographic <ref> [18] </ref> facilities (symmetric & public key algorithms, random number generation, etc), private keys, and certificates. The programs and non-volatile data contained in such T H devices are physically protected: attempts to access or modify them will render the device non-functional.
Reference: [19] <author> G. Necula. </author> <title> Proof-carrying code. </title> <booktitle> In Proceedings of POPL 97. ACM SIGPLAN, </booktitle> <year> 1997. </year>
Reference-contexts: Finally, produce a proof (usually by hand) that establishes that the verification condition is true given some initial conditions. Usually, the verification condition relates to the desired safety property. Clearly, it would be impractical for a host H to formally prove safety for all received programs. Necula <ref> [19] </ref> has proposed an elegant approach to code safety using formal verification. His work relies on the fact that proof checking is much simpler and faster than proof creation. In his framework, code vendors enhance binary programs with invariant assertions and package them together with a safety proof. <p> Run-time overhead (2) is significant: for a (roughly) 1 Kbyte program, the proof checking takes about 2 ms for published examples <ref> [19] </ref>. However, in general, proofs could be very long, which would result in increased checking time (and transmission time). Finally, depending on the particular proof, a lot (3) can be disclosed: the invariant assertions and the proof may reveal a lot about the program.
Reference: [20] <author> S. G. Stubblebine. </author> <title> Recent-secure authentication: Enforcing revocation in distributed systems. </title> <booktitle> In IEEE Computer Society Symposium on Security and Privacy, </booktitle> <address> Oakland, California, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Rather than distributing new versions of the software to every host, the ! effectively revokes the validity of the current version of the verifier software by assuming that hosts authenticate software subject to recent-secure authentication policies <ref> [20] </ref>. That is, if the hosts require recent statements concerning the authenticity of software configurations and T H, then we can assure bounded delays for fail-safe revocation of vulnerable configurations. For such vulnerable configurations, ! stops issuing timestamped certificates attesting to the validity of verification software indicated in D. <p> Consequently, the host treats the signed code as sus pect. This general approach for using a trusted-third party revocation service and recent-secure authentication is first described in <ref> [20] </ref>. The vendor in possession of the T H device is then alerted to obtain a new version T 2 . In figure 2 the release of T 2 would be handled by the configuration manager.
Reference: [21] <institution> The Kimera Project. </institution> <note> http://kimera.cs.wash-ington.edu. </note>
Reference-contexts: Third, resource limited computers with embedded JVMs will not have the resources to run bytecode verifiers and would benefit from our approach. Finally, for the browser user, security flaws in the bytecode verifier such as one discovered recently by the Kimera researchers <ref> [21] </ref> no longer necessitate downloading an entire new version of the browser: release management becomes a matter of key management! Rather than the vast number of web users updating their browsers, we can have a far more manageable number of updates for the applet developers. <p> Full details are omitted due to space constraints, and can be found in [14]. A suitably modified JVM can make use of this signed information, avoid verification, and speed up the linking process. This part of the work is still ongoing. As in <ref> [21] </ref>, we have adopted a "cleanroom" approach to our implementation of the verifier. There is no available formal description of the bytecode verifier; so we have tried to align our implementation closely with the description given in the JVM book [16]. <p> For each part of the JVM description, there is an allied, clearly identities portion of the source code in our implementation. As in the clean room approach, we use statistical testing, with millions of test cases generated by random mutations of legal applets <ref> [21] </ref>. Testing is underway; after comprehensive testing, embedding in a suitable T H device will be undertaken. 6 ANALYSIS AND CONCLUSION In this section, we discuss the problem of physical security compromise, and other approaches to the problem of trusted tools.
Reference: [22] <author> H.-P. V. Vliet. Mocha java bytecode decom-piler, </author> <year> 1996. </year> <note> http://web.inter.nl.net/users /H.P.van.Vliet/mocha.htm. </note>
Reference-contexts: Since Java bytecodes are essentially source code <ref> [22] </ref>, applets contain all the information available in source code. One defense is to use bytecode obfuscators; our approach is compatible this defense. In addition, Java bytecode recompilers [23] (which produce "fat", multi-platform binaries that contain binary code in addition to or instead of bytecodes) can also be accommodated. <p> This approach fits tightly into the framework we proposed with an important exception: the code is disclosed to the verification entities. In the case of byte code verification in Java, this is not a problem, since bytecodes are source code <ref> [22] </ref>. The configuration management technique illustrated in figure 2 would be fully applicable. Keys for old buggy versions of verifiers could be revoked.
Reference: [23] <author> F. Yellin. </author> <title> The java native code api http://java.- sun.com/docs/jit interface.html, </title> <year> 1996. </year>
Reference-contexts: Since Java bytecodes are essentially source code [22], applets contain all the information available in source code. One defense is to use bytecode obfuscators; our approach is compatible this defense. In addition, Java bytecode recompilers <ref> [23] </ref> (which produce "fat", multi-platform binaries that contain binary code in addition to or instead of bytecodes) can also be accommodated.
References-found: 23

