URL: http://polaris.cs.uiuc.edu/reports/1332.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: blume@csrd.uiuc.edu and eigenman@csrd.uiuc.edu  
Title: AN OVERVIEW OF SYMBOLIC ANALYSIS TECHNIQUES NEEDED FOR THE EFFECTIVE PARALLELIZATION OF THE PERFECT BENCHMARKS R  
Author: William Blume and Rudolf Eigenmann 
Keyword: Automatic parallelization, symbolic analysis, dependence analysis, Perfect Benchmarks  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: We have identified symbolic analysis techniques that will improve the effectiveness of parallelizing Fortran compilers, with emphasis upon data dependence analysis. We have done this by comparing the automatically and manually parallelized versions of the Perfect Benchmarks R fl . The techniques include: symbolic data dependence tests for nonlinear expressions, constraint propagation, array summary information, and run time tests. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic Program Parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: This is very important because new high-performance machines and their parallel programming environments are changing rapidly. Also, the availability of standard programming languages is indispensable for non-computer experts who wish to use high-performance machines. Paral-lelizing compilation tools are commercially available and their technology is well documented <ref> [1, 23] </ref>. Unfortunately these commercial tools are not yet very effective. We have quantified and analyzed this situation in previous reports [4]. There have also been studies of potential improvements by manually parallelizing real programs and reporting the necessary transformation techniques [12, 11]. Table 1 shows some of these results.
Reference: [2] <author> W. Blume and R. Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Nonlinear Expressions. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> April </month> <year> 1994. </year> <note> CSRD Report No. 1345. </note>
Reference-contexts: For example, it can prove that there are no output dependences for the array write a ((i fl i i)=2 + j) = , where 1 j i. We are currently developing a symbolic dependence test that can handle the examples we have seen <ref> [2] </ref>. CONSTANT PROPAGATION By propagating constant values along all execution paths the constant value of program variables can be determined. This aids the analysis of subscript expressions by eliminating symbolic terms.
Reference: [3] <author> W. Blume and R. Eigenmann. </author> <title> Symbolic Analysis Techniques Needed for the Effective Parallelization of the Perfect Benchmarks. </title> <type> Technical Report 1332, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: This paper gives an overview of such analysis techniques and illustrates their applicability by code examples taken from time-consuming sections of the Perfect Benchmarks R fl . A more detailed description of these techniques can be found in <ref> [3] </ref>. <p> We have seen situations where such techniques are applicable in the programs DYFESM, ADM, and MG3D. The situations range from simple tests, such as whether or not a variable is greater than a threshold, to tests for more complex symbolic conditions. Details and examples are given in <ref> [3] </ref>. There were a few other important symbolic analysis techniques that were necessary for the effective paralleliza-tion of the Perfect Benchmarks R fl . Although these techniques are computationally expensive or have limited applicability, we do believe they are worth mentioning. Again we refer to [3] for more details. <p> and examples are given in <ref> [3] </ref>. There were a few other important symbolic analysis techniques that were necessary for the effective paralleliza-tion of the Perfect Benchmarks R fl . Although these techniques are computationally expensive or have limited applicability, we do believe they are worth mentioning. Again we refer to [3] for more details. Compile Time Interpretation. One important, but very expensive technique is the compile time interpretation of programs. Essentially, the idea is to execute the program without input data; that is, to perform abstract interpretation [9] where the abstractions in the analysis are kept to a minimum.
Reference: [4] <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks Programs. </title> <journal> IEEE Transactions of Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Paral-lelizing compilation tools are commercially available and their technology is well documented [1, 23]. Unfortunately these commercial tools are not yet very effective. We have quantified and analyzed this situation in previous reports <ref> [4] </ref>. There have also been studies of potential improvements by manually parallelizing real programs and reporting the necessary transformation techniques [12, 11]. Table 1 shows some of these results. It compares automatically and hand-parallelized versions of the Perfect Benchmarks R fl that we ran on the Cedar multiprocessor. <p> For these slowdowns, either of the two techniques can be used to parallelize the important loop (s). CONCLUSION Other work has shown that current commercial paralleliz-ing compilers perform poorly on real codes <ref> [4] </ref>, and that a compiler can theoretically achieve good speedups for these codes [12, 11]. Motivated by this, we examined the Perfect Benchmarks R fl to determine what symbolic analysis techniques are required to get the observed good speedups.
Reference: [5] <author> F. Bourdoncle. </author> <title> Abstract Debugging of Higher-Order Imperative Languages. </title> <booktitle> Proceedings of the ACM SIG-PLAN '93 Conference on Programming Design and Implementation, </booktitle> <pages> pages 46-55, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: There has been some work in the determination of variable constraints. Much work has been done in determining the possible range, or interval, of values that variables can take, for the purpose of array bounds checking or program verification <ref> [16, 5] </ref>. These algorithms, however, only propagate integer ranges. Cousot and Halbwachs [10] offer a powerful algorithm for determining symbolic linear constraints between variables. Their algorithm is based upon the calculation, intersection, and merging of convex polyhedrons in the n-space of variable values.
Reference: [6] <author> P. Briggs, K. D. Cooper, M. W. Hall, and L. Torczon. </author> <title> Goal-Directed Interprocedural Optimization. </title> <type> Technical report, </type> <institution> Rice University, </institution> <month> November </month> <year> 1990. </year> <month> TR90-147. </month>
Reference-contexts: our comparisons of the automatically and manually par-allelized programs, we assume that the parallelizing compiler can do certain transformations well, (although these transformations may not yet exist in current commercial compilers); that is, the compiler can privatize arrays [20], parallelize loops containing reduction statements, and par-allelize loops with function calls <ref> [7, 6] </ref>. We also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others, including constant propagation of symbolic expressions [21], elimination of induction variables [15, 22], and symbolic simplification of expressions [8, 15]. <p> Interprocedural Constant Propagation with Procedure Cloning In our code analysis we have often found a need to propagate values across procedure boundaries, sometimes with the aid of procedure cloning <ref> [6] </ref>. For example, the code OCEAN requires both techniques to parallelize seven of its most important loop nests, accounting for 60% of the program's serial execution time. One of those loops is shown below.
Reference: [7] <author> D. Callahan, K. D. Cooper, K. Kennedy, and L. Torc-zon. </author> <title> Interprocedural Constant Propagation. </title> <booktitle> Proceedings of the SIGPLAN `86 Symposium on Compiler Construction, </booktitle> <pages> pages 152-161, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: our comparisons of the automatically and manually par-allelized programs, we assume that the parallelizing compiler can do certain transformations well, (although these transformations may not yet exist in current commercial compilers); that is, the compiler can privatize arrays [20], parallelize loops containing reduction statements, and par-allelize loops with function calls <ref> [7, 6] </ref>. We also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others, including constant propagation of symbolic expressions [21], elimination of induction variables [15, 22], and symbolic simplification of expressions [8, 15].
Reference: [8] <author> T. E. Cheatham Jr., G. H. Holloway, and J. A. Townley. </author> <title> Symbolic Evaluation and the Analysis of Programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(4):402-417, </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: We also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others, including constant propagation of symbolic expressions [21], elimination of induction variables [15, 22], and symbolic simplification of expressions <ref> [8, 15] </ref>. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information.
Reference: [9] <author> P. Cousot and R. Cousot. </author> <title> Abstract Interpretation: A unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints. </title> <booktitle> Proceedings of the 4th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Again we refer to [3] for more details. Compile Time Interpretation. One important, but very expensive technique is the compile time interpretation of programs. Essentially, the idea is to execute the program without input data; that is, to perform abstract interpretation <ref> [9] </ref> where the abstractions in the analysis are kept to a minimum. One example is the determination of whether an array is filled with the factors of some scalar, which we have seen in program ADM.
Reference: [10] <author> P. Cousot and N. Halbwachs. </author> <title> Automatic Discovery of Linear Restraints Among Variables of a Program. </title> <booktitle> Proceedings of the 5th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 84-97, </pages> <year> 1978. </year>
Reference-contexts: Much work has been done in determining the possible range, or interval, of values that variables can take, for the purpose of array bounds checking or program verification [16, 5]. These algorithms, however, only propagate integer ranges. Cousot and Halbwachs <ref> [10] </ref> offer a powerful algorithm for determining symbolic linear constraints between variables. Their algorithm is based upon the calculation, intersection, and merging of convex polyhedrons in the n-space of variable values. However, their algorithm cannot handle nonlinear expressions such as a &lt; b fl c.
Reference: [11] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res, & Dev., </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Unfortunately these commercial tools are not yet very effective. We have quantified and analyzed this situation in previous reports [4]. There have also been studies of potential improvements by manually parallelizing real programs and reporting the necessary transformation techniques <ref> [12, 11] </ref>. Table 1 shows some of these results. It compares automatically and hand-parallelized versions of the Perfect Benchmarks R fl that we ran on the Cedar multiprocessor. For these new transformation techniques, the role of symbolic analysis is very important. <p> For these slowdowns, either of the two techniques can be used to parallelize the important loop (s). CONCLUSION Other work has shown that current commercial paralleliz-ing compilers perform poorly on real codes [4], and that a compiler can theoretically achieve good speedups for these codes <ref> [12, 11] </ref>. Motivated by this, we examined the Perfect Benchmarks R fl to determine what symbolic analysis techniques are required to get the observed good speedups. The techniques that we identified ranged from minor extensions of current techniques to complex and expensive transformations.
Reference: [12] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Unfortunately these commercial tools are not yet very effective. We have quantified and analyzed this situation in previous reports [4]. There have also been studies of potential improvements by manually parallelizing real programs and reporting the necessary transformation techniques <ref> [12, 11] </ref>. Table 1 shows some of these results. It compares automatically and hand-parallelized versions of the Perfect Benchmarks R fl that we ran on the Cedar multiprocessor. For these new transformation techniques, the role of symbolic analysis is very important. <p> jj = jl, 64, 2*i2k js = 129*jj + mm - 129 h = data (js) - data (js2) data (js) = data (js) + data (js2) data (js2) = h * exj end do end do The need for a test that recognizes this situation was discussed by Eigenmann <ref> [12] </ref>. Maslov [17] presented the delinearization algorithm, which can handle any subscript expression c 0 + P n j=1 c j i j with symbolic loop-invariant ex pressions for the c j 's. <p> For these slowdowns, either of the two techniques can be used to parallelize the important loop (s). CONCLUSION Other work has shown that current commercial paralleliz-ing compilers perform poorly on real codes [4], and that a compiler can theoretically achieve good speedups for these codes <ref> [12, 11] </ref>. Motivated by this, we examined the Perfect Benchmarks R fl to determine what symbolic analysis techniques are required to get the observed good speedups. The techniques that we identified ranged from minor extensions of current techniques to complex and expensive transformations.
Reference: [13] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic Dependence Analysis for High-Performance Parallelizing Compilers. </title> <editor> In D. Gelernter, T. Gross, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 310-330. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: However, using the fact lsttrk &gt; ntrold at S2, gathered by constraint propagation, a compiler can determine that there is no dependence between S1 and S2. Haghighat <ref> [13] </ref> describes how a popular data dependence test, (i.e., Banerjee's inequalities test), can be extended to use symbolic constraint information. The array privatization technique needs to determine whether a range of array elements that is defined (e.g., a (1:m)) covers another range of elements used (a (1:n)).
Reference: [14] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic Analysis: A Basis for Paralleliziation, Optimization, and Scheduling of Programs. </title> <booktitle> Presented at the sixth Annual Languages and Compilers for Parallelism Workshop, </booktitle> <address> Portland, OR, </address> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: Essentially, the delinearization algorithm partitions the array expression into several independent subexpressions and tests these partitions separately for dependences. Haghighat <ref> [14] </ref> describes how to prove that a subscript expression is strictly increasing or decreasing. Although it cannot be used to prove independence between two unequal subscript expressions, it is able to handle a more general class of symbolic expressions.
Reference: [15] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic Program Analysis and Optimization for Parallelizing Compilers. </title> <booktitle> Presented at the fifth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August 3-5, </month> <year> 1992. </year>
Reference-contexts: We also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others, including constant propagation of symbolic expressions [21], elimination of induction variables <ref> [15, 22] </ref>, and symbolic simplification of expressions [8, 15]. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information. <p> We also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others, including constant propagation of symbolic expressions [21], elimination of induction variables [15, 22], and symbolic simplification of expressions <ref> [8, 15] </ref>. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information.
Reference: [16] <author> W. H. Harrison. </author> <title> Compiler Analysis of the Value Ranges for Variables. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(3):243-250, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: There has been some work in the determination of variable constraints. Much work has been done in determining the possible range, or interval, of values that variables can take, for the purpose of array bounds checking or program verification <ref> [16, 5] </ref>. These algorithms, however, only propagate integer ranges. Cousot and Halbwachs [10] offer a powerful algorithm for determining symbolic linear constraints between variables. Their algorithm is based upon the calculation, intersection, and merging of convex polyhedrons in the n-space of variable values.
Reference: [17] <author> V. Maslov. Delinearization: </author> <title> an Efficient Way to Break Multiloop Dependence Equations. </title> <booktitle> Proceedings of the SIG-PLAN `92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 152-161, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Maslov <ref> [17] </ref> presented the delinearization algorithm, which can handle any subscript expression c 0 + P n j=1 c j i j with symbolic loop-invariant ex pressions for the c j 's. Essentially, the delinearization algorithm partitions the array expression into several independent subexpressions and tests these partitions separately for dependences.
Reference: [18] <author> K. S. McKinley. </author> <title> Dependence Analysis of Arrays Subscripted by Index Arrays. </title> <type> Technical report, </type> <institution> Rice University, </institution> <month> June </month> <year> 1991. </year> <month> TR91-162. </month>
Reference-contexts: This knowledge can be used to parallelize the loop successfully. In cases where subscript arrays are non-constant, there can still be useful information to gather from the program. Information whether the array is singly valued or monotonically increasing or decreasing is very useful for eliminating false dependences <ref> [18] </ref>. Knowing the difference between adjacent index array values also aids dependence analysis. The minimum or maximum value of the array is very useful for array privatization, as shown previously in the example for the usefulness of constraint propagation in BDNA.
Reference: [19] <author> D. Padua, R. Eigenmann, J. Hoeflinger, P. Petersen, P. Tu, S. Weatherford, and K. Faigin. </author> <title> Polaris: A New-Generation Parallelizing Compiler for MPP's. </title> <type> Technical Report 1306, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The most interesting of these techniques are: symbolic, nonlinear expression data dependence tests, constraint propagation, guarded constant propagation, constant array propagation, subscript array analysis, and the generation of run time tests. We are currently implementing these techniques within the Po-laris parallelizing compiler <ref> [19] </ref>, which is being developed at the University of Illinois.
Reference: [20] <author> P. Tu and D. Padua. </author> <title> Automatic Array Privatization. </title> <booktitle> Presented at the Sixth Annual Languages and Compilers for Parallelism Workshop, </booktitle> <address> Portland, OR, </address> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: ANALYSIS OF THE PERFECT BENCHMARKS R fl In our comparisons of the automatically and manually par-allelized programs, we assume that the parallelizing compiler can do certain transformations well, (although these transformations may not yet exist in current commercial compilers); that is, the compiler can privatize arrays <ref> [20] </ref>, parallelize loops containing reduction statements, and par-allelize loops with function calls [7, 6]. <p> We have seen further examples in important sections of the programs ARC2D, MDG, and QCD where control flow must be taken into account in array def/use analysis for array privatization. An algorithm that deals with these situations was developed in a related project by Tu and Padua <ref> [20] </ref>. SYMBOLIC CONSTRAINT PROPAGATION Symbolic constraint propagation gathers information from the program that can determine equalities and inequalities between program variables (e.g., a &lt; b). This information can then be used to find the relationship between two arbitrary expressions.
Reference: [21] <author> M. N. Wegman and K. Zadeck. </author> <title> Constant Propagation with Conditional Branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others, including constant propagation of symbolic expressions <ref> [21] </ref>, elimination of induction variables [15, 22], and symbolic simplification of expressions [8, 15]. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information.
Reference: [22] <author> M. Wolfe. </author> <title> Beyond Induction Variables. </title> <booktitle> Proceedings of the SIGPLAN `92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: We also assume that the compiler is capable of performing symbolic analysis techniques that already have been well covered by others, including constant propagation of symbolic expressions [21], elimination of induction variables <ref> [15, 22] </ref>, and symbolic simplification of expressions [8, 15]. However, we will mention cases where the transformations or analysis techniques above need minor modifications or more accurate information.
Reference: [23] <author> H. P. Zima and B. M. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: This is very important because new high-performance machines and their parallel programming environments are changing rapidly. Also, the availability of standard programming languages is indispensable for non-computer experts who wish to use high-performance machines. Paral-lelizing compilation tools are commercially available and their technology is well documented <ref> [1, 23] </ref>. Unfortunately these commercial tools are not yet very effective. We have quantified and analyzed this situation in previous reports [4]. There have also been studies of potential improvements by manually parallelizing real programs and reporting the necessary transformation techniques [12, 11]. Table 1 shows some of these results.
References-found: 23

