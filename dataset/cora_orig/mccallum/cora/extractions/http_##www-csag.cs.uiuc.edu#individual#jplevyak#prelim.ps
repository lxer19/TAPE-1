URL: http://www-csag.cs.uiuc.edu/individual/jplevyak/prelim.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/stackhtml/jplevyak.html
Root-URL: http://www.cs.uiuc.edu
Title: Optimization of Object-Oriented and Concurrent Programs A Preliminary Examination Research Proposal  
Author: John Plevyak 
Date: November 13, 1994  
Abstract: Unfortunately, these tools and the programming style they encourage can result in poor performance. The OOP style encouraging programmers to use small functions, express new functionality by derivation from previous solutions (inheritance), share code (dynamic dispatch), and to separate use of operations from their implementation (data abstraction). Together, these techniques result in programs with high function call frequencies, and data dependent control flow. In addition, COOP abstracts out concurrency and locality. In the model each object is potentially remote and protects its data from simultaneous update. Such programs cross address space and protection domain boundaries very frequently. In addition to the direct overhead for function calls and domain crossings, these characteristics of OOP and COOP inhibit optimizations like instruction scheduling and register allocation which are critical to achieving good performance on modern RISC processors. Ironically, the key factor limiting optimization of such programs is the success of OOP at information hiding and code reuse. To achieve efficiency, this information must be uncovered by global analysis and specialized versions of reused code created by global transformation. In this way, the effects of encapsulation and abstraction can be stripped away, yielding an efficient implementation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> O. Agesen, J. Palsberg, and M. Schwartzbach. </author> <title> Type inference of Self: Analysis of objects with dynamic and multiple inheritance. </title> <booktitle> In Proceedings of ECOOP '93, </booktitle> <year> 1993. </year>
Reference-contexts: Constraint-based type inference is described by Palsberg and Schwartzbach in [60, 59]. Their approach was limited to a single level of flow-sensitivity and motivated my efforts to develop an extendible inference approach. Recently Agesen has extended the basic one level approach to handle the features of SELF [73] (see <ref> [1] </ref>). However, the problems with precision and cost inherent in a single pass approach are tackled by exploiting specialized knowledge about the SELF language.
Reference: [2] <author> G. Agha and C. Hewitt. </author> <title> Object-Oriented Concurrent Programming, chapter on Actors: </title> <booktitle> A Conceptual Foundation for Concurrent Object-Oriented Programming, </booktitle> <pages> pages 49-74. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors <ref> [37, 23, 3, 2] </ref>. A number of different systems based on this model have been created [52, 9, 5, 76, 43] most concentrating on the study of various language features.
Reference: [3] <author> Gul Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors <ref> [37, 23, 3, 2] </ref>. A number of different systems based on this model have been created [52, 9, 5, 76, 43] most concentrating on the study of various language features.
Reference: [4] <author> Alexander Aiken, Edward L. Wimmers, and T. K. Lakshman. </author> <title> Soft typing with conditional types. </title> <booktitle> In Twenty First Symposium on Principles of Programming Languages, </booktitle> <pages> pages 151-162, </pages> <address> Portland, Oregon, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The soft typing system of Cartwright and Fagan [14] extends a Hindley-Milner style type inference to support union and recursive types as well as insert type checks. To this Aiken, Wimmers, and Lakshman <ref> [4] </ref> add conditional and intersection types enabling the incorporation of flow sensitive information.
Reference: [5] <author> Pierre America. POOL-T: </author> <title> A parallel object-oriented language. </title> <editor> In Aki Yonezawa and Mario Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, </booktitle> <pages> pages 199-220. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors [37, 23, 3, 2]. A number of different systems based on this model have been created <ref> [52, 9, 5, 76, 43] </ref> most concentrating on the study of various language features. The work most similar in scope and concern for performance to that proposed for my thesis has been done on ABCL [75, 70, 76] which I will cover in more detail below.
Reference: [6] <author> Birger Andersen. </author> <title> A genereal, grain-size adaptable, object-oriented programming language for distributed computers. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Copenhagen, Copenhagen, Denmark, </institution> <month> June </month> <year> 1992. </year> <type> Ph.D. thesis (partial). </type>
Reference-contexts: However, their focus was not on extensive compile-time global transformation. In particular, efficient synchronization [70] and locality [71] optimization efforts focused on runtime techniques. The Ellie system <ref> [6] </ref> proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives.
Reference: [7] <author> Birger Andersen. </author> <title> A general , fine-grained, machine independent, object-oriented language. </title> <journal> ACM SIG-PLAN Notices, </journal> <pages> pages 17-26, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: However, their focus was not on extensive compile-time global transformation. In particular, efficient synchronization [70] and locality [71] optimization efforts focused on runtime techniques. The Ellie system [6] proposes some grain-size tuning features, however, as of recently <ref> [7] </ref> the user must supply explicit migration and locality directives. Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization [10, 20, 32, 46, 50, 64].
Reference: [8] <institution> Apple Computer, Inc., Cupertino, California. </institution> <note> Object Pascal User's Manual, 1988. 11 </note>
Reference-contexts: One simple approach is to statically bind calls when there is only one possible method <ref> [8] </ref>. This idea was extended by Calder and Grunwald [11] through `if conversion', essentially a static version of polymorphic inline caches [39]. However, this approach does not address the effects of high call frequency and lost optimization opportunities. Other approaches concentrate on type annotations [45] or special hardware [65].
Reference: [9] <author> W. C. Athas and C. L. Seitz. </author> <title> Cantor user report version 2.0. </title> <type> CalTech Internal Report, </type> <month> January </month> <year> 1987. </year>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors [37, 23, 3, 2]. A number of different systems based on this model have been created <ref> [52, 9, 5, 76, 43] </ref> most concentrating on the study of various language features. The work most similar in scope and concern for performance to that proposed for my thesis has been done on ABCL [75, 70, 76] which I will cover in more detail below.
Reference: [10] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software | Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives. Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization <ref> [10, 20, 32, 46, 50, 64] </ref>. Applicable work for procedural languages includes efficient futures and task creation [74, 48, 55] and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency.
Reference: [11] <author> Brad Calder and Dirk Grunwald. </author> <title> Reducing indirect function call overhead in C++ programs. </title> <booktitle> In Twenty-first Symposium on Principles of Programming Languages, </booktitle> <pages> pages 397-408. </pages> <booktitle> ACM SIGPLAN, </booktitle> <year> 1994. </year>
Reference-contexts: One simple approach is to statically bind calls when there is only one possible method [8]. This idea was extended by Calder and Grunwald <ref> [11] </ref> through `if conversion', essentially a static version of polymorphic inline caches [39]. However, this approach does not address the effects of high call frequency and lost optimization opportunities. Other approaches concentrate on type annotations [45] or special hardware [65].
Reference: [12] <author> Brad Calder, Dirk Grunwald, and Benjamin Zorn. </author> <title> Quantifying differences between C and C++ programs. </title> <type> Technical Report CU-CS-698-94, </type> <institution> University of Colorado, Boulder, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Unfortunately, OOP techniques significantly change the structure of programs <ref> [12] </ref>, typically incurring a performance degradation as a result. There is also every indication that as programmers develop an OOP style these differences will increase. In particular, data abstractions separate uses of operations from their definitions, and type-dependent dispatch can make the precise operation specified at a call site ambiguous.
Reference: [13] <author> Martin C. Carlisle, Anne Rogers, John H. Reppy, and Laurie J. Hendren. </author> <title> Early experiences with olden. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Machines, </booktitle> <year> 1993. </year>
Reference-contexts: Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization [10, 20, 32, 46, 50, 64]. Applicable work for procedural languages includes efficient futures and task creation [74, 48, 55] and thread shipping <ref> [44, 13] </ref> with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency. Programmers are currently penalized by using OOP and COOP styles and features.
Reference: [14] <author> Robert Cartwright and Mike Fagan. </author> <title> Soft typing. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 278-292, </pages> <address> Ontario, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Recently Agesen has extended the basic one level approach to handle the features of SELF [73] (see [1]). However, the problems with precision and cost inherent in a single pass approach are tackled by exploiting specialized knowledge about the SELF language. The soft typing system of Cartwright and Fagan <ref> [14] </ref> extends a Hindley-Milner style type inference to support union and recursive types as well as insert type checks. To this Aiken, Wimmers, and Lakshman [4] add conditional and intersection types enabling the incorporation of flow sensitive information.
Reference: [15] <author> C. Chambers and D. Ungar. </author> <title> Customization: Optimizing compiler technology for Self, a dynamically-typed object-oriented programming language. </title> <booktitle> In Proceedings of SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 146-60, </pages> <year> 1989. </year>
Reference-contexts: However, the first does not solve the inefficiencies resulting from polymorphism, while the second is infeasible in a practical sense. The work done for the Self [73] and Cecil [19] languages is perhaps the most well known. Chambers and Ungar <ref> [15] </ref> started with the idea of customization, whereby separate versions of methods are created for each receiver type. This is combined with static type estimation (essentially guessing based on common method names), and runtime test to verify the guesses.
Reference: [16] <author> C. Chambers and D. Ungar. </author> <title> Iterative type analysis and extended message splitting. </title> <booktitle> In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 150-60, </pages> <year> 1990. </year>
Reference-contexts: This is combined with static type estimation (essentially guessing based on common method names), and runtime test to verify the guesses. Based on the information so gained, they used splitting, essentially intraprocedural cloning of basic blocks <ref> [17, 16] </ref>, to preserve type information within a function. Later work showed static type estimation to be ineffective for real programs [38]. In addition to estimated types, other run time information and techniques have been used for optimization. Early work on Smalltalk used inline caches [28] to exploit type locality.
Reference: [17] <author> C. Chambers, D. Ungar, and E. Lee. </author> <title> An efficient implementation of self, a dynamically-typed object-oriented language based on prototypes. </title> <booktitle> In OOPSLA '89 Conference Proceedings, </booktitle> <pages> pages 49-70, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: This is combined with static type estimation (essentially guessing based on common method names), and runtime test to verify the guesses. Based on the information so gained, they used splitting, essentially intraprocedural cloning of basic blocks <ref> [17, 16] </ref>, to preserve type information within a function. Later work showed static type estimation to be ineffective for real programs [38]. In addition to estimated types, other run time information and techniques have been used for optimization. Early work on Smalltalk used inline caches [28] to exploit type locality.
Reference: [18] <author> Craig Chambers. </author> <title> The Design and Implementation of the Self Compiler, an Optimizing Compiler for Object-Oriented Programming Languages. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: In order to make a thorough evaluation and to enable direct comparisons with other work I have selected a test suite which includes standard sequential procedural benchmarks [53], OOP benchmarks <ref> [18, 38] </ref> and COOP benchmarks [70]. To these I have added a representative selection of applications developed as part of the Concert project [21].
Reference: [19] <author> Craig Chambers. </author> <title> The Cecil language: Specification and rationale. </title> <type> Technical Report TR 93-03-05, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Washington, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Other approaches concentrate on type annotations [45] or special hardware [65]. However, the first does not solve the inefficiencies resulting from polymorphism, while the second is infeasible in a practical sense. The work done for the Self [73] and Cecil <ref> [19] </ref> languages is perhaps the most well known. Chambers and Ungar [15] started with the idea of customization, whereby separate versions of methods are created for each receiver type. This is combined with static type estimation (essentially guessing based on common method names), and runtime test to verify the guesses.
Reference: [20] <author> K. Mani Chandy and Carl Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <booktitle> In Proceedings of the Fifth Workshop on Compilers and Languages for Parallel Computing, </booktitle> <address> New Haven, Connecticut, </address> <year> 1992. </year> <note> YALEU/DCS/RR-915, Springer-Verlag Lecture Notes in Computer Science, </note> <year> 1993. </year>
Reference-contexts: The other large body of COOP research has centered around C++. This work can be separated into two groups. ESKit C++ [67], Mentat [32], CHARM++ [46], and Compositional C++ <ref> [20] </ref> are all medium-grained explicitly parallel languages where the user controls grain size. The techniques considered in this thesis are targeted at enabling the system to control execution grain size for nominally fine-grained languages. <p> The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives. Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization <ref> [10, 20, 32, 46, 50, 64] </ref>. Applicable work for procedural languages includes efficient futures and task creation [74, 48, 55] and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency.
Reference: [21] <author> Andrew Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The concert system compiler and runtime support for efficient fine-grained concurrent object-oriented programs. </title> <type> Technical Report UIUCDCS-R-93-1815, </type> <institution> Department of Computer Science, University of Illinois, Urbana, Illinois, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: To these I have added a representative selection of applications developed as part of the Concert project <ref> [21] </ref>. <p> To some extent these inefficiencies are the result of the loss of information required by standard optimizations which are critical for efficiency on modern RISC processors. The programming and implementation models for the fine-grained COOP model considered here is discussed in more detail in <ref> [21, 47] </ref>. 4.1 Programming Model I will be using a simple programming model for object-oriented and concurrent object-oriented programs. Object-oriented programs consist of a set of classes, methods, and "global objects" which have a single instance variable value. Thus, global variables are the instance variables of these global objects. <p> Next, I will describe global data and program restructuring optimizations. Last, I will describe local transformations enabled by the global information and transformations. 5.1 Optimizing Compiler Framework The optimizing compiler framework which I will use is the Concert compiler <ref> [21] </ref>. The intermediate form used in this compiler is the Program Dependence Graph (PDG) [29] in Static Single Assignment (SSA) [26] form. Using the intermediate form, the compiler performs concrete type inference, global constant propagation, cloning, inlining and extension of access regions. <p> The Concurrent Systems Architecture Group under the direction of Prof. Chien has been developing the Concert system <ref> [21] </ref> which includes such a compiler and runtime as well as an emulator and debugger for program development. I designed and have been responsible for the majority of the implementation of the Concert compiler.
Reference: [22] <author> Andrew A. Chien, Vijay Karamcheti, John Plevyak, and Xingbin Zhang. </author> <title> Concurrent aggregates language report 2.0. </title> <note> Available via anonymous ftp from cs.uiuc.edu in /pub/csag or from http://www-csag.cs.uiuc.edu/, September 1993. </note>
Reference-contexts: Methods may invoke methods on several other objects concurrently, waiting on the responses only when required by data flow or the programmer. In this way, the programmer can safely and conveniently compose larger parallel abstractions and entire programs. A number of languages share this model <ref> [22, 42, 51, 76] </ref>. The model has three features: * a shared name space (location independence), * dynamic thread creation, and * object level access control. A shared name space allows programmers to separate data layout and functional correctness.
Reference: [23] <author> William D. Clinger. </author> <title> Foundations of actor semantics. </title> <type> Technical Report AI-TR-633, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <year> 1981. </year>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors <ref> [37, 23, 3, 2] </ref>. A number of different systems based on this model have been created [52, 9, 5, 76, 43] most concentrating on the study of various language features.
Reference: [24] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the R n environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: However, these systems are for languages which are purely functional where the question of types involving assignment does not arise and extensions to imperative languages are not fully developed. 4 3.4 Global Optimization Cooper <ref> [24] </ref> presents general interprocedural analysis and optimization techniques. Whole program (global) analysis is used to construct the call graph and solve a number of data flow problems. Transformation techniques are described to increase the availability of this information through linkage optimization including replication and specialization of procedures.
Reference: [25] <institution> Cray Research, Inc., Eagan, Minnesota 55121. CRAY T3D Software Overview Technical Note, </institution> <year> 1992. </year>
Reference-contexts: This machine will be characterized by comparing simulated instruction counts to real instruction counts and execution speeds. Runtime system overhead will be based on instruction counts and timing from the TMC CM5 [72] and/or Cray T3D <ref> [25] </ref>. Absolute OOP efficiency will be tested using real workstation and/or instruction and cache-level simulated workstations.
Reference: [26] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and F. Zadeck. </author> <title> An efficient method of computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: Last, I will describe local transformations enabled by the global information and transformations. 5.1 Optimizing Compiler Framework The optimizing compiler framework which I will use is the Concert compiler [21]. The intermediate form used in this compiler is the Program Dependence Graph (PDG) [29] in Static Single Assignment (SSA) <ref> [26] </ref> form. Using the intermediate form, the compiler performs concrete type inference, global constant propagation, cloning, inlining and extension of access regions. Next, instance variables are converted to SSA, and constant folding, common subexpression elimination, and strength reduction are performed.
Reference: [27] <author> Jeffrey Dean, Craig Chambers, and David Grove. </author> <title> Identifying profitable specialization in object-oriented languages. </title> <type> Technical Report TR 94-02-05, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Washington, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: This work is general over forward data flow problems, and presents mechanisms for preserving information across clones and minimizing their number. Again, this work does not extend to object-oriented languages with data dependent control flow. Recently, Dean, Chambers, and Grove <ref> [27] </ref> have proposed using information collected at run time to clone and share methods with respect to argument types.
Reference: [28] <author> L. Peter Deutsch and Allan M. Schiffman. </author> <title> Efficient implementation of the smalltalk-80 system. </title> <booktitle> In Eleventh Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-302. </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: Later work showed static type estimation to be ineffective for real programs [38]. In addition to estimated types, other run time information and techniques have been used for optimization. Early work on Smalltalk used inline caches <ref> [28] </ref> to exploit type locality. Holzle and Ungar [40] have shown the information obtained by polymorphic inline caches can be used to speculatively inline methods.
Reference: [29] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Laguages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-49, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Last, I will describe local transformations enabled by the global information and transformations. 5.1 Optimizing Compiler Framework The optimizing compiler framework which I will use is the Concert compiler [21]. The intermediate form used in this compiler is the Program Dependence Graph (PDG) <ref> [29] </ref> in Static Single Assignment (SSA) [26] form. Using the intermediate form, the compiler performs concrete type inference, global constant propagation, cloning, inlining and extension of access regions. Next, instance variables are converted to SSA, and constant folding, common subexpression elimination, and strength reduction are performed.
Reference: [30] <author> Adele Goldberg and David Robertson. </author> <title> Smalltalk-80: The language and its implementation. </title> <publisher> Addison-Wesley, </publisher> <year> 1985. </year>
Reference-contexts: Then I will describe current research in OOP optimization and COOP optimization. Finally, I will cover specific work related to the sort of global analysis and transformation I am proposing. 3.1 Languages and Execution Models Object-oriented programming was initially popularized in the United States by Smalltalk <ref> [30] </ref> and Flavors [56]. More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors [37, 23, 3, 2].
Reference: [31] <author> J. Graver and R. Johnson. </author> <title> A type system for smalltalk. </title> <booktitle> In Proceedings of POPL, </booktitle> <pages> pages 136-150, </pages> <year> 1990. </year>
Reference-contexts: The use of non-standard abstract semantic interpretation was used for type recovery in Scheme by Olin Shivers [66]. Type inference in object-oriented languages in particular has been studied for many years <ref> [69, 31] </ref>. These early approaches produced flow-insensitive information, limiting their usefulness for optimization. Constraint-based type inference is described by Palsberg and Schwartzbach in [60, 59]. Their approach was limited to a single level of flow-sensitivity and motivated my efforts to develop an extendible inference approach.
Reference: [32] <author> A. Grimshaw. </author> <title> Easy-to-use object-oriented parallel processing with Mentat. </title> <journal> IEEE Computer, </journal> <volume> 5(26) </volume> <pages> 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The other large body of COOP research has centered around C++. This work can be separated into two groups. ESKit C++ [67], Mentat <ref> [32] </ref>, CHARM++ [46], and Compositional C++ [20] are all medium-grained explicitly parallel languages where the user controls grain size. The techniques considered in this thesis are targeted at enabling the system to control execution grain size for nominally fine-grained languages. <p> The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives. Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization <ref> [10, 20, 32, 46, 50, 64] </ref>. Applicable work for procedural languages includes efficient futures and task creation [74, 48, 55] and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency.
Reference: [33] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1991. </year>
Reference-contexts: Transformation techniques are described to increase the availability of this information through linkage optimization including replication and specialization of procedures. However, this work does not extend to the data dependent control flow of object-oriented languages. Hall <ref> [33, 35, 34, 36] </ref> presents comprehensive interprocedural compilations techniques and cloning in particular in the context of Fortran compilation. This work is general over forward data flow problems, and presents mechanisms for preserving information across clones and minimizing their number.
Reference: [34] <author> M. W. Hall, S. Hiranandani, and K. Kennedy. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed memory machines. </title> <booktitle> In Supercomputing '92, </booktitle> <pages> pages 522-535, </pages> <year> 1992. </year>
Reference-contexts: Transformation techniques are described to increase the availability of this information through linkage optimization including replication and specialization of procedures. However, this work does not extend to the data dependent control flow of object-oriented languages. Hall <ref> [33, 35, 34, 36] </ref> presents comprehensive interprocedural compilations techniques and cloning in particular in the context of Fortran compilation. This work is general over forward data flow problems, and presents mechanisms for preserving information across clones and minimizing their number.
Reference: [35] <author> Mary W. Hall, Ken Kennedy, and Kathryn S. McKinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of the 4 th Annual Conference on High-Performance Computing (Supercomputing '91), </booktitle> <pages> pages 424-434, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Transformation techniques are described to increase the availability of this information through linkage optimization including replication and specialization of procedures. However, this work does not extend to the data dependent control flow of object-oriented languages. Hall <ref> [33, 35, 34, 36] </ref> presents comprehensive interprocedural compilations techniques and cloning in particular in the context of Fortran compilation. This work is general over forward data flow problems, and presents mechanisms for preserving information across clones and minimizing their number.
Reference: [36] <author> Mary W. Hall, John M. Mellor-Crummey, Alan Clarle, and Rene G. Rodr iguez. FIAT: </author> <title> A framework for interprocedural analysis and transformation. </title> <booktitle> In Proceedings of the Sixth Workshop for Languages and Compilers for Parallel Machines, </booktitle> <pages> pages 522-545, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Transformation techniques are described to increase the availability of this information through linkage optimization including replication and specialization of procedures. However, this work does not extend to the data dependent control flow of object-oriented languages. Hall <ref> [33, 35, 34, 36] </ref> presents comprehensive interprocedural compilations techniques and cloning in particular in the context of Fortran compilation. This work is general over forward data flow problems, and presents mechanisms for preserving information across clones and minimizing their number.
Reference: [37] <author> C. Hewitt and H. Baker. </author> <title> Actors and continuous functionals. </title> <booktitle> In Proceedings of the IFIP Working Conference on Formal Description of Programming Concepts, </booktitle> <pages> pages 367-87, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors <ref> [37, 23, 3, 2] </ref>. A number of different systems based on this model have been created [52, 9, 5, 76, 43] most concentrating on the study of various language features.
Reference: [38] <author> Urs Holzle. </author> <title> Adaptive Optimization for SELF: Reconciling High Performance iwth Exporatory Programming. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: In order to make a thorough evaluation and to enable direct comparisons with other work I have selected a test suite which includes standard sequential procedural benchmarks [53], OOP benchmarks <ref> [18, 38] </ref> and COOP benchmarks [70]. To these I have added a representative selection of applications developed as part of the Concert project [21]. <p> Based on the information so gained, they used splitting, essentially intraprocedural cloning of basic blocks [17, 16], to preserve type information within a function. Later work showed static type estimation to be ineffective for real programs <ref> [38] </ref>. In addition to estimated types, other run time information and techniques have been used for optimization. Early work on Smalltalk used inline caches [28] to exploit type locality. Holzle and Ungar [40] have shown the information obtained by polymorphic inline caches can be used to speculatively inline methods.
Reference: [39] <author> Urs Holzle, Craig Charmbers, and David Ungar. </author> <title> Optimizing dynamically-typed object-oriented languages iwth polymorphic inline caches. </title> <booktitle> In ECOOP'91 Conference Proceedings. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1991. </year> <note> Lecture Notes in Computer Science 512. </note>
Reference-contexts: One simple approach is to statically bind calls when there is only one possible method [8]. This idea was extended by Calder and Grunwald [11] through `if conversion', essentially a static version of polymorphic inline caches <ref> [39] </ref>. However, this approach does not address the effects of high call frequency and lost optimization opportunities. Other approaches concentrate on type annotations [45] or special hardware [65]. However, the first does not solve the inefficiencies resulting from polymorphism, while the second is infeasible in a practical sense.
Reference: [40] <author> Urs Holzle and David Ungar. </author> <title> Optimizing dynamically-dispatched calls with run-time type feedback. </title> <booktitle> In Proceedings of the 1994 ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 326-336, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Later work showed static type estimation to be ineffective for real programs [38]. In addition to estimated types, other run time information and techniques have been used for optimization. Early work on Smalltalk used inline caches [28] to exploit type locality. Holzle and Ungar <ref> [40] </ref> have shown the information obtained by polymorphic inline caches can be used to speculatively inline methods.
Reference: [41] <author> W. Horwat, A. Chien, and W. Dally. </author> <title> Experience with CST: </title> <booktitle> Programming and implementation. In Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 101-9. </pages> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: However, that work does not handle polymorphic instance variables which are common in object-oriented codes. 3.5 COOP Optimization Work on efficient COOP implementations includes that done for ABCL [76, 77] and Concurrent Smalltalk (CST) <ref> [41, 42] </ref>. However, their focus was not on extensive compile-time global transformation. In particular, efficient synchronization [70] and locality [71] optimization efforts focused on runtime techniques. The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives.
Reference: [42] <author> Waldemar Horwat. </author> <title> Concurrent Smalltalk on the message-driven processor. </title> <type> Master's thesis, </type> <institution> Mas-sachusetts Institute of Technology, Cambridge, Massachusetts, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: However, that work does not handle polymorphic instance variables which are common in object-oriented codes. 3.5 COOP Optimization Work on efficient COOP implementations includes that done for ABCL [76, 77] and Concurrent Smalltalk (CST) <ref> [41, 42] </ref>. However, their focus was not on extensive compile-time global transformation. In particular, efficient synchronization [70] and locality [71] optimization efforts focused on runtime techniques. The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives. <p> Methods may invoke methods on several other objects concurrently, waiting on the responses only when required by data flow or the programmer. In this way, the programmer can safely and conveniently compose larger parallel abstractions and entire programs. A number of languages share this model <ref> [22, 42, 51, 76] </ref>. The model has three features: * a shared name space (location independence), * dynamic thread creation, and * object level access control. A shared name space allows programmers to separate data layout and functional correctness. <p> Even the best implementations incur tens to hundreds of instructions for each method invocation <ref> [42, 77] </ref> due to the cost of managing a distributed memory (method invocations are location independent) and managing concurrency (locks). As a result, the cost of sending a message or checking the status of an object's lock can easily exceed the work inside a method invocation.
Reference: [43] <author> C. Houck and G. Agha. HAL: </author> <title> A high-level actor language and its distributed implementation. </title> <booktitle> In Proceedings of the 21st International Conference on Parallel Processing, </booktitle> <pages> pages 158-165, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors [37, 23, 3, 2]. A number of different systems based on this model have been created <ref> [52, 9, 5, 76, 43] </ref> most concentrating on the study of various language features. The work most similar in scope and concern for performance to that proposed for my thesis has been done on ABCL [75, 70, 76] which I will cover in more detail below.
Reference: [44] <author> Wilson C. Hsieh, Paul Wang, and William E. Weihl. </author> <title> Computation migration: Enhancing locality for distributed-memory parallel systems. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 239-248, </pages> <year> 1993. </year>
Reference-contexts: Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization [10, 20, 32, 46, 50, 64]. Applicable work for procedural languages includes efficient futures and task creation [74, 48, 55] and thread shipping <ref> [44, 13] </ref> with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency. Programmers are currently penalized by using OOP and COOP styles and features.
Reference: [45] <author> R. E. Johnson, J. O. Graver, and L. W. Zurawski. </author> <title> Ts: An optimizing compiler for smalltalk. </title> <booktitle> In OOPSLA '88 Proceedings, </booktitle> <pages> pages 18-26, </pages> <month> September </month> <year> 1988. </year> <month> 13 </month>
Reference-contexts: This idea was extended by Calder and Grunwald [11] through `if conversion', essentially a static version of polymorphic inline caches [39]. However, this approach does not address the effects of high call frequency and lost optimization opportunities. Other approaches concentrate on type annotations <ref> [45] </ref> or special hardware [65]. However, the first does not solve the inefficiencies resulting from polymorphism, while the second is infeasible in a practical sense. The work done for the Self [73] and Cecil [19] languages is perhaps the most well known.
Reference: [46] <author> L. V. Kale and Sanjeev Krishnan. CHARM++: </author> <title> A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of OOPSLA'93, </booktitle> <year> 1993. </year>
Reference-contexts: The other large body of COOP research has centered around C++. This work can be separated into two groups. ESKit C++ [67], Mentat [32], CHARM++ <ref> [46] </ref>, and Compositional C++ [20] are all medium-grained explicitly parallel languages where the user controls grain size. The techniques considered in this thesis are targeted at enabling the system to control execution grain size for nominally fine-grained languages. <p> The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives. Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization <ref> [10, 20, 32, 46, 50, 64] </ref>. Applicable work for procedural languages includes efficient futures and task creation [74, 48, 55] and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency.
Reference: [47] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert efficient runtime support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <year> 1993. </year>
Reference-contexts: To some extent these inefficiencies are the result of the loss of information required by standard optimizations which are critical for efficiency on modern RISC processors. The programming and implementation models for the fine-grained COOP model considered here is discussed in more detail in <ref> [21, 47] </ref>. 4.1 Programming Model I will be using a simple programming model for object-oriented and concurrent object-oriented programs. Object-oriented programs consist of a set of classes, methods, and "global objects" which have a single instance variable value. Thus, global variables are the instance variables of these global objects.
Reference: [48] <author> D. Kranz, R. Halstead Jr., and E. Mohr. Mul-t: </author> <title> A high-performance parallel lisp. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <year> 1989. </year>
Reference-contexts: Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization [10, 20, 32, 46, 50, 64]. Applicable work for procedural languages includes efficient futures and task creation <ref> [74, 48, 55] </ref> and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency. Programmers are currently penalized by using OOP and COOP styles and features.
Reference: [49] <author> James Larus. </author> <title> C**: a large-grain, object-oriented, </title> <booktitle> data parallel programming language. In Proceedings of the Fifth Workshop for Languages and Compilers for Parallel Machines, </booktitle> <pages> pages 326-341. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: Further, none of this systems has developed a global optimization framework, probably owing to a desire to leverage existing C++ compiler technology. The other group is data parallel C++, represented by pC++ [50] C** <ref> [49] </ref>, and pSather [57]. In these languages, the operations specified a single thread of computation may be executed on disjoint 3 data without interaction.
Reference: [50] <author> J. Lee and D. Gannon. </author> <title> Object oriented parallel programming. </title> <booktitle> In Proceedings of the ACM/IEEE Conference on Supercomputing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Further, none of this systems has developed a global optimization framework, probably owing to a desire to leverage existing C++ compiler technology. The other group is data parallel C++, represented by pC++ <ref> [50] </ref> C** [49], and pSather [57]. In these languages, the operations specified a single thread of computation may be executed on disjoint 3 data without interaction. <p> The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives. Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization <ref> [10, 20, 32, 46, 50, 64] </ref>. Applicable work for procedural languages includes efficient futures and task creation [74, 48, 55] and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency.
Reference: [51] <author> Henry Lieberman. </author> <title> Concurrent object oriented programming in ACT 1. </title> <editor> In Aki Yonezawa and Mario Tokoro, editors, </editor> <booktitle> Object-Oriented Concurrent Programming, </booktitle> <pages> pages 9-36. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Methods may invoke methods on several other objects concurrently, waiting on the responses only when required by data flow or the programmer. In this way, the programmer can safely and conveniently compose larger parallel abstractions and entire programs. A number of languages share this model <ref> [22, 42, 51, 76] </ref>. The model has three features: * a shared name space (location independence), * dynamic thread creation, and * object level access control. A shared name space allows programmers to separate data layout and functional correctness.
Reference: [52] <author> Carl R. Manning. Acore: </author> <title> The design of a core actor language and its compiler. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors [37, 23, 3, 2]. A number of different systems based on this model have been created <ref> [52, 9, 5, 76, 43] </ref> most concentrating on the study of various language features. The work most similar in scope and concern for performance to that proposed for my thesis has been done on ABCL [75, 70, 76] which I will cover in more detail below.
Reference: [53] <author> F. H. McMahon. </author> <title> The Livermore Fortran kernels: a computer test of the numerical performance range. </title> <type> Technical report UCRL-53745, </type> <institution> Lawerence Livermore National Laboratory, Livermore, California, </institution> <year> 1986. </year>
Reference-contexts: In order to make a thorough evaluation and to enable direct comparisons with other work I have selected a test suite which includes standard sequential procedural benchmarks <ref> [53] </ref>, OOP benchmarks [18, 38] and COOP benchmarks [70]. To these I have added a representative selection of applications developed as part of the Concert project [21].
Reference: [54] <author> Robin Milner, Mads Tofte, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The key to global analysis of object-oriented programs in precise type information, without which it is not possible to build precise interprocedural control and data flow graphs. The two main approaches to type inference of object-oriented programs are the data flow approach and the Hindley-Milner approach <ref> [54] </ref>. The use of non-standard abstract semantic interpretation was used for type recovery in Scheme by Olin Shivers [66]. Type inference in object-oriented languages in particular has been studied for many years [69, 31]. These early approaches produced flow-insensitive information, limiting their usefulness for optimization.
Reference: [55] <author> E. Mohr, D. Kranz, and R. Halstead Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the Conference on Lisp and Functional Languages. ACM SIGPLAN, </booktitle> <year> 1990. </year>
Reference-contexts: Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization [10, 20, 32, 46, 50, 64]. Applicable work for procedural languages includes efficient futures and task creation <ref> [74, 48, 55] </ref> and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency. Programmers are currently penalized by using OOP and COOP styles and features.
Reference: [56] <author> D. A. Moon. </author> <title> Object-Oriented Programming with Flavors. </title> <booktitle> SIGPLAN Notices (Proc. Intl. Conf. on Object-Oriented Programming: Systems, Languages, and Applications (OOPSLA)), </booktitle> <volume> 21(11), </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: Then I will describe current research in OOP optimization and COOP optimization. Finally, I will cover specific work related to the sort of global analysis and transformation I am proposing. 3.1 Languages and Execution Models Object-oriented programming was initially popularized in the United States by Smalltalk [30] and Flavors <ref> [56] </ref>. More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors [37, 23, 3, 2].
Reference: [57] <author> Stephan Murer, Jerome A. Feldman, Chu-Cheow Lim, and Martina-Maria Seidel. psather: </author> <title> Layered extensions to an object-oriented language for efficient parallel computation. </title> <type> Technical Report TR-93-028, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA, </address> <month> June </month> <year> 1993 </year> <month> November </month> <year> 1993. </year>
Reference-contexts: Further, none of this systems has developed a global optimization framework, probably owing to a desire to leverage existing C++ compiler technology. The other group is data parallel C++, represented by pC++ [50] C** [49], and pSather <ref> [57] </ref>. In these languages, the operations specified a single thread of computation may be executed on disjoint 3 data without interaction.
Reference: [58] <author> T. Ng, X. Zhang, V. Karamcheti, and A. A. Chien. </author> <title> Parallel macromolecular dynamics on the Concert System. </title> <note> In Submitted for publication, </note> <year> 1994. </year>
Reference-contexts: Loops anumerical sequential efficiency benchmark * Richards and Delta Blue - OO programs used in evaluation of Self * Network Simulations circuit and queuing network simulators * Graph/Hierarchical dynamic Mandelbrot and traveling salesman codes * Space-based Applications a particle-in-cell code and an ion simulation Additional large applications may be included <ref> [58] </ref>. 6.2 Characterization of Test Suite In order to understand the test suite and the effects of the optimizations for each application I will measure: * average function size * degree of polymorphism both for functions and for instance variables of classes * depth of inheritance hierarchy * amount of "true
Reference: [59] <author> N. Oxhtj, J. Palsberg, and M. Schwartzbach. </author> <title> Making type inference practical. </title> <booktitle> In Proceedings of OOPSLA '92, </booktitle> <year> 1992. </year>
Reference-contexts: Type inference in object-oriented languages in particular has been studied for many years [69, 31]. These early approaches produced flow-insensitive information, limiting their usefulness for optimization. Constraint-based type inference is described by Palsberg and Schwartzbach in <ref> [60, 59] </ref>. Their approach was limited to a single level of flow-sensitivity and motivated my efforts to develop an extendible inference approach. Recently Agesen has extended the basic one level approach to handle the features of SELF [73] (see [1]).
Reference: [60] <author> J. Palsberg and M. Schwartzbach. </author> <title> Object-oriented type inference. </title> <booktitle> In Proceedings of OOPSLA '91, </booktitle> <pages> pages 146-61, </pages> <year> 1991. </year>
Reference-contexts: Type inference in object-oriented languages in particular has been studied for many years [69, 31]. These early approaches produced flow-insensitive information, limiting their usefulness for optimization. Constraint-based type inference is described by Palsberg and Schwartzbach in <ref> [60, 59] </ref>. Their approach was limited to a single level of flow-sensitivity and motivated my efforts to develop an extendible inference approach. Recently Agesen has extended the basic one level approach to handle the features of SELF [73] (see [1]).
Reference: [61] <author> John Plevyak and Andrew Chien. </author> <title> Efficient cloning to eliminate dynamic dispatch in object-oriented languages. </title> <note> Submitted for Publication, </note> <year> 1994. </year>
Reference-contexts: Data transformations include the creation of concrete types (specialization of objects with respect to data layout and concrete type of instance variables), and inter-object locality and concurrency transformations. The latter require the creation of additional concrete types for each involved class. Creation of concrete types is described in <ref> [61] </ref>. Methods must be specialized for concrete types and the special versions must be used universally. In addition, methods can be specialized with respect to any forward data flow information. <p> In general, this requires cloning (replicating and specializing) the methods along the call path to ensure that it is unique. Cloning of methods requires special techniques for dynamic dispatch and for limiting the amount of code expansion. Cloning is discussed in detail in <ref> [61] </ref>. 5.4 Enabled OOP Optimizations The global information provided by the analysis enables other local transformations targeted at the problems illustrated in Section 4.2. <p> Other members of the group, various outside individuals and I have produced a number of programs, producing a interesting and diverse test suite. Within this framework I have developed a concrete type inference system [62] (Section 5.2), cloning techniques <ref> [61] </ref> (Section 5.3) and some local optimization techniques for COOP languages [63] (Section 5.5). I am currently involved in extending the analysis to a larger class of data flow problems, including locality information and implementing stack-based execution techniques.
Reference: [62] <author> John Plevyak and Andrew A. Chien. </author> <title> Precise concrete type inference of object-oriented programs. </title> <booktitle> In Proceedings of OOPSLA, </booktitle> <year> 1994. </year>
Reference-contexts: A global analysis is needed to simultaneous solve the data and control flow problems since these problems are intertwined by dynamic dispatch where concrete type (a data flow problem) to affects control flow. I have designed such a flow sensitive global analysis <ref> [62] </ref>. The analysis handles dynamic dispatch by abstract interpretation of message sends. <p> Since splitting can be applied to extract more precise information for any forward global data flow problem, this analysis provides a framework for general method specialization. My concrete type inference algorithm is described in detail in <ref> [62] </ref>. 7 5.3 Global Transformations Global transformations are those which require global coordination, and fall into two categories. Data transformations specialize data layout or differentiate objects within a class. Code transformations specialize methods. Data transformations are done first since they can require code transformations. <p> Other members of the group, various outside individuals and I have produced a number of programs, producing a interesting and diverse test suite. Within this framework I have developed a concrete type inference system <ref> [62] </ref> (Section 5.2), cloning techniques [61] (Section 5.3) and some local optimization techniques for COOP languages [63] (Section 5.5). I am currently involved in extending the analysis to a larger class of data flow problems, including locality information and implementing stack-based execution techniques.
Reference: [63] <author> John Plevyak, Xingbin Zhang, and Andrew A. Chien. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of the ACM Symposium on the Principles of Programming Languages, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: Some of these optimizations are described in <ref> [63] </ref>. 8 6 Test of the Thesis The thesis and approach presented in Section 5 will be evaluated by implementation and empirical evaluation on a suite of programs. Since the thesis requires a proof of efficiency, in this section I will first define efficient. <p> Within this framework I have developed a concrete type inference system [62] (Section 5.2), cloning techniques [61] (Section 5.3) and some local optimization techniques for COOP languages <ref> [63] </ref> (Section 5.5). I am currently involved in extending the analysis to a larger class of data flow problems, including locality information and implementing stack-based execution techniques.
Reference: [64] <author> R. J. Smith, </author> <title> II. Experimental systems kit final project report. </title> <type> Technical Report ACT-ESP-077-91, </type> <institution> Microelectronics and Computer Technology Corporation (MCC), Austin, Texas., </institution> <year> 1991. </year>
Reference-contexts: The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives. Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization <ref> [10, 20, 32, 46, 50, 64] </ref>. Applicable work for procedural languages includes efficient futures and task creation [74, 48, 55] and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency.
Reference: [65] <author> A. D. Samples, D. Ungar, and P. Hilfinger. </author> <title> Soar: Smalltalk without bytecodes. </title> <booktitle> In OOPSLA '86 Prodeedings, </booktitle> <pages> pages 107-18, </pages> <month> September </month> <year> 1986. </year> <month> 14 </month>
Reference-contexts: This idea was extended by Calder and Grunwald [11] through `if conversion', essentially a static version of polymorphic inline caches [39]. However, this approach does not address the effects of high call frequency and lost optimization opportunities. Other approaches concentrate on type annotations [45] or special hardware <ref> [65] </ref>. However, the first does not solve the inefficiencies resulting from polymorphism, while the second is infeasible in a practical sense. The work done for the Self [73] and Cecil [19] languages is perhaps the most well known.
Reference: [66] <author> Olin Shivers. </author> <title> Topics in Advanced Language Implementation, chapter Data-Flow Analysis and Type Recovery in Scheme, </title> <address> pages 47-88. </address> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: The two main approaches to type inference of object-oriented programs are the data flow approach and the Hindley-Milner approach [54]. The use of non-standard abstract semantic interpretation was used for type recovery in Scheme by Olin Shivers <ref> [66] </ref>. Type inference in object-oriented languages in particular has been studied for many years [69, 31]. These early approaches produced flow-insensitive information, limiting their usefulness for optimization. Constraint-based type inference is described by Palsberg and Schwartzbach in [60, 59].
Reference: [67] <author> K. Smith and R. Smith II. </author> <title> The experimental systems project at the microelectronics and computer technology corporation. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Computers, </booktitle> <year> 1989. </year>
Reference-contexts: The other large body of COOP research has centered around C++. This work can be separated into two groups. ESKit C++ <ref> [67] </ref>, Mentat [32], CHARM++ [46], and Compositional C++ [20] are all medium-grained explicitly parallel languages where the user controls grain size. The techniques considered in this thesis are targeted at enabling the system to control execution grain size for nominally fine-grained languages.
Reference: [68] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison Wesley, </publisher> <address> second edition, </address> <year> 1991. </year>
Reference-contexts: Finally, I will cover specific work related to the sort of global analysis and transformation I am proposing. 3.1 Languages and Execution Models Object-oriented programming was initially popularized in the United States by Smalltalk [30] and Flavors [56]. More recently, C++ <ref> [68] </ref> has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors [37, 23, 3, 2].
Reference: [69] <author> Norihisa Suzuki. </author> <title> Inferring types in Smalltalk. </title> <booktitle> In Eighth Symposium on Principles of Programming Languages, </booktitle> <pages> pages 187-199, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: The use of non-standard abstract semantic interpretation was used for type recovery in Scheme by Olin Shivers [66]. Type inference in object-oriented languages in particular has been studied for many years <ref> [69, 31] </ref>. These early approaches produced flow-insensitive information, limiting their usefulness for optimization. Constraint-based type inference is described by Palsberg and Schwartzbach in [60, 59]. Their approach was limited to a single level of flow-sensitivity and motivated my efforts to develop an extendible inference approach.
Reference: [70] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An efficient implementation scheme of concurrent object-oriented languages on stock multicomputers. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year>
Reference-contexts: In order to make a thorough evaluation and to enable direct comparisons with other work I have selected a test suite which includes standard sequential procedural benchmarks [53], OOP benchmarks [18, 38] and COOP benchmarks <ref> [70] </ref>. To these I have added a representative selection of applications developed as part of the Concert project [21]. <p> A number of different systems based on this model have been created [52, 9, 5, 76, 43] most concentrating on the study of various language features. The work most similar in scope and concern for performance to that proposed for my thesis has been done on ABCL <ref> [75, 70, 76] </ref> which I will cover in more detail below. The other large body of COOP research has centered around C++. This work can be separated into two groups. <p> However, their focus was not on extensive compile-time global transformation. In particular, efficient synchronization <ref> [70] </ref> and locality [71] optimization efforts focused on runtime techniques. The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives.
Reference: [71] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> Incorporating locality management and GC in massively-prallel object-oriented languages. </title> <booktitle> In Proceedings of the Joint Symposium on Parallel Processing, </booktitle> <pages> pages 277-282, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, their focus was not on extensive compile-time global transformation. In particular, efficient synchronization [70] and locality <ref> [71] </ref> optimization efforts focused on runtime techniques. The Ellie system [6] proposes some grain-size tuning features, however, as of recently [7] the user must supply explicit migration and locality directives.
Reference: [72] <institution> Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, MA 02154-1264. </address> <booktitle> The Connection Machine CM-5 Technical Summary, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: This machine will be characterized by comparing simulated instruction counts to real instruction counts and execution speeds. Runtime system overhead will be based on instruction counts and timing from the TMC CM5 <ref> [72] </ref> and/or Cray T3D [25]. Absolute OOP efficiency will be tested using real workstation and/or instruction and cache-level simulated workstations.
Reference: [73] <author> David Ungar and Randall B. Smith. </author> <title> Self: The power of simplicity. </title> <booktitle> In Proceedings of OOPSLA '87, </booktitle> <pages> pages 227-41. </pages> <booktitle> ACM SIGPLAN, </booktitle> <publisher> ACM Press, </publisher> <year> 1987. </year>
Reference-contexts: Other approaches concentrate on type annotations [45] or special hardware [65]. However, the first does not solve the inefficiencies resulting from polymorphism, while the second is infeasible in a practical sense. The work done for the Self <ref> [73] </ref> and Cecil [19] languages is perhaps the most well known. Chambers and Ungar [15] started with the idea of customization, whereby separate versions of methods are created for each receiver type. <p> Constraint-based type inference is described by Palsberg and Schwartzbach in [60, 59]. Their approach was limited to a single level of flow-sensitivity and motivated my efforts to develop an extendible inference approach. Recently Agesen has extended the basic one level approach to handle the features of SELF <ref> [73] </ref> (see [1]). However, the problems with precision and cost inherent in a single pass approach are tackled by exploiting specialized knowledge about the SELF language.
Reference: [74] <author> David B. Wagner and Bradley G. Calder. </author> <title> Leapfrogging: A portable technique for implementing efficient futures. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 208-217, </pages> <year> 1993. </year>
Reference-contexts: Parallel C++ projects, owing perhaps to a desire to leverage existing compiler technology, have concentrated on efficient run times with little or no special compiler optimization [10, 20, 32, 46, 50, 64]. Applicable work for procedural languages includes efficient futures and task creation <ref> [74, 48, 55] </ref> and thread shipping [44, 13] with many contributors, but also focuses on actions taken at run time. 4 Problem The central problem I will address is that of efficiency. Programmers are currently penalized by using OOP and COOP styles and features.
Reference: [75] <author> M. Yasugi, S. Matsuoka, and A. Yonezawa. ABCL/onEM-4: </author> <title> A new software/hardware architecture for object-oriented concurrent computing on an extended dataflow supercomputer. </title> <booktitle> In Proceedings of the ACM Conference on Supercomputing '92, </booktitle> <year> 1992. </year>
Reference-contexts: A number of different systems based on this model have been created [52, 9, 5, 76, 43] most concentrating on the study of various language features. The work most similar in scope and concern for performance to that proposed for my thesis has been done on ABCL <ref> [75, 70, 76] </ref> which I will cover in more detail below. The other large body of COOP research has centered around C++. This work can be separated into two groups.
Reference: [76] <editor> Akinori Yonezawa, editor. </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: More recently, C++ [68] has become very popular for general purpose programming in industry. The COOP programming model with which my thesis is concerned is based on Actors [37, 23, 3, 2]. A number of different systems based on this model have been created <ref> [52, 9, 5, 76, 43] </ref> most concentrating on the study of various language features. The work most similar in scope and concern for performance to that proposed for my thesis has been done on ABCL [75, 70, 76] which I will cover in more detail below. <p> A number of different systems based on this model have been created [52, 9, 5, 76, 43] most concentrating on the study of various language features. The work most similar in scope and concern for performance to that proposed for my thesis has been done on ABCL <ref> [75, 70, 76] </ref> which I will cover in more detail below. The other large body of COOP research has centered around C++. This work can be separated into two groups. <p> However, that work does not handle polymorphic instance variables which are common in object-oriented codes. 3.5 COOP Optimization Work on efficient COOP implementations includes that done for ABCL <ref> [76, 77] </ref> and Concurrent Smalltalk (CST) [41, 42]. However, their focus was not on extensive compile-time global transformation. In particular, efficient synchronization [70] and locality [71] optimization efforts focused on runtime techniques. <p> Methods may invoke methods on several other objects concurrently, waiting on the responses only when required by data flow or the programmer. In this way, the programmer can safely and conveniently compose larger parallel abstractions and entire programs. A number of languages share this model <ref> [22, 42, 51, 76] </ref>. The model has three features: * a shared name space (location independence), * dynamic thread creation, and * object level access control. A shared name space allows programmers to separate data layout and functional correctness.
Reference: [77] <author> Akinori Yonezawa, Satoshi Matsuoka, Masahiro Yasugi, and Kenjiro Taura. </author> <title> Implementing concurrent object-oriented languages on multicomputers. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <pages> pages 49-61, </pages> <month> May </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: However, that work does not handle polymorphic instance variables which are common in object-oriented codes. 3.5 COOP Optimization Work on efficient COOP implementations includes that done for ABCL <ref> [76, 77] </ref> and Concurrent Smalltalk (CST) [41, 42]. However, their focus was not on extensive compile-time global transformation. In particular, efficient synchronization [70] and locality [71] optimization efforts focused on runtime techniques. <p> Even the best implementations incur tens to hundreds of instructions for each method invocation <ref> [42, 77] </ref> due to the cost of managing a distributed memory (method invocations are location independent) and managing concurrency (locks). As a result, the cost of sending a message or checking the status of an object's lock can easily exceed the work inside a method invocation.
References-found: 77

