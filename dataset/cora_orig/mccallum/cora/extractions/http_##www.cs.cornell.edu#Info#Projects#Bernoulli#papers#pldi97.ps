URL: http://www.cs.cornell.edu/Info/Projects/Bernoulli/papers/pldi97.ps
Refering-URL: http://www.cs.cornell.edu/Info/Projects/Bernoulli/
Root-URL: http://www.cs.cornell.edu
Email: fprakas,ahmed,pingalig@cs.cornell.edu  
Title: Data-centric Multi-level Blocking  
Author: Induprakas Kodukula, Nawaaz Ahmed, and Keshav Pingali 
Address: Ithaca, NY 14853.  
Affiliation: Department of Computer Science, Cornell University,  
Abstract: We present a simple and novel framework for generating blocked codes for high-performance machines with a memory hierarchy. Unlike traditional compiler techniques like tiling, which are based on reasoning about the control flow of programs, our techniques are based on reasoning directly about the flow of data through the memory hierarchy. Our data-centric transformations permit a more direct solution to the problem of enhancing data locality than current control-centric techniques do, and generalize easily to multiple levels of memory hierarchy. We buttress these claims with performance numbers for standard benchmarks from the problem domain of dense numerical linear algebra. The simplicity and intuitive appeal of our approach should make it attractive to compiler writers as well as to library writers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ramesh C. Agarwal and Fred G. Gustavson. </author> <title> Algorithm and Architecture Aspects of Producing ESSL BLAS on POWER2. </title>
Reference-contexts: On the positive side, these results, coupled with careful analysis of the compiler-generated code, show that the compiler-generated code has the right block structure. What remains is to make the compilation of inner loops (by unrolling inner loops, prefetching data and doing scalar replacement <ref> [1, 25] </ref>) more effective in the IBM compiler. using Householder reflections. The input code has poor performance, and it is improved somewhat by blocking. The blocked code was generated by blocking only columns of the matrix, since dependences prevent complete two-dimensional blocking of the array being factored.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Dem-mel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> editors. LAPACK Users' Guide. Second Edition. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1995. </year>
Reference-contexts: Some of these programs are discussed in Section 2. The rest of this paper is organized as follows. In Section 3, we discuss current solutions to the problem of developing software with good data reuse, including hand-crafted libraries like LAPACK <ref> [2] </ref>, and automatic compiler techniques such as tiling [24]. The technology described in this paper was motivated by the limitations of these approaches, and is introduced in Section 4. <p> The well-known LA-PACK library contains block matrix algorithms implemented on top of the BLAS routines, and is written for good data reuse on a machine with a two-level memory hierarchy <ref> [2] </ref>. The LAPACK library has been successful in practice. However, it requires a set of machine-specific, hand-coded BLAS routines to run well. Since it is not a general-purpose tool, it cannot be used outside the realm of the dense numerical linear algebra.
Reference: [3] <author> Jennifer Anderson, Saman Amarsinghe, and Mon-ica Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> Jun </month> <year> 1995. </year>
Reference-contexts: Of course, nothing prevents us from reshaping the physical data array if the cost of converting back and forth from a standard representation is tolerable. Physical data reshaping has been explored by other researchers <ref> [11, 3] </ref>. Upto this point, we have assumed that every statement in the program contains a reference to the array being blocked by the data shackle. Although this assumption in valid for kernels like matrix multiplication and Cholesky factorization, it is obviously not true in general programs.
Reference: [4] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Workshop on Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Therefore, automatic program restructuring tools that promote data reuse through transformations provide an attractive alternative. The restructuring compiler community has devoted much attention to the development of such technology. The most important transformation is Mike Wolfe's iteration space tiling [24], preceded by linear loop transformations if necessary <ref> [4, 17, 23] </ref>. This approach is restricted to perfectly nested loops, although it can be extended to imperfectly nested loops if they are first transformed into perfectly nested loops.
Reference: [5] <author> David Bau, Induprakas Kodukula, Vladimir Kotl--yar, Keshav Pingali, and Paul Stodghil. </author> <title> Solving alignment using elementary linear algebra. </title> <booktitle> In Proceedings of the 7th LCPC Workshop, </booktitle> <month> August </month> <year> 1994. </year> <note> Also available as Cornell Computer Science Dept. tech report TR95-1478. </note>
Reference-contexts: This is clearly an issue that we need to revisit in the future, and we plan to use tools we developed for automatic data alignment to address this problem more carefully <ref> [5] </ref>. 6 Products of shackles We now show that there is a natural notion of taking the Cartesian product of a set of shackles.
Reference: [6] <author> Pierre Boulet, Alain Darte, Tanguy Risset, and Yves Robert. </author> <title> (Pen)-ultimate tiling? In INTEGRATION, </title> <journal> the VLSI Journal, </journal> <volume> volume 17, </volume> <pages> pages 33-51. </pages> <year> 1994. </year>
Reference-contexts: However, it is unclear how a compiler can discover automatically the right sequence of transformations to perform; it is also unclear whether this approach can be generalized for a machine with a multi-level memory hierarchy. Finally, there is a large body of work on determining good tile sizes <ref> [6, 14, 20, 22, 12] </ref>. This research focuses on perfectly nested loops with uniform dependences (i.e. dependence vectors can be represented as distances).
Reference: [7] <author> Steve Carr and K. Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: Interactions between loop jamming/distribution and linear loop transformations have been studied by McKinley et al [18]. However, no systematic procedure for exploring these options for obtaining perfectly nested loops from imperfectly nested loops is known. A somewhat different approach has been taken by Carr and Kennedy <ref> [7] </ref>. By doing a detailed study of matrix factorization codes in the LAPACK library, they came up with a list of transformations that must be performed to get code competitive with LAPACK code. These include strip-mine-and-interchange, preceded by index-set-splitting and loop distribution to make the interchange legal [8]. <p> These include strip-mine-and-interchange, preceded by index-set-splitting and loop distribution to make the interchange legal [8]. Additional transformations such as unroll-and-jam <ref> [7] </ref> and scalar replacement are performed on this code to obtain code competitive with hand-blocked codes used in conjunction with BLAS [8].
Reference: [8] <author> Steve Carr and R. B. Lehoucq. </author> <title> Compiler blockabil-ity of dense matrix factorizations. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <month> Oct </month> <year> 1996. </year>
Reference-contexts: By doing a detailed study of matrix factorization codes in the LAPACK library, they came up with a list of transformations that must be performed to get code competitive with LAPACK code. These include strip-mine-and-interchange, preceded by index-set-splitting and loop distribution to make the interchange legal <ref> [8] </ref>. Additional transformations such as unroll-and-jam [7] and scalar replacement are performed on this code to obtain code competitive with hand-blocked codes used in conjunction with BLAS [8]. <p> These include strip-mine-and-interchange, preceded by index-set-splitting and loop distribution to make the interchange legal <ref> [8] </ref>. Additional transformations such as unroll-and-jam [7] and scalar replacement are performed on this code to obtain code competitive with hand-blocked codes used in conjunction with BLAS [8]. However, it is unclear how a compiler can discover automatically the right sequence of transformations to perform; it is also unclear whether this approach can be generalized for a machine with a multi-level memory hierarchy.
Reference: [9] <author> Steven Carr and R. B. Lehoucq. </author> <title> A compiler-blockable algorithm for QR decomposition, </title> <year> 1994. </year>
Reference-contexts: The compiler-generated code uses the same algorithm as the "pointwise" algorithm for this problem; the LAPACK code on the other hand uses domain-specific information about the associativity of Householder reflections to generate a fully-blocked version of this algorithm <ref> [9] </ref>. IBM SP-2 the Gmtry kernel which is a SPEC benchmark kernel from Dnasa7. This code performs Gaussian elimination across rows, without pivoting. Data shackling blocked the array in both dimensions, and produced code similar to what we obtained in Cholesky factorization. <p> QR-factorization using Householder reflections is an example [15]. It is unclear to us whether a compiler could or even should attempt to restructure programs using this kind of domain-specific information. It is likely that the most plausible scenario is compiler blocking augmented with programmer directives for blocking such codes <ref> [9] </ref>. Acknowledgments: We would like to thank Rob Schreiber, Charlie van Loan, Vladimir Kotlyar, Paul Feautrier and Sanjay Rajopadhye for stimulating discussions on block matrix algorithms and restructuring compilers. This paper was much improved by the feedback we received from an anonymous "good shepherd" on the PLDI committee.
Reference: [10] <author> L. Carter, J. Ferrante, and S. Flynn Hummel. </author> <title> Hierarchical tiling for improved superscalar performance. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: The array is traversed repeatedly till all instances are performed. Determination of good block sizes can also be tricky, especially for a multi-level memory hierarchy <ref> [10] </ref>. This problem arises even in handwritten code | in this context, library writers are exploring the use of training sets to help library code determine good block sizes [13]. We can adopt this solution if it proves to be successful.
Reference: [11] <author> Michael Cierniak and Wei Li. </author> <title> Unifying data and control transformations for distributed shared memory machines. </title> <booktitle> In SIGPLAN 1995 conference on Programming Languages Design and Implementation, </booktitle> <month> Jun </month> <year> 1995. </year>
Reference-contexts: Of course, nothing prevents us from reshaping the physical data array if the cost of converting back and forth from a standard representation is tolerable. Physical data reshaping has been explored by other researchers <ref> [11, 3] </ref>. Upto this point, we have assumed that every statement in the program contains a reference to the array being blocked by the data shackle. Although this assumption in valid for kernels like matrix multiplication and Cholesky factorization, it is obviously not true in general programs.
Reference: [12] <author> Stephanie Coleman and Kathryn S. McKinley. </author> <title> Tile size selection using cache organization and data layout. </title> <editor> In David W. Wall, editor, </editor> <booktitle> ACM SIGPLAN '95 Conference on Programming Language Design and Implementation (PLDI), volume 30(6) of ACM SIGPLAN Notices, </booktitle> <pages> pages 279-290, </pages> <address> New York, NY, USA, June 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: However, it is unclear how a compiler can discover automatically the right sequence of transformations to perform; it is also unclear whether this approach can be generalized for a machine with a multi-level memory hierarchy. Finally, there is a large body of work on determining good tile sizes <ref> [6, 14, 20, 22, 12] </ref>. This research focuses on perfectly nested loops with uniform dependences (i.e. dependence vectors can be represented as distances).
Reference: [13] <author> Jim Demmel. </author> <type> Personal communication, </type> <month> Sep </month> <year> 1996. </year>
Reference-contexts: Determination of good block sizes can also be tricky, especially for a multi-level memory hierarchy [10]. This problem arises even in handwritten code | in this context, library writers are exploring the use of training sets to help library code determine good block sizes <ref> [13] </ref>. We can adopt this solution if it proves to be successful. We have presented data shackling as an alternative restructuring technology that avoids some of the problems of current control-centric approaches. However, it is unclear to us whether our approach can fully subsume loop transformation techniques.
Reference: [14] <author> Jack Dongarra and Robert Schreiber. </author> <title> Automatic blocking of nested loops. </title> <type> Technical Report UT-CS-90-108, </type> <institution> Department of Computer Science, University of Tennessee, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: However, it is unclear how a compiler can discover automatically the right sequence of transformations to perform; it is also unclear whether this approach can be generalized for a machine with a multi-level memory hierarchy. Finally, there is a large body of work on determining good tile sizes <ref> [6, 14, 20, 22, 12] </ref>. This research focuses on perfectly nested loops with uniform dependences (i.e. dependence vectors can be represented as distances).
Reference: [15] <author> Gene Golub and Charles Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1996. </year>
Reference-contexts: In some cases, it is necessary to develop new algorithms which exploit structure in the underlying problem to reuse data effectively; a well-known example of this is Bischof and van Loan's WY algorithm for QR factorization with Householder reflections, which was developed explicitly for improving data reuse in orthogonal factorizations <ref> [15] </ref>. Even when existing algorithms are sufficient, reorganizing a program to reuse data effectively can increase its size by orders of magnitude, and make the program less abstract and less portable by introducing machine dependencies. <p> The three outer loops enumerate the iteration space tiles, while the three inner loops enumerate the iteration space points within a tile. In this case, iteration space tiling produces the same code as the equivalent block matrix code <ref> [15] </ref>. Tiling can be applied to imperfectly nested loops if these loops are converted to perfectly nested loops through the use of code sinking [25]. Code sinking moves all statements into the innermost loop, inserting appropriate guards to ensure that these statements are executed the right number of times. <p> Using polyhedral algebra tools, we obtain the code in Figure 7. In this code, data shackling re-groups the iteration space into four sections as shown in Figure 8. Initially, all updates to the diagonal block from the left are performed (Figure 8 (i)), followed by a baby Cholesky factorization <ref> [15] </ref> of the diagonal block (Figure 8 (ii)). For each off-diagonal block, updates from the left (Figure 8 (iii)) are followed by interleaved scaling of the columns of the block by the diagonal block, and local updates (Figure 8 (iv)). <p> Since both these shackles are legal, their Cartesian product (in either order) is legal. It can be shown that one order gives a fully-blocked left-looking Cholesky, identical to the blocked Cholesky algorithm in <ref> [15] </ref>, while the other order gives a fully-blocked right-looking Cholesky. 6.2 Discussion Taking the Cartesian product of data shackles gives us finer control over data accesses in the blocked code. As discussed earlier, shackling just one reference in matrix multiplication (say C [I,J]) does not constrain all the data accesses. <p> Since shackling takes no position on how the remapped data is stored, the techniques described in Section 4 can be used to generate code even when the underlying data structure is reshaped. A good example of this is banded Cholesky factorization <ref> [15] </ref>. <p> In the event that both these approaches are required for program restructuring, an important open question is to determine the interaction between them. Finally, we note that there are programs for which handwritten blocked codes exploit algebraic properties of matrices. QR-factorization using Householder reflections is an example <ref> [15] </ref>. It is unclear to us whether a compiler could or even should attempt to restructure programs using this kind of domain-specific information. It is likely that the most plausible scenario is compiler blocking augmented with programmer directives for blocking such codes [9].
Reference: [16] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <address> Santa Clara, Cal-ifornia, </address> <month> April 8-11, </month> <year> 1991. </year> <journal> ACM SIGARCH, SIG-PLAN, SIGOPS, and the IEEE Computer Society. </journal>
Reference-contexts: This research focuses on perfectly nested loops with uniform dependences (i.e. dependence vectors can be represented as distances). While this work is not directly comparable to ours, the detailed memory models used in some of this research <ref> [18, 22, 16] </ref> are useful in general for estimating program performance. 4 Data-centric Transformations Since the goal of program transformation is to enhance data reuse and reduce data movement through the memory hierarchy, it would seem advantageous to have a tool that orchestrates data movement directly, rather than as a side-effect
Reference: [17] <author> W. Li and K. Pingali. </author> <title> Access Normalization: Loop restructuring for NUMA compilers. </title> <journal> ACM Transactions on Computer Systems, </journal> <year> 1993. </year>
Reference-contexts: Therefore, automatic program restructuring tools that promote data reuse through transformations provide an attractive alternative. The restructuring compiler community has devoted much attention to the development of such technology. The most important transformation is Mike Wolfe's iteration space tiling [24], preceded by linear loop transformations if necessary <ref> [4, 17, 23] </ref>. This approach is restricted to perfectly nested loops, although it can be extended to imperfectly nested loops if they are first transformed into perfectly nested loops. <p> We assume that all array access functions are linear functions of loop variables (if the functions are affine, we drop the constant terms); if so, they can be written as F flI where F is the data access matrix <ref> [17] </ref> and I is the vector of iteration space variables of loops surrounding this data reference. Theorem 2 For a given statement S, let F 1 ; : : : ; F n be the access matrices for the shackled data references in this statement.
Reference: [18] <author> Kathryn S. McKinley, Steve Carr, and Chau-Wen Tseng. </author> <title> Improving data locality with loop transformations. </title> <journal> In ACM Transactions on Programming Languages and Systems, </journal> <volume> volume 18, </volume> <pages> pages 424-453. </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Interactions between loop jamming/distribution and linear loop transformations have been studied by McKinley et al <ref> [18] </ref>. However, no systematic procedure for exploring these options for obtaining perfectly nested loops from imperfectly nested loops is known. A somewhat different approach has been taken by Carr and Kennedy [7]. <p> This research focuses on perfectly nested loops with uniform dependences (i.e. dependence vectors can be represented as distances). While this work is not directly comparable to ours, the detailed memory models used in some of this research <ref> [18, 22, 16] </ref> are useful in general for estimating program performance. 4 Data-centric Transformations Since the goal of program transformation is to enhance data reuse and reduce data movement through the memory hierarchy, it would seem advantageous to have a tool that orchestrates data movement directly, rather than as a side-effect <p> X (i,k) -= X (i-1,k)*A (i,k)/B (i-1,k) enddo do k = 1, n enddo enddo (i) Input code do t1 = 1, n S1: X (t2+1,t1)-=X (t2,t1)*A (t2+1,t1)/B (t2,t1) enddo enddo (ii) Transformed Code speeded up by a factor of 2. et al in their study of locality improving transformations <ref> [18] </ref>. This code was produced from FORTRAN-90 by a scalarizer. The traditional iteration-centric approach to obtain good locality in this code is to fuse the two k loops first, after which the outer i loop and the fused k loop are interchanged to obtain unit-stride accesses in the inner loop. <p> When there are multiple data shackles that are legal for a program, we need a way to determine the best one. This requires accurate cost models for the memory hierarchy, such as the ones developed by other researchers in this area <ref> [18, 22] </ref>. If the search space becomes large, heuristics may be useful to cut down the size of the search.
Reference: [19] <author> W. Pugh. </author> <title> A practical algorithm for exact array dependency analysis. </title> <journal> Comm. of the ACM, </journal> <volume> 35(8):102, </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Fortunately, since the conditionals are affine conditions on surrounding loop bounds, they can be simplified using any polyhedral algebra tool. We have used the Omega calculator <ref> [19] </ref> to produce the code shown in Figure 6.
Reference: [20] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling multidimensional iteration spaces for multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(2) </volume> <pages> 108-120, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: However, it is unclear how a compiler can discover automatically the right sequence of transformations to perform; it is also unclear whether this approach can be generalized for a machine with a multi-level memory hierarchy. Finally, there is a large body of work on determining good tile sizes <ref> [6, 14, 20, 22, 12] </ref>. This research focuses on perfectly nested loops with uniform dependences (i.e. dependence vectors can be represented as distances).
Reference: [21] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In SIGPLAN89 conference on Programming Languages, Design and Implementation, </booktitle> <month> Jun </month> <year> 1989. </year>
Reference-contexts: N do K = 1 .. N ((b2-1)*25 &lt; J &lt;= b2*25) matrix multiply The code in Figure 5 is not very efficient, and is similar to runtime resolution code generated when shared-memory programs are compiled for distributed-memory machines <ref> [21] </ref>. Fortunately, since the conditionals are affine conditions on surrounding loop bounds, they can be simplified using any polyhedral algebra tool. We have used the Omega calculator [19] to produce the code shown in Figure 6.
Reference: [22] <author> Vivek Sarkar. </author> <title> Automatic selection of high order transformations in the IBM ASTI optimizer. </title> <type> Technical Report ADTI-96-004, </type> <institution> Application Development Technology Institute, IBM Software Solutions Division, </institution> <month> July </month> <year> 1996. </year> <note> Submitted to special issue of IBM Journal of Research and Development. </note>
Reference-contexts: However, it is unclear how a compiler can discover automatically the right sequence of transformations to perform; it is also unclear whether this approach can be generalized for a machine with a multi-level memory hierarchy. Finally, there is a large body of work on determining good tile sizes <ref> [6, 14, 20, 22, 12] </ref>. This research focuses on perfectly nested loops with uniform dependences (i.e. dependence vectors can be represented as distances). <p> This research focuses on perfectly nested loops with uniform dependences (i.e. dependence vectors can be represented as distances). While this work is not directly comparable to ours, the detailed memory models used in some of this research <ref> [18, 22, 16] </ref> are useful in general for estimating program performance. 4 Data-centric Transformations Since the goal of program transformation is to enhance data reuse and reduce data movement through the memory hierarchy, it would seem advantageous to have a tool that orchestrates data movement directly, rather than as a side-effect <p> When there are multiple data shackles that are legal for a program, we need a way to determine the best one. This requires accurate cost models for the memory hierarchy, such as the ones developed by other researchers in this area <ref> [18, 22] </ref>. If the search space becomes large, heuristics may be useful to cut down the size of the search.
Reference: [23] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In SIGPLAN 1991 conference on Programming Languages Design and Implementation, </booktitle> <month> Jun </month> <year> 1991. </year>
Reference-contexts: Therefore, automatic program restructuring tools that promote data reuse through transformations provide an attractive alternative. The restructuring compiler community has devoted much attention to the development of such technology. The most important transformation is Mike Wolfe's iteration space tiling [24], preceded by linear loop transformations if necessary <ref> [4, 17, 23] </ref>. This approach is restricted to perfectly nested loops, although it can be extended to imperfectly nested loops if they are first transformed into perfectly nested loops. <p> Turning this around, we see that when a block becomes current, we perform all instances of statement S for which the reference R accesses data in that block. Therefore, this reference enjoys perfect self-temporal locality <ref> [23] </ref>. Considering all shackled references together, we see that we also have perfect group-temporal locality for this set of references; of course, references outside this set may not necessarily enjoy group-temporal locality with respect to this set.
Reference: [24] <author> M. Wolfe. </author> <title> Iteration space tiling for memory hierarchies. </title> <booktitle> In Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> December </month> <year> 1987. </year>
Reference-contexts: Some of these programs are discussed in Section 2. The rest of this paper is organized as follows. In Section 3, we discuss current solutions to the problem of developing software with good data reuse, including hand-crafted libraries like LAPACK [2], and automatic compiler techniques such as tiling <ref> [24] </ref>. The technology described in this paper was motivated by the limitations of these approaches, and is introduced in Section 4. This technology differs from standard restructuring compiler technology like tiling because it is based on reasoning about the data flow rather than the control flow of the program. <p> Therefore, automatic program restructuring tools that promote data reuse through transformations provide an attractive alternative. The restructuring compiler community has devoted much attention to the development of such technology. The most important transformation is Mike Wolfe's iteration space tiling <ref> [24] </ref>, preceded by linear loop transformations if necessary [4, 17, 23]. This approach is restricted to perfectly nested loops, although it can be extended to imperfectly nested loops if they are first transformed into perfectly nested loops. <p> Loops that carry reuse are moved as far inside the loop nest as possible by using linear loop transformations; if two or more inner loops carry reuse and they are fully permutable, these loops are tiled <ref> [24] </ref>. Intuitively, tiling improves performance by interleaving iterations of the tiled loops, which exploits data reuse in all those loops rather than in just the innermost one. It is easy to verify that all the three loops in the matrix multiplication code carry reuse and are fully permutable.
Reference: [25] <author> M. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1995. </year>
Reference-contexts: In this case, iteration space tiling produces the same code as the equivalent block matrix code [15]. Tiling can be applied to imperfectly nested loops if these loops are converted to perfectly nested loops through the use of code sinking <ref> [25] </ref>. Code sinking moves all statements into the innermost loop, inserting appropriate guards to ensure that these statements are executed the right number of times. <p> Let S1 write to an array location in iteration J w of the J loop, and let S2 read from that location in iteration (J r ; I r ) of the J and I loops. A flow dependence exists if the following linear inequalities have an integer solution <ref> [25] </ref>: 8 &gt; &gt; &gt; &gt; : J r = J w ; I r = J w (same location) N J w 1 (loop bounds) N J r 1 (loop bounds) N I r J r + 1 (loop bounds) J r J w (read after write) (1) Next, we <p> This is standard <ref> [25] </ref>. * Formulate the predicate M (S2,i2)M (S1,i1). * The conjunction of these conditions does not have an integer solution. Proof: Obvious, hence omitted. fl 5.3 Discussion Viewing blocking as a remapping of data co-ordinates simplifies the development of the legality test. <p> On the positive side, these results, coupled with careful analysis of the compiler-generated code, show that the compiler-generated code has the right block structure. What remains is to make the compilation of inner loops (by unrolling inner loops, prefetching data and doing scalar replacement <ref> [1, 25] </ref>) more effective in the IBM compiler. using Householder reflections. The input code has poor performance, and it is improved somewhat by blocking. The blocked code was generated by blocking only columns of the matrix, since dependences prevent complete two-dimensional blocking of the array being factored.
References-found: 25

