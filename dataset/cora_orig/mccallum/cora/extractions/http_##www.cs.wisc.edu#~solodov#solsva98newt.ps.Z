URL: http://www.cs.wisc.edu/~solodov/solsva98newt.ps.Z
Refering-URL: http://www.cs.wisc.edu/~solodov/solodov.html
Root-URL: 
Title: Reformulation: Nonsmooth, Piecewise Smooth, Semismooth and Smoothing Methods, A Globally Convergent Inexact Newton Method for
Author: M. Fukushima and L. Qi M. V. Solodov and B. F. Svaiter 
Keyword: Key Words nonlinear equations, Newton method, proximal point method, projection method, global convergence, superlinear convergence.  
Note: pp. 355-369 Edited by  c fl1998 Kluwer Academic Publishers  
Abstract: We propose an algorithm for solving systems of monotone equations which combines Newton, proximal point, and projection methodologies. An important property of the algorithm is that the whole sequence of iterates is always globally convergent to a solution of the system without any additional regularity assumptions. Moreover, under standard assumptions the local su-perlinear rate of convergence is achieved. As opposed to classical globalization strategies for Newton methods, for computing the stepsize we do not use line-search aimed at decreasing the value of some merit function. Instead, linesearch in the approximate Newton direction is used to construct an appropriate hy-perplane which separates the current iterate from the solution set. This step is followed by projecting the current iterate onto this hyperplane, which ensures global convergence of the algorithm. Computational cost of each iteration of our method is of the same order as that of the classical damped Newton method. The crucial advantage is that our method is truly globally convergent. In particular, it cannot get trapped in a stationary point of a merit function. The presented algorithm is motivated by the hybrid projection-proximal point method proposed in [25]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Armijo. </author> <title> Minimization of functions having Lipschitz continuous first partial derivatives. </title> <journal> Pacific Journal of Mathematics, </journal> <volume> 16 </volume> <pages> 1-3, </pages> <year> 1966. </year>
Reference-contexts: The most common globalization strategy is the damped Newton method x new = x + ffd which employs a linesearch procedure along the Newton direction d to compute the stepsize ff &gt; 0. This linesearch is typically based on an Armijo-type <ref> [1] </ref> sufficient descent condition for some merit function, usually the squared 2-norm merit function f (x) := kF (x)k 2 : Sometimes also the damped Gauss-Newton method is used, where the search direction d is obtained by solving the regularized linear system (rF (x) &gt; rF (x) + I)d = rF
Reference: [2] <author> D.P. Bertsekas. </author> <title> Nonlinear programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, Mas-sachusetts, </address> <year> 1995. </year>
Reference-contexts: Systems of monotone equations arise in various applications. One important example is subproblems in the generalized proximal algorithms with Bregman distances [9, 7]. Among numerous algorithms for solving systems of equations, the Newton method and its variants are of particular importance <ref> [16, 5, 19, 2] </ref>.
Reference: [3] <author> J.V. Burke and Maijian Qian. </author> <title> The variable metric proximal point algorithm, I: Basic convergence theory, 1996. </title> <institution> Department of Mathematics, University of Washington, </institution> <address> Seattle, WA. </address>
Reference-contexts: classical proximal point method [22] (of course, we are talking about the general context of operator equations here) is that the condition imposed on " k in the inexact proximal step is significantly less restrictive (and more constructive) than the corresponding tolerance requirements in the standard proximal point settings (see <ref> [22, 3] </ref>) : 1 X k &lt; 1 k" k k k k ky k x k k; k=0 Note that in the classical approach one further sets x k+1 := y k to obtain the next iterate.
Reference: [4] <author> R.S. Dembo, S.C Eisenstat, and T. Steihaug. </author> <title> Inexact Newton methods. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 19 </volume> <pages> 400-408, </pages> <year> 1982. </year>
Reference-contexts: Under the assumptions of differentiability and nonsingularity, we prove superlinear convergence of the inexact version of the algorithm, similar to the classical inexact Newton method <ref> [4] </ref>. Our algorithm is motivated, to some extent, by the hybrid projection-proximal point method proposed in [25] in the more general context of finding zeroes of set-valued maximal monotone operators in a Hilbert space. Let T be a maximal monotone operator on a real Hilbert space H. <p> We allow this Newton-type linear equation to be solved approximately (see Algorithm 2.1), much in the spirit of inexact Newton methods (see <ref> [4, 26, 14, 17, 15] </ref>). This feature is of particular importance for large-scale problems. Because a full step in the obtained Newton direction may not satisfy the tolerance conditions imposed on solving the proximal subproblems in Algorithm 1.1, we cannot immediately perform the projection step.
Reference: [5] <author> J.E. Dennis and R.B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year> <note> 368 REFORMULATION </note>
Reference-contexts: Systems of monotone equations arise in various applications. One important example is subproblems in the generalized proximal algorithms with Bregman distances [9, 7]. Among numerous algorithms for solving systems of equations, the Newton method and its variants are of particular importance <ref> [16, 5, 19, 2] </ref>.
Reference: [6] <author> J. Eckstein and D.P. Bertsekas. </author> <title> On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators. </title> <journal> Mathematical Programming, </journal> <volume> 55 </volume> <pages> 293-318, </pages> <year> 1992. </year>
Reference-contexts: Given a current iterate x k and a regularization parameter k &gt; 0, consider the proximal point subproblem (see <ref> [22, 6, 8, 12, 13] </ref>), The first step consists of solving the linearization of this subproblem at the point x k 0 = F (x k ) + G k (x x k ) + k (x x k ); where G k is a positive semidefinite matrix (this is similar
Reference: [7] <author> J. Eckstein and M.C. Ferris. </author> <title> Smooth methods of multipliers for complementarity problems. </title> <type> Technical Report RRR 27-96, </type> <institution> Rutgers Center for Operations Research, Rutgers University, </institution> <address> New Brunswick, New Jersey, </address> <month> August </month> <year> 1996. </year> <note> Revised February 1997. </note>
Reference-contexts: Systems of monotone equations arise in various applications. One important example is subproblems in the generalized proximal algorithms with Bregman distances <ref> [9, 7] </ref>. Among numerous algorithms for solving systems of equations, the Newton method and its variants are of particular importance [16, 5, 19, 2].
Reference: [8] <author> M.C. Ferris. </author> <title> Finite termination of the proximal point algorithm. </title> <journal> Mathematical Programming, </journal> <volume> 50 </volume> <pages> 359-366, </pages> <year> 1991. </year>
Reference-contexts: Given a current iterate x k and a regularization parameter k &gt; 0, consider the proximal point subproblem (see <ref> [22, 6, 8, 12, 13] </ref>), The first step consists of solving the linearization of this subproblem at the point x k 0 = F (x k ) + G k (x x k ) + k (x x k ); where G k is a positive semidefinite matrix (this is similar
Reference: [9] <author> A.N. Iusem and M.V. Solodov. </author> <title> Newton-type methods with generalized distances for constrained optimization. </title> <journal> Optimization, </journal> <volume> 41 </volume> <pages> 257-278, </pages> <year> 1997. </year>
Reference-contexts: Systems of monotone equations arise in various applications. One important example is subproblems in the generalized proximal algorithms with Bregman distances <ref> [9, 7] </ref>. Among numerous algorithms for solving systems of equations, the Newton method and its variants are of particular importance [16, 5, 19, 2]. <p> consists of solving the linearization of this subproblem at the point x k 0 = F (x k ) + G k (x x k ) + k (x x k ); where G k is a positive semidefinite matrix (this is similar to standard Newton-proximal point approaches, for example, <ref> [9] </ref>). We allow this Newton-type linear equation to be solved approximately (see Algorithm 2.1), much in the spirit of inexact Newton methods (see [4, 26, 14, 17, 15]). This feature is of particular importance for large-scale problems.
Reference: [10] <author> H. Jiang, L. Qi, X. Chen, and D. Sun. </author> <title> Semismoothness and superlinear copnvergence in nonsmooth optimization and nonsmooth equations. </title> <editor> In G. Di Pillo and F. Giannessi, editors, </editor> <booktitle> Nonlinear Optimization and Applications, </booktitle> <pages> pages 197-212. </pages> <publisher> Plenum Press, </publisher> <year> 1996. </year>
Reference-contexts: If the starting point is sufficiently close to some solution x of (1.1) where rF (x) is nonsingular, the sequence generated by the Newton method converges superlinearly or quadratically, depending on further assumptions. Similar local results hold in the more general case of semismooth equations (see <ref> [20, 21, 18, 15, 10] </ref>). Having in mind the problem under consideration, it is worth to mention that monotonicity of F is not needed for such local analysis. To enlarge the domain of convergence of the Newton method, some global-ization strategy has to be used.
Reference: [11] <author> H. Jiang and D. Ralph. </author> <title> Global and local superlinear convergence analysis of Newton-type methods for semismooth equations with smooth least squares. </title> <institution> Department of Mathematics, The University of Melbourne, Aus-tralia. </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: Note that in the case of semismooth equations, AN INEXACT NEWTON METHOD 357 some additional assumptions are needed to apply the linesearch globalization strategy (see <ref> [11] </ref>). To motivate the development of our algorithm, we emphasize the following drawbacks of damped Newton and damped Gauss-Newton methods : Either method can only ensure that all accumulation points of the generated sequence of iterates are stationary points of the merit function f (x).
Reference: [12] <author> B. Lemaire. </author> <title> The proximal algorithm. </title> <editor> In J.P. Penot, editor, </editor> <booktitle> International Series of Numerical Mathematics, </booktitle> <pages> pages 73-87. </pages> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1989. </year>
Reference-contexts: Given a current iterate x k and a regularization parameter k &gt; 0, consider the proximal point subproblem (see <ref> [22, 6, 8, 12, 13] </ref>), The first step consists of solving the linearization of this subproblem at the point x k 0 = F (x k ) + G k (x x k ) + k (x x k ); where G k is a positive semidefinite matrix (this is similar
Reference: [13] <author> F.J. Luque. </author> <title> Asymptotic convergence analysis of the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 22 </volume> <pages> 277-293, </pages> <year> 1984. </year>
Reference-contexts: Given a current iterate x k and a regularization parameter k &gt; 0, consider the proximal point subproblem (see <ref> [22, 6, 8, 12, 13] </ref>), The first step consists of solving the linearization of this subproblem at the point x k 0 = F (x k ) + G k (x x k ) + k (x x k ); where G k is a positive semidefinite matrix (this is similar
Reference: [14] <author> J.M. Martnez. </author> <title> Local convergence theory for inexact Newton methods based on structural least-squares updates. </title> <journal> Mathematics of Computation, </journal> <volume> 55 </volume> <pages> 143-168, </pages> <year> 1990. </year>
Reference-contexts: We allow this Newton-type linear equation to be solved approximately (see Algorithm 2.1), much in the spirit of inexact Newton methods (see <ref> [4, 26, 14, 17, 15] </ref>). This feature is of particular importance for large-scale problems. Because a full step in the obtained Newton direction may not satisfy the tolerance conditions imposed on solving the proximal subproblems in Algorithm 1.1, we cannot immediately perform the projection step.
Reference: [15] <author> J.M. Martnez and L. Qi. </author> <title> Inexact Newton methods for solving nonsmoooth equations. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 60 </volume> <pages> 127-145, </pages> <year> 1995. </year>
Reference-contexts: If the starting point is sufficiently close to some solution x of (1.1) where rF (x) is nonsingular, the sequence generated by the Newton method converges superlinearly or quadratically, depending on further assumptions. Similar local results hold in the more general case of semismooth equations (see <ref> [20, 21, 18, 15, 10] </ref>). Having in mind the problem under consideration, it is worth to mention that monotonicity of F is not needed for such local analysis. To enlarge the domain of convergence of the Newton method, some global-ization strategy has to be used. <p> We allow this Newton-type linear equation to be solved approximately (see Algorithm 2.1), much in the spirit of inexact Newton methods (see <ref> [4, 26, 14, 17, 15] </ref>). This feature is of particular importance for large-scale problems. Because a full step in the obtained Newton direction may not satisfy the tolerance conditions imposed on solving the proximal subproblems in Algorithm 1.1, we cannot immediately perform the projection step.
Reference: [16] <author> J.M. Ortega and W.C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: Systems of monotone equations arise in various applications. One important example is subproblems in the generalized proximal algorithms with Bregman distances [9, 7]. Among numerous algorithms for solving systems of equations, the Newton method and its variants are of particular importance <ref> [16, 5, 19, 2] </ref>.
Reference: [17] <author> J.-S. Pang and S.A. Gabriel. </author> <title> An inexact NE/SQP method for solving the nonlinear complementarity problem. </title> <journal> Computational Optimization and Applications, </journal> <volume> 1 </volume> <pages> 67-92, </pages> <year> 1992. </year>
Reference-contexts: We allow this Newton-type linear equation to be solved approximately (see Algorithm 2.1), much in the spirit of inexact Newton methods (see <ref> [4, 26, 14, 17, 15] </ref>). This feature is of particular importance for large-scale problems. Because a full step in the obtained Newton direction may not satisfy the tolerance conditions imposed on solving the proximal subproblems in Algorithm 1.1, we cannot immediately perform the projection step.
Reference: [18] <author> J.-S. Pang and L. Qi. </author> <title> Nonsmooth equations : Motivation and algorithms. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3 </volume> <pages> 443-465, </pages> <year> 1995. </year>
Reference-contexts: If the starting point is sufficiently close to some solution x of (1.1) where rF (x) is nonsingular, the sequence generated by the Newton method converges superlinearly or quadratically, depending on further assumptions. Similar local results hold in the more general case of semismooth equations (see <ref> [20, 21, 18, 15, 10] </ref>). Having in mind the problem under consideration, it is worth to mention that monotonicity of F is not needed for such local analysis. To enlarge the domain of convergence of the Newton method, some global-ization strategy has to be used.
Reference: [19] <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Systems of monotone equations arise in various applications. One important example is subproblems in the generalized proximal algorithms with Bregman distances [9, 7]. Among numerous algorithms for solving systems of equations, the Newton method and its variants are of particular importance <ref> [16, 5, 19, 2] </ref>. <p> It is also easy to verify that x + is the projection of x onto the halfspace fs 2 &lt; n j hF (y); s yi 0g. Since x belongs to this halfspace, it follows from the basic properties of the projection operator (see <ref> [19, p.121] </ref>) that hx x + ; x + xi 0.
Reference: [20] <author> L. Qi. </author> <title> Convergence analysis of some algorithms for solving nonsmooth equations. </title> <journal> Mathematics of Operations Research, </journal> <volume> 18 </volume> <pages> 227-244, </pages> <year> 1993. </year>
Reference-contexts: If the starting point is sufficiently close to some solution x of (1.1) where rF (x) is nonsingular, the sequence generated by the Newton method converges superlinearly or quadratically, depending on further assumptions. Similar local results hold in the more general case of semismooth equations (see <ref> [20, 21, 18, 15, 10] </ref>). Having in mind the problem under consideration, it is worth to mention that monotonicity of F is not needed for such local analysis. To enlarge the domain of convergence of the Newton method, some global-ization strategy has to be used.
Reference: [21] <author> L. Qi and J. Sun. </author> <title> A nonsmooth version of Newton's method. </title> <journal> Mathematical Programming, </journal> <volume> 58 </volume> <pages> 353-367, </pages> <year> 1993. </year> <title> AN INEXACT NEWTON METHOD 369 </title>
Reference-contexts: If the starting point is sufficiently close to some solution x of (1.1) where rF (x) is nonsingular, the sequence generated by the Newton method converges superlinearly or quadratically, depending on further assumptions. Similar local results hold in the more general case of semismooth equations (see <ref> [20, 21, 18, 15, 10] </ref>). Having in mind the problem under consideration, it is worth to mention that monotonicity of F is not needed for such local analysis. To enlarge the domain of convergence of the Newton method, some global-ization strategy has to be used.
Reference: [22] <author> R.T. Rockafellar. </author> <title> Monotone operators and the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 14 </volume> <pages> 877-898, </pages> <year> 1976. </year>
Reference-contexts: Separation arguments show that the distance to the solution set for thus constructed sequence monotonically decreases, which essentially ensures global convergence of the algorithm. The advantage of Algorithm 1.1 over the classical proximal point method <ref> [22] </ref> (of course, we are talking about the general context of operator equations here) is that the condition imposed on " k in the inexact proximal step is significantly less restrictive (and more constructive) than the corresponding tolerance requirements in the standard proximal point settings (see [22, 3]) : 1 X <p> classical proximal point method [22] (of course, we are talking about the general context of operator equations here) is that the condition imposed on " k in the inexact proximal step is significantly less restrictive (and more constructive) than the corresponding tolerance requirements in the standard proximal point settings (see <ref> [22, 3] </ref>) : 1 X k &lt; 1 k" k k k k ky k x k k; k=0 Note that in the classical approach one further sets x k+1 := y k to obtain the next iterate. <p> Given a current iterate x k and a regularization parameter k &gt; 0, consider the proximal point subproblem (see <ref> [22, 6, 8, 12, 13] </ref>), The first step consists of solving the linearization of this subproblem at the point x k 0 = F (x k ) + G k (x x k ) + k (x x k ); where G k is a positive semidefinite matrix (this is similar
Reference: [23] <author> M.V. Solodov and B.F. Svaiter. </author> <title> A new projection method for variational inequality problems. </title> <type> Technical Report B-109, </type> <institution> Instituto de Matematica Pura e Aplicada, Estrada Dona Castorina 110, Jardim Bot^anico, Rio de Janeiro, </institution> <address> RJ 22460, Brazil, </address> <month> November </month> <year> 1996. </year> <note> SIAM Journal on Control and Optimization, submitted. </note>
Reference-contexts: Moreover, the (linear) regularized Newton equation itself can be solved inexactly (see Algorithm 2.1). Because such step may fail to generate an appropriate separating hyperplane (i.e. the tolerance requirements of Algorithm 1.1 need not be met), a linesearch procedure similar to the one in <ref> [23] </ref> is employed. Finally, as in Algorithm 1.1, a projection step is made. This hybrid algorithm is globally convergent to a solution of the system of equations, provided one exists, under no assumptions on F other than continuity and monotonicity (see Theorem 2.1). <p> of the method is a linesearch procedure in the Newton direction d k (see Algorithm 2.1) which computes a point y k = x k + ff k d k such that 0 &lt; hF (y k ); x k y k i: A similar linesearch technique was used in <ref> [23] </ref>.
Reference: [24] <author> M.V. Solodov and B.F. Svaiter. </author> <title> Forcing strong convergence of proximal point iterations in a Hilbert space, </title> <note> 1997. Mathematical Programming, submitted. </note>
Reference-contexts: We refer the readers to [25] for complete analysis of Algorithm 1.1 and a more detailed comparison with the classical proximal point method. Another related work using inexact proximal iterations with relative error bounded away from zero is <ref> [24] </ref>. Now let us go back to our problem (1.1). To ensure global convergence to a solution, one can apply Algorithm 1.1, setting T = F . However, it is clear that this approach, without modifications, is not quite practical for the following AN INEXACT NEWTON METHOD 359 reasons.
Reference: [25] <author> M.V. Solodov and B.F. Svaiter. </author> <title> A hybrid projection proximal point algorithm. </title> <type> Technical Report B-115, </type> <institution> Instituto de Matematica Pura e Aplicada, Estrada Dona Castorina 110, Jardim Bot^anico, Rio de Janeiro, </institution> <address> RJ 22460, Brazil, </address> <month> January </month> <year> 1997. </year> <note> Journal of Convex Analysis, submitted. </note>
Reference-contexts: Under the assumptions of differentiability and nonsingularity, we prove superlinear convergence of the inexact version of the algorithm, similar to the classical inexact Newton method [4]. Our algorithm is motivated, to some extent, by the hybrid projection-proximal point method proposed in <ref> [25] </ref> in the more general context of finding zeroes of set-valued maximal monotone operators in a Hilbert space. Let T be a maximal monotone operator on a real Hilbert space H. And consider, for a moment, the problem of finding an x 2 H such that 0 2 T (x). <p> Let T be a maximal monotone operator on a real Hilbert space H. And consider, for a moment, the problem of finding an x 2 H such that 0 2 T (x). Algorithm 1.1 (Hybrid Projection-Proximal Point Method) <ref> [25] </ref> Choose any x 0 2 H and 2 [0; 1); set k := 0. Inexact proximal step. <p> If problem 0 2 T (x) has a solution and the sequence f k g is bounded above, then the generated sequence fx k g either is finite and terminates at a solution, or it is infinite and converges (weakly) to a solution (for complete properties of the method, see <ref> [25] </ref>). The idea of Algorithm 1.1 is to use an approximate proximal iteration to construct a hyperplane H k := fx 2 &lt; n j hv k ; x y k i = 0g; which separates the current iterate x k from the solutions of 0 2 T (x). <p> Thus Algorithm 1.1 has better robustness features while preserving computational costs and convergence properties of the classical proximal point method (computational costs are actually expected to be reduced, thanks to the relaxed tolerance requirements). We refer the readers to <ref> [25] </ref> for complete analysis of Algorithm 1.1 and a more detailed comparison with the classical proximal point method. Another related work using inexact proximal iterations with relative error bounded away from zero is [24]. Now let us go back to our problem (1.1).
Reference: [26] <author> T.J. Ypma. </author> <title> Local convergence of inexact Newton methods. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 21 </volume> <pages> 583-590, </pages> <year> 1984. </year>
Reference-contexts: We allow this Newton-type linear equation to be solved approximately (see Algorithm 2.1), much in the spirit of inexact Newton methods (see <ref> [4, 26, 14, 17, 15] </ref>). This feature is of particular importance for large-scale problems. Because a full step in the obtained Newton direction may not satisfy the tolerance conditions imposed on solving the proximal subproblems in Algorithm 1.1, we cannot immediately perform the projection step.
References-found: 26

