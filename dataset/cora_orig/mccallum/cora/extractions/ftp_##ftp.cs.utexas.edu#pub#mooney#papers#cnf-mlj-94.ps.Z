URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/cnf-mlj-94.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: mooney@cs.utexas.edu  
Author: Editor: Dennis Kibler 
Keyword: concept induction, experimental comparison, CNF, DNF, decision trees  
Affiliation: Department of Computer Sciences, University of Texas,  
Address: Netherlands.  Austin, TX 78712  
Note: Machine Learning, 1-15 c Kluwer Academic Publishers, Boston. Manufactured in The  RAYMOND J. MOONEY  
Pubnum: Technical Note  
Abstract: Encouraging Experimental Results on Abstract. This paper presents results comparing three simple inductive learning systems using different representations for concepts, namely: CNF formulae, DNF formulae, and decision trees. The CNF learner performs surprisingly well. Results on five natural data sets indicates that it frequently trains faster and produces more accurate and simpler concepts. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Monterey, CA: </address> <publisher> Wadsworth and Brooks. </publisher>
Reference: <author> Buntine, W., & Niblett, T. </author> <year> (1992). </year> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 75-86. </pages>
Reference: <author> Clark P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference: <author> Fayyad, </author> <title> U.M., & Irani, K.B. (1992). On handling of continuous-valued attributes in decision-tree generation. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 87-102. </pages>
Reference: <author> Michalski, R.S. </author> <year> (1975). </year> <title> Synthesis of optimal and quasi-optimal variable valued logic formulas. </title> <booktitle> Proceedings of the 1975 International Symposium on Multiple-Valued Logic (pp. </booktitle> <pages> 76-87). </pages> <note> Bloom-ington, IN. </note>
Reference-contexts: MOONEY 2. Learning Algorithms 2.1. DNF Learner The DNF learner is a propositional version of Foil called PFoil. Foil is a system for learning first-order Horn clauses; however, the basic algorithm is a heuristic covering algorithm for learning DNF similar to AQ <ref> (Michalski, 1975) </ref> or GREEDY3 (Pagallo & Haussler, 1990). Pseudocode for PFoil is shown below: 1. PFOIL (Pos, Neg) /*Inputs are positive and negative examples*/ 2. Let DNF be empty; 3. Until Pos is empty do: 4. Set Term to empty, Neg2 to Neg, and Pos2 to Pos; 5. <p> If a test example doesn't match any of the category descriptions, it is assigned to overall most common category, or to the negative category, if one exists. To help ensure that PFoil was a representative DNF learner, we ran several experiments comparing it to a comparable version of AQ <ref> (Michalski, 1975) </ref>. In almost all cases, there was no statistically significant difference in predictive accuracy; however, AQ generally took significantly longer to run.
Reference: <author> Michalski, R.S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. In R.S. Michalski, </title> <publisher> J.G. </publisher>
Reference-contexts: Pagallo and Haussler (1990) discuss similar duplication resulting from representing DNF formulae as decision trees. It should be noted that using intervals on linear features and internal disjunction (allowing disjunctions of values for the same feature in the "literals" of a DNF formula) <ref> (Michalski, 1983) </ref>, can help relieve the replication problem for DNF. However, most of the clauses in the sample CNF formulae are not amenable to either of these approaches. Unlike DNF formulae, decision trees can "share" conceptual structure across multiple disjuncts and avoid some of the duplication problems discussed above.
Reference: <editor> Carbonell, & T.M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Michalski, R.S., & Chilausky, S. </author> <year> (1980). </year> <title> Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of EXPERIMENTAL RESULTS ON LEARNING CNF 15 developing an expert system for soybean disease diagnosis. </title> <journal> Journal of Policy Analysis and Information Systems, </journal> <volume> 4, </volume> <pages> 126-161. </pages>
Reference-contexts: Data Sets This section briefly describes the data sets used in the experiments. All of them are available through the UCI repository of machine learning databases (Murphy, 1993). The soybean data set is the original data for soybean disease diagnosis <ref> (Michalski & Chilausky, 1980) </ref>. It contains 630 examples of 15 different diseases described using 35 features in which 5.1% of the feature values are missing. A few features can be viewed as discrete-valued numerical attributes; however, these are simply treated as nominal in the current experiments.
Reference: <author> Muggleton, S. </author> <year> (1987). </year> <title> Duce, an oracle-based approach to constructive induction. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 287-292). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: A standard representation that can completely avoid replication is multi-level Horn-clause theories. By appropriately introducing new intermediate terms, replication can be prevented. Inverse resolution operators like intra-construction and inter-construction <ref> (Muggleton, 1987) </ref> introduce new terms in order to remove redundancy and to compact concept representations. However, there has been little research on efficient, oracle-free methods for inducing multi-level Horn-clause theories. The symmetric Fringe and DCFringe algorithms discussed above are possible exceptions but typically require many passes through the data.
Reference: <author> Murphy, P.M., & Aha, D.W. </author> <year> (1993). </year> <title> UCI repository of machine learning databases (machine-readable data repository at ics.uci.edu). </title> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: Data Sets This section briefly describes the data sets used in the experiments. All of them are available through the UCI repository of machine learning databases <ref> (Murphy, 1993) </ref>. The soybean data set is the original data for soybean disease diagnosis (Michalski & Chilausky, 1980). It contains 630 examples of 15 different diseases described using 35 features in which 5.1% of the feature values are missing.
Reference: <author> Noordewier, M.O., Towell, G.G., & Shavlik, J.W. </author> <year> (1991). </year> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <booktitle> In Advances in neural information processing systems, </booktitle> <volume> Vol. </volume> <pages> 3. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Half of these are examples of a gene-starting sequence called a promoter (Towell, Shavlik, & Noordewier, 1990). There are no missing values. The splice-junction data set is another DNA sequence database <ref> (Noordewier, Towell, & Shavlik, 1991) </ref>. It consists of 3190 sequences of 60 nucleotides each. Only 0.03% of the feature values are missing. There are three categories. Two of them represent different boundary regions between protein-coding sections (exons) and non-coding sections (introns) of a DNA sequence.
Reference: <author> Pagallo, G. </author> <year> (1990). </year> <title> Adaptive decision tree algorithms for learning from examples. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Sciences, University of California, Santa, Cruz. </institution>
Reference-contexts: MOONEY 2. Learning Algorithms 2.1. DNF Learner The DNF learner is a propositional version of Foil called PFoil. Foil is a system for learning first-order Horn clauses; however, the basic algorithm is a heuristic covering algorithm for learning DNF similar to AQ (Michalski, 1975) or GREEDY3 <ref> (Pagallo & Haussler, 1990) </ref>. Pseudocode for PFoil is shown below: 1. PFOIL (Pos, Neg) /*Inputs are positive and negative examples*/ 2. Let DNF be empty; 3. Until Pos is empty do: 4. Set Term to empty, Neg2 to Neg, and Pos2 to Pos; 5. Until Neg2 is empty do: 6. <p> Unlike DNF formulae, decision trees can "share" conceptual structure across multiple disjuncts and avoid some of the duplication problems discussed above. However, decision trees have their own replication problem in which subtrees must be duplicated in order to represent certain disjunctions <ref> (Pagallo & Haussler, 1990) </ref>. An examination of a number of decision trees produced for the current data sets revealed a few, small, duplicated subtrees. However, replication was more clearly indicated by the repeated use of the same features in different parts of the trees. 12 R. MOONEY Table 2. <p> Related Research As previously mentioned, there has been very little experimental research on learning CNF. Some extensions to the Fringe <ref> (Pagallo & Haussler, 1990) </ref> and Citre (Matheus & Rendell, 1989) work on constructive induction in decision trees include feature-construction heuristics appropriate for learning CNF.
Reference: <author> Pagallo, G. & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 71-100. </pages>
Reference-contexts: MOONEY 2. Learning Algorithms 2.1. DNF Learner The DNF learner is a propositional version of Foil called PFoil. Foil is a system for learning first-order Horn clauses; however, the basic algorithm is a heuristic covering algorithm for learning DNF similar to AQ (Michalski, 1975) or GREEDY3 <ref> (Pagallo & Haussler, 1990) </ref>. Pseudocode for PFoil is shown below: 1. PFOIL (Pos, Neg) /*Inputs are positive and negative examples*/ 2. Let DNF be empty; 3. Until Pos is empty do: 4. Set Term to empty, Neg2 to Neg, and Pos2 to Pos; 5. Until Neg2 is empty do: 6. <p> Unlike DNF formulae, decision trees can "share" conceptual structure across multiple disjuncts and avoid some of the duplication problems discussed above. However, decision trees have their own replication problem in which subtrees must be duplicated in order to represent certain disjunctions <ref> (Pagallo & Haussler, 1990) </ref>. An examination of a number of decision trees produced for the current data sets revealed a few, small, duplicated subtrees. However, replication was more clearly indicated by the repeated use of the same features in different parts of the trees. 12 R. MOONEY Table 2. <p> Related Research As previously mentioned, there has been very little experimental research on learning CNF. Some extensions to the Fringe <ref> (Pagallo & Haussler, 1990) </ref> and Citre (Matheus & Rendell, 1989) work on constructive induction in decision trees include feature-construction heuristics appropriate for learning CNF.
Reference: <author> Pitt, L., & Valiant, L.G. </author> <year> (1988). </year> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35, </volume> <pages> 965-984. </pages>
Reference: <author> Porter, B., Bareiss, R., & Holte, R. </author> <year> (1990). </year> <title> Concept learning and heuristic classification in weak-theory domains. </title> <journal> Artificial Intelligence, </journal> <volume> 45, </volume> <pages> 229-263. </pages>
Reference-contexts: PFoil and PFoil-CNF were run with Democrats as positive and Republicans as negative. Similar results were obtained when Republicans were considered positive. The audiology data set consists of hearing-disorder cases from the Baylor College of Medicine <ref> (Porter, Bareiss, & Holte, 1990) </ref> in Quinlan's standardized form available from the UCI repository. There are 226 examples of 24 categories of hearing disorders using 69 features in which 2% of the feature values are missing.
Reference: <author> Quinlan, J.R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Very little experimental research has been done on learning conjunctive normal form (CNF). This paper presents empirical results comparing simple CNF, DNF, and decision-tree learners on five natural data sets. The decision-tree system is a version of ID3 <ref> (Quinlan, 1986) </ref>, the DNF system is a propositional version of Foil (Quinlan, 1990), and the CNF system is the logical dual of the Foil system. In the domains tested, the CNF learner consistently obtains greater or equal classification accuracy and generally runs faster and produces less complex concepts.
Reference: <author> Quinlan, J.R. </author> <year> (1989). </year> <title> Unknown attribute values in induction. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 164-168). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference-contexts: Very little experimental research has been done on learning conjunctive normal form (CNF). This paper presents empirical results comparing simple CNF, DNF, and decision-tree learners on five natural data sets. The decision-tree system is a version of ID3 (Quinlan, 1986), the DNF system is a propositional version of Foil <ref> (Quinlan, 1990) </ref>, and the CNF system is the logical dual of the Foil system. In the domains tested, the CNF learner consistently obtains greater or equal classification accuracy and generally runs faster and produces less complex concepts. <p> The metric computes the total information gained regarding the current positive examples, which is given by the number of them that satisfy the literal multiplied by the information gained regarding each one of them <ref> (Quinlan, 1990) </ref>. The current system only handles discrete-valued features, in which a "literal" is a specific feature-value pair, such as color=red, pregnant=true, or pregnant=false. No special processing is used to handle missing values.
Reference: <author> Schlimmer, J.C. </author> <year> (1987). </year> <title> Concept acquisition through representational adjustment. </title> <type> Doctoral dissertation, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: A few features can be viewed as discrete-valued numerical attributes; however, these are simply treated as nominal in the current experiments. The congressional voting data set contains records from the U.S. House of Representatives from the year 1984 <ref> (Schlimmer, 1987) </ref>. It consists of 435 examples from EXPERIMENTAL RESULTS ON LEARNING CNF 5 two classes (Democrat and Republican) with data from 15 key votes in which 5.8% of the feature values are missing.
Reference: <author> Shavlik, J.W., Mooney, R.J., & Towell, G.G. </author> <year> (1991). </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 111-143. </pages>
Reference-contexts: Half of these are examples of a gene-starting sequence called a promoter (Towell, Shavlik, & Noordewier, 1990). There are no missing values. The splice-junction data set is another DNA sequence database <ref> (Noordewier, Towell, & Shavlik, 1991) </ref>. It consists of 3190 sequences of 60 nucleotides each. Only 0.03% of the feature values are missing. There are three categories. Two of them represent different boundary regions between protein-coding sections (exons) and non-coding sections (introns) of a DNA sequence.
Reference: <author> Towell, G.G., Shavlik, J.W., & Noordewier, M.O. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based artificial neural networks. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 861-866). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: The promoter data set consists of 106 DNA sequences, each represented as a string of 57 nucleotides (one of A, G, T, or C). Half of these are examples of a gene-starting sequence called a promoter <ref> (Towell, Shavlik, & Noordewier, 1990) </ref>. There are no missing values. The splice-junction data set is another DNA sequence database (Noordewier, Towell, & Shavlik, 1991). It consists of 3190 sequences of 60 nucleotides each. Only 0.03% of the feature values are missing. There are three categories.
Reference: <author> Valiant, L.G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 27, </volume> <pages> 1134-1142. </pages>
Reference-contexts: CNF Learner There has been a noticeable absence of heuristic algorithms for learning CNF. The standard PAC algorithm for learning kCNF <ref> (Valiant, 1984) </ref> requires a fixed limit on the size of clauses (k) and its time complexity is exponential in k. Therefore, it is not practical for most realistic problems. The current experiments employ a logical dual of PFoil called PFoil-CNF, pseudocode for which is shown below: 1 1.
Reference: <author> Yang, D., Rendell, L., & Blix, G. </author> <year> (1991). </year> <title> A scheme for feature construction and a comparison of empirical methods. </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 699-704). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
References-found: 23

