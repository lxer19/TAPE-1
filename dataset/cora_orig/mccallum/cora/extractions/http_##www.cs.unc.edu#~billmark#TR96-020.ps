URL: http://www.cs.unc.edu/~billmark/TR96-020.ps
Refering-URL: http://www.cs.unc.edu/~gb/bishop_publications.html
Root-URL: http://www.cs.unc.edu
Title: (with author list and acknowledgements added). Abstract  
Author: William R. Mark, Gary Bishop, Leonard McMillan 
Note: Public Release on  
Date: January 10, 1996  Nov. 25, 1996  
Abstract: Post-Rendering Image Warping for Latency Compensation UNC-CH Computer Science Technical Report #96-020 Systems that provide remote viewing of three-dimensional data with interactive viewpoint control must confront two key problems: latency and bandwidth. The straightforward approach of transmitting and displaying rendered images results in a delay of one round-trip between viewpoint change and the corresponding change in the displayed image. We avoid this delay by transmitting a representation of the scene to the users machine, which then locally closes the viewpoint-to-display loop. If the scene representation is geometry-based, the bandwidth, userside storage, and userside graphics rendering capability required for updates to the scene are unbounded. We show that an image-based representation can allow for arbitrary scene changes, while requiring only fixed bandwidth, storage, and rendering power. We demonstrate a system that renders images on a rendering server, and then transmits them to the users machine where image warping using per-pixel disparity values compensates for system latency and synthesizes stereo images for display. We also develop enhancements to the warping technique that improve its quality and speed. 
Abstract-found: 1
Intro-found: 1
Reference: [Agrawala95] <author> M. Agrawala, A. C. Beers, and N. Chaddha, </author> <title> Model-Based Motion Estimation for Synthetic Animations, </title> <booktitle> ACM Multimedia 1995, </booktitle> <address> (San Francisco, CA) November 5-9, </address> <year> 1995. </year>
Reference-contexts: Normally this flow field is estimated using correlation techniques, but since our type of system is working from rendered images, it can take advantage of per-pixel depth information in conjunction with the view transform to compute the exact flow field. Agrawala et al. <ref> [Agrawala95] </ref> have described such a technique which uses the resulting flow field to compute accurate 2D transformations for blocks of pixels.
Reference: [Azuma95] <author> R. Azuma and G. Bishop, </author> <title> A Frequency-Domain Analysis of Head-Motion Prediction, </title> <booktitle> Proceedings of SIGGRAPH 95, In Computer Graphics, Annual Conference Series 1995, </booktitle> <address> pp.401-408. </address>
Reference-contexts: Unfortunately, for the case of a viewpoint controlled by head position, no known predictive tracking algorithm is sufficiently accurate over the necessary prediction interval of hundreds of milliseconds. Furthermore, frequency domain studies of user head motion <ref> [Azuma95] </ref> argue that no such algorithm will ever exist. Although predictive tracking is not sufficiently accurate to eliminate the need for a userside scene representation, it is useful for estimating future head positions. So and Griffin [So92] successfully combined predictive head tracking with simple image shifting. <p> These high frequency errors cause jitter in the displayed image which is extremely bothersome to users <ref> [Azuma95] </ref>. The final image-warping step which we propose can eliminate this jitter, while talking advantage of the greater meansquared accuracy of the high gain.
Reference: [Burbidge89] <author> D. Burbidge and P. M. Murray, </author> <title> Hardware Improvements to the Helmet Mounted Projector on the Visual Display Research Tool (VDRT) at the Naval Training Systems Center, Helmet-Mounted Displays, </title> <editor> Jerome T. Carollo, Editor, </editor> <booktitle> Proc. SPIE 1116, </booktitle> <pages> pp. </pages> <month> 52-60 </month> <year> (1989). </year>
Reference-contexts: When compensating for relatively short delays, such as those found in a non-distributed system, the errors due to translation may be small enough that they can reasonable be ignored. This approach is the one taken by earlier image-based latency compensation schemes, such as image shifting <ref> [Burbidge89, So92] </ref>, and Regan & Poses address recalculation [Regan94]. However, for the delays on the order of 100s of milliseconds encountered in remote display systems, the errors due to translation are too large to ignore.
Reference: [Costella93] <author> J. P. Costella, </author> <title> Motion Extrapolation at the Pixel Level, </title> <note> unpublished paper available from http://www.ph.unimelb.edu.au/~jpc., Jan 14, </note> <year> 1993. </year>
Reference-contexts: The warp algorithm then calculates final pixel positions as a function not just of translation and pixel-disparity, but also as a function of time, pixel-velocity, and pixel-acceleration. Costella has proposed this technique outside the context of perturbed-projective image warps <ref> [Costella93] </ref>, but we believe that it can be effectively integrated with this type of image warp. There also the potential to assign more complex timedependent behaviors to pixels.
Reference: [Gossweiler93] <author> R. Gossweiler, C. Long, S. Koga, and R. Pausch, DIVER: </author> <title> a Distributed Virtual Environment Research Platform, </title> <booktitle> 1993 IEEE Symposium on Research Frontiers in Virtual Reality. </booktitle>
Reference-contexts: The effect is that dynamic objects in the scene appear to move in a discontinuous and jerky manner, but the sense of presence the user gets from low latency head rotation/translation is preserved. This idea is not new see for example Gossweiler et al <ref> [Gossweiler93] </ref> - but it has important implications for our system. If there are few or no moving objects in the scene, then we may be able to tolerate scene update rates on the order of 1 Hz, thus reducing our transmission link bandwidth requirements and making our rendering server cheaper.
Reference: [Guenter93] <author> B. K. Guenter, H. C. Yun and R. M. Mersereau, </author> <title> Motion Compensated Compression of Computer Animation Frames, </title> <booktitle> Proceedings of SIGGRAPH 93. In Computer Graphics, Annual Conference Series 1993, </booktitle> <pages> pp. 297-304. </pages>
Reference-contexts: For 640x480 (NTSC) final resolution, the image-based representation thus contains 2.8 million pixels. This large size is mitigated by the fact that an image-based representation is extremely compressible. Conventional image compression techniques should be effective on the color portion of the image, and work by Guenter <ref> [Guenter93] </ref> implies that the disparity values should be highly compressible as well. We can further compress our image-based representation by taking advantage of its temporal coherence. <p> It may be possible to improve on Agrawalas strategy by using an image warp to directly produce the expected images (and depth values), bypassing the explicit computation and transmission of a flow field. The resulting technique would be similar in spirit to Guenters work compressing animation sequences <ref> [Guenter93] </ref> but would not use object information. This use an image warp is in addition to, but orthogonal to, its use for latency compensation. The two uses complement each other well, because they both require that depth or disparity values be transmitted to the userside computer.
Reference: [Katz94] <author> W. Katz, </author> <title> Military Networking Technology Applied to Location-Based, Theme Park and Home Entertainment Systems, </title> <journal> Computer Graphics, </journal> <volume> Vol. 28, No. 2, </volume> <month> May </month> <year> 1994. </year>
Reference-contexts: Changes to the scene sent from the database end are in the form of new geometry or changes in modeling transforms for existing geometry, or in some cases higher-level applicationspecific information. The best known examples of this type of system are based on the Distributed Interactive Simulation (DIS) standard. <ref> [Zyda93, Katz94] </ref> DIS systems transmit new vehicle locations (model transforms) to the users computer, as well as simulationspecific events such as explosions. DIS also transmits predictive information about future vehicle locations.
Reference: [Max95] <author> N.Max and K. Ohsaki, </author> <title> Rendering Trees from Precomputed Z-Buffer Views, </title> <booktitle> 6th Eurographics Workshop on Rendering, </booktitle> <address> (Dublin, Ireland) June 1995, </address> <pages> pp. 45-54. </pages>
Reference-contexts: When multiple source images are available, the extra images can be used to fill in the gaps left after warping the first image. An interesting variation on this idea is the multi-valued Z-buffer demonstrated by Nelson Max <ref> [Max95] </ref>.
Reference: [McMillan95a] <author> L. McMillan and G. Bishop, </author> <title> Head-Tracked Stereo Display Using Image Warping, </title> <booktitle> 1995 IS&T/SPIE Symposium on Electronic Imaging Science and Technology, SPIE Proceedings #2409A, </booktitle> <address> (San Jose, CA) February 5-10, </address> <year> 1995, </year> <month> pp.21-30. </month>
Reference-contexts: Image-warping techniques like those discussed in [Wolberg90] are used to reproject the reference images according to the desired viewing parameters. In our system we have used the perturbed-projective image warps described by <ref> [McMillan95a] </ref> to reproject and render the desired image. This method augments the typical image representation with a disparity value at each pixel. From this point on we will refer to this class of perturbed-projective image warps simply as an image warp. <p> Transmitting this representation uses less than 25% of the 155 Mbit/sec ATM bandwidth. Despite the excessively long interval between updates to the intermediate image-based representation, a user in a headmounted display feels a sense of presence. Speedup Techniques: Clipping and Parallelism We use the <ref> [McMillan95a] </ref> notation for the warp equations. <p> Finally, if the polygon is entirely visible, then we treat the entire row of the source image as potentially visible. A second important component of our warping implementation is its parallelization across three processors. Our serial implementation of the planar-to-planar warp relied on the occlusion-compatible rendering order described in <ref> [McMillan95a] </ref>, but this rendering order presents challenges when parallelizing. Our first attempt at parallelization assigned the different occlusion-compatible regions to different processors, but we got poor load balancing with this strategy because the regions can differ greatly in size.
Reference: [McMillan95b] <author> L. McMillan, </author> <title> A List-Priority Rendering Algorithm for Redisplaying Projected Surfaces, </title> <institution> UNC-CH Computer Science Technical Report TR95-005, University of North Carolina, </institution> <year> 1995. </year>
Reference: [Regan94] <author> M. Regan and R. </author> <title> Pose, Priority Rendering with a Virtual Reality Address Recalculation Pipeline, </title> <booktitle> Proceedings of SIGGRAPH 94. In Computer Graphics, Annual Conference Series 1994, </booktitle> <address> pp.155-162. </address>
Reference-contexts: This approach is the one taken by earlier image-based latency compensation schemes, such as image shifting [Burbidge89, So92], and Regan & Poses address recalculation <ref> [Regan94] </ref>. However, for the delays on the order of 100s of milliseconds encountered in remote display systems, the errors due to translation are too large to ignore. We overcome this rotation-only restriction by augmenting our images with per-pixel disparity values, and using the more general perturbed-projective image warp. <p> But, if our techniques are used to reduce the rate at which the rendering engine needs to generate frames, then the latency again becomes large. For local display, our technique thus presents an alternative to the hybrid address recalculation + priority rendering scheme proposed by Regan and Pose <ref> [Regan94] </ref>. The approaches differ in the types of artifacts they would produce - priority rendering has the potential for artifacts at priority boundaries, while our approach produces occlusion artifacts.
Reference: [So92] <author> R. H. Y. So and M. J. Griffin, </author> <title> Compensating Lags in Head-Coupled Displays Using Head Position Prediction and Image Deflection, </title> <journal> Journal of Aircraft, </journal> <volume> Vol. 29, No. 6, Nov.-Dec. </volume> <year> 1992. </year>
Reference-contexts: When compensating for relatively short delays, such as those found in a non-distributed system, the errors due to translation may be small enough that they can reasonable be ignored. This approach is the one taken by earlier image-based latency compensation schemes, such as image shifting <ref> [Burbidge89, So92] </ref>, and Regan & Poses address recalculation [Regan94]. However, for the delays on the order of 100s of milliseconds encountered in remote display systems, the errors due to translation are too large to ignore. <p> Furthermore, frequency domain studies of user head motion [Azuma95] argue that no such algorithm will ever exist. Although predictive tracking is not sufficiently accurate to eliminate the need for a userside scene representation, it is useful for estimating future head positions. So and Griffin <ref> [So92] </ref> successfully combined predictive head tracking with simple image shifting. We hope to combine it with the more general image warping we use to increase the quality of the warped images.
Reference: [Tharp92] <author> G. Tharp, A. Liu, L. French, S. Lai, and L. Stark, </author> <title> Timing Considerations of Helmet Mounted Display Perf ormance , Human Vision, Visual Processing, and Digital Display III, </title> <editor> B. Rogowitz, Editor, </editor> <booktitle> Proc. SPIE 1666, </booktitle> <pages> pp. </pages> <month> 570-576 </month> <year> (1992). </year>
Reference: [Wolberg90] <author> G. Wolberg, </author> <title> Digital Image Warping, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990. </year>
Reference-contexts: Image-based representations and image-based rendering are recent developments in computer graphics. Instead of using geometric models as a scene description, image-based methods use a series of reference images to model the scene. Image-warping techniques like those discussed in <ref> [Wolberg90] </ref> are used to reproject the reference images according to the desired viewing parameters. In our system we have used the perturbed-projective image warps described by [McMillan95a] to reproject and render the desired image. This method augments the typical image representation with a disparity value at each pixel.
Reference: [Zyda93] <author> M. Zyda, D. Pratt, J. Falby, P. Barham, and K. Kelleher, </author> <title> NPSNET and the Naval Postgraduate School Graphics and Video Laboratory, </title> <journal> PRESENCE, </journal> <volume> Vol. 2, No. 3, </volume> <month> Summer </month> <year> 1993, </year> <pages> pp. 244-258. </pages>
Reference-contexts: Changes to the scene sent from the database end are in the form of new geometry or changes in modeling transforms for existing geometry, or in some cases higher-level applicationspecific information. The best known examples of this type of system are based on the Distributed Interactive Simulation (DIS) standard. <ref> [Zyda93, Katz94] </ref> DIS systems transmit new vehicle locations (model transforms) to the users computer, as well as simulationspecific events such as explosions. DIS also transmits predictive information about future vehicle locations.
References-found: 15

