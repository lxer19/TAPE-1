URL: http://www5.in.tum.de/~schultem/nips97.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/1998/97abstracts.html
Root-URL: 
Email: email: marbach@mit.edu  email: oliver.mihatsch@  email: jnt@mit.edu  
Title: Submitted to NIPS-97. Category: Applications. Preference: Poster Presentation. Reinforcement Learning for Call Admission Control and
Author: Peter Marbach LIDS Oliver Mihatsch Miriam Schulte John N. Tsitsiklis LIDS 
Web: mchp.siemens.de  
Address: Room 35 -307 Cambridge, MA, 02139  D-81730 Munich, Germany  D-80290 Munich Germany  Cambridge, MA, 02139  
Affiliation: MIT,  Siemens AG Corporate Technology, ZT IK 4  Zentrum Mathematik Technische Universitat Munchen  MIT  
Abstract: In integrated service communication networks, an important problem is to exercise call admission control and routing so as to optimally use the network resources. This problem is naturally formulated as a dynamic programming problem, which, however, is too complex to be solved exactly. We use methods of reinforcement learning (RL), together with a decomposition approach, to find call admission control and routing policies. The performance of our policy for a network with approximately 10 45 different feature configurations is compared with a commonly used heuristic policy.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bertsekas, D. P. </author> <title> (1995) Dynamic Programming and Optimal Control. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference-contexts: The objective is to exercise call admission control and routing in such a way that the long term revenue obtained by accepting calls is maximized. We can formulate the call admission control and routing problem using dynamic programming <ref> (e. g. Bertsekas, 1995) </ref>. Events ! which incur state transitions, are arrivals of new calls and call terminations. The state x t at time t consists of a list for each route, indicating how many calls of each service type are currently using that route.
Reference: <author> Crites, R. H., Barto, A. G. </author> <title> (1996) Improving elevator performance using reinforcement learning. </title> <editor> In D. S. Touretzky, M. C. Mozer and M. E. Hasselmo (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pp. 10171023. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: We presented a solution for an example network with about 10 45 different feature configurations. Our RL policy clearly outperforms the commonly used heuristic OSPF. Besides the game of backgammon (Tesauro, 1992), the elevator scheduling <ref> (Crites & Barto, 1996) </ref>, the jop-shop scheduling (Zhang & Dietterich, 1996) and the dynamic channel allocation (Singh & Bertsekas, 1997), this is another successful application of RL to a large-scale dynamic programming problem for which a good heuristic is hard to find.
Reference: <author> Singh, S., Bertsekas, D. P. </author> <title> (1997) Reinforcement learning for dynamic channel allocation in cellular telephone systems. </title> <booktitle> To appear in Advances in Neural Information Processing Systems 9, </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Our RL policy clearly outperforms the commonly used heuristic OSPF. Besides the game of backgammon (Tesauro, 1992), the elevator scheduling (Crites & Barto, 1996), the jop-shop scheduling (Zhang & Dietterich, 1996) and the dynamic channel allocation <ref> (Singh & Bertsekas, 1997) </ref>, this is another successful application of RL to a large-scale dynamic programming problem for which a good heuristic is hard to find.
Reference: <author> Sutton, R. S. </author> <title> (1988) Learning to predict by the method of temporal differences. Machine Learning, </title> <publisher> 3:944. </publisher>
Reference-contexts: Usually, RL uses a global feature extractor f (x) to form an approximate compact representation of the state of the system, which forms the input to a function approximator ~ J (; ). Sutton's temporal difference (TD ()) algorithms <ref> (Sutton, 1988) </ref> can then be used to train ~ J (; ) to learn an estimate of J fl .
Reference: <author> Tesauro, G. J. </author> <title> (1992) Practical issues in temporal difference learning. Machine Learning, </title> <publisher> 8(3/4):257277. </publisher>
Reference-contexts: We presented a solution for an example network with about 10 45 different feature configurations. Our RL policy clearly outperforms the commonly used heuristic OSPF. Besides the game of backgammon <ref> (Tesauro, 1992) </ref>, the elevator scheduling (Crites & Barto, 1996), the jop-shop scheduling (Zhang & Dietterich, 1996) and the dynamic channel allocation (Singh & Bertsekas, 1997), this is another successful application of RL to a large-scale dynamic programming problem for which a good heuristic is hard to find.
Reference: <author> Zhang, W., Dietterich, T. G. </author> <title> (1996) High performance job-shop scheduling with a time-delay T D() network. </title> <editor> In D. S. Touretzky, M. C. Mozer and M. E. Hasselmo (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pp. 10241030. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: We presented a solution for an example network with about 10 45 different feature configurations. Our RL policy clearly outperforms the commonly used heuristic OSPF. Besides the game of backgammon (Tesauro, 1992), the elevator scheduling (Crites & Barto, 1996), the jop-shop scheduling <ref> (Zhang & Dietterich, 1996) </ref> and the dynamic channel allocation (Singh & Bertsekas, 1997), this is another successful application of RL to a large-scale dynamic programming problem for which a good heuristic is hard to find.
References-found: 6

