URL: http://renoir.csc.ncsu.edu/Faculty/Vouk/Papers/RAMS98/RAMS98.Notes.ps
Refering-URL: http://renoir.csc.ncsu.edu/Faculty/Vouk/vouk.se.html
Root-URL: http://www.csc.ncsu.edu
Title: SOFTWARE RELIABILITY ENGINEERING  
Author: Mladen A. Vouk 
Address: Box 8206  Raleigh, NC 27695  
Affiliation: Department of Computer Science,  North Carolina State University,  
Pubnum: RAMS98/SRE/Vouk/V1.0/15-Sep97  
Email: e-mail: vouk@csc.ncsu.edu  
Phone: Tel: 919-515-7886  
Date: 1  
Abstract-found: 0
Intro-found: 1
Reference: [Abr92] <author> S.R. Abramson et al., </author> <title> "Custommer Satisfaction-Based Product Development," </title> <booktitle> Proc. Intl. Switching Symp., </booktitle> <volume> Vol. 2. </volume> <publisher> Inst. Electronics, Information, Communications Engineers, </publisher> <address> Yokohama, Japan, </address> <pages> pp. 65-69, </pages> <year> 1992. </year>
Reference-contexts: The same system showed no serious service outages within the first two years after its release, a considerably increased customer satisfaction, and its sales were increased by a factor of 10 (only part of which is attributed to the increased quality) <ref> [Abr92, Mus93] </ref>. It is estimated that routine application of SRE does not add more than several percent to the overall cost of a project.
Reference: [AIA93] <institution> AIAA/ANSI Recommended Practice for Software Reliability, ANSI/AIAA, R-103-1992, American Inst, of Aeronautics and Astronautics, </institution> <year> 1993. </year> <title> [Bel90]. BELLCORE, Reliability and Quality Measurements for Telecommunications Systems (RQMS) - TR-TSY-000929, </title> <note> Issue 1, </note> <month> June </month> <year> 1990. </year>
Reference-contexts: Metrics and Models A significant set of SRE activities are concerned with measurement and prediction of software reliability and availability. This includes, modeling of software failure behavior, and modeling of the process that develops and removes faults. A number of metrics and models are available for that purpose <ref> [Mus87, IEE88a, IEE88b, Mal91, Xie91, AIA93] </ref>. This section examines the basic ideas. 4.1 Reliability We distinguish two situations. In one situation, detected problems are further pursued and fault identification and correction takes place, for example, during software development, system and field testing, and active field maintenance.
Reference: [Boe89] <author> B.W. Boehm, </author> <title> Tutorial: Software Risk Management, </title> <publisher> IEEE CS Press, </publisher> <year> 1989. </year>
Reference-contexts: The "balance" one is looking for is between the overall system reliability, and the development schedule and effort (cost). Available options include inclusion of good exception handling capabilities and use of different fault-tolerance techniques [Pha92, Lyu94] combined with systems and software risk analysis and management <ref> [Boe89] </ref>. Use of inspections and reviews is highly recommended in all phases of the software development process [IEE86]. They provide a means of tracking and managing faults during the stages where the software is not in executable form, as well as in the stages where it is.
Reference: [Bria93] <author> L.C. Briand, W.M. Thomas and C.J. Hetsmanski, </author> <title> "Modeling and Managing Risk Early in Software Development," </title> <booktitle> Proc. 15th ICSE, </booktitle> <pages> pp 55-65, </pages> <year> 1993. </year>
Reference: [Bro92] <author> S. Brocklehurst and B. Littlewood, </author> <title> "New Ways to Get Accurate Reliability Measures," </title> <journal> IEEE Software, </journal> <pages> pp. 34-42, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: All models have some advantages and some disadvantages. It is extremely important that an appropriate model be chosen on a case by case basis <ref> [Mus87, Bro92, Lyu96] </ref>. Two typical models are the "basic execution time" (BET) model [Goe79, Mus87] and the Logarithmic-Poisson execution time (LPET) model [Mus84, Mus87]. Both models assume that the testing uses operational profiles, and that every detected failure is immediately and perfectly repaired 1 . <p> It is also quite common to plot failure intensity against cumulative failures to see if the relationship given in equation (2) holds. While graphs can be used to screen the data for trends, statistical tests must be used to actually select a model <ref> [Mus87, Bro92] </ref>. In this case the tests show that the LPET model fits somewhat better than the BET model. However, in a different project the BET, or some other model, may be better than the LPET model.
Reference: [Chr90] <author> D.A. Christenson, S.T. Huang, and A.J. Lamperez, </author> <title> "Statistical Quality Control Applied to Code Inspections," </title> <journal> IEEE J. on Selected Areas in Communications, </journal> <volume> Vol. 8 (2), </volume> <pages> pp. 196-200, </pages> <year> 1990. </year>
Reference-contexts: If this metric is in the range 3 to 7 the inspection process, as well as the software process is probably under control, otherwise some corrective action is needed. More details can be found in <ref> [IEE86, Chr90] </ref>. Once executable software is available tracking and management can be supplemented using reliability models and reliability control charts.
Reference: [Cra92] <author> Cramp R., Vouk M.A., and Jones W., </author> <title> "On Operational Availability of a Large Software-Based Telecommunications System," </title> <booktitle> Proc. Thrid Intl. Symposium on Software Reliability Engineering, </booktitle> <publisher> IEEE CS, </publisher> <year> 1992, </year> <pages> pp. 358-366 </pages>
Reference-contexts: Figure 5 shows failure and recovery rates observed during operational use of a telecommunications product <ref> [Cra92] </ref>. Apart from the censored 3 "raw" data two other representations are shown. In one, the data are smoothed using an 11-point symmetrical moving average. In the other, we show cumulative average of the data. <p> The figure shows that, in this case, both models appear to predict future system behavior well. The models are described in <ref> [Cra92] </ref>. Other models are available, e.g., [Lap91]. 5 0 04 0 03 0 02 0 01 0 00 .000001 .0001 Inservice Time U a a b y Instantaneous unavailability Cutoff p o i n t Model for Instantaneous Unavailability Empirical average unavailability Model for Average Unavailability point" only.
Reference: [Ehr93] <author> W. Ehrlich, B. Prasanna. J. Sampfel, J. Wu, </author> <title> "Determining the Cost of Stop-Test Decisions," </title> <journal> IEEE Software, </journal> <volume> Vol 10(2), </volume> <pages> pp 33-42., </pages> <year> 1993 </year>
Reference-contexts: RAMS98/SRE/Vouk/V1.0/15-Sep97 2 Although direct economic information is usually difficult to obtain for proprietary reasons studies show that the cost-benefit ratio of using SRE techniques can be six or more <ref> [Ehr93] </ref>. In one case, SRE has been credited with reducing the incidence of customer-reported problems, and maintenance costs, by a factor of 10. In addition, in the system-test interval the number of software-related problems was reduced by a factor of two, and in the product introduction interval by 30 percent. <p> is known what effort expenditure is required to detect a failure, identify and correct the corresponding fault, and how much cost is associated with exposure time, it is possible to construct economic models that relate the testing not only to the resultant quality, but also to the expended effort (cost) <ref> [Mus87, Ehr93, Yam93] </ref>. The estimates given in the above example are known as point estimates since they involve only the "most likely" or the "best" value.
Reference: [Far88] <author> W.H. Farr, </author> <title> "Statistical Modeling and Estimation of Reliability Functions for Software (SMERFS) Library Access Guide", </title> <institution> TR84-371 (Rev.1), Naval Surface Warfare Center, </institution> <note> Dahlgren VA; also "SMERFS User's Guide," TR84-373 (Rev.1), </note> <year> 1988. </year>
Reference-contexts: Examples of reliability oriented datasets and tools can be found on the CD-ROMthat comes with the Handbook of Software Reliability Engineering [Lyu96]. Examples of tools that can aid in software reliability estimation are SMERFS <ref> [Far88, Lyu96] </ref> and RelTools [Mus90] on Unix , CASRE on DOS [Lyu92, Lyu96], and SoRel on Macintosh computers [Kan93a. Lyu96]. system test data set called T1 [Mus87]. The plot is of the natural logarithm of failure intensity vs. execution time.
Reference: [FCC92] <author> Federal Communications Commission, </author> <title> "Notification by Common Carriers of Service Disruptions," 47 CFR Part 63, </title> <journal> Federal Register, </journal> <volume> Vol. 57 (44), </volume> <month> March 5, </month> <year> 1992, </year> <pages> pp 7883-7885. </pages>
Reference-contexts: For example, U.S. Federal Communications Commission requires service disruptions, such as loss of telephone service, that exceed 30 minutes and affect more than 50,000 customer to be reported to FCC within 30 minutes of its occurrence <ref> [FCC92] </ref>.
Reference: [Fra88] <author> E.G. Frankel, </author> <title> Systems Reliability and Risk Analysis , Second Revised Edition, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: For example, the Bellcore 2 unavailability target for telecommunications network elements is about 3 minutes of downtime per year. Availability is the probability that a system, or a system component, will be available to start a mission at a specified "time" <ref> [Fra88] </ref>. Unavailability is the opposite, the probability that a system or a system component will not be available to start a mission at a specified "time". The concept of (un)availability is closely connected to the notion of repairable failures.
Reference: [Goe79] <author> A.L. Goel and K. Okumoto, </author> <title> "Time-Dependent Error-Detection Rate Model for Software Reliability and other Performance Measures, </title> " <journal> IEEE Trans. on Reliability, </journal> <volume> Vol R-28(3), </volume> <pages> pp. 206-211, </pages> <year> 1979. </year>
Reference-contexts: All models have some advantages and some disadvantages. It is extremely important that an appropriate model be chosen on a case by case basis [Mus87, Bro92, Lyu96]. Two typical models are the "basic execution time" (BET) model <ref> [Goe79, Mus87] </ref> and the Logarithmic-Poisson execution time (LPET) model [Mus84, Mus87]. Both models assume that the testing uses operational profiles, and that every detected failure is immediately and perfectly repaired 1 .
Reference: [IEE86] <author> IEEE Std. </author> <year> 1012-1986, </year> <title> IEEE Standard Software Verificationa and Validation Plans, </title> <booktitle> IEEE 1986. </booktitle>
Reference-contexts: RAMS98/SRE/Vouk/V1.0/15-Sep97 16 5. Practice It is not feasible to practice SRE without sound and solid software verification and validation plan and activities throughout software life-cycle. An example of such a plan can be found in the IEEE software engineering standards <ref> [IEE86] </ref>. SRE implies use of modern fault - avoidance, fault-identification, fault-elimination, and fault-tolerance technology. SRE extends that technology through quantification. This includes construction and quantification of software usage profiles, and specification of a balance between software reliability and other quality constraints. <p> Available options include inclusion of good exception handling capabilities and use of different fault-tolerance techniques [Pha92, Lyu94] combined with systems and software risk analysis and management [Boe89]. Use of inspections and reviews is highly recommended in all phases of the software development process <ref> [IEE86] </ref>. They provide a means of tracking and managing faults during the stages where the software is not in executable form, as well as in the stages where it is. A rule-of-thumb metric is the number of major faults per person-hour spent on preparation and conduct of inspections. <p> If this metric is in the range 3 to 7 the inspection process, as well as the software process is probably under control, otherwise some corrective action is needed. More details can be found in <ref> [IEE86, Chr90] </ref>. Once executable software is available tracking and management can be supplemented using reliability models and reliability control charts.
Reference: [IEE88a] <author> IEEE Std. 982.1-1988, </author> <title> IEEE Standard Dictionary of Measures to Produce Reliable Software, </title> <booktitle> IEEE 1988. </booktitle>
Reference-contexts: When a system in operation does not deliver its intended functionality and quality, it is said to fail. A failure is an observed departure of the external result of software operation from software requirements or user expectations <ref> [IEE88a, IEE88b, IEE90] </ref>. Failures can be caused by hardware or software faults (defects), or by how-to-use errors. A fault (or defect, or bug) is a defective, missing, or extra instruction, or a set of related instructions, that is the cause of one or more actual or potential failures. <p> Metrics and Models A significant set of SRE activities are concerned with measurement and prediction of software reliability and availability. This includes, modeling of software failure behavior, and modeling of the process that develops and removes faults. A number of metrics and models are available for that purpose <ref> [Mus87, IEE88a, IEE88b, Mal91, Xie91, AIA93] </ref>. This section examines the basic ideas. 4.1 Reliability We distinguish two situations. In one situation, detected problems are further pursued and fault identification and correction takes place, for example, during software development, system and field testing, and active field maintenance.
Reference: [IEE88b] <author> IEEE Std. 982.2-1988, </author> <title> IEEE Guide for the use of IEEE Standard Dictionary of Measures to Produce Reliable Software, </title> <booktitle> IEEE 1988. </booktitle>
Reference-contexts: When a system in operation does not deliver its intended functionality and quality, it is said to fail. A failure is an observed departure of the external result of software operation from software requirements or user expectations <ref> [IEE88a, IEE88b, IEE90] </ref>. Failures can be caused by hardware or software faults (defects), or by how-to-use errors. A fault (or defect, or bug) is a defective, missing, or extra instruction, or a set of related instructions, that is the cause of one or more actual or potential failures. <p> Metrics and Models A significant set of SRE activities are concerned with measurement and prediction of software reliability and availability. This includes, modeling of software failure behavior, and modeling of the process that develops and removes faults. A number of metrics and models are available for that purpose <ref> [Mus87, IEE88a, IEE88b, Mal91, Xie91, AIA93] </ref>. This section examines the basic ideas. 4.1 Reliability We distinguish two situations. In one situation, detected problems are further pursued and fault identification and correction takes place, for example, during software development, system and field testing, and active field maintenance.
Reference: [IEE90] <institution> IEEE Std. 610.12-1990, IEEE Standard Glosssary of Software Engineering Terminology, </institution> <note> IEEE 1990. RAMS98/SRE/Vouk/V1.0/15-Sep97 22 </note>
Reference-contexts: When a system in operation does not deliver its intended functionality and quality, it is said to fail. A failure is an observed departure of the external result of software operation from software requirements or user expectations <ref> [IEE88a, IEE88b, IEE90] </ref>. Failures can be caused by hardware or software faults (defects), or by how-to-use errors. A fault (or defect, or bug) is a defective, missing, or extra instruction, or a set of related instructions, that is the cause of one or more actual or potential failures.
Reference: [Jon93] <author> Jones, W. D., and Gregory, D., </author> <title> "Infinite Failure Models for a Finite World: A Simulation of the Fault Discovery Process," </title> <booktitle> Proceedings of the Fourth International Symposium on Software Reliability Engineering, </booktitle> <pages> pp. 284-293, </pages> <month> November </month> <year> 1993 </year>
Reference-contexts: Of course, both classes of models can be, and are being, used to describe software fault removal processes that involve only a finite number of actual faults <ref> [Jon93] </ref>. Given failure intensity data, it is possible to estimate model parameters. Estimation can be made in many ways. Two common methods are maximum likelihood and least squares [Mus87]. It is very important to understand that there are two distinct ways of using a model.
Reference: [Kan93a] <author> Kanoun K., Kaaniche M., Laprie J-C., and S. Metge "SoRel: </author> <title> A Tool for Software Reliability Analysis and Evaluation from Statistical Failure Data," </title> <booktitle> Proc. 23rd IEEE intl. Symp. on Fault-Tolerant Computing, </booktitle> <address> Toulouse, France, </address> <month> June </month> <year> 1993, </year> <pages> pp. 654-659. </pages>
Reference: [Kan93a] <author> Kanoun K., Kaaniche M., Laprie J-C., and S. Metge "SoRel: </author> <title> A Tool for Software Reliability Analysis and Evaluation from Statistical Failure Data," </title> <booktitle> Proc. 23rd IEEE intl. Symp. on Fault-Tolerant Computing, </booktitle> <address> Toulouse, France, </address> <month> June </month> <year> 1993, </year> <pages> pp. 654-659. </pages>
Reference: [Kan93] <author> Kanoun K., Kaaniche M., and Laprie J-C., </author> <title> "Experience in Software reliability: From Data Collection to Quantitative Evaluation," </title> <booktitle> Proc. Fourth Intl. Symposium on Software Reliability Engineering, </booktitle> <address> Denver, Colorado, </address> <month> November 3-6, </month> <year> 1993, </year> <month> pp.234-245. </month>
Reference: [Kho90] <author> T.M. Khoshgoftaar and J.C. Munson, </author> <title> "Predicting Software Development Errors Using Software Complexity Metrics," </title> <journal> IEEE J. on Selected Areas in Communications, </journal> <volume> Vol. 8 (2), </volume> <pages> pp 253-261, </pages> <year> 1990. </year>
Reference-contexts: Several authors have published models that attempt to relate some early software metrics, such as, the size of the code, Halstead length, or cyclomatic number, to the failure proneness of a program <ref> [Kho90, Mun92, Bri93] </ref>. A more processoriented approach is discussed in [Vou93, Lyu96]. Highly correlated nature of the early software verification and testing events may require the use of a more sophisticated, timeseries, approach [Sin92]. RAMS98/SRE/Vouk/V1.0/15-Sep97 13 4.2 Availability Another important practical measure for software quality is availability.
Reference: [Lap91] <author> J.C. Laprie, K. Kanoun, C. Beounes, and M. Kaaniche, </author> <title> "The KAT (Knowledge - Action-Transformation) Approach to the Modeling and Evaluation of Reliability and Availability Growth," </title> <journal> IEEE Transactions on Software Engineering, IEEE, </journal> <volume> vol. 18, no. 4, </volume> <month> April </month> <year> 1991, </year> <pages> pp. 701-714. </pages>
Reference-contexts: The figure shows that, in this case, both models appear to predict future system behavior well. The models are described in [Cra92]. Other models are available, e.g., <ref> [Lap91] </ref>. 5 0 04 0 03 0 02 0 01 0 00 .000001 .0001 Inservice Time U a a b y Instantaneous unavailability Cutoff p o i n t Model for Instantaneous Unavailability Empirical average unavailability Model for Average Unavailability point" only.
Reference: [Lyu 94] <editor> M.R. Lyu (ed.), </editor> <title> Software Fault Tolerance, Trends-in-Software Book Series, </title> <publisher> Wiley, </publisher> <year> 1994 </year>
Reference: [Lyu92] <author> M.R. Lyu and A. Nikora, </author> <title> "Applying Reliability Models More Effectively," </title> <journal> IEEE Software, </journal> <pages> pp. 43-45, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Examples of reliability oriented datasets and tools can be found on the CD-ROMthat comes with the Handbook of Software Reliability Engineering [Lyu96]. Examples of tools that can aid in software reliability estimation are SMERFS [Far88, Lyu96] and RelTools [Mus90] on Unix , CASRE on DOS <ref> [Lyu92, Lyu96] </ref>, and SoRel on Macintosh computers [Kan93a. Lyu96]. system test data set called T1 [Mus87]. The plot is of the natural logarithm of failure intensity vs. execution time.
Reference: [Lyu96] <institution> Handbook of Software Reliability Engineering, </institution> <note> McGraw Hill, </note> <editor> editor M. Lyu, </editor> <month> January </month> <year> 1996. </year>
Reference-contexts: Where software reliability growth is present, failure intensity, l (t), becomes a decreasing function of time t during which software is exposed to testing and usage under representative (operational) conditions. There is a large number of software reliability models that address this situation <ref> [Lyu96] </ref>, but before any modeling is undertaken, it is a good idea to confirm the presence of the growth using trend tests [Mus87, Kan93b]. All models have some advantages and some disadvantages. <p> All models have some advantages and some disadvantages. It is extremely important that an appropriate model be chosen on a case by case basis <ref> [Mus87, Bro92, Lyu96] </ref>. Two typical models are the "basic execution time" (BET) model [Goe79, Mus87] and the Logarithmic-Poisson execution time (LPET) model [Mus84, Mus87]. Both models assume that the testing uses operational profiles, and that every detected failure is immediately and perfectly repaired 1 . <p> It is recommended that both the data collection and the model estimation be automated and tool-based. Examples of reliability oriented datasets and tools can be found on the CD-ROMthat comes with the Handbook of Software Reliability Engineering <ref> [Lyu96] </ref>. Examples of tools that can aid in software reliability estimation are SMERFS [Far88, Lyu96] and RelTools [Mus90] on Unix , CASRE on DOS [Lyu92, Lyu96], and SoRel on Macintosh computers [Kan93a. Lyu96]. system test data set called T1 [Mus87]. <p> Examples of reliability oriented datasets and tools can be found on the CD-ROMthat comes with the Handbook of Software Reliability Engineering [Lyu96]. Examples of tools that can aid in software reliability estimation are SMERFS <ref> [Far88, Lyu96] </ref> and RelTools [Mus90] on Unix , CASRE on DOS [Lyu92, Lyu96], and SoRel on Macintosh computers [Kan93a. Lyu96]. system test data set called T1 [Mus87]. The plot is of the natural logarithm of failure intensity vs. execution time. <p> Examples of reliability oriented datasets and tools can be found on the CD-ROMthat comes with the Handbook of Software Reliability Engineering [Lyu96]. Examples of tools that can aid in software reliability estimation are SMERFS [Far88, Lyu96] and RelTools [Mus90] on Unix , CASRE on DOS <ref> [Lyu92, Lyu96] </ref>, and SoRel on Macintosh computers [Kan93a. Lyu96]. system test data set called T1 [Mus87]. The plot is of the natural logarithm of failure intensity vs. execution time. <p> These models derive their name from the S-like shape of their cumulative failure distributions. Figures 3 and 4 illustrate use of a Weibull-type model [Mus87] during unit and integration testing phases of a telecommunications software product <ref> [Vou93, Lyu96] </ref>. 0 50 100 150 200 250 300 350 Execution Time Empirical Intensity Weibull Fit Weibull Average Intensity Empirical Average Intensity Exposure is the cumulative test case execution time "t". <p> Several authors have published models that attempt to relate some early software metrics, such as, the size of the code, Halstead length, or cyclomatic number, to the failure proneness of a program [Kho90, Mun92, Bri93]. A more processoriented approach is discussed in <ref> [Vou93, Lyu96] </ref>. Highly correlated nature of the early software verification and testing events may require the use of a more sophisticated, timeseries, approach [Sin92]. RAMS98/SRE/Vouk/V1.0/15-Sep97 13 4.2 Availability Another important practical measure for software quality is availability.
Reference: [Mal91] <author> Y.K. Malaiya, </author> <title> Editor, Software Reliability Models: Theoretcial Developments, Evaluation and Application, </title> <publisher> IEEE CS Press, </publisher> <year> 1991. </year>
Reference-contexts: Metrics and Models A significant set of SRE activities are concerned with measurement and prediction of software reliability and availability. This includes, modeling of software failure behavior, and modeling of the process that develops and removes faults. A number of metrics and models are available for that purpose <ref> [Mus87, IEE88a, IEE88b, Mal91, Xie91, AIA93] </ref>. This section examines the basic ideas. 4.1 Reliability We distinguish two situations. In one situation, detected problems are further pursued and fault identification and correction takes place, for example, during software development, system and field testing, and active field maintenance.
Reference: [Mus87] <author> J.D. Musa, A. Iannino, and K. Okumoto, </author> <title> Software Reliability: Measurement, Prediction, </title> <address> Apllication, Mc-Graw-Hill, New York, </address> <year> 1987. </year>
Reference-contexts: Operational profile is a set of relative frequencies (or probabilities) of occurrence of disjoint software operations during its operational use. A detailed discussion of operational profile issues can be found in <ref> [Mus87, Mus93] </ref>. A software-based system may have one or more operational profiles. Operational profiles are used to select test cases and direct development, testing and maintenance efforts towards the most frequently used or most risky components. <p> Metrics and Models A significant set of SRE activities are concerned with measurement and prediction of software reliability and availability. This includes, modeling of software failure behavior, and modeling of the process that develops and removes faults. A number of metrics and models are available for that purpose <ref> [Mus87, IEE88a, IEE88b, Mal91, Xie91, AIA93] </ref>. This section examines the basic ideas. 4.1 Reliability We distinguish two situations. In one situation, detected problems are further pursued and fault identification and correction takes place, for example, during software development, system and field testing, and active field maintenance. <p> There is a large number of software reliability models that address this situation [Lyu96], but before any modeling is undertaken, it is a good idea to confirm the presence of the growth using trend tests <ref> [Mus87, Kan93b] </ref>. All models have some advantages and some disadvantages. It is extremely important that an appropriate model be chosen on a case by case basis [Mus87, Bro92, Lyu96]. Two typical models are the "basic execution time" (BET) model [Goe79, Mus87] and the Logarithmic-Poisson execution time (LPET) model [Mus84, Mus87]. <p> All models have some advantages and some disadvantages. It is extremely important that an appropriate model be chosen on a case by case basis <ref> [Mus87, Bro92, Lyu96] </ref>. Two typical models are the "basic execution time" (BET) model [Goe79, Mus87] and the Logarithmic-Poisson execution time (LPET) model [Mus84, Mus87]. Both models assume that the testing uses operational profiles, and that every detected failure is immediately and perfectly repaired 1 . <p> All models have some advantages and some disadvantages. It is extremely important that an appropriate model be chosen on a case by case basis [Mus87, Bro92, Lyu96]. Two typical models are the "basic execution time" (BET) model <ref> [Goe79, Mus87] </ref> and the Logarithmic-Poisson execution time (LPET) model [Mus84, Mus87]. Both models assume that the testing uses operational profiles, and that every detected failure is immediately and perfectly repaired 1 . <p> All models have some advantages and some disadvantages. It is extremely important that an appropriate model be chosen on a case by case basis [Mus87, Bro92, Lyu96]. Two typical models are the "basic execution time" (BET) model [Goe79, Mus87] and the Logarithmic-Poisson execution time (LPET) model <ref> [Mus84, Mus87] </ref>. Both models assume that the testing uses operational profiles, and that every detected failure is immediately and perfectly repaired 1 . <p> Given failure intensity data, it is possible to estimate model parameters. Estimation can be made in many ways. Two common methods are maximum likelihood and least squares <ref> [Mus87] </ref>. It is very important to understand that there are two distinct ways of using a model. One is to provide a description of historical (already available) data. <p> is known what effort expenditure is required to detect a failure, identify and correct the corresponding fault, and how much cost is associated with exposure time, it is possible to construct economic models that relate the testing not only to the resultant quality, but also to the expended effort (cost) <ref> [Mus87, Ehr93, Yam93] </ref>. The estimates given in the above example are known as point estimates since they involve only the "most likely" or the "best" value. <p> However, in practice, it is extremely important to compute confidence bounds for any estimated parameters and derived quantities in order to see how much one can rely on the obtained figures <ref> [Mus87] </ref>. This involves computation of probable errors (variances) for both the model parameters and the derived quantities. Instead of presenting the projections as single values we need to presented them as an appropriate interval (e.g., 70%, 90% or 95% confidence interval). <p> Examples of tools that can aid in software reliability estimation are SMERFS [Far88, Lyu96] and RelTools [Mus90] on Unix , CASRE on DOS [Lyu92, Lyu96], and SoRel on Macintosh computers [Kan93a. Lyu96]. system test data set called T1 <ref> [Mus87] </ref>. The plot is of the natural logarithm of failure intensity vs. execution time. It is also quite common to plot failure intensity against cumulative failures to see if the relationship given in equation (2) holds. <p> It is also quite common to plot failure intensity against cumulative failures to see if the relationship given in equation (2) holds. While graphs can be used to screen the data for trends, statistical tests must be used to actually select a model <ref> [Mus87, Bro92] </ref>. In this case the tests show that the LPET model fits somewhat better than the BET model. However, in a different project the BET, or some other model, may be better than the LPET model. <p> This oscillatory effect can make reliability growth modeling difficult, although several different approaches for handling this problem have been suggested <ref> [e.g., Mus87, Lyu92. Lyu96] </ref> . A large class of models that can be useful in the context of early testing phases, and nonoperational testing in general, are the so called "S-shaped " models that describe failure intensity that has a mode or a peak [Yam83, Ohb84, Mus87, Yam93]. <p> Lyu96] . A large class of models that can be useful in the context of early testing phases, and nonoperational testing in general, are the so called "S-shaped " models that describe failure intensity that has a mode or a peak <ref> [Yam83, Ohb84, Mus87, Yam93] </ref>. These models derive their name from the S-like shape of their cumulative failure distributions. <p> These models derive their name from the S-like shape of their cumulative failure distributions. Figures 3 and 4 illustrate use of a Weibull-type model <ref> [Mus87] </ref> during unit and integration testing phases of a telecommunications software product [Vou93, Lyu96]. 0 50 100 150 200 250 300 350 Execution Time Empirical Intensity Weibull Fit Weibull Average Intensity Empirical Average Intensity Exposure is the cumulative test case execution time "t". <p> A special form of control charts may be used to monitor progress and decide on whether to accept or reject the components <ref> [Mus87] </ref>. A crucial activity is definition of operational profiles and associated test cases. The process involves definition of customer, user and system-mode profiles, followed by the definition of functional and operational profile (s). For example, a customer is a person, a group, or an institution that acquires the system. <p> An input variable is any data item that exists external to a program is used by the program, while an output variable is any data item that exists external to the program and is set by the program <ref> [Mus87] </ref>. Note that in addition to the usual program parameters, externally initiated interrupts are also considered as input variables. Intermediate data items are neither input nor output variables.
Reference: [Mus90] <author> J.D. Musa, </author> <title> and W.W. Everett, "Software-Reliability Engineering: Technology for the 1990s," </title> <journal> IEEE Software, </journal> <volume> Vol. 7, </volume> <pages> pp. 36-43, </pages> <month> November </month> <year> 1990 </year>
Reference-contexts: Examples of reliability oriented datasets and tools can be found on the CD-ROMthat comes with the Handbook of Software Reliability Engineering [Lyu96]. Examples of tools that can aid in software reliability estimation are SMERFS [Far88, Lyu96] and RelTools <ref> [Mus90] </ref> on Unix , CASRE on DOS [Lyu92, Lyu96], and SoRel on Macintosh computers [Kan93a. Lyu96]. system test data set called T1 [Mus87]. The plot is of the natural logarithm of failure intensity vs. execution time. <p> The system can be used in several modes. A system mode is a set of functions or operations that are grouped for convenience in analyzing execution behavior <ref> [Mus90] </ref>. System can switch among modes, and two or more modes can be active at any one time. The procedure is to determine the operational profile for each system mode. <p> Probability may not be the only criterion for choosing the profile elements. Cost of failure (severity, importance) of the operations plays a role. In fact, separate profiles should be generated for each category of criticality (typically four separated by at least an order of magnitude in effects) <ref> [Mus90] </ref>. In the system and field testing phases standard activities include: i)execution of system and field acceptance tests, ii) checkout of the installation configurations, and, iii) validation of software functionality and quality.
Reference: [Mus93] <author> J.D. Mua, </author> <title> "Operational profiles in Software-Reliability Engineering," </title> <journal> IEEE Software, </journal> <volume> Vol. 10 (2), </volume> <pages> pp. 14-32, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: The same system showed no serious service outages within the first two years after its release, a considerably increased customer satisfaction, and its sales were increased by a factor of 10 (only part of which is attributed to the increased quality) <ref> [Abr92, Mus93] </ref>. It is estimated that routine application of SRE does not add more than several percent to the overall cost of a project. <p> Operational profile is a set of relative frequencies (or probabilities) of occurrence of disjoint software operations during its operational use. A detailed discussion of operational profile issues can be found in <ref> [Mus87, Mus93] </ref>. A software-based system may have one or more operational profiles. Operational profiles are used to select test cases and direct development, testing and maintenance efforts towards the most frequently used or most risky components.
Reference: [Ohb84] <author> M. Ohba, </author> <title> "Software Reliability Analysis Models," </title> <journal> IBM J. of Res. and Development, </journal> <volume> Vol. 28 (4), </volume> <pages> pp. 428-443, </pages> <year> 1984. </year>
Reference-contexts: Lyu96] . A large class of models that can be useful in the context of early testing phases, and nonoperational testing in general, are the so called "S-shaped " models that describe failure intensity that has a mode or a peak <ref> [Yam83, Ohb84, Mus87, Yam93] </ref>. These models derive their name from the S-like shape of their cumulative failure distributions.
Reference: [Pau93] <author> M.C. Paulk, B. Curtis, M. B. Chrissis, and C.V. Weber, </author> <title> "Capability Maturity Model, Version 1.1," </title> <journal> IEEE Software, </journal> <pages> pp. 18-27, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: For instance, it is unlikely that organizations below the third maturity level on the SEI Capability Maturity Model scale <ref> [Pau93] </ref> would have processes that could react to the feedback information in less than one software release cycle. Reliable latency of less than one phase, is probably not realistic for organizations below level 4 .
Reference: [Pha92] <author> H. Pham, ed., </author> <title> Fault-Tolerant Software Systems: </title> <publisher> Techniques and Applications , IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: The allocation approach should be iterative and it should include consideration of alternative solutions. The "balance" one is looking for is between the overall system reliability, and the development schedule and effort (cost). Available options include inclusion of good exception handling capabilities and use of different fault-tolerance techniques <ref> [Pha92, Lyu94] </ref> combined with systems and software risk analysis and management [Boe89]. Use of inspections and reviews is highly recommended in all phases of the software development process [IEE86].
Reference: [Pre92] <author> R.S. Pressman, </author> <title> 1992 ( Third Edition), Software Engineering: A Practitioner's Approach, </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: SRE tracking and analyses are used to improve organizational software development and maintenance process and maximize customer satisfaction. The following paragraphs provide an overview of the principal SRE activities during a typical <ref> [Pre92] </ref> software life-cycle. The IEEE verification and validation (V&V) standard suggests that following V&V tasks be conducted during a software requirements specification and analysis phases: i)software requirements traceability analysis, ii) software requirements evaluation, iii) software requirements interface analysis, iv) system test plan generation, and v) software acceptance test plan generation.
Reference: [San63] <author> Sandler, G. H., </author> <title> Systems Reliability Engineering, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1963. </year>
Reference-contexts: System availability can be expressed in several ways. For example, instantaneous availability is the probability that the system will be available at any random time t during its life. Average availability is the proportion of time, in a specified interval [0,T] that the system is available for use <ref> [San63] </ref>.
Reference: [Sho83] <author> M.L. Shooman, </author> <title> Software Engineering, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: c (T)) and recovery rates ( ^ r c (T)) estimates ^ l c (T) = TotalNumberofFailuresinPeriodT TotalTimeSystemwasOperationalDuringPeriodT (11) TotalNumberofFailuresinPeriodT TotalTimeSystemwasunderRepairorRecoveryDuringPeriodT (12) If the period T is long enough, the average availability approaches steady state availability, A (), which, given some simplifying assumptions, can be described by the following relationship <ref> [Tri82, Sho83] </ref>): 2 Bellcore is an organization that acts as a software quality "watchdog" from within the U.S. telecommunications community.
Reference: [Sin92] <author> N.D. Singpurwalla and R. Soyer, </author> <title> "Nonhomogenous auto-regressive process for tracking (software) reliability growth, and their Bayesian analysis," </title> <journal> J. of the Royal Statistical Society, </journal> <volume> B 54, </volume> <pages> 145-156, </pages> <year> 1992. </year>
Reference-contexts: A more processoriented approach is discussed in [Vou93, Lyu96]. Highly correlated nature of the early software verification and testing events may require the use of a more sophisticated, timeseries, approach <ref> [Sin92] </ref>. RAMS98/SRE/Vouk/V1.0/15-Sep97 13 4.2 Availability Another important practical measure for software quality is availability. For example, the Bellcore 2 unavailability target for telecommunications network elements is about 3 minutes of downtime per year.
Reference: [Tri82] <author> K. S. Trivedi, </author> <title> Probability & Statistics with Reliability, Queuing, </title> <booktitle> and Computer Science Applications, </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1982. </year> <note> RAMS98/SRE/Vouk/V1.0/15-Sep97 23 </note>
Reference-contexts: c (T)) and recovery rates ( ^ r c (T)) estimates ^ l c (T) = TotalNumberofFailuresinPeriodT TotalTimeSystemwasOperationalDuringPeriodT (11) TotalNumberofFailuresinPeriodT TotalTimeSystemwasunderRepairorRecoveryDuringPeriodT (12) If the period T is long enough, the average availability approaches steady state availability, A (), which, given some simplifying assumptions, can be described by the following relationship <ref> [Tri82, Sho83] </ref>): 2 Bellcore is an organization that acts as a software quality "watchdog" from within the U.S. telecommunications community.
Reference: [Vou93] <author> M. A. Vouk and K.C. Tai, </author> <title> "Some Issues in Multi-Phase Software Reliability Modeling," </title> <booktitle> in Proc. CASCON '93, </booktitle> <pages> pp. 513-523, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: These models derive their name from the S-like shape of their cumulative failure distributions. Figures 3 and 4 illustrate use of a Weibull-type model [Mus87] during unit and integration testing phases of a telecommunications software product <ref> [Vou93, Lyu96] </ref>. 0 50 100 150 200 250 300 350 Execution Time Empirical Intensity Weibull Fit Weibull Average Intensity Empirical Average Intensity Exposure is the cumulative test case execution time "t". <p> Several authors have published models that attempt to relate some early software metrics, such as, the size of the code, Halstead length, or cyclomatic number, to the failure proneness of a program [Kho90, Mun92, Bri93]. A more processoriented approach is discussed in <ref> [Vou93, Lyu96] </ref>. Highly correlated nature of the early software verification and testing events may require the use of a more sophisticated, timeseries, approach [Sin92]. RAMS98/SRE/Vouk/V1.0/15-Sep97 13 4.2 Availability Another important practical measure for software quality is availability.
Reference: [Xie91] <author> M. Xie, </author> <title> Software Reliability Modeling, </title> <publisher> World Scientific, </publisher> <address> Singapore, New Jersey, London, Hong Kong, </address> <year> 1991. </year>
Reference-contexts: Metrics and Models A significant set of SRE activities are concerned with measurement and prediction of software reliability and availability. This includes, modeling of software failure behavior, and modeling of the process that develops and removes faults. A number of metrics and models are available for that purpose <ref> [Mus87, IEE88a, IEE88b, Mal91, Xie91, AIA93] </ref>. This section examines the basic ideas. 4.1 Reliability We distinguish two situations. In one situation, detected problems are further pursued and fault identification and correction takes place, for example, during software development, system and field testing, and active field maintenance.
Reference: [Yam83] <author> S. Yamada, M. Ohba, and S. Osaki, </author> <title> "S-Shaped Reliability Growth Modeling for Software Error Detection," </title> <journal> IEEE Tran. on Reliability, </journal> <volume> Vol. R-32 (5), </volume> <pages> pp. 475-478, </pages> <year> 1983. </year>
Reference-contexts: Lyu96] . A large class of models that can be useful in the context of early testing phases, and nonoperational testing in general, are the so called "S-shaped " models that describe failure intensity that has a mode or a peak <ref> [Yam83, Ohb84, Mus87, Yam93] </ref>. These models derive their name from the S-like shape of their cumulative failure distributions.
Reference: [Yam93] <author> S. Yamada, J. Hishitani, and S. Osaki, </author> <title> "Software-Reliability Growth with Weibull Test-Effort: A Model and Application," </title> <journal> IEEE Trans. Reliability, </journal> <volume> Vol. 42(1), </volume> <pages> pp. 100-106, </pages> <year> 1993. </year>
Reference-contexts: is known what effort expenditure is required to detect a failure, identify and correct the corresponding fault, and how much cost is associated with exposure time, it is possible to construct economic models that relate the testing not only to the resultant quality, but also to the expended effort (cost) <ref> [Mus87, Ehr93, Yam93] </ref>. The estimates given in the above example are known as point estimates since they involve only the "most likely" or the "best" value. <p> Lyu96] . A large class of models that can be useful in the context of early testing phases, and nonoperational testing in general, are the so called "S-shaped " models that describe failure intensity that has a mode or a peak <ref> [Yam83, Ohb84, Mus87, Yam93] </ref>. These models derive their name from the S-like shape of their cumulative failure distributions.
References-found: 41

