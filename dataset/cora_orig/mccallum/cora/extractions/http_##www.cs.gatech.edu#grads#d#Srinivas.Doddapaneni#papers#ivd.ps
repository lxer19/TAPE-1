URL: http://www.cs.gatech.edu/grads/d/Srinivas.Doddapaneni/papers/ivd.ps
Refering-URL: http://www.cs.gatech.edu/grads/d/Srinivas.Doddapaneni/pubs.html
Root-URL: 
Title: Determining Transformation Sequences for Loop Parallelization  
Author: Bill Appelbe Srinivas Doddapaneni Charles Hardnett Kurt Stirewlt Kevin Smith 
Address: Atlanta, GA 30332  Atlanta, GA  
Affiliation: College of Computing Georgia Institute of Technology  Department of Mathematics and Computer Science Emory University  
Abstract: In this paper we present an algorithm for selecting a sequence of transformations which, applied to a given loop, will yield an equivalent maximally parallel loop. The model is extensible to loop nests, and loops with control dependences. We also discuss incorporating performance models to determine the profitability of parallelism into the algorithm. The algorithms provided in the paper have been implemented and tested in PAT, a tool for interactive parallelization of Fortran. Categories and Subject Descriptors: [Programming Languages]: Processors|compilers, optimization General Terms: Compilers Additional Key Words and Phrases: Dependence Analysis, Parallel Programming 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: Scalar assignments within a parallel loop can often be treated independently, as instances of reduction variables [16, 2] or they can be expanded to arrays <ref> [1] </ref>. Assignments to scalar variables can be handled without array expansion if the following restriction is met. After transformations on the loop body, the scalar value should be accessed either in the current iteration or in the next iteration. Therefore a scalar variable is sufficient to hold the current value.
Reference: [2] <author> Bill Appelbe, Charlie McDowell, and Kevin Smith. Start/Pat: </author> <title> A parallel-programming toolkit. </title> <journal> IEEE Software, </journal> <volume> 6(4) </volume> <pages> 29-38, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Alignment of assignments within inner loops can also require that both the inner and outer loops are also aligned if the sign (direction) of the dependence is different in the inner loop. Scalar assignments within a parallel loop can often be treated independently, as instances of reduction variables <ref> [16, 2] </ref> or they can be expanded to arrays [1]. Assignments to scalar variables can be handled without array expansion if the following restriction is met. After transformations on the loop body, the scalar value should be accessed either in the current iteration or in the next iteration.
Reference: [3] <author> Bill Appelbe and Kevin Smith. </author> <title> Analyzing loops for parallelism. </title> <type> Technical Report GIT-ICS-90/59, </type> <institution> Georgia Institute of Technology, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Therefore a scalar variable is sufficient to hold the current value. Reassignments to arrays can often be reduced to a single assignment by detecting the final assignment and replacing other references with temporaries <ref> [3] </ref>. Throughout the definitions of the transformations below we will adopt the following notation for programs: I is the loop variable, K, K1, K2, ... are integer constants, positive constants are denoted by + superscripts, such as K + . <p> Complete algorithms for constructing the sets T, C, and D from the set of NODES and EDGES for an IVD graph can be found in <ref> [3] </ref>. <p> It is extensible to special cases including constant subscripts, control branches, and nested loops <ref> [3] </ref>. 5.2 Reconstruction of a Program from the IVD Graph Reconstructing a program from an IVD graph is similar to algorithms for vectorization based upon partitioning the dependence graph into strongly connected components (PI-blocks) [19, page 206]. <p> Complete algorithms are given in <ref> [3] </ref>. There are a few local optimizations that can be ap plied to the generated program: * Optimization of alignment. The conditional form of alignment, page 10, is the simplest to generate but least efficient. <p> The effects of non-local memory access and cache misses The development of such a model is an ongoing research project, at present targeted at the KSR-1. It is relatively easy to develop an approximate model for the overhead of parallel constructs and transformed statments <ref> [3] </ref>.
Reference: [4] <author> William F. Appelbe and Bala Lakshmanan. </author> <title> Optimizing parallel programs using affinity regions. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: two arrays A (N; N ) and B (N; N ) into a single array AB (2; N; N ). * Prefetching data (either for this iteration or for subsequent iterations) Inter-loop transformations that can affect locality are * Interchanging loops * Fusing loops, or groups loops into affinity regions <ref> [4] </ref> * Loop interchange, or exchanging subscripts * Loop skewing (this usually degrades locality) The interplay between transformations for locality and transformations for parallelism is an open research issue.
Reference: [5] <author> Vasanth Balasundarum. </author> <title> Itereactive Parallelization of Numerical Scientific Programs. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> June </month> <year> 1989. </year> <title> Regular Sections summarize dependences in programs. </title>
Reference-contexts: Transformations which protect a dependence do not actually eliminate the dependence; instead, they introduce explicit synchronization (LOCKs or EVENTs) which prevents concurrent access/update. * Development of program representations such as SSA/PDG [10, 9] and algorithms for systematiz ing the removal of all dependences <ref> [7, 5] </ref>. 1 This paper addresses the third category. Callahan [7] developed a systematic approach to parallelization based upon minimizing the number of barrier synchronization points in a program.
Reference: [6] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: Research on loop paralleliza-tion can be classified as follows: * Development of more explicit dependence tests, so that ranges of complex subscript expressions can be analyzed to determine if two subscript expressions can reference the same memory loca tions <ref> [15, 18, 6, 14] </ref>. * Development of new and improved transformations for removing or protecting dependences. Transformations which remove a dependence restructure the loop so that the dependent references lie entirely within a single iteration.
Reference: [7] <author> David Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <year> 1987. </year> <type> Rice Tech Report, </type> <institution> COMP TR87-50. </institution>
Reference-contexts: Transformations which protect a dependence do not actually eliminate the dependence; instead, they introduce explicit synchronization (LOCKs or EVENTs) which prevents concurrent access/update. * Development of program representations such as SSA/PDG [10, 9] and algorithms for systematiz ing the removal of all dependences <ref> [7, 5] </ref>. 1 This paper addresses the third category. Callahan [7] developed a systematic approach to parallelization based upon minimizing the number of barrier synchronization points in a program. <p> Callahan <ref> [7] </ref> developed a systematic approach to parallelization based upon minimizing the number of barrier synchronization points in a program. By contrast, our approach to parallelization is based upon the fork-join model of parallelism, and focuses upon achieving maximum parallelism for an individual loop. <p> The correctness of the transformations are usually informally argued, based upon dependences and other program constraints. Some authors <ref> [7] </ref> have looked at sequences of transformations based upon global optimization criteria, but there has been no attempt to our knowledge to develop a formal framework for the entire process or to implement a systematic approach to selecting and applying sequences of transformations. <p> Once positive edges have been removed to reduce the connectivity of the DAG, negative edges must be removed by assignment replication of each subtree via bottom-up recursion. (This has exponential cost when done inside the loop <ref> [7] </ref>. However, if assignments are split into separate parallel loops, enforcing a barrier synchronization between the expressions, the exponential cost is reduced to a linear cost).
Reference: [8] <author> David Callahan. </author> <title> Recognizing and parallelizing bounded recurrences. </title> <booktitle> In Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: We identify a maximally par-allelized loop as one in which the only remaining dependences are in single statement recurrence (SSR) sequential loops, and in which the number of separate parallel loops is minimal. SSR's can sometimes be par-allelized somewhat further <ref> [8] </ref> but we do not attempt to do so. The analysis below assumes that the number of processors p is `significantly less' than the number of loop iterations N for loops considered for paralleliz-ing.
Reference: [9] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Transformations which protect a dependence do not actually eliminate the dependence; instead, they introduce explicit synchronization (LOCKs or EVENTs) which prevents concurrent access/update. * Development of program representations such as SSA/PDG <ref> [10, 9] </ref> and algorithms for systematiz ing the removal of all dependences [7, 5]. 1 This paper addresses the third category. Callahan [7] developed a systematic approach to parallelization based upon minimizing the number of barrier synchronization points in a program.
Reference: [10] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <year> 1987. </year>
Reference-contexts: Transformations which protect a dependence do not actually eliminate the dependence; instead, they introduce explicit synchronization (LOCKs or EVENTs) which prevents concurrent access/update. * Development of program representations such as SSA/PDG <ref> [10, 9] </ref> and algorithms for systematiz ing the removal of all dependences [7, 5]. 1 This paper addresses the third category. Callahan [7] developed a systematic approach to parallelization based upon minimizing the number of barrier synchronization points in a program. <p> Such equivalence is clearly transitive. To discuss sequences of transformations we need to represent the program more abstractly, with information that describes program dependences. Examples of such representations are Program Dependence Graphs (PDGs) <ref> [10] </ref>. The representation we adopt below, the IVD graph, is tailored to representing loop-carried dependences. Reconstructing programs from such representations is more difficult than the syntactic transformation between source programs and abstract programs. <p> Source code transformations such as if-conversion can be used to represent control-flow statements. A more general approach for representing control dependences is the Program Dependence Graph (PDG) <ref> [10] </ref>, and later we discuss how this representation can be merged with the IVD representation. Alignment of assignments within inner loops can also require that both the inner and outer loops are also aligned if the sign (direction) of the dependence is different in the inner loop.
Reference: [11] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability, A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco, California, </address> <year> 1979. </year>
Reference-contexts: Determining the minimum number of nodes which must be replicated to break all cycles is an NP-complete problem; it is equivalent to determining the minimum number of edges which must be removed in converting an arbitrary graph to a DAG <ref> [11, page 192] </ref>. The heuristic of choosing nodes in the maximum number of cycles is a simple but effective approach. Expression inlining can only shrink cycles to SSRs, or to cycles of nodes, each of which is an SSR.
Reference: [12] <author> Alan H. Karp. </author> <title> Programming for parallelism. </title> <journal> Computer, </journal> <volume> 20(5) </volume> <pages> 43-57, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: that is, both the program transformation and the graph transformation lead to programs that are equivalent to the original program. 3 A Parallel Programming Model The most widely used languages for programming shared memory multiprocessors are dialects of sequential languages such as Fortran and C with extensions for fork-join parallelism <ref> [12] </ref> in which a parallel program consists of sequences of alternating serial code and parallel loops and sections. All iterations of a parallel loop, or all cases of a parallel section, can be executed in parallel by an arbitrary number of tasks.
Reference: [13] <author> Ken Kennedy and Kathryn McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 323-334, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Some methods have been developed for combining sequences of inter-loop transformations [17], when dependence distances or directions are known to maximize the parallelism. Other authors <ref> [13] </ref> have developed algorithms for loop nests which focus upon optimizing data locality. The interaction between these approachs, and our approach for extracting maximum parallelism from a single loop, are an open research issue. We discuss the interaction between these approaches later. <p> of peeling iterations can be ignored). * Static scheduling is used so that the overhead of a parallel loop is independent of the number of iterations * The relative cost of statements (assignments and expressions) is known Unfortunately, in practice the effects of non-local memory and cache cannot be ignored <ref> [13] </ref>. A perfor mance model needs to take into account * New cache lines accessed (misses) by loop itera tions 15 * Non-local memory blocks accessed by loop itera- tions Estimating these requires a machine model.
Reference: [14] <author> William Pugh. </author> <title> The Omega test, a fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 4-13, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Research on loop paralleliza-tion can be classified as follows: * Development of more explicit dependence tests, so that ranges of complex subscript expressions can be analyzed to determine if two subscript expressions can reference the same memory loca tions <ref> [15, 18, 6, 14] </ref>. * Development of new and improved transformations for removing or protecting dependences. Transformations which remove a dependence restructure the loop so that the dependent references lie entirely within a single iteration.
Reference: [15] <author> Kevin Smith and Bill Appelbe. </author> <title> Interactive conversion of sequential to multitasking FORTRAN. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 225-234, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Research on loop paralleliza-tion can be classified as follows: * Development of more explicit dependence tests, so that ranges of complex subscript expressions can be analyzed to determine if two subscript expressions can reference the same memory loca tions <ref> [15, 18, 6, 14] </ref>. * Development of new and improved transformations for removing or protecting dependences. Transformations which remove a dependence restructure the loop so that the dependent references lie entirely within a single iteration.
Reference: [16] <author> Kevin S. Smith. PAT: </author> <title> An Interactive Fortran Parallelizing Assistant Tool. </title> <type> PhD thesis, </type> <institution> Georgia Institute of Technology, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: Alignment of assignments within inner loops can also require that both the inner and outer loops are also aligned if the sign (direction) of the dependence is different in the inner loop. Scalar assignments within a parallel loop can often be treated independently, as instances of reduction variables <ref> [16, 2] </ref> or they can be expanded to arrays [1]. Assignments to scalar variables can be handled without array expansion if the following restriction is met. After transformations on the loop body, the scalar value should be accessed either in the current iteration or in the next iteration.
Reference: [17] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: These transformations are interrelated, and like intra-loop transformations can be applied in various orders to yield different loop structures (to which intra-loop transformations can subsequently be applied to yield maximally parallel loop structures). Some methods have been developed for combining sequences of inter-loop transformations <ref> [17] </ref>, when dependence distances or directions are known to maximize the parallelism. Other authors [13] have developed algorithms for loop nests which focus upon optimizing data locality. The interaction between these approachs, and our approach for extracting maximum parallelism from a single loop, are an open research issue.
Reference: [18] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction The majority of the potential parallelism in numerical/scientific programs occurs in loops. However, before loops can be parallelized they must often be transformed. Such transformations have as their principal goal the removal of all loop-carried dependences <ref> [18] </ref>, in which an array variable is updated in one iteration and accessed in another. <p> Research on loop paralleliza-tion can be classified as follows: * Development of more explicit dependence tests, so that ranges of complex subscript expressions can be analyzed to determine if two subscript expressions can reference the same memory loca tions <ref> [15, 18, 6, 14] </ref>. * Development of new and improved transformations for removing or protecting dependences. Transformations which remove a dependence restructure the loop so that the dependent references lie entirely within a single iteration. <p> Other transformations such as unrolling, fusion, skewing and interchange <ref> [18] </ref>, are all inter-loop transformations. Inter-loop transformations can be used to transform a program to generate the largest par-allelizable loop for that program. <p> The IVD Graph representation of dependences is tailored to the transformations we wish to apply to a loop body. It omits information not required by our analysis (such as direction vectors for nested or containing loops <ref> [18] </ref>, or transitive dependences) and provides additional information required, including specific references rather than statements for the source and sink of a dependence, and distinguishing a node's type which is useful in selecting parallelizing transformations.
Reference: [19] <author> Hans Zima and Barbara Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1990. </year> <month> 16 </month>
Reference-contexts: It is extensible to special cases including constant subscripts, control branches, and nested loops [3]. 5.2 Reconstruction of a Program from the IVD Graph Reconstructing a program from an IVD graph is similar to algorithms for vectorization based upon partitioning the dependence graph into strongly connected components (PI-blocks) <ref> [19, page 206] </ref>. However, the IVD graph has already undergone transformations so that dependences which require loop distribution have been identified (loop edges).
References-found: 19

