URL: ftp://enws318.eas.asu.edu/pub/rao/der-po-aaai94.ps
Refering-URL: http://enuxsa.eas.asu.edu:80/~ihrig/
Root-URL: 
Email: email: laurie.ihrig@asu.edu rao@asu.edu  
Title: Derivation Replay for Partial-Order Planning  
Author: Laurie H. Ihrig Subbarao Kambhampati 
Address: Tempe, AZ 85287-5406  
Affiliation: Department of Computer Science and Engineering Arizona State University,  
Date: August, 1994  
Note: To appear in Proc. 12th Natl. Conf. on Artificial Intelligence (AAAI-94),  
Abstract: Derivation replay was first proposed by Carbonell as a method of transferring guidance from a previous problem-solving episode to a new one. Subsequent implementations have used state-space planning as the underlying methodology. This paper is motivated by the acknowledged superiority of partial-order (PO) planners in plan generation, and is an attempt to bring derivation replay into the realm of partial-order planning. Here we develop DerSNLP, a framework for doing replay in SNLP, a partial-order plan-space planner, and analyze its relative effectiveness. We will argue that the decoupling of planning (derivation) order and the execution order of plan steps, provided by partial-order planners, enables DerSNLP to exploit the guidance of previous cases in a more efficient and straightforward fashion. We validate our hypothesis through empirical comparisons between DerSNLP and two replay systems based on state-space planners. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barrett, A. and Weld, D. </author> <year> 1994. </year> <title> Partial order planning: evaluating possible efficience gains. </title> <booktitle> Artificial Intelligence 67(1). </booktitle>
Reference-contexts: We will validate this hypothesis by comparing DerSNLP to replay systems implemented on two different state-space planners: NOLIMIT (which was recently the basis for a comprehensive DA implementation in (Veloso 1992)), and TOPI <ref> (Barrett and Weld 1994) </ref>. DerSNLP: Derivation Replay for SNLP As we mentioned earlier, derivation replay involves storing traces of previous problem-solving decisions and replaying them to solve similar problems more efficiently. <p> Empirical Evaluation An empirical analysis was conducted in order to test our hypothesis regarding the relative effectiveness of eager replay for PO planners. To do this we chose two state-space planners, TOPI <ref> (Barrett and Weld 1994) </ref> and NOLIMIT (Veloso 1992). We implemented eager replay on these planners and compared their performance with DerSNLP. TOPI does simple backward search in the space of states, adding steps to the plan in reverse order of execution. <p> Thus our hypothesis is only that DerSNLP is less likely to be misled (and thus more likely to exploit the previous case) by eager replay. Domains ART-MD-NS domain: Experiments were run on problems drawn from two domains. The first was the artificial domain, ART-MD-NS, originally described in <ref> (Barrett and Weld 1994) </ref> and shown in the table below: ART-MD-NS (D m S 2 ): i precond : I i add : P i delete : fI j jj &lt; ig) A 2 i precond : P i add : G i delete : fI j j8jg [ fP j
Reference: <author> Bhansali, S. and Harandi, M. </author> <year> 1991. </year> <title> Synthesizing unix shell scripts using derivational analogy: an empirical assessment. </title> <booktitle> In Proceedings AAAI-91. </booktitle>
Reference: <author> Blumenthal, B. and Porter, B. </author> <year> 1994. </year> <title> Analysis and empirical studies of derivational analogy. </title> <journal> Artificial Intelligence. Forthcoming. </journal>
Reference-contexts: We will then use DerSNLP as a case-study to explore the relative advantages of doing replay within plan-space vs. state-space planners. One of the difficult decisions faced by the replay systems is that of deciding when and where to interleave from-scratch effort with derivation replay (c.f. <ref> (Blumenthal and Porter 1994) </ref>). In general, there are no domain-independent grounds for making this decision. This makes eager replay, i.e., replaying the entire trace before returning to from-scratch planning, the most straightforward strategy. <p> The early work on derivation replay that is rooted in state-space planning has therefore been forced to focus a good deal of attention on the problem of determining when to plan for additional goals <ref> (Blumenthal and Porter 1994) </ref>. It is not an easy matter to determine at what point in the derivation to stop in order to insert further choices corresponding to extra goals. For the PO planner, there is less need to make the difficult decision as to when to interrupt replay. <p> We believe the main reason for this may be the strong presence of interacting goals in our multi-goal problems, coupled with the fact that we used vanilla planning algorithms without any sophisticated backtracking strategies or pruning techniques (Veloso 1992). While our experiments used an eager-replay strategy, work by Blumenthal <ref> (Blumenthal and Porter 1994) </ref> provides heuristic strategies aimed at interleaving planning and replay effort. However, it is our contention that the shift to PO planning obviates to a large extent the need for interleaving.
Reference: <author> Carbonell, J. </author> <year> 1986. </year> <title> Derivational analogy: A theory of reconstructive problem solving and expertise acquisition. </title> <editor> In Michalski, Ryszard; Carbonell, Jaime; and Mitchell, Tom M., editors 1986, </editor> <booktitle> Machine Learning: an Artificial Intelligence approach: </booktitle> <volume> Volume 2. </volume> <month> Morgan-Kaufman. </month>
Reference-contexts: By this method, a trace of a previous search process is retrieved and replayed in solving a new problem. The decisions that led to a successful solution in the prior case are used to guide the new search process. After its first proposal in <ref> (Carbonell 1986) </ref>, DA has subsequently been found to be of use in many areas, including planning, problem solving, design and automatic programming (Veloso 1992; Mostow 1989; Blumenthal and Porter 1994; Bhansali and Harandi 1991). Much of this work has been done in state-space problem solvers.
Reference: <author> Kambhampati, S. and Chen, J. </author> <year> 1993. </year> <title> Relative utility of ebg based plan reuse in partial ordering vs total ordering planning. </title> <booktitle> In Proceedings AAAI-93. </booktitle> <address> 514--519. Washington, D.C. </address>
Reference-contexts: However, it is our contention that the shift to PO planning obviates to a large extent the need for interleaving. Finally, our conclusions regarding the advantages of PO planning in replay also complement the recent results regarding its advantages in reuse <ref> (Kambhampati and Chen 1993) </ref>. Conclusion In this paper, we described DerSNLP, a framework for doing derivation replay within SNLP, a partial-order planner. We then compared DerSNLP with replay systems for state-space planners.
Reference: <author> Kambhampati, S. </author> <year> 1994. </year> <title> Exploiting causal structure to control retrieval and refitting during plan reuse. </title> <journal> Computational Intelligence Journal 10(2). </journal>
Reference-contexts: The storage and retrieval aspects of DA remain the same whether we use plan-space or state-space planners. In particular, solutions to the storage problem such as those proposed in (Veloso 1992) and <ref> (Kambhampati 1994) </ref> can be used for this purpose. Only the contents of the trace and the details of the replay component depend on the underlying planner. Thus, in the current work, we focus on the automatic generation and replay of the solution trace.
Reference: <author> McAllester, D. and Rosenblitt, </author> <title> D 1991. Systematic nonlinear planning. </title> <booktitle> In Proceedings AAAI-91. </booktitle> <address> 634--639. </address>
Reference: <author> Minton, S.; Drummond, M.; Bresina, J.; and Philips, </author> <title> A 1992. Total order vs partial order planning: factors influencing performance. </title> <booktitle> In Proceedings KR-92. </booktitle>
Reference: <author> Mostow, J. </author> <year> 1989. </year> <title> Automated replay of design plans: Some issues in derivational analogy. </title> <journal> Artificial Intelligence 40:119--184. </journal>
Reference: <author> Veloso, M. </author> <year> 1992. </year> <title> Learning by analogical reasoning in general problem solving. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie-Mellon University. </institution>
Reference-contexts: Thanks to Manuela Veloso for helpful clarifications regarding Prodigy/Analogy, and to Suresh Katukam and Ed Smith for their comments. Derivational analogy includes all of the following elements <ref> (Veloso 1992) </ref>: a facility within the base-level planner to generate a trace of the derivation of a problem solution, the indexing and storage of the solution trace in a library of cases, the retrieval of a case in preparation for solving a new problem, and finally, a replay mechanism by which <p> The storage and retrieval aspects of DA remain the same whether we use plan-space or state-space planners. In particular, solutions to the storage problem such as those proposed in <ref> (Veloso 1992) </ref> and (Kambhampati 1994) can be used for this purpose. Only the contents of the trace and the details of the replay component depend on the underlying planner. Thus, in the current work, we focus on the automatic generation and replay of the solution trace. <p> We will validate this hypothesis by comparing DerSNLP to replay systems implemented on two different state-space planners: NOLIMIT (which was recently the basis for a comprehensive DA implementation in <ref> (Veloso 1992) </ref>), and TOPI (Barrett and Weld 1994). DerSNLP: Derivation Replay for SNLP As we mentioned earlier, derivation replay involves storing traces of previous problem-solving decisions and replaying them to solve similar problems more efficiently. <p> Choice points correspond to backtracking points in whatever planning algorithm has been selected. The content of each choice therefore reflects the underlying methodology. For example, a state-space means-ends analysis (MEA) planner such as NOLIMIT <ref> (Veloso 1992) </ref> makes decisions as to which subgoal to accomplish next, which step to use to achieve the subgoal, as well as when to add an applicable step to the plan. For SNLP, a search node corresponds to a partly-constructed partially-ordered plan. SNLP makes two types of decisions. <p> DerSNLP extends SNLP by including a replay facility. Its output is a trace of the decision process that led to its final solution. Figure 1 contains an example trace produced by DerSNLP while attempting a problem from the logistics transportation domain of <ref> (Veloso 1992) </ref>. This domain involves the movement of packages across locations by various transport devices. The trace corresponds to a simple problem which contains the goal of getting a single package, OB2, to a designated airport, AP1. <p> This avoids the decision of how to interleave the replay of multiple cases, as well as how to interleave replay with from-scratch planning, both decisions that must be faced by NOLIMIT <ref> (Veloso 1992) </ref>. It is our contention that interleaving is not as important for replay in a plan-space framework. Each instruction in the trace is therefore visited in succession. <p> Empirical Evaluation An empirical analysis was conducted in order to test our hypothesis regarding the relative effectiveness of eager replay for PO planners. To do this we chose two state-space planners, TOPI (Barrett and Weld 1994) and NOLIMIT <ref> (Veloso 1992) </ref>. We implemented eager replay on these planners and compared their performance with DerSNLP. TOPI does simple backward search in the space of states, adding steps to the plan in reverse order of execution. <p> We implemented eager replay on these planners and compared their performance with DerSNLP. TOPI does simple backward search in the space of states, adding steps to the plan in reverse order of execution. NOLIMIT is a version of PRODIGY which was the basis of the DA system reported in <ref> (Veloso 1992) </ref>. Like PRODIGY and STRIPS, it uses means-ends analysis, attempting goals by backward-chaining from the goal state. Applicable operators (operators whose preconditions are true in the current state) are added to the end of the plan and the current state is advanced appropriately. <p> The plan for the new conjunctive goal G 1 ^ G 2 ^ G 3 is A 1 2 ! A 1 1 ! A 2 3 . Logistics Transportation Domain: The logistics transportation domain of <ref> (Veloso 1992) </ref> was adopted for the second set of experiments. Initial conditions of each problem represented the location of various transport devices (one airplane and three trucks) over three cities, each city containing an airport and a post office. Four packages were randomly distributed over airports. <p> Related Work Previous research in DA which is rooted in state-space planning has demonstrated significant performance improvements with replay <ref> (Veloso 1992) </ref>. In contrast, our experiments show rather poor replay performance on the part of the state-space planners for the more complex problems. <p> We believe the main reason for this may be the strong presence of interacting goals in our multi-goal problems, coupled with the fact that we used vanilla planning algorithms without any sophisticated backtracking strategies or pruning techniques <ref> (Veloso 1992) </ref>. While our experiments used an eager-replay strategy, work by Blumenthal (Blumenthal and Porter 1994) provides heuristic strategies aimed at interleaving planning and replay effort. However, it is our contention that the shift to PO planning obviates to a large extent the need for interleaving.
References-found: 10

