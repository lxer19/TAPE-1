URL: http://www.tns.lcs.mit.edu/~djw/library/flowsim.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/
Root-URL: 
Email: fjahn,danzigg@usc.edu  
Phone: 213-740-7285 (Fax)  
Title: Speedup vs. Simulation Granularity  
Author: Jong-Suk Ahn and Peter B. Danzig 
Note: 213-740-4780 (Office)  
Address: Los Angeles, CA 90089-0781  
Affiliation: Computer Science Department University of Southern California  
Abstract: This paper describes a packet network simulator whose timing granularity can shift continuously from fine, packet-level detail to coarse, conversation-level detail. Simulation run time decreases with coarser timing granularity, but the details in the underlying model become faded as the timing granularity coarsens. The finer the granularity, the slower but more precise the simulation. If a simulation becomes resource limited, it is possible to coarsen the timing granularity to scale the simulation larger. This paper introduces a new simulation technique to speedup simulation of high speed, wide area networks. The new technique can yield order of magnitude speedup and memory savings on simulations of large-scale packet networks. The speedup is achieved by introducing a degree of approximation into abstracting packet streams. We call this technique Flowsim. Flowsim can yield different simulation metrics than packet simulation due to its different degree of simulation granularity. We have replicated simulations presented in the literature and, in this paper, show that it is frequently possible to employ coarse timing granularity when studying flow and congestion control algorithms. This paper motivates the need for faster and less memory intensive simulators, introduces two other simulation techniques which can together double the performance of well written simulators, describes Flowsim's traffic representation scheme and internal algorithms, explores the tradeoff between speed and accuracy, and explores some of the limitations of traditional simulation. It demonstrates that speedups of a factor of 50 are possible when simulating ATM networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dimitri Bertsekas and Robert Gallager. </author> <title> Data Networks. </title> <publisher> Prentice-Hall, </publisher> <year> 1987. </year>
Reference-contexts: A typical simulation might examine the interaction of transport layer flow control and router packet scheduling algorithms. To see why the performance analyst simulates rather than models, consider the difference between fixed window and slow-start TCP. While analytical methods enjoy some small success modeling fixed-window flow con trol algorithms <ref> [1] </ref>, no one uses fixed-window algorithms anymore; Jacobson's slow-start [15] TCP has completely replaced them. No one has successfully modeled slow-start analytically for several reasons.
Reference: [2] <author> Lawrence S. Brakmo, Sean W. O'Malley, and Larry L. Peterson. </author> <title> Tcp vegas: New techniques for congestion detection and avoidance. </title> <booktitle> ACM SIG-COMM '94, </booktitle> <month> May </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: Consider, for instance, two overlapping trains on the same output buffer. Let one have 10 packets on the interval <ref> [2, 8] </ref> and the other 7 packets on the interval [5, 12]. Flowsim would approximate that (3/6)*10 of the first train's packets are sent before the first packet of the second train. With priority scheduling, switches can only forward trains that are ready to depart before the lookahead time. <p> Flowsim also could be used to study call routing and call admission control algorithms in a mix of 11 real-time and data traffic. 6.2 Extensibility We applied Flowsim to a study of a network congestion detection algorithm, Tri-S, a variation of which is employed in Vegas TCP <ref> [24, 2] </ref>. Tri-S monitors the network traffic and stops increasing congestion window before packet loss when the measured traffic congestion is beyond a certain threshold. Figure 15 and Figure 16 plot the congestion window of an FTP competing for a gateway's finite buffer with another FTP.
Reference: [3] <author> Randy Brown. </author> <title> Calendar queues: A fast o(1) priority queue implementation for the simulation event set problem. </title> <journal> Communications of the ACM, </journal> <volume> 31(10) </volume> <pages> 1220-1227, </pages> <year> 1988. </year>
Reference-contexts: Event lists are built from priority queues. Although naive priority queues work best with a small number of events, sophisticated ones exhibit O (1) behavior even with a large number of events if the variation in the number of events remains low <ref> [3, 22] </ref>. This leads us to ponder how the number of events in the event list fluctuates with time. The bandwidth-delay product of a link establishes the maximum number of packets that it can carry. <p> When the number of enequeued events does not vary, calendar queues will use the same bucket width during the rest of simulation duration. Let's assume that when a calendar queue determines the bucket width, the calendar queue represents time interval <ref> [3, 10] </ref> which can be divided into two sub-intervals according to event distribution; [3, 4] with 0.1 millisecond average inter-event gap and [4, 10] with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> Let's assume that when a calendar queue determines the bucket width, the calendar queue represents time interval [3, 10] which can be divided into two sub-intervals according to event distribution; <ref> [3, 4] </ref> with 0.1 millisecond average inter-event gap and [4, 10] with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> when a calendar queue determines the bucket width, the calendar queue represents time interval [3, 10] which can be divided into two sub-intervals according to event distribution; <ref> [3, 4] </ref> with 0.1 millisecond average inter-event gap and [4, 10] with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10].
Reference: [4] <author> John B. Carter and Willy Zwaenepoel. </author> <title> Optimistic implementation of bulk data tranfer protocols. </title> <booktitle> 1989 ACM SIGMETRICS Conference, </booktitle> <pages> pages 61-69, </pages> <month> May 23-26, </month> <year> 1989. </year>
Reference-contexts: Let's assume that when a calendar queue determines the bucket width, the calendar queue represents time interval [3, 10] which can be divided into two sub-intervals according to event distribution; <ref> [3, 4] </ref> with 0.1 millisecond average inter-event gap and [4, 10] with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> Let's assume that when a calendar queue determines the bucket width, the calendar queue represents time interval [3, 10] which can be divided into two sub-intervals according to event distribution; [3, 4] with 0.1 millisecond average inter-event gap and <ref> [4, 10] </ref> with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> when a calendar queue determines the bucket width, the calendar queue represents time interval [3, 10] which can be divided into two sub-intervals according to event distribution; <ref> [3, 4] </ref> with 0.1 millisecond average inter-event gap and [4, 10] with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> calendar queue represents time interval [3, 10] which can be divided into two sub-intervals according to event distribution; [3, 4] with 0.1 millisecond average inter-event gap and <ref> [4, 10] </ref> with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> He called such a burst a packet train. He said that if the spacing between two packets exceeds some inter-train gap, that these packets belong to separate trains. In subsequent years, several protocol stack implementations were optimized to exploit this phenomenon <ref> [4, 16] </ref>. More recently, other investigators found that due to the high degree of multiplexing, back-to-back packet trains are not as prominent on wide-area networks as they are for local area networks [14, 10]. Researchers are already calling for a definition of packet trains for wide-area-networks [21].
Reference: [5] <author> Douglas Comer. </author> <title> Internetworking with TCP/IP, </title> <booktitle> volume 1 (2nd edition). </booktitle> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: The oscillating window size makes TCP's round trip time estimator oscillate, which impacts TCP's retransmission strategy. Finally, if only to dash any vestiges of hope, TCP receivers employ several different acknowledgement strategies and TCP senders employ two difficult-to-model retransmission strategies (fast retransmission and Karn's algorithm <ref> [5] </ref> ). Granted that one must simulate rather than model, what does one measure? The essential metrics are, of course, throughput, fairness, delay, loss rate, and link utilization. <p> Consider, for instance, two overlapping trains on the same output buffer. Let one have 10 packets on the interval [2, 8] and the other 7 packets on the interval <ref> [5, 12] </ref>. Flowsim would approximate that (3/6)*10 of the first train's packets are sent before the first packet of the second train. With priority scheduling, switches can only forward trains that are ready to depart before the lookahead time.
Reference: [6] <author> Peter B. Danzig, Sugih Jamin, Ramon Caceres, Danny J. Mitzel, and Deborah Estrin. </author> <title> An artificial workload model of TCP/IP internetworks. </title> <journal> Journal of Internetworking: Practice and Experience, </journal> <volume> 3(1) </volume> <pages> 1-26, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For example, Floyd showed that IP routers tend to self-synchronize and send routing update messages periodically only when the number of routers exceeds a certain threshold [12]. We are now employing randomly generated network topologies driven with models of dynamic data and real-time traffic <ref> [6] </ref> to evaluate various flow control and adaptive routing algorithms.
Reference: [7] <author> Lixia Zhang David Clark, Scott Shenker. </author> <title> Supporting real-time applications in an integrated services packet network: Architecture and mechanism. </title> <booktitle> ACM SIGCOMM 92, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: To contrast slow-start and packet-pair for interactive traffic, the essential metrics are one-way packet delay and loss rates. If the experimenter is investigating admission control and router scheduling algorithms for voice or video traffic <ref> [7] </ref>, the packet loss rate and the tail of the delay distribution are the metrics of interest. Packet simulators are computationally expensive. <p> Flowsim and packet simulation agree more closely for Fair Queueing routers than they do for FIFO routers because TCP conversations crossing Fair Queueing switches do not suffer phase effects. We have sketched but not implemented a way to approximate FIFO+, a scheduling technique designed for multimedia traffic <ref> [7] </ref>. In FIFO+, the header of each packet carries a field indicating whether the packet is behind schedule or ahead of it. In a simulator, this field can be derived from a time stamp that indicates the exact time at which a source emits a packet.
Reference: [8] <author> Alan Demers, Srinivasan Keshav, and Scott Shenker. </author> <title> Analysis and simulation of a fair queueing algorithm. </title> <journal> ACM SIGCOMM 89, </journal> <volume> 19(4) </volume> <pages> 2-12, </pages> <month> August 19-22, </month> <year> 1989. </year>
Reference-contexts: Consider, for instance, two overlapping trains on the same output buffer. Let one have 10 packets on the interval <ref> [2, 8] </ref> and the other 7 packets on the interval [5, 12]. Flowsim would approximate that (3/6)*10 of the first train's packets are sent before the first packet of the second train. With priority scheduling, switches can only forward trains that are ready to depart before the lookahead time. <p> Another important measure of Flowsim's usefulness is whether it can model relevant new network scheduling mechanisms. In addition to FIFO router scheduling, we regularly apply Flowsim to Fair Queueing routers <ref> [8] </ref>. We have even created a compacted buffer space representation for it. Flowsim and packet simulation agree more closely for Fair Queueing routers than they do for FIFO routers because TCP conversations crossing Fair Queueing switches do not suffer phase effects.
Reference: [9] <author> Julio Escobar and Craig Partridge. </author> <title> A proposed segmentation and re-assembly (SAR) protocol for use with asynchronous transfer mode (ATM). </title> <booktitle> IFIP WG6.1/WG6.4, </booktitle> <month> November 27-29 </month> <year> 1990. </year>
Reference-contexts: Given an ATM network's 53 byte per IP cell payload (Only 48 of ATM's 53 bytes carry data, the other 5 bytes get lost to the assembly reassembly protocol <ref> [9] </ref>), this number blossoms to 47,000 cells per link.
Reference: [10] <author> Deborah Estrin and Danny Mitzel. </author> <title> An assessment of state and lookup overhead in routers. </title> <booktitle> IEEE In-focom '92, </booktitle> <pages> pages 2332-2342, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: When the number of enequeued events does not vary, calendar queues will use the same bucket width during the rest of simulation duration. Let's assume that when a calendar queue determines the bucket width, the calendar queue represents time interval <ref> [3, 10] </ref> which can be divided into two sub-intervals according to event distribution; [3, 4] with 0.1 millisecond average inter-event gap and [4, 10] with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> Let's assume that when a calendar queue determines the bucket width, the calendar queue represents time interval [3, 10] which can be divided into two sub-intervals according to event distribution; [3, 4] with 0.1 millisecond average inter-event gap and <ref> [4, 10] </ref> with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> calendar queue represents time interval [3, 10] which can be divided into two sub-intervals according to event distribution; [3, 4] with 0.1 millisecond average inter-event gap and <ref> [4, 10] </ref> with 1 millisecond average inter-event gap. The bucket width determined based on [3, 4] would be too small for events in [4, 10]. <p> In subsequent years, several protocol stack implementations were optimized to exploit this phenomenon [4, 16]. More recently, other investigators found that due to the high degree of multiplexing, back-to-back packet trains are not as prominent on wide-area networks as they are for local area networks <ref> [14, 10] </ref>. Researchers are already calling for a definition of packet trains for wide-area-networks [21]. We now describe how, motivated by wide-area-network packet trains, Flowsim represents closely spaced packets. Each link in Flowsim places a conversation's packets on a linked list of packet trains called a flow descriptor.
Reference: [11] <editor> A Dupuy et al. </editor> <title> NEST: A network simulation and prototyping testbed. </title> <journal> Communications of the ACM, </journal> <volume> 33:10:63-74, </volume> <month> Oct 90. </month>
Reference-contexts: Flowsim is implemented in Jacobson's and McCanne's tcpsim, a simulator which evolved from Berkeley's REAL and Columbia's 8 NEST <ref> [11] </ref>. In the next section we report Flowsim's speedups and contrast its simulated metrics against our packet-by-packet version of this simulator.
Reference: [12] <author> Sally Floyd and Van Jacobson. </author> <title> The synchronization of periodic routing messages. </title> <booktitle> SIGCOMM 93, </booktitle> <pages> pages 33-44, </pages> <month> October, </month> <year> 1993. </year>
Reference-contexts: Consider, for instance, two overlapping trains on the same output buffer. Let one have 10 packets on the interval [2, 8] and the other 7 packets on the interval <ref> [5, 12] </ref>. Flowsim would approximate that (3/6)*10 of the first train's packets are sent before the first packet of the second train. With priority scheduling, switches can only forward trains that are ready to depart before the lookahead time. <p> Simulation of a dozen nodes can mislead one to choose a particular adaptive routing algorithm or flow control algorithm. For example, Floyd showed that IP routers tend to self-synchronize and send routing update messages periodically only when the number of routers exceeds a certain threshold <ref> [12] </ref>. We are now employing randomly generated network topologies driven with models of dynamic data and real-time traffic [6] to evaluate various flow control and adaptive routing algorithms.
Reference: [13] <author> Sally Floyd and Van Jacobson. </author> <title> Traffic phase effects in packet-switched gateways. </title> <journal> Journal of Inter-networking:Practice and Experience, </journal> <volume> 3(3) </volume> <pages> 115-156, </pages> <month> September, </month> <year> 1992. </year>
Reference-contexts: Our experiments investigated the stable unfairness between similarly routed bulk transfer conversations that Floyd identified and termed phase effects <ref> [13] </ref>. Our experiments investigated whether phase effects cause unfairness even with large flow control windows, high network bandwidths, and higher degrees of traffic multiplexing. 5.1 Speedups and Accuracy The total number of events generated during a simulation determines a simulation's run time. <p> Instead, it took over a year of experimentation for us to learn the following lesson: It is not easy to evaluate simulations of small topologies with relatively static traffic sources because the metrics that they yield can be sensitive to minuscule changes in network bandwidths and link lengths <ref> [13] </ref>. Fundamentally, the question is whether we have a firm enough grasp on the factors that affect a simulation's metrics to evaluate flow and congestion control algorithms via simulation. The lesson is that you cannot learn from your experiments if you misattribute the reason why the metrics differ.
Reference: [14] <author> Steven A. Heimlich. </author> <title> Traffic characterization of the NSFNET national backbone. </title> <booktitle> Proceedings Winter USENIX Conference, </booktitle> <pages> pages 207-227, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: In subsequent years, several protocol stack implementations were optimized to exploit this phenomenon [4, 16]. More recently, other investigators found that due to the high degree of multiplexing, back-to-back packet trains are not as prominent on wide-area networks as they are for local area networks <ref> [14, 10] </ref>. Researchers are already calling for a definition of packet trains for wide-area-networks [21]. We now describe how, motivated by wide-area-network packet trains, Flowsim represents closely spaced packets. Each link in Flowsim places a conversation's packets on a linked list of packet trains called a flow descriptor.
Reference: [15] <author> Van Jacobson. </author> <title> Congestion avoidance and control. </title> <booktitle> ACM SIGCOMM 88, </booktitle> <pages> pages 273-288, </pages> <year> 1988. </year>
Reference-contexts: To see why the performance analyst simulates rather than models, consider the difference between fixed window and slow-start TCP. While analytical methods enjoy some small success modeling fixed-window flow con trol algorithms [1], no one uses fixed-window algorithms anymore; Jacobson's slow-start <ref> [15] </ref> TCP has completely replaced them. No one has successfully modeled slow-start analytically for several reasons. A sender's slow-start window size oscillates over time; it grows when it detects that the network is not congested, but quickly shrinks as soon as congestion occurs.
Reference: [16] <author> Van Jacobson. </author> <title> Compressing TCP/IP headers for low-speed serial links. </title> <type> Technical Report RFC1144, </type> <institution> LBL, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: He called such a burst a packet train. He said that if the spacing between two packets exceeds some inter-train gap, that these packets belong to separate trains. In subsequent years, several protocol stack implementations were optimized to exploit this phenomenon <ref> [4, 16] </ref>. More recently, other investigators found that due to the high degree of multiplexing, back-to-back packet trains are not as prominent on wide-area networks as they are for local area networks [14, 10]. Researchers are already calling for a definition of packet trains for wide-area-networks [21].
Reference: [17] <author> Van Jacobson, R. Braden, </author> <title> and D Borman. TCP extensions for high performance. </title> <type> Technical Report RFC1323, </type> <institution> LBL, ISI, Cray Research, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Although Flowsim discards per packet information, it can approximate this timestamp at the cost of two additional timestamp fields per train. However Flowsim does have its limitations. The TCP/IP community will soon agree on a selective acknowledgement scheme so that TCP can employ large window sizes efficiently <ref> [17] </ref>. Selective acknowledgements will reduce Flowsim's packet train length somewhat, because sources will send packets out of order and some acknowledgements may not be compactable at all.
Reference: [18] <author> Raj Jain and Shawn A. Routhier. </author> <title> Packet trains-measurement and a new model for computer network traffic. </title> <journal> IEEE JSAC, </journal> <pages> pages 986-995, </pages> <address> Septem-ber, </address> <year> 1986. </year>
Reference-contexts: Beyond such factoring, further reducing memory consumption requires making approximations. 3.2 Flowsim on Links Jain noted that back-to-back packets on local area networks tend to arrive from the same source and are headed to the same destination <ref> [18] </ref>. He called such a burst a packet train. He said that if the spacing between two packets exceeds some inter-train gap, that these packets belong to separate trains. In subsequent years, several protocol stack implementations were optimized to exploit this phenomenon [4, 16].
Reference: [19] <author> Hemant Kanakia, S. Keshav, and Partho P. Mishra. </author> <title> A benchmark suite for comparing congestion control schemes. </title> <type> Technical Report (unpublished), </type> <institution> ATT Bell Laboratories, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: To contrast slow-start with some more exotic algorithm like Packet-Pair [20] for bulk transfer traffic, the file transfer time and equitable distribution of network bandwidth are the essential metrics <ref> [19] </ref>. To contrast slow-start and packet-pair for interactive traffic, the essential metrics are one-way packet delay and loss rates.
Reference: [20] <author> Srinivasan Keshav. </author> <title> A control-theoretic approach to flow control. </title> <booktitle> ACM SIGCOMM '91, </booktitle> <pages> pages 3-15, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Granted that one must simulate rather than model, what does one measure? The essential metrics are, of course, throughput, fairness, delay, loss rate, and link utilization. To contrast slow-start with some more exotic algorithm like Packet-Pair <ref> [20] </ref> for bulk transfer traffic, the file transfer time and equitable distribution of network bandwidth are the essential metrics [19]. To contrast slow-start and packet-pair for interactive traffic, the essential metrics are one-way packet delay and loss rates.
Reference: [21] <author> Paul E. McKenney and Ken F. Dove. </author> <title> Efficient demultiplexing of incoming TCP packets. </title> <booktitle> ACM SIGCOMM 92 Conference, </booktitle> <pages> pages 269-279, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: More recently, other investigators found that due to the high degree of multiplexing, back-to-back packet trains are not as prominent on wide-area networks as they are for local area networks [14, 10]. Researchers are already calling for a definition of packet trains for wide-area-networks <ref> [21] </ref>. We now describe how, motivated by wide-area-network packet trains, Flowsim represents closely spaced packets. Each link in Flowsim places a conversation's packets on a linked list of packet trains called a flow descriptor.
Reference: [22] <author> George Varghese and Tony Lauck. </author> <title> Hashed and hierarchical timing wheels: Data structures for the efficient implementation of a timer facility. </title> <booktitle> Symposium on Operating System Principles, </booktitle> <pages> pages 25-38, </pages> <year> 1987. </year>
Reference-contexts: Event lists are built from priority queues. Although naive priority queues work best with a small number of events, sophisticated ones exhibit O (1) behavior even with a large number of events if the variation in the number of events remains low <ref> [3, 22] </ref>. This leads us to ponder how the number of events in the event list fluctuates with time. The bandwidth-delay product of a link establishes the maximum number of packets that it can carry.
Reference: [23] <author> David B. Wagner, Edward D. Lazowska, and Brian N. Bershad. </author> <title> Techniques for efficient shared-memory parallel simulation. </title> <booktitle> SCS Multiconference: Advances in Parallel and Distributed Simulation, </booktitle> <pages> pages 29-37, </pages> <year> 1990. </year>
Reference-contexts: It does this by exploiting two ideas from the field of conservative, parallel distributed simulation <ref> [23] </ref>. On the example topologies we tested, this feature makes Flowsim 3 to 11 times faster than the optimized packet-by-packet simulator from which it was developed.
Reference: [24] <author> Zheng Wang and Jon Crowcroft. </author> <title> A new congestion control scheme: Slow start and search(tri-s). </title> <journal> Computer Communication Review, </journal> <pages> pages 21(1) 32-43, </pages> <month> January </month> <year> 1991. </year> <month> 13 </month>
Reference-contexts: Flowsim also could be used to study call routing and call admission control algorithms in a mix of 11 real-time and data traffic. 6.2 Extensibility We applied Flowsim to a study of a network congestion detection algorithm, Tri-S, a variation of which is employed in Vegas TCP <ref> [24, 2] </ref>. Tri-S monitors the network traffic and stops increasing congestion window before packet loss when the measured traffic congestion is beyond a certain threshold. Figure 15 and Figure 16 plot the congestion window of an FTP competing for a gateway's finite buffer with another FTP.
References-found: 24

