URL: http://www.ai.mit.edu/people/las/papers/wessler-stein-97.ps.gz
Refering-URL: http://www.ai.mit.edu/people/las/papers/
Root-URL: 
Title: Robust Active Vision from Simple Symbiotic Subsystems  
Author: Mike Wessler and Lynn Andrea Stein 
Abstract: We present an active vision head-eye system that mimics the low-level visual reflexes of an infant. The software architecture is fast and robust, consisting of very simple modules that interact to correct each other's errors. The system produces stable output and is designed to be the base framework of a more complete visual system. It tracks objects at 30 Hz with smooth motion, running on a single DSP chip, and decides on its own when and where to saccade to a moving object. 
Abstract-found: 1
Intro-found: 1
Reference: [BCS92] <author> Christopher Brown, David Coombs, and John Soong. </author> <title> Real-time smooth pursuit tracking. </title> <editor> In Andrew Blake and Alan Yuille, editors, </editor> <title> Active Vision, chapter 8. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: High-speed filters can be made using simple brightness or color segmentation [Wri93, KS92, BGDE94]. Unfortunately, filters like these require that the object being tracked is visually distinctive. They don't work well for most ordinary objects. Coombs et al. <ref> [BCS92] </ref> used a more complex stereo filter that does not require the objects to be visually distinctive. Their system requires two cameras and uses a cepstral filter to highlight the portions of the scene that have zero disparity between the two cameras.
Reference: [BGDE94] <author> S.A. Brock-Gunn, G.R. Dowling, and T.J. Ellis. </author> <title> Tracking using colour information. </title> <type> Technical Report TCU/CS/1994/7, </type> <institution> City University London, </institution> <year> 1994. </year>
Reference-contexts: The filter-and-follow method depends on the idea that the image from the camera can be sent through a filter that highlights only the object of interest. High-speed filters can be made using simple brightness or color segmentation <ref> [Wri93, KS92, BGDE94] </ref>. Unfortunately, filters like these require that the object being tracked is visually distinctive. They don't work well for most ordinary objects. Coombs et al. [BCS92] used a more complex stereo filter that does not require the objects to be visually distinctive.
Reference: [CF88] <author> James J. Clark and Nicola Ferrier. </author> <title> Modal control of an attentive vision system. </title> <booktitle> In Proceedings of the Second International Conference on Computer Vision, </booktitle> <pages> pages 514-523, </pages> <address> Tampa, Florida, </address> <month> December </month> <year> 1988. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: This idea of the use of filters and selective weighting was originally developed by Ullman [Ull83] refined by Chapman [Cha90], and used on the Harvard head for saccade control <ref> [CF88] </ref>. More recently, Horswill [Hor95] implemented an entire visual routine processor on the same hardware used by Odie.
Reference: [Cha90] <author> David Chapman. </author> <title> Vision, instruction and action. </title> <type> Technical Report 1204, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: By turning on the maps for motion and redness for example, Odie will be most attracted to a bouncing red rubber ball. This idea of the use of filters and selective weighting was originally developed by Ullman [Ull83] refined by Chapman <ref> [Cha90] </ref>, and used on the Harvard head for saccade control [CF88]. More recently, Horswill [Hor95] implemented an entire visual routine processor on the same hardware used by Odie.
Reference: [Hil84] <author> Ellen C. Hildreth. </author> <title> Computations underlying the measurement of visual motion. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> pages 309-354, </pages> <year> 1984. </year>
Reference-contexts: Objects to be tracked do not need to have special features or coloring. However, local area correlations suffer from the aperture problem, in which it is impossible to determine the motion of certain patterns using only local information <ref> [Hil84] </ref>. Inoue [ITI92] uses the correlation method to track multiple regions in the image. He employs a special correlation processor, originally designed to help with MPEG compression, to track up to 20 regions in the frame simultaneously.
Reference: [Hor95] <author> Ian Horswill. </author> <title> Visual routines and visual search: a real-time implementation and an automata-theoretic analysis. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: This idea of the use of filters and selective weighting was originally developed by Ullman [Ull83] refined by Chapman [Cha90], and used on the Harvard head for saccade control [CF88]. More recently, Horswill <ref> [Hor95] </ref> implemented an entire visual routine processor on the same hardware used by Odie. Horswill's system translates English questions like, Is there a green block above a blue block? into a set of procedures on the visual data that finds the correct block in the process of answering the question. <p> It would also serve as a clean input to an object recognition system, which could be run off the stable match image from within the tracker. For the attention shift system, several other interesting thing filters may be added. In addition, a system similar to Horswill's visual routine processor <ref> [Hor95] </ref> can provide higher level reasoning to intelligently guide the selection of a new saccade target. 11 Conclusion Odie consists of three very simple visual systems that work together to provide a robust low-level visual framework. None of the individual components is particularly sophisticated or robust under all circumstances.
Reference: [ITI92] <author> Hirochika Inoue, Tetsuya Tachikawa, and Masayuki Inaba. </author> <title> Robot vision system with a correlation chip for real time tracking, optical flow and depth map generation. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1621-1626, </pages> <address> Nice, France, </address> <month> May </month> <year> 1992. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: Objects to be tracked do not need to have special features or coloring. However, local area correlations suffer from the aperture problem, in which it is impossible to determine the motion of certain patterns using only local information [Hil84]. Inoue <ref> [ITI92] </ref> uses the correlation method to track multiple regions in the image. He employs a special correlation processor, originally designed to help with MPEG compression, to track up to 20 regions in the frame simultaneously.
Reference: [Kal60] <author> R. E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> In Trans. ASME J. of Basic Engineering, </journal> <year> 1960. </year>
Reference-contexts: This suggests that a simple guess location based on the motion from the previous frame should be adequate for most cases. In a busier or noisier environment, a Kalman filter can be used to filter out some of the noise to produce a smoother, more accurate guess <ref> [Kal60] </ref>. 4.2.2 Determining the search region There is an interesting relationship between the area of the search region, the frame rate of the system given a specific search area and the range of motions the system will be able to track successfully.
Reference: [KK92] <author> Stephen M. Kosslyn and Olivier Koenig. </author> <title> Wet Mind: The New Cognitive Neuroscience. </title> <publisher> The Free Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: This particular diagram is abstracted from a model designed by Kosslyn and Koenig. It comes from studies of the major neural connections and pathways in the human brain, and case studies of what goes wrong when certain of these connections break. See <ref> [KK92] </ref> for the full development of the model. The visual pathway begins at the retina and continues in a reasonably direct way to an area called V1 at the back of the brain. This is a visual buffer, where the retinotopically mapped data is processed and stored temporarily. <p> As long as the object being tracked is not too close to the eyes, the difference in shape caused by parallax would not be great enough to confuse the systems. The attention window itself should be scalable and shapeable, as the human attention window appears to be <ref> [KK92] </ref>. A larger attention window subsamples the visual buffer so that the number of pixels involved remains constant and the running time does not change. Such a window can resize to adapt to the scale of the object being tracked.
Reference: [KS92] <author> H. Kimura and J. J. E. Slotine. </author> <title> Adaptive visual tracking and gaussian network algorithms for robotic catching. </title> <booktitle> In Advances in Robust and Nonlinear Control Systems, </booktitle> <pages> pages 67-74, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The filter-and-follow method depends on the idea that the image from the camera can be sent through a filter that highlights only the object of interest. High-speed filters can be made using simple brightness or color segmentation <ref> [Wri93, KS92, BGDE94] </ref>. Unfortunately, filters like these require that the object being tracked is visually distinctive. They don't work well for most ordinary objects. Coombs et al. [BCS92] used a more complex stereo filter that does not require the objects to be visually distinctive.
Reference: [RM93] <author> Ian D. Reid and David W. Murray. </author> <title> Tracking foveated corner clusters using affine structure. </title> <booktitle> In Proceedings of the Fourth International Conference on Computer Vision, </booktitle> <pages> pages 76-83, </pages> <address> Berlin, Germany, </address> <month> May </month> <year> 1993. </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: Inoue uses correlation between left and right camera images to calculate depth information as well as using correlation between consecutive frames to perform tracking. The final form of tracking identifies specific feature locations in the image, and then performs correlation over those locations. Reid and Murray <ref> [RM93] </ref> demonstrate a robot head named Yorick that uses the feature correlation method to track objects. Reid and Murray's feature filter identifies locations in the image that look like corners.
Reference: [Ull83] <author> Shimon Ullman. </author> <title> Visual routines. </title> <type> AI Memo 723, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> June </month> <year> 1983. </year>
Reference-contexts: By turning on the maps for motion and redness for example, Odie will be most attracted to a bouncing red rubber ball. This idea of the use of filters and selective weighting was originally developed by Ullman <ref> [Ull83] </ref> refined by Chapman [Cha90], and used on the Harvard head for saccade control [CF88]. More recently, Horswill [Hor95] implemented an entire visual routine processor on the same hardware used by Odie.
Reference: [Vio90] <author> Paul Viola. </author> <title> Adaptive gaze control. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: Using this information, Odie can simply look up the nearest stored x; y image locations and interpolate to find the motor commands necessary to bring the location to the center of the image. This table is similar to the technique used by Viola in the A-eye system <ref> [Vio90] </ref>. A-eye generated the table by making practice saccades, determining where the target spot moved in the field of view and updating the table by small amounts. This technique assumes that there is a single target that is uniquely identifiable in the entire image.
Reference: [Wri93] <author> Anne Wright. </author> <title> A high speed low latency portable vision sensing system. </title> <editor> In David P. Casasent, editor, </editor> <booktitle> Intelligent Robots and Computer Vision XII: Algorithms and Techniques, volume 2055, </booktitle> <pages> pages 263-270. SPIE, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The filter-and-follow method depends on the idea that the image from the camera can be sent through a filter that highlights only the object of interest. High-speed filters can be made using simple brightness or color segmentation <ref> [Wri93, KS92, BGDE94] </ref>. Unfortunately, filters like these require that the object being tracked is visually distinctive. They don't work well for most ordinary objects. Coombs et al. [BCS92] used a more complex stereo filter that does not require the objects to be visually distinctive.
References-found: 14

