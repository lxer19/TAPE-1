URL: http://iacoma.cs.uiuc.edu/iacoma-papers/iprefetch.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: Instruction Prefetching of Systems Codes With Layout Optimized for Reduced Cache Misses 1  
Author: Chun Xia and Josep Torrellas 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: High-performing on-chip instruction caches are crucial to keep fast processors busy. Unfortunately, while on-chip caches are usually successful at intercepting instruction fetches in loop-intensive engineering codes, they are less able to do so in large systems codes. To improve the performance of the latter codes, the compiler can be used to lay out the code in memory for reduced cache conflicts. Interestingly, such an operation leaves the code in a state that can be exploited by a new type of instruction prefetching: guarded sequential prefetching. The idea is that the compiler leaves hints in the code as to how the code was laid out. Then, at run time, the prefetching hardware detects these hints and uses them to prefetch more effectively. This scheme can be implemented very cheaply: one bit encoded in control transfer instructions and a prefetch module that requires minor extensions to existing next-line sequential prefetchers. Furthermore, the scheme can be turned off and on at run time with the toggling of a bit in the TLB. The scheme is evaluated with simulations using complete traces from a 4-processor machine. Overall, for 16-Kbyte primary instruction caches, guarded sequential prefetching removes, on average, 66% of the instruction misses remaining in an operating system with an optimized layout, speeding up the operating system by 10%. Moreover, the scheme is more cost-effective and robust than existing sequential prefetching techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: To minimize cache conflicts, we place instruction sequences in memory ordered by their frequency of invocation. As a result, a frequently-invoked instruction sequence does not conflict with another frequently-invoked one. Finally, to assess the potential impact of loop locality, we use dataflow analysis <ref> [1] </ref> to identify the loops in the code. We divide the loops into those that do not call routines and those that do.
Reference: [2] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Each day-long session corresponds to about 15 seconds of real time. The loads are as follows: TRFD 4 is a mix of 4 runs of a hand-parallelized version of the TRFD Perfect Club code <ref> [2] </ref>. Each program runs with 4 processes. The code is composed of matrix multiplies and data interchanges. It is highly parallel yet synchronization intensive. The most important operating system activities present are page fault handling, process scheduling, cross-processor interrupts, processor synchronization, and other multiprocessor management functions.
Reference: [3] <author> W. Y. Chen, P. P. Chang, T. M. Conte, and W. W. Hwu. </author> <title> The Effect of Code Expanding Optimizations on Instruction Cache Design. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(9) </volume> <pages> 1045-1057, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: For example, basic blocks that are usually fetched in sequence are laid out in sequence, while basic blocks that form a loop are placed to avoid any conflicts within the loop. Overall, it has been shown that these schemes work well, both for engineering <ref> [3, 7] </ref> and for systems codes [12]. They work particularly well for systems codes because the original unoptimized layout has poor performance. <p> In function inlining, the whole callee routine is inserted between the caller's basic blocks, not just the callee's most important basic blocks. As a result, function inlining, even if it is done at a single point, may increase cache conflicts. Indeed, while Chen et al <ref> [3] </ref> limited inlining to frequent routines only, their results reveal that inlining may not be a stable and effective scheme. For this reason, we do not use inlining.
Reference: [4] <author> W. W. Hwu and P. P. Chang. </author> <title> Achieving High Instruction Cache Performance with an Optimizing Compiler. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 242-251, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The resulting layout is shown in a logical cache and then from left to right. This algorithm improves both the placement of the basic blocks within routines and the relative placement of the routines. These two aspects are already addressed by Hwu and Chang's algorithm <ref> [4] </ref>. However, our algorithm further exposes spatial locality by generating instruction sequences that cross routine boundaries. <p> Indeed, while Chen et al [3] limited inlining to frequent routines only, their results reveal that inlining may not be a stable and effective scheme. For this reason, we do not use inlining. Our algorithm also differs from <ref> [4] </ref> in that we exploit temporal locality: as indicated above, we use a SelfConfFree area and order the placement of the instruction sequences by frequency of invocation. <p> This is our main contribution. There is, however, a large body of related work. First, McFarling [7], Hwu and Chang <ref> [4] </ref>, and Torrellas et al [12] studied code layout optimization for cache performance. However, they did not investigate instruction prefetching on the optimized codes. Instruction prefetching without layout optimization has been a topic frequently addressed in the past.
Reference: [5] <author> N. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These misses slow down the guarded sequential prefetcher. This suggests that we can further reduce the execution time by placing stream buffers <ref> [5] </ref> between the secondary caches and the memory bus to complement the guarded sequential prefetcher. We add two 8-entry direct-access stream buffers per processor. The buffers are allowed to look up the secondary cache, thus avoiding unnecessary bus accesses when the line is in the secondary cache. <p> Our results agree with his on the relative performance of the schemes. These schemes do not tolerate long latencies because they prefetch only one line ahead. An effective improvement is the stream buffer proposed by Jouppi <ref> [5] </ref>. Stream buffers prefetch successive lines after a miss. We do not compare stream buffers to guarded sequential prefetching because our scheme is a much cheaper addition to next-line prefetchers. Instead, we add stream buffers between the secondary caches and the bus.
Reference: [6] <author> D. Lee, J. Baer, B. Calder, and D. Grunwald. </author> <title> Instruction Cache Fetch Policies for Speculative Execution. </title> <booktitle> In Proceedings of the 22th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 357-367, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Conceptually, the implementation needs to use a decoder [9] or a connection to a branch target buffer <ref> [6, 11] </ref> in order to determine the address of the branch target. A stack may also be necessary to maintain the addresses of procedure returns. Obviously, traditional sequential prefetchers need non-trivial changes to support it. <p> Smith and Hsu [11] investigated the fetch-ahead distance in a next-line prefetch scheme. This distance is the number of instructions that remain to be issued in a line before a prefetch request for the next line should be issued. Next-line prefetching has also been studied by Lee et al <ref> [6] </ref> in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines [10, 11, 13]. Smith and Hsu [11] studied target prefetching with the support of a target prediction table.
Reference: [7] <author> S. McFarling. </author> <title> Program Optimization for Instruction Caches. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: For example, basic blocks that are usually fetched in sequence are laid out in sequence, while basic blocks that form a loop are placed to avoid any conflicts within the loop. Overall, it has been shown that these schemes work well, both for engineering <ref> [3, 7] </ref> and for systems codes [12]. They work particularly well for systems codes because the original unoptimized layout has poor performance. <p> This is our main contribution. There is, however, a large body of related work. First, McFarling <ref> [7] </ref>, Hwu and Chang [4], and Torrellas et al [12] studied code layout optimization for cache performance. However, they did not investigate instruction prefetching on the optimized codes. Instruction prefetching without layout optimization has been a topic frequently addressed in the past.
Reference: [8] <author> D. Nagle, R. Uhlig, T. Mudge, and S. Sechrest. </author> <title> Optimal Allocation of On-chip Memory for Multiple-API Operating Systems. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 358-369, </pages> <month> April </month> <year> 1994. </year>
Reference: [9] <author> J. Pierce and T. Mudge. </author> <title> Wrong-Path Instruction Prefetch-ing. </title> <type> Technical Report CSE-222-94, </type> <institution> University of Michigan, </institution> <year> 1994. </year>
Reference-contexts: Conceptually, the implementation needs to use a decoder <ref> [9] </ref> or a connection to a branch target buffer [6, 11] in order to determine the address of the branch target. A stack may also be necessary to maintain the addresses of procedure returns. Obviously, traditional sequential prefetchers need non-trivial changes to support it. <p> Smith and Hsu [11] studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best. A different approach taken by Pierce and Mudge <ref> [9] </ref> is the wrong-path prefetching. This scheme prefetches both paths of a conditional branch. They found that 70-80% of the performance gain came from prefetch-ing the fall-through path instead of prefetching the target.
Reference: [10] <author> A. J. Smith. </author> <title> Cache Memories. </title> <booktitle> In Computing Surveys, </booktitle> <pages> pages 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: Finally, we consider the microarchitecture support required. We design it as an add-on feature to traditional sequential prefetching schemes. We consider two types of traditional sequential prefetching schemes: non-tagged and tagged <ref> [10] </ref>. Non-tagged schemes are simple schemes that prefetch line i + 1 after a reference to line i, or after a miss on line i. In tagged schemes, a bit is added to each cache line to improve the prefetching algorithm. <p> We first consider three traditional schemes described in <ref> [10] </ref>: next-line prefetch on access (NLalways), next-line prefetch on miss (NLmiss), and tagged prefetch as discussed in Section 5.2.1 (NLtagged). <p> However, they did not investigate instruction prefetching on the optimized codes. Instruction prefetching without layout optimization has been a topic frequently addressed in the past. For example, Smith examined the three next-line sequential prefetching schemes described in Section 5.2.1 <ref> [10] </ref>. Our results agree with his on the relative performance of the schemes. These schemes do not tolerate long latencies because they prefetch only one line ahead. An effective improvement is the stream buffer proposed by Jouppi [5]. Stream buffers prefetch successive lines after a miss. <p> Next-line prefetching has also been studied by Lee et al [6] in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines <ref> [10, 11, 13] </ref>. Smith and Hsu [11] studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best.
Reference: [11] <author> J. E. Smith and W.-C. Hsu. </author> <title> Prefetching in Supercomputer Instruction Caches. </title> <booktitle> In Proceedings of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 588-597, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Conceptually, the implementation needs to use a decoder [9] or a connection to a branch target buffer <ref> [6, 11] </ref> in order to determine the address of the branch target. A stack may also be necessary to maintain the addresses of procedure returns. Obviously, traditional sequential prefetchers need non-trivial changes to support it. <p> Prefetching the next N lines after a miss was evaluated by Uhlig et al for large codes [13] and found effective. To reduce cache pollution, they suggested caching prefetched lines only if they are used. This optimization, however, actually reduced performance. Smith and Hsu <ref> [11] </ref> investigated the fetch-ahead distance in a next-line prefetch scheme. This distance is the number of instructions that remain to be issued in a line before a prefetch request for the next line should be issued. <p> Next-line prefetching has also been studied by Lee et al [6] in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines <ref> [10, 11, 13] </ref>. Smith and Hsu [11] studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best. <p> Next-line prefetching has also been studied by Lee et al [6] in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines [10, 11, 13]. Smith and Hsu <ref> [11] </ref> studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best. A different approach taken by Pierce and Mudge [9] is the wrong-path prefetching.
Reference: [12] <author> J. Torrellas, C. Xia, and R. Daigle. </author> <title> Optimizing Instruction Cache Performance for Operating System Intensive Work-loads. </title> <booktitle> In IEEE Trans. on Computers, to appear. A shorter version appeared in Proceedings of the 1st International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 360-369, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Since these applications are used frequently in real life, it is important to understand and improve their cache performance. The first step to improve the instruction cache performance of these codes is to optimize the layout of their instructions in memory <ref> [12] </ref>. The purpose of this step is to expose more locality in the code and, as a result, minimize conflicts in the cache. The approach taken usually involves building the basic block graph of the code and then, based on profile information, carefully placing the basic blocks in memory. <p> Overall, it has been shown that these schemes work well, both for engineering [3, 7] and for systems codes <ref> [12] </ref>. They work particularly well for systems codes because the original unoptimized layout has poor performance. After this optimization, the misses that remain tend to be spread out in the code in a uniform manner; there are no obvious hot spot areas of conflict misses. <p> We use a multiprocessor to capture a larger range of systems activity, including multiprocessor scheduling and cross-processor interrupts. In this section, we discuss the hardware and software setup used and the workloads traced. More details on the setup and workload characteristics can be found in <ref> [12, 14] </ref>. 2.1 Hardware and Software Setup We gather the traces from a 4-processor bus-based Alliant FX/8 multiprocessor. The operating system running in the machine is a slightly modified version of Alliant's Concentrix 3.0. Concentrix is multithreaded, symmetric, and is based on Unix BSD 4.2. <p> The operating system runs for 47.0% of the time. 3 Optimizing Code Layouts In this section, we describe our approach to optimizing the layouts of large systems codes. We first discuss the general approach and then present the algorithm used. More information can be found in <ref> [12] </ref>. 3.1 General Approach Our approach is to lay out the code to expose the three localities in it: spatial, temporal, and loop locality. If these localities are exposed, cache conflict misses should decrease. <p> In the lowest SelfConfFree bytes of each logical cache except the first one, we place seldom-executed code. In the SelfConfFree area of the first logical cache, we place the most frequently executed basic blocks (Section 3.1). We use a SelfConfFree area of about 1 Kbyte <ref> [12] </ref>. The resulting layout is shown in a logical cache and then from left to right. This algorithm improves both the placement of the basic blocks within routines and the relative placement of the routines. These two aspects are already addressed by Hwu and Chang's algorithm [4]. <p> The table shows that the large majority of the misses (80% on average) are sequential misses. This means that there is more spatial locality to exploit. Cache lines longer than the 32 bytes used can reduce the number of misses <ref> [12] </ref>. However, longer lines may not be acceptable in systems with a unified secondary cache like the one simulated (Section 6.1) because data caches may not work well with long lines. Furthermore, long lines tend to increase the miss penalty. <p> Furthermore, long lines tend to increase the miss penalty. Another alternative may be to increase the associativity of the cache. This approach, however, does not help as much as expected because, as shown in <ref> [12] </ref>, the layout algorithm has already removed most of the conflict misses. The remaining misses are hard to remove. Furthermore, associative caches are slower. To reduce sequential misses in codes with optimized layouts, instead, we propose a novel and cost-effective approach called guarded sequential prefetching. <p> This is our main contribution. There is, however, a large body of related work. First, McFarling [7], Hwu and Chang [4], and Torrellas et al <ref> [12] </ref> studied code layout optimization for cache performance. However, they did not investigate instruction prefetching on the optimized codes. Instruction prefetching without layout optimization has been a topic frequently addressed in the past. For example, Smith examined the three next-line sequential prefetching schemes described in Section 5.2.1 [10].
Reference: [13] <author> R. Uhlig, D. Nagle, T. Mudge, S. Sechrest, and J. Emer. </author> <title> Instruction Fetching: Coping with Code Bloat. </title> <booktitle> In Proceedings of the 22th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 345-356, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Instead, we add stream buffers between the secondary caches and the bus. Prefetching the next N lines after a miss was evaluated by Uhlig et al for large codes <ref> [13] </ref> and found effective. To reduce cache pollution, they suggested caching prefetched lines only if they are used. This optimization, however, actually reduced performance. Smith and Hsu [11] investigated the fetch-ahead distance in a next-line prefetch scheme. <p> Next-line prefetching has also been studied by Lee et al [6] in the context of speculative execution. Overall, several authors have pointed out that smallish cache lines with sequential prefetching are better than long lines <ref> [10, 11, 13] </ref>. Smith and Hsu [11] studied target prefetching with the support of a target prediction table. They found that target prefetching performed slightly worse than next-line sequential prefetching. However, the combination of both schemes was best.
Reference: [14] <author> C. Xia and J. Torrellas. </author> <title> Improving the Data Cache Performance of Multiprocesor Operating Systems. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 85-94, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: We use a multiprocessor to capture a larger range of systems activity, including multiprocessor scheduling and cross-processor interrupts. In this section, we discuss the hardware and software setup used and the workloads traced. More details on the setup and workload characteristics can be found in <ref> [12, 14] </ref>. 2.1 Hardware and Software Setup We gather the traces from a 4-processor bus-based Alliant FX/8 multiprocessor. The operating system running in the machine is a slightly modified version of Alliant's Concentrix 3.0. Concentrix is multithreaded, symmetric, and is based on Unix BSD 4.2.
References-found: 14

