URL: ftp://ilk.kub.nl/pub/papers/wonderland.ps.gz
Refering-URL: http://ilk.kub.nl/papers.html
Root-URL: 
Email: E-mail: walter@kub.nl  
Title: RAPID DEVELOPMENT OF NLP MODULES WITH MEMORY-BASED LEARNING  
Author: Walter Daelemans, Antal van den Bosch, Jakub Zavrel, Jorn Veenstra, Sabine Buchholz, Bertjan Busser 
Web: URL: http://ilk.kub.nl,  
Address: P.O. Box 90153, NL-5000 LE, Tilburg, The Netherlands  
Affiliation: ILK Computational Linguistics Tilburg University  
Abstract: The need for software modules performing natural language processing (NLP) tasks is growing. These modules should perform efficiently and accurately, while at the same time rapid development is often mandatory. Recent work has indicated that machine learning techniques in general, and memory-based learning (MBL) in particular, offer the tools to meet both ends. We present examples of modules trained with MBL on three NLP tasks: (i) text-to-speech conversion, (ii) part-of-speech tagging, and (iii) phrase chunking. We demonstrate that the three modules display high generalization accuracy, and argue why MBL is applicable similarly well to a large class of other NLP tasks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Steven Abney. </author> <title> Parsing by chunks. In Principle-Based Parsing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dor-drecht, </address> <year> 1991. </year>
Reference-contexts: Chunking can be seen as light parsing. In this sec tion we present two applications of MBL to light parsing: NP chunking, and the more complicated task of bracket prediction. In NP chunking sentences are segmented into non-recursive NP's, so called baseNP's <ref> [1] </ref>. NP chunking can e.g. be used to reduce the complexity of sub-sequential parsing, or to identify named entities for information retrieval.
Reference: [2] <author> D. W. Aha, D. Kibler, and M. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6:37 66, </volume> <year> 1991. </year>
Reference-contexts: The approach has surfaced in different contexts using a variety of alternative names such as similarity-based, example-based, exemplar-based, analogical, case-based, instance-based, and lazy learning <ref> [23, 7, 17, 2, 3] </ref>. Historically, memory-based learning algorithms are descendants of the k-nearest neighbor (henceforth k-NN) algorithm [8, 16, 2]. <p> The approach has surfaced in different contexts using a variety of alternative names such as similarity-based, example-based, exemplar-based, analogical, case-based, instance-based, and lazy learning [23, 7, 17, 2, 3]. Historically, memory-based learning algorithms are descendants of the k-nearest neighbor (henceforth k-NN) algorithm <ref> [8, 16, 2] </ref>. A MBL system, visualized schematically in Figure 1, contains two components: a learning component which is memory-based (from which MBL borrows its name), and a performance component which is similarity-based. <p> TIMBL 1 is the name of a software package developed by the ILK group at Tilburg University, containing variations of the memory-based learning algorithms IB1,IB1-IG, and MVDM <ref> [2, 13, 12, 7] </ref>, and IGTREE [12], a decision-tree optimization of memory-based learning. Below, we outline the functioning of IB1-IG and IGTREE in Subsections 2.1 and 2.2, respectively. 1 The TIMBL software package is freely available for research purposes from the ILK web pages; consult URL http://ilk.kub.nl. 2.1. <p> The k-NN algorithm with this metric, and equal weighting for all features is called IB1 <ref> [2] </ref>. Usually k is set to 1. (X; Y ) = i=1 where: ffi (x i ; y i ) = 0 if x i = y i ; else 1 (2) We have made two additions to the original algorithm [2] in our version of IB1. <p> and equal weighting for all features is called IB1 <ref> [2] </ref>. Usually k is set to 1. (X; Y ) = i=1 where: ffi (x i ; y i ) = 0 if x i = y i ; else 1 (2) We have made two additions to the original algorithm [2] in our version of IB1. First, in the case of nearest neighbor sets larger than one instance (k &gt; 1 or ties), our version of IB1 selects the classification with the highest frequency in the class distribution of the nearest neighbor set.
Reference: [3] <institution> Lazy learning: </institution> <note> Special issue editorial. Artificial Intelligence Review, 11:710, </note> <year> 1997. </year>
Reference-contexts: The approach has surfaced in different contexts using a variety of alternative names such as similarity-based, example-based, exemplar-based, analogical, case-based, instance-based, and lazy learning <ref> [23, 7, 17, 2, 3] </ref>. Historically, memory-based learning algorithms are descendants of the k-nearest neighbor (henceforth k-NN) algorithm [8, 16, 2].
Reference: [4] <author> R. H. Baayen, R. Piepenbrock, and H. van Rijn. </author> <title> The CELEX lexical data base on CD-ROM. Linguistic Data Consortium, </title> <address> Philadelphia, PA, </address> <year> 1993. </year>
Reference-contexts: Data preprocessing takes some time, depending of the completeness of annotation of the source data. For many languages, well-annotated electronic pronunciation dictionaries are available that can be used for this purpose directly; for example, our English and Dutch data was extracted from the CELEX lexical data base <ref> [4] </ref>. Preprocessing of this type of data typically does not take more than a few days. 3.2. MBT: Part-of-speech tagging The MBT tagger-generator [15, 14] takes an annotated corpus as input, and produces a lexicon and memory-based part-of-speech (POS) tagger as output.
Reference: [5] <author> Eric Brill. </author> <title> Some advances in transformation-based part-of-speech tagging. </title> <booktitle> In AAAI'94, </booktitle> <year> 1994. </year>
Reference-contexts: A case is constructed for each focus word. The features are words and POS tags of the focus and adjacent words. In one test we used Ramshaw & Marcus' tags as assigned by a Brill tagger <ref> [5] </ref>, and in another test we used the tags assigned by MBT as discussed in section 3.2. As can be seen in the upper half of table 6, the results are comparable, although Ramshaw & Marcus' error-driven transformation-based approach slightly outperforms the memory-based methods.
Reference: [6] <author> Claire Cardie. </author> <title> Automatic feature set selection for case-based learning of linguistic knowledge. </title> <booktitle> In Proc. of Conference on Empirical Methods in NLP. </booktitle> <institution> University of Pennsylvania, </institution> <year> 1996. </year> <note> 2 For more information, publications, and web demos, see the ILK homepage: http://ilk.kub.nl/ </note>
Reference-contexts: The distance metric in equation 2 simply counts the number of (mis)matching feature values in both patterns. In the absence of information about feature relevance, this is a reasonable choice. Otherwise, we can add linguistic bias to weight or select different features <ref> [6] </ref> or look at the behavior of features in the set of examples used for training. We can compute statistics about the relevance of features by looking at which features are good predictors of the class labels.
Reference: [7] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbour algorithm for learning with symbolic features. </title> <booktitle> Machine Learning, </booktitle> <address> 10:5778, </address> <year> 1993. </year>
Reference-contexts: The approach has surfaced in different contexts using a variety of alternative names such as similarity-based, example-based, exemplar-based, analogical, case-based, instance-based, and lazy learning <ref> [23, 7, 17, 2, 3] </ref>. Historically, memory-based learning algorithms are descendants of the k-nearest neighbor (henceforth k-NN) algorithm [8, 16, 2]. <p> TIMBL 1 is the name of a software package developed by the ILK group at Tilburg University, containing variations of the memory-based learning algorithms IB1,IB1-IG, and MVDM <ref> [2, 13, 12, 7] </ref>, and IGTREE [12], a decision-tree optimization of memory-based learning. Below, we outline the functioning of IB1-IG and IGTREE in Subsections 2.1 and 2.2, respectively. 1 The TIMBL software package is freely available for research purposes from the ILK web pages; consult URL http://ilk.kub.nl. 2.1.
Reference: [8] <author> T. M. Cover and P. E. Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> 13:2127, </volume> <year> 1967. </year>
Reference-contexts: The approach has surfaced in different contexts using a variety of alternative names such as similarity-based, example-based, exemplar-based, analogical, case-based, instance-based, and lazy learning [23, 7, 17, 2, 3]. Historically, memory-based learning algorithms are descendants of the k-nearest neighbor (henceforth k-NN) algorithm <ref> [8, 16, 2] </ref>. A MBL system, visualized schematically in Figure 1, contains two components: a learning component which is memory-based (from which MBL borrows its name), and a performance component which is similarity-based.
Reference: [9] <author> W. Daelemans. </author> <title> Memory-based lexical acquisition and processing. </title> <editor> In P. Steffens, editor, </editor> <booktitle> Machine Translation and the Lexicon, volume 898 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 8598. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: On the other hand, we argue <ref> [9, 10] </ref> that all NLP tasks can be seen as either * light NLP tasks, involving disambiguation or segmentation [9] locally at one language level or between two closely-related language levels; or as * compositions of light NLP tasks, when the task sur passes the complexity of single light NLP tasks. <p> On the other hand, we argue [9, 10] that all NLP tasks can be seen as either * light NLP tasks, involving disambiguation or segmentation <ref> [9] </ref> locally at one language level or between two closely-related language levels; or as * compositions of light NLP tasks, when the task sur passes the complexity of single light NLP tasks.
Reference: [10] <author> W. Daelemans. </author> <title> Experience-driven language acquisition and processing. </title> <editor> In M. Van der Avoird and C. Cor-sius, editors, </editor> <booktitle> Proceedings of the CLS Opening Academic Year 1996-1997, </booktitle> <pages> pages 8395. </pages> <address> CLS, Tilburg, </address> <year> 1996. </year>
Reference-contexts: On the other hand, we argue <ref> [9, 10] </ref> that all NLP tasks can be seen as either * light NLP tasks, involving disambiguation or segmentation [9] locally at one language level or between two closely-related language levels; or as * compositions of light NLP tasks, when the task sur passes the complexity of single light NLP tasks. <p> In other words, apart from a core of generalizations, there is a relatively large periphery of irregularities <ref> [10] </ref>. In rule-based NLP, this problem has to be solved using mechanisms such as rule ordering, subsumption, inheritance, or default reasoning (in linguistics this type of priority to the most specific mechanism is called the elsewhere condition). <p> Finally, from the results in this paper and those reported earlier <ref> [10, 24] </ref>, it appears that the properties of NLP tasks are such that non-abstracting approaches such as MBL are at an advantage in learning from the highly disjunctive datasets typical in NLP in contrast to eager methods such as decision tree induction and statistical inference.
Reference: [11] <author> W. Daelemans and A. Van den Bosch. </author> <title> Language-independent data-oriented grapheme-to-phoneme conversion. </title> <editor> In J. P. H. Van Santen, R. W. Sproat, J. P. Olive, and J. Hirschberg, editors, </editor> <booktitle> Progress in Speech Processing, </booktitle> <pages> pages 7789. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: In this section we illustrate some examples of recent work on MBLE on three `light' NLP tasks: (i) text-to-speech conversion in TREETALK, (ii) part-of-speech tagging in MBT, and (iii) phrase chunking in MBC. 3.1. TREETALK: Text-to-speech conversion The TREETALK system <ref> [26, 25, 11, 24] </ref> has originally been designed for isolated word pronunciation, i.e., converting a written word to its phonemic representation as found in a pronunciation dictionary, and efforts are underway to extend it to modeling speech phenomena in texts, such as sentence accents and prosody.
Reference: [12] <author> W. Daelemans, A. Van den Bosch, and A. Weijters. IGTree: </author> <title> using trees for compression and classification in lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11:407423, </volume> <year> 1997. </year>
Reference-contexts: TIMBL 1 is the name of a software package developed by the ILK group at Tilburg University, containing variations of the memory-based learning algorithms IB1,IB1-IG, and MVDM <ref> [2, 13, 12, 7] </ref>, and IGTREE [12], a decision-tree optimization of memory-based learning. Below, we outline the functioning of IB1-IG and IGTREE in Subsections 2.1 and 2.2, respectively. 1 The TIMBL software package is freely available for research purposes from the ILK web pages; consult URL http://ilk.kub.nl. 2.1. <p> TIMBL 1 is the name of a software package developed by the ILK group at Tilburg University, containing variations of the memory-based learning algorithms IB1,IB1-IG, and MVDM [2, 13, 12, 7], and IGTREE <ref> [12] </ref>, a decision-tree optimization of memory-based learning. Below, we outline the functioning of IB1-IG and IGTREE in Subsections 2.1 and 2.2, respectively. 1 The TIMBL software package is freely available for research purposes from the ILK web pages; consult URL http://ilk.kub.nl. 2.1. <p> Below, we outline the functioning of IB1-IG and IGTREE in Subsections 2.1 and 2.2, respectively. 1 The TIMBL software package is freely available for research purposes from the ILK web pages; consult URL http://ilk.kub.nl. 2.1. Weighted MBL in TIMBL: IB1-IG IB1-IG <ref> [13, 12] </ref> is a memory-based learning algorithm that builds a data base of instances (the instance base or case base) during learning. An instance consists of a fixed-length vector of n feature-value pairs, and an information field containing the classification of that particular feature-value vector.
Reference: [13] <author> Walter Daelemans and Antal van den Bosch. </author> <title> Gen-eralisation performance of backpropagation learning on a syllabification task. </title> <editor> In M. F. J. Drossaers and A. Nijholt, editors, </editor> <booktitle> Proc. of TWLT3: Connectionism and Natural Language Processing, </booktitle> <pages> pages 2737, </pages> <address> En-schede, </address> <year> 1992. </year> <institution> Twente University. </institution>
Reference-contexts: TIMBL 1 is the name of a software package developed by the ILK group at Tilburg University, containing variations of the memory-based learning algorithms IB1,IB1-IG, and MVDM <ref> [2, 13, 12, 7] </ref>, and IGTREE [12], a decision-tree optimization of memory-based learning. Below, we outline the functioning of IB1-IG and IGTREE in Subsections 2.1 and 2.2, respectively. 1 The TIMBL software package is freely available for research purposes from the ILK web pages; consult URL http://ilk.kub.nl. 2.1. <p> Below, we outline the functioning of IB1-IG and IGTREE in Subsections 2.1 and 2.2, respectively. 1 The TIMBL software package is freely available for research purposes from the ILK web pages; consult URL http://ilk.kub.nl. 2.1. Weighted MBL in TIMBL: IB1-IG IB1-IG <ref> [13, 12] </ref> is a memory-based learning algorithm that builds a data base of instances (the instance base or case base) during learning. An instance consists of a fixed-length vector of n feature-value pairs, and an information field containing the classification of that particular feature-value vector. <p> Optimized MBL in TIMBL: IGTREE Using information gain rather than Overlap distance to define similarity in IB1 improves its performance on several NLP tasks <ref> [13, 26, 24] </ref>. The positive effect of information gain on performance prompted us to develop an alternative approach in which the instance memory is restructured in such a way that it contains the same information as before, but in a compressed decision tree structure.
Reference: [14] <author> Walter Daelemans, Jakub Zavrel, and Peter Berck. </author> <title> Part-of-speech tagging for dutch with mbt, a memory-based tagger generator. </title> <editor> In K. van der Meer, editor, In-formatiewetenschap 1996, Wetenschappelijke bijdrage aan de Vierde Interdisciplinaire Onderzoeksconferen-tie Informatiewetenchap, </editor> <booktitle> pages 3340, The Nether-lands, </booktitle> <year> 1996. </year> <institution> TU Delft. </institution>
Reference-contexts: Preprocessing of this type of data typically does not take more than a few days. 3.2. MBT: Part-of-speech tagging The MBT tagger-generator <ref> [15, 14] </ref> takes an annotated corpus as input, and produces a lexicon and memory-based part-of-speech (POS) tagger as output. The problem of POS tagging (morphosyntactic disambiguation) is the following: given a text, provide for each word in the text its contextually disambiguated part of speech (morphosyntactic category). <p> As with the text-to-speech application, tagger development time with these methods is rapid once a suitable annotated training corpus is available. Currently, the MBT approach has been applied to Dutch <ref> [14] </ref>, English [15], Span-ish (CRATER Multi-Lingual Aligned Corpus), and Czech (An annotated corpus of newspaper texts from the Institute for the Czech Language). The results are shown in Table 5. 3.3.
Reference: [15] <author> Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. MBT: </author> <title> A memory-based part of speech tagger generator. </title> <editor> In E. Ejerhed and I.Dagan, editors, </editor> <booktitle> Proc. of Fourth Workshop on Very Large Corpora, pages 1427. ACL SIGDAT, </booktitle> <year> 1996. </year>
Reference-contexts: Preprocessing of this type of data typically does not take more than a few days. 3.2. MBT: Part-of-speech tagging The MBT tagger-generator <ref> [15, 14] </ref> takes an annotated corpus as input, and produces a lexicon and memory-based part-of-speech (POS) tagger as output. The problem of POS tagging (morphosyntactic disambiguation) is the following: given a text, provide for each word in the text its contextually disambiguated part of speech (morphosyntactic category). <p> As with the text-to-speech application, tagger development time with these methods is rapid once a suitable annotated training corpus is available. Currently, the MBT approach has been applied to Dutch [14], English <ref> [15] </ref>, Span-ish (CRATER Multi-Lingual Aligned Corpus), and Czech (An annotated corpus of newspaper texts from the Institute for the Czech Language). The results are shown in Table 5. 3.3.
Reference: [16] <author> P. .A. Devijver and J. Kittler. </author> <title> Pattern recognition. A statistical approach. </title> <publisher> Prentice-Hall, </publisher> <address> London, UK, </address> <year> 1982. </year>
Reference-contexts: The approach has surfaced in different contexts using a variety of alternative names such as similarity-based, example-based, exemplar-based, analogical, case-based, instance-based, and lazy learning [23, 7, 17, 2, 3]. Historically, memory-based learning algorithms are descendants of the k-nearest neighbor (henceforth k-NN) algorithm <ref> [8, 16, 2] </ref>. A MBL system, visualized schematically in Figure 1, contains two components: a learning component which is memory-based (from which MBL borrows its name), and a performance component which is similarity-based.
Reference: [17] <author> J. Kolodner. </author> <title> Case-based reasoning. </title> <publisher> Morgan Kauf--mann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The approach has surfaced in different contexts using a variety of alternative names such as similarity-based, example-based, exemplar-based, analogical, case-based, instance-based, and lazy learning <ref> [23, 7, 17, 2, 3] </ref>. Historically, memory-based learning algorithms are descendants of the k-nearest neighbor (henceforth k-NN) algorithm [8, 16, 2].
Reference: [18] <author> M. Marcus, B. Santorini, and M.A. Marcinkiewicz. </author> <title> Building a large annotated corpus of english: The penn treebank. </title> <booktitle> Computational Linguistics, </booktitle> <address> 19(2):313330, </address> <year> 1993. </year>
Reference-contexts: The size of the tag-set used, the size of train and test set and the generalization accuracy (combines known and unknown) are given. All taggers use the IGTREE algorithm. struction and known and unknown words case-base construction), and tested on 200,000 test words, both from the Penn Treebank tagged <ref> [18] </ref> Wall Street Journal corpus. Generalization performance on known words (96.7%), unknown words (90.6%), and total (96.4%) is competitive with alternative hand-crafted and statistical approaches, and both training and testing speed are excellent (text tagging is possible with a speed of 1200 words per second). <p> As an example, the IOB tagged sentence: The/I postman/I gave/O the/I man/I a/B letter/I ./O will result in the following baseNP bracketed sentence: [The postman] gave [the man] [a letter]. We applied IB1-IG and IGTREE to a corpus of Wall Street Journal text from the parsed Penn Tree-bank <ref> [18] </ref> corpus, using the same train and test set as [21]. A case is constructed for each focus word. The features are words and POS tags of the focus and adjacent words.
Reference: [19] <author> J.R. Quinlan. </author> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning, </booktitle> <address> 1:81206, </address> <year> 1986. </year>
Reference-contexts: We can compute statistics about the relevance of features by looking at which features are good predictors of the class labels. Information Theory gives us a useful tool for measuring feature relevance in this way <ref> [19, 20] </ref>. Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label.
Reference: [20] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: We can compute statistics about the relevance of features by looking at which features are good predictors of the class labels. Information Theory gives us a useful tool for measuring feature relevance in this way <ref> [19, 20] </ref>. Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label.
Reference: [21] <author> L.A. Ramshaw and M.P. Marcus. </author> <title> Text chunking using transformation-based learning. </title> <booktitle> In Proc. of third workshop on very large corpora, </booktitle> <pages> pages 8294, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In NP chunking sentences are segmented into non-recursive NP's, so called baseNP's [1]. NP chunking can e.g. be used to reduce the complexity of sub-sequential parsing, or to identify named entities for information retrieval. To perform this task, we used the baseNP tag set as presented in <ref> [21] </ref>: I for inside a baseNP, O for outside a baseNP, and B for the first word in a baseNP following another baseNP. <p> We applied IB1-IG and IGTREE to a corpus of Wall Street Journal text from the parsed Penn Tree-bank [18] corpus, using the same train and test set as <ref> [21] </ref>. A case is constructed for each focus word. The features are words and POS tags of the focus and adjacent words.
Reference: [22] <author> T. J. Sejnowski and C. S. Rosenberg. </author> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1:145168, </volume> <year> 1987. </year>
Reference-contexts: We henceforth refer to the task as GS, an acronym of Grapheme-phoneme conversion and Stress assignment. To generate the instances, window-ing is used <ref> [22] </ref>. Table 1 displays four example instances (of English word pronunciation) and their classifications. Classifications, i.e., phonemes with stress markers, are denoted by composite labels.
Reference: [23] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12):1213 1228, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: The approach has surfaced in different contexts using a variety of alternative names such as similarity-based, example-based, exemplar-based, analogical, case-based, instance-based, and lazy learning <ref> [23, 7, 17, 2, 3] </ref>. Historically, memory-based learning algorithms are descendants of the k-nearest neighbor (henceforth k-NN) algorithm [8, 16, 2].
Reference: [24] <author> A. Van den Bosch. </author> <title> Learning to pronounce written words: A study in inductive language learning. </title> <type> PhD thesis, </type> <institution> Universiteit Maastricht, </institution> <year> 1997. </year>
Reference-contexts: Optimized MBL in TIMBL: IGTREE Using information gain rather than Overlap distance to define similarity in IB1 improves its performance on several NLP tasks <ref> [13, 26, 24] </ref>. The positive effect of information gain on performance prompted us to develop an alternative approach in which the instance memory is restructured in such a way that it contains the same information as before, but in a compressed decision tree structure. <p> In this section we illustrate some examples of recent work on MBLE on three `light' NLP tasks: (i) text-to-speech conversion in TREETALK, (ii) part-of-speech tagging in MBT, and (iii) phrase chunking in MBC. 3.1. TREETALK: Text-to-speech conversion The TREETALK system <ref> [26, 25, 11, 24] </ref> has originally been designed for isolated word pronunciation, i.e., converting a written word to its phonemic representation as found in a pronunciation dictionary, and efforts are underway to extend it to modeling speech phenomena in texts, such as sentence accents and prosody. <p> Finally, from the results in this paper and those reported earlier <ref> [10, 24] </ref>, it appears that the properties of NLP tasks are such that non-abstracting approaches such as MBL are at an advantage in learning from the highly disjunctive datasets typical in NLP in contrast to eager methods such as decision tree induction and statistical inference.
Reference: [25] <author> A. Van den Bosch, A. Content, W. Daelemans, and B. De Gelder. </author> <title> Measuring the complexity of writing systems. </title> <journal> Journal of Quantitative Linguistics, </journal> <volume> 1(3), </volume> <year> 1995. </year>
Reference-contexts: In this section we illustrate some examples of recent work on MBLE on three `light' NLP tasks: (i) text-to-speech conversion in TREETALK, (ii) part-of-speech tagging in MBT, and (iii) phrase chunking in MBC. 3.1. TREETALK: Text-to-speech conversion The TREETALK system <ref> [26, 25, 11, 24] </ref> has originally been designed for isolated word pronunciation, i.e., converting a written word to its phonemic representation as found in a pronunciation dictionary, and efforts are underway to extend it to modeling speech phenomena in texts, such as sentence accents and prosody.
Reference: [26] <author> A. Van den Bosch and W. Daelemans. </author> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL, </booktitle> <pages> pages 4553, </pages> <year> 1993. </year>
Reference-contexts: Optimized MBL in TIMBL: IGTREE Using information gain rather than Overlap distance to define similarity in IB1 improves its performance on several NLP tasks <ref> [13, 26, 24] </ref>. The positive effect of information gain on performance prompted us to develop an alternative approach in which the instance memory is restructured in such a way that it contains the same information as before, but in a compressed decision tree structure. <p> In this section we illustrate some examples of recent work on MBLE on three `light' NLP tasks: (i) text-to-speech conversion in TREETALK, (ii) part-of-speech tagging in MBT, and (iii) phrase chunking in MBC. 3.1. TREETALK: Text-to-speech conversion The TREETALK system <ref> [26, 25, 11, 24] </ref> has originally been designed for isolated word pronunciation, i.e., converting a written word to its phonemic representation as found in a pronunciation dictionary, and efforts are underway to extend it to modeling speech phenomena in texts, such as sentence accents and prosody.
Reference: [27] <author> S. Weiss and C. </author> <title> Kulikowski. Computer systems that learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: One way to visualize this disjunctiv-ity is by looking at the average number of friendly neighbors for each instance in a leave-one-out experiment <ref> [27] </ref>. For each instance in the GS, POS, and CHUNK data sets a distance ranking of the 50 nearest neighbors to an instance was produced.
Reference: [28] <author> F. Yvon. Prononcer par analogie: </author> <title> motivation, for-malisation et evaluation. </title> <type> PhD thesis, </type> <institution> Ecole Nationale Superieure des Telecommunication, Paris, </institution> <year> 1996. </year>
Reference-contexts: For each language the algorithm used and the size of training and test sets is given, as well as generalization accuracy. to 5% errors on phonemes <ref> [28] </ref>). Thus, MBL of word pronunciation using the simple classification task definition as displayed in Table 1 leads to systems performing at a high level of accuracy.
Reference: [29] <author> J. Zavrel and W. Daelemans. </author> <title> Memory-based learning: Using similarity for smoothing. </title> <booktitle> In Proc. of 35th annual meeting of the ACL, </booktitle> <address> Madrid, </address> <year> 1997. </year>
Reference-contexts: MBLE shares these properties with statistical approaches to NLP. However, because of its computational efficiency and its non-parametric nature, the approach is better equipped than current statistical techniques to handle tasks described with relatively large numbers of features without estimation problems <ref> [29] </ref>. As described in the Introduction, MBLE treats NLP tasks either as light classification tasks, or as decomposed into multiple light classification subtasks. <p> The combined effect is that the extrapolation is always based on the most specific relevant training instances in the training set (including low-frequency items), and that back-off to more general relevant instances is automatic <ref> [29] </ref>.
References-found: 29

