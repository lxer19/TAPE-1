URL: http://www.first.gmd.de/persons/Kohlmorgen.Jens/nips97.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/1998/97abstracts.html
Root-URL: 
Title: Analysis of Drifting Dynamics with Neural Network Hidden Markov Models  
Author: J. Kohlmorgen K.-R. Muller K. Pawelzik MPI f. Stromungsforschung 
Address: FIRST Rudower Chaussee 5 12489 Berlin, Germany  FIRST Rudower Chaussee 5 12489 Berlin, Germany  37073 Gottingen, Germany  
Affiliation: GMD  GMD  Bunsenstr. 10  
Abstract: We present a method for the analysis of nonstationary time series with multiple operating modes. In particular, it is possible to detect and to model both a switching of the dynamics and a less abrupt, time consuming drift from one mode to another. This is achieved in two steps. First, an unsupervised training method provides prediction experts for the inherent dynamical modes. Then, the trained experts are used in a hidden Markov model that allows to model drifts. An application to physiological wake/sleep data demonstrates that analysis and modeling of real-world time series can be improved when the drift paradigm is taken into account.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jacobs, R.A., Jordan, M.A., Nowlan, S.J., Hinton, G.E. </author> <year> (1991). </year> <title> Adaptive Mixtures of Local Experts, </title> <booktitle> Neural Computation 3, </booktitle> <pages> 79-87. </pages>
Reference-contexts: It is a special case of the Mixtures of Experts <ref> [1] </ref> learning rule, with the gating network being omitted. Note that according to Bayes' rule the term in brackets is the posterior probability that expert i is the correct choice for the given data y, i.e. p (i j y).
Reference: [2] <author> Kohlmorgen, J., Muller, K.-R., Pawelzik, K. </author> <year> (1995). </year> <title> Improving short-term prediction with competing experts. </title> <journal> ICANN'95, EC2 & Cie, Paris, </journal> <volume> 2 </volume> <pages> 215-220. </pages>
Reference-contexts: Moreover, they do not reveal the dynamical structure of the system. Time series from alternating dynamics of this type can originate from many kinds of systems in physics, biology and engineering. In <ref> [2, 6, 8] </ref>, we have described a framework for time series from switching dynamics, in which an ensemble of neural network predictors specializes on the respective operating modes. <p> Therefore, we can simply write @ log L / p (i j y)(y f i ): (4) Furthermore, we imposed a low-pass filter on the prediction errors " i = (y f i ) 2 and used deterministic annealing of fi in the training process (see <ref> [2, 8] </ref> for details). We found that these modifications can be essential for a successful segmentation and prediction of time series from switching dynamics.
Reference: [3] <author> Kohlmorgen, J., Muller, K.-R., Rittweger, J., Pawelzik, K., </author> <note> in preparation. </note>
Reference-contexts: Moreover, it resolves the dynamical structure of the signal to more detail. For a more comprehensive analysis of wake/sleep data, we refer to our forthcoming publication <ref> [3] </ref>. 6 Summary and Discussion We presented a method for the unsupervised segmentation and identification of nonstationary drifting dynamics. It applies to time series where the dynamics is drifting or switching between different operating modes.
Reference: [4] <author> Mackey, M., Glass, L. </author> <year> (1977). </year> <title> Oscillation and Chaos in a Physiological Control System, </title> <booktitle> Science 197, </booktitle> <pages> 287. </pages>
Reference-contexts: or as infrequent switches. 4 Drifting Mackey-Glass Dynamics As an example, consider a high-dimensional chaotic system generated by the Mackey-Glass delay differential equation dx (t) = 0:1x (t) + 1 + x (t t d ) 10 : (9) It was originally introduced as a model of blood cell regulation <ref> [4] </ref>. Two stationary operating modes, A and B, are established by using different delays, t d = 17 and 23, respectively. After operating 100 time steps in mode A (with respect to a subsampling step size t = 6), the dynamics is drifting to mode B.
Reference: [5] <author> Moody, J., Darken, C. </author> <year> (1989). </year> <title> Fast Learning in Networks of Locally-Tuned Processing Units. </title> <booktitle> Neural Computation 1, </booktitle> <pages> 281-294. </pages>
Reference-contexts: The optimal choice of function approximators f i depends on the specific application. In general, however, neural networks are a good choice for the prediction of time series [11]. In this paper, we use radial basis function (RBF) networks of the Moody-Darken type <ref> [5] </ref> as predictors, because they offer a fast and robust learning method.
Reference: [6] <author> Muller, K.-R., Kohlmorgen, J., Pawelzik, K. </author> <year> (1995). </year> <title> Analysis of Switching Dynamics with Competing Neural Networks, </title> <journal> IEICE Trans. on Fundamentals of Electronics, Communications and Computer Sc., E78-A, </journal> <volume> No.10, </volume> <pages> 1306-1315. </pages>
Reference-contexts: Moreover, they do not reveal the dynamical structure of the system. Time series from alternating dynamics of this type can originate from many kinds of systems in physics, biology and engineering. In <ref> [2, 6, 8] </ref>, we have described a framework for time series from switching dynamics, in which an ensemble of neural network predictors specializes on the respective operating modes.
Reference: [7] <author> Muller, K.-R., Kohlmorgen, J., Rittweger, J., Pawelzik, K. </author> <year> (1995). </year> <title> Analysing Physiological Data from the Wake-Sleep State Transition with Competing Predictors, </title> <booktitle> NOLTA'95: Symposium on Nonlinear Theory and its Appl., </booktitle> <pages> 223-226. </pages>
Reference-contexts: example, between t = 100 and 200 it denotes a drift from net 3 to net 5, which appears to be exponential. (c) Increase of the prediction error when predictors are successively removed. (d) The two remaining predictors model the dynamics of the time series properly. 5 Wake/Sleep EEG In <ref> [7] </ref>, we analyzed physiological data recorded from the wake/sleep transition of a human.
Reference: [8] <author> Pawelzik, K., Kohlmorgen, J., Muller, K.-R. </author> <year> (1996). </year> <title> Annealed Competition of Experts for a Segmentation and Classification of Switching Dynamics, </title> <booktitle> Neural Computation, 8:2, </booktitle> <pages> 342-358. </pages>
Reference-contexts: Moreover, they do not reveal the dynamical structure of the system. Time series from alternating dynamics of this type can originate from many kinds of systems in physics, biology and engineering. In <ref> [2, 6, 8] </ref>, we have described a framework for time series from switching dynamics, in which an ensemble of neural network predictors specializes on the respective operating modes. <p> Therefore, we can simply write @ log L / p (i j y)(y f i ): (4) Furthermore, we imposed a low-pass filter on the prediction errors " i = (y f i ) 2 and used deterministic annealing of fi in the training process (see <ref> [2, 8] </ref> for details). We found that these modifications can be essential for a successful segmentation and prediction of time series from switching dynamics.
Reference: [9] <author> Rabiner, </author> <title> L.R. (1988). A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. In Readings in Speech Recognition, </title> <editor> ed. A. Waibel, K. Lee, </editor> <address> 267-296. San Mateo: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: For a detailed description of HMMs, see <ref> [9] </ref> and the references therein. <p> Next, the transition matrix A = fp ^s;s g has to be chosen. It determines the transition probability for each pair of states. In principle, this matrix can be found using a training procedure, as e.g. the Baum-Welch method <ref> [9] </ref>. However, this is hardly feasible in this case, because of the immense size of the matrix. In the above example, the matrix A has (896 + 8) 2 = 817216 elements that would have to be estimated. <p> Defining p (y j s) and is straightforward. Following eq.(1) and eq.(2), we assume gaussian noise p (y j s) = Ke fi (yg s ) 2 and equally probable initial states, s = jSj 1 . The Viterbi algorithm <ref> [9] </ref> can then be applied to the above stated HMM, without any further training of the HMM parameters.
Reference: [10] <author> Takens, F. </author> <year> (1981). </year> <title> Detecting Strange Attractors in Turbulence. </title> <editor> In: Rand, D., Young, L.-S., (Eds.), </editor> <booktitle> Dynamical Systems and Turbulence, Springer Lecture Notes in Mathematics, </booktitle> <volume> 898, </volume> <pages> 366. </pages>
Reference-contexts: 1 Introduction Modeling dynamical systems through a measured time series is commonly done by reconstructing the state space with time-delay coordinates <ref> [10] </ref>. The prediction of the time series can then be accomplished by training neural networks [11]. If, however, a system operates in multiple modes and the dynamics is drifting or switching, standard approaches like multi-layer perceptrons are likely to fail to represent the underlying input-output relations.
Reference: [11] <author> Weigend, </author> <title> A.S., </title> <editor> Gershenfeld, N.A. (Eds.) </editor> <year> (1994). </year> <title> Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: 1 Introduction Modeling dynamical systems through a measured time series is commonly done by reconstructing the state space with time-delay coordinates [10]. The prediction of the time series can then be accomplished by training neural networks <ref> [11] </ref>. If, however, a system operates in multiple modes and the dynamics is drifting or switching, standard approaches like multi-layer perceptrons are likely to fail to represent the underlying input-output relations. Moreover, they do not reveal the dynamical structure of the system. <p> The optimal choice of function approximators f i depends on the specific application. In general, however, neural networks are a good choice for the prediction of time series <ref> [11] </ref>. In this paper, we use radial basis function (RBF) networks of the Moody-Darken type [5] as predictors, because they offer a fast and robust learning method.
References-found: 11

