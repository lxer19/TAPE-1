URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-37.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Data filtering and distribution modeling algorithms for machine learning  
Author: by Yoav Freund Manfred K. Warmuth David Haussler David P. Helmbold Dean 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in  The dissertation of Yoav Freund is approved:  
Date: September 1993  
Affiliation: University of California Santa Cruz  Computer and Information Sciences  of Graduate Studies and Research  
Abstract-found: 0
Intro-found: 1
Reference: [Ackley et al., 1985] <author> D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. </author> <title> A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169, </pages> <year> 1985. </year>
Reference-contexts: Newer and more exciting models include Bayes networks [Pearl, 1988] and Markov random fields [Pearl, 1988, Geman and Geman, 1984, Geman, 1986]. In the neural network area, both Hopfield nets [Hopfield, 1982] and Boltzmann machines <ref> [Ackley et al., 1985] </ref> can be used as models of probability distributions on f1g n for relatively large n. We will look at a class of models defined by a special type of Boltzmann machine. <p> The combination machine is a simple Connectionist type model which is a special case of the Boltzmann Machine <ref> [Ackley et al., 1985] </ref>. As we shall see, the simplicity of this special case makes it easier to analyze than the general Boltzmann machine and allows the use of more efficient learning algorithms. At the same time, the model is still powerful enough to approximate any distribution of binary vectors. <p> The goal of learning is reduced to finding the set of parameters for the combination model that maximizes the (log of the) probability of the set of examples S. In fact, this gives the standard learning algorithm for general Boltzmann machines <ref> [Ackley et al., 1985] </ref>. For a general Boltzmann machine this would require stochastic estimation of the parameters. As stochastic estimation is very time-consuming, the result is that learning is very slow. In this section we show that stochastic estimation need not be used for the combination model.
Reference: [Angluin and Smith, 1983] <author> Dana Angluin and Carl H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15(3) </volume> <pages> 237-269, </pages> <month> September </month> <year> 1983. </year>
Reference-contexts: Various disciplines provide mathematical frameworks in which to analyze and compare such algorithms. Among these disciplines are pattern recognition [Duda and Hart, 1973a], estimation theory [Vapnik, 1982], the theory of stochastic modeling [Rissanen, 1986], the theory of inductive inference <ref> [Gold, 1967, Angluin and Smith, 1983] </ref> and computational learning theory [Valiant, 1984a]. Computational learning theory is a part of theoretical computer science. For this reason its natural emphasis is on learning questions that arise in computation theory, such as learning finite automata and Boolean formulas.
Reference: [Angluin, 1988a] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: In this paradigm the learner is passive and has no control over the information that it receives. In contrast, in the query paradigm, the learner is given the power to ask questions. What does the learner gain from this additional power? Study of the use of queries in learning <ref> [Valiant, 1984b, Angluin, 1988a] </ref>, has mostly concentrated on algorithms for exact identification of the target concept. This type of analysis concentrates on the worst case behavior of the algorithm, and no probabilistic assumptions are made.
Reference: [Angluin, 1988b] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: For a survey of query learning see <ref> [Angluin, 1988b] </ref>. The goal of most of the algorithms for learning using queries is exact identification of the underlying model. A less studied goal is to use queries in the context of approximate identification to reduce the number of labeled instances that the learner needs for learning.
Reference: [Atkinson and Donev, 1992] <author> A. C. Atkinson and A. N. Donev. </author> <title> Optimum Experimental Designs. </title> <publisher> Oxford science publications, </publisher> <year> 1992. </year>
Reference-contexts: The problem of selecting the optimal examples for learning is closely related to the problem of experimental design in statistics (see e.g. <ref> [Fedorov, 1972, Atkinson and Donev, 1992] </ref>). Experimental design is the analysis of methods for selecting sets of experiments, which correspond to membership queries in the context of learning theory.
Reference: [Barland, 1992] <author> Ian Barland. </author> <title> Some ideas on learning with directional feedback. </title> <type> Master's thesis, </type> <institution> University of California at Santa Cruz, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: The theorem proves a lower bound on the value of G (F ~x ). The proof is based on finding the convex version space that produces the smallest value of G (F ~x ). This body is constructed of two equivalent cones connected at their bases. Barland <ref> [Barland, 1992, Theorem 5] </ref>, analyzes a similar problem. He finds the convex body that achieves the minimal value of the functional R +1 1 min (F ~x (t); 1 F ~x (t)) dt. Interestingly enough, he finds that the same body produces the minimal value of this functional.
Reference: [Baum and Lang, 1991] <author> E. B. Baum and K. Lang. </author> <title> Constructing hidden units using examples and queries. </title> <booktitle> In Advances in Neural Information Processing, </booktitle> <volume> volume 3, </volume> <year> 1991. </year>
Reference-contexts: Baum [Baum, 1991], proposed a learning algorithm that uses membership queries to avoid the intractability of learning neural networks with hidden units. His algorithm is proved to work for networks with at most 4 hidden units, and there is experimental evidence <ref> [Baum and Lang, 1991] </ref> that it works for larger networks. However, when Baum and Lang tried to use this algorithm to train a network for classifying handwritten characters, they encountered an unexpected problem [Baum and Lang, 1992].
Reference: [Baum and Lang, 1992] <author> E. B. Baum and K. Lang. </author> <title> Query learning can work poorly when a human oracle is used. </title> <booktitle> In International Joint Conference in Neural Networks, </booktitle> <address> Beijing, China, </address> <year> 1992. </year>
Reference-contexts: However, when Baum and Lang tried to use this algorithm to train a network for classifying handwritten characters, they encountered an unexpected problem <ref> [Baum and Lang, 1992] </ref>. The problem was that many of the images generated by the algorithm as queries did not contain any recognizable character, they were artificial combinations of character images that had no natural meaning.
Reference: [Baum, 1991] <author> E. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 2 </volume> <pages> 5-19, </pages> <year> 1991. </year>
Reference-contexts: This added capability enables the learner to maintain its sensitivity to the input distribution, while reducing the number of labels that it needs to know. Baum <ref> [Baum, 1991] </ref>, proposed a learning algorithm that uses membership queries to avoid the intractability of learning neural networks with hidden units. His algorithm is proved to work for networks with at most 4 hidden units, and there is experimental evidence [Baum and Lang, 1991] that it works for larger networks.
Reference: [Blumer et al., 1986] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Classifying learnable geometric concepts with the Vapnik-Chervonenkis dimension. </title> <booktitle> In 18th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 273-282, </pages> <address> Berkeley, </address> <year> 1986. </year>
Reference-contexts: Theorem 2.3.12 shows that for any learnable concept class there exists an efficient learning algorithm for which the dependence of the sample size on the required accuracy, when all other parameters are fixed, is O (1=*(log 1=*) 3=2 ). A general lower bound of (1=*) is given in <ref> [Blumer et al., 1986] </ref> for learning any "non-trivial" concept class. This lower bound holds without regard to computational constraints on the learning algorithm.
Reference: [Blumer et al., 1987] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Denote the set of hypotheses generated by B Samp by H M . The size of H M is jH j c log m , where c = 1=(2fl 2 ). Following the well-known analysis of the Occam's razor principle <ref> [Blumer et al., 1987] </ref> we get that the probability that the final hypothesis is consistent with a random sample of size m but has error larger than * is smaller than jH M j (1 *) m = jH j c log m (1 *) m .
Reference: [Blumer et al., 1989] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: This might seem to be a very inefficient way of using the random examples. However, the total number of examples used for learning is very close to the lower bound on the number of examples needed for learning proven by Blumer et. al. <ref> [Blumer et al., 1989] </ref>. In fact, the boosting algorithm makes such efficient use of its resources, that its analysis gives the best general upper bounds that are currently known on the resources required for PAC learning algorithms that use polynomial resources. <p> As the distribution of the instances is within D from the uniform distribution, the probability of this set is at least D cos 1 (ff). On the other as the VC dimension of the d dimensional perceptron is d we can use the classical uniform convergence bounds from <ref> [Blumer et al., 1989] </ref>. Theorem 2.1 in [Blumer et al., 1989] guarantees that a hypothesis that is consistent with m labeled examples, chosen independently at random from an arbitrary distribution, has error smaller than * with probability 1 ffi if m max 4 log ffi 8d log * : Combining these <p> On the other as the VC dimension of the d dimensional perceptron is d we can use the classical uniform convergence bounds from <ref> [Blumer et al., 1989] </ref>. Theorem 2.1 in [Blumer et al., 1989] guarantees that a hypothesis that is consistent with m labeled examples, chosen independently at random from an arbitrary distribution, has error smaller than * with probability 1 ffi if m max 4 log ffi 8d log * : Combining these two arguments, we get the statement of
Reference: [Bollobas, 1985] <author> B. Bollobas. </author> <title> Random Graphs. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: Proof : The LHS of Inequality 2.2 is equal to the probability of the tail of a binomial distribution with p = 1=2 + fl and k trials, and can be bounded by Chernoff bounds <ref> [Bollobas, 1985, page 11] </ref>. as follows.
Reference: [Bonnesen and Fenchel, 1987] <author> T. Bonnesen and W. Fenchel. </author> <title> Theory of Convex Bodies. </title> <publisher> BCS Associates, </publisher> <address> Moscow, Idaho, USA, </address> <year> 1987. </year>
Reference: [Cohn et al., 1990] <author> D. Cohn, L. Atlas, and R. Ladner. </author> <title> Training connectionist networks with queries and selective sampling. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 2 </volume> <pages> 566-573, </pages> <year> 1990. </year>
Reference-contexts: The learning algorithm that is analyzed in this paper uses random unlabeled instances as queries and in this way avoids the problem encountered by Baum's algorithm. Our work is derived within the query filtering paradigm. In this paradigm, proposed by <ref> [Cohn et al., 1990] </ref>, the learner is given access to a stream of inputs drawn at random from the input distribution. The 53 learner sees every input, but chooses whether or not to query the teacher for the label. <p> Initially, most examples will be informative for the learner, but as the process continues, the prediction capabilities of the learner improve, and it discards most of the examples as non-informative, thus saving the human teacher a large amount of work. In <ref> [Cohn et al., 1990] </ref> there are several suggestions for query filters together with some empirical tests of their performance on simple problems. Seung et al.[Seung et al., 1992] have suggested a filter called "query by committee," and analytically calculated its performance for some perceptron-type learning problems.
Reference: [Cox and Snell, 1989] <author> D. R. Cox and E. J. Snell. </author> <title> Analysis of binary data. </title> <publisher> Chapman and Hall, </publisher> <year> 1989. </year>
Reference-contexts: The most popular mainstream statistics models for distributions on f1g n for large n appear to be small mixtures of Bernoulli product distributions 1 [Everitt and Hand, 1981, Nowlan, 1990], and models in which only k-wise dependencies between the components of the input are allowed, for some k &lt;< n <ref> [Freeman, 1987, Cox and Snell, 1989] </ref>. Newer and more exciting models include Bayes networks [Pearl, 1988] and Markov random fields [Pearl, 1988, Geman and Geman, 1984, Geman, 1986].
Reference: [Dempster et al., 1977] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Roy. Statist. Soc. B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: We have previously described how gradient ascent can be used for finding model with highest log-likelihood. However, for the special case where there is only a single hidden unit in the model, a much faster method can be used. This method is an Expectation-Maximization (EM) method <ref> [Dempster et al., 1977] </ref>. EM is a general method for estimating the parameters of distribution models that have both observable and unobservable random variables. <p> It starts with some initial guess of the parameters init , and proceeds by iterating the following two steps. It can be shown <ref> [Dempster et al., 1977] </ref>, that each of these iterations improves the likelihood of the parameters. 1. Using the old setting of the parameters, old , as if they were the actual parameters, some statistics of the joint distribution of the hidden and the observable variables are calculated. 2.
Reference: [Diaconis and Freedman, 1984] <author> P. Diaconis and D. Freedman. </author> <title> Asymptotics of graphical projection pursuit. </title> <journal> Annals of Statistics, </journal> <volume> 12 </volume> <pages> 793-815, </pages> <year> 1984. </year>
Reference-contexts: It is easy to show that every one dimensional projection of a Gaussian distribution generates a Normal marginal distribution. Thus the marginal distribution that is generated by the real-valued combination model is a mixture of normal distributions. Diaconis and Friedman <ref> [Diaconis and Freedman, 1984] </ref> have shown that, in some sense, most "well-behaved" distributions generate a marginal distribution that is close to normal when projected on a randomly chosen direction.
Reference: [Drucker et al., 1993] <author> Harris Drucker, Robert E. Schapire, and Ptrice Simard. </author> <title> Improving performance in neural networks using a boosting algorithm. </title> <booktitle> In Proceedings of the fifth Conf. on Neural Informations Processing Systems, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Last but not least, boosting has been successfully applied to some practical machine learning problems <ref> [Drucker et al., 1993] </ref>. Further experimentation with boosting methods will hopefully achieve even better results.
Reference: [Drucker, 1992 1993] <author> H. </author> <title> Drucker. private correspondence, </title> <type> 1992-1993. </type>
Reference-contexts: The analysis of this variant is quite straight forward, and its performance is close to the best performance we achieve. It also seems to be the variant whose application to practical learning 2 A full analysis of this algorithm is given in Appendix A.1. 12 problems is more efficient <ref> [Drucker, 1992 1993] </ref>. The major drawback of this method is that it requires storage of the whole training set, which makes the space complexity dependence on * be O ((log 1=*) 2 =*) (assuming that the concept class is fixed and that its VC dimension is finite). <p> One undesired property of our boosting algorithm is that it requires prior knowledge of a distribution-independent bound on the accuracy of the hypotheses that WeakLearn generates. While guessing a bound is a theoretically feasible solution, it is expensive in practical applications <ref> [Drucker, 1992 1993] </ref>. Schapire's algorithm is somewhat better in that respect, because if sample complexity is ignored it can be used without having prior knowledge of such a bound, and achieve an improvement over the performance of WeakLearn if such a uniform bound exists.
Reference: [Duda and Hart, 1973a] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: In general, a machine learning algorithm searches for a simple model that explains the past experience of the algorithm and then uses this model to predict future events. Various disciplines provide mathematical frameworks in which to analyze and compare such algorithms. Among these disciplines are pattern recognition <ref> [Duda and Hart, 1973a] </ref>, estimation theory [Vapnik, 1982], the theory of stochastic modeling [Rissanen, 1986], the theory of inductive inference [Gold, 1967, Angluin and Smith, 1983] and computational learning theory [Valiant, 1984a]. Computational learning theory is a part of theoretical computer science. <p> Other approaches to learning optimal prediction rules, such as pattern recognition <ref> [Duda and Hart, 1973a] </ref>, incorporate the assumption that the distribution of the instances is informative directly into their basic mathematical framework. It is thus interesting to compare our approach to the approach used for learning in pattern recognition. <p> We shall briefly describe a pattern recognition approach to this problem, and then compare it to the approaches described above. The classical mathematical description of the apple variety prediction problem that is used in pattern recognition <ref> [Duda and Hart, 1973a] </ref> is the following. We assume that each of the two varieties of apples has some probability of having any particular color. Thus each variety of apple corresponds to a distribution over the plane. <p> The estimation of the distributions can be performed by either parametric or non-parametric methods <ref> [Duda and Hart, 1973a] </ref>. The parametric methods search for a distribution from a particular family of distributions, such as Gaussian mixtures, that best fits the data. Non-parametric methods, such as the k-nearest neighbor classification method, estimate the probabilities locally around each point of the input space.
Reference: [Duda and Hart, 1973b] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year> <month> 120 </month>
Reference-contexts: There has been considerable work on density/parameter estimation for distributions on real vector spaces (see e.g. <ref> [Duda and Hart, 1973b] </ref>), and less on binary vector spaces.
Reference: [Eisenberg and Rivest, 1990] <author> Bonnie Eisenberg and Ronald L. Rivest. </author> <title> On the sample complexity of pac-learning using random and chosen examples. </title> <booktitle> In Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 154-162, </pages> <year> 1990. </year>
Reference-contexts: This question was previously studied by Eisenberg and Rivest <ref> [Eisenberg and Rivest, 1990] </ref> in the PAC learning framework. <p> We assume that both the examples and the target concept are chosen randomly. In particular, we show that queries can help accelerate learning of concept classes that are already learnable from just unlabeled data. This question was previously studied by Eisenberg and Rivest <ref> [Eisenberg and Rivest, 1990] </ref> in the PAC learning framework. They give a negative result, and show that, for a natural set of concept classes, which they call "dense in themselves", queries are essentially useless.
Reference: [Everitt and Hand, 1981] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite mixture distributions. </title> <publisher> Chapman and Hall, </publisher> <year> 1981. </year>
Reference-contexts: One type of algorithm selects projections of the input based on Principle Component analysis [Sanger, 1989, Oja, 1989]. Another type of algorithm clusters data based on an assumption that the underlying distribution is a mixture of Gaussians <ref> [Everitt and Hand, 1981, Nowlan, 1990] </ref>. The combination model presented in this paper is related to both of these lines of work and has some advantages over each of them. <p> The most popular mainstream statistics models for distributions on f1g n for large n appear to be small mixtures of Bernoulli product distributions 1 <ref> [Everitt and Hand, 1981, Nowlan, 1990] </ref>, and models in which only k-wise dependencies between the components of the input are allowed, for some k &lt;< n [Freeman, 1987, Cox and Snell, 1989].
Reference: [Fedorov, 1972] <author> V. V. Fedorov. </author> <title> Theory of Optimal Experiments. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: The problem of selecting the optimal examples for learning is closely related to the problem of experimental design in statistics (see e.g. <ref> [Fedorov, 1972, Atkinson and Donev, 1992] </ref>). Experimental design is the analysis of methods for selecting sets of experiments, which correspond to membership queries in the context of learning theory.
Reference: [Floyd and Warmuth, 1993] <author> Sally Floyd and Manfred Warmuth. </author> <title> Sample compressions, learnability, and the vapnik-chervonenkis dimension. </title> <type> Technical Report UCSC-CRL-93-13, </type> <institution> Computer and Information Sciences, University of California, Santa Cruz, </institution> <year> 1993. </year>
Reference-contexts: Littlestone, Warmuth and Floyd <ref> [Littlestone and Warmuth, 1986, Floyd and Warmuth, 1993] </ref> have analyzed algorithms that represent their hypotheses as sets of examples. As the above observation is of independent interest in the context of their work, we state it as a theorem. <p> In the proof of this theorem we use a technique invented by Littlestone and Warmuth [Littlestone and Warmuth, 1986] that appears as Appendix A in <ref> [Floyd and Warmuth, 1993] </ref>. Theorem 2.3.3: Let WeakLearn be a deterministic learning algorithm that generates, with probability &gt; 0 over the random training examples with which it is trained, a deterministic hypothesis whose error is smaller than 1=2 fl, for some fl &gt; 0.
Reference: [Freeman, 1987] <author> D. H. Freeman. </author> <title> Applied Catagorical Data Analysis. </title> <publisher> Marcel Dekker, </publisher> <year> 1987. </year>
Reference-contexts: The most popular mainstream statistics models for distributions on f1g n for large n appear to be small mixtures of Bernoulli product distributions 1 [Everitt and Hand, 1981, Nowlan, 1990], and models in which only k-wise dependencies between the components of the input are allowed, for some k &lt;< n <ref> [Freeman, 1987, Cox and Snell, 1989] </ref>. Newer and more exciting models include Bayes networks [Pearl, 1988] and Markov random fields [Pearl, 1988, Geman and Geman, 1984, Geman, 1986].
Reference: [Freund and Haussler, 1992] <author> Y. Freund and D. Haussler. </author> <title> Unsupevised learning of distributions on binary vectors using two-layer networks. </title> <booktitle> In Proceedings of the 1991 Conf. on Neural Informations Processing Systems, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In these experiments we show that the distribution model that is generated is meaningful and that it competes favorably with another popular distribution model. Part of this work was previously published in <ref> [Freund and Haussler, 1992] </ref>. 10 2. Boosting a weak learning algorithm by ma jority 2.1 Introduction The field of computational learning is concerned with mathematical analysis of algorithms that learn from their experience. One of the main problems studied in computational learning theory is that of concept learning.
Reference: [Freund et al., 1993] <author> Y. Freund, H.S Seung, E. Shamir, and N. Tishby. </author> <title> Accelerating learning using query by committee. </title> <booktitle> In Proceedings of the 1992 Conf. on Neural Informations Processing Systems (To appear), </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We also show that, in general, when such fast reduction is achieved by the Query by Committee algorithm, and the concept class is learnable, then the prediction error decreases exponentially fast in the number of queries asked. Part of this work was previously published in <ref> [Freund et al., 1993] </ref>. 1.3 Learning distributions of binary vectors The task of the learner in Chapters 2 and 3 is to generate a hypothesis that approximates a hidden concept, mapping instances to f+; g. Tasks of this type are often referred to as supervised learning tasks.
Reference: [Freund, 1990] <author> Y. Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Workshop on Computational Learning Theory, </booktitle> <pages> pages 202-216, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our analysis shows that our boosting algorithm can be used with such algorithms, and that the accuracy of the hypothesis that it outputs is proportional to the sensitivity of the given learning algorithm to changes in the distribution of the instances. Parts of this work were previously published in <ref> [Freund, 1990] </ref> and [Freund, 1992]. 1.2 Query By Committee As we have discussed in the previous section, all random training examples are not created equal.
Reference: [Freund, 1992] <author> Y. Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Parts of this work were previously published in [Freund, 1990] and <ref> [Freund, 1992] </ref>. 1.2 Query By Committee As we have discussed in the previous section, all random training examples are not created equal.
Reference: [Friedman and Tukey, 1974] <author> J.H. Friedman and J.W. Tukey. </author> <title> A projection pursuit algorithm for exploratory data analysis. </title> <journal> IEEE Trans. Comput., </journal> <pages> pages 881-889, </pages> <year> 1974. </year>
Reference-contexts: We present two algorithms for the unsupervised learning of this model from random unlabeled instances. The first algorithm is a standard gradient ascent algorithm for maximizing the likelihood of the distribution model. The other model is an adaptation of the "Projection Pursuit" algorithm developed by Friedman and Tukey <ref> [Friedman and Tukey, 1974, Friedman et al., 1984, Friedman, 1987] </ref>, and by Huber [Huber, 1985]. The basic idea of this algorithm is that the important structure of a high dimensional distribution is conveyed by those projections of the distribution that are most different from the normal distribution.
Reference: [Friedman et al., 1984] <author> J. H. Friedman, W.Stuetzle, and A. Schroeder. </author> <title> Projection pursuit density estimation. </title> <journal> J. Amer. Stat.Assoc., </journal> <volume> 79 </volume> <pages> 599-608, </pages> <year> 1984. </year>
Reference-contexts: We present two algorithms for the unsupervised learning of this model from random unlabeled instances. The first algorithm is a standard gradient ascent algorithm for maximizing the likelihood of the distribution model. The other model is an adaptation of the "Projection Pursuit" algorithm developed by Friedman and Tukey <ref> [Friedman and Tukey, 1974, Friedman et al., 1984, Friedman, 1987] </ref>, and by Huber [Huber, 1985]. The basic idea of this algorithm is that the important structure of a high dimensional distribution is conveyed by those projections of the distribution that are most different from the normal distribution. <p> However, under certain natural conditions we show that there exists a good approximation that requires only polynomial time. We then explore the relationships between the distributions generated by the combination model and those studied in Projection Pursuit density estimation <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. We show that the search for hidden variables that have a strong influence on the input distribution can be interpreted as a search for projections of the input that have a non-Normal marginal distribution. <p> Each factor in the product is associated with one hidden unit in the corresponding machine. This product form is particular to the combination model, and does not hold for general Boltzmann machines. Product form distribution models have been used for density estimation in Projection Pursuit <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. We shall look further into this relationship in Section 4.3.3. <p> Such methods would compute an approximation to the gradient by computing only the largest terms in the sum that defines it. 4.3.3 Projection Pursuit methods A statistical method that has a close relationship with the combination model is the Projection Pursuit (PP) technique <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. In this section we give a short overview of the technique, show how it relates to the combination model, and present a learning algorithm for the combination model based on Projection Pursuit methods. <p> Given a choice of ~ff (i) , the optimal choice of the function g i () in terms of maximizing the likelihood is the following <ref> [Friedman et al., 1984] </ref>. Define p ~ff (i) i (t) to be the marginal density on R generated by projecting the density p i on the direction ~ff (i) .
Reference: [Friedman, 1987] <author> J. H. Friedman. </author> <title> Exploratory projection pursuit. </title> <journal> J. Amer. Stat.Assoc., </journal> <volume> 82(397) </volume> <pages> 599-608, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: We present two algorithms for the unsupervised learning of this model from random unlabeled instances. The first algorithm is a standard gradient ascent algorithm for maximizing the likelihood of the distribution model. The other model is an adaptation of the "Projection Pursuit" algorithm developed by Friedman and Tukey <ref> [Friedman and Tukey, 1974, Friedman et al., 1984, Friedman, 1987] </ref>, and by Huber [Huber, 1985]. The basic idea of this algorithm is that the important structure of a high dimensional distribution is conveyed by those projections of the distribution that are most different from the normal distribution. <p> However, under certain natural conditions we show that there exists a good approximation that requires only polynomial time. We then explore the relationships between the distributions generated by the combination model and those studied in Projection Pursuit density estimation <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. We show that the search for hidden variables that have a strong influence on the input distribution can be interpreted as a search for projections of the input that have a non-Normal marginal distribution. <p> Each factor in the product is associated with one hidden unit in the corresponding machine. This product form is particular to the combination model, and does not hold for general Boltzmann machines. Product form distribution models have been used for density estimation in Projection Pursuit <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. We shall look further into this relationship in Section 4.3.3. <p> Such methods would compute an approximation to the gradient by computing only the largest terms in the sum that defines it. 4.3.3 Projection Pursuit methods A statistical method that has a close relationship with the combination model is the Projection Pursuit (PP) technique <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. In this section we give a short overview of the technique, show how it relates to the combination model, and present a learning algorithm for the combination model based on Projection Pursuit methods. <p> The search for a description of the distribution of a sample in terms of its projections can be formalized in the context of maximal likelihood density estimation in the following way <ref> [Friedman, 1987] </ref>. Define p 0 (~x) to be the initial estimate of the density over R n , i.e. the Gaussian density with appropriate mean and covariance. <p> The main problem of designing a projection pursuit method is finding a good projection index whose calculation can be performed efficiently. Various projection indices have been discussed in the literature <ref> [Huber, 1985, Friedman, 1987] </ref>. Selection of a direction that has a high projection index is usually performed using gradient following methods. <p> Normally distributed. So called "structure removal" methods have been devised towards this goal <ref> [Huber, 1985, Friedman, 1987] </ref>. These methods alter the sample in such a way that a specific single projection that has been interesting is made uninteresting while all orthogonal projections are left unchanged. <p> In this way projection pursuit avoids, to some degree, the infamous "curse of dimensionality" in the estimation of the distribution of high dimensional data. 98 Projection Pursuit has proved itself successful in some experiments <ref> [Friedman, 1987] </ref>. However, the search for best density is performed in a greedy manner and might not succeed in finding the optimal density in PP m .
Reference: [Gefner and Pearl, 1987] <author> Hector Gefner and Judea Pearl. </author> <title> On the probabilistic semantics of connectionist networks. </title> <type> Technical Report CSD-870033, </type> <institution> UCLA Computer Science Department, </institution> <month> July </month> <year> 1987. </year>
Reference-contexts: The computational complexity of 3 Recent work on modeling correlations by hidden units has also been done by Radford M. Neal [Neal, 1990]. In his work he gives a different variant of the Boltzmann Machine algorithm that uses distribution models similar to Judea Pearl's Bayes Networks <ref> [Pearl, 1988, Gefner and Pearl, 1987] </ref>. His model is superior to the Boltzmann Machine in the sense that the connection weights are interpreted as conditional probabilities, which is a more accessible interpretation than local energy interactions. The learning algorithms that Neal used are based on stochastic approximation.
Reference: [Geman and Geman, 1984] <author> S Geman and D Geman. </author> <title> Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-742, </pages> <year> 1984. </year>
Reference-contexts: Newer and more exciting models include Bayes networks [Pearl, 1988] and Markov random fields <ref> [Pearl, 1988, Geman and Geman, 1984, Geman, 1986] </ref>. In the neural network area, both Hopfield nets [Hopfield, 1982] and Boltzmann machines [Ackley et al., 1985] can be used as models of probability distributions on f1g n for relatively large n.
Reference: [Geman, 1986] <author> Stuart Geman. </author> <title> Stochastic relaxation methods for image restoration and expert systems. In D.B. </title> <editor> Cooper, R.L.Launer, and D.E. McClure, editors, </editor> <title> Automated Image Analysis: Theory and Experiments. </title> <publisher> Academic Press, </publisher> <year> 1986. </year>
Reference-contexts: Newer and more exciting models include Bayes networks [Pearl, 1988] and Markov random fields <ref> [Pearl, 1988, Geman and Geman, 1984, Geman, 1986] </ref>. In the neural network area, both Hopfield nets [Hopfield, 1982] and Boltzmann machines [Ackley et al., 1985] can be used as models of probability distributions on f1g n for relatively large n.
Reference: [Gold, 1967] <author> E. Mark Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: Various disciplines provide mathematical frameworks in which to analyze and compare such algorithms. Among these disciplines are pattern recognition [Duda and Hart, 1973a], estimation theory [Vapnik, 1982], the theory of stochastic modeling [Rissanen, 1986], the theory of inductive inference <ref> [Gold, 1967, Angluin and Smith, 1983] </ref> and computational learning theory [Valiant, 1984a]. Computational learning theory is a part of theoretical computer science. For this reason its natural emphasis is on learning questions that arise in computation theory, such as learning finite automata and Boolean formulas.
Reference: [Goldmann et al., 1992] <author> M. Goldmann, J. Hastad, and A. Razborov. </author> <title> Majority gates vs. general weighted threshold gates. </title> <booktitle> In Structures conference Proceedings, </booktitle> <pages> pages 2-13, </pages> <year> 1992. </year>
Reference-contexts: Schapire [Schapire, 1992], noted that the results presented in this paper can be used to show an interesting relationship between representation and approximation using majority gates. These results 13 were independently discovered by Hastad et. al. <ref> [Goldmann et al., 1992] </ref>. However, while their proof technique is very elegant, our proof is more constructive (for details see Section 2.2.2). It is surprising to note that the boosting algorithm uses only a small fraction of the examples in the training set. <p> This application of boosting has been discovered by Schapire [Schapire, 1992]. A slightly weaker version of this result was independently proven by Goldmann, Hastad, and Razborov <ref> [Goldmann et al., 1992] </ref> using a completely different proof technique. In the following presentation we follow their notation. Let f denote a Boolean function whose domain is f1; 1g n and range is f1; 1g. Let H be a set of Boolean functions defined over the same domain and range. <p> However, it is not a constructive proof. On the other hand, our proof is constructive in that it shows how to generate the distributions that correspond to the desired functions in H. For completeness we give a simple lemma (Lemma 3.2 in <ref> [Goldmann et al., 1992] </ref>) that gives an approximate converse to Theorem 2.2.5. Lemma 2.2.6: Let f and H be as in Theorem 2.2.5.
Reference: [Graham et al., 1991] <author> Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. </author> <title> Concrete mathematics, a foundation for computer science. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference: [Haussler et al., 1988] <author> David Haussler, Nick Littlestone, and Manfred Warmuth. </author> <title> Predicting 0,1-functions on randomly drawn points. </title> <booktitle> In Proceedings of the 29th Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 100-109. </pages> <publisher> IEEE, </publisher> <year> 1988. </year> <month> 121 </month>
Reference-contexts: A general lower bound of (1=*) is given in [Blumer et al., 1986] for learning any "non-trivial" concept class. This lower bound holds without regard to computational constraints on the learning algorithm. There exists a matching upper-bound, given in <ref> [Haussler et al., 1988] </ref>[Theorem 5.1], which says that, ignoring dependence on other parameters, any concept class that can be learned using a sample of size polynomial in 1=* can be learned using a sample of size O (1=*).
Reference: [Haussler et al., 1991a] <author> David Haussler, Michael Kearns, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Information and Computation, </journal> <volume> 95 </volume> <pages> 129-161, </pages> <year> 1991. </year>
Reference-contexts: said to have accuracy * with reliability 1 ffi if the probability, over the random choice of the examples and possible internal randomization of the learning algorithm, of generating a hypothesis that has error smaller than *, is larger than 1 ffi. 1 As was recognized by Haussler et. al. <ref> [Haussler et al., 1991a] </ref>, increasing the reliability of any learning algorithm is easy. This can be done by testing the hypothesis generated by the algorithm on an independent set of examples to validate its accuracy. <p> In order for the algorithm B Filt to work successfully, we need the reliability of WeakLearn to be high. However, as noted by Haussler et. al. <ref> [Haussler et al., 1991a] </ref>, it is easy to boost the reliability of a learning algorithm. We give the performance of one possible reliability-boosting algorithm, B Rel in the following lemma. The proof of the lemma and the description of the algorithm are given in Appendix A.1. <p> This framework is one of the most well studied frameworks in computational learning theory. In this section we show the implications of our work on the PAC learning framework. We start by presenting some notation following Haussler et. al. <ref> [Haussler et al., 1991a] </ref>. Assume that the sample space is a union of sample spaces of increasing complexity: X = [ 1 n=1 X n . <p> A simple application of Theorem 2.3.9 gives the following upper bound on the resources required for PAC learning. Theorem 2.3.12: If C is a weakly PAC learnable concept class, parameterized by n and s in the standard way <ref> [Haussler et al., 1991a] </ref>, then there exists a PAC learning algorithm for C that learns with accuracy * and reliability ffi and: * requires a sample of size (1=*)(log 1=*) 3=2 (log log 1=* + log 1=ffi)p 1 (n; s), 12 PAC learning stands for Probably Approximately Correct learning. 40 *
Reference: [Haussler et al., 1991b] <author> David Haussler, Michael Kearns, and Robert Schapire. </author> <title> Bounds on the sample complexity of bayesian learning using information theory and the vc dimension. </title> <booktitle> In Proceedings of the 1991 Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In Section 3.7 we give a broader view on using unlabeled examples for accelerating learning, and in Section3.8 we summarize and point to some potential future directions. 54 3.2 Preliminaries We work in a Bayesian model of concept learning <ref> [Haussler et al., 1991b] </ref>. As in the PAC model, we denote by X an arbitrary sample space over which a distribution D is defined. In this paper we concentrate on the case where X is a Euclidean space R d . <p> The random choice of h is made according to the prior distribution P restricted to the version space. It is a simple observation (see <ref> [Haussler et al., 1991b] </ref>), that the expected error of this prediction error is at most twice larger than the expected error of the optimal prediction rule which is the Bayes rule. <p> From the definition of t n we get that (1 *) * ln 2 (n+1) 2 3ffi = 2 (n + 1) 2 : Summing this probability over all possible values of n from zero to infinity we get the statement of the lemma. In <ref> [Haussler et al., 1991b] </ref> it was shown that if the VC-dimension of a concept class is d, then the expected information gain from m random examples is bounded by (d + 1) log (m=d).
Reference: [Haussler et al., to appear] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference: [Hertz et al., 1991] <author> John Hertz, Anders Krogh, and Richard G. Palmer. </author> <title> Introduction To The Theory Of Neural Computation. </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Today, as computers are getting faster, the trend is towards using non-linear and hierarchical models to describe very complex high dimensional data. Methods that go under titles such as "neural networks" <ref> [Rumelhart and McClelland, 1986, Hertz et al., 1991] </ref>, "hidden Markov chains" [Rabiner and Juang, 1986], and "radial basis functions" [Poggio and Girosi, 1989] are becoming increasingly popular. <p> The final result 10 The algorithm used a standard momentum term (see <ref> [Hertz et al., 1991] </ref>, page 123) to accelerate the convergence. 11 The difference between the measurements of the quality of the true model on the test set and on the training set are due to the random fluctuations between the two sets of examples.
Reference: [Hopfield, 1982] <author> J.J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad Sci. USA, </institution> <month> 79 </month> <pages> 2554-2558, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Newer and more exciting models include Bayes networks [Pearl, 1988] and Markov random fields [Pearl, 1988, Geman and Geman, 1984, Geman, 1986]. In the neural network area, both Hopfield nets <ref> [Hopfield, 1982] </ref> and Boltzmann machines [Ackley et al., 1985] can be used as models of probability distributions on f1g n for relatively large n. We will look at a class of models defined by a special type of Boltzmann machine.
Reference: [Huber, 1985] <author> P.J. Huber. </author> <title> Projection pursuit (with discussion). </title> <journal> Ann. Stat., </journal> <volume> 13 </volume> <pages> 435-525, </pages> <year> 1985. </year>
Reference-contexts: The first algorithm is a standard gradient ascent algorithm for maximizing the likelihood of the distribution model. The other model is an adaptation of the "Projection Pursuit" algorithm developed by Friedman and Tukey [Friedman and Tukey, 1974, Friedman et al., 1984, Friedman, 1987], and by Huber <ref> [Huber, 1985] </ref>. The basic idea of this algorithm is that the important structure of a high dimensional distribution is conveyed by those projections of the distribution that are most different from the normal distribution. <p> However, under certain natural conditions we show that there exists a good approximation that requires only polynomial time. We then explore the relationships between the distributions generated by the combination model and those studied in Projection Pursuit density estimation <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. We show that the search for hidden variables that have a strong influence on the input distribution can be interpreted as a search for projections of the input that have a non-Normal marginal distribution. <p> Each factor in the product is associated with one hidden unit in the corresponding machine. This product form is particular to the combination model, and does not hold for general Boltzmann machines. Product form distribution models have been used for density estimation in Projection Pursuit <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. We shall look further into this relationship in Section 4.3.3. <p> Such methods would compute an approximation to the gradient by computing only the largest terms in the sum that defines it. 4.3.3 Projection Pursuit methods A statistical method that has a close relationship with the combination model is the Projection Pursuit (PP) technique <ref> [Huber, 1985, Friedman et al., 1984, Friedman, 1987] </ref>. In this section we give a short overview of the technique, show how it relates to the combination model, and present a learning algorithm for the combination model based on Projection Pursuit methods. <p> The main problem of designing a projection pursuit method is finding a good projection index whose calculation can be performed efficiently. Various projection indices have been discussed in the literature <ref> [Huber, 1985, Friedman, 1987] </ref>. Selection of a direction that has a high projection index is usually performed using gradient following methods. <p> Normally distributed. So called "structure removal" methods have been devised towards this goal <ref> [Huber, 1985, Friedman, 1987] </ref>. These methods alter the sample in such a way that a specific single projection that has been interesting is made uninteresting while all orthogonal projections are left unchanged.
Reference: [Jolliffe, 1986] <author> I.T. Jolliffe. </author> <title> Principle Component Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: states of ~ h. 6 This interpretation of the real-valued model will be used in Section (4.3.6) in a Projection Pursuit algorithm for learning the combination model. 4.2.4 Comparison with principal components analysis Principal Component Analysis (PCA) is a popular method for the analysis of high order correlations (see e.g. <ref> [Jolliffe, 1986] </ref>). Many algorithms for unsupervised learning are based on this method, among them some learning rules for neural networks [Sanger, 1989, Oja, 1989]. The method is based on the covariance matrix, which measures pairwise correlations among input bits.
Reference: [Kearns and Schapire, 1990] <author> Michael J. Kearns and Robert E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 382-391, </pages> <year> 1990. </year>
Reference-contexts: However, the results regarding real-valued concept classes are still rather weak, and one would hope that stronger types of boosting can be achieved in that context. The use of boosting in the context of p-concepts <ref> [Kearns and Schapire, 1990] </ref> is another long standing open problem. Some progress on the problem of boosting in the context of independent label noise has been achieved in a recent work by Aslam and Decatur about boosting learning algorithms in the the statistical query model introduced by Kearns [Kearns, 1993]. <p> However, several obstacles remain in the path to this type of analysis. First, we wish to consider ambiguous concepts, i.e. concepts that allow mapping a particular color to both types of apples. This type of mapping has been formalized by Kearns and Schapire <ref> [Kearns and Schapire, 1990] </ref> using the notion of p-concepts.
Reference: [Kearns and Valiant, 1988] <author> M. Kearns and L.G. Valiant. </author> <title> Learning boolean formulae or finite automata is as hard as factoring. </title> <type> Technical Report TR-14-88, </type> <institution> Harvard University Aiken Computation Laboratory, </institution> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: Kearns and Valiant <ref> [Kearns and Valiant, 1988, Kearns and Valiant, 1989] </ref> introduced a weaker form of learnability in which the error cannot necessarily be made arbitrarily small.
Reference: [Kearns and Valiant, 1989] <author> M. Kearns and L. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <booktitle> In 21st ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <address> Seattle, WA, </address> <year> 1989. </year>
Reference-contexts: For many concept classes it is hard to find such algorithms, moreover, for an increasing number of concept classes there are proofs that no efficient algorithm exist (modulo some technical assumptions <ref> [Kearns and Valiant, 1989, Kharitonov, 1993] </ref>). A natural question is how the requirements of the PAC learning model can be weakened to make the problem easier. One possible weakening of the model, appropriately called weak PAC learning was suggested by Kearns and Valiant [Kearns and Valiant, 1989]. <p> A natural question is how the requirements of the PAC learning model can be weakened to make the problem easier. One possible weakening of the model, appropriately called weak PAC learning was suggested by Kearns and Valiant <ref> [Kearns and Valiant, 1989] </ref>. In this model one omits the requirement that * and ffi can be arbitrarily small. For example, one can ask for a learning algorithm that with a probability of 10% generates a hypothesis whose error is smaller than 25%. <p> Two different variants of the PAC model were introduced by Kearns and Valiant <ref> [Kearns and Valiant, 1989] </ref> to address this issue. In strong PAC learning, which is the more common model, the learner is given the required accuracy, *, as input, and is required to generate a hypothesis whose error is smaller than *. <p> Kearns and Valiant <ref> [Kearns and Valiant, 1988, Kearns and Valiant, 1989] </ref> introduced a weaker form of learnability in which the error cannot necessarily be made arbitrarily small.
Reference: [Kearns, 1993] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 392-401. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Some progress on the problem of boosting in the context of independent label noise has been achieved in a recent work by Aslam and Decatur about boosting learning algorithms in the the statistical query model introduced by Kearns <ref> [Kearns, 1993] </ref>. Last but not least, boosting has been successfully applied to some practical machine learning problems [Drucker et al., 1993]. Further experimentation with boosting methods will hopefully achieve even better results.
Reference: [Kharitonov, 1993] <author> M. Kharitonov. </author> <title> Cryptographic hardness of distribution-specific learning. </title> <booktitle> In Proceedings of the 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 372-381. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: For many concept classes it is hard to find such algorithms, moreover, for an increasing number of concept classes there are proofs that no efficient algorithm exist (modulo some technical assumptions <ref> [Kearns and Valiant, 1989, Kharitonov, 1993] </ref>). A natural question is how the requirements of the PAC learning model can be weakened to make the problem easier. One possible weakening of the model, appropriately called weak PAC learning was suggested by Kearns and Valiant [Kearns and Valiant, 1989].
Reference: [Kinzel and Rujan, 1990] <author> W. Kinzel and P. Rujan. </author> <title> Improving a network generalization ability by selecting examples. </title> <journal> Europhys. Lett., </journal> <volume> 13 </volume> <pages> 473-477, </pages> <year> 1990. </year>
Reference-contexts: In the context of Bayesian estimation a very general measure of the quality of a query is the reduction in the probability of the set of possible hypotheses that is induced by the answer to the query. Similar suggestions have been made in the perceptron learning literature <ref> [Kinzel and Rujan, 1990] </ref>. A different experimental design criterion is the accuracy with which the outcome of future experiments, chosen from some constrained domain, can be predicted using the hypothesis. This criterion is very similar to criteria used in learning theory. Both criteria are important for us in this paper.
Reference: [Lindley, 1956] <author> D. V. Lindley. </author> <title> On a measure of the information provided by an experiment. </title> <journal> Ann. Math. Statist., </journal> <volume> 27 </volume> <pages> 986-1005, </pages> <year> 1956. </year>
Reference-contexts: One natural criterion is the accuracy with which the parameters that define the hypothesis can be estimated <ref> [Lindley, 1956] </ref>. In the context of Bayesian estimation a very general measure of the quality of a query is the reduction in the probability of the set of possible hypotheses that is induced by the answer to the query.
Reference: [Littlestone and Warmuth, 1986] <author> Nick Littlestone and Manfred Warmuth. </author> <title> Relating data compression and learnability. </title> <note> This early and hard-to-locate work is referenced and partly re-written in FW93, </note> <year> 1986. </year>
Reference-contexts: Littlestone, Warmuth and Floyd <ref> [Littlestone and Warmuth, 1986, Floyd and Warmuth, 1993] </ref> have analyzed algorithms that represent their hypotheses as sets of examples. As the above observation is of independent interest in the context of their work, we state it as a theorem. <p> We now move on to prove a bound on the size of the sample that B Samp has to use in order to guarantee that the final hypothesis has error smaller than *. In the proof of this theorem we use a technique invented by Littlestone and Warmuth <ref> [Littlestone and Warmuth, 1986] </ref> that appears as Appendix A in [Floyd and Warmuth, 1993]. <p> We now give the formal proof. Which is an adaptation of a technique used by Warmuth and Littlestone in <ref> [Littlestone and Warmuth, 1986] </ref>. Fix any concept c 2 C.
Reference: [Littlestone and Warmuth, 1989] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 256-261, </pages> <year> 1989. </year>
Reference-contexts: We will define a measure of 13 Restricting a learning algorithm in such a way is natural in the context of hierarchical learning models such as weighted-majority <ref> [Littlestone and Warmuth, 1989] </ref> and layered neural networks. Some of the analysis of the weighted majority algorithm is concerned with efficiently searching for a good learning algorithm in a pool of algorithms.
Reference: [McDiarmid, 1989] <author> C. McDiarmid. </author> <title> On the method of bounded differences. In Survey of Combinatorics, </title> <booktitle> 10th British Combinatorial Conference, </booktitle> <year> 1989. </year>
Reference-contexts: Put in another way, this means that the random variables 62 form a sequence of sub-martingale differences. As the instantaneous information gain is bounded between 0 and 1, we get that g Y i 1 g. We can thus use Hoeffding's bound on the tails of bounded step sub-martingales <ref> [McDiarmid, 1989] </ref> 4 from which we know that for any * &gt; 0 Pr ( i=1 g ) g+* ( 1 g * n Setting * = g and taking logs we get Pr ( i=1 Y i gn) 1 (1+)g n Choosing = 1=2 we get the bound Lemma 3.5.3: <p> Fix a sequence of examples ~ X, recall that ~ X M denotes the first m examples. Then Pr c2P I (h ~ X M ; c ( ~ X M )i) (d + 1)(log d d : (3:7) 4 The bound as it appears in <ref> [McDiarmid, 1989] </ref> is given for martingales. However, it is easily checked that it is also true for super-martingales.
Reference: [Mitchell, 1978] <author> Tom Mitchell. </author> <title> Version spaces: as approach to concept learning. </title> <type> Technical Report Tech. Report CS-78-711, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <year> 1978. </year>
Reference-contexts: We use ~ X 1:::m to denote the sequence of the first m elements in ~ X. We use the terminology of Mitchell <ref> [Mitchell, 1978] </ref>, and define the version space generated by the sequence of labeled examples h ~ X 1:::m ; c ( ~ X 1:::m )i to be the set of concepts c 0 2 C that are consistent with c on ~ X, i.e. that c 0 (x i ) =
Reference: [Natarajan, 1991] <author> Balas K. Natarajan. </author> <title> Machine Learning aTheoretical approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: described in the next section might be more relevant. 2.4.3 Boosting real valued concepts A modification of the boosting algorithm can be used for boosting learning algorithms for concept classes whose range is a real number (for a review of algorithms for learning real valued functions, see Chapter 5 in <ref> [Natarajan, 1991] </ref>). This variant of the boosting algorithm transforms learning algorithms that generate hypotheses whose expected error, with respect to the input distribution, is small to algorithms that generate hypotheses whose error is small for most of the input domain.
Reference: [Neal, 1990] <author> Radford M. Neal. </author> <title> Learning stochastic feedforward networks. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Toronto, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Thus we get a faster learning algorithm than the standard Boltzmann rule that is also exact. The computational complexity of 3 Recent work on modeling correlations by hidden units has also been done by Radford M. Neal <ref> [Neal, 1990] </ref>. In his work he gives a different variant of the Boltzmann Machine algorithm that uses distribution models similar to Judea Pearl's Bayes Networks [Pearl, 1988, Gefner and Pearl, 1987].
Reference: [Nowlan, 1990] <author> S. Nowlan. </author> <title> Maximum likelihood competitive learning. </title> <editor> In D. Touretsky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 574-582. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year> <month> 122 </month>
Reference-contexts: One type of algorithm selects projections of the input based on Principle Component analysis [Sanger, 1989, Oja, 1989]. Another type of algorithm clusters data based on an assumption that the underlying distribution is a mixture of Gaussians <ref> [Everitt and Hand, 1981, Nowlan, 1990] </ref>. The combination model presented in this paper is related to both of these lines of work and has some advantages over each of them. <p> The most popular mainstream statistics models for distributions on f1g n for large n appear to be small mixtures of Bernoulli product distributions 1 <ref> [Everitt and Hand, 1981, Nowlan, 1990] </ref>, and models in which only k-wise dependencies between the components of the input are allowed, for some k &lt;< n [Freeman, 1987, Cox and Snell, 1989].
Reference: [Oja, 1989] <author> E. Oja. </author> <title> Neural networks, principle components, and subspaces. </title> <journal> Int. J. Neural Systems, </journal> <volume> 1(1) </volume> <pages> 61-68, </pages> <year> 1989. </year>
Reference-contexts: The idea that features that are useful for classification can be deduced from the distribution of typical inputs is the basis of several existing algorithms for unsupervised learning. One type of algorithm selects projections of the input based on Principle Component analysis <ref> [Sanger, 1989, Oja, 1989] </ref>. Another type of algorithm clusters data based on an assumption that the underlying distribution is a mixture of Gaussians [Everitt and Hand, 1981, Nowlan, 1990]. <p> Many algorithms for unsupervised learning are based on this method, among them some learning rules for neural networks <ref> [Sanger, 1989, Oja, 1989] </ref>. The method is based on the covariance matrix, which measures pairwise correlations among input bits. The main assumption underlying the method is that the low dimension projections of the data that retain the largest amount of information are those projections that have the largest variance.
Reference: [Pearl, 1988] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: The idea is to introduce a set of so-called "hidden variables" that represent events that cannot be directly observed, but have influence on the observable variables. Variables of this type are popular in so-called "Connectionist" models, such as Boltzmann Machines [Peterson and Anderson, 1987] and Belief Networks <ref> [Pearl, 1988] </ref>. These popular models can be interpreted as models of distributions over the observable variables. However, the distributions defined in this way cannot be described by a reasonably simple closed form expression. <p> Newer and more exciting models include Bayes networks <ref> [Pearl, 1988] </ref> and Markov random fields [Pearl, 1988, Geman and Geman, 1984, Geman, 1986]. In the neural network area, both Hopfield nets [Hopfield, 1982] and Boltzmann machines [Ackley et al., 1985] can be used as models of probability distributions on f1g n for relatively large n. <p> Newer and more exciting models include Bayes networks [Pearl, 1988] and Markov random fields <ref> [Pearl, 1988, Geman and Geman, 1984, Geman, 1986] </ref>. In the neural network area, both Hopfield nets [Hopfield, 1982] and Boltzmann machines [Ackley et al., 1985] can be used as models of probability distributions on f1g n for relatively large n. <p> The computational complexity of 3 Recent work on modeling correlations by hidden units has also been done by Radford M. Neal [Neal, 1990]. In his work he gives a different variant of the Boltzmann Machine algorithm that uses distribution models similar to Judea Pearl's Bayes Networks <ref> [Pearl, 1988, Gefner and Pearl, 1987] </ref>. His model is superior to the Boltzmann Machine in the sense that the connection weights are interpreted as conditional probabilities, which is a more accessible interpretation than local energy interactions. The learning algorithms that Neal used are based on stochastic approximation. <p> The learning algorithms that Neal used are based on stochastic approximation. The question of whether a two-layer model of this type has universal representation capabilities is open. 4 Noisy-OR gates have been introduced in the framework of Bayes Networks to allow for such combinations <ref> [Pearl, 1988] </ref>. However, using this in networks with hidden units has not been studied, to the best of our knowledge. 83 the learning rule is exponential in the number of hidden units. However, under certain natural conditions we show that there exists a good approximation that requires only polynomial time.
Reference: [Peterson and Anderson, 1987] <author> Carsten Peterson and James R. Anderson. </author> <title> A mean field theory learning algorithm for neural networks. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 995-1019, </pages> <year> 1987. </year>
Reference-contexts: The idea is to introduce a set of so-called "hidden variables" that represent events that cannot be directly observed, but have influence on the observable variables. Variables of this type are popular in so-called "Connectionist" models, such as Boltzmann Machines <ref> [Peterson and Anderson, 1987] </ref> and Belief Networks [Pearl, 1988]. These popular models can be interpreted as models of distributions over the observable variables. However, the distributions defined in this way cannot be described by a reasonably simple closed form expression. <p> Many methods have been proposed to speed up learning in Boltzmann machines. One of these methods is the mean-field approximation <ref> [Peterson and Anderson, 1987] </ref>. <p> However, here the clamped term is easy to calculate, it requires summing a logistic type function over all training examples. 93 The same term is obtained by making the mean field approximation for the clamped phase in the general algorithm <ref> [Peterson and Anderson, 1987] </ref>, which is exact in this case.
Reference: [Poggio and Girosi, 1989] <author> Tomaso Poggio and Federico Girosi. </author> <title> A theory of networks for approximation and learning. </title> <type> Technical Report A.I. Memo No. 1140, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Methods that go under titles such as "neural networks" [Rumelhart and McClelland, 1986, Hertz et al., 1991], "hidden Markov chains" [Rabiner and Juang, 1986], and "radial basis functions" <ref> [Poggio and Girosi, 1989] </ref> are becoming increasingly popular. Such algorithms are used for tasks ranging from controlling the arms of a robot to playing backgammon and from predicting the price of commodities to recognizing human faces.
Reference: [Rabiner and Juang, 1986] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Today, as computers are getting faster, the trend is towards using non-linear and hierarchical models to describe very complex high dimensional data. Methods that go under titles such as "neural networks" [Rumelhart and McClelland, 1986, Hertz et al., 1991], "hidden Markov chains" <ref> [Rabiner and Juang, 1986] </ref>, and "radial basis functions" [Poggio and Girosi, 1989] are becoming increasingly popular. Such algorithms are used for tasks ranging from controlling the arms of a robot to playing backgammon and from predicting the price of commodities to recognizing human faces.
Reference: [Rissanen, 1986] <author> Jorma Rissanen. </author> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14(3) </volume> <pages> 1080-1100, </pages> <year> 1986. </year>
Reference-contexts: Various disciplines provide mathematical frameworks in which to analyze and compare such algorithms. Among these disciplines are pattern recognition [Duda and Hart, 1973a], estimation theory [Vapnik, 1982], the theory of stochastic modeling <ref> [Rissanen, 1986] </ref>, the theory of inductive inference [Gold, 1967, Angluin and Smith, 1983] and computational learning theory [Valiant, 1984a]. Computational learning theory is a part of theoretical computer science.
Reference: [Rumelhart and McClelland, 1986] <author> D. E. Rumelhart and J. L. McClelland. </author> <title> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </title> <booktitle> Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: Today, as computers are getting faster, the trend is towards using non-linear and hierarchical models to describe very complex high dimensional data. Methods that go under titles such as "neural networks" <ref> [Rumelhart and McClelland, 1986, Hertz et al., 1991] </ref>, "hidden Markov chains" [Rabiner and Juang, 1986], and "radial basis functions" [Poggio and Girosi, 1989] are becoming increasingly popular. <p> We call this model the influence combination machine, or, for short, the combination machine. We refer to the distribution that is defined on the binary vectors by the combination machine as the combination model. This type of Boltzmann machine was previously studied by Smolensky in his harmony theory <ref> [Rumelhart and McClelland, 1986] </ref>[Ch.6]. In his work he discusses several possible ways of using this type of model for solving problems in Artificial Intelligence. In our work we concentrate on the mathematical properties of the model and on efficient algorithms for learning the model from random instances. <p> For a given B , the energy of a state of the combination machine is defined as E (~x; ~ hj B ) = i=1 and the probability of a state is defined to be 5 In <ref> [Rumelhart and McClelland, 1986] </ref>[Ch.6], binary connection weights are used, here we use real-valued weights. 85 1 e E (~x; ~ hj B ) where Z B = X e E (~x; ~ hj B ) : We find it useful to define the "combined weight" of a particular state of the <p> More precisely, the mistakes it makes when used to predict the value of each single bit in each of the instances in the sample, when given the values of all the other bits of 9 The results are given using Hinton diagrams <ref> [Rumelhart and McClelland, 1986] </ref>, i.e. positive values are displayed as full rectangles, negative values as empty rectangles, and the area of the rectangle is proportional to the absolute value. 103 that instance. The combination model defines a probability for any possible instance.
Reference: [Sanger, 1989] <author> T.D. Sanger. </author> <title> Optimal unsupervised learning in a single-layer linear feedforward neural network. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 459-473, </pages> <year> 1989. </year>
Reference-contexts: The idea that features that are useful for classification can be deduced from the distribution of typical inputs is the basis of several existing algorithms for unsupervised learning. One type of algorithm selects projections of the input based on Principle Component analysis <ref> [Sanger, 1989, Oja, 1989] </ref>. Another type of algorithm clusters data based on an assumption that the underlying distribution is a mixture of Gaussians [Everitt and Hand, 1981, Nowlan, 1990]. <p> Many algorithms for unsupervised learning are based on this method, among them some learning rules for neural networks <ref> [Sanger, 1989, Oja, 1989] </ref>. The method is based on the covariance matrix, which measures pairwise correlations among input bits. The main assumption underlying the method is that the low dimension projections of the data that retain the largest amount of information are those projections that have the largest variance.
Reference: [Sauer, 1972] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory (Series A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: However, it is easily checked that it is also true for super-martingales. Reversing the sign of the Y i we get an equivalent theorem for sub-martingales. 63 Proof : From Sauer's Lemma <ref> [Sauer, 1972] </ref> we know that the number of different labelings created by m examples is at most P d m (em=d) d .
Reference: [Schapire, 1990] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-226, </pages> <year> 1990. </year>
Reference-contexts: In chapter 2 we present an algorithm that can increase the accuracy of distribution free algorithms. This work is an improvement over previous work by Schapire <ref> [Schapire, 1990] </ref>. One of the main outcomes of the analysis of this algorithm are improved upper bounds on the number of examples and on the computational resources required for learning in Valiant's model. <p> For example, one can ask for a learning algorithm that with a probability of 10% generates a hypothesis whose error is smaller than 25%. Such a requirement seems to be much less demanding than that of the standard, or strong, PAC learning model. However, a surprising result by Schapire <ref> [Schapire, 1990] </ref>, shows that the two models are essentially equivalent. <p> This seemed to indicate that weak and strong distribution-free learning should also be separated. However, Schapire <ref> [Schapire, 1990] </ref> proved that weak and strong PAC learning are equivalent in the distribution-free case. Schapire presented an algorithm that, given access to a weak learning algorithm, can generate hypotheses of arbitrary accuracy using time and space resources that are polynomial in 1=*. This algorithm is called the "boosting" algorithm. <p> The concept of a boosting algorithm was first presented by Schapire in <ref> [Schapire, 1990] </ref>. A boosting algorithm is a learning algorithm that uses as a subroutine a different learning algorithm. The goal of the boosting algorithm is to efficiently generate high-accuracy hypotheses using a learning algorithm that can efficiently generate only low-accuracy hypotheses. The boosting algorithm invented by Schapire [Schapire, 1990], was a <p> by Schapire in <ref> [Schapire, 1990] </ref>. A boosting algorithm is a learning algorithm that uses as a subroutine a different learning algorithm. The goal of the boosting algorithm is to efficiently generate high-accuracy hypotheses using a learning algorithm that can efficiently generate only low-accuracy hypotheses. The boosting algorithm invented by Schapire [Schapire, 1990], was a breakthrough in that it showed that any polynomial time learning algorithm that generates hypotheses whose error is just slightly smaller than 1=2 can be transformed into a polynomial time learning algorithm that generates hypotheses whose error is arbitrarily small. <p> In many cases this size is very large, moreover, often H is infinite or even uncountable. These cases can be analyzed using the notion of VC-dimension. However, Schapire <ref> [Schapire, 1990] </ref>, suggested the following elegant proof that is based only on the assumption that the size of the sample used by WeakLearn is uniformly bounded. <p> This avoids storing many examples in memory and decreases the space complexity to O (log (1=*)). Selecting examples directly out of the input stream is the basis of Schapire's boosting algorithm <ref> [Schapire, 1990] </ref>. Schapire coined the term "filtering" to describe this process. The selection is viewed as a "filter" that lies between the source of examples, EX, and the weak learning algorithm. <p> In other words, a weak learning algorithm produces a prediction rule that performs just slightly better than random guessing. Schapire <ref> [Schapire, 1990] </ref> has shown that the notions of weak and strong PAC learning are equivalent. Moreover, the boosting algorithm he invented provides an effective way for translating any weak learning algorithm into a strong learning algorithm. <p> Compare this theorem to Theorem 4 in <ref> [Schapire, 1990] </ref>. The statement there is that the dependence of the sample and time complexity on * is O (1=* poly (1=*)), and that that the other dependencies on 1=* are poly-logarithmic.
Reference: [Schapire, 1991] <author> Robert E. Schapire. </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <type> PhD thesis, </type> <institution> M.I.T., </institution> <year> 1991. </year>
Reference-contexts: To avoid this problem the required error has to be set to a smaller value, thus making the detection of a good hypothesis easier. We omit the details of this variant of the boosting algorithm. 2.4.2 Boosting multiple valued concepts As was noted by Schapire <ref> [Schapire, 1991] </ref>, the generalization of the equivalence between strong and weak learning to concepts with more than two labels does not enjoy the same tightness as the two label case. <p> In this case, using a random coin flip to choose one of the two possible labels will give a correct answer half of the time, but the concept class might still be unlearnable <ref> [Schapire, 1991] </ref>. 46 incorrect hypothesis whose output is not equal to the incorrect label with the largest number of votes.
Reference: [Schapire, 1992] <author> Robert E. Schapire. </author> <title> private correspondence, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: The divergence between the distributions of the instances can be measured by the Kullback-Leibler measure of divergence. Schapire <ref> [Schapire, 1992] </ref>, noted that the results presented in this paper can be used to show an interesting relationship between representation and approximation using majority gates. These results 13 were independently discovered by Hastad et. al. [Goldmann et al., 1992]. <p> While it needs (1=*) examples to generate a hypothesis that has accuracy *, only O (log 1=*) of them are passed to the weak learners. Two interesting implications arise from this fact. The first implication was pointed out to us by Schapire <ref> [Schapire, 1992] </ref>. <p> This application of boosting has been discovered by Schapire <ref> [Schapire, 1992] </ref>. A slightly weaker version of this result was independently proven by Goldmann, Hastad, and Razborov [Goldmann et al., 1992] using a completely different proof technique. In the following presentation we follow their notation.
Reference: [Serfling, 1980] <author> R. J. Serfling. </author> <title> Approximation Theorems of Mathematical Statistics. </title> <publisher> John Wiley & Sons, </publisher> <year> 1980. </year>
Reference: [Seung et al., 1992] <author> H.S Seung, M. Opper, and H. Sompolinsky. </author> <title> Query by committee. </title> <booktitle> In Proceedings of the Fifth Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In particular, we show that the number of labeled examples required for learning the Perceptron concept class 1 can be drastically reduced using a simple method for query selection proposed by Seung et. al. <ref> [Seung et al., 1992] </ref>. In the frameworks described above, the learner's main source of information are the labels of the instances. The instances can usually be considered to encode a "state of nature", while the labels can be seen as given by a knowledgeable "teacher". <p> We study one particular algorithm for learning in this framework, that was presented by Seung et. al. <ref> [Seung et al., 1992] </ref>, called "Query-by-Committee". The algorithm uses a "committee" of learners, that is to say, a set of independent learning algorithms, each of which generates a hypothesis that is consistent 8 with the answers to all the queries asked so far. <p> In the next section we present such a method. 3.4 The Query by Committee learning algorithm Seung, Opper and Sompolinsky <ref> [Seung et al., 1992] </ref> have devised an algorithm for learning by queries which they called "Query by Committee" and we shall refer to as the QBC algorithm. <p> If the two predictions differ, it calls Label with input x, and adds the labeled example to the set of labeled examples that define the version space. It then proceeds to the next iteration. In <ref> [Seung et al., 1992] </ref> Seung et. al. treat the query by committee algorithm as an on-line learning algorithm, and analyze the rate at which the error of the two Gibbs learners reduces as a function of the number of queries made. <p> This establishes, by rigorous mathematical analysis, results that were found By Seung et. al. in <ref> [Seung et al., 1992] </ref> using methods from statistical mechanics, which are only partially formalized and which consider only the high-dimensional limit of the problem. 79 We have proved that, in general, if the queries that are selected by the query by committee algorithm have high expected information gain then the prediction
Reference: [Shamir, 1992] <author> E. Shamir. </author> <title> private correspondence, </title> <year> 1992. </year>
Reference-contexts: The second implication was found jointly with Eli Shamir <ref> [Shamir, 1992] </ref>. We observed that if training examples can be accumulated in parallel by several parallel processors, then our methods can translate any PAC learning algorithm to a version that runs in time O (log 1=*) on a parallel computer with fi (1=*) processors.
Reference: [Smith, 1985] <author> Peter Smith. </author> <title> Convexity Methods in Variational Calculus. </title> <publisher> Research studies press, John Wiley & sons, </publisher> <year> 1985. </year>
Reference-contexts: In terms of the volume function F , for t &gt; 0, 8 Details on how the Frechet derivative is defined and calculated can be found in standard books on variational analysis, such as <ref> [Smith, 1985] </ref>. 68 F increases when K decreases and vice versa.
Reference: [Turan, 1993] <author> G. Turan. </author> <title> Lower bounds for pac learning with queries. </title> <booktitle> In Proceedings of the sixth Workshop on Computational Learning Theory, </booktitle> <pages> pages 384-391, </pages> <year> 1993. </year>
Reference-contexts: Previous work by Eisenberg and Rivest has shown that, in the PAC learning model, no significant reduction of this type is possible for a natural set of concept classes. These results have been strengthened by Turan <ref> [Turan, 1993] </ref>. In chapter 3 we show that if we allow the learner access to random unlabeled instances, this problem can sometimes be alleviated.
Reference: [Valiant, 1984a] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-42, </pages> <year> 1984. </year>
Reference-contexts: Various disciplines provide mathematical frameworks in which to analyze and compare such algorithms. Among these disciplines are pattern recognition [Duda and Hart, 1973a], estimation theory [Vapnik, 1982], the theory of stochastic modeling [Rissanen, 1986], the theory of inductive inference [Gold, 1967, Angluin and Smith, 1983] and computational learning theory <ref> [Valiant, 1984a] </ref>. Computational learning theory is a part of theoretical computer science. For this reason its natural emphasis is on learning questions that arise in computation theory, such as learning finite automata and Boolean formulas. Emphasis is also put on bounding the computational resources that are required for learning. <p> One of the most popular mathematical frameworks of approximate concept learning is the model introduced by Valiant in <ref> [Valiant, 1984a] </ref>, also called the distribution free or PAC (probably approximately correct) learning model. In this framework the labeled instances that are given to the learner as examples are chosen independently at random from the instance space. <p> One of the main results of this paper is an upper bound on the resources required for learning in the distribution-free model of learnability introduced by Valiant <ref> [Valiant, 1984a] </ref>. In Valiant's model, commonly referred to as the PAC (Probably Approximately Correct) learning model, or the distribution-free learning model, the quality of a learning algorithm is defined as follows. <p> However, most learning algorithms can be used for 39 a family of concept classes, and one is then interested in the way the performance of the learning algorithm depends on the complexity of the concept class. Valiant <ref> [Valiant, 1984a] </ref> presented a framework, called the PAC 12 learning framework, in which such quantification can be done. This framework is one of the most well studied frameworks in computational learning theory. In this section we show the implications of our work on the PAC learning framework.
Reference: [Valiant, 1984b] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Comm. ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: In this paradigm the learner is passive and has no control over the information that it receives. In contrast, in the query paradigm, the learner is given the power to ask questions. What does the learner gain from this additional power? Study of the use of queries in learning <ref> [Valiant, 1984b, Angluin, 1988a] </ref>, has mostly concentrated on algorithms for exact identification of the target concept. This type of analysis concentrates on the worst case behavior of the algorithm, and no probabilistic assumptions are made.
Reference: [Vapnik, 1982] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year> <month> 123 </month>
Reference-contexts: Various disciplines provide mathematical frameworks in which to analyze and compare such algorithms. Among these disciplines are pattern recognition [Duda and Hart, 1973a], estimation theory <ref> [Vapnik, 1982] </ref>, the theory of stochastic modeling [Rissanen, 1986], the theory of inductive inference [Gold, 1967, Angluin and Smith, 1983] and computational learning theory [Valiant, 1984a]. Computational learning theory is a part of theoretical computer science.
References-found: 82

