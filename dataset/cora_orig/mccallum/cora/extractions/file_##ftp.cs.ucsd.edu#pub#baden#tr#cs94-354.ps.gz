URL: file://ftp.cs.ucsd.edu/pub/baden/tr/cs94-354.ps.gz
Refering-URL: http://www.cs.ucsd.edu/groups/hpcl/scg/tr.html
Root-URL: http://www.cs.ucsd.edu
Note: To appear in the 1994 Scalable High Performance Computing Conference, May 23-25, 1994, Knoxvilee, TN, U.S.A.  
Address: La Jolla, California 92093-0114  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Abstract: A Robust Parallel Programming Model for Dynamic Non-Uniform Scientific Computations Scott R. Kohn and Scott B. Baden CSE Technical Report Number CS94-354 March 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> High Performance Fortran Forum, Rice University, Houston, Texas, High Performance Fortran Language Specification, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The recent development of the High Performance Fortran (HPF) <ref> [1] </ref> standard has given hope to the scientific computing community that the power of parallel computers may soon be within grasp.
Reference: [2] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> "Fortran d language specification," </title> <type> Tech. Rep. </type> <institution> TR90-141, Dept. of Computer Science, Rice University, Houston, TX, </institution> <month> De-cember </month> <year> 1989. </year>
Reference-contexts: 1 Introduction The recent development of the High Performance Fortran (HPF) [1] standard has given hope to the scientific computing community that the power of parallel computers may soon be within grasp. The initial work on HPF, based on contributions from a number of Fortran dialects such as Fortran-D <ref> [2] </ref>, Connection Machine Fortran [3], and Vienna Fortran [4], has primarily focused on compile-time optimizations for regular, static applications. An important class of scientific calculations employ dynamic, irregular, block fl This work was supported by NSF contract ASC-9110793 and ONR contract N00014-93-1-0152.
Reference: [3] <institution> Thinking Machines Corporation, Inc., </institution> <address> Cambridge, MA, </address> <note> Getting Started in CM Fortran, </note> <month> January </month> <year> 1991. </year>
Reference-contexts: The initial work on HPF, based on contributions from a number of Fortran dialects such as Fortran-D [2], Connection Machine Fortran <ref> [3] </ref>, and Vienna Fortran [4], has primarily focused on compile-time optimizations for regular, static applications. An important class of scientific calculations employ dynamic, irregular, block fl This work was supported by NSF contract ASC-9110793 and ONR contract N00014-93-1-0152.
Reference: [4] <author> B. Chapman, P. Mehrotra, H. Moritsch, and H. Zima, </author> <title> "Dynamic data distribution in vienna fortran," </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: The initial work on HPF, based on contributions from a number of Fortran dialects such as Fortran-D [2], Connection Machine Fortran [3], and Vienna Fortran <ref> [4] </ref>, has primarily focused on compile-time optimizations for regular, static applications. An important class of scientific calculations employ dynamic, irregular, block fl This work was supported by NSF contract ASC-9110793 and ONR contract N00014-93-1-0152. Paragon and Cray C-90 time was provided by a UCSD School of Engineering Block Grant.
Reference: [5] <author> A. Brandt, </author> <title> "Multi-level adaptive solutions to boundary-value problems," </title> <journal> Mathematics of Computation, </journal> <volume> vol. 31, </volume> <pages> pp. 333-390, </pages> <month> April </month> <year> 1977. </year>
Reference-contexts: CM-5 time was provided by the Computer Science Division of the University of California, Berkeley under NSF Infrastructure Grant CDA-8722788 and also by the Advanced Computing Laboratory of Los Alamos National Laboratory. oriented data structures. Such data structures arise in adaptive multilevel finite difference methods, such as adaptive multigrid <ref> [5] </ref> and adaptive mesh refinement for computational fluid dynamics [6]. They also occur in applications with non-uniform workload distributions, e.g. particle methods, which require irregular data decompositions on parallel machines to balance workloads. These applications often exhibit rapidly changing data dependencies and communication patterns. <p> We did not attempt to use the vector units on the CM-5 because of the difficulty of vectorizing the numerical kernels by hand. 4.3 Adaptive Multigrid Solver We have implemented 2d and 3d adaptive multigrid solvers based on Brandt's FAS algorithm <ref> [5] </ref>. Adaptive mesh applications are difficult to parallelize because error estimation, regridding [21], data decomposition, assignment of work to processors, and calculation of data dependencies must be made at run-time. smoothed point charges arranged in a ring.
Reference: [6] <author> M. J. Berger and P. Colella, </author> <title> "Local adaptive mesh refinement for shock hydrodynamics," </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 82, </volume> <pages> pp. 64-84, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Such data structures arise in adaptive multilevel finite difference methods, such as adaptive multigrid [5] and adaptive mesh refinement for computational fluid dynamics <ref> [6] </ref>. They also occur in applications with non-uniform workload distributions, e.g. particle methods, which require irregular data decompositions on parallel machines to balance workloads. These applications often exhibit rapidly changing data dependencies and communication patterns. Little compile-time information is available for such applications; hence, they require elaborate run-time support.
Reference: [7] <author> H. Berryman, J. Saltz, and J. Scroggs, </author> <title> "Execution time support for adaptive scientific algorithms on distributed memory machines," </title> <type> Tech. Rep. 90-41, </type> <institution> ICASE, NASA Langley Research Center, Hampton, Virginia, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: They also occur in applications with non-uniform workload distributions, e.g. particle methods, which require irregular data decompositions on parallel machines to balance workloads. These applications often exhibit rapidly changing data dependencies and communication patterns. Little compile-time information is available for such applications; hence, they require elaborate run-time support. PARTI <ref> [7] </ref> and Multiblock PARTI [8] provide run-time support for unstructured and block-structured applications; however, neither directly support irregular block decompositions. LPARX provides run-time support for dynamic, non-uniform scientific calculations. LPARX's abstractions extend HPF's data decomposition model to include dynamic, block irregular data structures.
Reference: [8] <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> "Compiler and runtime support for structured and block structured applications," </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: These applications often exhibit rapidly changing data dependencies and communication patterns. Little compile-time information is available for such applications; hence, they require elaborate run-time support. PARTI [7] and Multiblock PARTI <ref> [8] </ref> provide run-time support for unstructured and block-structured applications; however, neither directly support irregular block decompositions. LPARX provides run-time support for dynamic, non-uniform scientific calculations. LPARX's abstractions extend HPF's data decomposition model to include dynamic, block irregular data structures.
Reference: [9] <author> J. J. Monaghan, </author> <title> "Smoothed particle hydrodynamics," </title> <booktitle> Annual Review of Astronomy and Astrophysics, </booktitle> <volume> vol. 30, </volume> <pages> pp. 543-574, </pages> <year> 1992. </year>
Reference-contexts: Finally, LPARX should provide support for arrays of complicated objects in addition to standard built-in types such as floating point numbers and integers. Modern scientific codes often use elaborate data structures which require special treatment when communicated across address spaces. We have developed a 3d smoothed particle hydro-dynamics code <ref> [9] </ref> and 2d and 3d adaptive multigrid solvers in LPARX; these codes could not have been efficiently implemented using HPF as it does not currently support block irregular data structures. Because LPARX hides architecture-dependent details, applications are portable across a wide range of parallel computers.
Reference: [10] <author> V. S. Sunderam, </author> <title> "Pvm: A framework for parallel distributed computing," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 2, </volume> <pages> pp. 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: LPARX, implemented as a C ++ class library, requires only basic message passing support and is currently running on the CM-5, Paragon, iPSC/860, nCUBE/2, KSR-1, single processor workstations, Cray C-90 (single processor), and networks of workstations under PVM <ref> [10] </ref>. LPARX application development and debugging is generally performed on workstations, which provide a more mature programming environment than most multiprocessors. Applications are then easily moved to the parallel machines for production runs. This paper is organized as follows.
Reference: [11] <author> S. B. Baden and S. R. Kohn, </author> <title> "A comparison of load balancing strategies for particle methods running on mimd multiprocessors," </title> <booktitle> in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> (Houston, Texas), </address> <month> March </month> <year> 1991. </year>
Reference-contexts: Of the three methods, cyclic and pointwise array decompositions are appropriate for irregular compu tations. Uniform block decompositions are ineffective in balancing non-uniform workloads. While cyclic decompositions can usually balance non-uniform work-loads, they disrupt the locality found in many numerical methods, resulting in increased interprocessor communication costs <ref> [11] </ref>. Maintaining data locality is a fundamental goal on both message passing and distributed shared memory machines. Communication on message passing architectures is generally expensive relative to computation, and distributed shared memory penalizes non-local (non-cache) memory accesses.
Reference: [12] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis, </author> <title> "Distributed memory compiler methods for irregular problems | data copy reuse and runtime partitioning," </title> <type> Tech. Rep. 91-73, </type> <institution> ICASE, Hampton, VA, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Applications incur considerable run-time overhead in the computation of communication schedules, which describe the interprocessor communication necessary to satisfy data dependencies. While the cost of generating a communication schedule may be amortized by reusing the schedule <ref> [12] </ref>, this is often not possible for dynamic applications with rapidly changing data dependencies. LPARX supports a different style of data decomposition: block-structured, irregular decompositions. Such data distributions provide more freedom in balancing workload than either block or cyclic decompositions.
Reference: [13] <author> M. J. Berger and S. H. Bokhari, </author> <title> "A partitioning strategy for nonuniform problems on multiprocessors," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 570-580, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: LPARX supports a different style of data decomposition: block-structured, irregular decompositions. Such data distributions provide more freedom in balancing workload than either block or cyclic decompositions. For example, many load balancing algorithms, such as recursive bisection <ref> [13] </ref>, render irregular block decompositions. Unlike cyclic decompositions, irregular block decompositions preserve locality because they are still block structured. Mapping information is only needed for each block, instead of each array point as in pointwise decompositions.
Reference: [14] <author> F. B. Irisa, P. Beckman, D. Gannon, S. Yang, S. Ke-savan, A. Malony, and B. Mohr, </author> <title> "Implementing a parallel c++ runtime system for scalable parallel systems," </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <month> Novem-ber </month> <year> 1993. </year>
Reference-contexts: In LPARX, decompositions are first-class language objects. They may be allocated at run-time, passed to procedures, and manipulated using LPARX's domain calculus, described in Section 3.3. The pC++ language <ref> [14] </ref> also supports distributions and decompositions as first-class language objects; however, in pC++, the application has limited ability in specifying irregular mappings. 3 LPARX Model Description The LPARX model is based on the FIDIL programming language [15] and on our previous work with LPAR [16].
Reference: [15] <author> P. N. Hilfinger and P. Colella, "Fidil: </author> <title> A language for scientific programming," </title> <type> Tech. Rep. </type> <institution> UCRL-98057, Lawrence Livermore National Laboratory, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: The pC++ language [14] also supports distributions and decompositions as first-class language objects; however, in pC++, the application has limited ability in specifying irregular mappings. 3 LPARX Model Description The LPARX model is based on the FIDIL programming language <ref> [15] </ref> and on our previous work with LPAR [16]. LPARX provides a uniform framework for manipulating irregular block-structured data. It separates the expression of data decomposition and communication from numerical computation. <p> In the execution of the forall loop, we assume that the Grids are decoupled: Grids are processed independently and the computation proceeds asynchronously. 3.3 Domain Calculus LPARX defines a domain calculus <ref> [15] </ref> which enables the programmer to manipulate index sets (Regions) in geometric terms. The domain calculus represents data dependencies in a manner which is logically independent of data decomposition and problem dimension.
Reference: [16] <author> S. R. Kohn and S. B. Baden, </author> <title> "An implementation of the lpar parallel programming model for scientific computations," </title> <booktitle> in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: The pC++ language [14] also supports distributions and decompositions as first-class language objects; however, in pC++, the application has limited ability in specifying irregular mappings. 3 LPARX Model Description The LPARX model is based on the FIDIL programming language [15] and on our previous work with LPAR <ref> [16] </ref>. LPARX provides a uniform framework for manipulating irregular block-structured data. It separates the expression of data decomposition and communication from numerical computation. The exact method of data decomposition is external to LPARX, which gives LPARX the generality to address a number of different types of applications.
Reference: [17] <author> R. W. Hockney and J. W. Eastwood, </author> <title> Computer Simulation Using Particles. </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: For example, in addition to representing a finite difference mesh of floating point numbers, the Grid may also be used to implement the spatial data structures <ref> [17] </ref> common in particle calculations. LPARX does not decompose Grids across processors. Instead, LPARX specifies data decomposition using the XArray, an array of Grids which represents the high-level structure of a parallel computation. The Grids in an XArray may have different origins, sizes, and index sets.
Reference: [18] <author> M. J. Berger and J. Oliger, </author> <title> "Adaptive mesh refinement for hyperbolic partial differential equations," </title> <journal> Journal of Computational Physics, </journal> <volume> vol. 53, </volume> <pages> pp. 484-512, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: Another form, A &lt;<= B on C, where C is a Region, limits the copy to the index space in which A, B, and C intersect. 3.4 Sample LPARX Code for the computation of a single level of refinement in an adaptive mesh refinement (AMR) application <ref> [18] </ref>. AMR applications place localized refinement patches over areas of high numerical error, such as over the shock wave in Figure 5. While this example is presented using pseudo-code, the equivalent LPARX C ++ code would be similar.
Reference: [19] <author> W. Hart and R. Belew, </author> <title> "A mimd design for geographically structured genetic algorithms." </title> <note> (in preparation), </note> <year> 1993. </year>
Reference-contexts: This particular style exploits the respective strengths of both C ++ and Fortran. Furthermore, programmers are not required to rewrite highly optimized Fortran kernels when paral-lelizing applications. LPARX has been used to parallelize a number of applications, including a genetics algorithm code <ref> [19] </ref>, a 3d particle simulation, 2d and 3d multigrid solvers, and 2d and 3d adaptive multigrid solvers. In the following sections, we provide a brief analysis of LPARX overheads and computational results for a 3d particle simulation and a 3d adaptive multigrid solver.
Reference: [20] <author> S. R. Kohn and S. B. Baden, </author> <title> "Object-oriented parallel programming with distributed parallel objects." </title> <note> To be submitted to Supercomputing '94, </note> <month> April </month> <year> 1994. </year>
Reference-contexts: Software versions and compiler options for all benchmarks are reported in Table 1. Further details about the LPARX software structure will be presented in an upcoming paper <ref> [20] </ref>. 4.1 LPARX Overheads To provide an estimate of LPARX overhead as compared to hand-coding, we implemented a 3d Jacobi iterative solver with a 19 point stencil in LPARX and by hand using message passing.
Reference: [21] <author> M. J. Berger and I. Rigoutsos, </author> <title> "An algorithm for point clustering and grid generation," </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> vol. 21, no. 5, </volume> <pages> pp. 1278-1286, </pages> <year> 1991. </year>
Reference-contexts: Adaptive mesh applications are difficult to parallelize because error estimation, regridding <ref> [21] </ref>, data decomposition, assignment of work to processors, and calculation of data dependencies must be made at run-time. smoothed point charges arranged in a ring. Grid refinements were placed over the areas where the solution was changing most rapidly, i.e. surrounding the point charges.
Reference: [22] <author> W. Y. Crutchfield and M. L. </author> <title> Welcome, "Object oriented implementation of adaptive mesh refinement algorithms," </title> <type> Tech. Rep. </type> <institution> UCRL-JC-113502, Lawrence Livermore National Laboratory, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: We are collaborating in the development of LPARX-based adaptive mesh methods for the first principles simulations of real materials. LPARX has been adopted by collaborators at Lawrence Livermore National Laboratory for the parallelization of 2d and 3d gas dynamics calculations <ref> [22] </ref>. We are investigating LPARX as an intermediate form for compilation and plan to extend the Region abstraction to unstructured problems such as sweeps over finite element meshes.
References-found: 22

