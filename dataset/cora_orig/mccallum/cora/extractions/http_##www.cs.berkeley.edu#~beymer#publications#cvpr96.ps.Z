URL: http://www.cs.berkeley.edu/~beymer/publications/cvpr96.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~beymer/publications.html
Root-URL: 
Email: email: beymer@ai.mit.edu  
Title: Feature Correspondence by Interleaving Shape and Texture Computations  
Author: David Beymer 
Address: Cambridge, MA 02139, USA  
Affiliation: Artificial Intelligence Laboratory, and Center for Biological and Computational Learning Massachusetts Institute of Technology  
Abstract: The correspondence problem in computer vision is basically a matching task between two or more sets of features. In this paper, we introduce a vectorized image representation, which is a feature-based representation where correspondence has been established with respect to a reference image. The representation consists of two image measurements made at the feature points: shape and texture. Feature geometry, or shape, is represented using the (x; y) locations of features relative to the some standard reference shape. Image grey levels, or texture, are represented by mapping image grey levels onto the standard reference shape. Computing this representation is essentially a correspondence task, and in this paper we explore an automatic technique for "vectorizing" face images. Our face vectorizer alternates back and forth between computation steps for shape and texture, and a key idea is to structure the two computations so that each one uses the output of the other. In addition to describing the vectorizer, an application to the problem of facial feature detection will be presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adam Baumberg and David Hogg. </author> <title> Learning flexible models from image sequences. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <pages> pages 299-308, </pages> <address> Stockholm, Sweden, </address> <year> 1994. </year>
Reference-contexts: This technique for modeling shape is similar to the work of Cootes, et al. [6], Baumberg and Hogg <ref> [1] </ref>, and Jones and Poggio [8]. 6.3 Multiple poses The straightforward way to handle different out-of-plane image rotations with the vectorizer is simply to use several vectorizers, each tuned to a different pose.
Reference: [2] <author> Thaddeus Beier and Shawn Neely. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In SIGGRAPH '92 Proceedings, </booktitle> <pages> pages 35-42, </pages> <address> Chicago, IL, </address> <year> 1992. </year>
Reference-contexts: New shapes (a) (b) shape. Part (a) shows some example segments manually placed on a prototype face, and (b) shows the features averaged over 14 prototypes, providing our definition for standard shape. After Beier and Neely <ref> [2] </ref>. are then written as linear combinations of these eigen-shapes. Texture modeling in their approach, however, is weaker than in ours. Texture is only modeled locally along 1D contours at each of the feature points defining shape. <p> First, we manually define the shape of the example faces by positioning a set of example images. of line segment features for each example. The features, shown in Fig. 1 (a), follow Beier and Neely's <ref> [2] </ref> manual correspondence technique for morphing face images. Next, our definition of standard face shape is created by averaging the line segments over the example faces (see Fig. 1 (b)). Finally, images are geometrically normalized using the local deformation technique of Beier and Neely [2]. <p> 1 (a), follow Beier and Neely's <ref> [2] </ref> manual correspondence technique for morphing face images. Next, our definition of standard face shape is created by averaging the line segments over the example faces (see Fig. 1 (b)). Finally, images are geometrically normalized using the local deformation technique of Beier and Neely [2]. This deformation technique is driven by the pairing of line segments in the example image with line segments in the standard shape. Consider a single pairing of line segments, one segment from the example image l ex and one from the standard shape l std .
Reference: [3] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1431, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: The main idea is to interpolate among the b t a images of the different vectorizers to produce a new image that reconstructs both the grey levels and the pose of the input image (see Beymer, Shashua and Poggio <ref> [3] </ref> for examples of interpolation across different poses). 7 Conclusion In this paper, we first introduced a vectorized image representation, a feature-based representation where correspondence has been established with respect to a reference image. Two image measurements are made at the feature points.
Reference: [4] <author> David Beymer. </author> <title> Vectorizing face images by interleaving shape and texture computations. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1537, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: That is, we approximate t a by a linear combination of the n original image textures t p i b t a = i=1 fi i t p i : (4) Using this basis set requires computing the pseudoin verse of T off-line; see Beymer <ref> [4] </ref> for the details. 4.2 Run-time vectorization In this section we go over the details of the vectoriza-tion procedure. The inputs to the vectorizer are an image i a to vectorize and a texture model consisting of N eigenimages e i and mean image t mean . <p> In this section we describe experimental results from using these correspondences to locate facial features. Beymer <ref> [4] </ref> also discusses using the correspondences to register the features on two arbitrary faces at the same pose. The shape component y std astd can be sampled to locate a set of feature points in input i a . <p> The distances between the manual and computed segments were thresholded to evaluate the closeness of fit. A feature will be considered properly detected when all of its constituent segments are within a threshold. As explained more in Beymer <ref> [4] </ref>, we used a distance threshold of 10% of the interocular distance and an angle threshold of 20 ffi . The resulting detection rates are shown in Table 1.
Reference: [5] <author> Peter J. Burt and Edward H. Adelson. </author> <title> The laplacian pyramid as a compact image code. </title> <journal> IEEE Trans. on Communications, </journal> <volume> COM-31(4):532-540, </volume> <month> April </month> <year> 1983. </year>
Reference-contexts: No images of this person were used among the examples used to create the eigenspaces. We have implemented the vectorizer in C on an SGI Indy R4600 based machine using a hierarchical coarse-to-fine approach. First, the Gaussian pyramid (Burt and Adelson <ref> [5] </ref>) is computed to provide a multiresolution representation over 4 scales. Next, at the coarsest scale - where the interocular distance is only around 8 pixels a face detector locates the face. The face detector is based on correlation with two whole-face templates.
Reference: [6] <author> T.F. Cootes, C.J. Taylor, A. Lanitis, D.H. Cooper, and J. Graham. </author> <title> Building and using flexible models incorporating grey-level information. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 242-246, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This absolute representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [12]), the linear combinations approach to recognition (Ullman and Basri [16]), active shape models (Cootes, et al. <ref> [6] </ref>) and face recognition (Craw and Cameron [7]). A relative shape measured with respect to a standard reference shape y std is simply the difference y a y std ; which we denote using the shorthand notation y astd . <p> This technique for modeling shape is similar to the work of Cootes, et al. <ref> [6] </ref>, Baumberg and Hogg [1], and Jones and Poggio [8]. 6.3 Multiple poses The straightforward way to handle different out-of-plane image rotations with the vectorizer is simply to use several vectorizers, each tuned to a different pose.
Reference: [7] <author> Ian Craw and Peter Cameron. </author> <title> Parameterizing images for recognition and reconstruction. </title> <booktitle> In Proc. British Machine Vision Conference, </booktitle> <pages> pages 367-370, </pages> <year> 1991. </year>
Reference-contexts: This absolute representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [12]), the linear combinations approach to recognition (Ullman and Basri [16]), active shape models (Cootes, et al. [6]) and face recognition (Craw and Cameron <ref> [7] </ref>). A relative shape measured with respect to a standard reference shape y std is simply the difference y a y std ; which we denote using the shorthand notation y astd . <p> That is, the geometrical differences among face images are factored out by warping the images to the standard reference shape. This strategy for representing texture has been used, for example, in the face recognition works of Craw and Cameron <ref> [7] </ref>, and Shackleton and Welsh [13]. <p> One side of the coupling using shape to geometrically normalize images for textural analysis has already been discussed by Craw and Cameron <ref> [7] </ref>, and Shackleton and Welsh [13]. As mentioned for the texture vector t, the basic point is to factor out shape by warping images to a common geometry. Establishing correspondence this way justifies speaking of images as vectors and forming vector spaces out of them (e.g. linear combination of eigenimages).
Reference: [8] <author> Michael J. Jones and Tomaso Poggio. </author> <title> Model-based matching of line drawings by linear combinations of prototypes. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 531-536, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: This technique for modeling shape is similar to the work of Cootes, et al. [6], Baumberg and Hogg [1], and Jones and Poggio <ref> [8] </ref>. 6.3 Multiple poses The straightforward way to handle different out-of-plane image rotations with the vectorizer is simply to use several vectorizers, each tuned to a different pose.
Reference: [9] <author> A. Lanitis, C.J. Taylor, and T.F. Cootes. </author> <title> A unified approach to coding and interpreting face images. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 368-373, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference: [10] <author> Baback Moghaddam and Alex Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 786-793, </pages> <address> Cam-bridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: To im prove face detection, we would incorporate the "face ness" metrics in the learning approaches of Sung and Poggio [14] or Moghaddam and Pentland <ref> [10] </ref>. 6.2 Parameterized shape model In the current vectorizer, shape is measured in a "data-driven" manner using optical flow.
Reference: [11] <author> Alex Pentland, Baback Moghaddam, and Thad Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 84-91, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Going one step further, in this paper we use a statistical model for facial texture in order to assist the correspondence process. Our texture model relies on the assumption, commonly made in the eigenface approach to face recognition and detection (Turk and Pentland [15], Pentland, et al. <ref> [11] </ref>), that the space of grey level images of faces is linearly spanned by a set of example views.
Reference: [12] <author> T. Poggio and S. Edelman. </author> <title> A network that learns to recognize three-dimensional objects. </title> <journal> Nature, </journal> <volume> 343(6255) </volume> <pages> 263-266, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This absolute representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman <ref> [12] </ref>), the linear combinations approach to recognition (Ullman and Basri [16]), active shape models (Cootes, et al. [6]) and face recognition (Craw and Cameron [7]).
Reference: [13] <author> M.A. Shackleton and W.J. Welsh. </author> <title> Classification of facial features for recognition. </title> <booktitle> In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 573-579, </pages> <address> Lahaina, Maui, Hawaii, </address> <year> 1991. </year>
Reference-contexts: That is, the geometrical differences among face images are factored out by warping the images to the standard reference shape. This strategy for representing texture has been used, for example, in the face recognition works of Craw and Cameron [7], and Shackleton and Welsh <ref> [13] </ref>. If we let shape y std be the reference shape, then the geometrically normalized image t a is given by the 2D warp t a (x) = i a (x + y std astd (x)); where x = (x; y) is a 2D pixel location in standard shape. <p> One side of the coupling using shape to geometrically normalize images for textural analysis has already been discussed by Craw and Cameron [7], and Shackleton and Welsh <ref> [13] </ref>. As mentioned for the texture vector t, the basic point is to factor out shape by warping images to a common geometry. Establishing correspondence this way justifies speaking of images as vectors and forming vector spaces out of them (e.g. linear combination of eigenimages).
Reference: [14] <author> Kah-Kay Sung and Tomaso Poggio. </author> <title> Example-based learning for view-based human face detection. </title> <booktitle> In Proceedings Image Understanding Workshop, </booktitle> <volume> volume II, </volume> <pages> pages 843-850, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In order to demonstrate the vectorizer working in cluttered environments, both the face detection and vectorizer should be made more robust to the presense of false positive matches. To im prove face detection, we would incorporate the "face ness" metrics in the learning approaches of Sung and Poggio <ref> [14] </ref> or Moghaddam and Pentland [10]. 6.2 Parameterized shape model In the current vectorizer, shape is measured in a "data-driven" manner using optical flow.
Reference: [15] <author> Matthew Turk and Alex Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: The two primary components of the vectorized representation are shape and texture. Previous approaches in analyzing faces have stressed either one component or the other, such as feature localization or decomposing texture as a linear combination of eigenfaces (see Turk and Pentland <ref> [15] </ref>). The key aspect of our vectorization algorithm, or "vectorizer", is that the two processes for the analysis of shape and texture are coupled. That is, the shape and texture processes are coupled by making each process use the output of the other. <p> Going one step further, in this paper we use a statistical model for facial texture in order to assist the correspondence process. Our texture model relies on the assumption, commonly made in the eigenface approach to face recognition and detection (Turk and Pentland <ref> [15] </ref>, Pentland, et al. [11]), that the space of grey level images of faces is linearly spanned by a set of example views.
Reference: [16] <author> Shimon Ullman and Ronen Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference-contexts: This absolute representation for 2D shape has been widely used, including network-based object recognition (Poggio and Edelman [12]), the linear combinations approach to recognition (Ullman and Basri <ref> [16] </ref>), active shape models (Cootes, et al. [6]) and face recognition (Craw and Cameron [7]). A relative shape measured with respect to a standard reference shape y std is simply the difference y a y std ; which we denote using the shorthand notation y astd .
References-found: 16

