URL: ftp://cns.brown.edu/nin/papers/epp-ppr.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: nin@cns.brown.edu  
Phone: 1843,  
Title: Combining Exploratory Projection Pursuit And Projection Pursuit Regression With Application To Neural Networks  
Author: Nathan Intrator yz 
Date: Revised September, 1992  
Address: Box  Providence, RI 02912  
Affiliation: Institute for Brain and Neural Systems,  Brown University  
Abstract: We present a novel classification and regression method that combines exploratory projection pursuit (unsupervised training) with projection pursuit regression (supervised training), to yield a new family of cost/complexity penalty terms. Some improved generalization properties are demonstrated on real world problems.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barron, A. R. </author> <year> (1989). </year> <title> Statistical properties of artificial neural networks. </title> <booktitle> In Proc. IEEE Conf. on Decision and Control, </booktitle> <pages> pages 280-285. </pages> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference: <author> Barron, A. R. and Barron, R. L. </author> <year> (1988). </year> <title> Statistical learning networks: A unifying view. </title> <editor> In Wegman, E., editor, </editor> <booktitle> Computing Science and Statistics: Proc. 20th Symp. Interface, </booktitle> <pages> pages 192-203. </pages> <publisher> American Statistical Association, </publisher> <address> Washington, DC. </address>
Reference: <author> Bichsel, M. and Seitz, P. </author> <year> (1989). </year> <title> Minimum class entropy: A maximum information approach to layered netowrks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 133-141. </pages>
Reference-contexts: Combining EPP and PPR 6 become @w ij = * [ @w ij @(w 1 ; : : : ; w n ) +(Contribution of cost=complexity terms)]: An approach of this type has been used in image compression, with a penalty aimed at minimizing the entropy of the projected distribution <ref> (Bichsel and Seitz, 1989) </ref>.
Reference: <author> Bienenstock, E. L., N Cooper, L., and Munro, P. W. </author> <year> (1982). </year> <title> Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48. </pages>
Reference-contexts: This penalty certainly measures deviation from normality, since entropy is maximized for a Gaussian distribution. 5 Projection Index for Classification: The Unsupervised BCM Neuron Intrator (1990) has recently shown that a variant of the Bienenstock, Cooper and Munro neuron <ref> (Bienenstock et al., 1982) </ref> performs exploratory projection pursuit using a projection index that measures multi-modality. This neuron version allows theoretical analysis of some visual deprivation experiments (Intrator and Cooper, 1992), and is in agreement with the vast experimental results on visual cortical plasticity (Clothiaux et al., 1991).
Reference: <author> Bridle, J. S. and MacKay, D. J. C. </author> <year> (1992). </year> <title> Unsupervised classifiers, mutual information and `Phantom Targets'. </title> <editor> In Moody, J., Hanson, S., and Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 1096-1101. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Carroll, S. M. and Dickinson, B. W. </author> <year> (1989). </year> <title> Construction of neural net using the radon transform. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 607-611, </pages> <address> New York, NY. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Clothiaux, E. E., N Cooper, L., and Bear, M. F. </author> <year> (1991). </year> <title> Synaptic plasticity in visual cortex: Comparison of theory with experiment. </title> <journal> Journal of Neurophysiology, </journal> <volume> 66 </volume> <pages> 1785-1804. </pages>
Reference-contexts: This neuron version allows theoretical analysis of some visual deprivation experiments (Intrator and Cooper, 1992), and is in agreement with the vast experimental results on visual cortical plasticity <ref> (Clothiaux et al., 1991) </ref>. A network implementation which can find several projections in parallel while retaining its computational efficiency, was found to be applicable for extracting features from very high dimensional vector spaces (Intrator and Gold, 1993; Intrator et al., 1991; Intrator, 1992) N.
Reference: <author> Cybenko, G. </author> <year> (1989). </year> <title> Approximations by superpositions of a sigmoidal function. </title> <journal> Mathematics of Control, Signals and Systems, </journal> <volume> 2 </volume> <pages> 303-314. </pages>
Reference: <author> Diaconis, P. and Freedman, D. </author> <year> (1984). </year> <title> Asymptotics of graphical projection pursuit. </title> <journal> Annals of Statistics, </journal> <volume> 12 </volume> <pages> 793-815. </pages>
Reference-contexts: The notion of interesting projections is motivated by an observation that for most high-dimensional data clouds, most low-dimensional projections are approximately normal <ref> (Diaconis and Freedman, 1984) </ref>. This finding suggests that the important information in the data is conveyed in those directions whose single dimensional projected distribution is far from Gaussian.
Reference: <author> Fisher, R. A. </author> <year> (1936). </year> <title> The use of multiple measurements in taxonomic problems. </title> <journal> Annals of Eugenics, </journal> <volume> 7 </volume> <pages> 179-188. </pages>
Reference: <author> Friedman, J. H. </author> <year> (1987). </year> <title> Exploratory projection pursuit. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 249-266. </pages>
Reference: <author> Friedman, J. H. and Stuetzle, W. </author> <year> (1981). </year> <title> Projection pursuit regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76 </volume> <pages> 817-823. </pages>
Reference-contexts: 1 Introduction Parameter estimation becomes difficult in high-dimensional spaces due to the increasing sparseness of the data. Therefore, when a low dimensional representation is embedded in the data, dimensionality reduction methods become useful. One such method projection pursuit regression <ref> (Friedman and Stuetzle, 1981) </ref> (PPR) is capable of performing dimensionality reduction by composition, namely, it constructs an approximation to the desired response function using a composition of lower dimensional smooth functions. These functions depend on low dimensional projections through the data. <p> This process is initialized by setting r i0 = y i . Usually, the initial values of a j are taken to be the first few principal components of the data. Estimation of the ridge functions can be achieved by various nonparametric smoothing techniques such as locally linear functions <ref> (Friedman and Stuetzle, 1981) </ref>, k-nearest neighbors (Hall, 1989b), splines or variable degree polynomials. The smoothness constraint imposed on g, implies that the actual projection pursuit is achieved by minimizing at iteration j, the sum n X r 2 for some smoothness measure C.
Reference: <author> Friedman, J. H. and Tukey, J. W. </author> <year> (1974). </year> <title> A projection pursuit algorithm for exploratory data analysis. </title> <journal> IEEE Transactions on Computers, C(23):881-889. </journal>
Reference: <author> Funahashi, K. </author> <year> (1989). </year> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192. </pages>
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias-variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference: <author> Gutfinger, D. and Sklansky, J. </author> <year> (1991). </year> <title> Robust classifiers by mixed adaptation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 552-567. </pages>
Reference: <author> Hall, P. </author> <year> (1988). </year> <title> Estimating the direction in which data set is most interesting. </title> <journal> Probab. Theory Rel. Fields, </journal> <volume> 80 </volume> <pages> 51-78. </pages>
Reference: <author> N. Intrator Combining EPP and PPR 10 Hall, P. </author> <year> (1989a). </year> <title> On polynomial-based projection indices for exploratory projection pursuit. </title> <journal> The Annals of Statistics, </journal> <volume> 17 </volume> <pages> 589-605. </pages>
Reference: <author> Hall, P. </author> <year> (1989b). </year> <title> On projection pursuit regression. </title> <journal> The Annals of Statistics, </journal> <volume> 17 </volume> <pages> 573-588. </pages>
Reference-contexts: Usually, the initial values of a j are taken to be the first few principal components of the data. Estimation of the ridge functions can be achieved by various nonparametric smoothing techniques such as locally linear functions (Friedman and Stuetzle, 1981), k-nearest neighbors <ref> (Hall, 1989b) </ref>, splines or variable degree polynomials. The smoothness constraint imposed on g, implies that the actual projection pursuit is achieved by minimizing at iteration j, the sum n X r 2 for some smoothness measure C.
Reference: <author> Harman, H. H. </author> <year> (1967). </year> <title> Modern Factor Analysis. </title> <publisher> University of Chicago Press, </publisher> <address> Second Edition, Chicago and London. </address>
Reference: <author> Hecht-Nielsen, R. </author> <year> (1989). </year> <title> Theory of the backpropagation neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 593-606, </pages> <address> New York, NY. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Hornik, K. </author> <year> (1991). </year> <title> Approximation capabilities of multilayer feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 251-257. </pages>
Reference: <author> Hornik, K., Stinchcombe, M., and White, H. </author> <year> (1989). </year> <title> Multilayer feedforward networks are universal approx-imators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366. </pages>
Reference: <author> Hornik, K., Stinchcombe, M., and White, H. </author> <year> (1990). </year> <title> Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 551-560. </pages>
Reference-contexts: speech recognition (Lippmann, 1989), and since the class of functions that can be approximated by a back-propagation type network is very large; This architecture (with an unlimited number of projections) can uniformly approximate arbitrary continuous functions on compact sets (Cybenko, 1989; Hornik et al., 1989) as well as their derivatives <ref> (Hornik et al., 1990) </ref>, and do so efficiently. Related results can be found in (Carroll and Dickinson, 1989; Funahashi, 1989; Hecht-Nielsen, 1989; Hornik, 1991; Ito, 1991). In this method, the error is efficiently propagated backwards to the previous layer for modifi N.
Reference: <author> Huber, P. J. </author> <year> (1985). </year> <title> Projection pursuit. (with discussion). </title> <journal> The Annals of Statistics, </journal> <volume> 13 </volume> <pages> 435-475. </pages>
Reference: <author> Intrator, N. </author> <year> (1990). </year> <title> Feature extraction using an unsupervised neural network. </title> <editor> In Touretzky, D. S., Ellman, J. L., Sejnowski, T. J., and Hinton, G. E., editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 310-318. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: In such a case, the projection index should emphasize multi-modality as a specific deviation from normality. A projection index that emphasizes multimodalities in the projected distribution (without relying on the class labels) has recently been introduced <ref> (Intrator, 1990) </ref> and implemented efficiently using a variant of a biologically motivated unsupervised network (Intrator and Cooper, 1992).
Reference: <author> Intrator, N. </author> <year> (1992). </year> <title> Feature extraction using an unsupervised neural network. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 98-107. </pages>
Reference-contexts: A projection index that emphasizes multimodalities in the projected distribution (without relying on the class labels) has recently been introduced (Intrator, 1990) and implemented efficiently using a variant of a biologically motivated unsupervised network <ref> (Intrator and Cooper, 1992) </ref>. Its integration into a back-propagation classifier will be discussed below. 4 A Variant of Projection Pursuit Regression: Back-Propagation Network In this section, we consider a parametric approach the back-propagation network as a variant of PPR. <p> This neuron version allows theoretical analysis of some visual deprivation experiments <ref> (Intrator and Cooper, 1992) </ref>, and is in agreement with the vast experimental results on visual cortical plasticity (Clothiaux et al., 1991). <p> The inhibited activity and threshold of the k'th neuron is given by ~c k = (c k j6=k M = E [~c 2 The threshold ~ fi k M is the point at which the modification function changes sign <ref> (see Intrator and Cooper, 1992 for further details) </ref>. <p> Additional experiments on vowel tokens appear in Tajchman and Intrator (1992). Another application is in the area of face recognition from gray level pixels <ref> (Intrator et al., 1992) </ref>. After aligning and normalizing the images, the input was set to 37 fi 62 pixels (total of 2294 dimensions).
Reference: <author> Intrator, N. and Cooper, L. N. </author> <year> (1992). </year> <title> Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 3-17. </pages>
Reference-contexts: A projection index that emphasizes multimodalities in the projected distribution (without relying on the class labels) has recently been introduced (Intrator, 1990) and implemented efficiently using a variant of a biologically motivated unsupervised network <ref> (Intrator and Cooper, 1992) </ref>. Its integration into a back-propagation classifier will be discussed below. 4 A Variant of Projection Pursuit Regression: Back-Propagation Network In this section, we consider a parametric approach the back-propagation network as a variant of PPR. <p> This neuron version allows theoretical analysis of some visual deprivation experiments <ref> (Intrator and Cooper, 1992) </ref>, and is in agreement with the vast experimental results on visual cortical plasticity (Clothiaux et al., 1991). <p> The inhibited activity and threshold of the k'th neuron is given by ~c k = (c k j6=k M = E [~c 2 The threshold ~ fi k M is the point at which the modification function changes sign <ref> (see Intrator and Cooper, 1992 for further details) </ref>. <p> Additional experiments on vowel tokens appear in Tajchman and Intrator (1992). Another application is in the area of face recognition from gray level pixels <ref> (Intrator et al., 1992) </ref>. After aligning and normalizing the images, the input was set to 37 fi 62 pixels (total of 2294 dimensions).
Reference: <author> Intrator, N. and Gold, J. I. </author> <year> (1993). </year> <title> Three-dimensional object recognition of gray level images: The usefulness of distinguishing features. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 61-74. </pages>
Reference: <author> Intrator, N., Gold, J. I., Bulthoff, H. H., and Edelman, S. </author> <year> (1991). </year> <title> Three-dimensional object recognition using an unsupervised neural network: Understanding the distinguishing features. </title> <editor> In Feldman, Y. and Bruckstein, A., editors, </editor> <booktitle> Proceedings of the 8th Israeli Conference on AICV, </booktitle> <pages> pages 113-123. </pages> <publisher> Elsevier. </publisher>
Reference-contexts: N. Intrator Combining EPP and PPR 8 6 Applications We have applied this hybrid classification method to various speech and image recognition problems in high dimensional space. In one speech application we used voiceless stop consonants extracted from the TIMIT database as training tokens <ref> (Intrator and Tajchman, 1991) </ref>. A detailed biologically motivated speech representation was produced by Lyon's cochlear model (Lyon, 1982; Slaney, 1988). This representation produced 5040 dimensions (84 channels fi 60 time slices).
Reference: <author> Intrator, N., Reisfeld, D., and Yeshurun, Y. </author> <year> (1992). </year> <title> Face recognition using a hybrid supervised/unsupervised neural network. </title> <type> Preprint. </type>
Reference-contexts: A projection index that emphasizes multimodalities in the projected distribution (without relying on the class labels) has recently been introduced (Intrator, 1990) and implemented efficiently using a variant of a biologically motivated unsupervised network <ref> (Intrator and Cooper, 1992) </ref>. Its integration into a back-propagation classifier will be discussed below. 4 A Variant of Projection Pursuit Regression: Back-Propagation Network In this section, we consider a parametric approach the back-propagation network as a variant of PPR. <p> This neuron version allows theoretical analysis of some visual deprivation experiments <ref> (Intrator and Cooper, 1992) </ref>, and is in agreement with the vast experimental results on visual cortical plasticity (Clothiaux et al., 1991). <p> The inhibited activity and threshold of the k'th neuron is given by ~c k = (c k j6=k M = E [~c 2 The threshold ~ fi k M is the point at which the modification function changes sign <ref> (see Intrator and Cooper, 1992 for further details) </ref>. <p> Additional experiments on vowel tokens appear in Tajchman and Intrator (1992). Another application is in the area of face recognition from gray level pixels <ref> (Intrator et al., 1992) </ref>. After aligning and normalizing the images, the input was set to 37 fi 62 pixels (total of 2294 dimensions).
Reference: <author> Intrator, N. and Tajchman, G. </author> <year> (1991). </year> <title> Supervised and unsupervised feature extraction from a cochlear model for speech recognition. </title> <editor> In Juang, B. H., Kung, S. Y., and Kamm, C. A., editors, </editor> <booktitle> Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop, </booktitle> <pages> pages 460-469. </pages> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference-contexts: N. Intrator Combining EPP and PPR 8 6 Applications We have applied this hybrid classification method to various speech and image recognition problems in high dimensional space. In one speech application we used voiceless stop consonants extracted from the TIMIT database as training tokens <ref> (Intrator and Tajchman, 1991) </ref>. A detailed biologically motivated speech representation was produced by Lyon's cochlear model (Lyon, 1982; Slaney, 1988). This representation produced 5040 dimensions (84 channels fi 60 time slices).
Reference: <author> Ito, Y. </author> <year> (1991). </year> <title> Representation of functions by superpositions of a step or sigmoid functin and their applications to neural network theory. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 385-394. </pages>
Reference: <author> Jones, M. C. and Sibson, R. </author> <year> (1987). </year> <title> What is projection pursuit? (with discussion). </title> <journal> J. Roy. Statist. Soc., Ser. A(150):1-36. </journal>
Reference: <author> Kruskal, J. B. </author> <year> (1969). </year> <title> Toward a practical method which helps uncover the structure of the set of multivariate observations by finding the linear transformation which optimizes a new 'index of condensation'. </title> <editor> In Milton, R. C. and Nelder, J. A., editors, </editor> <booktitle> Statistical Computation, </booktitle> <pages> pages 427-440. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> N. Intrator Combining EPP and PPR 11 Kruskal, J. B. </author> <year> (1972). </year> <title> Linear transformation of multivariate data to reveal clustering. In Shepard, </title> <editor> R. N., Romney, A. K., and Nerlove, S. B., editors, </editor> <title> Multidimensional Scaling: Theory and Application in the Behavioral Sciences, I, </title> <booktitle> Theory, </booktitle> <pages> pages 179-191. </pages> <publisher> Seminar Press, </publisher> <address> New York and London. </address>
Reference: <author> Le Cun, Y. </author> <year> (1985). </year> <title> Une procedure d'apprentissage pour reseau a seuil assymetrique. </title> <booktitle> In Cognitiva 85: </booktitle> <institution> A la Frontiere de l'Intelligence Artificielle des Sciences de la Connaissance des Neurosciences, </institution> <address> pages 599-604, Paris. (Paris 1985), CESTA. </address>
Reference: <author> Le Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. </author> <year> (1989). </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551. </pages>
Reference: <author> Le Cun, Y., Denker, J., and Solla, S. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 598-605, </pages> <address> San Mateo. (Denver 1989), </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lincoln, W. P. and Skrzypek, J. </author> <year> (1990). </year> <title> Synergy of clustering multiple back-propagation networks. </title> <editor> In Touretzky, D. S. and Lippmann, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 650-657. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Lippmann, R. P. </author> <year> (1989). </year> <title> Review of neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1(1) </volume> <pages> 1-38. </pages>
Reference-contexts: Back-propagation (Werbos, 1974; Le Cun, 1985; Rumelhart et al., 1986) has been chosen as a possible representative for the first two alternatives presented in section 2, since it has become a useful tool for solving complicated pattern recognition tasks such as speech recognition <ref> (Lippmann, 1989) </ref>, and since the class of functions that can be approximated by a back-propagation type network is very large; This architecture (with an unlimited number of projections) can uniformly approximate arbitrary continuous functions on compact sets (Cybenko, 1989; Hornik et al., 1989) as well as their derivatives (Hornik et al.,
Reference: <author> Lyon, R. F. </author> <year> (1982). </year> <title> A computational model of filtering, detection, and compression in the cochlea. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Paris, France. </address>
Reference: <author> Moody, J. E. </author> <year> (1991). </year> <title> Note on generalization, regularization and architecture selection in nonlinear learning systems. </title> <editor> In Juang, B. H., Kung, S. Y., and Kamm, C. A., editors, </editor> <booktitle> Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop, </booktitle> <pages> pages 1-10. </pages>
Reference: <author> Mougeot, M., Azencott, R., and Angeniol, B. </author> <year> (1991). </year> <title> Image compression with back propagation: Improvement of the visual restoration using different cost functions. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 467-476. </pages>
Reference-contexts: Probably, all the possible measures should emphasize some form of deviation from normality but the specific type may depend on the problem at hand. For example, a measure based on the Karhunen Loeve expansion <ref> (Mougeot et al., 1991) </ref> may be useful for image compression with autoassociative networks, since in this case one is interested in minimizing the L 2 norm of the distance between the reconstructed image and the original one, and under mild conditions, the Karhunen Loeve expansion gives the optimal solution.
Reference: <author> Mozer, M. C. and Smolensky, P. </author> <year> (1989). </year> <title> Using relevance to reduce network size automatically. </title> <journal> Connection Science, </journal> <volume> 1(1) </volume> <pages> 3-16. </pages>
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 473-493. </pages>
Reference-contexts: An extension of this idea is the "soft weight sharing" which favors irregularities in the weight distribution in the form of multimodality <ref> (Nowlan and Hinton, 1992) </ref>. This penalty improved generalization results obtained by weight elimination penalty. Both these methods make an explicit assumption about the structure of the weight space, but with no regard to the structure of the input space.
Reference: <author> Pearlmutter, B. A. and Rosenfeld, R. </author> <year> (1991). </year> <title> Chaitin-Kolmogorov complexity and generalization in neural networks. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 925-931. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Perrone, M. P. and Cooper, L. N. </author> <year> (1993). </year> <title> When networks disagree: Ensemble method for neural networks. </title> <editor> In Mammone, R. J., editor, </editor> <title> Neural Networks for Speech and Image processing. </title> <publisher> Chapman-Hall. [In press]. </publisher>
Reference: <author> Plaut, D. C., Nowlan, S. J., and Hinton, G. E. </author> <year> (1986). </year> <title> Experiments on learning by back-propagation. </title> <type> Technical Report CMU-CS-86-126, </type> <institution> Carnegie-Mellon University. </institution>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Sebestyen, G. </author> <year> (1962). </year> <title> Decision Making Processes in Pattern Recognition. </title> <publisher> Macmillan, </publisher> <address> New York. </address>
Reference: <author> Slaney, M. </author> <year> (1988). </year> <title> Lyon's cochlear model. </title> <type> Technical report, </type> <institution> Apple Corporate Library, Cupertino, </institution> <address> CA 95014. </address>
Reference: <author> Switzer, P. </author> <year> (1970). </year> <title> Numerical classification. </title> <editor> In Barnett, V., editor, Geostatistics. </editor> <publisher> Plenum Press, </publisher> <address> New York. </address>
Reference: <author> N. Intrator Combining EPP and PPR 12 Tajchman, G. N. and Intrator, N. </author> <year> (1992). </year> <title> Phonetic classification of TIMIT segments preprocessed with lyon's cochlear model using a supervised/unsupervised hybrid neural network. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing, </booktitle> <address> Banff, Alberta, Canada. </address>
Reference-contexts: A projection index that emphasizes multimodalities in the projected distribution (without relying on the class labels) has recently been introduced (Intrator, 1990) and implemented efficiently using a variant of a biologically motivated unsupervised network <ref> (Intrator and Cooper, 1992) </ref>. Its integration into a back-propagation classifier will be discussed below. 4 A Variant of Projection Pursuit Regression: Back-Propagation Network In this section, we consider a parametric approach the back-propagation network as a variant of PPR. <p> This neuron version allows theoretical analysis of some visual deprivation experiments <ref> (Intrator and Cooper, 1992) </ref>, and is in agreement with the vast experimental results on visual cortical plasticity (Clothiaux et al., 1991). <p> The inhibited activity and threshold of the k'th neuron is given by ~c k = (c k j6=k M = E [~c 2 The threshold ~ fi k M is the point at which the modification function changes sign <ref> (see Intrator and Cooper, 1992 for further details) </ref>. <p> Additional experiments on vowel tokens appear in Tajchman and Intrator (1992). Another application is in the area of face recognition from gray level pixels <ref> (Intrator et al., 1992) </ref>. After aligning and normalizing the images, the input was set to 37 fi 62 pixels (total of 2294 dimensions).
Reference: <author> Turk, M. and Pentland, A. </author> <year> (1991). </year> <title> Eigenfaces for recognition. </title> <journal> J. of Cognitive Neuroscience, </journal> <volume> 3 </volume> <pages> 71-86. </pages>
Reference: <author> Wahba, G. </author> <year> (1990). </year> <title> Splines Models for Observational Data. </title> <booktitle> Series in Applied Mathematics, </booktitle> <volume> Vol. 59, </volume> <publisher> SIAM, </publisher> <address> Philadelphia. </address>
Reference: <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K. </author> <year> (1989). </year> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on ASSP, </journal> <volume> 37 </volume> <pages> 328-339. </pages>
Reference: <author> Weigend, A. S., Rumelhart, D. E., and Huberman, B. A. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Werbos, P. </author> <year> (1974). </year> <title> Beyond regression: New tools for prediction and analysis in the behavioral sciences. </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University. </institution>
Reference: <author> White, H. </author> <year> (1990). </year> <title> Connectionists nonparametric regression: multilayer feedforward networks can learn arbitraty mappings. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 535-549. </pages>
Reference: <author> Yamac, M. </author> <year> (1969). </year> <title> Can we do better by combining `supervised' and `nonsupervised' machine learning for pattern analysis. </title> <type> Ph.D. dissertation, </type> <institution> Brown University. </institution>
References-found: 61

