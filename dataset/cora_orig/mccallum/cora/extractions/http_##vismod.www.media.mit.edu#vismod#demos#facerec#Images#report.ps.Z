URL: http://vismod.www.media.mit.edu/vismod/demos/facerec/Images/report.ps.Z
Refering-URL: http://vismod.www.media.mit.edu/vismod/demos/facerec/index.html
Root-URL: 
Phone: 2  3  
Title: Video-based Biometric Person Authentication, Crans-Montana, Switzerland,  The FERET September 1996 Database and Evaluation Procedure  
Author: P. Jonathon Phillips, ? Hyeonjoon Moon, y Patrick Rauss, and Syed A. Rizvi y 
Address: 2800 Powder Mill Rd., Adelphi, MD 20783-1197  New York at Buffalo, Amherst, NY 14260  New York, Staten Island, NY 10314  
Affiliation: 1 U.S. Army Research Laboratory, AMSRL-SE-SE,  Department of Electrical and Computer Engineering State University of  Department of Applied Sciences College of Staten Island of City University of  
Date: March 1997.  
Note: To appear in the proceedings of the First International Conference on Audio and  
Pubnum: 12-14  
Abstract: Two of the most critical requirements in support of producing reliable face-recognition systems are a large database of facial images and a testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through the FERET database of facial images and the establishment of the FERET tests. In this paper, we report on the FERET database and the Septem-ber 1996 FERET test. This test is the third in a series of supervised face-recognition test administered under the FERET program.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> R. Brunelli and T. Poggio. </author> <title> Face recognition: Features versus templates. </title> <journal> IEEE Trans. PAMI, </journal> <volume> 15(10), </volume> <year> 1993. </year>
Reference-contexts: We found that the performance for this similarity measure is better than the Euclidean distance. Our correlation algorithm compared faces by correlating the whole face, where each face had been normalized and masked. This method yielded better performance than the feature-based correlation of Brunelli and Poggio <ref> [1] </ref>. We present results for algorithm performance on different categories of probe sets and the effects of different galleries. The results presented are a small subset of the actual ongoing analysis to be completed. We report on two sets of experiments.
Reference: 2. <author> K. Etemad and R. Chellappa. </author> <title> Discriminant analysis for recognition of human face images. </title> <booktitle> In ICASSP '96, </booktitle> <pages> pages 2148-2151, </pages> <year> 1996. </year>
Reference-contexts: This algorithm was retested so that any improvement since March 1995 could be determined. The second algorithm was based on more recent work [5]. Algorithms were also tested from Excalibur Corp., Rutgers University [12], and the University of Maryland <ref> [2] </ref>. The final two algorithms were our implementation of Turk and Pentland's eigenfaces [11] and normalized correlation. These two algorithms provide a baseline for algorithm performance. In our implementation of eigenfaces, we measured the similarity between two faces by the angle between feature vectors in eigenspace.
Reference: 3. <author> G. G. Gordon. </author> <title> Face recognition from frontal and profile views. </title> <editor> In M. Bichsel, editor, </editor> <booktitle> International Workshop on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 47-52, </pages> <year> 1995. </year>
Reference-contexts: The FERET database has fulfilled the data requirements for both development and testing, becoming the de facto standard for face recognition from still images. (Results on the FERET database have been reported by Gordon <ref> [3] </ref>, Mauer and von der Malsburg [4], Moghaddamet al. [5], Pentland et al. [7], Phillips and Vardi [9], and Wiskott et al. [13].) 4 Performance Evaluation Procedure The traditional method of testing a face-recognition algorithm is to provide the algorithm with two sets of images, the gallery and the probe set,
Reference: 4. <author> T. Maurer and C. von der Malsburg. </author> <title> Single-view based recognition of faces rotated in depth. </title> <editor> In M. Bichsel, editor, </editor> <booktitle> International Workshop on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 248-253, </pages> <year> 1995. </year>
Reference-contexts: The FERET database has fulfilled the data requirements for both development and testing, becoming the de facto standard for face recognition from still images. (Results on the FERET database have been reported by Gordon [3], Mauer and von der Malsburg <ref> [4] </ref>, Moghaddamet al. [5], Pentland et al. [7], Phillips and Vardi [9], and Wiskott et al. [13].) 4 Performance Evaluation Procedure The traditional method of testing a face-recognition algorithm is to provide the algorithm with two sets of images, the gallery and the probe set, which do not intersect [8].
Reference: 5. <author> B. Moghaddam, C. Nastar, and A. Pentland. </author> <title> Bayesian face recognition using de-formable intemsity surfaces. </title> <booktitle> In Proceedings Computer Vision and Pattern Recognition 96, </booktitle> <pages> pages 638-645, </pages> <year> 1996. </year>
Reference-contexts: The FERET database has fulfilled the data requirements for both development and testing, becoming the de facto standard for face recognition from still images. (Results on the FERET database have been reported by Gordon [3], Mauer and von der Malsburg [4], Moghaddamet al. <ref> [5] </ref>, Pentland et al. [7], Phillips and Vardi [9], and Wiskott et al. [13].) 4 Performance Evaluation Procedure The traditional method of testing a face-recognition algorithm is to provide the algorithm with two sets of images, the gallery and the probe set, which do not intersect [8]. <p> The first was the same algorithm that took the March 1995 test [6, 7]. This algorithm was retested so that any improvement since March 1995 could be determined. The second algorithm was based on more recent work <ref> [5] </ref>. Algorithms were also tested from Excalibur Corp., Rutgers University [12], and the University of Maryland [2]. The final two algorithms were our implementation of Turk and Pentland's eigenfaces [11] and normalized correlation. These two algorithms provide a baseline for algorithm performance.
Reference: 6. <author> B. Moghaddam and A. Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <booktitle> In Proceedings of the Inter. Conf. on Computer Vision, </booktitle> <pages> pages 786-793, </pages> <year> 1995. </year>
Reference-contexts: The first was the same algorithm that took the March 1995 test <ref> [6, 7] </ref>. This algorithm was retested so that any improvement since March 1995 could be determined. The second algorithm was based on more recent work [5]. Algorithms were also tested from Excalibur Corp., Rutgers University [12], and the University of Maryland [2].
Reference: 7. <author> A. Pentland, B. Moghaddam, and T. Starner. </author> <title> View-based and modular eignspaces for face recognition. </title> <booktitle> In Proceedings Computer Vision and Pattern Recognition 94, </booktitle> <year> 1994. </year>
Reference-contexts: The FERET database has fulfilled the data requirements for both development and testing, becoming the de facto standard for face recognition from still images. (Results on the FERET database have been reported by Gordon [3], Mauer and von der Malsburg [4], Moghaddamet al. [5], Pentland et al. <ref> [7] </ref>, Phillips and Vardi [9], and Wiskott et al. [13].) 4 Performance Evaluation Procedure The traditional method of testing a face-recognition algorithm is to provide the algorithm with two sets of images, the gallery and the probe set, which do not intersect [8]. <p> The first was the same algorithm that took the March 1995 test <ref> [6, 7] </ref>. This algorithm was retested so that any improvement since March 1995 could be determined. The second algorithm was based on more recent work [5]. Algorithms were also tested from Excalibur Corp., Rutgers University [12], and the University of Maryland [2].
Reference: 8. <author> P. J. Phillips, P. Rauss, and S. Der. </author> <title> FERET (face recognition technology) recognition algorithm development and test report. </title> <type> Technical Report TR # 995, U.S. </type> <institution> Army Research Laboratory, </institution> <year> 1996. </year>
Reference-contexts: assessment, the community learns in an unbiased and open manner of the important technical problems to be addressed and how the community is progressing toward solving these problems. 2 FERET Testing History The first FERET tests took place in August 1994 and March 1995 (for details see Phillips et al. <ref> [8] </ref>); FERET database collection had begun in September 1993 along with the FERET program [8, 10]. The August 1994 test established a performance baseline for face-recognition algorithms. This test was designed to measure performance on algorithms that could automatically locate, normalize, and identify faces from a database. <p> problems to be addressed and how the community is progressing toward solving these problems. 2 FERET Testing History The first FERET tests took place in August 1994 and March 1995 (for details see Phillips et al. [8]); FERET database collection had begun in September 1993 along with the FERET program <ref> [8, 10] </ref>. The August 1994 test established a performance baseline for face-recognition algorithms. This test was designed to measure performance on algorithms that could automatically locate, normalize, and identify faces from a database. The test uses both gallery and probe images. The gallery is the set of known individuals. <p> Malsburg [4], Moghaddamet al. [5], Pentland et al. [7], Phillips and Vardi [9], and Wiskott et al. [13].) 4 Performance Evaluation Procedure The traditional method of testing a face-recognition algorithm is to provide the algorithm with two sets of images, the gallery and the probe set, which do not intersect <ref> [8] </ref>. Unfortunately, this method severely limits one's ability to analyze the data. To overcome this deficiency, we modified the testing protocol to allow for a more detailed analysis of face-recognition algorithm performance. <p> All the images in the target set were frontal images. The query set consisted of all the images in the target set plus rotated images and digitally modified images. We designed the digitally modified images to test the effects of illumination and scale <ref> [8] </ref>. For each query image q i , an algorithm outputs the similarity measure s i (k) for all images t k in the target set. The output for each query image q i is sorted by the similarity scores s i (). <p> For a probe set P, we calculate the percentage of probes that have rank n or less, for n = 1; :::; 100. Performance statistics are reported as a cumulative match score graph <ref> [8] </ref>, where the horizontal axis is the rank. The vertical axis is the percentage of probes in a probe set that have rank n or less. In the large gallery experiments, the gallery consisted of 1196 frontal images with one image per person.
Reference: 9. <author> P. J. Phillips and Y. Vardi. </author> <title> Data driven methods in face recognition. </title> <editor> In M. Bichsel, editor, </editor> <booktitle> International Workshop on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 65-70, </pages> <year> 1995. </year>
Reference-contexts: FERET database has fulfilled the data requirements for both development and testing, becoming the de facto standard for face recognition from still images. (Results on the FERET database have been reported by Gordon [3], Mauer and von der Malsburg [4], Moghaddamet al. [5], Pentland et al. [7], Phillips and Vardi <ref> [9] </ref>, and Wiskott et al. [13].) 4 Performance Evaluation Procedure The traditional method of testing a face-recognition algorithm is to provide the algorithm with two sets of images, the gallery and the probe set, which do not intersect [8]. Unfortunately, this method severely limits one's ability to analyze the data.
Reference: 10. <author> P. Rauss, P. J. Phillips, A. T. DePersia, and M. Hamilton. </author> <title> Face recognition technology program overview and results. </title> <booktitle> In AIPR-96, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: problems to be addressed and how the community is progressing toward solving these problems. 2 FERET Testing History The first FERET tests took place in August 1994 and March 1995 (for details see Phillips et al. [8]); FERET database collection had begun in September 1993 along with the FERET program <ref> [8, 10] </ref>. The August 1994 test established a performance baseline for face-recognition algorithms. This test was designed to measure performance on algorithms that could automatically locate, normalize, and identify faces from a database. The test uses both gallery and probe images. The gallery is the set of known individuals.
Reference: 11. <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> J. Neuroscience, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: The second algorithm was based on more recent work [5]. Algorithms were also tested from Excalibur Corp., Rutgers University [12], and the University of Maryland [2]. The final two algorithms were our implementation of Turk and Pentland's eigenfaces <ref> [11] </ref> and normalized correlation. These two algorithms provide a baseline for algorithm performance. In our implementation of eigenfaces, we measured the similarity between two faces by the angle between feature vectors in eigenspace. We found that the performance for this similarity measure is better than the Euclidean distance.
Reference: 12. <author> J. Wilder. </author> <title> face recognition using transform coding of gray scale projection projections and the neural tree network. In Artifical neural networks with applications in speech and vision. </title> <publisher> Chapman Hall, </publisher> <year> 1994. </year>
Reference-contexts: The first was the same algorithm that took the March 1995 test [6, 7]. This algorithm was retested so that any improvement since March 1995 could be determined. The second algorithm was based on more recent work [5]. Algorithms were also tested from Excalibur Corp., Rutgers University <ref> [12] </ref>, and the University of Maryland [2]. The final two algorithms were our implementation of Turk and Pentland's eigenfaces [11] and normalized correlation. These two algorithms provide a baseline for algorithm performance.
Reference: 13. <author> L. Wiskott, J.-M. Fellous, N. Kruger, and C. von der Malsburg. </author> <title> Face recognition and gender determination. </title> <editor> In M. Bichsel, editor, </editor> <booktitle> International Workshop on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 92-97, </pages> <year> 1995. </year>
Reference-contexts: data requirements for both development and testing, becoming the de facto standard for face recognition from still images. (Results on the FERET database have been reported by Gordon [3], Mauer and von der Malsburg [4], Moghaddamet al. [5], Pentland et al. [7], Phillips and Vardi [9], and Wiskott et al. <ref> [13] </ref>.) 4 Performance Evaluation Procedure The traditional method of testing a face-recognition algorithm is to provide the algorithm with two sets of images, the gallery and the probe set, which do not intersect [8]. Unfortunately, this method severely limits one's ability to analyze the data.
References-found: 13

