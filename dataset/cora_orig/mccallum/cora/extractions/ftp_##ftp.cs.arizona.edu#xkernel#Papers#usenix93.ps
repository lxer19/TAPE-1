URL: ftp://ftp.cs.arizona.edu/xkernel/Papers/usenix93.ps
Refering-URL: http://www.cs.arizona.edu/xkernel/bibliography.html
Root-URL: http://www.cs.arizona.edu
Title: A Fast and General Implementation of Mach IPC in a Network  
Author: H. Orman E. Menze III S. O'Malley L. Peterson 
Address: Tucson, AZ 85721  
Affiliation: Department of Computer Science University of Arizona  
Abstract: This paper describes an implementation of the Mach IPC abstraction on a network. Our implementation, called Mach NetIPC, is done in the context of the x -kernel, which provides a networking subsystem for Mach. The paper motivates the design choices we made, describes the x -kernel protocol graph that implements the design, and reports on the performance of the resulting system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. S. Barrera. </author> <title> A fast Mach network IPC implementation. </title> <booktitle> In Proceedings of the Usenix Mach Symposium, </booktitle> <pages> pages 1-12, </pages> <address> 2560 Ninth Street, Suite 215, Berkeley CA 94710, </address> <month> November </month> <year> 1991. </year> <institution> Usenix Association. </institution>
Reference-contexts: Our implementation of Mach NetIPC is not the first; there have been several earlier implementations of the Netmsgserver [5], and an in-kernel implementation tailored for NORMA architectures <ref> [1] </ref>. Because of our use of the x -kernel, however, our implementation has the following distinct advantages. * The x -kernel supports the decomposition of complex protocols into "microprotocols" that are small, easy to understand, implement, debug, and tune. <p> Finally, the correctness of this approach is almost solely dependent upon the correctness of one simple protocol an easily analyzed piece of code. This failure model is different from that found in the Mach 3 NORMA IPC implementation <ref> [1] </ref>. The NORMA code assumes that there is a fixed and known collection of communicating nodes, that network partitions do not happen, and that there is no independent node failure (e.g. if one node in the group fails all nodes fail). <p> At least another 20% is estimated to be due to overhead of the cthreads package: scheduling and activating a thread in the client and server. The NORMA implementation cites 2.5 milliseconds for RPC roundtrip time <ref> [1] </ref> for i486 PC's. While it is difficult to compare expected performance between the PC and a DecStation, we expect the DecStation to be at least as fast. <p> It is possible to operate over either LAN's or internetwork configurations without any additional coding. Placing the transport protocols into the Mach kernel yields a significant performance benefit, a fact that has been previously noted with respect to the Mach 2.5 kernel with TCP/IP <ref> [1] </ref>. The timing breakdown indicates that moving the port manipulation functions into the Mach kernel would make a large difference. This implementation does not define the legal participants of a communicating network. It is likely that administrators would like to limit the communication to selected node lists and/or subnets.
Reference: [2] <author> A. Birrell and B. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> Feb. </month> <year> 1984. </year>
Reference-contexts: Therefore, one of our first design decisions was to put an RPC protocol at the heart of Mach NetIPC. Previous x -kernel work had led to an efficient RPC protocol based on Sprite RPC <ref> [9, 3, 2] </ref>. This was an obvious starting point for our design. The more complex problem, however, was how to match this to Mach IPC; the communication abstraction does not have a direct match to RPC semantics, and there is some latitude possible in choosing a mapping.
Reference: [3] <author> N. Hutchinson, L. Peterson, S. O'Malley, E. Menze, and H. Orman. </author> <title> The x-Kernel Programmer's Manual (version 3.2). </title> <institution> Computer Science Department, University of Arizona, Tucson, Arizona, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Therefore, one of our first design decisions was to put an RPC protocol at the heart of Mach NetIPC. Previous x -kernel work had led to an efficient RPC protocol based on Sprite RPC <ref> [9, 3, 2] </ref>. This was an obvious starting point for our design. The more complex problem, however, was how to match this to Mach IPC; the communication abstraction does not have a direct match to RPC semantics, and there is some latitude possible in choosing a mapping.
Reference: [4] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Multiple host architectures, individual node failures, lost messages, network partitions, and variable network delays all combine to make the implementation of NetIPC a difficult proposition. This paper describes an implementation of Mach NetIPC in the context of the x -kernel|a protocol implementation environment that has been incorporated into Mach <ref> [4, 7] </ref>. Our implementation of Mach NetIPC is not the first; there have been several earlier implementations of the Netmsgserver [5], and an in-kernel implementation tailored for NORMA architectures [1].
Reference: [5] <author> D. P. Julin. </author> <title> Network server protocol specification (draft). </title> <institution> Computer Science Department, </institution> <address> CMU, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: This paper describes an implementation of Mach NetIPC in the context of the x -kernel|a protocol implementation environment that has been incorporated into Mach [4, 7]. Our implementation of Mach NetIPC is not the first; there have been several earlier implementations of the Netmsgserver <ref> [5] </ref>, and an in-kernel implementation tailored for NORMA architectures [1]. Because of our use of the x -kernel, however, our implementation has the following distinct advantages. * The x -kernel supports the decomposition of complex protocols into "microprotocols" that are small, easy to understand, implement, debug, and tune. <p> While this approach has performance advantages, it does not scale well beyond distributed memory multiprocessors and tightly coupled workstation clusters. The Netmsgserver <ref> [5] </ref> uses reboot notifications and liveness checks to deallocate port rights, but the method is susceptible to inconsistent views arising from network partitions. A disadvantage of our approach is a lack of timely notifications.
Reference: [6] <author> K. Loepere. </author> <title> Mach 3 kernel principles, draft proposed specification. Open Software Foundation, </title> <month> July 28 </month> <year> 1992. </year>
Reference-contexts: These concerns have lead us to suggest three changes to the Mach 3.0 IPC specification: data streams, ordered messages, and untyped data. These changes have been tentatively accepted in the ongoing process of creating a new common Mach interface specification <ref> [6] </ref>. Note that the software described earlier in this paper implements the original Mach 3.0 IPC definition and does not reflect these changes. The proposed changes all relate the to the Mach message format. The current Mach message format is a flat unordered series of tagged data items.
Reference: [7] <author> S. O'Malley and L. Peterson. </author> <title> A dynamic network architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Multiple host architectures, individual node failures, lost messages, network partitions, and variable network delays all combine to make the implementation of NetIPC a difficult proposition. This paper describes an implementation of Mach NetIPC in the context of the x -kernel|a protocol implementation environment that has been incorporated into Mach <ref> [4, 7] </ref>. Our implementation of Mach NetIPC is not the first; there have been several earlier implementations of the Netmsgserver [5], and an in-kernel implementation tailored for NORMA architectures [1]. <p> This minimizes the overhead of starting up a channel and eliminates the complexity necessary to reliably tear down a channel (cf TCP). Channel objects are destroyed upon notification of a peer rebooting. 3.4.2 Virtual Protocols: VSIZE, VMUX, and VNET Below BootId and CHAN, a series of virtual protocols <ref> [7] </ref> are used to select the appropriate transport protocols, based upon various properties of the message or connection. The first virtual protocol, VSIZE, dynamically directs outgoing messages to either VMUX or BLAST depending upon the size of the message.
Reference: [8] <institution> Network computing architecture, </institution> <note> version 2.0 specifications. Open Software Foundation, Septem-ber 25 1992. </note>
Reference-contexts: As currently proposed, the sending MIG stub will be responsible for affixing a NIDL-like <ref> [8] </ref> architecture tag to data; the tag will be checked by the receiving MIG stub before unbundling the Mach message. This removes data representation from the responsibility of Mach NetIPC and makes it a strictly end-to-end issue.
Reference: [9] <author> J. K. Ousterhout et al. </author> <title> The Sprite network operating system. </title> <booktitle> IEEE Computer, </booktitle> <year> 1988. </year>
Reference-contexts: Therefore, one of our first design decisions was to put an RPC protocol at the heart of Mach NetIPC. Previous x -kernel work had led to an efficient RPC protocol based on Sprite RPC <ref> [9, 3, 2] </ref>. This was an obvious starting point for our design. The more complex problem, however, was how to match this to Mach IPC; the communication abstraction does not have a direct match to RPC semantics, and there is some latitude possible in choosing a mapping.
Reference: [10] <author> R. Schlichting and F. Schneider. </author> <title> Fail-stop processors: An approach to designing fault tolerant computing systems. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 1(3) </volume> <pages> 222-238, </pages> <month> Aug. </month> <year> 1983. </year>
Reference-contexts: We define a simple failure as one that exhibits crash semantics, that is, a node fails silently without transmitting any extraneous messages <ref> [10] </ref>. To meet this requirement, we have designed a failure handing system that is consistent with the fully decentralized communications model presented in the previous section. We have adopted a "retry until reboot" strategy that dictates the equivalence of node reboot with node failure notification.
References-found: 10

