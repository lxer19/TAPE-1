URL: http://www.cs.washington.edu/homes/map/research/papers/www8.ps
Refering-URL: http://www.cs.washington.edu/homes/map/adaptive/perkowitz.html
Root-URL: http://www.cs.washington.edu
Email: fmap, etzionig@cs.washington.edu  
Phone: (206) 616-1845 Fax: (206) 543-2969  
Title: Towards Adaptive Web Sites: Conceptual Framework and Case Study  
Author: Mike Perkowitz Oren Etzioni 
Keyword: adaptive, clustering, data mining  
Address: Box 352350  Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering,  University of Washington,  
Abstract: The creation of a complex web site is a thorny problem in user interface design. In this paper we explore the notion of adaptive web sites: sites that semi-automatically improve their organization and presentation by learning from visitor access patterns. It is easy to imagine and implement web sites that offer shortcuts to popular pages. Are more sophisticated adaptive web sites feasible? What degree of automation can we achieve? To address the questions above, we describe the design space of adaptive web sites and consider a case study: the problem of synthesizing new index pages that facilitate navigation of a web site. We present the PageGather algorithm, which automatically identifies candidate link sets to include in index pages based on user access logs. We demonstrate experimentally that PageGather outperforms the Apriori data mining algorithm on this task. In addition, we compare PageGather's link sets to pre-existing, human-authored index pages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining Association Rules between Sets of Items in Large Databases. </title> <booktitle> In Proceedings of SIGMOD-93, </booktitle> <pages> pages 207-216, </pages> <year> 1993. </year>
Reference-contexts: We chose K-means as it is particularly fast, and HAC as it is widely used. We found that PageGather's clustering component was faster than both HAC and K-means and found higher-quality clusters. Frequent set algorithms are designed to find sets of similar items in large collections (see, for example, <ref> [1, 2, 19, 21] </ref>). We therefore compare PageGather to the standard Apriori algorithm for finding frequent sets (see [3]).
Reference: [2] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. </author> <title> Verkamo. </title> <booktitle> Fast Discovery of Association Rules, </booktitle> <pages> pages 307-328. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: We chose K-means as it is particularly fast, and HAC as it is widely used. We found that PageGather's clustering component was faster than both HAC and K-means and found higher-quality clusters. Frequent set algorithms are designed to find sets of similar items in large collections (see, for example, <ref> [1, 2, 19, 21] </ref>). We therefore compare PageGather to the standard Apriori algorithm for finding frequent sets (see [3]).
Reference: [3] <author> R. Agrawal and R. Srikant. </author> <title> Fast Algorithms for Mining Association Rules. </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <year> 1994. </year>
Reference-contexts: We then present PageGather, analyzing variants of PageGather and both data mining and clustering algorithms as potential solutions. In section 3, we experimentally evaluate variants of PageGather, and compare the performance of PageGather to that of Apriori, the classical data mining algorithm for the discovery of frequent sets <ref> [3] </ref>. We also compare PageGather's output to pre-existing, human-authored index pages available at our experimental web site. We conclude with a discussion of future work and a summary of our contributions. 1.1 Design Space Adaptive web sites vary along a number of design axes. * Types of adaptations. <p> Frequent set algorithms are designed to find sets of similar items in large collections (see, for example, [1, 2, 19, 21]). We therefore compare PageGather to the standard Apriori algorithm for finding frequent sets (see <ref> [3] </ref>). In a traditional frequent set problem, the data is a collection of 10 0 20 40 60 80 Candidate Link Sets A v e r a g e V i s i t P e r c e n t a g e PG-CC PG-CLIQUE reduction.
Reference: [4] <author> E. Andre, W. Graf, J. Muller, H.-J. Profitlich, T. Rist, and W. Wahlster. AiA: </author> <title> Adaptive Communication Assistant for Effective Infobahn Access. Document, </title> <institution> DFKI, Saarbrucken, </institution> <year> 1996. </year>
Reference-contexts: With all of the site's content so encoded, its presentation may be easily adapted. A number of projects have explored client-side customization, in which a user has her own associated agent who learns about her interests and customizes her web experience accordingly. The AiA project <ref> [4, 17] </ref> explores the customization of web page information by adding a "presentation agent" who can direct the user's attention to topics of interest.
Reference: [5] <author> R. Armstrong, D. Freitag, T. Joachims, and T. Mitchell. Webwatcher: </author> <title> A learning apprentice for the world wide web. </title> <booktitle> In Working Notes of the AAAI Spring Symposium: Information Gathering from Heterogeneous, Distributed Environments, </booktitle> <pages> pages 6-12, </pages> <address> Stanford University, 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Some sites also allow users to describe interests and will present information | news articles, for example | relevant to those interests. More sophisticated sites attempt path prediction: guessing where the user wants to go and taking her there immediately (or at least providing a link). The WebWatcher <ref> [5] </ref> 2 learns to predict what links users will follow on a particular page as a function of their specified interests. A link that WebWatcher believes a particular user is likely to follow will be highlighted graphically and duplicated at the top of the page when it is presented.
Reference: [6] <author> M. Fernandez, D. Florescu, J. Kang, A. Levy, and D. Suciu. </author> <title> System Demonstration Strudel: A Web-site Management System. </title> <booktitle> In ACM SIGMOD Conference on Management of Data, </booktitle> <year> 1997. </year>
Reference-contexts: The use of meta-information to customize or optimize web sites has been explored in a number of projects (see, for example, XML annotations [9], Apple's Meta-Content Format, and other projects <ref> [6, 11] </ref>). One example of this approach is the STRUDEL web-site management system [6] which attempts to separate the information available at a web site from its graphical presentation. Instead of manipulating web sites at the level of pages and links, web sites may be specified using STRUDEL's view-definition language. <p> The use of meta-information to customize or optimize web sites has been explored in a number of projects (see, for example, XML annotations [9], Apple's Meta-Content Format, and other projects [6, 11]). One example of this approach is the STRUDEL web-site management system <ref> [6] </ref> which attempts to separate the information available at a web site from its graphical presentation. Instead of manipulating web sites at the level of pages and links, web sites may be specified using STRUDEL's view-definition language. <p> Clusters found by P G CLIQUE perform significantly better than the existing index pages. closest to a PageGather-generated link set is straightforward. Alternatively, if we have a predicate language for describing the different pages at the site (in XML or a-la-STRUDEL <ref> [6] </ref>), then we can apply concept learning techniques [12] to generate a description of the PageGather link set in that language.
Reference: [7] <author> J. Fink, A Kobsa, and A. Nill. </author> <title> User-oriented Adaptivity and Adaptability in the AVANTI Project. In Designing for the Web: Empirical Studies, Microsoft Usability Group, </title> <address> Redmond (WA)., </address> <year> 1996. </year>
Reference-contexts: A site may also try to customize to a user by trying to guess her general interests dynamically as she browses. The AVANTI Project <ref> [7] </ref> 3 focuses on dynamic customization based on users' needs and tastes. As with the WebWatcher, AVANTI relies partly on users providing information about themselves when they enter the site.
Reference: [8] <author> A. Fox, S. Gribble, Y. Chawathe, and E. Brewer. </author> <title> Adapting to Network and Client Variation Using Infrastruc-tural Proxies: Lessons and Perspectives. </title> <journal> IEEE Personal Communications, </journal> <volume> 5(4) </volume> <pages> 10-19, </pages> <year> 1998. </year>
Reference-contexts: Other projects have investigated performing customization at neither the client nor the server but as part of the network in between, particularly by using transcoding proxies. Transend <ref> [8] </ref>, for example, is a proxy server at the University of California at Berkeley that performs image compression and allows each of thousands of users to customize the degree of compression, the interface for image refinement, and the web pages to which compression is applied. 1.3 Our Approach Our survey of
Reference: [9] <author> R. Khare and A. Rifkin. XML: </author> <title> A Door to Automated Web Applications. </title> <journal> IEEE Internet Computing, </journal> <volume> 1(4) </volume> <pages> 78-87, </pages> <year> 1997. </year>
Reference-contexts: The use of meta-information to customize or optimize web sites has been explored in a number of projects (see, for example, XML annotations <ref> [9] </ref>, Apple's Meta-Content Format, and other projects [6, 11]). One example of this approach is the STRUDEL web-site management system [6] which attempts to separate the information available at a web site from its graphical presentation.
Reference: [10] <author> H. Lieberman. Letizia: </author> <title> An agent that assists web browsing. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 924-929, </pages> <year> 1995. </year>
Reference-contexts: In the AiA model, the presentation agent is on the client side, but similar techniques could be applied to customized presentation by a web server as well. Letizia <ref> [10] </ref> is a personal agent that learns a model of its user by observing her behavior. Letizia explores the web ahead of the user (investigating links off of the current page) and uses its user model to recommend pages it thinks the user will enjoy.
Reference: [11] <author> S. Luke, L. Spector, D. Rager, and J. Hendler. </author> <title> Ontology-based web agents. </title> <booktitle> In Proc. First Intl. Conf. Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: The use of meta-information to customize or optimize web sites has been explored in a number of projects (see, for example, XML annotations [9], Apple's Meta-Content Format, and other projects <ref> [6, 11] </ref>). One example of this approach is the STRUDEL web-site management system [6] which attempts to separate the information available at a web site from its graphical presentation. Instead of manipulating web sites at the level of pages and links, web sites may be specified using STRUDEL's view-definition language.
Reference: [12] <author> T. Mitchell. </author> <title> Machine Learning. </title> <publisher> McGraw Hill, </publisher> <year> 1997. </year>
Reference-contexts: Clusters found by P G CLIQUE perform significantly better than the existing index pages. closest to a PageGather-generated link set is straightforward. Alternatively, if we have a predicate language for describing the different pages at the site (in XML or a-la-STRUDEL [6]), then we can apply concept learning techniques <ref> [12] </ref> to generate a description of the PageGather link set in that language.
Reference: [13] <author> M. Perkowitz and O. Etzioni. </author> <title> Adaptive web sites: an AI challenge. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1997. </year>
Reference-contexts: Fourth, a site may be designed for a particular kind of use, but be used in many different ways in practice; the designer's a priori expectations may be violated. Too often web site designs are fossils cast in HTML, while web navigation is dynamic, time-dependent, and idiosyncratic. In 1 <ref> [13] </ref>, we challenged the AI community to address this problem by creating adaptive web sites: sites that semi-automatically improve their organization and presentation by learning from visitor access patterns. Many web sites can be viewed as user interfaces to complex information stores. <p> We have implemented one such transformation: shortcutting, in which we attempt to provide links on each page to visitors' eventual goals, thus skipping the in-between pages. As reported in <ref> [13] </ref>, we found a significant number of visitors used these automatic shortcuts. However, our long-term goal is to demonstrate that more fundamental adaptations are feasible. An example of this is change in view, where a site could offer an alternative organization of its contents based on user access patterns.
Reference: [14] <author> M. Perkowitz and O. Etzioni. </author> <title> Adaptive web sites: Automatically learning from user access patterns. </title> <booktitle> In Proceedings of the Sixth Int. WWW Conference, </booktitle> <address> Santa Clara, CA, </address> <year> 1997. </year>
Reference-contexts: Thus, we accumulate statistics over many visits by numerous visitors and search for overall trends. It is not difficult to devise a number of simple, non-destructive transformations that could improve a site; we describe several in <ref> [14] </ref>. Examples include highlighting popular links, promoting popular links to the top of a page or to the site's front page, and linking together pages that seem to be related.
Reference: [15] <author> M. Perkowitz and O. Etzioni. </author> <title> Adaptive Web Sites: Automatically Synthesizing Web Pages. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence, </booktitle> <year> 1998. </year>
Reference-contexts: The frequency information on the arcs is ignored in this step for the sake of efficiency. By creating a sparse graph, and using efficient graph algorithms for cluster mining, we can identify high quality clusters substantially faster than by relying on traditional clustering methods <ref> [15] </ref>. <p> The webmaster is responsible for placing the new page at the site. 2.3 Time Complexity What is the running time of the PageGather algorithm? We summarize here; a more complete analysis is found in <ref> [15] </ref>. Let L be the number of page views in the log and N the number of pages at the site. In step (1), we must group the page views by their originating machine. We do this by sorting page views by origin and time, which requires O (LlogL) time. <p> As discussed above, cluster mining is a variation on traditional clustering. PageGather uses a graph-based clustering component which is specialized to cluster mining, but it is possible to adapt traditional clustering algorithms to this problem. In <ref> [15] </ref> and follow-up work, we compared PageGather's clustering component (both P G CLIQUE and P G CC variants) to two standard algorithms: K-means clustering [18] and hierarchical agglomerative clustering (HAC)[22]. There are literally hundreds of clustering algorithms and variations thereof.
Reference: [16] <author> E. Rasmussen. </author> <title> Clustering algorithms. </title> <editor> In W.B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval, </booktitle> <pages> pages 419-442. </pages> <publisher> Prentice Hall, </publisher> <address> Eaglewood Cliffs, N.J., </address> <year> 1992. </year>
Reference-contexts: Given a large access log, our task is to find collections of pages that tend to co-occur in visits. Clustering (see <ref> [22, 16, 24] </ref>) is a natural technique to consider for this task. In clustering, documents are represented in an N-dimensional space (for example, as word vectors). Roughly, a cluster is a collection of documents close to each other and relatively distant from other clusters.
Reference: [17] <author> T. Rist, E. Andre, and J. Muller. </author> <title> Adding Animated Presentation Agents to the Interface. </title> <booktitle> In Proceedings of the 1997 International Conference on Intelligent User Interfaces, </booktitle> <pages> pages 79-86, </pages> <address> Orlando, Florida, </address> <year> 1997. </year> <month> 16 </month>
Reference-contexts: With all of the site's content so encoded, its presentation may be easily adapted. A number of projects have explored client-side customization, in which a user has her own associated agent who learns about her interests and customizes her web experience accordingly. The AiA project <ref> [4, 17] </ref> explores the customization of web page information by adding a "presentation agent" who can direct the user's attention to topics of interest.
Reference: [18] <author> J. Rocchio. </author> <title> Document Retrieval Systems | Optimization and Evaluation. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1966. </year>
Reference-contexts: PageGather uses a graph-based clustering component which is specialized to cluster mining, but it is possible to adapt traditional clustering algorithms to this problem. In [15] and follow-up work, we compared PageGather's clustering component (both P G CLIQUE and P G CC variants) to two standard algorithms: K-means clustering <ref> [18] </ref> and hierarchical agglomerative clustering (HAC)[22]. There are literally hundreds of clustering algorithms and variations thereof. We chose K-means as it is particularly fast, and HAC as it is widely used. We found that PageGather's clustering component was faster than both HAC and K-means and found higher-quality clusters.
Reference: [19] <author> A. Savasere, E. Omiecinski, and S. Navathe. </author> <title> An Efficient Algorithm for Mining Association Rules in Large Databases. </title> <booktitle> In Proceedings of the 21st VLDB Conference, </booktitle> <year> 1995. </year>
Reference-contexts: We chose K-means as it is particularly fast, and HAC as it is widely used. We found that PageGather's clustering component was faster than both HAC and K-means and found higher-quality clusters. Frequent set algorithms are designed to find sets of similar items in large collections (see, for example, <ref> [1, 2, 19, 21] </ref>). We therefore compare PageGather to the standard Apriori algorithm for finding frequent sets (see [3]).
Reference: [20] <author> R. Segal. </author> <title> Data Mining as Massive Search. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1996. </year> <note> http://www.cs.washington.edu/homes/segal/brute.html. </note>
Reference-contexts: Furthermore, whereas traditional clustering is concerned with placing each document in exactly one cluster, cluster mining may place a single document in multiple overlapping clusters. The relationship between traditional clustering and cluster mining is parallel to that between classification and data mining as described in <ref> [20] </ref>. Segal contrasts mining "nuggets" | finding high-accuracy rules that capture patterns in the data | with traditional classification | classifying all examples as positive or negative | and shows that traditional classification algorithms do not make the best mining algorithms.
Reference: [21] <author> H. Toivonen. </author> <title> Sampling Large Databases for Association Rules. </title> <booktitle> In Proceedings of the 22nd VLDB Conference, </booktitle> <pages> pages 134-145, </pages> <year> 1996. </year>
Reference-contexts: We chose K-means as it is particularly fast, and HAC as it is widely used. We found that PageGather's clustering component was faster than both HAC and K-means and found higher-quality clusters. Frequent set algorithms are designed to find sets of similar items in large collections (see, for example, <ref> [1, 2, 19, 21] </ref>). We therefore compare PageGather to the standard Apriori algorithm for finding frequent sets (see [3]).
Reference: [22] <author> E.M. Voorhees. </author> <title> Implementing agglomerative hierarchical clustering algorithms for use in document retrieval. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 22 </volume> <pages> 465-476, </pages> <year> 1986. </year>
Reference-contexts: Given a large access log, our task is to find collections of pages that tend to co-occur in visits. Clustering (see <ref> [22, 16, 24] </ref>) is a natural technique to consider for this task. In clustering, documents are represented in an N-dimensional space (for example, as word vectors). Roughly, a cluster is a collection of documents close to each other and relatively distant from other clusters.
Reference: [23] <author> A. Wexelblat and P. Maes. Footprints: </author> <title> History-rich web browsing. </title> <booktitle> In Proc. Conf. Computer-Assisted Information Retrieval (RIAO), </booktitle> <pages> pages 75-84, </pages> <year> 1997. </year>
Reference-contexts: Firefly 4 uses a more individualized form of collaborative filtering in which members may rate hundreds of CDs or movies, building up a very detailed personal profile; Firefly then compares this profile with those of other members to make new recommendations. Footprints <ref> [23] </ref> takes an access-based transformation approach. Their motivating metaphor is that of travelers creating footpaths in the grass over time.

References-found: 23

