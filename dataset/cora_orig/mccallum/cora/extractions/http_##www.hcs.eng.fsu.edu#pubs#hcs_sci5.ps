URL: http://www.hcs.eng.fsu.edu/pubs/hcs_sci5.ps
Refering-URL: http://www.hcs.eng.fsu.edu/Pubs.html
Root-URL: 
Title: Parallel Processing Experiments on an SCI-based Workstation Cluster  
Author: Alan George, Robert Todd, William Phipps, Michael Miars, and Warren Rosen 
Date: Abstract  
Note: Rosen is with the Naval Air Warfare Center, Aircraft  
Address: Dr.  PA  
Affiliation: High-performance Computing and Simulation (HCS) Research Laboratory Electrical Engineering Department, FAMU-FSU College of Engineering Florida State University and Florida A&M University  Division, Warminster,  
Abstract: This paper describes the SCI-based workstation cluster system being developed at the HCS Research Lab and the parallel processing and network experiments that have been conducted and the results achieved. Using several different input sizes and degrees of partitioning and granularity for the parallel processing algorithms employed (i.e. matrix multiply and data sorting) and experimenting with both ring and switch-based topologies and other permutations (e.g. number of workstations), significant reductions in execution time have been achieved. These results help to illustrate what types of parallelism can be achieved today on SCI-based workstation clusters. 
Abstract-found: 1
Intro-found: 1
Reference: [ALNE93] <author> Alnes, K., </author> <title> Enabling Products for Cluster Computing using SCI, </title> <booktitle> Proceedings of the First International Workshop on SCI-based High-Performance Low-Cost Computing, </booktitle> <pages> pp. 58-64, </pages> <month> August, </month> <year> 1994. </year>
Reference: [DOLP95] <author> Dolphin Inc., </author> <title> 1 Gbit/sec SBus-SCI Cluster Adapter Card, White Paper, Dolphin Interconnect Solutions, </title> <month> March </month> <year> 1995. </year>
Reference-contexts: This allows the SCI card to take direct advantage of fast 64-byte transfers without the need for breaking apart a message. Sbus commands are mapped to SCI commands and back again using the simple table shown in Table 1. Table 1. Sbus to SCI Mapping <ref> [DOLP95] </ref> Sbus cmd Sbus size SCI Request SCI Response read () 2 0 - 2 3 bytes read_sb () response_16 write () 2 0 - 2 3 bytes write_sb () response_00 In addition to several instances of the SCI-based ring topology shown in Figure 3 (i.e. 2-node ring, 4-node ring, and
Reference: [GEOR95] <author> A.D. George, R.W. Todd, and W. Rosen, </author> <title> "A Cluster Testbed for SCI-based Parallel Processing," </title> <booktitle> Proceedings of the 3rd International. Workshop on SCI-based High-Performance Low-Cost Computing, </booktitle> <month> August </month> <year> 1995, </year> <pages> pp. 43-48. </pages>
Reference-contexts: One such platform is the system being developed at the HCS Research Lab which consists of SPARCstation computers connected by first-generation SCI/Sbus adapters in various permutations based upon ring and switch topologies <ref> [GEOR95] </ref>. In this paper we present the latest results achieved in adapting SCI/Sbus workstation clusters to the requirements of several parallel processing algorithms. By taking advantage of both shared-memory and message-passing modes of operation, execution times are significantly improved with the 1-Gbps SCI interconnect even despite the Sbus bottleneck. <p> Throughput and Latency Tests In order to gauge the potential performance benefits of the SCI/Sbus-1 interconnect in a workstation cluster environment, a number of basic benchmarking programs have been developed and their results measured and collected. In <ref> [GEOR95] </ref> these measurements were presented for a ring of SS5/85 workstations running SunOS 4.1.3. In this section we present the latest measurements for a ring of workstations running Solaris 2.4 which includes SS20/85, SS20/50, and SS5/85 systems as well as data from switched rings of these workstations.
Reference: [GUST95] <author> Gustavson, D.B. and Q. Li, </author> <title> Local-Area MultiProcessor: the Scalable Coherent Interface, </title> <booktitle> Proceedings of the Second International Workshop on SCI-based High-Performance Low-Cost Computing, </booktitle> <pages> pp. 131-154, </pages> <month> March, </month> <year> 1995. </year>
Reference-contexts: The Scalable Coherent Interface [SCI93] is one of the most promising interconnects for the design of future parallel processing systems in general and workstation clusters in particular <ref> [GUST95] </ref>. One such platform is the system being developed at the HCS Research Lab which consists of SPARCstation computers connected by first-generation SCI/Sbus adapters in various permutations based upon ring and switch topologies [GEOR95].
Reference: [MPIF93] <author> Message Passing Interface Forum, </author> <title> MPI: A Message Passing Interface, </title> <booktitle> Proceedings of Supercomputing 1993, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 878-883, </pages> <year> 1993. </year>
Reference-contexts: Among these is MPI which is used in this study for performance comparison purposes. The Message Passing Interface (MPI) specification was created by the Message Passing Interface Forum <ref> [MPIF93] </ref> will the goal of providing a portable parallel API that allows for efficient communication, heterogeneous implementations, convenient C and FORTRAN-77 language bindings, and consistency with current message-passing paradigm practices (such as PVM, NX, p4, etc.).
Reference: [SCI93] <institution> Scalable Coherent Interface, ANSI/IEEE Standard 1596-1992, IEEE Service Center, </institution> <address> Piscataway, New Jersey, </address> <year> 1993. </year>
Reference-contexts: The Scalable Coherent Interface <ref> [SCI93] </ref> is one of the most promising interconnects for the design of future parallel processing systems in general and workstation clusters in particular [GUST95].
References-found: 6

