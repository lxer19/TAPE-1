URL: http://www.isr.umd.edu/Labs/CACSE/FSQP/minimax.ps
Refering-URL: http://www.isr.umd.edu/Labs/CACSE/FSQP/fsqp.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: high end AN SQP ALGORITHM FOR FINELY DISCRETIZED CONTINUOUS MINIMAX PROBLEMS AND OTHER MINIMAX PROBLEMS
Author: JIAN L. ZHOUy AND ANDR E L. TITSz 
Keyword: Key words. continuous minimax, semi-infinite programming, many constraints, sequential quadratic programming, discretization, global convergence.  
Date: 461-487, May 1996 011  
Note: SIAM J. NUM. ANAL. c fl1996 Society for Industrial and Applied Mathematics Vol. 6, No. 2, pp.  AMS(MOS) subject classifications. 49M07, 49M37, 49M39, 65K05, 90C06, 90C30, 90C34  
Abstract: A common strategy for achieving global convergence in the solution of semi-infinite programming (SIP) problems, and in particular of continuous minimax problems, is to (approximately) solve a sequence of discretized problems, with a progressively finer discretization meshes. Finely discretized minimax and SIP problems, as well as other problems with many more objectives/constraints than variables, call for algorithms in which successive search directions are computed based on a small but significant subset of the objectives/constraints, with ensuing reduced computing cost per iteration and decreased risk of numerical difficulties. In this paper, an SQP-type algorithm is proposed that incorporates this idea in the particular case of minimax problems. The general case will be considered in a separate paper. The quadratic programming subproblem that yields the search direction involves only a small subset of the objective functions. This subset is updated at each iteration in such a way that global convergence is insured. Heuristics are suggested that take advantage of a possible close relationship between "adjacent" objective functions. Numerical results demonstrate the efficiency of the proposed algorithm.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. C. BIGGS, </author> <title> Constrained Minimization Using Recursive Equality Quadratic Programming, in Numerical Methods for Non-Linear Optimization, </title> <editor> F. A. Lootsma, ed., </editor> <publisher> Academic Press, </publisher> <address> London, New York, </address> <year> 1972, </year> <pages> pp. 411-428. </pages>
Reference-contexts: Various approaches have been proposed to circumvent these difficulties (see [18] for a recent survey). Some algorithms are based on the characterization of maximizers of (x; ) over <ref> [0; 1] </ref> in the neighborhood of a local solution of (SI) (see, e.g., [12], [17], [20], [34]). Under mild assumptions, the set of such maximizers contains a "small" number of points (for small n). <p> However global convergence, when insured at all, involves a potentially very costly line search ([5], [42]). A large class of globally convergent algorithms, on the other hand, is based on approximating [0;1] by means of progressively finer discretizations of <ref> [0; 1] </ref>, i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., [10], [13], [16], [27], [31], [32], [34], [38]). <p> Indeed, as these algorithms use quadratic programs as successive models, progress between (expensive) function evaluations is typically significantly better than with algorithms making use of mere linear systems of equations as models. In the context of SQP-type algorithms for the solution of problems with many constraints, Biggs <ref> [1] </ref> proposed to replace with equality constraints the active inequality constraints and to ignore all other inequality constraints in the computation of the search direction. <p> Lemma 3.15. The entire sequence fd k g converges to zero. This leads to the main result of this section Proposition 3.16. For k large enough, b (3:6) (x k + td k ; !) (x k ) ffthd k ; H k d k i 8t 2 <ref> [0; 1] </ref>; ! 2 n max (x fl ); and Proof. To prove the first claim, in view of Lemma 3.14, it suffices to show that, for k large enough, b k max (x fl ). <p> OET 4: (x; !) = exp (!) ~ 1 +~ 2 ! OET 5: (x; !) = ! (~ 4 (~ 1 ! 2 + ~ 2 ! + ~ 3 ) 2 ), I = <ref> [0:25; 1] </ref>; x 0 = (1; 1; 1; 1). 1+! (~ 1 exp (~ 3 !)+~ 2 exp (~ 4 !)), I = [0:5; 0:5]; x 0 = (1; 1; 3; 1). 1+! (~ 1 exp (~ 4 !)+~ 2 exp (~ 5 !)+~ 3 exp (~ 6 !)), I = <p> 4 !)), I = [0:5; 0:5]; x 0 = (1; 1; 3; 1). 1+! (~ 1 exp (~ 4 !)+~ 2 exp (~ 5 !)+~ 3 exp (~ 6 !)), I = [0:5; 0:5]; x 0 = HET-Z: (x; !) = (1 ! 2 ) (0:5x 2 2x!), I = <ref> [1; 1] </ref>; x 0 = 1. Problem PT is of the form minimize max !2I with (x; !) = (2! 2 1)x + !(1 !)(1 x), I = [0; 1] and x 0 = 5. <p> Problem PT is of the form minimize max !2I with (x; !) = (2! 2 1)x + !(1 !)(1 x), I = <ref> [0; 1] </ref> and x 0 = 5. To assess the efficiency of the scheme proposed in this paper, we compared the CFSQP implementation of Algorithm 2.1 with two algorithms differing from it only in the selection of k at each iteration. <p> Proof. In view of Assumption 1' and boundedness of fd k g, there exist c 1 &gt; 0 and c 2 &gt; 0 such that, for all ! 2 , all t 2 <ref> [0; 1] </ref> and all k, (x k + td k ; !) (x k ; !) + c 1 tkd k k (x k + td k ; !) (x k ; !) + thr x (x k ; !); d k i + c 2 t 2 kd k k <p> (x k ; !) + thr x (x k ; !); d k i + c 2 t 2 kd k k 2 : Thus, it follows from (2.2) applied to QP (x k ; H k ; k ) that, for all ! 2 k , all t 2 <ref> [0; 1] </ref> and all k, (1 t)(x k ; !) + tf (x k ; !) + hr x (x k ; !); d k ig + c 2 t 2 kd k k 2 !2 k = (1 t)(x k ; !) + t !2 k (1 t)(x k ) <p> Define the quadratic function in - Q (-) = 2 X k;! g k+1 (!)k 2 + -fl k+1 (! k ) X k;! fl k+1 (!) 1 k-g k+1 (! k ) + (1 -)p + k : Let - 2 <ref> [0; 1] </ref>. Let k;! , ! 2 k , be the KKT multipliers associated to QP (x k ; H k ; k ). <p> O (kp k p + 24 jian l. zhou and andr e l. tits On the other hand, since M &gt; 1, inequality (6.1) implies that kg k+1 (! k ) p + Substituting all these into (6.8) yields, for all k k 1 , k 2 K, - 2 <ref> [0; 1] </ref>, Q (-) 2 k + O (kp k p + = 2 k ) k k): In view of Lemma 6.2, since duality holds, jv k j = v k = 2 Thus, for all k k 1 , k 2 K, Q (-) jv k j + 2M <p> Since ~ff &lt; 1=2, jv k j M and M &gt; 1, it follows that -k 2 <ref> [0; 1] </ref> and thus, for all k k 1 , k 2 K, min Q (-) Q (-k ) jv k j 32M 2 c 2 jv k j 2 + O (j k k + j) + O (kp k p + Now, in view of Lemma 3.3 (i,iv) and
Reference: [2] <author> S. P. BOYD AND C. H. BARRATT, </author> <title> Linear Controller Design: Limits of Performance, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: 1. Introduction. Optimization problems that arise in engineering design often belong to the class of Semi-Infinite Programming (SIP) problems, i.e., they involve specifications that are to be satisfied over an interval of values of an independent parameter such as time, frequency, temperature or modeling error (see, e.g., <ref> [2] </ref>, [3], [30], [33]). <p> Problems OET 1 through OET 7 and HET-Z are of the form minimize max !2I with and I as follows (the ~'s are the components of x): OET 1: (x; !) = ! 2 (~ 1 ! + ~ 2 exp (!)), I = <ref> [0; 2] </ref>; x 0 = (1; 1). 1+! ~ 1 exp (~ 2 !), I = [0:5; 0:5]; x 0 = (1; 1).
Reference: [3] <author> R. K. BRAYTON, G. D. HACHTEL, and A. L. SANGIOVANNI-VINCENTELLI, </author> <title> A Survey of Optimization Techniques for Integrated Circuit Design, </title> <booktitle> IEEE Proc., 69 (1981), </booktitle> <pages> pp. 1334-1362. </pages>
Reference-contexts: 1. Introduction. Optimization problems that arise in engineering design often belong to the class of Semi-Infinite Programming (SIP) problems, i.e., they involve specifications that are to be satisfied over an interval of values of an independent parameter such as time, frequency, temperature or modeling error (see, e.g., [2], <ref> [3] </ref>, [30], [33]).
Reference: [4] <author> A. R. CONN AND Y. LI, </author> <title> A Structure-Exploiting Algorithm for Nonlinear Minimax Problems, </title> <note> SIAM J. Optimization, 2 (May, 1992 ), pp. 242-263. </note>
Reference-contexts: Again, however, in the case of finely discretized SIP problems, the number of constraints may be unduly large. Recently, Conn and Li <ref> [4] </ref> proposed a working set scheme for the minimax problem and obtained promising numerical results. Finally, in [41], Schittkowski proposes modifications of standard SQP methods for the solution of problems with many constraints.
Reference: [5] <author> I. D. COOPE AND G. A. WATSON, </author> <title> A Projected Lagrangian Algorithm for Semi-Infinite Programming, </title> <journal> Math. Programming, </journal> <volume> 32 (1985), </volume> <pages> pp. 337-356. </pages>
Reference: [6] <editor> H. A. ESCHENAUER, C. MATTHECK, and N. OLHOFF, eds., </editor> <booktitle> Engineering Optimization in Design Processes,, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year> <note> sqp for finely discretized minimax 27 </note>
Reference-contexts: The minimax problem (here, with finitely many objective functions) is an important special case of this problem. Examples of (MC) include mechanical design problems involving trusses (see, e.g., [37], [43] or papers in <ref> [6] </ref>, [25]). Note that there is no essential difference between (DSI) and (MC). Their similarity is particularly strong if the constraints in (MC) are "sequentially related" in the sense that the values taken by i are typically close to those taken by i+1 .
Reference: [7] <author> M. K. H. FAN, A. L. TITS, J. L. ZHOU, L. S. WANG, and J. KONINCKX, </author> <note> CONSOL-OPTCAD User's Manual (Version c1.2-s1.7, Released 8/1992), </note> <institution> Systems Research Center, University of Maryland, </institution> <type> Technical Report TR-87-212r4, </type> <institution> College Park, Maryland 20742, </institution> <year> 1992. </year>
Reference-contexts: There is no conceptual difficulty in extending the algorithm to tackle discretized versions of continuous minimax problems where the maximization is with respect to several free variables ranging over arbitrary compact sets. The proposed algorithm, with appropriate modifications, has been implemented in an optimization-based design package <ref> [7] </ref> and has proven very successful in solving various types of engineering design problems. 6. Appendix: Proofs. 6.1. Proof of Lemma 3.6.
Reference: [8] <author> A. V. FIACCO, </author> <title> Introduction to Sensitivity and Stability Analysis in Nonlinear Programming, </title> <publisher> in Academic Press, </publisher> <editor> R. Bellman, ed., </editor> <booktitle> Mathematics in Science and Engineering, </booktitle> <volume> 165, </volume> <year> 1983. </year>
Reference-contexts: (see, e.g., [14]), fl ! &gt; 0 for all ! 2 max (x fl ) and sqp for finely discretized minimax 11 with r 2 xx L (x fl ; fl ) = !2 max (x fl ) and The following result is standard for ordinary constrained problems (see, e.g., <ref> [8, Theorem 2.3.2] </ref>). A proof in the minimax case is given in the appendix for sake of completeness. Lemma 3.10. The point x fl is an isolated KKT point for (P). Proposition 3.11. The entire sequence fx k g converges to x fl . Proof.
Reference: [9] <author> P. E. GILL, W. MURRAY, M. A. SAUNDERS, and M. H. WRIGHT, </author> <title> User's Guide for QPSOL (Version 3.2): A Fortran Package for Quadratic Programming, </title> <institution> Systems Optimization Laboratory, Stanford University, </institution> <type> Technical Report SOL 84-6, </type> <institution> Stanford, </institution> <address> CA 94305, </address> <year> 1984. </year>
Reference-contexts: To investigate this issue, we conducted additional tests with QPSOL <ref> [9] </ref> (which allows for crash starts) replacing QLD. It was observed that a crash start is helpful only in the final iterations, when the active set is correctly identified.
Reference: [10] <author> C. GONZAGA AND E. POLAK, </author> <title> On Constraint Dropping Schemes and Optimality Functions for a Class of Outer Approximations Algorithms, </title> <journal> SIAM J. Control Optim., </journal> <volume> 17 (1979), </volume> <pages> pp. 477-493. </pages>
Reference-contexts: is based on approximating [0;1] by means of progressively finer discretizations of [0; 1], i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., <ref> [10] </ref>, [13], [16], [27], [31], [32], [34], [38]). The overall performance of these algorithms depends heavily on the performance at each discretization level, especially when q becomes large. Problem (DSI) involves finitely many smooth constraints and thus in principle can be solved by classical constrained optimization techniques.
Reference: [11] <author> C. GONZAGA, E. POLAK, and R. TRAHAN, </author> <title> An Improved Algorithm for Optimization Problems with Functional Inequality Constraints, </title> <journal> IEEE Trans. Automat. Control, </journal> <month> AC-25 </month> <year> (1980), </year> <pages> pp. 49-54. </pages>
Reference-contexts: Thus a compromise must be struck. Various heuristics come to mind (see, e.g., [41]). The current CFSQP implementation focusses on the frequent case where "adjacent" objectives are closely related (objectives are "sequentially related"). It follows the idea, used in <ref> [11] </ref>, [27], to include in k the set ``m * (x k ) of "*-active left local maximizers" at x k , for some * &gt; 0.
Reference: [12] <author> G. GRAMLICH, R. HETTICH, and E. W. SACHS, </author> <title> Local Convergence of SQP-Methods in Semi-Infinite Programming, </title> <note> SIAM J. Optimization, 1993 (to appear). </note>
Reference-contexts: Various approaches have been proposed to circumvent these difficulties (see [18] for a recent survey). Some algorithms are based on the characterization of maximizers of (x; ) over [0; 1] in the neighborhood of a local solution of (SI) (see, e.g., <ref> [12] </ref>, [17], [20], [34]). Under mild assumptions, the set of such maximizers contains a "small" number of points (for small n). The solution of the original problem can then be reduced to the solution of a problem involving approximations to these maximizers ! i (x).
Reference: [13] <author> S. A. GUSTAFSON, </author> <title> A Three-Phase Algorithm for Semi-Infinite Programs, in Semi-Infinite Programming and Applications, </title> <editor> A. V. Fiacco and K. O. Kortanek, eds., </editor> <booktitle> Lecture Notes in Economics and Mathematical Systems, </booktitle> <volume> 215, </volume> <publisher> Springer-Verlag, </publisher> <address> New York-Heidelberg-Berlin, </address> <year> 1983, </year> <pages> pp. 138-157. </pages>
Reference-contexts: based on approximating [0;1] by means of progressively finer discretizations of [0; 1], i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., [10], <ref> [13] </ref>, [16], [27], [31], [32], [34], [38]). The overall performance of these algorithms depends heavily on the performance at each discretization level, especially when q becomes large. Problem (DSI) involves finitely many smooth constraints and thus in principle can be solved by classical constrained optimization techniques.
Reference: [14] <author> S. P. HAN, </author> <title> Superlinear Convergence of a Minimax Method, </title> <institution> Department of Computer Science, Cornell University, TR78-336, </institution> <year> 1978. </year> <title> [15] , Variable Metric Methods for Minimizing a Class of Nondifferentiable Functions, </title> <journal> Math. Programming, </journal> <volume> 20 (1981), </volume> <pages> pp. 1-13. </pages>
Reference-contexts: where (1:1) 0 (x; d) = max f (x; !) + hr x (x; !); dig ^ (x) is a first order approximation to ^ (x + d) ^ (x), with ^ (x) = max (x; !): A line search (e.g., of Armijo type such as that suggested by Han <ref> [14] </ref>, [15]) is performed along direction d k to obtain a next iterate x k+1 = x k + t k d k , with t k 2 (0; 1]; H k is updated to H k+1 ; and a new subset k+1 of is constructed according to a scheme inspired <p> While the essence of the ideas put forth in this paper is independent of the specifics of this line search, for the sake of exposition, we will consider the case of an Armijo-type line search inspired from the line search used by Han <ref> [14] </ref>, [15]. <p> Set k = k + 1. Go back to Step 1. 3. Convergence Analysis. Although (P) takes the form of an ordinary min imax problem, the classical convergence analysis for such problems (e.g., <ref> [14] </ref>, [15]) cannot be directly applied to the present situation since, at each iteration, only a subset of the discretized set is employed to construct a search direction. 3.1. Global convergence. <p> Thus the KKT multipliers fl ! , ! 2 , corresponding to x fl , for problem (P), are unique. Assumption 5. The second order sufficiency conditions with strict complementary slackness are satisfied at x fl , i.e. (see, e.g., <ref> [14] </ref>), fl ! &gt; 0 for all ! 2 max (x fl ) and sqp for finely discretized minimax 11 with r 2 xx L (x fl ; fl ) = !2 max (x fl ) and The following result is standard for ordinary constrained problems (see, e.g., [8, Theorem 2.3.2]). <p> Thus H k is eventually updated at every iteration and the local behavior of Algorithm 2.1 becomes identical to that of the algorithm proposed by Han <ref> [14] </ref>, [15] (except for a different rule for selecting t k satisfying (2.4)). <p> Then (see <ref> [14] </ref>), the convergence rate is two-step superlinear, i.e., lim kx k+2 x fl k = 0: To achieve t k = 1 for k large enough, it is necessary to introduce a scheme to avoid the Maratos effect. <p> To assess the efficiency of the scheme proposed in this paper, we compared the CFSQP implementation of Algorithm 2.1 with two algorithms differing from it only in the selection of k at each iteration. In algorithm FULL, k = at each iteration, which essentially corresponds to Han's algorithm <ref> [14] </ref>, [15]. In algorithm *-ACT, a simple "*-active" scheme is used, specifically, k = * (x k ) for all k, with * = 0:1 (both for + and in the case of the first 8 problems).
Reference: [16] <author> R. HETTICH, </author> <title> An Implementation of a Discretization Method for Semi-Infinite Programming, </title> <journal> Math. Programming, </journal> <volume> 34 (1986), </volume> <pages> pp. 354-361. </pages>
Reference-contexts: on approximating [0;1] by means of progressively finer discretizations of [0; 1], i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., [10], [13], <ref> [16] </ref>, [27], [31], [32], [34], [38]). The overall performance of these algorithms depends heavily on the performance at each discretization level, especially when q becomes large. Problem (DSI) involves finitely many smooth constraints and thus in principle can be solved by classical constrained optimization techniques.
Reference: [17] <author> R. HETTICH AND W. van HONSTEDE, </author> <title> On Quadratically Convergent Methods for Semi-Infinite Programming, in Semi-Infinite Programming, </title> <editor> R. Hettich, ed., </editor> <booktitle> Lecture Notes in Control and Information Sciences, </booktitle> <volume> 15, </volume> <publisher> Springer Verlag, </publisher> <year> 1979, </year> <pages> pp. 97-111. </pages>
Reference-contexts: Various approaches have been proposed to circumvent these difficulties (see [18] for a recent survey). Some algorithms are based on the characterization of maximizers of (x; ) over [0; 1] in the neighborhood of a local solution of (SI) (see, e.g., [12], <ref> [17] </ref>, [20], [34]). Under mild assumptions, the set of such maximizers contains a "small" number of points (for small n). The solution of the original problem can then be reduced to the solution of a problem involving approximations to these maximizers ! i (x).
Reference: [18] <author> R. HETTICH AND K. O. KORTANEK, </author> <title> Semi-Infinite Programming: Theory, Methods, </title> <journal> and Applications, SIAM Rev., </journal> <volume> 35 (1993), </volume> <pages> pp. 380-429. </pages>
Reference-contexts: Various approaches have been proposed to circumvent these difficulties (see <ref> [18] </ref> for a recent survey). Some algorithms are based on the characterization of maximizers of (x; ) over [0; 1] in the neighborhood of a local solution of (SI) (see, e.g., [12], [17], [20], [34]).
Reference: [19] <author> R. HETTICH AND P. ZENCKE, </author> <title> Numerische Methoden der Approximation und Semi-Infiniten Optimierung, </title> <publisher> Teubner Studienbucher, </publisher> <year> 1982. </year>
Reference-contexts: The numerical results reported below were obtained on discretized versions of nine test problems borrowed from the literature. Problems OET 1 through OET 7 are 16 jian l. zhou and andr e l. tits taken from [26], HET-Z from <ref> [19] </ref>, and PT from [34].
Reference: [20] <author> W. van HONSTEDE, </author> <title> An Approximation Method for Semi-Infinite Problems, in Semi-Infinite Programming, </title> <editor> R. Hettich, ed., </editor> <booktitle> Lecture Notes in Control and Information Sciences, </booktitle> <volume> 15, </volume> <publisher> Springer Verlag, </publisher> <year> 1979, </year> <pages> pp. 126-136. </pages>
Reference-contexts: Various approaches have been proposed to circumvent these difficulties (see [18] for a recent survey). Some algorithms are based on the characterization of maximizers of (x; ) over [0; 1] in the neighborhood of a local solution of (SI) (see, e.g., [12], [17], <ref> [20] </ref>, [34]). Under mild assumptions, the set of such maximizers contains a "small" number of points (for small n). The solution of the original problem can then be reduced to the solution of a problem involving approximations to these maximizers ! i (x).
Reference: [21] <author> K. C. KIWIEL, </author> <title> Methods of Descent in Nondifferentiable Optimization, </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> 1133, </volume> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New-York, Tokyo, </address> <year> 1985. </year>
Reference-contexts: In [27], it is shown that only a small subset of these points need be used, by suitably detecting "critical" values of ! and "remembering" them from iteration to iteration in a manner reminiscent of bundle type methods in nonsmooth optimization (see, e.g., <ref> [21] </ref>, [23]). Specifically, at iteration k, a first order direction d k is computed using a certain subset k of . <p> The next lemma, which is the same as Lemma 4.7 in <ref> [21, Chapter 3] </ref>, is central to the proof of global convergence. Lemma 3.5. Let x fl 2 IR n be such that lim inf maxfjv k j; kx k x fl kg = 0: sqp for finely discretized minimax 9 Then, x fl is a KKT point for (P). Proof. <p> Thus, the conclusion follows from Lemma 3.4. The establishment of the global convergence of Algorithm 2.1 employs a contradiction argument inspired from <ref> [21, Chapter 3] </ref>. <p> A careful repeated application of this argument shows that jv k j becomes smaller than * fl on a sequence at "finite distance" from K, a contradiction. The proof of the following lemma is inspired from that of Lemma 4.11 in <ref> [21, Chapter 3] </ref> (see also the proof of Lemma 3.15 in [44]) and is given in the appendix. <p> As in <ref> [21, Chapter 3] </ref>, using the dual of QP (x k ; H k ; k ) facilitates the analysis. Lemma 6.2.
Reference: [22] <author> C. T. LAWRENCE, J. L. ZHOU, and A. L. </author> <title> TITS, User's Guide for CFSQP Version 2.0: A C Code for Solving Constrained Nonlinear (Minimax) Optimization Problems, Generating Iterates Satisfying All Inequality and Linear Constraints., </title> <institution> Institute for Systems Research, University of Maryland, TR-94-16, College Park, MD 20742, </institution> <note> 1994. </note> <author> 28 jian l. zhou and andr e l. </author> <note> tits </note>
Reference-contexts: It follows that two-step superlinear convergence is preserved when (3.8) holds. 4. Implementation and numerical results. An efficient implementation of Algorithm 2.1, including the Maratos effect avoidance scheme described at the end of x3, has been developed as part of a C code dubbed CFSQP <ref> [22] </ref>. 7 Version 2.0 of CFSQP was used to perform the numerical tests described below. The specifics of the CFSQP implementation are as follows. In Algorithm 2.1, the rule for updating k only specifies that it must contain a certain subset of "critical" points of .
Reference: [23] <author> C. LEMAR ECHAL, </author> <title> Nondifferentiable Optimization, in Optimization, </title> <editor> G. Nemhauser, A. Rinnooy-Kan, and M. Todd, eds., </editor> <booktitle> Handbooks in Operations Research and Management Science, </booktitle> <publisher> North Holland, </publisher> <year> 1989. </year>
Reference-contexts: In [27], it is shown that only a small subset of these points need be used, by suitably detecting "critical" values of ! and "remembering" them from iteration to iteration in a manner reminiscent of bundle type methods in nonsmooth optimization (see, e.g., [21], <ref> [23] </ref>). Specifically, at iteration k, a first order direction d k is computed using a certain subset k of .
Reference: [24] <author> H. MINE, M. FUKUSHIMA, and Y. TANAKA, </author> <title> On the Use of *-Most-Active Constraints in an Exact Penalty Function Method for Nonlinear Optimization, </title> <journal> IEEE Trans. Automat. Control, </journal> <month> AC-29 </month> <year> (1984), </year> <pages> pp. 1040-1042. </pages>
Reference-contexts: Much later, Polak and Tits [34] and Mine et al. <ref> [24] </ref> adapted the "*-active" idea to the SQP context, and Pow-ell [36] proposed a "tolerant" algorithm for linearly constrained problems, which also borrows from the "*-active" concept. Again, however, in the case of finely discretized SIP problems, the number of constraints may be unduly large.
Reference: [25] <author> C. A. MOTA SOARES, ed., </author> <title> Computer-Aided Optimal Design: Structural and Mechanical Systems, </title> <booktitle> NATO ASI Series, </booktitle> <publisher> Series F , 27 , Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987. </year>
Reference-contexts: The minimax problem (here, with finitely many objective functions) is an important special case of this problem. Examples of (MC) include mechanical design problems involving trusses (see, e.g., [37], [43] or papers in [6], <ref> [25] </ref>). Note that there is no essential difference between (DSI) and (MC). Their similarity is particularly strong if the constraints in (MC) are "sequentially related" in the sense that the values taken by i are typically close to those taken by i+1 .
Reference: [26] <author> K. OETTERSHAGEN, </author> <title> Ein Superlinear Konvergenter Algorithmus zur Losung Semi-Infiniter Optimierungsprobleme, </title> <type> Ph.D. Thesis, </type> <institution> Bonn University, </institution> <year> 1982. </year>
Reference-contexts: The numerical results reported below were obtained on discretized versions of nine test problems borrowed from the literature. Problems OET 1 through OET 7 are 16 jian l. zhou and andr e l. tits taken from <ref> [26] </ref>, HET-Z from [19], and PT from [34].
Reference: [27] <author> E. R. PANIER AND A. L. </author> <title> TITS, A Globally Convergent Algorithm with Adaptively Refined Discretization for Semi-Infinite Optimization Problems Arising in Engineering Design, </title> <journal> IEEE Trans. Automat. Control, </journal> <month> AC-34 </month> <year> (1989), </year> <pages> pp. </pages> <month> 903-908. </month> <title> [28] , A Superlinearly Convergent Method of Feasible Directions for Optimization Problems Arising in the Design of Engineering Systems, </title> <booktitle> in Proc. of the Seventh International Conference on Analysis and Optimization of Systems | Antibes, </booktitle> <month> June 25-27, </month> <year> 1986, </year> <editor> A. Bensoussan and J. L. Lions, eds., </editor> <booktitle> Lecture Notes in Control and Information Sciences, </booktitle> <volume> 83, </volume> <publisher> Springer Verlag, </publisher> <address> Berlin-Heidelberg-New York-Tokyo, </address> <month> June </month> <year> 1986, </year> <pages> pp. 65-73. </pages>
Reference-contexts: approximating [0;1] by means of progressively finer discretizations of [0; 1], i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., [10], [13], [16], <ref> [27] </ref>, [31], [32], [34], [38]). The overall performance of these algorithms depends heavily on the performance at each discretization level, especially when q becomes large. Problem (DSI) involves finitely many smooth constraints and thus in principle can be solved by classical constrained optimization techniques. <p> Note that there is no essential difference between (DSI) and (MC). Their similarity is particularly strong if the constraints in (MC) are "sequentially related" in the sense that the values taken by i are typically close to those taken by i+1 . In [32], <ref> [27] </ref>, (DSI) is solved by means of first order (thus, slow) methods. <p> When the discretization is fine, however, the set of such points is often unduly large as it contains entire neighborhoods of local maximizers. In <ref> [27] </ref>, it is shown that only a small subset of these points need be used, by suitably detecting "critical" values of ! and "remembering" them from iteration to iteration in a manner reminiscent of bundle type methods in nonsmooth optimization (see, e.g., [21], [23]). <p> This scheme is shown in <ref> [27] </ref> to induce global convergence. It is efficient because, under mild assumptions, the dimension of the quadratic programming problem that yields d k is moderate, and gradient evaluations are only required at a few grid points. <p> It is efficient because, under mild assumptions, the dimension of the quadratic programming problem that yields d k is moderate, and gradient evaluations are only required at a few grid points. However, at each level of discretization (i.e., for each fixed q), the algorithm proposed in <ref> [27] </ref> (like that proposed in [32]) exhibits at best a linear rate of convergence. <p> However, no convergence analysis is provided; in practice global convergence may or may not take place, depending on the heuristics used to update an active working set of constraints. In this paper, we propose and analyze an SQP-type algorithm based on the scheme introduced in <ref> [27] </ref> for the special case of the discretized minimax problem (P) minimize max !2 where is again a finite set. The general discretized SIP case involves additional intrinsic difficulties and will be considered in a separate paper. <p> direction d k to obtain a next iterate x k+1 = x k + t k d k , with t k 2 (0; 1]; H k is updated to H k+1 ; and a new subset k+1 of is constructed according to a scheme inspired from that used in <ref> [27] </ref>. In particular, if t k &lt; 1, k+1 includes a point ! k that caused the last trial point to be rejected by the line search. However, in the present context, a difficulty arises. Suppose ! k was not in k . <p> Suppose ! k was not in k . The rationale for including it in k+1 is that, had it been included in k , a larger step would likely have been accepted (since ! k is now preventing a larger step). In the context of <ref> [27] </ref> where a first order search direction is used (i.e., H k = I for all k), it follows that d k+1 will likely allow a larger step to be taken. <p> We are now ready to make precise the rule for updating k . Following <ref> [27] </ref>, k+1 contains the union of three sets. 3 Given x 2 R n , let max (x) = f! 2 : (x; !) = (x)g be the set of maximizers of (x; ). The first component of k+1 is max (x k+1 ). <p> Thus x k+1 = x k + t k d k , where t k is the largest number t in f1; fi; fi 2 ; : : :g satisfying (2:3) (x k + td k ) (x k ) ffthd k ; H k d k i; 3 In <ref> [27] </ref>, k+1 is set to be equal to this union. 6 jian l. zhou and andr e l. tits where ff 2 (0; 1=2) and fi 2 (0; 1) are fixed. <p> A next search direction taking this into account is called for. Thus, k+1 will include some ! k such that (x k + fi t k hd k ; H k d k i: Finally, to avoid zigzagging (which could prevent global convergence; see the example in <ref> [27] </ref>) it is important that key elements in k be kept in k+1 . <p> Thus a compromise must be struck. Various heuristics come to mind (see, e.g., [41]). The current CFSQP implementation focusses on the frequent case where "adjacent" objectives are closely related (objectives are "sequentially related"). It follows the idea, used in [11], <ref> [27] </ref>, to include in k the set ``m * (x k ) of "*-active left local maximizers" at x k , for some * &gt; 0.
Reference: [29] <author> E. POLAK, </author> <title> Computational Methods in Optimization, </title> <publisher> Academic Press, </publisher> <address> New York, N.Y., </address> <year> 1971. </year> <note> [30] , On the Mathematical Foundations of Nondifferentiable Optimization, SIAM Rev., </note> <month> 29 </month> <year> (1987), </year> <pages> pp. 21-89. </pages>
Reference-contexts: In [32], [27], (DSI) is solved by means of first order (thus, slow) methods. In [32], based on ideas of Zoutendijk [46] and Polak <ref> [29, Section 4.3] </ref>, the construction of the search direction at iteration k makes use of the gradients r x (x k ; !) at all points ! 2 at which (x k ; !) * ("*-active" constraints), where * &gt; 0 is appropriately sqp for finely discretized minimax 3 small.
Reference: [31] <author> E. POLAK AND L. </author> <title> HE, Rate Preserving Discretization Strategies for Semi-Infinite Programming and Optimal Control, </title> <booktitle> in Proceedings of the 29th IEEE Conference on Decision and Control, </booktitle> <month> December </month> <year> 1990, </year> <pages> pp. 2444-2449. </pages>
Reference-contexts: [0;1] by means of progressively finer discretizations of [0; 1], i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., [10], [13], [16], [27], <ref> [31] </ref>, [32], [34], [38]). The overall performance of these algorithms depends heavily on the performance at each discretization level, especially when q becomes large. Problem (DSI) involves finitely many smooth constraints and thus in principle can be solved by classical constrained optimization techniques.
Reference: [32] <author> E. POLAK AND D. Q. MAYNE, </author> <title> An Algorithm for Optimization Problems with Functional Inequality Constraints, </title> <journal> IEEE Trans. Automat. Control, </journal> <volume> 21 (1976), </volume> <pages> pp. 184-193. </pages>
Reference-contexts: by means of progressively finer discretizations of [0; 1], i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., [10], [13], [16], [27], [31], <ref> [32] </ref>, [34], [38]). The overall performance of these algorithms depends heavily on the performance at each discretization level, especially when q becomes large. Problem (DSI) involves finitely many smooth constraints and thus in principle can be solved by classical constrained optimization techniques. <p> Note that there is no essential difference between (DSI) and (MC). Their similarity is particularly strong if the constraints in (MC) are "sequentially related" in the sense that the values taken by i are typically close to those taken by i+1 . In <ref> [32] </ref>, [27], (DSI) is solved by means of first order (thus, slow) methods. In [32], based on ideas of Zoutendijk [46] and Polak [29, Section 4.3], the construction of the search direction at iteration k makes use of the gradients r x (x k ; !) at all points ! 2 <p> Their similarity is particularly strong if the constraints in (MC) are "sequentially related" in the sense that the values taken by i are typically close to those taken by i+1 . In <ref> [32] </ref>, [27], (DSI) is solved by means of first order (thus, slow) methods. In [32], based on ideas of Zoutendijk [46] and Polak [29, Section 4.3], the construction of the search direction at iteration k makes use of the gradients r x (x k ; !) at all points ! 2 at which (x k ; !) * ("*-active" constraints), where * &gt; 0 is <p> However, at each level of discretization (i.e., for each fixed q), the algorithm proposed in [27] (like that proposed in <ref> [32] </ref>) exhibits at best a linear rate of convergence.
Reference: [33] <author> E. POLAK, D. Q. MAYNE, and D. M. STIMLER, </author> <title> Control System Design via Semi-Infinite Optimization: A Review, </title> <booktitle> IEEE Proc., 72 (1984), </booktitle> <pages> pp. 1777-1794. </pages>
Reference-contexts: 1. Introduction. Optimization problems that arise in engineering design often belong to the class of Semi-Infinite Programming (SIP) problems, i.e., they involve specifications that are to be satisfied over an interval of values of an independent parameter such as time, frequency, temperature or modeling error (see, e.g., [2], [3], [30], <ref> [33] </ref>).
Reference: [34] <author> E. POLAK AND A. L. </author> <title> TITS, A Recursive Quadratic Programming Algorithm for Semi-Infinite Optimization Problems, </title> <journal> Appl. Math. Optim., </journal> <volume> 8 (1982), </volume> <pages> pp. 325-349. </pages>
Reference-contexts: Various approaches have been proposed to circumvent these difficulties (see [18] for a recent survey). Some algorithms are based on the characterization of maximizers of (x; ) over [0; 1] in the neighborhood of a local solution of (SI) (see, e.g., [12], [17], [20], <ref> [34] </ref>). Under mild assumptions, the set of such maximizers contains a "small" number of points (for small n). The solution of the original problem can then be reduced to the solution of a problem involving approximations to these maximizers ! i (x). <p> means of progressively finer discretizations of [0; 1], i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., [10], [13], [16], [27], [31], [32], <ref> [34] </ref>, [38]). The overall performance of these algorithms depends heavily on the performance at each discretization level, especially when q becomes large. Problem (DSI) involves finitely many smooth constraints and thus in principle can be solved by classical constrained optimization techniques. <p> In the context of SQP-type algorithms for the solution of problems with many constraints, Biggs [1] proposed to replace with equality constraints the active inequality constraints and to ignore all other inequality constraints in the computation of the search direction. Much later, Polak and Tits <ref> [34] </ref> and Mine et al. [24] adapted the "*-active" idea to the SQP context, and Pow-ell [36] proposed a "tolerant" algorithm for linearly constrained problems, which also borrows from the "*-active" concept. Again, however, in the case of finely discretized SIP problems, the number of constraints may be unduly large. <p> The numerical results reported below were obtained on discretized versions of nine test problems borrowed from the literature. Problems OET 1 through OET 7 are 16 jian l. zhou and andr e l. tits taken from [26], HET-Z from [19], and PT from <ref> [34] </ref>.
Reference: [35] <author> M. J. D. POWELL, </author> <title> A Fast Algorithm for Nonlinearly Constrained Optimization Calculations, in Numerical Analysis, </title> <note> Dundee, 1977, Lecture Notes in Mathematics 630, </note> <editor> G. A. Watson, ed., </editor> <publisher> Springer-Verlag, </publisher> <year> 1978, </year> <pages> pp. </pages> <month> 144-157. </month> <title> [36] , A Tolerant Algorithm for Linearly Constrained Optimization Calculations, </title> <journal> Math. Programming, </journal> <volume> 45 (1989), </volume> <pages> pp. 547-566. </pages>
Reference-contexts: For the solution of the QP subproblems, CFSQP invokes QLD, a code due to Powell and Schittkowski [40]. H k is updated using the BFGS formula with Powell's modification <ref> [35] </ref> with the following stipulations: the evaluation of the gradient of the Lagrangian function is based on the KKT multipliers corresponding to the QP subproblem and multipliers associated with values of ! not used in the QP are set to 0.
Reference: [37] <author> J. RAKOWSKA, R. T. HAFTKA, and L. T. WATSON, </author> <title> Multi-Objective Control-Structure Optimization Via Homotopy Methods, </title> <journal> SIAM J. Optimization, </journal> <volume> 3 (1993), </volume> <pages> pp. 654-667. </pages>
Reference-contexts: The minimax problem (here, with finitely many objective functions) is an important special case of this problem. Examples of (MC) include mechanical design problems involving trusses (see, e.g., <ref> [37] </ref>, [43] or papers in [6], [25]). Note that there is no essential difference between (DSI) and (MC). Their similarity is particularly strong if the constraints in (MC) are "sequentially related" in the sense that the values taken by i are typically close to those taken by i+1 .
Reference: [38] <author> R. REEMTSEN, </author> <title> Discretization methods for the solution of semi-infinite programming problems, </title> <journal> J. Optim. Theory Appl., </journal> <volume> 71 (1991), </volume> <pages> pp. 85-103. </pages>
Reference-contexts: of progressively finer discretizations of [0; 1], i.e., substituting for (SI) the problems (DSI) minimize f (x) s.t. (x; !) 0 8! 2 with, for instance, = f0; q 2 ; ; q where q, a positive integer, is progressively increased (see, e.g., [10], [13], [16], [27], [31], [32], [34], <ref> [38] </ref>). The overall performance of these algorithms depends heavily on the performance at each discretization level, especially when q becomes large. Problem (DSI) involves finitely many smooth constraints and thus in principle can be solved by classical constrained optimization techniques.
Reference: [39] <author> S. M. ROBINSON, </author> <title> Perturbed Kuhn-Tucker Points and Rates of Convergence for a Class of Nonlinear-Programming Algorithms, </title> <journal> Math. Programming, </journal> <volume> 7 (1974), </volume> <pages> pp. 1-16. </pages> <note> sqp for finely discretized minimax 29 </note>
Reference-contexts: Since max (x fl ) ^ , QP (x fl ; H fl ; ^ ) has d = 0 as its unique solution (Lemma 3.2). It follows from <ref> [39, Theorem 2.1] </ref> that fd k g ! 0 as k ! 1, k 2 K 00 , contradicting the fact that fd k g is bounded away from zero on K 0 . Lemma 3.14. For k large enough, max (x fl ) b k . Proof.
Reference: [40] <author> K. SCHITTKOWSKI, </author> <title> QLD : A FORTRAN Code for Quadratic Programming, User's Guide, </title> <institution> Mathematisches Institut, Universitat Bayreuth, Germany, </institution> <year> 1986. </year> <title> [41] , Solving Nonlinear Programming Problems with Very Many Constraints, </title> <institution> Mathematisches Institut, Universitat Bayreuth, </institution> <type> Report No. 294, </type> <address> Bayreuth, Germany, </address> <year> 1991. </year>
Reference-contexts: The following parameter values are used in CFSQP: ff = 0:1, fi = 0:5, ffi is the square root of the machine precision, and * = 1 (in ``m * (x)). For the solution of the QP subproblems, CFSQP invokes QLD, a code due to Powell and Schittkowski <ref> [40] </ref>.
Reference: [42] <author> G. A. WATSON, </author> <title> Globally Convergent Methods for Semi-Infinite Programming, </title> <journal> BIT, </journal> <volume> 21 (1981), </volume> <pages> pp. 362-373. </pages>
Reference-contexts: Application of Newton's method, or of a Sequential Quadratic Programming (SQP) method to the reduced problem (with constraints (x; ! i (x)) 0) brings about a fast local rate of convergence. However global convergence, when insured at all, involves a potentially very costly line search ([5], <ref> [42] </ref>).
Reference: [43] <author> W. ZHAO AND S. AZARM, </author> <title> A Cross-Sectional Shape Multiplier Method for Two-Level Optimum Design of Frames, in Advances in Design Automation, Volume II Optimal Design and Mechanical Systems Analysis, </title> <publisher> ASME Publication DE, </publisher> <pages> 23-2, </pages> <month> September </month> <year> 1990, </year> <pages> pp. 197-205. </pages>
Reference-contexts: The minimax problem (here, with finitely many objective functions) is an important special case of this problem. Examples of (MC) include mechanical design problems involving trusses (see, e.g., [37], <ref> [43] </ref> or papers in [6], [25]). Note that there is no essential difference between (DSI) and (MC). Their similarity is particularly strong if the constraints in (MC) are "sequentially related" in the sense that the values taken by i are typically close to those taken by i+1 .
Reference: [44] <author> J. L. ZHOU, </author> <title> Fast, Globally Convergent Optimization Algorithms, with Application to Engineering System Design, </title> <institution> University of Maryland, </institution> <type> PhD Thesis, </type> <institution> College Park, MD 20742, </institution> <year> 1992. </year>
Reference-contexts: It will be shown that this can be avoided by incorporating in the basic algorithm standard techniques such as a second order correction (see, e.g., [28], <ref> [44] </ref>). The algorithm stated and analyzed below (Algorithm 2.1) allows that additional !'s be included in k at each iteration. Clever heuristics may significantly speed up the algorithm, especially in early iterations. <p> The proof of the following lemma is inspired from that of Lemma 4.11 in [21, Chapter 3] (see also the proof of Lemma 3.15 in <ref> [44] </ref>) and is given in the appendix.
Reference: [45] <author> J. L. ZHOU AND A. L. </author> <title> TITS, Nonmonotone Line Search for Minimax Problems, </title> <journal> J. Optim. Theory Appl., </journal> <volume> 76 (1993), </volume> <pages> pp. 455-476. </pages>
Reference-contexts: One option is to adopt a second order correction such as that used in [28] and <ref> [45] </ref> (in the latter, it is combined with a "nonmonotone line search"; using such line search here would entail a more complicated analysis). Specifically, Step 1 (ii) in Algorithm 2.1 is replaced with the following. Step 1 (ii 1 ). <p> Thus the modified algorithm will eventually behave as if solving (3.7) with, at each iteration, k selected to be equal to max (x fl ) and H k normally updated. It is shown in <ref> [45] </ref> that, if (3.8) holds, the step t k = 1 will always be accepted for k large enough (in fact the proof in [45] makes use of the weaker assumption lim kd T kd k k 2 = 0: It is also shown in [45] that ~ d k = <p> It is shown in <ref> [45] </ref> that, if (3.8) holds, the step t k = 1 will always be accepted for k large enough (in fact the proof in [45] makes use of the weaker assumption lim kd T kd k k 2 = 0: It is also shown in [45] that ~ d k = O (kd k k 2 ). It follows that two-step superlinear convergence is preserved when (3.8) holds. 4. Implementation and numerical results. <p> It is shown in <ref> [45] </ref> that, if (3.8) holds, the step t k = 1 will always be accepted for k large enough (in fact the proof in [45] makes use of the weaker assumption lim kd T kd k k 2 = 0: It is also shown in [45] that ~ d k = O (kd k k 2 ). It follows that two-step superlinear convergence is preserved when (3.8) holds. 4. Implementation and numerical results.
Reference: [46] <author> G. ZOUTENDIJK, </author> <title> Methods of Feasible Directions, </title> <publisher> Elsevier Scientific Publishing Company, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1960. </year>
Reference-contexts: In [32], [27], (DSI) is solved by means of first order (thus, slow) methods. In [32], based on ideas of Zoutendijk <ref> [46] </ref> and Polak [29, Section 4.3], the construction of the search direction at iteration k makes use of the gradients r x (x k ; !) at all points ! 2 at which (x k ; !) * ("*-active" constraints), where * &gt; 0 is appropriately sqp for finely discretized minimax
References-found: 41

