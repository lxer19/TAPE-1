URL: http://robotics.stanford.edu/~ronnyk/dtable.ps.gz
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fronnyk,sommdag@engr.sgi.com  
Title: Targeting Business Users with Decision Table Classifiers  
Author: Ron Kohavi and Daniel Sommerfield 
Address: 2011 N. Shoreline Blvd Mountain View, CA 94043-1389  
Affiliation: Data Mining and Visualization Silicon Graphics, Inc.  
Note: To Appear in KDD-98  
Abstract: Business users and analysts commonly use spreadsheets and 2D plots to analyze and understand their data. On-line Analytical Processing (OLAP) provides these users with added flexibility in pivoting data around different attributes and drilling up and down the multi-dimensional cube of aggregations. Machine learning researchers, however, have concentrated on hypothesis spaces that are foreign to most users: hyper-planes (Perceptrons), neural networks, Bayesian networks, decision trees, nearest neighbors, etc. In this paper we advocate the use of decision table classifiers that are easy for line-of-business users to understand. We describe several variants of algorithms for learning decision tables, compare their performance, and describe a visualization mechanism that we have implemented in MineSet. The performance of decision tables is comparable to other known algorithms, such as C4.5/C5.0, yet the resulting classifiers use fewer attributes and are more comprehensible. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: We used decision tables for classification, but regression is a possible extension. The visualizer naturally extends to regression, but the entropy-based attribute selection needs to be replaced by another criteria since it relies on discrete labels. Common regression-tree measures <ref> (Breiman et al. 1984) </ref> might work well. Techniques for multiple model generation, such as Bagging Boosting, or Option Trees might be helpful, especially if they yield different top-level attributes and users can select between them.
Reference: <author> Cover, T. M., and Thomas, J. A. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1993. </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1022-1027. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-90. </pages>
Reference: <author> Kohavi, R., and John, G. H. </author> <year> 1997. </year> <title> Wrappers for feature subset selection. </title> <journal> Artificial Intelligence 97(1-2):273-324. </journal>
Reference-contexts: At the time, a new On-Line Analytical Processing (OLAP) system was being fielded in our company, and our clients found the projections of multi-dimensional cubes onto spreadsheets very intuitive. Our prior work with decision tables (Kohavi 1995a; 1995b) and General Logic Diagrams <ref> (Kohavi, Sommerfield, & Dougherty 1997) </ref> was encouraging and we decided to develop the algorithm and the visualizations further. The prior work built decision tables by conducting a wrapper-based attribute selection, starting from the empty set of attributes. Our contributions in this paper include: 1. <p> Jay Desouza was funded by Silicon Graphics for his work. We wish to thank Rick Kufrin from NCSA for his initial help. The work for this paper was done using MLC ++ <ref> (Kohavi, Sommerfield, & Dougherty 1997) </ref>.
Reference: <author> Kohavi, R., and Li, C.-H. </author> <year> 1995. </year> <title> Oblivious decision trees, graphs, and top-down pruning. </title> <editor> In Mellish, C. S., ed., </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1071-1077. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A different way of storing the data using an oblivious decision tree structure <ref> (Kohavi & Li 1995) </ref>, which not only provides an efficient structure for building decision tables, but also provides support for the above mechanism and extends much better to parallel im plementations. 4. An empirical evaluation of several variants for select ing attributes. 5.
Reference: <author> Kohavi, R., and Sahami, M. </author> <year> 1996. </year> <title> Error-based and entropy-based discretization of continuous features. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> 114-119. </pages>
Reference: <author> Kohavi, R.; Sommerfield, D.; and Dougherty, J. </author> <year> 1997. </year> <title> Data mining using MLC ++ : A machine learning library in C ++ . International Journal on Artificial Intelligence Tools 6(4) </title> <address> 537-566. http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: At the time, a new On-Line Analytical Processing (OLAP) system was being fielded in our company, and our clients found the projections of multi-dimensional cubes onto spreadsheets very intuitive. Our prior work with decision tables (Kohavi 1995a; 1995b) and General Logic Diagrams <ref> (Kohavi, Sommerfield, & Dougherty 1997) </ref> was encouraging and we decided to develop the algorithm and the visualizations further. The prior work built decision tables by conducting a wrapper-based attribute selection, starting from the empty set of attributes. Our contributions in this paper include: 1. <p> Jay Desouza was funded by Silicon Graphics for his work. We wish to thank Rick Kufrin from NCSA for his initial help. The work for this paper was done using MLC ++ <ref> (Kohavi, Sommerfield, & Dougherty 1997) </ref>.
Reference: <author> Kohavi, R. </author> <year> 1995a. </year> <title> The power of decision tables. </title> <editor> In Lavrac, N., and Wrobel, S., eds., </editor> <booktitle> Proceedings of the European Conference on Machine Learning, Lecture Notes in Artificial Intelligence 914, </booktitle> <pages> 174-189. </pages> <address> Berlin, Heidelberg, New York: </address> <publisher> Springer Verlag. </publisher> <address> http://robotics.stanford.edu/~ronnyk. </address>
Reference-contexts: Common regression-tree measures (Breiman et al. 1984) might work well. Techniques for multiple model generation, such as Bagging Boosting, or Option Trees might be helpful, especially if they yield different top-level attributes and users can select between them. Summary Early work on decision tables <ref> (Kohavi 1995a) </ref> was promising but was too slow for common use because it was based on a forward selection of attributes using the wrapper model. Our research showed that entropy-based methods, which are much faster, are practically indistinguishable from their more expensive predecessors for error minimization.
Reference: <author> Kohavi, R. </author> <year> 1995b. </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University, Computer Science department. STAN-CS-TR-95-1560, </institution> <note> http://robotics.Stanford.EDU/~ronnyk/teza.ps.Z. </note>
Reference: <author> LeBlank, J.; Ward, M.; and Wittels, N. </author> <year> 1990. </year> <title> Exploring n-dimensional databases. </title> <booktitle> In Proceedings of Visualization, </booktitle> <pages> 230-237. </pages>
Reference: <author> Merz, C., and Murphy, P. </author> <year> 1998. </year> <title> UCI repository of machine learning databases. </title>
Reference-contexts: We found that this worked well and usually selected the version that actually had the lower estimated error on the test set. Empirical Evaluation We conducted an empirical evaluation of the induction algorithms on several datasets from the UCI repository <ref> (Merz & Murphy 1998) </ref>. We chose the larger datasets available, mostly natural but also a few artificial ones (m-of-n and the monk problems). The artificial datasets were tested on the given training and test sets.
Reference: <author> Michalski, R. S. </author> <year> 1978. </year> <title> A planar geometric model for representing multidimensional discrete spaces and multiple-valued logic functions. </title> <type> Technical Report UIUCDCS-R-78-897, </type> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wnek, J., and Michalski, R. S. </author> <year> 1994. </year> <title> Hypothesis-driven constructive induction in AQ17-HCI : A method and experiments. </title> <booktitle> Machine Learning 14(2) </booktitle> <pages> 139-168. </pages>
References-found: 15

