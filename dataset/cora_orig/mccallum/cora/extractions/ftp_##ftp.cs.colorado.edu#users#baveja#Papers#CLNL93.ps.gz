URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/CLNL93.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: http://www.cs.colorado.edu
Email: singh@cs.umass.edu  
Title: Soft Dynamic Programming Algorithms: Convergence Proofs  
Author: Satinder P. Singh 
Note: Based on Poster at CLNL-93  
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Algorithms based on dynamic programming (DP) find optimal solutions to finite-state optimal control tasks by iterating a "backup" operator that only considers the consequences of executing the "best" action in a state. In many problem domains, the optimal solution may be "brittle" and it may be desirable to find robust, if suboptimal, solutions that prefer states that have many "good" actions to choose from, over states that have a single good action. I present a family of iterative approximation algorithms constructed by replacing the "hard" max operator in classical DP algorithms by a "soft" generalized means of order p operator (Rivest [8]) (e.g., a non-linearly weighted l p norm). The soft DP algorithms converge to solutions that are more robust than solutions found by classical DP algorithms. I prove that for each index p 1, the corresponding soft DP algorithm converges to a unique fixed point, and that the approximate but robust solution gets uniformly better as the index p is increased, converging in the limit (p ! 1) to the optimal solution determined by classical DP algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.G. Barto, R.S. Sutton, and C. Watkins. </author> <title> Sequential decision problems and neural networks. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 686-693, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: (V t+1 )(x) = &lt; 1 X [r (x; a) + fl y2S 9 ; p Note that even though in this abstract the new algorithms are developed by modifying the Value Iteration algorithm, similar modifications can be made to most other DP algorithms as well as reinforcement learning algorithms <ref> [1] </ref> that involve the max operator and are related to DP, such as Watkins's popular Q-learning [11] algorithm (see Appendix). Fact 7. By the "Convergence" (Fact 1) property of the generalized mean operator, lim p!1 T p = T . 3.1 Convergence Results Fact 8.
Reference: [2] <author> R.E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: I thank Peter Dayan and Andrew Barto for their contributions to this paper. This work was supported by the Air Force Office of Scientific Research, Bolling AFB, under Grant AFOSR-89-0526 and by the National Science Foundation under Grant ECS-8912623. 2 2 Dynamic Programming Algorithms Dynamic programming (DP) <ref> [2] </ref> provides an approach to solving optimal control problems under general conditions. <p> The DP solution specifies the optimal control at every state of the system at every instant of time, and is often implemented as a feedback (closed-loop) controller in which the state of the system is constantly measured and the corresponding control applied. The DP approach is based on Bellman's <ref> [2] </ref> Principle Of Optimality which states that "an optimal policy has the property that whatever the initial state and decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision." This converts the simultaneous determination of the entire optimal control policy into
Reference: [3] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: The main contribution of this paper is the new family of approximation algorithms and their convergence results. The implications for neural network researchers are also discussed. Markovian Decision Tasks (MDTs) (e.g., <ref> [3] </ref>) are a subset of optimal control tasks important to learning researchers because many interesting sequential learning tasks can be formulated as multi-stage MDTs. Let S be the set of states of the system and A be the set of actions available to the controller. <p> An optimal policy fl maximizes the return for each state. I will denote V fl simply as V fl . 3 For MDTs, Bellman's Optimality Principle can be stated as follows; 8 x 2 S: V fl (x) = max [r (x; a) + fl y2S Value Iteration (e.g., <ref> [3] </ref>) is an iterative improvement algorithm, V t+1 = T (V t ), for computing the optimal value function V fl . <p> Assumption 1. 0 &lt; fl &lt; 1 Assumption 2. 8 x 2 S; 8 a 2 A, r (x; a) 0. This is not a restriction for MDTs with 0 fl &lt; 1, because of Fact 8. The development of the convergence proofs follows Bertsekas and Tsitsiklis <ref> [3] </ref> closely. Throughout, I will assume that assumptions 1 and 2 hold. Note that assumption 2 guarantees that the optimal value function will be non-negative. Proposition 1. <p> Corollary 1 (a): Let V fl be the optimal value function. Then lim p!1 V fl p = V fl . Proof : Bertsekas and Tsitsiklis <ref> [3] </ref> show that the iteration V t+1 = T (V t ), where the operator T is as defined in Equation 2, converges to the optimal value function V fl .
Reference: [4] <author> W. Fun and M.I. Jordan. </author> <title> The moving basin: Effective action-search in adaptive control, </title> <note> 1992. submitted to Neural Computation. </note>
Reference-contexts: is similar in spirit to the "soft-max" function used by Jacobs et al. [6] and may provide a probabilistic framework for action selection in DP-based algorithms. 5 Conclusion Several researchers are investigating the advantages of combining nonlinear neural net work based function approximation methods and traditional adaptive control techniques (e.g., <ref> [7, 4] </ref>). The algorithms presented in this paper have the dual advantages of leading to more robust solutions and of employing a differentiable backup operator.
Reference: [5] <author> G.H. Hardy, J.E. Littlewood, and G. Polya. </author> <title> Inequalities. </title> <publisher> University Press, </publisher> <address> Cambridge, Eng-land, 2 edition, </address> <year> 1952. </year>
Reference-contexts: This paper introduces a family of iterative approximation algorithms constructed by replacing the hard max operator in DP-based algorithms by "soft" generalized means <ref> [5] </ref> of order p (e.g., a non-linearly weighted l p norm). These soft DP algorithms converge to solutions that are more robust than those of classical DP. <p> Define A p = [ 1 n i=1 (a i ) p ] p , called a generalized mean of order p. The following facts are proved in Hardy et al. <ref> [5] </ref> under the conditions, 1 i n, a i ; a 0 Fact 1. (Convergence) lim p!1 A p = A max .
Reference: [6] <author> R.A. Jacobs, M.I. Jordan, S.J. Nowlan, and G.E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: An advantage of using H is that it requires less computation than the operator T p . The operator H is similar in spirit to the "soft-max" function used by Jacobs et al. <ref> [6] </ref> and may provide a probabilistic framework for action selection in DP-based algorithms. 5 Conclusion Several researchers are investigating the advantages of combining nonlinear neural net work based function approximation methods and traditional adaptive control techniques (e.g., [7, 4]).
Reference: [7] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Learning to control an unstable system with forward modeling. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: is similar in spirit to the "soft-max" function used by Jacobs et al. [6] and may provide a probabilistic framework for action selection in DP-based algorithms. 5 Conclusion Several researchers are investigating the advantages of combining nonlinear neural net work based function approximation methods and traditional adaptive control techniques (e.g., <ref> [7, 4] </ref>). The algorithms presented in this paper have the dual advantages of leading to more robust solutions and of employing a differentiable backup operator.
Reference: [8] <author> R.L. Rivest. </author> <title> Game tree searching by min/max approximation. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 77-96, </pages> <year> 1988. </year>
Reference-contexts: Following Rivest <ref> [8] </ref>, who replaced the min-max operators in game tree search with generalized means, I replace the max operator with the generalized mean for the more general class of DP-based algorithms. 2.1 Facts about Generalized Means Let A = fa 1 ; a 2 ; : : : ; a n g, <p> Being able to compute derivatives allows sensitivity analysis and may lead to some new ways of addressing the difficult exploration versus control issue [10] in optimal control tasks. Indeed, the motivation for Rivest's work <ref> [8] </ref>, which inspired this paper, was to use sensitivity analysis to address the analogous exploration issue in game tree search. Note that derivatives of the values of states with respect to the transition probabilities and the immediate payoffs can also be derived. <p> It is conjectured that convergence results similar to the ones derived for the discrete case can be derived for continuous state MDTs. 8 As discussed by Rivest <ref> [8] </ref>, other forms of generalized means exist, e.g., for any continuous monotone increasing function, f , we can consider mean values of the form f 1 ( 1 n i=1 f (a i )).
Reference: [9] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol.1: Foundations. </booktitle> <publisher> Bradford Books/MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 0 (a)V (y 0 ) # p1 X P xz (a) @V (y) = : (6) Note that one can use the chain rule to compute the above derivatives for states that are not neighbors in the state graph of the MDT in much the same way as the backpropagation <ref> [9] </ref> algorithm for multi-layer connectionist networks. Being able to compute derivatives allows sensitivity analysis and may lead to some new ways of addressing the difficult exploration versus control issue [10] in optimal control tasks.
Reference: [10] <author> R.S. Sutton. </author> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proc. of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Being able to compute derivatives allows sensitivity analysis and may lead to some new ways of addressing the difficult exploration versus control issue <ref> [10] </ref> in optimal control tasks. Indeed, the motivation for Rivest's work [8], which inspired this paper, was to use sensitivity analysis to address the analogous exploration issue in game tree search.
Reference: [11] <author> C.J.C.H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: Note that even though in this abstract the new algorithms are developed by modifying the Value Iteration algorithm, similar modifications can be made to most other DP algorithms as well as reinforcement learning algorithms [1] that involve the max operator and are related to DP, such as Watkins's popular Q-learning <ref> [11] </ref> algorithm (see Appendix). Fact 7. By the "Convergence" (Fact 1) property of the generalized mean operator, lim p!1 T p = T . 3.1 Convergence Results Fact 8. <p> Clearly, by perturbing the optimal value function by less than ffi 2 , the greedy action in any state will still be optimal. Differentiable Approximations to Q-learning Q-learning <ref> [11] </ref> is a DP-based algorithm that estimates Q-values, Q (x; a) that are defined via the following equations: Q (x; a) = r (x; a) + fl y2S a 0 2A Thus, Q-values associate scalars to state-action pairs instead of just the states.
References-found: 11

