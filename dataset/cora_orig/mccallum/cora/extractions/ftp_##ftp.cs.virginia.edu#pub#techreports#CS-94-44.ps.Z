URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-94-44.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: NPSI adaptive synchronization algorithms for PDES  
Author: Sudhir Srinivasan Paul F. Reynolds, Jr. 
Abstract: Computer Science Report No. CS-94-44 Issued Nov. 8, 1994; revised April 6, 1995 
Abstract-found: 1
Intro-found: 1
Reference: [BaHo90] <author> Ball, D. and Hoyt, S., </author> <title> The adaptive Time-Warp concurrency control algorithm, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <month> January </month> <year> 1990, </year> <pages> 174-177. </pages>
Reference-contexts: Penalty based: This approach assumes that the recent past is a good predictor of the near future. Based on their recent behavior, some LPs are penalized (and consequently block) while others are favored (and consequently continue). Examples are <ref> [ReJe89, BaHo90] </ref>. In [Madi93], the penalty is based on the difference between an LPs logical clock and estimates of the logical clocks of other LPs. Knowledge based: The basic idea is to contain the propagation of incorrect computation as soon as it is determined that the computation is incorrect. <p> Not surprisingly, we observe that all existing adaptive protocols admit tunable parameters of some sort (the time-window-bound in [TuXu92], N 1 and N 2 in [Stei93], cluster size in [RaAT93], blocking window size in <ref> [BaHo90] </ref>, the scaling factor c i in [HaTr94] and M f and M p in [DaFu94]). Clearly, there is a need to invent a scheme by which adaptive protocols may tune the values of such parameters dynamically. The Adaptive Time Warp algorithm [BaHo90] computes its blocking period based on a particular <p> cluster size in [RaAT93], blocking window size in <ref> [BaHo90] </ref>, the scaling factor c i in [HaTr94] and M f and M p in [DaFu94]). Clearly, there is a need to invent a scheme by which adaptive protocols may tune the values of such parameters dynamically. The Adaptive Time Warp algorithm [BaHo90] computes its blocking period based on a particular logistic response function. Thus its efficacy depends on how accurately this function reects reality. Moreover, it is an entirely local optimization.
Reference: [Bell93] <author> Bellenot, S., </author> <title> Performance of a riskfree Time Warp operating system, </title> <booktitle> Proceedings of the 7th Workshop on Parallel and Distributed Simulation, </booktitle> <month> May </month> <year> 1993, </year> <pages> 155-158. </pages>
Reference-contexts: In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93]. An interesting special case is when each cluster contains exactly one LP; then we have a risk-free system <ref> [DiRe90, Mehl91, Stei91, Bell93] </ref>. Penalty based: This approach assumes that the recent past is a good predictor of the near future. Based on their recent behavior, some LPs are penalized (and consequently block) while others are favored (and consequently continue). Examples are [ReJe89, BaHo90].
Reference: [DaFu94] <author> Das, S. and Fujimoto, </author> <title> R.M., An adaptive memory management protocol for Time Warp parallel simulation, </title> <booktitle> Proceedings of SIGMETRICS 94, </booktitle> <month> May </month> <year> 1994, </year> <pages> 201-210. </pages>
Reference-contexts: Moreover, the protocols in [HaTr94, FeTr94] cannot determine if a predecessor is rolling back until they receive a null or anti-message conveying that information. NPSI protocols are able to determine this information earlier. The third protocol in this class <ref> [DaFu94] </ref> uses memory consumption as the basis for limiting optimism. The protocol limits the memory consumption of LPs adaptively and consequently, also limits their optimism. It is based on a memory management protocol such as Cancelback or Artificial Rollback. <p> A synthetic workload generator is used instead of actual applications because it allows us to mimic those applications without the excessive time and effort required to implement each of them. Our workload generator is very similar to the PHOLD model <ref> [DaFu94] </ref>. We tested ETA on several workloads and observed that it outperformed Time Warp on all of them. We present results for the following: a) Workload 1 consists of four LPs with the T 1 communication topology shown in Figure 4 (a torus). <p> We expect that NPSI adaptive protocols will eliminate the need for these schemes for two reasons. First, any approach that limits the aggressiveness of LPs inherently reduces memory requirements by not permitting runaway processes to get too far ahead. Second, an adaptive, memory-based ow control scheme <ref> [DaFu94] </ref> can be integrated naturally with NPSI protocols by including information about the memory availability of down-stream LPs (perhaps immediate successors only) in the mapping M 1 . In this way, an LP could slow down when any of its successors is at the risk of running out of memory. <p> Not surprisingly, we observe that all existing adaptive protocols admit tunable parameters of some sort (the time-window-bound in [TuXu92], N 1 and N 2 in [Stei93], cluster size in [RaAT93], blocking window size in [BaHo90], the scaling factor c i in [HaTr94] and M f and M p in <ref> [DaFu94] </ref>). Clearly, there is a need to invent a scheme by which adaptive protocols may tune the values of such parameters dynamically. The Adaptive Time Warp algorithm [BaHo90] computes its blocking period based on a particular logistic response function. Thus its efficacy depends on how accurately this function reects reality. <p> This has the drawback that it cannot distinguish between a decrease in porgress rate due to excessive rollbacks from one that occurs inherently in the application. The scheme of <ref> [DaFu94] </ref> tunes two parameters, M f and M p dynamically based on an analysis of the ow of memory buffers in a Time Warp simulation. This scheme is very specific to the nature of the tunable parameters.
Reference: [Dick93] <author> Dickens, </author> <title> P.M., Analysis of an aggressive global windowing algorithm, </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Similarly only those messages within a (possibly different) window are sent out. This ensures that all of the LPs remain close to each other in logical time. Uncontrolled echoing and cascading rollbacks cannot occur. Examples are <ref> [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93] </ref>. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93].
Reference: [DiRe90] <author> Dickens, P.M. and Reynolds, P.F., Jr., </author> <title> SRADS with local rollback, </title> <booktitle> Proceedings of the 1990 SCS Multiconference on Distributed Simulation, </booktitle> <month> January </month> <year> 1990, </year> <pages> 161-164. </pages>
Reference-contexts: In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93]. An interesting special case is when each cluster contains exactly one LP; then we have a risk-free system <ref> [DiRe90, Mehl91, Stei91, Bell93] </ref>. Penalty based: This approach assumes that the recent past is a good predictor of the near future. Based on their recent behavior, some LPs are penalized (and consequently block) while others are favored (and consequently continue). Examples are [ReJe89, BaHo90].
Reference: [FeZh87] <author> Ferrari, D. and Zhou, S., </author> <title> An empirical investigation of load indices for load balancing applications, Report number CSD87-353, </title> <institution> Computer Science Division, University of California at Berkeley, </institution> <year> 1987. </year>
Reference-contexts: The key is to identify metrics that can be obtained by each LP during the simulation and that reect the progress of the computation accurately. Once again, an analogy may be drawn with load balancing where a good load index is essential <ref> [FeZh87] </ref>. In parallel simulation, as in many parallel computations, local (per processor) optmization does not imply global optmization (in fact it may be detrimental). Consequently, a purely local metric may not be advisable.
Reference: [FeTr94] <author> Ferscha, A. and Tripathi, </author> <title> S.K., Parallel and distributed simulation of discrete event systems, </title> <institution> University of Maryland at College Park Technical Report number CS-TR-3336, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Thus, these protocols are similar to NPSI protocols. The first two of these <ref> [HaTr94, FeTr94] </ref> are similar to each other in that they both utilize channel information to decide when and for how long LPs should wait. <p> The benefit comes from the fact that in a channel protocol, information has to percolate through the predecessors of an LP before it reaches that LP. During this time, the LP may have moved farther ahead than it should have. Moreover, the protocols in <ref> [HaTr94, FeTr94] </ref> cannot determine if a predecessor is rolling back until they receive a null or anti-message conveying that information. NPSI protocols are able to determine this information earlier. The third protocol in this class [DaFu94] uses memory consumption as the basis for limiting optimism.
Reference: [Fuji90] <author> Fujimoto, </author> <title> R.M., Parallel discrete event simulation, </title> <journal> CACM, </journal> <volume> Vol. 33, No. 10, </volume> <month> October </month> <year> 1990, </year> <pages> 30-53. </pages>
Reference-contexts: 1 Introduction The state-of-the-art in parallel discrete event simulation (PDES) is captured well in the recent survey by Fujimoto <ref> [Fuji90] </ref>. Although this survey is dated eleven years since the inception of PDES, it is dedicated entirely to the discussion of synchronization mechanisms for PDES. The synchronization problem has remained the central challenge in PDES. <p> We refer to these protocols as NPSI (Near-Perfect State Information) adaptive protocols. Given the inherently dynamic nature of simulations [NiRe90], we believe NPSI adaptive protocols offer the best hope of finding a consistently efficient, general protocol. We assume familiarity with the common approach to PDES <ref> [Fuji90] </ref>, namely the partitioning of a simulation into logical processes (LPs). Each LP is itself a sequential discrete event simulator which can schedule events at other LPs using timestamped messages. The LPs must execute events, whether generated internally or scheduled by other LPs, without violating causality constraints (effectively).
Reference: [FuNi92] <author> Fujimoto, R.M. and Nicol, </author> <title> D.M., </title> <booktitle> State of the art in parallel simulation, Proceedings of the 1992 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1992, </year> <pages> 246-254. </pages>
Reference-contexts: A large number of protocols have been proposed to solve the problem and many have reported good performance. However, a general solution which can be applied to a wide range of simulations still eludes the community. A more recent survey <ref> [FuNi92] </ref> confirms that this state has not changed. We present initial results of research into a novel and unique class of protocols - those in which LPs adapt their behavior dynamically to changes in the simulation using low-cost near-perfect system state information.
Reference: [Gima89] <author> Gimarc, </author> <title> R.L., Distributed simulation using hierarchical rollback, </title> <booktitle> Proceedings of the 1989 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1989, </year> <pages> 621-629. </pages>
Reference-contexts: Examples are [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93]. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are <ref> [Gima89, RaAT93] </ref>. An interesting special case is when each cluster contains exactly one LP; then we have a risk-free system [DiRe90, Mehl91, Stei91, Bell93]. Penalty based: This approach assumes that the recent past is a good predictor of the near future.
Reference: [HaTr94] <author> Hamnes, D.O. and Tripathi, A., </author> <title> Evaluation of a local adaptive protocol for distributed discrete event simulation, </title> <booktitle> Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1994, </year> <title> Vol. </title> <booktitle> III, </booktitle> <pages> 127-134. </pages>
Reference-contexts: Thus, these protocols are similar to NPSI protocols. The first two of these <ref> [HaTr94, FeTr94] </ref> are similar to each other in that they both utilize channel information to decide when and for how long LPs should wait. <p> The benefit comes from the fact that in a channel protocol, information has to percolate through the predecessors of an LP before it reaches that LP. During this time, the LP may have moved farther ahead than it should have. Moreover, the protocols in <ref> [HaTr94, FeTr94] </ref> cannot determine if a predecessor is rolling back until they receive a null or anti-message conveying that information. NPSI protocols are able to determine this information earlier. The third protocol in this class [DaFu94] uses memory consumption as the basis for limiting optimism. <p> Not surprisingly, we observe that all existing adaptive protocols admit tunable parameters of some sort (the time-window-bound in [TuXu92], N 1 and N 2 in [Stei93], cluster size in [RaAT93], blocking window size in [BaHo90], the scaling factor c i in <ref> [HaTr94] </ref> and M f and M p in [DaFu94]). Clearly, there is a need to invent a scheme by which adaptive protocols may tune the values of such parameters dynamically. The Adaptive Time Warp algorithm [BaHo90] computes its blocking period based on a particular logistic response function. <p> The Adaptive Time Warp algorithm [BaHo90] computes its blocking period based on a particular logistic response function. Thus its efficacy depends on how accurately this function reects reality. Moreover, it is an entirely local optimization. The Local Adaptive Protocol <ref> [HaTr94] </ref> also uses a local optimization strategy in which the parameter is tuned based on observed rate of progress of virtual time per channel. This has the drawback that it cannot distinguish between a decrease in porgress rate due to excessive rollbacks from one that occurs inherently in the application.
Reference: [Jeff85] <author> Jefferson, </author> <title> D.R., Virtual time, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 7, No. 3, </volume> <month> July </month> <year> 1985, </year> <pages> 404-425. </pages>
Reference-contexts: As we shall see later, our first NPSI adaptive protocol shows significant improvements over pure Time Warp <ref> [Jeff85] </ref>, in terms of both time and space. 2 Previous work We categorize protocols that limit optimism based on the criterion for limiting optimism: Window based: Only those events within a (common or independent) window are executed aggressively. Similarly only those messages within a (possibly different) window are sent out. <p> Error Potential Event processing and message sending System State 4 6.1 M 1 : computing EP i M 1 is the following function: M 1 : EP i = logical clock i - GVT where GVT (global virtual time <ref> [Jeff85] </ref>) is defined as the minimum of the logical clocks of all LPs and the timestamps of any messages in transit. Thus, ETA is based on near-perfect values of GVT being available to the LPs.
Reference: [Lin92] <author> Lin. Y-B., </author> <title> Memory management algorithms for optimistic parallel simulation, </title> <booktitle> Proceedings of the 6th Workshop on Parallel and Distributed Simulation, </booktitle> <month> January </month> <year> 1992, </year> <pages> 43-52. </pages>
Reference-contexts: Several memory management schemes have been proposed to reclaim memory from future events (since this memory cannot be reclaimed by fossil collection) <ref> [Lin92] </ref>. We expect that NPSI adaptive protocols will eliminate the need for these schemes for two reasons. First, any approach that limits the aggressiveness of LPs inherently reduces memory requirements by not permitting runaway processes to get too far ahead.
Reference: [LuWS89] <author> Lubachevsky, B., Weiss, A. and Shwartz, A., </author> <title> Rollback sometimes works . . . if filtered, </title> <booktitle> Proceedings of the 1989 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1989, </year> <pages> 630-639. </pages>
Reference-contexts: Similarly only those messages within a (possibly different) window are sent out. This ensures that all of the LPs remain close to each other in logical time. Uncontrolled echoing and cascading rollbacks cannot occur. Examples are <ref> [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93] </ref>. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93]. <p> Each message schedules a job event. Execution of a job event creates an output event, which generates a message. Every LP has 25 events initially. Thus, this workload resembles a closed queueing network with density 25. c) Workload 3 is an implementation of the echoing example described in <ref> [LuWS89] </ref>. LP 0 and LP 1 execute in self-initiating mode, sending messages to LP 2 after each event. In addition, there is a single message-initiating event that is at LP 0 initially. Upon processing this event, LP 0 sends a message to LP 1 .
Reference: [Madi93] <author> Madisetti, V.K., </author> <title> Randomized algorithms for self-synchronization, Private communication, 1993. </title> <editor> [MaWM88]Madisetti, V.K., Walrand, J. and Messerschmitt, D., WOLF: </editor> <title> A rollback algorithm for optimistic distributed simulation systems, </title> <booktitle> Proceedings of the 1988 Winter Simulation Conferences, </booktitle> <month> December </month> <year> 1988, </year> <pages> 296-305. </pages>
Reference-contexts: Penalty based: This approach assumes that the recent past is a good predictor of the near future. Based on their recent behavior, some LPs are penalized (and consequently block) while others are favored (and consequently continue). Examples are [ReJe89, BaHo90]. In <ref> [Madi93] </ref>, the penalty is based on the difference between an LPs logical clock and estimates of the logical clocks of other LPs. Knowledge based: The basic idea is to contain the propagation of incorrect computation as soon as it is determined that the computation is incorrect.
Reference: [McAf90] <author> McAffer, J., </author> <title> A unified distributed simulation system, </title> <booktitle> Proceedings of the 1990 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1990, </year> <pages> 415-422. </pages>
Reference-contexts: Similarly only those messages within a (possibly different) window are sent out. This ensures that all of the LPs remain close to each other in logical time. Uncontrolled echoing and cascading rollbacks cannot occur. Examples are <ref> [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93] </ref>. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93].
Reference: [Mehl91] <author> Mehl, H., </author> <title> Speed-up of conservative distributed discrete event simulation methods by speculative computing, </title> <booktitle> Proceedings of the 5th Workshop on Parallel and Distributed Simulation, </booktitle> <month> January </month> <year> 1991, </year> <pages> 163-166. </pages>
Reference-contexts: In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93]. An interesting special case is when each cluster contains exactly one LP; then we have a risk-free system <ref> [DiRe90, Mehl91, Stei91, Bell93] </ref>. Penalty based: This approach assumes that the recent past is a good predictor of the near future. Based on their recent behavior, some LPs are penalized (and consequently block) while others are favored (and consequently continue). Examples are [ReJe89, BaHo90].
Reference: [Nico91] <author> Nicol, </author> <title> D.M., Performance bounds on parallel self-initiating discrete-event simulations, </title> <journal> ACM Transactions on Modeling and Computer Simulations, </journal> <volume> Vol. 1, No. 1, </volume> <year> 1991, </year> <pages> pp 24-50. </pages>
Reference-contexts: The number on an arc is the probability that a generated message is sent along that arc. The workload is self-initiating (each event schedules the next local event <ref> [Nico91] </ref>) and the probability of an LP sending a message after an event is 0.2. Messages may cause rollbacks, but do not schedule events. b) Workload 2 also uses topology T 1 but is message initiating (messages cause events to be scheduled). Each message schedules a job event.
Reference: [NiRe90] <author> Nicol, D.M. and Reynolds, P.F., Jr., </author> <title> Optimal dynamic remapping of parallel computations, </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. 39, No. 2, </volume> <month> February </month> <year> 1990. </year>
Reference-contexts: We refer to these protocols as NPSI (Near-Perfect State Information) adaptive protocols. Given the inherently dynamic nature of simulations <ref> [NiRe90] </ref>, we believe NPSI adaptive protocols offer the best hope of finding a consistently efficient, general protocol. We assume familiarity with the common approach to PDES [Fuji90], namely the partitioning of a simulation into logical processes (LPs).
Reference: [PaWi93] <author> Palaniswamy, A.C. and Wilsley, </author> <title> P.A., Adaptive bounded time window in an optimistically synchronized simulator, </title> <booktitle> Proceedings of the third Great Lakes symposium on VLSI, </booktitle> <year> 1993, </year> <pages> 114-118. </pages>
Reference-contexts: R C , which is a global metric, can be computed for NPSI protocols accurately at very low cost to the simulation using the feedback system. We expect to have performance results for a scheme that uses these metrics in the near future. In <ref> [PaWi93] </ref>, the authors suggest the use of similar metrics. Similar to R C , they define W i as the rate of commitment of events at LP i in one GVT cycle. Thus, W i is a local metric. While [PaWi93] argue for an indicator of rollback behavior, they do not <p> In <ref> [PaWi93] </ref>, the authors suggest the use of similar metrics. Similar to R C , they define W i as the rate of commitment of events at LP i in one GVT cycle. Thus, W i is a local metric. While [PaWi93] argue for an indicator of rollback behavior, they do not suggest one - the algorithm they propose uses W i only. Larger systems For the purpose of experimentation with implementations of NPSI adaptive protocols, the prototype reduction network restricts us to four processors.
Reference: [PrSu91] <author> Prakash, A. and Subramanian, R., </author> <title> Filter: An algorithm for reducing cascaded rollbacks in optimistic distributed simulations, </title> <booktitle> Proceedings. of the 24th Annual Simulation Symposium, </booktitle> <month> April </month> <year> 1991, </year> <pages> 123-132. </pages>
Reference-contexts: When an LP suffers a primary rollback, any aggressive processing it has done and sent to other LPs is potentially incorrect. So it 2 broadcasts a message to all affected LPs informing them that their computation is potentially incorrect. These LPs then limit their optimistic processing. Examples are <ref> [MaWM88, PrSu91] </ref>. Probabilistic: A special process periodically makes a probabilistic decision to send out synchronization messages to all LPs, forcing them to synchronize. Most of these protocols have demonstrated better performance than pure Time Warp under specific tests.
Reference: [RaAT93] <author> Rajaei, H., Ayani, R. and Thorelli, L-E., </author> <title> The Local Time Warp approach to parallel simulation, </title> <booktitle> Proceedings of the 7th Workshop on Parallel and Distributed Simulation, </booktitle> <month> May </month> <year> 1993, </year> <pages> 119-126. </pages>
Reference-contexts: Examples are [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93]. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are <ref> [Gima89, RaAT93] </ref>. An interesting special case is when each cluster contains exactly one LP; then we have a risk-free system [DiRe90, Mehl91, Stei91, Bell93]. Penalty based: This approach assumes that the recent past is a good predictor of the near future. <p> Not surprisingly, we observe that all existing adaptive protocols admit tunable parameters of some sort (the time-window-bound in [TuXu92], N 1 and N 2 in [Stei93], cluster size in <ref> [RaAT93] </ref>, blocking window size in [BaHo90], the scaling factor c i in [HaTr94] and M f and M p in [DaFu94]). Clearly, there is a need to invent a scheme by which adaptive protocols may tune the values of such parameters dynamically.
Reference: [RaSZ89] <author> Ramamritham, K., Stankovic, J.A. and Zhao, W., </author> <title> Ditributed scheduling of tasks with deadlines and resource requirements, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 38, No. 8, </volume> <month> August </month> <year> 1989, </year> <pages> 1110-1123. 10 </pages>
Reference-contexts: These differ from previous approaches to adaptiveness in that they base their adaptive decisions on near-perfect information about the state of relevant parts of the entire system. In <ref> [RaSZ89] </ref>, it has been shown that a load sharing policy that assumes perfect state information at zero cost offers the best solution. Correspondingly, we believe that NPSI adaptive protocols will provide a general, efficient solution to the synchronization problem of PDES.
Reference: [ReJe89] <author> Reiher, P.L. and Jefferson, D., </author> <title> Limitation of optimism in the Time Warp operating system, </title> <booktitle> Proceedings of the 1989 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1989, </year> <pages> 765-770. </pages>
Reference-contexts: Similarly only those messages within a (possibly different) window are sent out. This ensures that all of the LPs remain close to each other in logical time. Uncontrolled echoing and cascading rollbacks cannot occur. Examples are <ref> [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93] </ref>. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93]. <p> Penalty based: This approach assumes that the recent past is a good predictor of the near future. Based on their recent behavior, some LPs are penalized (and consequently block) while others are favored (and consequently continue). Examples are <ref> [ReJe89, BaHo90] </ref>. In [Madi93], the penalty is based on the difference between an LPs logical clock and estimates of the logical clocks of other LPs. Knowledge based: The basic idea is to contain the propagation of incorrect computation as soon as it is determined that the computation is incorrect.
Reference: [Reyn88] <author> Reynolds, P.F., Jr., </author> <title> A spectrum of options for parallel simulation, </title> <booktitle> Proceedings of the 1988 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1988, </year> <pages> 325-332. </pages>
Reference-contexts: The LPs must execute events, whether generated internally or scheduled by other LPs, without violating causality constraints (effectively). Typically, this is the responsibility of a protocol. The nine design variables in <ref> [Reyn88] </ref> define the design space for protocols. An aggressive protocol is one which executes events without the guarantee of freedom from causality errors. A protocol has risk if it propagates messages based on aggressive or inaccurate computation. <p> Based on these two variables, a conservative protocol is nonaggressive and without risk, while an optimistic protocol is aggressive and with risk. These two categories form the end points of a spectrum of protocols with limited optimism (aggressiveness and risk). Adaptiveness for PDES protocols is defined in <ref> [Reyn88] </ref> as the capability of a protocol to modify the bindings of one or more of its design variables during the simulation. Several protocols have been proposed that limit optimism, but most of them are not adaptive by this definition.
Reference: [RePS92] <author> Reynolds, P.F., Jr., Pancerella, C.M. and Srinivasan, S., </author> <title> Making parallel simulations go fast, </title> <booktitle> Proceedings of the 1992 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1992, </year> <pages> 646-656. </pages>
Reference-contexts: This is done for three reasons: (i) we believe NPSI adaptive protocols have significant potential and thus warrant a detailed study, (ii) a feasible implementation for the feedback system is a high-speed reduction network, and (iii) an implementation exists for such a reduction network <ref> [RePS92] </ref>. <p> A global reduction network is one in which binary, associative operations (minimum, summation, etc.) are used to reduce state information so that, say, the minimum simulation clock value can be computed across all LPs. Our experience with the design, construction and testing of a global reduction network <ref> [RePS92] </ref> suggests that a production version of such a network can operate at very high speeds (less than 20 nanoseconds per stage in the tree). These low latencies, combined with its pipelined, tree structure make such a network scalable up to thousands of processors. <p> We describe the testing environment, followed by test cases, metrics used and results. Hardware The hardware consists of a cluster of four Sparc 2 workstations connected by Ethernet and a parallel reduction network (PRN) that we have designed and built <ref> [RePS92] </ref>.
Reference: [SoBW88] <author> Sokol, L.M., Briscoe, </author> <title> D.P. and Wieland, A.P., MTW: a strategy for scheduling discrete events for concurrent execution, </title> <booktitle> Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <year> 1988, </year> <pages> 34-42. </pages>
Reference-contexts: Similarly only those messages within a (possibly different) window are sent out. This ensures that all of the LPs remain close to each other in logical time. Uncontrolled echoing and cascading rollbacks cannot occur. Examples are <ref> [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93] </ref>. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93].
Reference: [SrRe93] <author> Srinivasan, S. and Reynolds, P.F., Jr., </author> <title> Non-interfering GVT computation via asynchronous global reductions, </title> <booktitle> Proceedings of the 1993 Winter Simulation Conference, </booktitle> <month> December </month> <year> 1993, </year> <pages> 740-749. </pages>
Reference-contexts: Thus, ETA is based on near-perfect values of GVT being available to the LPs. We use the reduction network to provide accurate GVT at high frequency and low cost <ref> [SrRe93] </ref>. The rationale behind this M 1 is that if an LP is far ahead of others, it is likely to be rolled back soon and therefore should be slowed down. <p> The LPs use aggressive cancellation and do not support event pre-emption; they include the M 1 and M 2 for ETA. The GVT computation algorithm for the APs is described in <ref> [SrRe93] </ref>. Test cases We employ a parameterized synthetic workload generator to create our test cases with the important parameters being event execution time, average timestamp increment, state saving cost, communication topology and distributions, number of local events per message, and state saving and fossil collection frequencies.
Reference: [Stei91] <author> Steinman, J.S., </author> <title> Interactive SPEEDES, </title> <booktitle> Proceedings of the 24th Annual Simulation Symposium, </booktitle> <year> 1991, </year> <pages> 149-158. </pages>
Reference-contexts: In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93]. An interesting special case is when each cluster contains exactly one LP; then we have a risk-free system <ref> [DiRe90, Mehl91, Stei91, Bell93] </ref>. Penalty based: This approach assumes that the recent past is a good predictor of the near future. Based on their recent behavior, some LPs are penalized (and consequently block) while others are favored (and consequently continue). Examples are [ReJe89, BaHo90].
Reference: [Stei93] <author> Steinman, J.S., </author> <title> Breathing Time Warp, </title> <booktitle> Proceedings of the 7th Workshop on Parallel and Distributed Simulation, </booktitle> <month> May </month> <year> 1993, </year> <pages> 109-118. </pages>
Reference-contexts: Similarly only those messages within a (possibly different) window are sent out. This ensures that all of the LPs remain close to each other in logical time. Uncontrolled echoing and cascading rollbacks cannot occur. Examples are <ref> [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93] </ref>. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93]. <p> It is well known that good load balancing requires good estimation of tunable parameters such as thresholds and maximum number of transfer-hops. Not surprisingly, we observe that all existing adaptive protocols admit tunable parameters of some sort (the time-window-bound in [TuXu92], N 1 and N 2 in <ref> [Stei93] </ref>, cluster size in [RaAT93], blocking window size in [BaHo90], the scaling factor c i in [HaTr94] and M f and M p in [DaFu94]). Clearly, there is a need to invent a scheme by which adaptive protocols may tune the values of such parameters dynamically.
Reference: [TuXu92] <author> Turner, S.J. and Xu, M.Q., </author> <title> Performance evaluation of the Bounded Time Warp algorithm, </title> <booktitle> Proceedings of the 6th Workshop on Parallel and Distributed Simulation, </booktitle> <month> January </month> <year> 1992, </year> <pages> 117-126. </pages>
Reference-contexts: Similarly only those messages within a (possibly different) window are sent out. This ensures that all of the LPs remain close to each other in logical time. Uncontrolled echoing and cascading rollbacks cannot occur. Examples are <ref> [SoBW88, LuWS89, ReJe89, McAf90, TuXu92, Dick93, Stei93] </ref>. Space based: Here, the boundaries for limiting optmism are spatial rather than temporal. In general, the processors are divided into clusters which use Time Warp internally. Interaction among clusters follows some non-aggressive protocol. Examples are [Gima89, RaAT93]. <p> It is well known that good load balancing requires good estimation of tunable parameters such as thresholds and maximum number of transfer-hops. Not surprisingly, we observe that all existing adaptive protocols admit tunable parameters of some sort (the time-window-bound in <ref> [TuXu92] </ref>, N 1 and N 2 in [Stei93], cluster size in [RaAT93], blocking window size in [BaHo90], the scaling factor c i in [HaTr94] and M f and M p in [DaFu94]).
References-found: 31

