URL: http://infopad.eecs.berkeley.edu/~renu/papers/Tcad95.ps
Refering-URL: http://infopad.eecs.berkeley.edu/~renu/work.html
Root-URL: 
Title: Optimizing Power Using Transformations  
Author: Anantha P. Chandrakasan Miodrag Potkonjak Renu Mehra Jan Rabaey Robert W. Brodersen 
Address: Berkeley  
Affiliation: EECS Department, University of California at  
Note: 1 of 27  1.0 Introduction  
Abstract: The increasing demand for portable computing has elevated power consumption to be one of the most critical design parameters. A high-level synthesis system, HYPER-LP , is presented for minimizing power consumption in application specific datapath intensive CMOS circuits using a variety of architectural and computational transformations. The synthesis environment consists of high-level estimation of power consumption, a library of transformation primitives, and heuristic/pr obabilistic optimization search mechanisms for fast and efficient scanning of the design space. Examples with varying degree of computational complexity and structures are optimized and synthesized using the HYPER-LP system. The results indicate that more than an order of magnitude reduction in power can be achieved over current-day design methodologies while maintaining the system throughput; in some cases this can be accomplished while preserving or reducing the implementation area. VLSI research and development efforts have focused primarily on optimizing speed to realize computation-ally intensive real-time tasks such as video compression and speech recognition. As a result, many systems have successfully integrated various complex signal processing modules to meet users computation and entertainment demands. While these solutions have provided answers to the real-time problem, they have not addressed the rapidly increasing demand for portable operation. The strict limitation on power dissipation which portability imposes, must be met by the designer while still meeting ever higher computational requirements. The desirability of portable operation of all types of electronic systems has become clear . It seems that by merely providing the same functionality of a wired system in an untethered implementation, a significant market advantage can be obtained for virtually any electronic function. Consumer electronics companies have made available portable televisions, camcoders, compact disk players, and recently hand held multimedia 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> N. Weste and K. Eshragian, </author> <title> Principles of CMOS VLSI Design: A Systems Perspective, </title> <publisher> Addison-Wesley, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: The switching component, however, is the only one that can not be made negligible if proper design techniques are followed. The switching power for a CMOS gate with a load capacitor, C L , is given by <ref> [1] </ref>: P switching = Energy per Transition f = C avg V dd 2 f = (p t C L ) V dd where f is the clock frequency, and p t is the probability of a power consuming transition (0 -&gt; 1).
Reference: [2] <author> A. Chandrakasan, S. Sheng, R. Brodersen, </author> <title> Low-power CMOS Digital Design, </title> <journal> IEEE Journal of Solid-state circuit, </journal> <pages> pp. 473-484, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The energy consumed is therefore a quadratic function of the operating voltage, as verif ied in Figure 1a, which is an experimentally derived plot of the normalized energy vs. V dd . This dependence on supply voltage has been verified for a number of logic functions and logic styles <ref> [2] </ref>. The average capacitance switched, C avg = S p t C L , for a uniformly distributed set of input values has been characterized for each logic and memory element in the cell library. <p> Figure 1b shows experimental data of normalized delay vs. V dd for a typical CMOS gate implemented in 2.0 mm technology. Once again, the delay dependence on supply voltage was verified to be relatively independent of various logic functions and logic styles <ref> [2] </ref>. Previous Work 3 of 27 It is clear that operating at the lowest possible voltage is most desirable, however , this comes at the cost of increased delays and thus reduced throughput. <p> This was found to achieve a 60% reduction in power for a 3.3V system when compared to a 5 volt operation [4]. <ref> [2] </ref> presents an architecture based voltage scaling strategy that results in an optimal voltage for power that is much lower (in the 1-1.5V range) than obtained from the technology based scaling. The idea is to maintain throughput at reduced supply voltages through hardware duplication or pipelining. <p> By using parallel, identical units, the speed requirements on each unit are reduced, allowing for a reduction in voltage. This work presents an ef ficient transformation based high-level synthesis approach to explore the architecture based voltage scaling strategy <ref> [2] </ref>. Power reduction can also be achieved by minimizing the capacitance switched at all levels of the design. There are a number of options available in choosing and optimizing the basic circuit approach and topology for implementing various logic and arithmetic functions. <p> Optimizing transistor sizing is yet another degree of freedom available in minimizing the ener gy. To minimize the parasitic capacitances, it is desirable to use minimal sized devices as much as possible, which also assists in minimizing the interconnect routing lengths and their associated parasitic capacitances <ref> [2] </ref>. To minimize the total switched capacitance in random logic modules, several logic synthesis optimization techniques have been proposed to lower the power dissipation [7]. 3.2 Transformations in High-level Synthesis Over the last few years, several high-level synthesis systems have incorporated comprehensive sets of transformations, coupled with powerful optimization strategies. <p> The basic idea is to reduce the number of control steps, so that slower control clock cycles can be used for a fixed throughput, allowing for a reduction in supply voltage <ref> [2] </ref>. The reduction in control step requirements is most often possible due to the exploitation of concurrency. Many transformations profoundly affect the amount of concurrency in the computation. This includes retiming/pipelining, algebraic transformations and loop transformations.
Reference: [3] <author> M. Kakumu and M Kinugawa, </author> <title> Power-Supply Voltage Impact on Circuit Performance for Half and Lower Submi-crometer CMOS LSI, </title> <journal> IEEE Transactions on Electron Devices, </journal> <volume> Vol 37, No. 8, </volume> <pages> pp. 1902-1908, </pages> <month> August </month> <year> 1990. </year> <note> Conclusions 26 of 27 </note>
Reference-contexts: A technology based approach proposes choosing the power supply voltage based on maintaining the speed performance for a given submicron technology <ref> [3] </ref>. By exploiting the relative independence of delay on supply voltage at high electric f ields, the voltage can be dropped to some extent for a velocity-saturated device with very little penalty in speed performance.
Reference: [4] <author> D. Dahle, </author> <title> Designing High Performance Systems to Run from 3.3V or Lower Sources, </title> <booktitle> Silicon Valley Personal Computer Conference, </booktitle> <pages> pp. 685-691, </pages> <year> 1991. </year>
Reference-contexts: This was found to achieve a 60% reduction in power for a 3.3V system when compared to a 5 volt operation <ref> [4] </ref>. [2] presents an architecture based voltage scaling strategy that results in an optimal voltage for power that is much lower (in the 1-1.5V range) than obtained from the technology based scaling. The idea is to maintain throughput at reduced supply voltages through hardware duplication or pipelining.
Reference: [5] <author> K. Yano, et al., </author> <title> A 3.8ns CMOS 16x16 Multiplier Using Complementary Pass Transistor Logic, </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <pages> pp. 388-395, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: There are a number of options available in choosing and optimizing the basic circuit approach and topology for implementing various logic and arithmetic functions. A pass-transistor logic family was found to minimize physical capacitance when compared to a conventional CMOS logic family <ref> [5] </ref>. At a another level, there are various topological choices for implementing a given function. For example, an adder can implemented using ripple-carry or carry-lookahead approaches.
Reference: [6] <author> T. Callaway and E. Swartzlander, </author> <title> Optimizing Aritmitic Elements for Signal Processing, </title> <booktitle> VLSI Signal Processing Workshop, </booktitle> <address> pp.91-100, </address> <year> 1992. </year>
Reference-contexts: At a another level, there are various topological choices for implementing a given function. For example, an adder can implemented using ripple-carry or carry-lookahead approaches. The power trade-off between various types of adders and multipliers were investigated in <ref> [6] </ref> and they concluded that a carry-lookahead topology was the best after taking into Previous Work 4 of 27 account the speed-capacitance tradeoff. Optimizing transistor sizing is yet another degree of freedom available in minimizing the ener gy.
Reference: [7] <author> A. Shen, A. Ghosh, S. Devedas, K. Keutzer, </author> <title> On Average Power Dissipation and Random Pattern Testability of CMOS Combinational Logic Networks, </title> <booktitle> Proc. IEEE ICCAD, </booktitle> <pages> pp. 402-407, </pages> <year> 1992. </year>
Reference-contexts: To minimize the total switched capacitance in random logic modules, several logic synthesis optimization techniques have been proposed to lower the power dissipation <ref> [7] </ref>. 3.2 Transformations in High-level Synthesis Over the last few years, several high-level synthesis systems have incorporated comprehensive sets of transformations, coupled with powerful optimization strategies. Example systems with elaborate applications of transformations are Flamel [8], SAW [9], SPAID [10], HYPER [11], and CATHEDRAL [12].
Reference: [8] <author> H. Trickey, Flamel: </author> <title> A high-Level Hardware Compiler, </title> <journal> IEEE Transaction on CAD, </journal> <volume> Vol. 6, No. 2, </volume> <pages> pp. 259-269, </pages> <year> 1987. </year>
Reference-contexts: Example systems with elaborate applications of transformations are Flamel <ref> [8] </ref>, SAW [9], SPAID [10], HYPER [11], and CATHEDRAL [12]. Among the set of transformations used by the Flamel design system are loop transformations, height reduction and constant propagation.
Reference: [9] <author> R.A. Walker, D.E. Thomas, </author> <title> Behavioral Transformation for Algorithmic Level IC Design IEEE Trans. </title> <journal> on CAD, </journal> <volume> Vol 8. No.10, </volume> <pages> pp. 1115-1127, </pages> <year> 1989. </year>
Reference-contexts: Example systems with elaborate applications of transformations are Flamel [8], SAW <ref> [9] </ref>, SPAID [10], HYPER [11], and CATHEDRAL [12]. Among the set of transformations used by the Flamel design system are loop transformations, height reduction and constant propagation.
Reference: [10] <author> B.S. Haroun, M.I. Elmasry, </author> <title> Architectural Synthesis for DSP Silicon Compilers, </title> <journal> IEEE Transaction on CAD for IC, </journal> <volume> Vol. 8, No. 4, </volume> <pages> pp. 431-447, </pages> <year> 1989. </year>
Reference-contexts: Example systems with elaborate applications of transformations are Flamel [8], SAW [9], SPAID <ref> [10] </ref>, HYPER [11], and CATHEDRAL [12]. Among the set of transformations used by the Flamel design system are loop transformations, height reduction and constant propagation.
Reference: [11] <author> J. Rabaey, C. Chu, P. Hoang, M. Potkonjak, </author> <title> Fast Prototyping of Data Path Intensive Architecture, </title> <journal> IEEE Design and Test, </journal> <volume> Vol. 8, No. 2, </volume> <pages> pp. 40-51, </pages> <year> 1991. </year>
Reference-contexts: Example systems with elaborate applications of transformations are Flamel [8], SAW [9], SPAID [10], HYPER <ref> [11] </ref>, and CATHEDRAL [12]. Among the set of transformations used by the Flamel design system are loop transformations, height reduction and constant propagation. <p> The use of transformations makes it possible to explore a number of alternative architectures and to choose those which result in the lowest power. We will use the control-data ow graph format to represent computation <ref> [11] </ref>, in which nodes represent operations, and edges represent data and control dependencies. Transformations have primarily been used until now to optimize either the implementation area or the system throughput. <p> The run time for all examples were less than 1 minute on a SP ARC 2 workstation. The bitwidth requirement was established using the HYPER simulator tool <ref> [11] </ref>. 7.1 Example #1: Wavelet Filter The first example is a wavelet f ilter, used in speech and video applications. The implementation contains high-pass and low-pass filters that are realized as 14th order FIR modules using a wordlength of 16-bits (determined by high-level simulation).
Reference: [12] <author> F.H.M. Franssen, F. Balasa, M.F.X.B. van Swaaij, F.V.M. Catthoor, H.J. De Man: </author> <title> Modeling Multidimensional Data and Control Flow, </title> <journal> IEEE Trans. on VLSI Systems, </journal> <volume> Vol. 1, No. 3, </volume> <pages> pp. 319-327, </pages> <year> 1993. </year>
Reference-contexts: Example systems with elaborate applications of transformations are Flamel [8], SAW [9], SPAID [10], HYPER [11], and CATHEDRAL <ref> [12] </ref>. Among the set of transformations used by the Flamel design system are loop transformations, height reduction and constant propagation. SAW uses among other transformations inline expansion, dead code elimination, four types of transformations for conditional statements and pipelining as supporting steps during the behavioral and structural partitioning.
Reference: [13] <author> M. Cirit, </author> <title> Estimating Dynamic Power Consumption of CMOS Circuits, </title> <booktitle> IEEE International Conference on Computer Aided Design, </booktitle> <pages> pp. 534-537, </pages> <year> 1987. </year>
Reference-contexts: The primary tradeoff between these approaches is the computational complexity vs. accuracy. Circuit-level approaches result in the most accurate estimates, while being the most computationally intensive. Gate-level probabilistic approaches estimate the internal node activities of a network given the distribution of the input signals <ref> [13] </ref>, [14], [15].
Reference: [14] <author> F. Najm, </author> <title> Transition Density, A Stochastic Measure of Activities in Digital Circuits, </title> <booktitle> DAC, </booktitle> <pages> pp. 644-649, </pages> <year> 1991. </year>
Reference-contexts: The primary tradeoff between these approaches is the computational complexity vs. accuracy. Circuit-level approaches result in the most accurate estimates, while being the most computationally intensive. Gate-level probabilistic approaches estimate the internal node activities of a network given the distribution of the input signals [13], <ref> [14] </ref>, [15].
Reference: [15] <author> A. Ghosh, S. Devedas, K. Keutzer, and J. White, </author> <title> Estimation of Average Switching Activity, </title> <booktitle> Proc. DAC, </booktitle> <year> 1992. </year>
Reference-contexts: The primary tradeoff between these approaches is the computational complexity vs. accuracy. Circuit-level approaches result in the most accurate estimates, while being the most computationally intensive. Gate-level probabilistic approaches estimate the internal node activities of a network given the distribution of the input signals [13], [14], <ref> [15] </ref>.
Reference: [16] <author> N. Kimura, J. Tsujimoto, </author> <title> Calculation of Total Dynamic Current of VLSI Using a Switch Level Timing Simulator (RSIM-FX), </title> <address> CICC, </address> <year> 1991. </year>
Reference-contexts: The total power is then estimated as C avg * V dd 2 * f clk An approach for estimating the power consumption in CMOS circuits using a switch-level simulator is presented in <ref> [16] </ref>. The basic idea is to monitor the number of times each node in the circuit transitions during the simulation period.
Reference: [17] <author> A. Salz, M. Horowitz, IRSIM: </author> <title> An Incremental MOS Switch-level Simulator, </title> <booktitle> Proceedings of the 26th ACM/IEEE Design Automation Conference, </booktitle> <month> June </month> <year> 1989, </year> <pages> pp. 173-178. </pages>
Reference-contexts: C avg is given by N i / N C i , where N i is the total number of power consuming transitions for node i, N is the number of simulation cycles, and C i is the physical capacitance of node i. This approach (using the IRSIM simulator <ref> [17] </ref>) was used to estimate power at the layout level. The results from a few fabricated chips, indicate that the predicted power from IRSIM is within 30% of the measured power. At the lowest level, power can be estimated using a circuit simulator such as SPICE. <p> The capacitance switched for a chained implementation is a factor of 1.5 larger than the tree implementation for a four input addition and 2.5 larger for an eight input addition. The above simulations were done on layouts generated by the LagerIV silicon compiler [25] using the IRSIM <ref> [17] </ref> switch-level simulator over 1000 uncorrelated random input patterns. The results presented above indicate that increasing the logic depth (through more cascading) will increase the capacitance due to glitching while reducing the logic depth will increase register power.
Reference: [18] <author> P. E. Landman and J. M. Rabaey, </author> <title> Power Estimation for High Level Synthesis, </title> <booktitle> Proceedings of the European Conference on Design Automation 93, </booktitle> <address> Paris, France, </address> <month> Feb. </month> <year> 1993, </year> <pages> pp. 361-366. </pages>
Reference-contexts: Also, the estimation time for each new topology is too long to meaningfully explore many architectures. Hence, power must be estimated efficiently from a high level of abstraction. Recently , techniques to estimate the power consumption at the architecture level (after the flowgraph has been scheduled) have been developed <ref> [18] </ref>. In this work, power is estimated from an algorithmic level so the design space can be quickly explored. 4.0 Using Transformations to Optimize Power Transformations are changes to the computational structure in a manner that the input/output behavior is preserved. <p> This is not completely correct since the capacitance switched is a function of signal correlation. Techniques to model signal correlation at a high-level have been recently developed <ref> [18] </ref>. 5.1.2 Registers Registers are treated the same way as the execution units. The existing estimation program gives information about the total number of register accesses (read/write) within a given sampling period. This is essentially the activity of the registers.
Reference: [19] <author> K.K. Parhi: </author> <title> Algorithm Transformation Techniques for Concurrent Processors, </title> <journal> Proc. of the IEEE, </journal> <volume> Vol. 77., No. 12, </volume> <pages> pp. 1879-1895. </pages>
Reference-contexts: This includes retiming/pipelining, algebraic transformations and loop transformations. To illustrate the application of speedup transformations to lower power , consider a first order IIR filter, as shown in Figure 2a, with a critical path of 2. Due to the recursive bottleneck <ref> [19] </ref> imposed by the f ilter structure, it is impossible to reduce the critical path using retiming or pipelining. Also, the simple structure does not provide opportunities for the application of algebraic transformations and applying a single transformation is not enough to reduce power in this example.
Reference: [20] <author> E. Feig, S. Winograd: </author> <title> Fast Algorithms for Discrete Cosine Transform, </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> Vol. 40, No. 9, </volume> <pages> pp. 2174-2193, </pages> <year> 1992. </year>
Reference-contexts: Now consider a bigger and more widely used example, the DCT (Discrete Cosine Transform), to illustrate this point further. We will compare two implementation of the DCT: one proposed by Feig and Winograd <ref> [20] </ref> and the other, the direct maximally fast form. Feigs DCT algorithm can be derived from the direct form using the exceptionally sophisticated application of common subexpression elimination/replication and algebraic rules.
Reference: [21] <author> M. Potkonjak, J. Rabaey: </author> <title> Maximally Fast and Arbitrarily Fast Implementation of Linear Computations, </title> <journal> IEEE ICCAD, </journal> <pages> pp. 304-308. </pages>
Reference-contexts: The direct form can be derived from the Feigs DCT, by the simple application of the transformation set using the procedure for maximally fast implementation of linear computation <ref> [21] </ref>. While Feigs DCT has a critical path of 11 cycles, the maximally fast DCT has a critical path of only 7 cycles. Therefore the Vdd can be reduced from 5 V to 3.25 V . <p> For example, it has been shown that common subexpression can yield, for many classes of design implementation, arbitrarily high throughput at the expense of additional number of operations <ref> [21] </ref>.
Reference: [22] <author> M. Potkonjak and J. Rabaey, </author> <title> Optimizing the Resource Utilization Using Transformations, </title> <booktitle> Proc. IEEE ICCAD Conference, </booktitle> <pages> pp. 88-91, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: For example, it has been shown that common subexpression can yield, for many classes of design implementation, arbitrarily high throughput at the expense of additional number of operations [21]. Similarly , it has been demonstrated that retiming for throughput often results in designs with exceptionally high interconnect requirements <ref> [22] </ref>. 4.2 Operation Reduction The most obvious approach to reduce the switched capacitance, is to reduce the number of operations (and hence the number of switching events) in the data control ow graph. <p> interconnect power due to a reduction in the length of the bus lines (since the final implementation has a smaller area), resulting in a total power savings of 62%. 4.4 Resource Utilization It is often possible to reduce the required amount of hardware, while preserving the number of control steps <ref> [22] </ref>. This is possible because after certain transformations, operations are more uniformly distributed over the available time, resulting in a denser schedule. These transformations include retiming for resource utilization, associativity, distributivity and commutativity. Figure 6 shows the result of applying retiming on a second order IIR filter. <p> In the library of local moves we have implemented three algebraic transformations: Optimization Algorithm 21 of 27 associativity (generalized to include properties of inverse elements as introduced in <ref> [22] </ref>), commutativity , and local retiming. All global moves minimize the critical path using polynomial optimal algorithms. Their use is motivated by the need for shorter run-times.
Reference: [23] <author> G. Goertzel, </author> <title> An algorithm for the Evaluation of Finite Trigonometric Series, </title> <journal> Amer. Math. Monthly, </journal> <volume> Vol. 65, No. 1, </volume> <pages> pp. 34-35, </pages> <year> 1968. </year>
Reference-contexts: To illustrate this tradeoff, consider evaluating second and third order polynomials. Computation of polynomials is very common in digital signal processing, and Horner s scheme (the final structure in our examples) is often suggested in filter design and FFT calculations when very few frequency components are needed <ref> [23] </ref>. First we will analyze the second order polynomial X 2 + AX + B. The left side of Figure 4a shows the straightforward implementation which requires two multiplications and two additions and has a critical path of 3 (assuming that each operation takes one control cycle).
Reference: [24] <author> A. V. Aho, J.D. Ullman, </author> <booktitle> Principles of compiler Design, </booktitle> <address> Reading, </address> <publisher> Addison-Wesley, </publisher> <year> 1977. </year>
Reference-contexts: Using Transformations to Optimize Power 9 of 27 4.3 Operation Substitution Certain operations inherently require less energy per computation than other operations. A prime example of transformation which explores this tradeoff is strength reduction, often used in software compilers, in which multiplications are substituted by additions <ref> [24] </ref>. Although this situation is not as common as the ones presented in the previous section, sometimes it is possible to achieve signif icant savings using this type of tradeoff. Unfortunately, this type of transformation often comes at the expense of an increase in the critical path length.
Reference: [25] <author> C. S. Shung et. al., </author> <title> An Integrated CAD System for Algorithm-Specific IC Design., </title> <journal> IEEE Transactions on CAD of Integrated Circuits and Systems, </journal> <month> April </month> <year> 1991. </year>
Reference-contexts: The capacitance switched for a chained implementation is a factor of 1.5 larger than the tree implementation for a four input addition and 2.5 larger for an eight input addition. The above simulations were done on layouts generated by the LagerIV silicon compiler <ref> [25] </ref> using the IRSIM [17] switch-level simulator over 1000 uncorrelated random input patterns. The results presented above indicate that increasing the logic depth (through more cascading) will increase the capacitance due to glitching while reducing the logic depth will increase register power. <p> Of course, an accurate model for such a complex system can be built only when a particular set of design tools is targeted. As mentioned earlier, we targeted the HYPER high-level synthesis tools and the Lager IV silicon assembler <ref> [25] </ref>. W e used the scalable CMOS design rules provided by Mosis and targeted feature sizes of 1.2mm and 2mm. The selection of this particular suit of design tools, enabled us to somewhat simplify the estimation process.
Reference: [26] <author> D. Goldberg, </author> <title> What every computer scientist should know about oating-point arithmetic, </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 23, No. 1, </volume> <pages> pp. 5-48, </pages> <year> 1991. </year>
Reference-contexts: The inuence of various transformation on numerical stability (and therefore the required wordlength) varies a lot. While some transformations, for example retiming, pipelining and commutativity , do not af fect wordlength, associativity and distributivity often have a dramatic inuence <ref> [26] </ref>. In some cases, it is possible to reduce both the number of power expensive operations and the required wordlength. In other cases, however, a reduction in wordlength comes at the expense of an increased number of operations.
Reference: [27] <author> M. B. Srivastava, M. Potkonjak: </author> <title> Transforming Linear Systems for Joint Latency and Throughput Optimization, </title> <booktitle> EDAC-94, </booktitle> <pages> pp. 267-271, </pages> <year> 1994. </year>
Reference-contexts: Note that the direct form can be transformed to the parallel form (or vice-versa) using a specific ordered set of transformations <ref> [27] </ref>.
Reference: [28] <author> J. Rabaey and M. Potkonjak, </author> <title> Estimating Implementation Bounds for Real Time Application Specific Circuits, </title> <booktitle> European Solid-State Circuits Conference, Milano, Italy, </booktitle> <pages> pp. 201-204, </pages> <month> September 11-13, </month> <year> 1991. </year>
Reference-contexts: four components: C total = C exu + C registers + C interconnect + C control (EQ 5) The capacitance estimation is built on top of an existing estimation routine in HYPER that determines bounds and activity of various execution, register and interconnect components as well as the implementation area <ref> [28] </ref>, [29]. <p> In the hardware model used, the clock inverse is generated in the local controllers and fed to the execution units. The HYPER high level synthesis system provides a lower bound on the number of execution units within as accuracy of 10% <ref> [28] </ref>. The lower bound on the busses tracks the actual number of busses closely and the maximum number of busses tracks the total number of bus accesses. The numbers predicted by HYPER were therefore used in building the correlation model.
Reference: [29] <author> D. Schultz, </author> <title> The Inuence of Hardware Mapping on High-Level Synthesis, </title> <institution> U.C. Berkeley M.S. </institution> <note> report, ERL M92/ 54. </note>
Reference-contexts: components: C total = C exu + C registers + C interconnect + C control (EQ 5) The capacitance estimation is built on top of an existing estimation routine in HYPER that determines bounds and activity of various execution, register and interconnect components as well as the implementation area [28], <ref> [29] </ref>. The details of the capacitance estimation routines are described below. 5.1.1 Execution Units The capacitance switched by the execution units is estimated by multiplying (over all types of operations) the number of times the operation is performed per sample period and the average capacitance per access of the operation.
Reference: [30] <author> F.J. Kurdahi, A.C. Parker: </author> <title> Techniques and Area Estimation of VLSI Layouts, </title> <journal> IEEE Trans. on CAD, </journal> <volume> Vol. 8, No. 1, </volume> <pages> pp. 81-92, </pages> <month> Jan </month> <year> 1989. </year> <note> Conclusions 27 of 27 </note>
Reference-contexts: Driven by yield, oorplanning and synthesis considerations for throughput and area optimization, several elaborate prediction models for total chip and interconnect area have been built and successfully used <ref> [30] </ref>, [31]. However , high-level synthesis adds additional requirements on the prediction tools next to accuracy; during the optimization process in high level synthesis, it is necessary to estimate the f inal cost frequently and therefore computationally intensive models are prohibited, regardless of their precision.
Reference: [31] <author> W.R. Heller et al. </author> <title> Prediction of wiring space requirements for LSI, </title> <booktitle> IEEE/ACM Proc. 14th Design Automation Conference, </booktitle> <pages> pp. 20-22, </pages> <year> 1977. </year>
Reference-contexts: Driven by yield, oorplanning and synthesis considerations for throughput and area optimization, several elaborate prediction models for total chip and interconnect area have been built and successfully used [30], <ref> [31] </ref>. However , high-level synthesis adds additional requirements on the prediction tools next to accuracy; during the optimization process in high level synthesis, it is necessary to estimate the f inal cost frequently and therefore computationally intensive models are prohibited, regardless of their precision.
Reference: [32] <author> L. Breiman, J.H. Friedman, R.A. Olshen, C.J. Stone: </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth & Brooks, </publisher> <address> Monterey, CA 1984. </address>
Reference-contexts: It immediately became apparent that the best correlation is one between the total interconnect capacitance and the implementation area predicted by HYPER. It is widely recognized that the quality of the prediction model is inversely proportional to number of parameters used during the prediction model building <ref> [32] </ref>. The number of parameters is equal to the sum of the number of input variables in the model, and amount of data needed to describe the model.
Reference: [33] <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, W.T. Vetterling: </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: This relationship (of delay-V dd ) was modeled using Nevilles algorithm for rational function interpolation and extrapolation <ref> [33] </ref>. Nevilles algorithm provides an indirect way for constructing a polynomial of degree N-1 so that all of the used points are exactly matched. 26 with increments of 0.1) when compared to the experimental data. Figure 16 shows an overview of the power estimation routine.
Reference: [34] <author> C.E. Leiserson, J.B. Saxe, </author> <title> Retiming Synchronous Circuitry, </title> <journal> Algorithmica, </journal> <volume> Vol. 6., No. 1, </volume> <pages> pp. 5-35, </pages> <year> 1991. </year>
Reference-contexts: For example, we have shown that retiming for power minimization is a NP-complete problem. Computational complexity of retiming for power optimization is particularly interesting and somewhat surprising, since retiming for critical path reduction has several algorithms of polynomial-time complexity <ref> [34] </ref>. The computational complexity of the power minimization problem implies that it is very unlikely, even when the set of applied transformations is restricted, that polynomial time optimal algorithm can be designed. T wo widely used alternatives for design of high quality suboptimal optimization algorithms are probabilistic and heuristic algorithms.
Reference: [35] <author> G.B. Sorkin, </author> <title> Efficient Simulated Annealing on Fractal Energy Landscapes, </title> <journal> Algoritmica, </journal> <volume> Vol. 6, No. 3, </volume> <pages> pp. 367-418, </pages> <year> 1991. </year>
Reference-contexts: Boltzmann machine). Although recently there has been a considerable effort in analyzing these algorithms and the type of problems they are best suited for (for example a deep relationship between simulated annealing and solution space with fractal topology have been verified both experimentally and theoretically <ref> [35] </ref>), algorithm selection for the task at hand is still mainly an experimental and intuitive art. In order to satisfy all major considerations for the power minimization problem, we decided to use a combination of heuristic and probabilistic algorithms so that we can leverage on the advantages of both approaches.
Reference: [36] <author> Algoritmica, </author> <title> Special Issue, Simulated Annealing, </title> <editor> A. Sangiovanni-Vincentelli, ed. </editor> <volume> Vol. 6, No. 3, </volume> <pages> pp. 295-478, </pages> <year> 1991. </year>
Reference-contexts: This step is followed by simulated annealing which tries to improve the initial solution by applying local moves probabilistically. For simulated annealing we used the most popular set of parameters <ref> [36] </ref>. For example, we used a geometric cooling schedule with a stopping criteria which Examples and Results 22 of 27 terminates the probabilistic search when no improvement was observed on three consecutive temperatures. After each move, a list of all possible moves is generated for each local transformation.
Reference: [37] <institution> Comsat General Integrated Systems, </institution> <note> Super-Filsyn Users Manual, </note> <month> March </month> <year> 1982. </year>
Reference-contexts: Table 4 shows the statistics of the initial and final designs. Note that area has been traded for lower power. 7.2 Example #2: IIR Filter The second example is a 16-bit 7th order IIR filter with a 4th order equalizer designed using the filter design program Filsyn <ref> [37] </ref>. This example is representative of the lar gest class of examples where feedback is an inherent but not particularly limiting part of the computation structure.
Reference: [38] <author> A. P. Chandrakasan, A. Burstein, R. W. Brodersen, </author> <title> A Low-power Chipset for Multimedia Applications, </title> <booktitle> IEEE International Solid-state Circuits Conference, </booktitle> <pages> pp. 82-83, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: Voltage level conversion circuitry can be implemented with very low area and power overhead <ref> [38] </ref>. The use of multiple supply voltages (in which each part of the system operates at its own optimum voltage) can result in signif icant system power reduction compared to a solution which uses a single power supply voltage.
References-found: 38

