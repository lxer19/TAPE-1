URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/jass/www/papers/ppopp97.ps
Refering-URL: http://www.cs.rice.edu/~adve/comp615.html
Root-URL: 
Email: fjass,bwoleng@cs.cmu.edu  
Title: A New Model for Integrated Nested Task and Data Parallel Programming  
Author: Jaspal Subhlok and Bwolen Yang 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Note: Appears in PPOPP 97  
Abstract: High Performance Fortran (HPF) has emerged as a standard language for data parallel computing. However, a wide variety of scientific applications are best programmed by a combination of task and data parallelism. Therefore, a good model of task parallelism is important for continued success of HPF for parallel programming. This paper presents a task parallelism model that is simple, elegant, and relatively easy to implement in an HPF environment. Task parallelism is exploited by mechanisms for dividing processors into subgroups and mapping computations and data onto processor subgroups. This model of task parallelism has been implemented in the Fx compiler at Carnegie Mellon University. The paper addresses the main issues in compiling integrated task and data parallel programs and reports on the use of this model for programming various flat and nested task structures. Performance results are presented for a set of programs spanning signal processing, image processing, computer vision and environment modeling. A variant of this task model is a new approved extension of HPF and this paper offers insight into the power of expression and ease of implementation of this extension. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agrawal, G., Sussman, A., and Saltz, J. </author> <title> An integrated runtime and compile-time approach for par-allelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 6, </journal> <month> 7 (July </month> <year> 1995), </year> <pages> 747-754. </pages>
Reference-contexts: The reason is that individual data sets for these applications are often small, and therefore, a pure data parallel implementation may not scale well. Multiblock codes containing irregularly structured regular meshes <ref> [1] </ref> are more naturally programmed as interacting tasks with each task representing a regular mesh, rather than as a single large irregular application. Tree structured algorithms like Barnes-Hut for N-body problems [2] need task parallelism to recursively divide the available processors to solve the recursively generated subproblems.
Reference: [2] <author> Barnes, J., and P.Hut. </author> <title> A hierarchical O(N log N) force calculation algorithm. </title> <booktitle> Nature 4, 324 (1986), </booktitle> <pages> 446-449. </pages>
Reference-contexts: Multiblock codes containing irregularly structured regular meshes [1] are more naturally programmed as interacting tasks with each task representing a regular mesh, rather than as a single large irregular application. Tree structured algorithms like Barnes-Hut for N-body problems <ref> [2] </ref> need task parallelism to recursively divide the available processors to solve the recursively generated subproblems. Multidisciplinary applications like air quality modeling [13] are often modeled as sets of interacting modules, where each module represents a different scientific discipline.
Reference: [3] <author> Chapman, B., Mehrotra, P., Van Rosendale, J., and Zima, H. </author> <title> A software architecture for multidisciplinary applications: Integrating task and data parallelism. </title> <type> Tech. Rep. 94-18, </type> <institution> ICASE, NASA Langley Research Center, Hampton, VA, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Finally, applications that interact with external devices like cameras and display devices need special tasks for such activities. Integration of task and data parallelism is currently an active area of research and several compiler based and run-time system based approaches have been proposed <ref> [3, 6, 7, 9] </ref>. Compiler based solutions presented in the literature are either limited in terms of the forms of task parallelism structures they can support or have not demonstrated that they can be compiled efficiently. <p> We shall state just a few of them and discuss the relative merits of the model presented in this paper. In Opus <ref> [3, 14] </ref>, task parallelism is obtained by invocation of task programs (like subroutines) onto a specified set of processor resources. This is broadly similar to our approach but the task parallelism can only be exploited at subroutine call level.
Reference: [4] <author> Chapman, B., Mehrotra, P., and Zima, H. </author> <title> Programming in Vienna Fortran. </title> <booktitle> Scientific Programming 1, </booktitle> <month> 1 (Aug. </month> <year> 1992), </year> <pages> 31-50. </pages>
Reference-contexts: 1 Introduction Use of high level languages is gaining popularity as a means to portable and efficient parallel programming. Research and development in parallel languages has primarily focused on data parallelism <ref> [4, 11, 18] </ref> and High Performance Fortran [12] has emerged as a promising standard language for parallel programming. A disadvantage of using a parallel Effort sponsored by the Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-96-1-0287. The U.S.
Reference: [5] <author> Dinda, P., Gross, T., O'Hallaron, D., Segall, E., Stichnoth, J., Subhlok, J., Webb, J., and Yang, B. </author> <title> The CMU task parallel program suite. </title> <type> Tech. Rep. </type> <institution> CMU-CS-94-131, School of Computer Science, Carnegie Mellon University, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: A variation of this model is now an approved extension of HPF [10]. Most parallel applications contain data parallelism, but a large number of such applications need a combination of task and data parallelism to represent the natural computation structure or to enhance performance. Pipelines of data parallel tasks <ref> [5] </ref>, a common computation structure in image processing, signal processing, and computer vision, require a combination of task and data parallelism to achieve good performance. The reason is that individual data sets for these applications are often small, and therefore, a pure data parallel implementation may not scale well. <p> We will skip further details of these applications, which are described in <ref> [5] </ref> and corresponding results from an earlier Fx implementation of task parallelism are discussed in [20]. The structure of each of these three programs is a pipeline of data parallel tasks that processes streams of independent input data sets.
Reference: [6] <author> Foster, I., Avalani, B., Choudhary, A., and Xu, M. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proceeding of 1994 Scalable High Performance Computing Conference (Knoxville, </booktitle> <address> TN, </address> <month> October </month> <year> 1994), </year> <pages> pp. 293-300. </pages>
Reference-contexts: Finally, applications that interact with external devices like cameras and display devices need special tasks for such activities. Integration of task and data parallelism is currently an active area of research and several compiler based and run-time system based approaches have been proposed <ref> [3, 6, 7, 9] </ref>. Compiler based solutions presented in the literature are either limited in terms of the forms of task parallelism structures they can support or have not demonstrated that they can be compiled efficiently.
Reference: [7] <author> Foster, I., Kohr, D., Krishnaiyer, R., and Choudhary, A. </author> <title> Double standards: Bringing task parallelism to HPF via the Message Passing Interface. </title> <booktitle> In Supercomputing '96 (Pittsburgh, </booktitle> <address> PA, </address> <month> November </month> <year> 1996). </year>
Reference-contexts: Finally, applications that interact with external devices like cameras and display devices need special tasks for such activities. Integration of task and data parallelism is currently an active area of research and several compiler based and run-time system based approaches have been proposed <ref> [3, 6, 7, 9] </ref>. Compiler based solutions presented in the literature are either limited in terms of the forms of task parallelism structures they can support or have not demonstrated that they can be compiled efficiently. <p> An alternate approach to integrated task and data parallelism is use of a coordination language to communicate between tasks. An example of such an approach is presented in <ref> [7] </ref> where a subset of MPI is used to coordinate HPF tasks. This approach has the obvious and important advantage that the data parallel language does not have to be modified at all, but consequently, it does not offer a single clear programming model or semantic notion.
Reference: [8] <author> Gropp, W., Lusk, E., and Skjellum, A. </author> <title> Using MPI: Portable parallel processing with the Message Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Government. language like HPF, as compared to explicit parallel programming using, e.g., a message passing library like MPI <ref> [8] </ref>, is that the user is constrained by the models of parallelism supported by the language. Application developers are often reluctant to use HPF because many applications do not completely fit the HPF data parallelism model.
Reference: [9] <author> Gross, T., O'Hallaron, D., and Subhlok, J. </author> <title> Task parallelism in a High Performance Fortran framework. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 3 (1994), </volume> <pages> 16-26. </pages>
Reference-contexts: Finally, applications that interact with external devices like cameras and display devices need special tasks for such activities. Integration of task and data parallelism is currently an active area of research and several compiler based and run-time system based approaches have been proposed <ref> [3, 6, 7, 9] </ref>. Compiler based solutions presented in the literature are either limited in terms of the forms of task parallelism structures they can support or have not demonstrated that they can be compiled efficiently. <p> The implementation of data parallelism in Fx is discussed in [18, 24]. An earlier version of task parallelism is discussed in <ref> [9] </ref> and related work in automatic mapping in [21, 22]. In this section, we outline the major issues in implementing our new task parallelism model in Fx. <p> Opus also uses shared data abstraction to exchange data between executing tasks. While some form of sharing between tasks is indeed desirable, it makes the language and its implementation more complex and there is not enough evidence yet to justify the added complexity. Earlier work on Fx <ref> [9] </ref> was based on exploiting task parallelism by specifying task subroutines and their input and output parameters, but leaving all the details of task management to the compiler.
Reference: [10] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <note> Draft Version 2.0, </note> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: This paper presents a simple model of task parallelism which significantly extends the domain of applications that can be efficiently programmed in a language like HPF. A variation of this model is now an approved extension of HPF <ref> [10] </ref>. Most parallel applications contain data parallelism, but a large number of such applications need a combination of task and data parallelism to represent the natural computation structure or to enhance performance. <p> The execution directives include TASK REGION directives, which partition current processors and define the task parallelism region, and ON SUBGROUP directives which specify the execution of a code block on a processor subgroup. 1 A similar concept is described with active processors in HPF terminology <ref> [10] </ref> Declaration directives A TASK PARTITION directive provides a template for partitioning the current group of processors into subgroups.
Reference: [11] <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <title> Compiling fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM 35, </journal> <month> 8 (August </month> <year> 1992), </year> <pages> 66-80. </pages>
Reference-contexts: 1 Introduction Use of high level languages is gaining popularity as a means to portable and efficient parallel programming. Research and development in parallel languages has primarily focused on data parallelism <ref> [4, 11, 18] </ref> and High Performance Fortran [12] has emerged as a promising standard language for parallel programming. A disadvantage of using a parallel Effort sponsored by the Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-96-1-0287. The U.S.
Reference: [12] <author> Koelbel, C., Loveman, D., Steele, G., and Zosel, M. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Use of high level languages is gaining popularity as a means to portable and efficient parallel programming. Research and development in parallel languages has primarily focused on data parallelism [4, 11, 18] and High Performance Fortran <ref> [12] </ref> has emerged as a promising standard language for parallel programming. A disadvantage of using a parallel Effort sponsored by the Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-96-1-0287. The U.S.
Reference: [13] <author> McRae, G., Russell, A., and Harley, R. </author> <title> CIT Photochemical Airshed Model Systems Manual. </title> <institution> Carnegie Mellon University, Pittsburgh, PA, and California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: Tree structured algorithms like Barnes-Hut for N-body problems [2] need task parallelism to recursively divide the available processors to solve the recursively generated subproblems. Multidisciplinary applications like air quality modeling <ref> [13] </ref> are often modeled as sets of interacting modules, where each module represents a different scientific discipline. Finally, applications that interact with external devices like cameras and display devices need special tasks for such activities. <p> The main point is that integrated task and data parallelism offers a variety of ways in which such applications can be mapped, and facilitates the development of automatic tools to achieve different performance goals. 5.2 Airshed simulation Airshed air quality simulation model developed by McRae and Russell <ref> [13] </ref> models the formation, reaction, and transport of atmospheric pollutants and related chemical species. The main data structure used in airshed simulation is a concentration matrix whose dimensions (with typical values) are number of atmospheric layers (5), number of grid points (500-5000) and number of chemical species (35).
Reference: [14] <author> Mehrotra, P., and Haines, M. </author> <title> An overview of the Opus language and runtime system. </title> <type> Tech. Rep. 94-39, </type> <institution> ICASE, NASA Langley Research Center, Hampton, VA, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: We shall state just a few of them and discuss the relative merits of the model presented in this paper. In Opus <ref> [3, 14] </ref>, task parallelism is obtained by invocation of task programs (like subroutines) onto a specified set of processor resources. This is broadly similar to our approach but the task parallelism can only be exploited at subroutine call level.
Reference: [15] <author> Okutomi, M., and Kanade, T. </author> <title> A multiple-baseline stereo. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 15, </journal> <volume> 4 (1993), </volume> <pages> 353-363. </pages>
Reference-contexts: The processing of an input data set by the radar program consists of four steps: a corner turn to form a transposed matrix, independent row-FFTs, scaling, and thresholding. The multibase-line stereo uses an algorithm developed at Carnegie Mellon to measure depth in a scene accurately by using multiple cameras <ref> [15, 23] </ref>. Input consists of three (or more) images acquired from the cameras.
Reference: [16] <author> Ramaswamy, S., Sapatnekar, S., and Banerjee, P. </author> <title> A convex programming approach for exploiting data and functional parallelism. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing (St Charles, </booktitle> <address> IL, </address> <month> August </month> <year> 1994), </year> <journal> vol. </journal> <volume> 2, </volume> <pages> pp. 116-125. </pages>
Reference: [17] <author> Shaw, G., Gabel, R., Martinez, D., Rocco, A., Pohlig, S., Gerber, A., Noonan, J., and Teitel-baum, K. </author> <title> Multiprocessors for radar signal processing. </title> <type> Tech. Rep. 961, </type> <institution> MIT Lincoln Laboratory, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The narrowband tracking radar benchmark was developed by researchers at MIT Lincoln Labs <ref> [17] </ref>. The processing of an input data set by the radar program consists of four steps: a corner turn to form a transposed matrix, independent row-FFTs, scaling, and thresholding.
Reference: [18] <author> Stichnoth, J., O'Hallaron, D., and Gross, T. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing 21, </journal> <volume> 1 (1994), </volume> <pages> 150-159. </pages>
Reference-contexts: 1 Introduction Use of high level languages is gaining popularity as a means to portable and efficient parallel programming. Research and development in parallel languages has primarily focused on data parallelism <ref> [4, 11, 18] </ref> and High Performance Fortran [12] has emerged as a promising standard language for parallel programming. A disadvantage of using a parallel Effort sponsored by the Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-96-1-0287. The U.S. <p> Fx provides directives to guide the layout of data arrays onto groups of processors to help exploit data parallelism <ref> [18] </ref>. Supported data distributions include block, cyclic and block-cyclic. Loop parallelism is expressed by a special parallel loop construct that combines loop and reduction parallelism [24]. <p> The implementation of data parallelism in Fx is discussed in <ref> [18, 24] </ref>. An earlier version of task parallelism is discussed in [9] and related work in automatic mapping in [21, 22]. In this section, we outline the major issues in implementing our new task parallelism model in Fx.
Reference: [19] <author> Stricker, T., Stichnoth, J., O'Hallaron, D., Hinrichs, S., and Gross, T. </author> <title> Decoupling synchronization and data transfer in message passing systems of parallel computers. </title> <booktitle> In Proceedings of the 1995 International Conference on Supercomputing (Barcelona, </booktitle> <month> July </month> <year> 1995), </year> <booktitle> ACM, </booktitle> <pages> pp. 1-10. </pages>
Reference-contexts: Localization Localization of computation, communication, and synchronization to a subgroup of processors is critical for exploiting task parallelism. The basic communication model in Fx is direct deposit of data by a sender to a receiver's memory space with the use of global barriers for synchronization <ref> [19] </ref>. To support task parallelism, the barriers are replaced by subset barriers on subgroups of virtual processors. At run-time, the physical processors are identified and only those processors participate in a barrier.
Reference: [20] <author> Subhlok, J., O'Hallaron, D., Gross, T., Dinda, P., and Webb, J. </author> <title> Communication and memory requirements as the basis for mapping task and data parallel programs. </title> <booktitle> In Supercomputing '94 (Washington, </booktitle> <address> DC, </address> <month> November </month> <year> 1994), </year> <pages> pp. 330-339. </pages>
Reference-contexts: We will skip further details of these applications, which are described in [5] and corresponding results from an earlier Fx implementation of task parallelism are discussed in <ref> [20] </ref>. The structure of each of these three programs is a pipeline of data parallel tasks that processes streams of independent input data sets.
Reference: [21] <author> Subhlok, J., and Vondran, G. </author> <title> Optimal mapping of sequences of data parallel tasks. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (Santa Barbara, </booktitle> <address> CA, </address> <month> July </month> <year> 1995), </year> <pages> pp. 134-143. </pages>
Reference-contexts: The implementation of data parallelism in Fx is discussed in [18, 24]. An earlier version of task parallelism is discussed in [9] and related work in automatic mapping in <ref> [21, 22] </ref>. In this section, we outline the major issues in implementing our new task parallelism model in Fx. Our model of task parallelism allows the programmer to assert that a computation can be efficiently and correctly performed by a subgroup of processors. <p> If there is no throughput requirement (i.e. minimizing latency is the only goal) then the simple data parallel mapping on the left is optimal, but with different throughput requirements, the best mapping changes. This model, along with the use of mapping algorithms presented in <ref> [21, 22] </ref>, allows us to automatically determine the best mapping for a program for different performance goals. Table 1 shows the results obtained for this set of applications by setting a throughput goal and optimizing for minimum latency.
Reference: [22] <author> Subhlok, J., and Vondran, G. </author> <title> Optimal latency-throughput tradeoffs for data parallel pipelines. </title> <booktitle> In Eighth Annual ACM Symposium on Parallel Algorithms and Architectures (Padua, </booktitle> <address> Italy, </address> <month> June </month> <year> 1996), </year> <pages> pp. 62-71. </pages>
Reference-contexts: The implementation of data parallelism in Fx is discussed in [18, 24]. An earlier version of task parallelism is discussed in [9] and related work in automatic mapping in <ref> [21, 22] </ref>. In this section, we outline the major issues in implementing our new task parallelism model in Fx. Our model of task parallelism allows the programmer to assert that a computation can be efficiently and correctly performed by a subgroup of processors. <p> If there is no throughput requirement (i.e. minimizing latency is the only goal) then the simple data parallel mapping on the left is optimal, but with different throughput requirements, the best mapping changes. This model, along with the use of mapping algorithms presented in <ref> [21, 22] </ref>, allows us to automatically determine the best mapping for a program for different performance goals. Table 1 shows the results obtained for this set of applications by setting a throughput goal and optimizing for minimum latency.
Reference: [23] <author> Webb, J. </author> <title> Latency and bandwidth consideration in parallel robotics image processing. </title> <booktitle> In Supercomputing '93 (Portland, </booktitle> <address> OR, </address> <month> Nov. </month> <year> 1993), </year> <pages> pp. 230-239. </pages>
Reference-contexts: The processing of an input data set by the radar program consists of four steps: a corner turn to form a transposed matrix, independent row-FFTs, scaling, and thresholding. The multibase-line stereo uses an algorithm developed at Carnegie Mellon to measure depth in a scene accurately by using multiple cameras <ref> [15, 23] </ref>. Input consists of three (or more) images acquired from the cameras.
Reference: [24] <author> Yang, B., Webb, J., Stichnoth, J., O'Hallaron, D., and Gross, T. Do&merge: </author> <title> Integrating parallel loops and reductions. </title> <booktitle> In Sixth Annual Workshop on Languages and Compilers for Parallel Computing (Portland, </booktitle> <address> Oregon, </address> <month> Aug </month> <year> 1993). </year> <month> 12 </month>
Reference-contexts: Fx provides directives to guide the layout of data arrays onto groups of processors to help exploit data parallelism [18]. Supported data distributions include block, cyclic and block-cyclic. Loop parallelism is expressed by a special parallel loop construct that combines loop and reduction parallelism <ref> [24] </ref>. Since Fx is conceptually similar to HPF in terms of data parallelism, and the details are not directly relevant to this research, we will not discuss them any further. <p> The implementation of data parallelism in Fx is discussed in <ref> [18, 24] </ref>. An earlier version of task parallelism is discussed in [9] and related work in automatic mapping in [21, 22]. In this section, we outline the major issues in implementing our new task parallelism model in Fx.
References-found: 24

