URL: http://www.cs.monash.edu.au/~jono/TechReports/ml96.ps
Refering-URL: http://www.cs.monash.edu.au/~jono/
Root-URL: 
Email: jono@cs.monash.edu.au  rohan@cs.monash.edu.au  csw@cs.monash.edu.au  
Title: Unsupervised Learning Using MML  
Author: Jonathan J. Oliver Rohan A. Baxter Chris S. Wallace 
Address: Victoria, 3168, Australia  Victoria, 3168, Australia  Victoria, 3168, Australia  
Affiliation: Computer Science Dept. Monash University, Clayton,  Computer Science Dept. Monash University, Clayton,  Computer Science Dept. Monash University, Clayton,  
Abstract: This paper discusses the unsupervised learning problem. An important part of the unsupervised learning problem is determining the number of constituent groups (components or classes) which best describes some data. We apply the Minimum Message Length (MML) criterion to the unsupervised learning problem, modifying an earlier such MML application. We give an empirical comparison of criteria prominent in the literature for estimating the number of components in a data set. We conclude that the Minimum Message Length criterion performs better than the alternatives on the data considered here for unsupervised learning tasks.
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> R.A. Baxter. </author> <title> Finding overlapping distributions with MML. </title> <type> Technical Report 244, </type> <institution> Dept. of Computer Science, Monash University, Clayton 3168, Australia, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: There are k! permutations for labelling the components in the message. This creates a saving in message length of: log (k!) (11) Substituting Equations (6), (10) and (11) into Equation (5) gives us the expression we wish to minimise: 3 Baxter <ref> [ 1 ] </ref> discusses other approximations of the Fisher Information Matrix.
Reference: [ 2 ] <author> J.C. Bezdek. </author> <title> Pattern Recognition with Fuzzy Objective Function Algorithms. </title> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Hypothesis testing using the 2 test proposed by Wolfe [ 30 ] has been shown to be invalid [ 3 ] . A range of criteria for selecting k have been proposed, including: * Minimum message length (MML) [ 25 ] , * Partition coefficient (PC) <ref> [ 2 ] </ref> , * Akaike's information criterion (AIC) [ 6 ] , * ICOMP [ 4; 5 ] and * Minimum description length (MDL) [ 19 ] (identi cal to Schwarz's criterion [ 22 ] ).
Reference: [ 3 ] <author> D.A. Binder. </author> <title> Bayesian cluster analysis. </title> <journal> Biometrika, </journal> <volume> 65 </volume> <pages> 31-38, </pages> <year> 1978. </year>
Reference-contexts: For example, a unimodal distribution may have more than one component (examples of this type are given in Section 6.4). Hypothesis testing using the 2 test proposed by Wolfe [ 30 ] has been shown to be invalid <ref> [ 3 ] </ref> .
Reference: [ 4 ] <author> H. Bozdogan. </author> <title> On the information-based measure of covariance complexity and its application to the evaluation of multivariate linear models. </title> <journal> Communications in Statistics: Theory and Methods (A), </journal> <volume> 19(1) </volume> <pages> 221-278, </pages> <year> 1990. </year>
Reference-contexts: A range of criteria for selecting k have been proposed, including: * Minimum message length (MML) [ 25 ] , * Partition coefficient (PC) [ 2 ] , * Akaike's information criterion (AIC) [ 6 ] , * ICOMP <ref> [ 4; 5 ] </ref> and * Minimum description length (MDL) [ 19 ] (identi cal to Schwarz's criterion [ 22 ] ). All these methods use a two step process to determine the number of components. <p> We select the k with the minimum value of M DL (k). 3.4 ICOMP CRITERION ICOMP is a non-coding information theoretic criterion developed by Bozdogan <ref> [ 4; 5 ] </ref> .
Reference: [ 5 ] <author> H. Bozdogan. </author> <title> Mixture-model cluster analysis using model selection criteria and a new informational measure of complexity. </title> <booktitle> In Proc. of the First US/Japan Conf. on the Frontiers of Statistical Modeling: An Informational Approach, </booktitle> <pages> pages 69-113. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: A range of criteria for selecting k have been proposed, including: * Minimum message length (MML) [ 25 ] , * Partition coefficient (PC) [ 2 ] , * Akaike's information criterion (AIC) [ 6 ] , * ICOMP <ref> [ 4; 5 ] </ref> and * Minimum description length (MDL) [ 19 ] (identi cal to Schwarz's criterion [ 22 ] ). All these methods use a two step process to determine the number of components. <p> We select the k with the minimum value of M DL (k). 3.4 ICOMP CRITERION ICOMP is a non-coding information theoretic criterion developed by Bozdogan <ref> [ 4; 5 ] </ref> . <p> We select the k with the minimum value of M DL (k). 3.4 ICOMP CRITERION ICOMP is a non-coding information theoretic criterion developed by Bozdogan [ 4; 5 ] . ICOMP is <ref> [ 5, page 87 ] </ref> : "based on the generalization and utilization of an entropic covariance complexity index . . . for a multivariate normal distribution." ICOM P (k) = L k + C 1 (I () 1 ) where L k is the logarithm of the likelihood at the maximum <p> C 1 (I () 1 ) = 2 trace (I () 1 ) n p 2 where n p is the number of estimated parameters. ICOMP has been claimed to balance `lack of fit and the model complexity' and to `represent a compromise between MDL and AIC' <ref> [ 5, page 88 ] </ref> . ICOMP is a measure dependent on the coordinate system of the parameters [ 10 ] . 4 MINIMUM MESSAGE LENGTH CRITERION (MML) Wallace and Freeman [ 27 ] argue that statistical estimation can be performed as a coding process. <p> popm 2k and replace the determinant of the Fisher Information matrix in Equation (10), with 1 log det (F ()) m=1 j=1 p 2 + 2 1 k X log p j Future work will extend MML to general covariance matrices and covariance matrices of different struc tures, following Bozdogan <ref> [ 5 ] </ref> and Cheeseman [ 8 ] . 6 IMPLEMENTATION AND RESULTS We implemented a mixture modelling system in C (the source is available from the authors by email).
Reference: [ 6 ] <author> H. Bozdogan. </author> <title> Determining the number of component clusters in the standard multivariate normal mixture model using model-selection criteria. </title> <type> TR UIC/DQM/A83-1, </type> <institution> Quantitative Methods Dept., University of Illinois, Chicago, Illinois 60680, </institution> <month> June 16, </month> <year> 1983. </year>
Reference-contexts: A range of criteria for selecting k have been proposed, including: * Minimum message length (MML) [ 25 ] , * Partition coefficient (PC) [ 2 ] , * Akaike's information criterion (AIC) <ref> [ 6 ] </ref> , * ICOMP [ 4; 5 ] and * Minimum description length (MDL) [ 19 ] (identi cal to Schwarz's criterion [ 22 ] ). All these methods use a two step process to determine the number of components. <p> We use the following AIC criterion offered by Bozdogan <ref> [ 6 ] </ref> and used by Windham and Cutler [ 28 ] in their comparison of mixture modelling methods.
Reference: [ 7 ] <author> P. Cheeseman, M. Self, J. Kelly, W. Taylor, D. Freeman, and J. Stutz. </author> <title> Bayesian classification. </title> <booktitle> In Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 607-611, </pages> <address> Saint Paul, Minnesota, </address> <year> 1988. </year>
Reference-contexts: There are many approaches to unsupervised learning. Within AI there have been systems such as (a) CLUSTER - Michalski and Stepp [ 16 ] , (b) COBWEB - Fisher [ 13 ] , (c) AQ17 - Wnek and Michalski [ 29 ] , (d) AUTOCLASS - Cheeseman et al. <ref> [ 7 ] </ref> and (e) Snob - Wallace et al. [ 25; 26 ] which uses MML.
Reference: [ 8 ] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (AUTOCLASS): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> The AAAI Press, </publisher> <address> Menlo Park, </address> <year> 1995. </year>
Reference-contexts: determinant of the Fisher Information matrix in Equation (10), with 1 log det (F ()) m=1 j=1 p 2 + 2 1 k X log p j Future work will extend MML to general covariance matrices and covariance matrices of different struc tures, following Bozdogan [ 5 ] and Cheeseman <ref> [ 8 ] </ref> . 6 IMPLEMENTATION AND RESULTS We implemented a mixture modelling system in C (the source is available from the authors by email). We tested the effectiveness of the methods for selecting the number of components in a mixture model by repeatedly generating data from artificial mixture models.
Reference: [ 9 ] <author> J.H. Conway and N.J.A Sloane. </author> <title> Sphere Packings, Lattices and Groups. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: We used values for n p from Table 2.3 of Conway and Sloane <ref> [ 9 ] </ref> . To use Equation (5), we must first choose a prior distribution, h (), and derive an expression for the deter minant of the Fisher Information matrix, F (). 1 Given that the likelihood function satisfies some regularity conditions, and that the prior is not rapidly changing.
Reference: [ 10 ] <author> A. </author> <title> Cutler and M.P. Windham. Information-based validity functionals for mixture analysis. </title> <editor> In H. Bozdogan and other, editors, </editor> <booktitle> Proc. of the First US/Japan Conf. on the Frontiers of Statistical Modeling: An Informational Approach, </booktitle> <pages> pages 149-170. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: ICOMP has been claimed to balance `lack of fit and the model complexity' and to `represent a compromise between MDL and AIC' [ 5, page 88 ] . ICOMP is a measure dependent on the coordinate system of the parameters <ref> [ 10 ] </ref> . 4 MINIMUM MESSAGE LENGTH CRITERION (MML) Wallace and Freeman [ 27 ] argue that statistical estimation can be performed as a coding process.
Reference: [ 11 ] <author> N.E. Day. </author> <title> Estimating the components of a mixture of normal distributions. </title> <journal> Biometrika, </journal> <volume> 56 </volume> <pages> 463-474, </pages> <year> 1969. </year>
Reference-contexts: A variety of approaches has been used to estimate these parameters. Wolfe [ 30 ] and Day <ref> [ 11 ] </ref> give maximum likelihood estimates for j ; j and p j given k. The standard maximum likelihood approach has difficulty if we allow each component to have distinct standard deviations, since in this case the likelihood is unbounded [ 11 ] . <p> Wolfe [ 30 ] and Day <ref> [ 11 ] </ref> give maximum likelihood estimates for j ; j and p j given k. The standard maximum likelihood approach has difficulty if we allow each component to have distinct standard deviations, since in this case the likelihood is unbounded [ 11 ] . In practice researchers take the largest finite maximum of the likelihood function. Snob uses Wallace's Minimum Message Length (MML) method [ 25; 27 ] for mixture modelling of Gaussian, von Mises and other distributions. <p> This paper compares alternative techniques for estimating the number of components k, with Wallace's MML method which has been previously implemented in Snob [ 25; 26 ] . 2 ESTIMATING THE p j , j AND j Typically, the p j , j and j are estimated using the EM <ref> [ 11; 30 ] </ref> algorithm.
Reference: [ 12 ] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: An important part of the unsupervised learning problem is determining the number of constituent groups (which we shall call components for the remainder of this paper) which best describes some data. Snob and AUTOCLASS are in the category of mixture modellers <ref> [ 12; 24; 15 ] </ref> . They assume that the data under consideration was generated from a distribution which is the sum of simpler distributions. <p> An inherent problem with this estimation task is that "by using enough components, an adequate fit can always be found, but its adequacy does not always imply its meaningfulness" <ref> [ 12 ] </ref> . Graphical tools such as histograms and probability plots to compare a range of candidate k values. However, there is evidence that visual inspection of such graphs is not an adequate method.
Reference: [ 13 ] <author> D.H. Fisher. </author> <title> Conceptual clustering, learning from examples, and inference. </title> <booktitle> In Machine Learning: Proceedings of the Fourth International Workshop, </booktitle> <pages> pages 38-49. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: 1 INTRODUCTION We discuss the unsupervised learning problem. There are many approaches to unsupervised learning. Within AI there have been systems such as (a) CLUSTER - Michalski and Stepp [ 16 ] , (b) COBWEB - Fisher <ref> [ 13 ] </ref> , (c) AQ17 - Wnek and Michalski [ 29 ] , (d) AUTOCLASS - Cheeseman et al. [ 7 ] and (e) Snob - Wallace et al. [ 25; 26 ] which uses MML.
Reference: [ 14 ] <author> A.C. Harvey. </author> <title> The Econometric Analysis of Time Series. </title> <editor> Philip Allan, </editor> <publisher> Oxford, </publisher> <year> 1981. </year>
Reference-contexts: of the Fisher Information matrix for each component, times the determinant of the Fisher Information matrix of the p j : det (F ()) det (F (p)) fi j=1 where the Fisher Information matrix for a single Gaussian distribution with mean, j , and the standard de viation, j is <ref> [ 14, Page 90 ] </ref> : F j ( j ; j ) = 2 0 2n j j where n j = p j fi n is the expected number of items in component j.
Reference: [ 15 ] <author> G.J. McLachlan and K.E. Basford. </author> <title> Mixture Models. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: An important part of the unsupervised learning problem is determining the number of constituent groups (which we shall call components for the remainder of this paper) which best describes some data. Snob and AUTOCLASS are in the category of mixture modellers <ref> [ 12; 24; 15 ] </ref> . They assume that the data under consideration was generated from a distribution which is the sum of simpler distributions.
Reference: [ 16 ] <author> R.S. Michalski and R. Stepp. </author> <title> Learning from observation: conceptual clustering. In R.S. </title> <editor> Michal-ski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: 1 INTRODUCTION We discuss the unsupervised learning problem. There are many approaches to unsupervised learning. Within AI there have been systems such as (a) CLUSTER - Michalski and Stepp <ref> [ 16 ] </ref> , (b) COBWEB - Fisher [ 13 ] , (c) AQ17 - Wnek and Michalski [ 29 ] , (d) AUTOCLASS - Cheeseman et al. [ 7 ] and (e) Snob - Wallace et al. [ 25; 26 ] which uses MML.
Reference: [ 17 ] <author> G.W. Milligan and M.C. Cooper. </author> <title> An examination of procedures for determining the number of clusters in data set. </title> <journal> Psychometrika, </journal> <volume> 50 </volume> <pages> 159-179, </pages> <year> 1985. </year>
Reference-contexts: For a comparative study of these approaches, see Milligan and Cooper <ref> [ 17 ] </ref> . These techniques come in three broad categories: (1) graphical tools and visual inspection, (2) hypothesis testing, and (3) criteria for comparing models.
Reference: [ 18 ] <author> J.J. Oliver and R.A. Baxter. </author> <title> MML and Bayesianism: Similarities and differences. </title> <type> Technical report TR 206, </type> <institution> Dept. of Computer Science, Monash University, </institution> <address> Clay-ton, Victoria 3168, Australia, </address> <year> 1994. </year> <note> Available on the WWW from http://www.cs.monash.edu.au/ ~ jono. </note>
Reference-contexts: MML is an information theoretic criterion for parameter estimation and model selection, which is superficially similar to Rissanen's MDL criterion [ 19; 21 ] . However, MML is a Bayesian criterion <ref> [ 18 ] </ref> , and hence uses an explicit prior distribution over parameter values. We note that the AU-TOCLASS criterion is similar to the Snob criterion. <p> A full derivation of Equation (5) is given Section 5 of Oliver and Baxter <ref> [ 18 ] </ref> . 2 The expected Fisher information matrix used by MML differs from the observed Fisher information matrix used by ICOMP in two ways. <p> Firstly the expected Fisher information matrix is the information integrated over all possible data values <ref> [ 18, Section 4.4.3 ] </ref> , while the observed Fisher information matrix is evaluated for the data sample at hand.
Reference: [ 19 ] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: Snob uses Wallace's Minimum Message Length (MML) method [ 25; 27 ] for mixture modelling of Gaussian, von Mises and other distributions. MML is an information theoretic criterion for parameter estimation and model selection, which is superficially similar to Rissanen's MDL criterion <ref> [ 19; 21 ] </ref> . However, MML is a Bayesian criterion [ 18 ] , and hence uses an explicit prior distribution over parameter values. We note that the AU-TOCLASS criterion is similar to the Snob criterion. <p> A range of criteria for selecting k have been proposed, including: * Minimum message length (MML) [ 25 ] , * Partition coefficient (PC) [ 2 ] , * Akaike's information criterion (AIC) [ 6 ] , * ICOMP [ 4; 5 ] and * Minimum description length (MDL) <ref> [ 19 ] </ref> (identi cal to Schwarz's criterion [ 22 ] ). All these methods use a two step process to determine the number of components. <p> We select the k with the minimum value of AIC (k). 3.3 MINIMUM DESCRIPTION LENGTH CRITERION (MDL) In 1978, Rissanen <ref> [ 19 ] </ref> gave his MDL criterion for model selection, and in the same year Schwarz [ 22 ] independently developed the same criterion: M DL (k) = L k + 2 where L k is the logarithm of the likelihood at the maximum likelihood solution and n p is the
Reference: [ 20 ] <author> J. Rissanen. </author> <title> Stochastic complexity. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 223-239, </pages> <year> 1987. </year>
Reference-contexts: The basic philosophy of the minimum encoding methods was summarized by Wallace and Freeman at a meeting of the Royal Statistical Society <ref> [ 27; 20 ] </ref> : "We may first estimate the parameters and then encode the data under the assumption that these are the true values. The encoded string must now, however, contain a specification of the estimated values.
Reference: [ 21 ] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: Snob uses Wallace's Minimum Message Length (MML) method [ 25; 27 ] for mixture modelling of Gaussian, von Mises and other distributions. MML is an information theoretic criterion for parameter estimation and model selection, which is superficially similar to Rissanen's MDL criterion <ref> [ 19; 21 ] </ref> . However, MML is a Bayesian criterion [ 18 ] , and hence uses an explicit prior distribution over parameter values. We note that the AU-TOCLASS criterion is similar to the Snob criterion.
Reference: [ 22 ] <author> G. Schwarz. </author> <title> Estimating dimension of a model. </title> <journal> Ann. Stat., </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: have been proposed, including: * Minimum message length (MML) [ 25 ] , * Partition coefficient (PC) [ 2 ] , * Akaike's information criterion (AIC) [ 6 ] , * ICOMP [ 4; 5 ] and * Minimum description length (MDL) [ 19 ] (identi cal to Schwarz's criterion <ref> [ 22 ] </ref> ). All these methods use a two step process to determine the number of components. The first step uses the EM algorithm to calculate the maximum likelihood estimates of the p j , j and j for a range of k values. <p> We select the k with the minimum value of AIC (k). 3.3 MINIMUM DESCRIPTION LENGTH CRITERION (MDL) In 1978, Rissanen [ 19 ] gave his MDL criterion for model selection, and in the same year Schwarz <ref> [ 22 ] </ref> independently developed the same criterion: M DL (k) = L k + 2 where L k is the logarithm of the likelihood at the maximum likelihood solution and n p is the number of parameters estimated.
Reference: [ 23 ] <author> H. Spath. </author> <title> Cluster Analysis Algorithms. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester, West Sussex, </address> <year> 1980. </year>
Reference-contexts: Spath <ref> [ 23, Page 7 ] </ref> defines unsupervised learning (or cluster analysis) in the following way: "The objective of cluster analysis is to separate a set of objects into constituent groups (classes, clumps, clusters) so that the members of one group differ from one another as little as possible, according to a
Reference: [ 24 ] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1985. </year>
Reference-contexts: An important part of the unsupervised learning problem is determining the number of constituent groups (which we shall call components for the remainder of this paper) which best describes some data. Snob and AUTOCLASS are in the category of mixture modellers <ref> [ 12; 24; 15 ] </ref> . They assume that the data under consideration was generated from a distribution which is the sum of simpler distributions.
Reference: [ 25 ] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: there have been systems such as (a) CLUSTER - Michalski and Stepp [ 16 ] , (b) COBWEB - Fisher [ 13 ] , (c) AQ17 - Wnek and Michalski [ 29 ] , (d) AUTOCLASS - Cheeseman et al. [ 7 ] and (e) Snob - Wallace et al. <ref> [ 25; 26 ] </ref> which uses MML. <p> The standard maximum likelihood approach has difficulty if we allow each component to have distinct standard deviations, since in this case the likelihood is unbounded [ 11 ] . In practice researchers take the largest finite maximum of the likelihood function. Snob uses Wallace's Minimum Message Length (MML) method <ref> [ 25; 27 ] </ref> for mixture modelling of Gaussian, von Mises and other distributions. MML is an information theoretic criterion for parameter estimation and model selection, which is superficially similar to Rissanen's MDL criterion [ 19; 21 ] . <p> We note that the AU-TOCLASS criterion is similar to the Snob criterion. This paper compares alternative techniques for estimating the number of components k, with Wallace's MML method which has been previously implemented in Snob <ref> [ 25; 26 ] </ref> . 2 ESTIMATING THE p j , j AND j Typically, the p j , j and j are estimated using the EM [ 11; 30 ] algorithm. <p> Hypothesis testing using the 2 test proposed by Wolfe [ 30 ] has been shown to be invalid [ 3 ] . A range of criteria for selecting k have been proposed, including: * Minimum message length (MML) <ref> [ 25 ] </ref> , * Partition coefficient (PC) [ 2 ] , * Akaike's information criterion (AIC) [ 6 ] , * ICOMP [ 4; 5 ] and * Minimum description length (MDL) [ 19 ] (identi cal to Schwarz's criterion [ 22 ] ). <p> Secondly, ICOMP utilises the information per observation, and hence I () is not a function of n. 4.1 A PRIOR DISTRIBUTION FOR PARAMETER VALUES We use the prior distributions for p j , j and j which were used by Wallace and Boulton <ref> [ 25 ] </ref> .
Reference: [ 26 ] <author> C.S. Wallace and D.L. Dowe. </author> <title> Intrinsic classification by MML the Snob program. </title> <editor> In C. Zhang, J. Deben-ham, and D Lukose, editors, </editor> <booktitle> Proceedings of the 7th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 37-44. </pages> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1994. </year>
Reference-contexts: there have been systems such as (a) CLUSTER - Michalski and Stepp [ 16 ] , (b) COBWEB - Fisher [ 13 ] , (c) AQ17 - Wnek and Michalski [ 29 ] , (d) AUTOCLASS - Cheeseman et al. [ 7 ] and (e) Snob - Wallace et al. <ref> [ 25; 26 ] </ref> which uses MML. <p> We note that the AU-TOCLASS criterion is similar to the Snob criterion. This paper compares alternative techniques for estimating the number of components k, with Wallace's MML method which has been previously implemented in Snob <ref> [ 25; 26 ] </ref> . 2 ESTIMATING THE p j , j AND j Typically, the p j , j and j are estimated using the EM [ 11; 30 ] algorithm.
Reference: [ 27 ] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 240-252, </pages> <year> 1987. </year>
Reference-contexts: The standard maximum likelihood approach has difficulty if we allow each component to have distinct standard deviations, since in this case the likelihood is unbounded [ 11 ] . In practice researchers take the largest finite maximum of the likelihood function. Snob uses Wallace's Minimum Message Length (MML) method <ref> [ 25; 27 ] </ref> for mixture modelling of Gaussian, von Mises and other distributions. MML is an information theoretic criterion for parameter estimation and model selection, which is superficially similar to Rissanen's MDL criterion [ 19; 21 ] . <p> ICOMP is a measure dependent on the coordinate system of the parameters [ 10 ] . 4 MINIMUM MESSAGE LENGTH CRITERION (MML) Wallace and Freeman <ref> [ 27 ] </ref> argue that statistical estimation can be performed as a coding process. <p> The basic philosophy of the minimum encoding methods was summarized by Wallace and Freeman at a meeting of the Royal Statistical Society <ref> [ 27; 20 ] </ref> : "We may first estimate the parameters and then encode the data under the assumption that these are the true values. The encoded string must now, however, contain a specification of the estimated values. <p> The second part of the message states x i ; i = 1 : : : n, under the assumption that the parameter estimates in Part 1 of the message were the true values. Wallace and Freeman <ref> [ 27 ] </ref> give us a formula which estimates the message length 1 : M essLen log h () + 1 log det (F ()) n p log n p + 2 where h () is a prior distribution over parameter values, F () is the expected Fisher Information matrix 2
Reference: [ 28 ] <author> M.P. Windham and A. Cutler. </author> <title> Information ratios for validating mixture analyses. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 87 </volume> <pages> 1188-1192, </pages> <year> 1992. </year>
Reference-contexts: We use the following AIC criterion offered by Bozdogan [ 6 ] and used by Windham and Cutler <ref> [ 28 ] </ref> in their comparison of mixture modelling methods. <p> We tested the effectiveness of the methods for selecting the number of components in a mixture model by repeatedly generating data from artificial mixture models. We have followed an experimental procedure similar to the procedure used by Windham and Cutler <ref> [ 28 ] </ref> . In the following sections, we have generated bivariate data. Figure 1 shows an example of 50 data points generated from a bivariate Gaussian distribution with mean ( x = 0:0; y = 0:0) and standard deviations x = 1:0; y = 1:0. <p> Although the PC appears to be the superior method for this experiment for high , recall that the PC cannot select k = 1, and so k = 2 is its default selection. 6.3 THREE COMPONENT DISTRIBUTION Following Windham and Cutler <ref> [ 28 ] </ref> , we generated data from a bivariate mixture with three components.
Reference: [ 29 ] <author> J. Wnek and R.S. Michalski. </author> <title> Hypothesis-driven constructive induction in AQ17-HCI: A method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 139-168, </pages> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION We discuss the unsupervised learning problem. There are many approaches to unsupervised learning. Within AI there have been systems such as (a) CLUSTER - Michalski and Stepp [ 16 ] , (b) COBWEB - Fisher [ 13 ] , (c) AQ17 - Wnek and Michalski <ref> [ 29 ] </ref> , (d) AUTOCLASS - Cheeseman et al. [ 7 ] and (e) Snob - Wallace et al. [ 25; 26 ] which uses MML.
Reference: [ 30 ] <author> J.H. Wolfe. </author> <title> Pattern clustering by multivariate mixture analysis. </title> <journal> Multivariate Behavioural Research, </journal> <volume> 5 </volume> <pages> 329-350, </pages> <year> 1970. </year>
Reference-contexts: A variety of approaches has been used to estimate these parameters. Wolfe <ref> [ 30 ] </ref> and Day [ 11 ] give maximum likelihood estimates for j ; j and p j given k. The standard maximum likelihood approach has difficulty if we allow each component to have distinct standard deviations, since in this case the likelihood is unbounded [ 11 ] . <p> This paper compares alternative techniques for estimating the number of components k, with Wallace's MML method which has been previously implemented in Snob [ 25; 26 ] . 2 ESTIMATING THE p j , j AND j Typically, the p j , j and j are estimated using the EM <ref> [ 11; 30 ] </ref> algorithm. <p> However, there is evidence that visual inspection of such graphs is not an adequate method. For example, a unimodal distribution may have more than one component (examples of this type are given in Section 6.4). Hypothesis testing using the 2 test proposed by Wolfe <ref> [ 30 ] </ref> has been shown to be invalid [ 3 ] .
References-found: 30

