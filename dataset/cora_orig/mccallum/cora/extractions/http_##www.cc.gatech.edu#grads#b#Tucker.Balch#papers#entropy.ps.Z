URL: http://www.cc.gatech.edu/grads/b/Tucker.Balch/papers/entropy.ps.Z
Refering-URL: http://www.cs.gatech.edu/aimosaic/robot-lab/publications/multi-agent.html
Root-URL: 
Email: email tucker@cc.gatech.edu  
Title: Social Entropy: a New Metric for Learning Multi-robot Teams  
Author: Tucker Balch 
Address: Atlanta, Georgia 30332  
Affiliation: Mobile Robotics Laboratory College of Computing Georgia Institute of Technology  
Abstract: As robotics research expands into multiagent tasks and learning, investigators need new tools for evaluating the artificial robot societies they study. Is it enough, for example, just to say a team is "heterogeneous?" Perhaps heterogeneity is more properly viewed on a sliding scale. To address these issues this paper presents new metrics for learning robot teams. The metrics evaluate diversity in societies of mechanically similar but behaviorally heterogeneous agents. Behavior is an especially important dimension of diversity in learning teams since, as they learn, agents choose between hetero- or homogeneity based solely on their behavior. This paper introduces metrics of behavioral difference and behavioral diversity. Behavioral difference refers to disparity between two specific agents, while diversity is a measure of an entire society. Social Entropy, inspired by Shannon's Information Entropy [5], is proposed as a metric of behavioral diversity. It captures important components of diversity including the number and size of castes in a society. The new metrics are illustrated in the evaluation of an example learning robot soccer team. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.C. Arkin. </author> <title> Motor schema based mobile robot navigation. </title> <journal> International Journal of Robotics Research, </journal> <volume> 8(4) </volume> <pages> 92-112, </pages> <year> 1989. </year>
Reference-contexts: Ball position and defended goal sensors are used in the experiments examined here. Space precludes a more detailed description of the system. The skills provided to the soccer agents are designed as motor schema-based behavioral assemblages. Motor schemas are the reactive component of Arkin's Autonomous Robot Architecture (AuRA) <ref> [1] </ref>. AuRA's design integrates deliberative planning at a top level with behavior-based motor control at the bottom. The lower levels, concerned with executing the reactive behaviors are incorporated in this research. Motor schemas may be grouped to form more complex, emergent behaviors.
Reference: [2] <author> T. Balch. Clay: </author> <title> Integrating motor schemas and reinforcement learning. </title> <institution> Coll. of comp. </institution> <type> tech. report, </type> <institution> Ga. Inst. of Tech., </institution> <year> 1997. </year>
Reference-contexts: The learning teams are developed using the same behavioral assemblages and perceptual features as the control team. Clay (the system used for configuring the robots) includes both fixed (non-learning) and learning coordination operators <ref> [2] </ref>. The control team's configuration uses a fixed selector for coordination. Learning is introduced by replacing the fixed mechanism with a learning selector. A Q-learning [6] module is embedded in the learning selector.
Reference: [3] <author> T. Balch. </author> <title> Learning roles: Behavioral diversity in robot teams. </title> <institution> Coll. of comp. </institution> <type> tech. report, </type> <institution> Georgia Inst. of Technology, </institution> <year> 1997. </year>
Reference-contexts: The abbreviations for the assemblages are introduced in the text. this article focuses on evaluation metrics for robot teams, the learning system is presented in overview only. For more detail, the reader is referred to <ref> [3] </ref>. The control team includes three agents that move to the ball when behind it and another that remains in the backfield. For convenience, we refer to them as "forward" and "goalie" policies.
Reference: [4] <author> H. Kitano, M. Asada, Y. Kuniyoshi, I. Noda, and E. Osawa. </author> <title> Robocup: The robot world cup initiative. </title> <booktitle> In Proc. Autonomous Agents 97. ACM, </booktitle> <year> 1997. </year> <institution> Marina Del Rey, California. </institution>
Reference-contexts: A heterogeneous team (right) has settled on diverse policies which spread them apart into the forward middle and back of the field. 2 Robot Soccer Robot soccer is an increasingly popular focus of robotics research <ref> [4] </ref>. It it is an attractive domain for multiagent investigations because a robot team's success against a strong opponent often requires some form of cooperation. Additionally, many people are familiar with the human version of soccer and can easily identify with and understand the problem.
Reference: [5] <author> C. E. Shannon. </author> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press, </publisher> <year> 1949. </year>
Reference-contexts: Both teams have the same number of robots (100) and the same number of robot types (2), but intuitively it seems R b is "more diverse" than R a . How can the difference be quantified? This paper suggests Social Entropy, inspired by Shannon's Information Entropy <ref> [5] </ref>, as an appropriate measure of diversity in robot teams. Investigation of diversity at the societal level forces several related issues to the surface. First, since diversity is based on differences between individuals in a group, a measure of robot difference is necessary. <p> H (X), referred to as Information Entropy meets all these criteria <ref> [5] </ref>. The Information Entropy of a symbol system X is used in coding theory as a lower-bound on the average number of bits required per symbol to send multi-symbol messages.
Reference: [6] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Technical note: Q learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Clay (the system used for configuring the robots) includes both fixed (non-learning) and learning coordination operators [2]. The control team's configuration uses a fixed selector for coordination. Learning is introduced by replacing the fixed mechanism with a learning selector. A Q-learning <ref> [6] </ref> module is embedded in the learning selector. At each step, the learning module is provided the current reward and perceptual state, it returns an integer indicating which assemblage the selector should activate. The Q-learner automatically tracks previous perceptions and rewards to refine its policy.
References-found: 6

