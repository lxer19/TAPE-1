URL: http://robotics.stanford.edu/~ronnyk/lazyDT.ps
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: jhf@playfair.stanford.edu  ronnyk@sgi.com  yygirl@cs.stanford.edu  
Title: Lazy Decision Trees  
Author: Jerome H. Friedman Ron Kohavi Yeogirl Yun 
Address: Stanford, CA 94305  2011 N. Shoreline Blvd Mountain View, CA 94043-1389  Stanford, CA 94305  
Affiliation: Statistics Department and Stanford Linear Accelerator Center Stanford University  Data Mining and Visualization Silicon Graphics, Inc.  Electrical Engineering Department Stanford University  
Note: To appear in AAAI-96  
Abstract: Lazy learning algorithms, exemplified by nearest-neighbor algorithms, do not induce a concise hypothesis from a given training set; the inductive process is delayed until a test instance is given. Algorithms for constructing decision trees, such as C4.5, ID3, and CART create a single "best" decision tree during the training phase, and this tree is then used to classify test instances. The tests at the nodes of the constructed tree are good on average, but there may be better tests for classifying a specific instance. We propose a lazy decision tree algorithm|LazyDT|that conceptually constructs the "best" decision tree for each test instance. In practice, only a path needs to be constructed, and a caching scheme makes the algorithm fast. The algorithm is robust with respect to missing values without resorting to the complicated methods usually seen in induction of decision trees. Experiments on real and artificial problems are presented. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <note> to appear. AI review journal: Special issue on lazy learning. </note>
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Cover, T. M., and Thomas, J. A. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, Inc. </publisher>
Reference-contexts: X, sometimes called the information gain of Y given X, measures the relative entropy between the joint distribution and the product distribution: I (Y ; X) = y2Y x2X p (x; y) (5) The mutual information is symmetric, i.e., I (Y ; X) = I (X; Y ), and non-negative <ref> (Cover & Thomas 1991) </ref>. As can be seen from Equation 6, the mutual information measures the reduction in uncertainty in Y after observing X.
Reference: <author> Dasarathy, B. V. </author> <year> 1990. </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference: <author> Dougherty, J.; Kohavi, R.; and Sahami, M. </author> <year> 1995. </year> <title> Supervised and unsupervised discretization of continuous features. </title> <editor> In Prieditis, A., and Russell, S., eds., </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> 194-202. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The LazyDT Algorithm We now describe the exact details of the LazyDT algorithm including the way it handles continuous features and the caching scheme used to speed the classification. Since the LazyDT algorithm described is only capable of processing nominal features, the training set is first discretized <ref> (Dougherty, Kohavi, & Sahami 1995) </ref>. We chose to discretize the instances using recursive minimization of entropy as proposed by Fayyad & Irani (1993) and as implemented in MLC ++ (Kohavi et al. 1994), which is publicly available and thus allows replication of this discretization step.
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1993. </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1022-1027. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Friedman, J. H. </author> <year> 1994. </year> <title> Flexible metric nearest neighbor classification. </title> <type> Technical Report 113, </type> <institution> Stanford University Statistics Department. </institution>
Reference: <author> Geman, S.; Bienenstock, E.; and Doursat, R. </author> <year> 1992. </year> <title> Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Computation 4 </booktitle> <pages> 1-48. </pages>
Reference: <author> Hastie, T., and Tibshirani, R. </author> <year> 1995. </year> <title> Discriminant adaptive nearest neighbor classification. </title> <type> Technical report, </type> <institution> Stanford University Statistics Department. </institution>
Reference: <author> Holte, R. C.; Acker, L. E.; and Porter, B. W. </author> <year> 1989. </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 813-818. </pages>
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-90. </pages>
Reference-contexts: In practice, of course, the caching scheme might be avoided altogether; a doctor, for example, can wait a few seconds for classification. Our experiments required hundreds of test instances to be classified for twenty-eight datasets, so caching was a necessity. The dynamic complexity of an algorithm <ref> (Holte 1993) </ref> is the number of features used on average. An interesting experiment would be to compare the dynamic complexity of C4.5 with that of LazyDT. Summary We introduced a novel lazy algorithm, LazyDT, that can be used in supervised classification.
Reference: <author> John, G.; Kohavi, R.; and Pfleger, K. </author> <year> 1994. </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> 121-129. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hayes-roth and glass2 also have large differences probably because they have many strongly relevant features and few weakly relevant features <ref> (John, Kohavi, & Pfleger 1994) </ref>. If we ignore the artificial problems, the average accuracy for LazyDT on the datasets without missing values is 82.15% and the accuracy on the datasets with 20% missing values is 78.40%.
Reference: <author> Kohavi, R., and Wolpert, D. H. </author> <year> 1996. </year> <title> Bias plus variance decomposition for zero-one loss functions. </title> <editor> In Saitta, L., ed., </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kauf-mann Publishers, </publisher> <address> Inc. </address> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference: <author> Kohavi, R.; John, G.; Long, R.; Manley, D.; and Pfleger, K. </author> <year> 1994. </year> <title> MLC++: A machine learning library in C++. </title> <booktitle> In Tools with Artificial Intelligence, </booktitle> <pages> 740-743. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Since the LazyDT algorithm described is only capable of processing nominal features, the training set is first discretized (Dougherty, Kohavi, & Sahami 1995). We chose to discretize the instances using recursive minimization of entropy as proposed by Fayyad & Irani (1993) and as implemented in MLC ++ <ref> (Kohavi et al. 1994) </ref>, which is publicly available and thus allows replication of this discretization step. The exact details are unimportant for this paper. We considered two univariate test criteria. The first is similar to that of C4.5 (i.e., a multi-way split). <p> Hayes-roth and glass2 also have large differences probably because they have many strongly relevant features and few weakly relevant features <ref> (John, Kohavi, & Pfleger 1994) </ref>. If we ignore the artificial problems, the average accuracy for LazyDT on the datasets without missing values is 82.15% and the accuracy on the datasets with 20% missing values is 78.40%.
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1996. </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn. </note>
Reference: <author> Pagallo, G., and Haussler, D. </author> <year> 1990. </year> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 71-99. </pages>
Reference-contexts: Top-down decision-tree induction algorithms implement a greedy approach that attempts to find a small tree. All the common selection measures are based on one level of lookahead. Two related problems inherent to the representation structure are replication and fragmentation <ref> (Pagallo & Haussler 1990) </ref>.
Reference: <author> Quinlan, J. R. </author> <year> 1991. </year> <title> Improved estimates for the accuracy of small disjuncts. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 93-98. </pages>
Reference-contexts: We believe that LazyDT suffers less from the problem of small disjuncts because the training set is being "fitted" to the specific instance and hence is likely to be less fragmented. The normalization of class probabilities in LazyDT is in line with Quinlan's suggestions <ref> (Quinlan 1991) </ref> of taking the context (the parent node in our case) into account. Quinlan (1994) characterizes classification problems as sequential or parallel. In parallel tasks, all input features are relevant to the classification; in sequential type tasks, the relevance of features depends on the values of other features.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The Algorithms and Datasets We compare LazyDT to three algorithms: simple ID3, C4.5, and C4.5-NP. Simple ID3 is a basic basic top-down induction of decision trees algorithm. It selects the features based on information gain and considers unknowns to be a separate value. C4.5 <ref> (Quinlan 1993) </ref> is a state-of-the-art algorithm that penalizes multi-way splits using the gain-ratio, prunes the tree, and splits every instance into multiple branches when hitting unknown values. We used the default parameter settings. C4.5-NP is C4.5 without pruning and it is compared in order to estimate the effect of pruning.
Reference: <author> Quinlan, J. R. </author> <year> 1994. </year> <title> Comparing connectionist and symbolic learning methods. </title> <editor> In Hanson, S. J.; Drastal, G. A.; and Rivest, R. L., eds., </editor> <title> Computational Learning Theory and Natural Learning Systems, volume I: Constraints and Prospects. </title> <publisher> MIT Press. </publisher> <address> chapter 15, </address> <month> 445|456. </month>
Reference: <author> Schaffer, C. </author> <year> 1993. </year> <title> Selecting a classification method by cross-validation. </title> <booktitle> Machine Learning 13(1) </booktitle> <pages> 135-143. </pages>
Reference: <author> Schaffer, C. </author> <year> 1994. </year> <title> A conservation law for generalization performance. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> 259-265. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Smyth, P., and Goodman, R. </author> <year> 1992. </year> <title> An information theoretic approach to rule induction from databases. </title> <journal> IEEE Transactions on Knowledge and Data Engineering 4(4) </journal> <pages> 301-316. </pages>
Reference: <author> Wettschereck, D. </author> <year> 1994. </year> <title> A Study of Distance-Based Machine Learning Algorithms. </title> <type> Ph.D. Dissertation, </type> <institution> Oregon State University. </institution>
Reference: <author> Wolpert, D. H. </author> <year> 1994. </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework. </title> <editor> In Wolpert, D. H., ed., </editor> <title> The Mathemtatics of Generalization. </title> <publisher> Addison Wesley. </publisher>
References-found: 24

