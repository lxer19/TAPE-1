URL: http://ftp.eecs.umich.edu/people/peter/ijcai95a.ps
Refering-URL: http://ftp.eecs.umich.edu/people/peter/
Root-URL: http://www.eecs.umich.edu
Email: peter@umich.edu  
Title: Implications of an Automatic Lexical Acquisition System  
Author: Peter M. Hastings ()-(voice) ()-(fax) 
Date: February 24, 1995  
Address: 1101 Beal Avenue Ann Arbor, MI 48109  
Affiliation: Artificial Intelligence Lab The University of Michigan  
Abstract: This paper describes Camille, the Contextual Acquisition Mechanism for Incremental Lexeme LEarning. Camille operates as an addition to Lytinen's LINK parser in the context of an information extraction task, automatically infering the meanings of unknown words from context. Unlike many previous lexical acquisition systems, Camille was thoroughly tested within a complex, real-world domain. The implementation of this system produced many lessons which are applicable to language learning in general. This paper describes Camille's implications for evaluation, for knowledge representation, and for cog nitive modeling. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. </author> <year> 1981. </year> <title> What's necessary to hide?: Modeling action verbs. </title> <booktitle> In Proceedings of the 19 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 77-81. </pages>
Reference-contexts: Allen (1981) made his task somewhat simpler by taking a more pragmatic approach. He considered what it would take to represent the verb " hide", and answer reasonable questions about it. Allen's answer to his question involved a temporal logic that could address "notions of belief, intention, and causality." <ref> (Allen 1981, p. 81) </ref> This work takes the same approach as Allen's, in effect rephrasing the previous question as, "In order to meet the functional requirements of the overall task, what does the NLP system need to know?" Instead of requiring the system to answer all possible questions about the consequences
Reference: <author> Barwise, J., and Etchemendy, J. </author> <year> 1989. </year> <title> Model-theoretic semantics. </title> <editor> In Posner, M., ed., </editor> <booktitle> Foundations of Cognitive Science. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Brent, M. </author> <year> 1993a. </year> <title> From grammar to lexicon: Unsupervised learning of lexical syntax. </title> <note> Computational Linguistics. </note>
Reference: <author> Brent, M. </author> <year> 1993b. </year> <title> Surface cues and robust inference as a basis for the early acquisition of subcategorization frames. </title> <type> Lingua. </type>
Reference: <author> Cardie, C. </author> <year> 1993. </year> <title> A case-based approach to knowledge acquisition for domain-specific sentence analysis. </title> <booktitle> In Proceedings of the 11 th National Conference on Artificial Intelligence, </booktitle> <pages> 798-803. </pages>
Reference: <author> Chinchor, N. </author> <year> 1992. </year> <title> MUC-4 evaluation metrics. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Clearly this lack of distinction should be included in the scores. In order to demonstrate the tradeoff between inferring correct hypotheses and limiting the number of hypotheses generated, we established a system of scores which was adapted from the MUC conferences <ref> (Chinchor 1992) </ref>. These measures, Recall and Precision (originally taken from the field of Information Retrieval), are defined below. Two other calculations, Accuracy and Production, describe respectively the system's performance on the hypotheses it made (as reported above), and the percentage of possible verbs for which it produced hypotheses.
Reference: <author> Church, K., and Hanks, P. </author> <year> 1990. </year> <title> Word association norms, mutual information, and lexicography. </title> <note> Computational Linguistics 16. </note>
Reference: <author> Clark, E. </author> <year> 1989. </year> <title> On the logic of contrast. </title> <journal> Journal of Child Language 15 </journal> <pages> 317-335. </pages>
Reference: <author> Fernald, A., and Morikawa, H. </author> <year> 1993. </year> <title> Common themes and cultural variations in Japanese and American mothers' speech to infants. </title> <booktitle> Child Development 64 </booktitle> <pages> 637-656. </pages>
Reference: <author> Granger, R. </author> <year> 1977. </year> <title> Foul-up: A program that figures out meanings of words from context. </title> <booktitle> In Proceedings of Fifth International Joint Conference on Artificial Intelligence. </booktitle> <volume> 13 Hastings, </volume> <editor> P., and Lytinen, S. </editor> <year> 1994a. </year> <title> Objects, actions, nouns, and verbs. </title> <editor> In Ram, A., and Eiselt, K., eds., </editor> <booktitle> Proceedings of the 16 th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 397-402. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Hastings, P., and Lytinen, S. </author> <year> 1994b. </year> <title> The ups and downs of lexical acquisition. </title> <booktitle> In Proceedings of the 12 th National Conference on Artificial Intelligence, </booktitle> <pages> 754-759. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: 1 Introduction As reported in <ref> (Hastings & Lytinen 1994b) </ref> and (Hastings 1994), the Camille (Contextual Acquisition Mechanism for Incremental Lexical LEarning) sys 1 tem implements an algorithm for learning the meanings of words from exam-ple sentences without the help of a human trainer. <p> Camille was developed with the goal of leveraging all of the available knowledge for learning word meanings | without requiring additional special-purpose knowledge. The difficulty of this task led to several implications about lexical acquisition in general. A fundamental difference between nouns and verbs was described in <ref> (Hastings & Lytinen 1994b) </ref>. Camille relies on argument structure and examples to enable it to learn word meanings. But nouns and verbs play different roles as far as argument structure is concerned. <p> Thus Camille is forced to make guesses to limit hypothesis sets. As described above and in <ref> (Hastings & Lytinen 1994b) </ref>, different classes of words provide different types of constraints to the system, forcing Camille to infer the most specific consistent concept for a verb's meaning, and the most general for a noun's. In the example given above, Camille would initially infer that "frooble" means Arson.
Reference: <author> Hastings, P. </author> <year> 1994. </year> <title> Automatic Acquisition of Word Meaning from Context. </title> <type> Ph.D. Dissertation, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, MI. </address>
Reference-contexts: 1 Introduction As reported in (Hastings & Lytinen 1994b) and <ref> (Hastings 1994) </ref>, the Camille (Contextual Acquisition Mechanism for Incremental Lexical LEarning) sys 1 tem implements an algorithm for learning the meanings of words from exam-ple sentences without the help of a human trainer. The system was developed to operate on real-world texts as part of an information extraction task.
Reference: <author> Hindle, D. </author> <year> 1990. </year> <title> Noun classification from predicate-argument structures. </title> <booktitle> In Proceedings of the 28 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 268-275. </pages>
Reference: <author> Hobbs, J.; Appelt, D.; Tyson, M.; Bear, J.; and Israel, D. </author> <year> 1992. </year> <title> SRI International: Description of the FASTUS system used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Some rather successful systems turned assumptions about the knowledge required for NLP on their ear by dispensing entirely with lexica and grammars and concept representations. Instead they reduced the task to a simple pattern-matching problem (for example, SRI's FASTUS system <ref> (Hobbs et al. 6 1992) </ref>). When simple patterns were matched in the input, the appropriate part of the text was extracted. Unfortunately, because these systems do not have full grammar, it is fairly easy to come up with examples that the pattern-matchers cannot handle.
Reference: <author> Huttenlocher, J.; Haight, W.; Bryk, A.; Seltzer, M.; and Lyons, T. </author> <year> 1991. </year> <title> Early vocabulary growth: Relation to language input and gender. </title> <booktitle> Developmental Psychology 27(2) </booktitle> <pages> 236-248. </pages>
Reference: <author> Kaplan, S.; Weaver, M.; and French, R. </author> <year> 1990. </year> <title> Active symbols and internal models: Towards a cognitive connectionism. </title> <publisher> London: Springer Verlag. Springer Series on Artificial Intelligence and Society. </publisher>
Reference: <author> Katz, J. J., and Fodor, J. A. </author> <year> 1963. </year> <title> The structure of a semantic theory. </title> <booktitle> Language 39. </booktitle>
Reference: <author> Keil, F. </author> <year> 1991. </year> <title> Theories, concepts, and the acquisition of word meaning. </title>
Reference-contexts: The general framework consists of an IS-A inheritance hierarchy, a type of representation that is widely used in Artificial Intelligence. Various psychological studies support the existence 10 of hierarchical structures in the brain ((Kaplan, Weaver, & French 1990) and <ref> (Keil 1991) </ref>, for example). At the lowest level, this representation is clearly not "brain-like". It is highly unlikely that the brain uses such a rule-like arrangement for representing constraints. But the hierarchical structure has advantages that make it a powerful representation scheme for computers and humans.
Reference: <editor> In Byrnes, J. P., and Gelman, S. A., eds., </editor> <booktitle> Perspectives on language and thought: Interrelations in development. </booktitle> <address> Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Lenat, D. </author> <year> 1990. </year> <title> Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference: <author> Lytinen, S. </author> <year> 1991. </year> <title> A unification-based, integrated natural language processing system. Computers and Mathematics with Applications 23(6-9):403-418. </title> <type> 14 MacGregor, </type> <institution> R. </institution> <year> 1990. </year> <title> The evolving technology of classification-based knowl-edge representation systems. </title> <editor> In Sowa, J., ed., </editor> <booktitle> Principles of Semantic Nets: Explorations in the Representation of Knowledge. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Finally, the paper describes the implications of the system for cognitive modeling, and concludes with a general discussion. 2 Camille As previously mentioned, Camille was implemented as an addition to an information extraction system which was based on Lytinen's LINK parser <ref> (Lytinen 1991) </ref>. This particular task had a strong influence on some of the design decisions for Camille. It also provided the primary motivation.
Reference: <author> Markman, E. </author> <year> 1991. </year> <title> The whole object, taxonomic, and mutual exclusivity assumptions as initial constraints on word meanings. </title> <editor> In Byrnes, J. P., and Gelman, S. A., eds., </editor> <booktitle> Perspectives on language and thought: Interrelations in development. </booktitle> <address> Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: One psycholinguistic theory for a mechanism that children might use to reduce the complexity of the task is called Mutual Exclusivity <ref> (Markman 1991) </ref>. 2 This theory posits that when children are first learning the meanings of words, they assume that no two words have overlapping meanings. This simplifies their task by eliminating those possible meanings for a new word that they already know a word for.
Reference: <author> Resnik, P. </author> <year> 1992. </year> <title> A class-based approach to lexical discovery. </title> <booktitle> In Proceedings of the 30 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 327-329. </pages>
Reference: <author> Riloff, E. </author> <year> 1993. </year> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> In Proceedings of the 11 th National Conference on Artificial Intelligence, </booktitle> <pages> 811-816. </pages>
Reference: <author> Salveter, S. </author> <year> 1979. </year> <title> Inferring conceptual graphs. </title> <booktitle> Cognitive Science 3 </booktitle> <pages> 141-166. </pages>
Reference: <author> Siskind, J. </author> <year> 1991. </year> <title> Dispelling myths about language bootstrapping. In Powers, </title> <editor> D., and Reeker, L., eds., </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning of Natural Language and Ontology, Document D-91-09. </booktitle> <institution> University of Kaiserslautern, FRG: DFKI. </institution>
Reference: <author> Sundheim, B. </author> <year> 1992. </year> <title> Overview of the fourth message understanding evaluation and conference. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Given a sentence with an unknown word, Camille searches the existing concept hierarchy for a concept which adequately matches the current example. Figure 1 shows a subset of the hierarchy that was used in the terrorism domain of the MUC-3 and MUC-4 conferences <ref> (Sundheim 1992) </ref>. Each node shows its name and the semantic constraints on its slot fillers. <p> What is required to successfully perform the information extraction task? In the MUC competitions <ref> (Sundheim 1992) </ref>, systems with greatly varying depths of knowledge representation performed at similar levels of efficiency. Some rather successful systems turned assumptions about the knowledge required for NLP on their ear by dispensing entirely with lexica and grammars and concept representations.
Reference: <author> Yarowsky, D. </author> <year> 1992. </year> <title> Word-sense disambiguation using statistical models of roget's categories trained on large corpora. </title> <booktitle> In Proceedings, COLING-92. </booktitle>
Reference: <author> Zernik, U. </author> <year> 1987. </year> <title> How do machine language paradigms fare in language acquisition. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Zernik, U. </author> <year> 1991. </year> <title> Train1 vs. train2: Tagging word senses in corpus. </title> <editor> In Zernik, U., ed., </editor> <title> Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates, Inc. </publisher> <pages> 15 </pages>
References-found: 30

