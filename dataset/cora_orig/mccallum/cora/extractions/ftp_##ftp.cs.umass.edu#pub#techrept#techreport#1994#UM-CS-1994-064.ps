URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1994/UM-CS-1994-064.ps
Refering-URL: http://ccs-www.cs.umass.edu/lory/publications.html
Root-URL: 
Email: e-mail: lory@cs.umass.edu, krithi@cs.umass.edu  
Title: Recovery Protocols for Shared Memory Database Systems  
Author: Lory D. Molesky and Krithi Ramamritham 
Address: Amherst MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Significant performance advantages can be gained by implementing a database system on a cache-coherent shared memory multiprocessor. However, problems arise when failures occur. A single node (where a node refers to a processor/memory pair) crash may require a reboot of the entire shared memory system. Fortunately, shared memory multiprocessors that isolate individual node failures are currently being developed. Even with these, because of the side effects of the cache coherency protocol, a transaction executing strictly on a single node may become dependent on the validity of the memory of many nodes thereby inducing unnecessary transaction aborts. This happens when database objects, such as records, and database support structures, such as index structures and shared lock tables, are stored in shared memory. In this paper, we propose crash recovery protocols for shared memory database systems which avoid the unnecessary transaction aborts induced by the failure dependencies that occur due to the use of shared physical memory. Our recovery protocols guarantee that if one or more nodes crash, all effects of active transactions running on the crashed nodes will be undone, and no effects of transactions running on nodes which did not crash will be undone. In order to show the practicality of our protocols, we discuss how existing features of cache-coherent multiprocessors can be utilized to implement these recovery protocols. Specifically, we demonstrate that (1) for many types of database objects and support structures, volatile (in-memory) logging is sufficient to avoid unnecessary transaction aborts, and (2) a very low overhead implementation of this strategy can be achieved with existing multiprocessor features. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Archibald and J. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Typically, the hardware elements implementing the cache coherency scheme include a cache controller, a cache directory, and the cache itself. The cache contains the cached data, while the cache directory contains the addresses of all cached data. Our discussions in this paper assume a write-invalidate cache coherency protocol <ref> [1, 12] </ref> where, before a write to a cache line by one node occurs, all other cached copies of the line are first invalidated 2 . 2 Our results also apply to a write-broadcast cache coherency protocol, but a discussion of this has been omitted due to space limitations. 2 However,
Reference: [2] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: Note that several shared requests on r can be granted concurrently. * No-force/steal buffer management policy is used [7]. * Each node maintains a log, and in-place updating is used in conjunction with the write-ahead log protocol (WAL) <ref> [2] </ref>. All update operations to this log take place in the node's cache. This in-cache log is volatile, but can be made stable by writing it to one of the shared disks.
Reference: [3] <author> A. Bhide. </author> <title> An Analysis of Three Transaction Processing Architectures. </title> <booktitle> Proceedings of the 14th International Conference on Very Large Data Bases, </booktitle> <volume> 14 </volume> <pages> 339-350, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: In [26], an analytical and simulation study compares SN (shared nothing), SD, and SIM (shared intermediate memory 7 ). This comparison concludes that the data sharing architectures, especially SIM, are more resilient to transaction load surges. In <ref> [3, 4] </ref>, simulation studies compare SN, SD, and SM (called SE (shared everything) in this reference), and concludes that SM outperforms SN and SD by a fairly wide margin. Our work exploits the performance advantages of SM systems, yet guarantees good failure properties for transactions.
Reference: [4] <author> A. Bhide and M. Stonebraker. </author> <title> A Performance Comparison of Two Architectures for Fast Transaction Processing. </title> <booktitle> IEEE Proc. 4th Intl. Conference on Data Engineering, </booktitle> <pages> pages 536-545, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: In [26], an analytical and simulation study compares SN (shared nothing), SD, and SIM (shared intermediate memory 7 ). This comparison concludes that the data sharing architectures, especially SIM, are more resilient to transaction load surges. In <ref> [3, 4] </ref>, simulation studies compare SN, SD, and SM (called SE (shared everything) in this reference), and concludes that SM outperforms SN and SD by a fairly wide margin. Our work exploits the performance advantages of SM systems, yet guarantees good failure properties for transactions.
Reference: [5] <author> J. Chase, H. Levy, E. Lazowska, and M. Barker-Harvey. </author> <title> Lightweight Shared Objects in a 64-Bit Operating System. </title> <booktitle> Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 397-413, </pages> <year> 1992. </year>
Reference-contexts: In addition to providing support for SM database systems, our recovery protocols can also be applied to provide operating system support for handling independent node failures. For example, in multiprocessor systems where a single virtual address space is employed <ref> [5, 21] </ref>, virtual memory allocation is most efficiently implemented by maintaining operating system tables in stored shared memory. When the data structures describing the allocation of virtual memory migrate between systems, just as in our database scenarios, failure dependencies will form between nodes.
Reference: [6] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kauf-mann, </publisher> <year> 1993. </year>
Reference-contexts: Thus, either the volatile and stable LBM policy is sufficient to ensure condition 2. Consider how the LBM policies can ensure condition 1 in the case where node x crashes. Prior to acquiring (or releasing) a lock on node x, a logical log record <ref> [6] </ref> is written to the log on node x. Storing the transaction ID in the LCB is sufficient to guarantee condition 1 with the stable LBM policy.
Reference: [7] <author> T. Haerder and A. Reuter. </author> <title> Principles of Transaction-Oriented Database Recovery. </title> <journal> ACM Computing Surveys, </journal> <volume> 15(4) </volume> <pages> 287-317, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: An exclusive lock on a record r guarantees that no other transaction will read or modify r, while a shared lock on r ensures that no other transaction will modify r. Note that several shared requests on r can be granted concurrently. * No-force/steal buffer management policy is used <ref> [7] </ref>. * Each node maintains a log, and in-place updating is used in conjunction with the write-ahead log protocol (WAL) [2]. All update operations to this log take place in the node's cache. <p> By utilizing mechanisms and structures which are already part of the database management system, the incremental overheads associated with adopting our recovery protocols are minimized. For example, our LBM policies exploit the existence of undo and redo log records which are part of most database systems <ref> [7] </ref>. When a database record is updated, a redo log record (containing the value of the updated database record) is written to the volatile log. When a transaction first performs an update to a database record, an undo log record is written to the volatile log.
Reference: [8] <author> M. Herlihy and E. Moss. </author> <title> Transactional Memory: Architectural Support for Lock-free Data Structures. </title> <booktitle> Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: However, non-volatile memory is much more expensive than volatile memory, and is a significant departure from a database implementation on off-the-shelf shared memory multiprocessors wherein the cache where (parts of) data structures may reside is volatile. Transactional Memory <ref> [8] </ref> is another approach to supporting transactions on a cache-coherent shared-memory architecture. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory.
Reference: [9] <author> D. Johnson and W. Zwaenepoel. </author> <title> Sender-Based Message Logging. </title> <booktitle> Proceedings of the 17th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 14-19, </pages> <year> 1987. </year>
Reference-contexts: Our recovery protocols do not have these restrictions. 7 The SIM model is slightly different than the shared memory model. In SIM, a shared intermediate memory serves as a global shared buffer for all nodes. 19 Volatile logging has been used in the context of process checkpointing schemes <ref> [9, 24] </ref> to ensure a consistent state of a distributed computation without necessitating the rollback of any processes other than ones that failed.
Reference: [10] <author> N. Kronenberg, H. Levy, and W. Streker. Vaxclusters: </author> <title> A Closely-Coupled Distributed System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 130-146, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: This information is used by the restart recovery procedure to ensure that all locks which were acquired by transactions running on nodes which had crashed are released. Some issues related to ensuring the failure atomicity of database management structures are covered in <ref> [23, 22, 10] </ref>, where crash recovery issues for an SD lock manager implementation on a VAXcluster are discussed. When a node crash is detected, all locking activity in the database system is stopped. Then, any updates performed by failed transactions are undone.
Reference: [11] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stan-ford FLASH Multiprocessor. </title> <booktitle> Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year> <month> 22 </month>
Reference-contexts: Sigmod94 panel). This is unfortunate because database applications can easily exploit the performance advantages of shared memory architectures. While the support for independent node failures in an SM system requires certain low-level mechanisms to be in place, such as mechanisms to detect and isolate hardware faults <ref> [11] </ref>, these alone are not enough to guarantee that unnecessary failure dependencies do not form between nodes. <p> While the unit of I/O is a page, the unit of coherency is a cache line, and is typically smaller than a page. Motivated by the proposed enhancements to SM architectures whereby individual node failures are isolated <ref> [11] </ref>, we assume that nodes in the shared memory multiprocessor fail independently. When node x crashes while holding the only copy of a cache line, we assume that this cache line is unavailable for use by other nodes until a recovery procedure is executed. <p> Due to typical cache line sizes of current shared memory multiprocessors, it is likely (unless a lot of space is wasted) that multiple records will be stored in a cache line. For example, on the KSR-1 and KSR-2 multiprocessors [21], and on Stanford's FLASH <ref> [11] </ref> distributed shared memory machine, the cache line size is 128 bytes.
Reference: [12] <author> D. Lilja. </author> <title> Cache Coherence in Large-Scale Shared-Memory Multiprocessors: Issues and Comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Typically, the hardware elements implementing the cache coherency scheme include a cache controller, a cache directory, and the cache itself. The cache contains the cached data, while the cache directory contains the addresses of all cached data. Our discussions in this paper assume a write-invalidate cache coherency protocol <ref> [1, 12] </ref> where, before a write to a cache line by one node occurs, all other cached copies of the line are first invalidated 2 . 2 Our results also apply to a write-broadcast cache coherency protocol, but a discussion of this has been omitted due to space limitations. 2 However,
Reference: [13] <author> D. Lomet and B. Salzberg. </author> <title> Access Method Concurrency with Recovery. </title> <booktitle> Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, </booktitle> <volume> 21 </volume> <pages> 351-360, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: To avoid this, when one transaction performs a structural change, it is customary to commit these changes immediately (regardless of the future commit or abort of the transaction that caused the change), prior to being released to other transactions <ref> [16, 13, 14] </ref>. For these reasons, we also assume this convention of the independent commit of structural changes done to database management structures. Given this assumption, structural changes will not result in dependencies between active transactions and the memory of some other node.
Reference: [14] <author> C. Mohan and D. Haderle. </author> <title> Algorithms for Flexible Space Management in Transaction Systems Supporting Fine-Granularity Locking. </title> <booktitle> Proc. International Conference on Extending Data Base Technology, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: To avoid this, when one transaction performs a structural change, it is customary to commit these changes immediately (regardless of the future commit or abort of the transaction that caused the change), prior to being released to other transactions <ref> [16, 13, 14] </ref>. For these reasons, we also assume this convention of the independent commit of structural changes done to database management structures. Given this assumption, structural changes will not result in dependencies between active transactions and the memory of some other node. <p> To ensure that the space freed by a delete is not used until the transaction which performed the delete commits, it is customary to perform record deletes logically, by marking the record as deleted <ref> [14] </ref>. Once the transaction which performed the delete commits, the space freed by the deleted record can be used by other transactions.
Reference: [15] <author> C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz. </author> <title> ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 17 </volume> <pages> 94-162, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Consistent with existing commercial database implementations, we assume that strict write locks are obtained on the record to be inserted or deleted. We also assume that the associated logical log records are written for insert and delete operations, enabling the logical undo and redo of these operations <ref> [16, 15] </ref>. In conjunction with the WAL protocol, these assumptions ensure that each B+-tree update is associated with at most one transaction, simplifying the implementation of transaction aborts. <p> Although many of the implementation issues of interest to us have been addressed in the context of SD systems <ref> [17, 15, 23, 22, 19, 20] </ref>, significant differences between SD and SM systems require that, for the most efficient implementation, different mechanisms be developed for SM. These differences stem from the different approaches used to achieving coherency. <p> Page p can be written to the StableDB only after all systems which have updated p have forced their logs up to this LSN. Closely related to the page transfer schemes is the ordered update logging rule <ref> [17, 15] </ref>, which is important for supporting the repeating history paradigm (even for a multiprogrammed uniprocessor database system). This rule guarantees that the order of logging of updates to a 17 page is the same as the order with which those updates are performed on the page. <p> This rule guarantees that the order of logging of updates to a 17 page is the same as the order with which those updates are performed on the page. In <ref> [15] </ref>, the ordered update logging rule is satisfied as part of the update protocol. This guarantee is provided by acquiring and holding a semaphore on the page to be updated for the duration of the update and the log write.
Reference: [16] <author> C. Mohan and F. Levine. ARIES/IM: </author> <title> An Efficient and High Concurrency Index Management Method Using Write-Ahead Logging. </title> <booktitle> Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, </booktitle> <volume> 21 </volume> <pages> 371-380, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: To avoid this, when one transaction performs a structural change, it is customary to commit these changes immediately (regardless of the future commit or abort of the transaction that caused the change), prior to being released to other transactions <ref> [16, 13, 14] </ref>. For these reasons, we also assume this convention of the independent commit of structural changes done to database management structures. Given this assumption, structural changes will not result in dependencies between active transactions and the memory of some other node. <p> Consistent with existing commercial database implementations, we assume that strict write locks are obtained on the record to be inserted or deleted. We also assume that the associated logical log records are written for insert and delete operations, enabling the logical undo and redo of these operations <ref> [16, 15] </ref>. In conjunction with the WAL protocol, these assumptions ensure that each B+-tree update is associated with at most one transaction, simplifying the implementation of transaction aborts.
Reference: [17] <author> C. Mohan and I. Narang. </author> <title> Recovery and Coherency-Control Protocols for Fast Intersystem Page Transfer and Fine-Granularity Locking in a Shared-Disk Transaction Environment. </title> <booktitle> Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <volume> 17 </volume> <pages> 193-207, </pages> <year> 1991. </year>
Reference-contexts: One strategy for implementing a lock manager in a multi-node system is to designate some node as being responsible for managing each database object, and allow remote nodes to access locks by using message passing. This is the approach of many shared-disk (SD) systems <ref> [22, 17, 19] </ref>. The presence of shared memory in an SM database system allows a more efficient approach to be taken for the implementation of database locking. <p> Any LCB's, unaffected by the crash of node x, which contain lock acquisition entries for transactions running on node x will be repaired by the restart 3 An exception to this rule occurs in some SD systems <ref> [17] </ref> where since lock acquisition is expensive in SD systems, locks are sometimes retained on local nodes after the transaction commit has occurred. 4 The recovery issues for transactions which are waiting for locks are similar, but the discussion has been ommitted due to space limitations. 12 recovery procedure. <p> Although many of the implementation issues of interest to us have been addressed in the context of SD systems <ref> [17, 15, 23, 22, 19, 20] </ref>, significant differences between SD and SM systems require that, for the most efficient implementation, different mechanisms be developed for SM. These differences stem from the different approaches used to achieving coherency. <p> These differences stem from the different approaches used to achieving coherency. In SM, cache coherency is achieved transparently by the underlying hardware cache coherency protocol. In contrast, in an SD system, coherency is achieved entirely in software and is closely coupled with the lock and buffer managers <ref> [17, 19] </ref>. Two significant implications of these factors are (1) an SM database need not include the SD mechanisms used for ensuring coherency, and (2) an SM database can utilize the low latency access to shared memory to yield very efficient adaptations of other SD mechanisms. <p> In the rest of this section, we discuss how these implications affect the transaction processing components needed for SM systems in the context of our recovery mechanisms. We focus on the protocols and implementation mechanisms used in a recent SD system <ref> [17] </ref>. This system supports record level locking and uses in-place updating in conjunction with the WAL protocol. It uses the repeating history paradigm followed by undos to recover from failures. <p> We discuss how WAL and the techniques for ensuring the repeating history paradigm can be efficiently implemented in SM, in light of the mechanisms for achieving LBM. Since this system has also addressed some of the issues related to the migration of uncommitted data, we discuss this aspect of <ref> [17] </ref> first. In order to ensure inter-node coherency, [17] defines four inter-node page transfer schemes, two of which allow the migration of uncommitted data, called fast, and super-fast schemes. In the fast scheme, all updates to page p must be stable logged prior to p's migration. <p> Since this system has also addressed some of the issues related to the migration of uncommitted data, we discuss this aspect of <ref> [17] </ref> first. In order to ensure inter-node coherency, [17] defines four inter-node page transfer schemes, two of which allow the migration of uncommitted data, called fast, and super-fast schemes. In the fast scheme, all updates to page p must be stable logged prior to p's migration. <p> To implement the WAL rule for SD, each updating system remembers an LSN (log sequence number) greater than or equal to its last update to page p <ref> [17] </ref>. Page p can be written to the StableDB only after all systems which have updated p have forced their logs up to this LSN. <p> Page p can be written to the StableDB only after all systems which have updated p have forced their logs up to this LSN. Closely related to the page transfer schemes is the ordered update logging rule <ref> [17, 15] </ref>, which is important for supporting the repeating history paradigm (even for a multiprogrammed uniprocessor database system). This rule guarantees that the order of logging of updates to a 17 page is the same as the order with which those updates are performed on the page. <p> This guarantee is provided by acquiring and holding a semaphore on the page to be updated for the duration of the update and the log write. On the surface, there are many similarities between the SD protocols described in <ref> [17] </ref> and our recovery protocols. However, there are important differences. Many of the protocols of [17], such as the page transfer protocols, are designed for the purpose of enforcing inter-system page coherency. <p> On the surface, there are many similarities between the SD protocols described in <ref> [17] </ref> and our recovery protocols. However, there are important differences. Many of the protocols of [17], such as the page transfer protocols, are designed for the purpose of enforcing inter-system page coherency. In contrast, for cache-coherent SM, our LBM policies are designed specifically to isolate the crash of one node from affecting transactions which execute on other nodes. Thus, whereas the protocols of [17] are aimed <p> protocols of <ref> [17] </ref>, such as the page transfer protocols, are designed for the purpose of enforcing inter-system page coherency. In contrast, for cache-coherent SM, our LBM policies are designed specifically to isolate the crash of one node from affecting transactions which execute on other nodes. Thus, whereas the protocols of [17] are aimed at achieving page coherency, we are motivated by the need to eliminate the ill-effects of system-ensured cache coherency! Furthermore, as was discussed in section 5, efficiently enforcing the LBM policies on a cache-coherent SM multiprocessor requires a careful analysis of the coherency protocol, and a novel application of <p> Finally, the availability of shared memory in SM systems allows for more efficient implementations of many transaction processing mechanisms. To illustrate this, next we show how shared memory can be utilized in the adaptation of two mechanisms used in the SD system of <ref> [17] </ref>: the ordered update rule and enforcing the WAL protocol under the volatile LBM policy. In section 4, we discussed how line locks could be used to enforce the volatile LBM policy as part of the update protocol. <p> Our work exploits the performance advantages of SM systems, yet guarantees good failure properties for transactions. In the previous sections, we discussed how our work is related to work in shared disk systems <ref> [17, 23, 22, 19] </ref>. Architectural differences between SD and cache-coherent SM, such as the unit of inter-system data sharing, how coherency is achieved, and whether shared memory is available, have a significant influence on the design and implementation of SM crash recovery protocols.
Reference: [18] <author> L. D. Molesky and K. Ramamritham. </author> <title> Efficient Locking for Shared-Memory Database Systems. </title> <type> Technical Report 94-10, </type> <institution> University of Massachusetts Dept. of Computer Science, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: In this strategy, which we call SM locking, LCB's (lock control blocks) are stored in shared memory, and transactions acquire and release locks via operations on these LCB's. The performance gains of SM locking stem from the elimination of all inter-process communication <ref> [18] </ref>. Consider acquiring a record lock using SM locking. A lock request consists of a lock name and a lock mode. Using a hash function, the name is translated to an LCB address specific to one lock. <p> The update is performed, and the log record is written prior to releasing the line with releaseline (l). Our experience in the implementation and empirical performance evaluation of database mechanisms on the KSR-1 confirms the expected performance gains provided by the line lock primitive. In <ref> [18] </ref>, we implemented a prototype lock manager, using the line lock to ensure mutually exclusive updates to the shared memory implementation of the lock space.
Reference: [19] <author> E. Rahm. </author> <title> Concurrency and Coherency Control in Database Sharing Systems. </title> <type> Technical Report, </type> <institution> University of Kaiserslautern, Germany, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: One strategy for implementing a lock manager in a multi-node system is to designate some node as being responsible for managing each database object, and allow remote nodes to access locks by using message passing. This is the approach of many shared-disk (SD) systems <ref> [22, 17, 19] </ref>. The presence of shared memory in an SM database system allows a more efficient approach to be taken for the implementation of database locking. <p> Although many of the implementation issues of interest to us have been addressed in the context of SD systems <ref> [17, 15, 23, 22, 19, 20] </ref>, significant differences between SD and SM systems require that, for the most efficient implementation, different mechanisms be developed for SM. These differences stem from the different approaches used to achieving coherency. <p> These differences stem from the different approaches used to achieving coherency. In SM, cache coherency is achieved transparently by the underlying hardware cache coherency protocol. In contrast, in an SD system, coherency is achieved entirely in software and is closely coupled with the lock and buffer managers <ref> [17, 19] </ref>. Two significant implications of these factors are (1) an SM database need not include the SD mechanisms used for ensuring coherency, and (2) an SM database can utilize the low latency access to shared memory to yield very efficient adaptations of other SD mechanisms. <p> Our work exploits the performance advantages of SM systems, yet guarantees good failure properties for transactions. In the previous sections, we discussed how our work is related to work in shared disk systems <ref> [17, 23, 22, 19] </ref>. Architectural differences between SD and cache-coherent SM, such as the unit of inter-system data sharing, how coherency is achieved, and whether shared memory is available, have a significant influence on the design and implementation of SM crash recovery protocols.
Reference: [20] <author> E. Rahm. </author> <title> Use of Global Extended Memory for Distributed Transaction Processing. </title> <booktitle> Proceedings of the 4th Int. Workshop on High Performance Transaction Systems, Asilomar, </booktitle> <address> CA., </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Although many of the implementation issues of interest to us have been addressed in the context of SD systems <ref> [17, 15, 23, 22, 19, 20] </ref>, significant differences between SD and SM systems require that, for the most efficient implementation, different mechanisms be developed for SM. These differences stem from the different approaches used to achieving coherency. <p> Furthermore, our goal was to achieve IFA with minimal extra overheads while capitalizing on features available on or proposed for SM multiprocessor systems. In <ref> [20] </ref>, augmenting an SD system with a non-volatile global extended memory (GEM) is considered. System performance can be improved by adding GEM to a system that otherwise can only communicate by message passing. Failure atomicity for data structures can be ensured by propagating their updates to the GEM.
Reference: [21] <author> Kendall Square Research. </author> <title> KSR1 Principles of Operation. KSR Research, </title> <address> Waltham, Mass., </address> <year> 1992. </year>
Reference-contexts: Consider the storage requirements of records. Due to typical cache line sizes of current shared memory multiprocessors, it is likely (unless a lot of space is wasted) that multiple records will be stored in a cache line. For example, on the KSR-1 and KSR-2 multiprocessors <ref> [21] </ref>, and on Stanford's FLASH [11] distributed shared memory machine, the cache line size is 128 bytes. <p> We discuss the enforcement of the volatile LBM policy in section 5.1, and the stable LBM policy in section 5.2. 5.1 Enforcing the Volatile LBM Policy A (cache) line lock <ref> [21] </ref>, commercially available on the KSR-1 multiprocessor, is an example of a mechanism useful for efficiently implementing critical sections in a cache-coherent multiprocessor. Thus, they can be utilized to achieve a very low overhead implementation of the volatile LBM policy. <p> In addition to providing support for SM database systems, our recovery protocols can also be applied to provide operating system support for handling independent node failures. For example, in multiprocessor systems where a single virtual address space is employed <ref> [5, 21] </ref>, virtual memory allocation is most efficiently implemented by maintaining operating system tables in stored shared memory. When the data structures describing the allocation of virtual memory migrate between systems, just as in our database scenarios, failure dependencies will form between nodes.
Reference: [22] <author> W. Snaman and D. Thiel. </author> <title> The VAX/VMS Distributed Lock Manager. </title> <journal> Digital Technical Journal, </journal> (5):29-44, September 1987. <volume> 23 </volume>
Reference-contexts: One strategy for implementing a lock manager in a multi-node system is to designate some node as being responsible for managing each database object, and allow remote nodes to access locks by using message passing. This is the approach of many shared-disk (SD) systems <ref> [22, 17, 19] </ref>. The presence of shared memory in an SM database system allows a more efficient approach to be taken for the implementation of database locking. <p> This information is used by the restart recovery procedure to ensure that all locks which were acquired by transactions running on nodes which had crashed are released. Some issues related to ensuring the failure atomicity of database management structures are covered in <ref> [23, 22, 10] </ref>, where crash recovery issues for an SD lock manager implementation on a VAXcluster are discussed. When a node crash is detected, all locking activity in the database system is stopped. Then, any updates performed by failed transactions are undone. <p> Although many of the implementation issues of interest to us have been addressed in the context of SD systems <ref> [17, 15, 23, 22, 19, 20] </ref>, significant differences between SD and SM systems require that, for the most efficient implementation, different mechanisms be developed for SM. These differences stem from the different approaches used to achieving coherency. <p> Our work exploits the performance advantages of SM systems, yet guarantees good failure properties for transactions. In the previous sections, we discussed how our work is related to work in shared disk systems <ref> [17, 23, 22, 19] </ref>. Architectural differences between SD and cache-coherent SM, such as the unit of inter-system data sharing, how coherency is achieved, and whether shared memory is available, have a significant influence on the design and implementation of SM crash recovery protocols.
Reference: [23] <author> T. Rengarajan P. Spiro and W. Wright. </author> <title> High Availability Mechanisms of VAX DBMS Software. </title> <journal> Digital Technical Journal, </journal> (8):88-98, February 1989. 
Reference-contexts: This information is used by the restart recovery procedure to ensure that all locks which were acquired by transactions running on nodes which had crashed are released. Some issues related to ensuring the failure atomicity of database management structures are covered in <ref> [23, 22, 10] </ref>, where crash recovery issues for an SD lock manager implementation on a VAXcluster are discussed. When a node crash is detected, all locking activity in the database system is stopped. Then, any updates performed by failed transactions are undone. <p> Although many of the implementation issues of interest to us have been addressed in the context of SD systems <ref> [17, 15, 23, 22, 19, 20] </ref>, significant differences between SD and SM systems require that, for the most efficient implementation, different mechanisms be developed for SM. These differences stem from the different approaches used to achieving coherency. <p> Our work exploits the performance advantages of SM systems, yet guarantees good failure properties for transactions. In the previous sections, we discussed how our work is related to work in shared disk systems <ref> [17, 23, 22, 19] </ref>. Architectural differences between SD and cache-coherent SM, such as the unit of inter-system data sharing, how coherency is achieved, and whether shared memory is available, have a significant influence on the design and implementation of SM crash recovery protocols.
Reference: [24] <author> R. Strom, D. Bacon, and S. Yemini. </author> <title> Volatile Logging in n-Fault-Tolerant Distributed Systems. </title> <booktitle> Proceedings of the 18th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 44-49, </pages> <year> 1988. </year>
Reference-contexts: Our recovery protocols do not have these restrictions. 7 The SIM model is slightly different than the shared memory model. In SIM, a shared intermediate memory serves as a global shared buffer for all nodes. 19 Volatile logging has been used in the context of process checkpointing schemes <ref> [9, 24] </ref> to ensure a consistent state of a distributed computation without necessitating the rollback of any processes other than ones that failed.
Reference: [25] <author> S. Thakkar and M. Sweiger. </author> <title> Performance of an OLTP Application on Symmetry Multiprocessor System. </title> <booktitle> Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 228-238, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Based on a TP1 benchmark performed on a Sequent Symmetry shared memory multiprocessor, <ref> [25] </ref> conclude that an SM database system can deliver very high performance. In [26], an analytical and simulation study compares SN (shared nothing), SD, and SIM (shared intermediate memory 7 ). This comparison concludes that the data sharing architectures, especially SIM, are more resilient to transaction load surges.
Reference: [26] <author> P. Yu and A. Dan. </author> <title> Performance Evaluation of Transaction Processing Coupling Ar-chitecutres for Handling System Dynamics. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(2) </volume> <pages> 139-153, </pages> <month> June </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Based on a TP1 benchmark performed on a Sequent Symmetry shared memory multiprocessor, [25] conclude that an SM database system can deliver very high performance. In <ref> [26] </ref>, an analytical and simulation study compares SN (shared nothing), SD, and SIM (shared intermediate memory 7 ). This comparison concludes that the data sharing architectures, especially SIM, are more resilient to transaction load surges.
References-found: 26

