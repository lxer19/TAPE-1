URL: http://rakaposhi.eas.asu.edu/ihrig-cbr-wkshp.ps
Refering-URL: http://rakaposhi.eas.asu.edu/yochan.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: laurie.ihrig@asu.edu, rao@asu.edu  
Title: Evaluating the Effectiveness of Derivation Replay in Partial-order vs State-space Planning  
Author: Laurie H. Ihrig Subbarao Kambhampati 
Address: Tempe, AZ 85287-5406  
Affiliation: Department of Computer Science and Engineering Arizona State University,  
Date: 1994  
Note: Appears in the Proceedings of AAAI-94 Workshop on Case-based Reasoning,  
Abstract: Case-based planning involves storing individual instances of problem-solving episodes and using them to tackle new planning problems. This paper is concerned with derivation replay, which is the main component of a form of case-based planning called derivational analogy (DA). Prior to this study, implementations of derivation replay have been based within state-space planning. We are motivated by the acknowledged superiority of partial-order (PO) planners in plan generation. Here we demonstrate that plan-space planning also has an advantage in replay. We will argue that the decoupling of planning (derivation) order and the execution order of plan steps, provided by partial-order planners, enables them to exploit the guidance of previous cases in a more efficient and straightforward fashion. We validate our hypothesis through a focused empirical comparison. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barrett, A. and Weld, D. </author> <year> 1994. </year> <title> Partial order planning: evaluating possible efficience gains. </title> <booktitle> Artificial Intelligence 67(1). </booktitle>
Reference-contexts: This means that steps must be added to the plan according to their execution order. As an example, consider the state-space planner TOPI <ref> (Barrett and Weld 1994) </ref>. TOPI does simple backward search in a space of states, adding steps to the plan in reverse order of their execution. <p> To facilitate fair comparisons, the three planning methods, SNLP, TOPI, and NOLIMIT, were (re)implemented on the same substrate. ART-MD-NS Domain: Experiments were run on problems drawn from two domains. The first was the artificial domain, ART-MD-NS, originally described in <ref> (Barrett and Weld 1994) </ref> and shown in the table below: ART-MD-NS (D m S 2 ): i precond : I i add : P i delete : fI j jj &lt; ig) A 2 i precond : P i add : G i delete : fI j j8jg [ fP j
Reference: <author> Blumenthal, B. and Porter, B. </author> <year> 1994. </year> <title> Analysis and empirical studies of derivational analogy. </title> <journal> Artificial Intelligence. Forthcoming. </journal>
Reference-contexts: Then we will analyze the relative advantages of doing replay within plan-space vs state-space planners. One of the difficult decisions faced by the replay systems is that of deciding when and where to interleave from-scratch effort with derivation replay (c.f. <ref> (Blumenthal and Porter 1994) </ref>). In general, there are no domain-independent grounds for making this decision. This makes eager replay, i.e., replaying the entire trace before returning to from-scratch planning, the most straightforward strategy. <p> Analyzing the Generality of Experimental Results The experiments reported in this paper concentrated on the relative effectiveness of state-space and plan-space planners in supporting eager replay. Some implemented replay systems, such as REMAID <ref> (Blumenthal and Porter 1994) </ref> and PRODIGY/ANALOGY (Veloso 1992) utilize more complex forms of replay strategies to support derivational analogy. This raises a question as to the applicability of our experimental results to such frameworks. We will address this issue below. <p> This raises a question as to the applicability of our experimental results to such frameworks. We will address this issue below. Interrupting Replay : Some replay systems interrupt replay of the trace to interleave from-scratch effort (e.g. <ref> (Blumenthal and Porter 1994) </ref>). It would seem as if such strategies could offset the inflexibility of state-space planners in replaying cases when step order is critical. Consider again our example problem from the logistics domain. <p> The early work on derivation replay that is rooted in state-space planning has thus been forced to focus a good deal of attention on the problem of determining when to plan for additional goals <ref> (Blumenthal and Porter 1994) </ref>. In general, there are no domain-independent grounds for deciding when and where the from-scratch problem-solving effort should be interleaved with replay. This leaves the planner with heuristic guesses, which may turn out to be incorrect, leading to backtracking once again.
Reference: <author> Carbonell, J. </author> <year> 1986. </year> <title> Derivational analogy: A theory of reconstructive problem solving and expertise acquisition. </title> <editor> In Michalski, Ryszard; Carbonell, Jaime; and Mitchell, Tom M., editors 1986, </editor> <booktitle> Machine Learning: an Artificial Intelligence approach: </booktitle> <volume> Volume 2. </volume> <month> Morgan-Kaufman. </month>
Reference: <author> Ihrig, L. and Kambhampati, S. </author> <year> 1994. </year> <title> Derivation replay for partial-order planning. </title> <booktitle> In Proceedings AAAI--94. </booktitle>
Reference-contexts: The next section describes an empirical evaluation of this hypothesis. Empirical Evaluation An empirical analysis was conducted in order to test our hypothesis regarding the relative effectiveness of eager replay for PO planners. For a more detailed description of these experiments, see <ref> (Ihrig and Kambhampati 1994) </ref>. We implemented an eager-replay strategy on the state-space planner, TOPI (DerTOPI), and the plan-space planner SNLP (Der-SNLP). With this strategy, the search process is interrupted to replay the entire derivation trace before returning to from-scratch planning. <p> Problem size was increased by one goal for each phase. A library of cases was formed over the entire run. Each time a problem was attempted, the library was searched for a previous case that was similar (See <ref> (Ihrig and Kambhampati 1994) </ref>). If one was found, the new problem was run both in scratch and replay mode, and the problem became part of the 30 problem set for that phase. If there was no previous case that applied, the problem was merely added to the library.
Reference: <author> Kambhampati, S. </author> <year> 1994. </year> <title> Exploiting causal structure to control retrieval and refitting during plan reuse. </title> <journal> Computational Intelligence Journal 10(2). </journal>
Reference-contexts: The storage and retrieval aspects of DA remain the same whether we use plan-space or state-space planners. In particular, solutions to the storage problem such as those proposed in (Veloso 1992) and <ref> (Kambhampati 1994) </ref> can be used for this purpose. Only the contents of the trace and the details of the replay component depend on the underlying planner. Thus, in our evaluation we focus on the automatic generation and replay of the solution trace. <p> The next section describes an empirical evaluation of this hypothesis. Empirical Evaluation An empirical analysis was conducted in order to test our hypothesis regarding the relative effectiveness of eager replay for PO planners. For a more detailed description of these experiments, see <ref> (Ihrig and Kambhampati 1994) </ref>. We implemented an eager-replay strategy on the state-space planner, TOPI (DerTOPI), and the plan-space planner SNLP (Der-SNLP). With this strategy, the search process is interrupted to replay the entire derivation trace before returning to from-scratch planning. <p> Problem size was increased by one goal for each phase. A library of cases was formed over the entire run. Each time a problem was attempted, the library was searched for a previous case that was similar (See <ref> (Ihrig and Kambhampati 1994) </ref>). If one was found, the new problem was run both in scratch and replay mode, and the problem became part of the 30 problem set for that phase. If there was no previous case that applied, the problem was merely added to the library.
Reference: <author> McAllester, D. and Rosenblitt, </author> <title> D 1991. Systematic nonlinear planning. </title> <booktitle> In Proceedings AAAI-91. </booktitle> <address> 634--639. </address>
Reference: <author> Minton, S.; Drummond, M.; Bresina, J.; and Philips, </author> <title> A 1992. Total order vs partial order planning: factors influencing performance. </title> <booktitle> In Proceedings KR-92. </booktitle>
Reference: <author> Veloso, M. </author> <year> 1992. </year> <title> Learning by analogical reasoning in general problem solving. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie-Mellon University. </institution>
Reference-contexts: The storage and retrieval aspects of DA remain the same whether we use plan-space or state-space planners. In particular, solutions to the storage problem such as those proposed in <ref> (Veloso 1992) </ref> and (Kambhampati 1994) can be used for this purpose. Only the contents of the trace and the details of the replay component depend on the underlying planner. Thus, in our evaluation we focus on the automatic generation and replay of the solution trace. <p> ((AT-PL ?P1 AP1) 1) PL=PLANE,AP=AIRPORT,OB=OBJECT Final Plan: (FLY-PL PL1 AP3 AP2) Created 4 (LOAD-PL OB2 PL1 AP2) Created 3 (FLY-PL PL1 AP2 AP1) Created 2 (UNLOAD-PL OB2 PL1 AP1) Created 1 Ordering of Steps: ((4 &lt; 3) (3 &lt; 2) (2 &lt; 1)) problem from the logistics transportation domain of <ref> (Veloso 1992) </ref> is provided in Figure 1. This domain involves the movement of packages across locations by various transport devices. <p> With this strategy, the search process is interrupted to replay the entire derivation trace before returning to from-scratch planning. We also implemented eager replay on a second state-space planner, NOLIMIT, since it recently served as a substrate for a comprehensive derivational analogy system <ref> (Veloso 1992) </ref>. Although NOLIMIT differs from TOPI in several respects 1 , it is also a state-space planner in the sense that it extends a current partial plan by adding steps to the end of the plan. <p> The plan for the new conjunctive goal G 1 ^ G 2 ^ G 3 is A 1 2 ! A 1 1 ! A 2 3 . Logistics Transportation Domain: The logistics transportation domain of <ref> (Veloso 1992) </ref> was adopted for the second set of experiments. Initial conditions of each problem represented the location of various transport devices (one airplane and three trucks) over three cities, each city containing an airport and a post office. <p> Analyzing the Generality of Experimental Results The experiments reported in this paper concentrated on the relative effectiveness of state-space and plan-space planners in supporting eager replay. Some implemented replay systems, such as REMAID (Blumenthal and Porter 1994) and PRODIGY/ANALOGY <ref> (Veloso 1992) </ref> utilize more complex forms of replay strategies to support derivational analogy. This raises a question as to the applicability of our experimental results to such frameworks. We will address this issue below. <p> In our example above, this will involve replaying the old trace once, and when the planner fails to extend it to deal with the new goal, backtracking, working on the extra goal, and then replaying the case again. Veloso <ref> (Veloso 1992) </ref> discusses an interesting mechanism for keeping track of the extent to which the case has been replayed, and moving the pointer up appropriately whenever backtracking occurs. <p> Before concluding this section, we would like to make some observations about the apparent disparity between our results and the previous work on DA rooted in state-space planning which has demonstrated significant performance improvements with replay <ref> (Veloso 1992) </ref>. We believe the main reason for this may be the strong presence of interacting goals in our multi-goal problems, coupled with the fact that we used vanilla planning algorithms without any sophisticated backtracking strategies or pruning techniques.
References-found: 8

