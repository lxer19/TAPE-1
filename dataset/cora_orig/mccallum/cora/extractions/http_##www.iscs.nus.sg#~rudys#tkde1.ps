URL: http://www.iscs.nus.sg/~rudys/tkde1.ps
Refering-URL: 
Root-URL: 
Email: fluhj,rudys,liuhg@iscs.nus.sg  
Title: Effective Data Mining Using Neural Networks  
Author: Hongjun Lu Rudy Setiono Huan Liu 
Address: Ridge, Singapore 119260  
Affiliation: Department of Information Systems Computer Science National University of Singapore Kent  
Abstract: Classification is one of the data mining problems receiving great attention recently in the database community. This paper presents an approach to discover symbolic classification rules using neural networks. Neural networks have not been thought suited for data mining because how the classifications were made is not explicitly stated as symbolic rules that are suitable for verification or interpretation by humans. With the proposed approach, concise symbolic rules with high accuracy can be extracted from a neural network. The network is first trained to achieve the required accuracy rate. Redundant connections of the network are then removed by a network pruning algorithm. The activation values of the hidden units in the network are analyzed, and classification rules are generated using the result of this analysis. The effectiveness of the proposed approach is clearly demonstrated by the experimental results on a set of standard data mining test problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 5(6), </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: The algorithm is order sensitive, that is, the resulting clusters depend on the order in which the hidden units are clustered. 2.2 An illustrative example In <ref> [1] </ref>, ten classification problems are defined on datasets having nine attributes: salary, commission, age, elevel, car, zipcode, house-value, house-years, and loan. We use one of the problems, Function 3 as an example to illustrate how classification rules can be generated from a network. <p> Attribute No. of inputs Subintervals salary 6 [20k; 25k); [25k; 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; <ref> [1] </ref>; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that <p> Attribute No. of inputs Subintervals salary 6 [20k; 25k); [25k; 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 <ref> [1; 5] </ref>; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test <p> 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 <ref> [1; 3] </ref>; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1. <p> [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 <ref> [1; 10] </ref>; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1. <p> There were two clusters of activation values found at each of the two hidden units. Tuples were split into two sets by the first hidden unit: those with activation values in the subinterval [1; 0:46) and those in [0:46; 1). The subintervals of the second hidden unit were <ref> [1; 0:81) and [0:81; 1] </ref>. Let ff j = 1 or 2 for j = 1 or 2 represent an activation value of a pattern at hidden unit j located in its first or second subinterval. <p> Tuples were split into two sets by the first hidden unit: those with activation values in the subinterval [1; 0:46) and those in [0:46; 1). The subintervals of the second hidden unit were [1; 0:81) and <ref> [0:81; 1] </ref>. Let ff j = 1 or 2 for j = 1 or 2 represent an activation value of a pattern at hidden unit j located in its first or second subinterval. <p> Three strings labeled 1 corresponded to original input tuples that had activation values in the interval [1; 0:46). The remaining six strings were labeled 2 since they represented inputs that had activation values in the second subinterval, <ref> [0:46; 1] </ref>. The rules generated were as follows: Rule 1. If (I 11 = I 13 = I 18 = 1), then ff 1 = 1; Rule 2. If (I 13 = I 18 = 1 and I 16 = 0), then ff 1 = 1. <p> However, the number of conditions per rule will generally be higher than that of decision trees' rules. 8 3 Experimental results To test the neural network approach more rigorously, a series of experiments were conducted to solve the problems defined by Agrawal et al. <ref> [1] </ref>. Among 10 functions described, we found that functions 8 and 10 produced highly skewed data that made classification not meaningful. We will only discuss functions other than these two. The values of the attributes of each tuple were generated randomly according to the distributions given in [1]. <p> Agrawal et al. <ref> [1] </ref>. Among 10 functions described, we found that functions 8 and 10 produced highly skewed data that made classification not meaningful. We will only discuss functions other than these two. The values of the attributes of each tuple were generated randomly according to the distributions given in [1]. Following Agrawal et al. [1], we also included a perturbation factor as one of the parameters of the random data generator. This perturbation factor was set at 5 percent. For each tuple, a class label was determined according to the rules that define the function. <p> We will only discuss functions other than these two. The values of the attributes of each tuple were generated randomly according to the distributions given in <ref> [1] </ref>. Following Agrawal et al. [1], we also included a perturbation factor as one of the parameters of the random data generator. This perturbation factor was set at 5 percent. For each tuple, a class label was determined according to the rules that define the function. For each problem, 3000 tuples were generated.
Reference: [2] <author> J.E. Dennis Jr. and R.B. Schnabel. </author> <title> Numerical methods for unconstrained optimization and nonlinear equations. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1983. </year>
Reference-contexts: Attribute No. of inputs Subintervals salary 6 [20k; 25k); [25k; 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; <ref> [2] </ref>; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved <p> To express the rules in terms of the original attribute values, we refer to Table 2 which shows the binary representations for attributes age and elevel. Referring to Table 2, the above set of rules is equivalent to: R1. If age 60 and elevel 2 <ref> [2; 3; 4] </ref>, then Group A. R2. If age 40 and elevel 2 [2; 3], then Group A. R3. If age &lt; 60 and elevel = 1, then Group A. R4. If age &lt; 40 and elevel = 0, then Group A. Default rule. Group B. <p> Referring to Table 2, the above set of rules is equivalent to: R1. If age 60 and elevel 2 [2; 3; 4], then Group A. R2. If age 40 and elevel 2 <ref> [2; 3] </ref>, then Group A. R3. If age &lt; 60 and elevel = 1, then Group A. R4. If age &lt; 40 and elevel = 0, then Group A. Default rule. Group B.
Reference: [3] <author> H. Liu and S.T. Tan. X2R: </author> <title> A fast rule generator. </title> <booktitle> In Proceedings of IEEE International Conference on Systems, Man and Cybernetics. IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: It takes as input a set of discrete patterns with the class labels and produces the rules describing the relationship between the patterns and their class labels. The details of this rule generation algorithm can be found in our earlier work <ref> [3] </ref>. To cluster the activation values, we used a simple clustering algorithm which consists of the following steps: 1. Find the smallest integer d such that if all the network activation values are rounded to d-decimal-place, the network still retains its accuracy rate. 2. <p> Attribute No. of inputs Subintervals salary 6 [20k; 25k); [25k; 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; <ref> [3] </ref>; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% <p> 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 <ref> [1; 3] </ref>; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1. <p> 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; <ref> [3; 6] </ref>; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1. <p> To express the rules in terms of the original attribute values, we refer to Table 2 which shows the binary representations for attributes age and elevel. Referring to Table 2, the above set of rules is equivalent to: R1. If age 60 and elevel 2 <ref> [2; 3; 4] </ref>, then Group A. R2. If age 40 and elevel 2 [2; 3], then Group A. R3. If age &lt; 60 and elevel = 1, then Group A. R4. If age &lt; 40 and elevel = 0, then Group A. Default rule. Group B. <p> Referring to Table 2, the above set of rules is equivalent to: R1. If age 60 and elevel 2 [2; 3; 4], then Group A. R2. If age 40 and elevel 2 <ref> [2; 3] </ref>, then Group A. R3. If age &lt; 60 and elevel = 1, then Group A. R4. If age &lt; 40 and elevel = 0, then Group A. Default rule. Group B.
Reference: [4] <author> H. Lu, R. Setiono, and H. Liu. Neurorule: </author> <title> A connectionist approach to data mining. </title> <booktitle> In Proceedings of VLDB'95, </booktitle> <pages> 478-489, </pages> <year> 1995. </year>
Reference-contexts: In some cases, neural networks give a lower classification error rate than the decision trees but require longer learning time [7, 8]. In this paper, we present our results from applying neural networks to mine classification rules for large databases <ref> [4] </ref> with the focus on articulating the classification rules represented by neural networks. <p> Attribute No. of inputs Subintervals salary 6 [20k; 25k); [25k; 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; <ref> [4] </ref> car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy <p> To express the rules in terms of the original attribute values, we refer to Table 2 which shows the binary representations for attributes age and elevel. Referring to Table 2, the above set of rules is equivalent to: R1. If age 60 and elevel 2 <ref> [2; 3; 4] </ref>, then Group A. R2. If age 40 and elevel 2 [2; 3], then Group A. R3. If age &lt; 60 and elevel = 1, then Group A. R4. If age &lt; 40 and elevel = 0, then Group A. Default rule. Group B.
Reference: [5] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <booktitle> Ellis Horwood Series in Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference-contexts: Articulating the classification rules becomes a difficult problem. 3. For the same reason, available domain knowledge is rather difficult to be incorporated to a neural network. On the other hand, the use of neural networks in classification is not uncommon in machine learning community <ref> [5] </ref>. In some cases, neural networks give a lower classification error rate than the decision trees but require longer learning time [7, 8]. <p> Attribute No. of inputs Subintervals salary 6 [20k; 25k); [25k; 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 <ref> [1; 5] </ref>; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test
Reference: [6] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: of inputs Subintervals salary 6 [20k; 25k); [25k; 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; <ref> [6; 10] </ref>; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set <p> 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; <ref> [3; 6] </ref>; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1. <p> 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; <ref> [6; 9] </ref> hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1. <p> These averages are obtained from 3 fi 10 neural networks. Error due to perturbation in the test data set was subtracted from the total error, and the average accuracy rates shown in the table reflect this correction. For comparison, we have also run C4.5 <ref> [6] </ref> for the same data sets. Classification rules were generated from the trees by C4.5rules. The same binary coded data for neural networks were used for C4.5 and C4.5rules. Figures 3-5 show the accuracy, number of rules, and the average number of conditions in the rules generated by two approaches.
Reference: [7] <author> J.R. Quinlan. </author> <title> Comparing connectionist and symbolic learning methods. </title> <editor> In S.J. Hanson, G.A. Drastall, and R.L. Rivest, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> volume 1, </volume> <pages> 445-456. </pages> <publisher> A Bradford Book, The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: On the other hand, the use of neural networks in classification is not uncommon in machine learning community [5]. In some cases, neural networks give a lower classification error rate than the decision trees but require longer learning time <ref> [7, 8] </ref>. In this paper, we present our results from applying neural networks to mine classification rules for large databases [4] with the focus on articulating the classification rules represented by neural networks.
Reference: [8] <author> J.W. Shavlik, R.J. Mooney, and G.G. Towell. </author> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6(2) </volume> <pages> 111-143, </pages> <year> 1991. </year>
Reference-contexts: On the other hand, the use of neural networks in classification is not uncommon in machine learning community [5]. In some cases, neural networks give a lower classification error rate than the decision trees but require longer learning time <ref> [7, 8] </ref>. In this paper, we present our results from applying neural networks to mine classification rules for large databases [4] with the focus on articulating the classification rules represented by neural networks.
Reference: [9] <author> R. Setiono. </author> <title> A neural network construction algorithm which maximizes the likelihood function. </title> <journal> Connection Sccience, </journal> <volume> 7(2) </volume> <pages> 147-166, </pages> <year> 1995. </year>
Reference-contexts: Due to space limitation, in this paper we omit the discussion of the first two phases. Details of these phases can be found in our earlier work <ref> [9, 10] </ref>. We shall elaborate in this paper the third phase. Section 2 describes our algorithms to extract classification rules from a neural network and uses an example to illustrate how the rules are generated using the proposed approach. Section 3 presents some experimental results obtained. <p> 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; <ref> [6; 9] </ref> hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1.
Reference: [10] <author> R. Setiono. </author> <title> A penalty function approach for pruning feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 8(8), </volume> <year> 1996. </year> <month> 13 </month>
Reference-contexts: Due to space limitation, in this paper we omit the discussion of the first two phases. Details of these phases can be found in our earlier work <ref> [9, 10] </ref>. We shall elaborate in this paper the third phase. Section 2 describes our algorithms to extract classification rules from a neural network and uses an example to illustrate how the rules are generated using the proposed approach. Section 3 presents some experimental results obtained. <p> of inputs Subintervals salary 6 [20k; 25k); [25k; 50k); [50k; 75k); [75k; 100k); [100k; 125k); [125k; 150k] commission 3 [0; 25k); [25k; 50k); [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; <ref> [6; 10] </ref>; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set <p> [50k; 75k] age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 <ref> [1; 10] </ref>; [10; 20]; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1. <p> age 6 [20; 30); [30; 40); [40; 50); [50; 60); [60; 70); [70; 80] elevel 4 [0]; [1]; [2]; [3]; [4] car 4 [1; 5]; [6; 10]; [11; 15]; [15; 20] zipcode 3 [1; 3]; [3; 6]; [6; 9] hvalue 3 [0:450k); [450k; 900k); [900k; 1350k] hyears 3 [1; 10]; <ref> [10; 20] </ref>; [20; 30] loan 5 [0; 100k); [100k; 200k); [200k; 300k); [300k; 400k); [400k; 500k] network that achieved 100% accuracy on the test data set is depicted in Figure 1. Recall that the initial network had 38 input units, 6 hidden units, 1 output unit, and therefore 234 links.
References-found: 10

