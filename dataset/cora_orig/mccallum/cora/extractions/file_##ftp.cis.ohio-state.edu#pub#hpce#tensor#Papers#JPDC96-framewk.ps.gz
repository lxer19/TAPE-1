URL: file://ftp.cis.ohio-state.edu/pub/hpce/tensor/Papers/JPDC96-framewk.ps.gz
Refering-URL: http://www.cis.ohio-state.edu/~chh/Publication/tensor-papers.html
Root-URL: 
Email: e-mail: fsandeep, chh, sadayg@cis.ohio-state.edu  e-mail: rwj@eeyore.stcloud.msus.edu  
Title: A Framework for Generating Distributed-Memory Parallel Programs for Block Recursive Algorithms  
Author: S. K. S. Gupta, C.-H. Huang, and P. Sadayappan and R. W. Johnson 
Keyword: Distributed-memory multiprocessor, program synthesis, data distribution, communication optimization, tensor (Kronecker) product, block recursive algorithm, fast Fourier transform.  
Address: Columbus, OH 43210  St. Cloud, MN 56301  
Affiliation: Department of Computer and Information Science The Ohio State University  Department of Computer Science St. Cloud State University  
Note: To appear in Journal of Parallel and Distributed Computing 1996.  This work was supported in part by ARPA, order number 7898, monitored by NIST under grant number 60NANB1D1151, and ARPA, order number 7899, monitored by NIST under grant number 60NANB1D1150.  
Abstract: In this paper, we present a framework for synthesizing communication-efficient distributed-memory parallel programs for block recursive algorithms such as the fast Fourier transform (FFT) and Strassen's matrix multiplication. This framework is based on an explicitly-parallel algebraic representation of the algorithms, which involves the tensor (Kronecker) product and other matrix operations. This representation is useful in analyzing the communication implications of computation partitioning and data distribution. In this framework, programs are synthesized under two different target program models. These two models are based on different ways of managing the distribution of data for optimizing communication. In the first model, distributions of arrays are kept static and so communication is needed whenever a processor requires data elements which it does not own. In the second model, distributions of the data arrays are dynamically changed to ensure that computation is localized in every computation step. This results in programs with different communication overhead characteristics due to use of different communication primitives for performing data movement. The first model uses point-to-point interprocessor communication primitives whereas the second model uses data redistribution primitives involving collective all-to-many communication. These two program models are shown to be suitable for different ranges of the problem size. The programs with redistributions have lower communication overhead than programs using point-to-point communication primitives when the problem size is large. The methodology is illustrated by synthesizing communication-efficient programs for the Cooley-Tukey FFT and the Stockham FFT. Performance results on the Intel iPSC/860 system are presented. This framework has been incorporated in the EXTENT parallel programming environment for automatic generation of parallel/vector programs for block recursive algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Malony, and B. Mohr. </author> <title> Implementing a 28 parallel C++ runtime system for scalable parallel systems. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 588-597, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: These distributions are proposed in Fortran D [3, 13], Vienna Fortran [5], High Performance Fortran (HPF) [8], and pC++ <ref> [1] </ref>. Block and cyclic distributions may be viewed as special cases of the block-cyclic distribution. A block-cyclic distribution partitions an array into equal sized blocks of consecutive elements and then maps them to the processors in a cyclic manner.
Reference: [2] <author> S. H. Bokhari. </author> <title> Complete exchange on the iPSC. </title> <type> Technical Report 91-4, </type> <institution> ICASE, </institution> <year> 1991. </year>
Reference-contexts: The Intel iPSC/860 is a distributed-memory multiprocessor with a hypercube topology. Redistribution primitives, which require all-to-all communication, were implemented using pairwise exchange <ref> [2] </ref>. Performance was measured using the millisecond node timer mclock. Parallel programs under the point-to-point message passing model were implemented for both the Cooley-Tukey FFT (CT-PP) and the Stockham FFT (ST-PP). On a hypercube of size P , both CT-PP and ST-PP uses log (P ) communication steps.
Reference: [3] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF compiler for distributed memory MIMD computers: Design implementation and performance results. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 351-360, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: These distributions are proposed in Fortran D <ref> [3, 13] </ref>, Vienna Fortran [5], High Performance Fortran (HPF) [8], and pC++ [1]. Block and cyclic distributions may be viewed as special cases of the block-cyclic distribution.
Reference: [4] <author> J. W. Brewer. </author> <title> Kronecker products and matrix calculus in system theory. </title> <journal> IEEE Transaction on Ciruits and Systems, </journal> <volume> 25 </volume> <pages> 772-781, </pages> <year> 1978. </year>
Reference-contexts: The mathematical framework used is based on the tensor (Kronecker) product [9] and other matrix operations. The tensor product has been used for modeling algorithms with recursive computational structure which occur in application areas such as digital signal processing [10, 21], image processing [22], linear system design <ref> [4] </ref>, and statistics [11]. In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) [17, 24] and matrix multiplication [14, 15] for shared-memory vector multiprocessors.
Reference: [5] <author> B. M. Chapman, P. Mehrotra, and H. P. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Language, Compilers and Runtime Environments for Distributed Memory Machines, </booktitle> <pages> pages 39-62. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference-contexts: These distributions are proposed in Fortran D [3, 13], Vienna Fortran <ref> [5] </ref>, High Performance Fortran (HPF) [8], and pC++ [1]. Block and cyclic distributions may be viewed as special cases of the block-cyclic distribution. A block-cyclic distribution partitions an array into equal sized blocks of consecutive elements and then maps them to the processors in a cyclic manner.
Reference: [6] <author> D. L. Dai, S. K. S. Gupta, S. D. Kaushik, J. H. Lu, R. V. Singh, C.-H. Huang, P. Sadayappan, and R. W. Johnson. </author> <title> EXTENT: A portable programming environment for designing and implementing high performance block recursive algorithms. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: This framework has been incorporated in EXTENT (An EXpert system for TENsor product Translation) which is a parallel programming environment for automatic generation of parallel/vector programs from tensor product formulas <ref> [6] </ref>. The paper is organized as follows. Section 2 gives an overview of the theory of tensor products. In Section 3, we motivate the two programming models used in this paper. Section 4 presents the algebraic semantics of regular data distributions using tensor products.
Reference: [7] <author> M. Davio. </author> <title> Kronecker products and shu*e algebra. </title> <journal> IEEE Transaction on Computers, </journal> <volume> 30(2) </volume> <pages> 116-125, </pages> <month> Feb. </month> <year> 1981. </year>
Reference-contexts: How ever, to interpret the application (A p;n I m ) to X mn as parallel operations we need to understand stride permutations. Stride permutations (shu*e permutations <ref> [7] </ref>) belong to a special class of permutations called the tensor permutations [16].
Reference: [8] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <year> 1993. </year>
Reference-contexts: These distributions are proposed in Fortran D [3, 13], Vienna Fortran [5], High Performance Fortran (HPF) <ref> [8] </ref>, and pC++ [1]. Block and cyclic distributions may be viewed as special cases of the block-cyclic distribution. A block-cyclic distribution partitions an array into equal sized blocks of consecutive elements and then maps them to the processors in a cyclic manner.
Reference: [9] <author> A. Graham. </author> <title> Kronecker Products and Matrix Calculus: With Applications. </title> <publisher> Ellis Horwood Limited, </publisher> <year> 1981. </year>
Reference-contexts: 1 Introduction In this paper, we present a framework for synthesizing communication-efficient distributed-memory programs. This framework is useful in synthesizing programs for block recursive algorithms. The mathematical framework used is based on the tensor (Kronecker) product <ref> [9] </ref> and other matrix operations. The tensor product has been used for modeling algorithms with recursive computational structure which occur in application areas such as digital signal processing [10, 21], image processing [22], linear system design [4], and statistics [11].
Reference: [10] <author> J. Granta, M. Conner, and R. Tolimieri. </author> <title> Recursive fast algorithms and the role of the tensor product. </title> <journal> IEEE Transaction on Signal Processing, </journal> <volume> 40(12) </volume> <pages> 2921-2930, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: The mathematical framework used is based on the tensor (Kronecker) product [9] and other matrix operations. The tensor product has been used for modeling algorithms with recursive computational structure which occur in application areas such as digital signal processing <ref> [10, 21] </ref>, image processing [22], linear system design [4], and statistics [11]. In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) [17, 24] and matrix multiplication [14, 15] for shared-memory vector multiprocessors. <p> Some other examples of block recursive algorithms are Strassen's matrix multiplication [15, 18], convolution <ref> [10] </ref>, and fast sine/cosine transforms [24].
Reference: [11] <author> F. A. Graybill. </author> <title> Matrices, with applications in Statistics. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, </address> <year> 1983. </year>
Reference-contexts: The tensor product has been used for modeling algorithms with recursive computational structure which occur in application areas such as digital signal processing [10, 21], image processing [22], linear system design [4], and statistics <ref> [11] </ref>. In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) [17, 24] and matrix multiplication [14, 15] for shared-memory vector multiprocessors.
Reference: [12] <author> S. K. S. Gupta, S. D. Kaushik, C.-H. Huang, J. R. Johnson, R. W. Johnson, and P. Sadayappan. </author> <title> A methodology for the generation of data distributions to optimize communication. </title> <booktitle> In Fourth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 436-441, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: However, by performing a single redistribution, the remaining two steps have been made communication-free. 8 We will next present a tensor product based algebraic representation, called the distribution basis, to describe the semantics of regular data distributions <ref> [12] </ref>. 4 Algebraic Semantics of Data Distributions The most common distributions used for arrays are the block, cyclic, and block-cyclic distributions. These distributions are proposed in Fortran D [3, 13], Vienna Fortran [5], High Performance Fortran (HPF) [8], and pC++ [1]. <p> An algebraic representation for a block-cyclic distribution of A (0 : N 1) on P processors can be obtained by associating index k of A to vector basis e N k <ref> [12] </ref>. Associating indices of an array with vector bases helps in determining the indexing needed when the array is segmented or viewed as a multi-dimensional array. For example, consider the segmented view of an array of size N corresponding to block distribution on P processors.
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran-D for MIMD distributed-memory machines. </title> <journal> Communication of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: These distributions are proposed in Fortran D <ref> [3, 13] </ref>, Vienna Fortran [5], High Performance Fortran (HPF) [8], and pC++ [1]. Block and cyclic distributions may be viewed as special cases of the block-cyclic distribution.
Reference: [14] <author> C.-H. Huang, J. R. Johnson, and R. W. Johnson. </author> <title> A tensor product formulation of Strassen's matrix multiplication algorithm. </title> <journal> Appl. Math Letters, </journal> <volume> 3(3) </volume> <pages> 67-71, </pages> <year> 1990. </year>
Reference-contexts: In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) [17, 24] and matrix multiplication <ref> [14, 15] </ref> for shared-memory vector multiprocessors. The significance of the tensor product lies in its ability to model computational structures occurring in a wide spectrum of supercomputer architectures, as well as, the underlying hardware structures, like the interconnection networks [19, 20].
Reference: [15] <author> C.-H. Huang, J. R. Johnson, and R. W. Johnson. </author> <title> Generating parallel programs from tensor product formulas: A case study of Strassen's matrix multiplication algorithm. </title> <booktitle> In Proc. International Conference on Parallel Processing 1992, </booktitle> <pages> pages 104-108, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) [17, 24] and matrix multiplication <ref> [14, 15] </ref> for shared-memory vector multiprocessors. The significance of the tensor product lies in its ability to model computational structures occurring in a wide spectrum of supercomputer architectures, as well as, the underlying hardware structures, like the interconnection networks [19, 20]. <p> Some other examples of block recursive algorithms are Strassen's matrix multiplication <ref> [15, 18] </ref>, convolution [10], and fast sine/cosine transforms [24].
Reference: [16] <author> J. R. Johnson, C.-H. Huang, and R. W. Johnson. </author> <title> Tensor permutations and block matrix allocation. </title> <editor> In G. Hains and L. Mullin, editors, </editor> <booktitle> Second International Workshop on Array Structures (ATABLE-92), Pub. </booktitle> <volume> No. </volume> <pages> 841. </pages> <institution> Dept. of Info. and Oper. Research, Univ. of Montreal, </institution> <year> 1992. </year> <month> 29 </month>
Reference-contexts: How ever, to interpret the application (A p;n I m ) to X mn as parallel operations we need to understand stride permutations. Stride permutations (shu*e permutations [7]) belong to a special class of permutations called the tensor permutations <ref> [16] </ref>.
Reference: [17] <author> J. R. Johnson, R. W. Johnson, D. Rodriguez, and R. Tolimieri. </author> <title> A methodology for designing, modi-fying and implementing fourier transform algorithms on various architectures. </title> <journal> Circuits Systems Signal Process, </journal> <volume> 9(4) </volume> <pages> 450-500, </pages> <year> 1990. </year>
Reference-contexts: In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) <ref> [17, 24] </ref> and matrix multiplication [14, 15] for shared-memory vector multiprocessors. The significance of the tensor product lies in its ability to model computational structures occurring in a wide spectrum of supercomputer architectures, as well as, the underlying hardware structures, like the interconnection networks [19, 20]. <p> We next present tensor product formulations of two FFT algorithms which are used as examples in this paper. Fast Fourier Transform The tensor product formulations of various FFT algorithms are presented in <ref> [17, 24] </ref>. These formulations are obtained by different tensor factorization of the discrete Fourier transform matrix. Although all of these algorithms are computationally equivalent, they have different computational structures and different data access patterns.
Reference: [18] <author> R. W. Johnson, C.-H. Huang, and J. R. Johnson. </author> <title> Multilinear algebra and parallel programming. </title> <journal> Journal of Supercomputing, </journal> <volume> 5 </volume> <pages> 189-218, </pages> <year> 1991. </year>
Reference-contexts: Some other examples of block recursive algorithms are Strassen's matrix multiplication <ref> [15, 18] </ref>, convolution [10], and fast sine/cosine transforms [24].
Reference: [19] <author> S. D. Kaushik, S. Sharma, and C.-H. Huang. </author> <title> An algebraic theory for modeling multistage interconnection networks. </title> <journal> Journal of Information Science and Engineering, </journal> <volume> 9 </volume> <pages> 1-26, </pages> <year> 1993. </year>
Reference-contexts: The significance of the tensor product lies in its ability to model computational structures occurring in a wide spectrum of supercomputer architectures, as well as, the underlying hardware structures, like the interconnection networks <ref> [19, 20] </ref>. The goal of this paper is to develop a framework for synthesizing efficient distributed-memory programs for block recursive algorithms. In this framework we start with a mathematical specification of the computation using tensor products.
Reference: [20] <author> S. D. Kaushik, S. Sharma, C.-H. Huang, J. R. Johnson, R. W. Johnson, and P. Sadayappan. </author> <title> An algebraic theory for modelling direct interconnection networks. </title> <booktitle> In Supercomputing '92, </booktitle> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The significance of the tensor product lies in its ability to model computational structures occurring in a wide spectrum of supercomputer architectures, as well as, the underlying hardware structures, like the interconnection networks <ref> [19, 20] </ref>. The goal of this paper is to develop a framework for synthesizing efficient distributed-memory programs for block recursive algorithms. In this framework we start with a mathematical specification of the computation using tensor products.
Reference: [21] <author> P. A. Regalia and S. K. Mitra. </author> <title> Kronecker products, unitary matrices and signal processing applications. </title> <journal> SIAM Reviews, </journal> <volume> 31(4) </volume> <pages> 586-613, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: The mathematical framework used is based on the tensor (Kronecker) product [9] and other matrix operations. The tensor product has been used for modeling algorithms with recursive computational structure which occur in application areas such as digital signal processing <ref> [10, 21] </ref>, image processing [22], linear system design [4], and statistics [11]. In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) [17, 24] and matrix multiplication [14, 15] for shared-memory vector multiprocessors.
Reference: [22] <author> G. X. Ritter and P. D. Gader. </author> <title> Image algebra techniques and parallel image processing. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 4 </volume> <pages> 7-44, </pages> <year> 1987. </year>
Reference-contexts: The mathematical framework used is based on the tensor (Kronecker) product [9] and other matrix operations. The tensor product has been used for modeling algorithms with recursive computational structure which occur in application areas such as digital signal processing [10, 21], image processing <ref> [22] </ref>, linear system design [4], and statistics [11]. In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) [17, 24] and matrix multiplication [14, 15] for shared-memory vector multiprocessors.
Reference: [23] <author> T. G. Stockham. </author> <title> High speed convolution and correlation. </title> <booktitle> In Spring Joint Computer Conf., Proc. of AFIPS, </booktitle> <pages> pages 229-233, </pages> <year> 1966. </year>
Reference-contexts: As another example, consider the tensor product formulation of the radix-2 decimation-in-time Stockham FFT <ref> [23] </ref>: n Y ((F 2 I 2 n1 )(T 2 i 2 I 2 ni )): (2) As in the case of the Cooley-Tukey FFT, the 2 n points Stockham FFT has n steps.
Reference: [24] <author> C. Van Loan. </author> <title> Computational frameworks for the fast Fourier transform. </title> <publisher> SIAM, </publisher> <year> 1992. </year> <month> 30 </month>
Reference-contexts: In recent years, the tensor product framework has been successfully used to design and implement high-performance algorithms to compute discrete Fourier transform (DFT) <ref> [17, 24] </ref> and matrix multiplication [14, 15] for shared-memory vector multiprocessors. The significance of the tensor product lies in its ability to model computational structures occurring in a wide spectrum of supercomputer architectures, as well as, the underlying hardware structures, like the interconnection networks [19, 20]. <p> Some other examples of block recursive algorithms are Strassen's matrix multiplication [15, 18], convolution [10], and fast sine/cosine transforms <ref> [24] </ref>. <p> We next present tensor product formulations of two FFT algorithms which are used as examples in this paper. Fast Fourier Transform The tensor product formulations of various FFT algorithms are presented in <ref> [17, 24] </ref>. These formulations are obtained by different tensor factorization of the discrete Fourier transform matrix. Although all of these algorithms are computationally equivalent, they have different computational structures and different data access patterns.
References-found: 24

