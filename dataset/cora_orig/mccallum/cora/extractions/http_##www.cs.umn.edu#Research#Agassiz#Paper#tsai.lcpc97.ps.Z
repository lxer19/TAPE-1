URL: http://www.cs.umn.edu/Research/Agassiz/Paper/tsai.lcpc97.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Email: j-tsai1@uiuc.edu fzjiang,yewg@cs.umn.edu  
Title: Program Optimization for Concurrent Multithreaded Architectures  
Author: Jenn-Yuan Tsai Zhenzhen Jiang and Pen-Chung Yew 
Address: Urbana, IL 61801 Minneapolis, MN 55455  
Affiliation: Department of Computer Science Department of Computer Science University of Illinois University of Minnesota  
Abstract: This paper presents some compiler and program transformation techniques for concurrent multithreaded architectures, in particular the superthreaded architecture [9], which adopts a thread pipelining execution model that allows threads with data dependences and control dependences to be executed in parallel. In this paper, we identify several important program analysis and transformation techniques that allow the superthreaded architecture to exploit more parallelism in programs with less run-time overhead. We evaluate the performance of the su-perthreaded architecture and the effectiveness of the program transformation techniques by manually compiling several benchmark programs and running them through a trace-driven, cycle-by-cycle superthreaded processor simulator. The simulation results show that a superthreaded processor can achieve promising speedups for most of the benchmark programs with the proposed program transformation techniques applied. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Ding-Kai Chen and Pen-Chung Yew. </author> <title> Statement reordering for doacross loops. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <volume> volume Vol. II, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: It also generates target store instructions to facilitate run-time data dependence checking. The compiler will try to increase the execution overlap of concurrent threads by minimizing the stalls caused by data dependences between threads. To do this, we will perform statement reordering <ref> [1] </ref> to schedule target stores as early as possible, and to schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 3.2 Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar [4, 8] and SPSM [2],
Reference: 2. <author> Pradeep K. Dubey, Kevin O'Brien, Kathryn O'Brien, and Charles Barton. </author> <title> Single-program speculative multithreading (SPSM) architecture: Compiler-assisted fine-grained multithreading. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT '95, </booktitle> <pages> pages 109-121, </pages> <month> June 27-29, </month> <year> 1995. </year>
Reference-contexts: Therefore, it is very difficult to build a single-threaded microprocessor with an issue rate greater than ten instructions per cycle. Recently, a number of concurrent multithreaded execution models <ref> [5, 8, 10, 3, 2, 9] </ref> have been proposed as an alternative to the single-threaded execution model for the future generations of microprocessors. <p> In addition, the concurrent multithreaded processors are targeted for general-purpose programs, which are mostly written in C and usually have complicated control flows and implicit data dependences that cannot be analyzed by the compiler. Most of the proposed multithreaded architectural models provide some hardware support for thread-level speculation <ref> [5, 8, 2, 9] </ref> and run-time data dependence checking between threads. [8, 2, 9]. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. <p> Most of the proposed multithreaded architectural models provide some hardware support for thread-level speculation [5, 8, 2, 9] and run-time data dependence checking between threads. <ref> [8, 2, 9] </ref>. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. For numerical programs written in Fortran, we can leverage existing powerful parallelizing compilers which are originally developed for multiprocessors to extract loop-level parallelism and generate multiple-threaded code. <p> [1] to schedule target stores as early as possible, and to schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 3.2 Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar [4, 8] and SPSM <ref> [2] </ref>, provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time.
Reference: 3. <author> Marco Fillo, Stephen W. Keckler, William J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The m-machine multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146-156, </pages> <month> November 29-December 1, </month> <year> 1995. </year>
Reference-contexts: Therefore, it is very difficult to build a single-threaded microprocessor with an issue rate greater than ten instructions per cycle. Recently, a number of concurrent multithreaded execution models <ref> [5, 8, 10, 3, 2, 9] </ref> have been proposed as an alternative to the single-threaded execution model for the future generations of microprocessors.
Reference: 4. <author> Manoj Franklin and Gurindar S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grained parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 58-67, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: will perform statement reordering [1] to schedule target stores as early as possible, and to schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 3.2 Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar <ref> [4, 8] </ref> and SPSM [2], provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time.
Reference: 5. <author> Hiroaki Hirata, Kozo Kimura, Satoshi Nagamine, Yoshiyuki Mochizuki, Akio Nishimura, Yoshimori Nakase, and Teiji Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: Therefore, it is very difficult to build a single-threaded microprocessor with an issue rate greater than ten instructions per cycle. Recently, a number of concurrent multithreaded execution models <ref> [5, 8, 10, 3, 2, 9] </ref> have been proposed as an alternative to the single-threaded execution model for the future generations of microprocessors. <p> In addition, the concurrent multithreaded processors are targeted for general-purpose programs, which are mostly written in C and usually have complicated control flows and implicit data dependences that cannot be analyzed by the compiler. Most of the proposed multithreaded architectural models provide some hardware support for thread-level speculation <ref> [5, 8, 2, 9] </ref> and run-time data dependence checking between threads. [8, 2, 9]. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs.
Reference: 6. <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the 6th ACM International Conference on Supercomputing, </booktitle> <pages> pages 313-322, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The compiler need not generate target store instructions for the update of the private variables because they will not cause read-after-write data dependence across threads. For this purpose, 8 the compiler can perform analysis (on both scalars and arrays) to identify privatizable variables <ref> [11, 6] </ref>. Loop unrolling/interchange: Loop unrolling and loop interchange are common techniques used in parallelizing compilers to increase the granularity of parallel loops.
Reference: 7. <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical report, </type> <institution> Stanford University, Stanford, </institution> <address> California 94305, </address> <month> November </month> <year> 1991. </year> <note> Technical Report CSL-TR-91-497. </note>
Reference-contexts: In the transformed programs, special superthreading instructions, such as fork and store ts, are represented as function calls to specific subroutines, so that they 11 could be recognized by the simulator. The transformed programs are compiled by the SGI C compiler. The programs are then instrumented by pixie <ref> [7] </ref> to generate instruction and memory reference traces. The simulator executes the instrumented programs on the host SGI machine and collects the traces. During the trace collection phase, the function calls which represent the superthreading instructions are converted into the corresponding superthreading instructions for simulation.
Reference: 8. <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Therefore, it is very difficult to build a single-threaded microprocessor with an issue rate greater than ten instructions per cycle. Recently, a number of concurrent multithreaded execution models <ref> [5, 8, 10, 3, 2, 9] </ref> have been proposed as an alternative to the single-threaded execution model for the future generations of microprocessors. <p> In addition, the concurrent multithreaded processors are targeted for general-purpose programs, which are mostly written in C and usually have complicated control flows and implicit data dependences that cannot be analyzed by the compiler. Most of the proposed multithreaded architectural models provide some hardware support for thread-level speculation <ref> [5, 8, 2, 9] </ref> and run-time data dependence checking between threads. [8, 2, 9]. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. <p> Most of the proposed multithreaded architectural models provide some hardware support for thread-level speculation [5, 8, 2, 9] and run-time data dependence checking between threads. <ref> [8, 2, 9] </ref>. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. For numerical programs written in Fortran, we can leverage existing powerful parallelizing compilers which are originally developed for multiprocessors to extract loop-level parallelism and generate multiple-threaded code. <p> will perform statement reordering [1] to schedule target stores as early as possible, and to schedule load instructions that may be data dependent on the target stores of some predecessor threads as late as possible. 3.2 Conversion of Data Speculation to Control Speculation Some concurrent multiple-threaded architectures, such as Multiscalar <ref> [4, 8] </ref> and SPSM [2], provide hardware support for data speculation. With such hardware support, the compiler can speculate on potential data dependences between threads by assuming that they would not be violated, and rely on the hardware to detect violations at run-time.
Reference: 9. <author> Jenn-Yuan Tsai and Pen-Chung Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of the 1996 Conference on Parallel Architectures and Compilation Techniques, PACT '96, </booktitle> <pages> pages 35-46, </pages> <month> October 20-23, </month> <year> 1996. </year>
Reference-contexts: Therefore, it is very difficult to build a single-threaded microprocessor with an issue rate greater than ten instructions per cycle. Recently, a number of concurrent multithreaded execution models <ref> [5, 8, 10, 3, 2, 9] </ref> have been proposed as an alternative to the single-threaded execution model for the future generations of microprocessors. <p> In addition, the concurrent multithreaded processors are targeted for general-purpose programs, which are mostly written in C and usually have complicated control flows and implicit data dependences that cannot be analyzed by the compiler. Most of the proposed multithreaded architectural models provide some hardware support for thread-level speculation <ref> [5, 8, 2, 9] </ref> and run-time data dependence checking between threads. [8, 2, 9]. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. <p> Most of the proposed multithreaded architectural models provide some hardware support for thread-level speculation [5, 8, 2, 9] and run-time data dependence checking between threads. <ref> [8, 2, 9] </ref>. To take advantage of multiple threads of control, the compiler needs to generated multiple-threaded codes from source programs. For numerical programs written in Fortran, we can leverage existing powerful parallelizing compilers which are originally developed for multiprocessors to extract loop-level parallelism and generate multiple-threaded code. <p> Therefore, new compiler techniques are needed to take advantages of such hardware support. Those compiler techniques are more aggressive than those in conventional parallelizing compilers. In this study, we use a concurrent multithreaded architecture, called the Su-perthreaded architecture, proposed in <ref> [9] </ref>. The superthreaded architecture uses a thread pipelining execution model to enhance overlapping between threads and to facilitate data dependence enforcement between threads through hardware supported thread-level control speculation and run-time data dependence checking.
Reference: 10. <author> Dean M. Tullsen, Susan J. Eggers, and Henry M. Levy. </author> <title> Simultaneous multithread-ing: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Therefore, it is very difficult to build a single-threaded microprocessor with an issue rate greater than ten instructions per cycle. Recently, a number of concurrent multithreaded execution models <ref> [5, 8, 10, 3, 2, 9] </ref> have been proposed as an alternative to the single-threaded execution model for the future generations of microprocessors.
Reference: 11. <author> Michael J. Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <type> Technical report, </type> <institution> University of Illinois, </institution> <month> October </month> <year> 1982. </year> <title> Technical Report UIUCDCS-R-82-1105. This article was processed using the L A T E X macro package with LLNCS style 17 </title>
Reference-contexts: The compiler need not generate target store instructions for the update of the private variables because they will not cause read-after-write data dependence across threads. For this purpose, 8 the compiler can perform analysis (on both scalars and arrays) to identify privatizable variables <ref> [11, 6] </ref>. Loop unrolling/interchange: Loop unrolling and loop interchange are common techniques used in parallelizing compilers to increase the granularity of parallel loops.
References-found: 11

