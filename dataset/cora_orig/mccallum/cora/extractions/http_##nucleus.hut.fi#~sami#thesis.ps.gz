URL: http://nucleus.hut.fi/~sami/thesis.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00276.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Data Exploration Using Self-Organizing Maps  
Author: SAMUEL KASKI 
Degree: Thesis for the degree of Doctor of Technology to be presented with due permission for  
Date: 1997  
Address: Rakentajanaukio 2C FIN-02150 Espoo, Finland  ESPOO  
Affiliation: Helsinki University of Technology Neural Networks Research Centre  public  Helsinki University of Technology Department of Computer Science and Engineering Laboratory of Computer and Information Science  
Note: Ma 82 UDC 681.327.12:159.95:519.2 ACTA POLYTECHNICA SCANDINAVICA MATHEMATICS, COMPUTING AND MANAGEMENT IN ENGINEERING SERIES No. 82  examination and criticism in Auditorium F1 of the Helsinki University of Technology on the 21st of March, at 12 o'clock noon.  
Abstract-found: 0
Intro-found: 1
Reference: <author> Alander, J. T., Frisk, M., Holmstrm, L., Hmlinen, A., and Tuominen, J. </author> <title> (1991) Process error detection using self-organizing feature maps. </title> <editor> In Koho-nen, T., Mkisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artitcial Neural Networks. Proceedings of ICANN'91, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 12291232. </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Amari, S. </author> <title> (1980) Topographic organization of nerve telds. </title> <journal> Bulletin of Mathematical Biology, 42:339364. </journal>
Reference: <author> Anderberg, M. R. </author> <title> (1973) Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Andrews, D. F. </author> <title> (1972) Plots of high-dimensional data. Biometrics, </title> <publisher> 28:125136. </publisher>
Reference-contexts: For example the lengths of rays emanating from the center of the icon may visualize the values of the rest of the components (Fig. 1 b). Also the familiar pie diagrams can be used. Andrews' curves <ref> (Andrews, 1972) </ref>, one curve for each data item, are obtained by using the components of the data vectors as coecients of orthogonal sinusoids, which are then added together pointwise (Fig. 1 c). Cherno's faces (Cherno, 1973) are among the most famous visual displays.
Reference: <author> Angniol, B., de La Croix Vaubois, G., and Le Texier, J.-Y. </author> <year> (1988). </year> <title> Self-organizing feature maps and the traveling salesman problem. Neural Networks, </title> <publisher> 1:289293. </publisher>
Reference: <author> Back, B., Sere, K., and Vanharanta, H. </author> <title> (1996) Data mining accounting numbers using self-organizing maps. </title> <editor> In Alander, J., Honkela, T., and Jakobsson, M., editors, </editor> <booktitle> Proceedings of STeP'96, Finnish Artitcial Intelligence Conference, pages 3547. Finnish Artitcial Intelligence Society, </booktitle> <address> Vaasa, Finland. </address>
Reference: <author> Bellman, R. E. </author> <title> (1961) Adaptive Control Processes: A Guided Tour. </title> <publisher> Princeton University Press, </publisher> <address> New Jersey, NJ. </address>
Reference-contexts: Discontinuities in the mapping could then indicate the presence of such folds. This motivation may be misleading for high-dimensional data spaces which are necessarily sparse because of the curse of dimensionality <ref> (cf. Bellman, 1961) </ref>. There may be too few samples for distinguishing between a non-linear curve and a sample from a higher-dimensional distribution, for instance.
Reference: <author> Bezdek, J. C. and Pal, N. R. </author> <title> (1995) An index of topological preservation for feature extraction. Pattern Recognition, </title> <publisher> 28:381391. </publisher>
Reference: <author> Bishop, C. M., Svensn, M., and Williams, C. K. I. </author> <year> (1996a). </year> <title> EM optimization of latent-variable models. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 465471. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Bishop, C. M., Svensn, M., and Williams, C. K. I. </author> <year> (1996b). </year> <title> GTM: a principled alternative to the self-organizing map. </title> <editor> In von der Malsburg, C., von Seelen, W., Vorbrggen, J. C., and Sendho, B., editors, </editor> <booktitle> Proceedings of ICANN'96, International Conference on Artitcial Neural Networks, Lecture Notes in Computer Science, </booktitle> <volume> vol. 1112, </volume> <pages> pages 165170. </pages> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: The GTM is claimed to have several advantageous properties when compared with the SOM, and no signitcant disadvantages <ref> (Bishop et al., 1996b) </ref>. It may be useful to study these properties more closely to get a deeper understanding of the relations and relative merits of the methods. The GTM has an objective function, which may facilitate theoretical analyses of the method. <p> It remains to be seen how important the advantages of the GTM are in practical applications. In addition to the advantages there is one undisputed disadvantage, however: the computational complexity. Computation of the GTM requires almost twice the time of the SOM <ref> (Bishop et al., 1996b) </ref>. Moreover, this dierence is probably enhanced if the speedup methods discussed in this section are introduced, since it may be dicult to apply similar speedups to GTM.
Reference: <author> Biswas, G., Jain, A. K., and Dubes, R. C. </author> <title> (1981) Evaluation of projection algorithms. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <note> 3:701708. 46 Blackmore, </note> <author> J. and Miikkulainen, R. </author> <title> (1993) Incremental grid growing: encoding high-dimensional structure into a two-dimensional feature map. </title> <booktitle> In Proceedings of ICNN'93, IEEE International Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 450455. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: The triangulation can be computed quickly, compared to the MDS methods, but since it only tries to preserve a small fraction of the distances the projection may be dicult to interpret for large data sets. The method may, however, be useful in connection with Sammon's mapping <ref> (Biswas et al., 1981) </ref>. The dimensionality of data sets can also be reduced with the aid of autoas-sociative neural networks that represent their inputs using a smaller number of variables than there are dimensions in the input data.
Reference: <author> Blayo, F. and Demartines, P. </author> <title> (1992) Algorithme de Kohonen: </title> <institution> application l'analyse de donnes conomiques. Bulletin des Schweizerischen Elektrotechnischen Vereins & des Verbandes Schweizerischer Elektrizittswerke, 83(5):2326. </institution>
Reference: <author> Bruske, J. and Sommer, G. </author> <title> (1995) Dynamic cell structure learns perfectly topology preserving map. Neural Computation, </title> <publisher> 7:845865. </publisher>
Reference: <author> Budinich, M. </author> <year> (1996). </year> <title> A self-organizing neural network for the traveling salesman problem that is competitive with simulated annealing. Neural Computation, </title> <publisher> 8:416424. </publisher>
Reference: <author> Carlson, E. </author> <title> (1991) Self-organizing feature maps for appraisal of land value of shore parcels. </title> <editor> In Kohonen, T., Mkisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artitcial Neural Networks. Proceedings of ICANN'91, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 13091312, </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Chang, C. L. and Lee, R. C. T. </author> <title> (1973) A heuristic relaxation method for nonlinear mapping in cluster analysis. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, 3:197200. </journal>
Reference-contexts: It is also possible to use faster versions of the algorithm, discussed in Section 2.4.4, or a parallel implementation. 33 In Publications 3, 4, and 5 a massively parallel CNAPS neurocomputer was used to perform part of the SOM computations. It has been proposed <ref> (Chang and Lee, 1973) </ref> that the computational complexity of Sammon's mapping could be reduced by applying it only to a representative subset of the total data set.
Reference: <author> Cheng, G., Liu, X., and Wu, J. X. </author> <title> (1994) Interactive knowledge discovery through self-organizing feature maps. </title> <booktitle> In Proceedings of WCNN'94, World Congress on Neural Networks, volume IV, </booktitle> <pages> pages 430434. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hills-dale, NJ. </address>
Reference: <author> Cherno, H. </author> <title> (1973) The use of faces to represent points in k-dimensional space graphically. </title> <journal> Journal of the American Statistical Association, 68:361368. </journal>
Reference-contexts: Also the familiar pie diagrams can be used. Andrews' curves (Andrews, 1972), one curve for each data item, are obtained by using the components of the data vectors as coecients of orthogonal sinusoids, which are then added together pointwise (Fig. 1 c). Cherno's faces <ref> (Cherno, 1973) </ref> are among the most famous visual displays. Each dimension of the data determines the size, location, or shape of some component of a facial caricature (Fig. 1 d). For example, one component is associated with the width of the mouth, another with the separation of the eyes, etc.
Reference: <author> Cichocki, A. and Unbehauen, R. </author> <title> (1993) Neural Networks for Optimization and Signal Processing. </title> <publisher> John Wiley, </publisher> <address> Chichester, England. </address>
Reference: <author> Cooley, W. W. and Lohnes, P. R. </author> <title> (1971) Multivariate Data Analysis. </title> <publisher> Wiley, </publisher> <address> New York, NY. </address> <publisher> de Leeuw, </publisher> <editor> J. and Heiser, W. </editor> <title> (1982) Theory of multidimensional scaling. </title> <editor> In Krishnaiah, P. R. and Kanal, L. N., editors, </editor> <booktitle> Handbook of Statistics, </booktitle> <volume> volume 2, </volume> <pages> pages 285316. </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Demartines, P. </author> <title> (1994) Analyse de donnes par rseaux de neurones auto-organiss (Data analysis through self-organized neural networks). </title> <type> PhD thesis, </type> <institution> Institut National Polytechnique de Grenoble, Grenoble, France. </institution> <note> 47 Demartines, </note> <author> P. and Hrault, J. </author> <year> (1997). </year> <title> Curvilinear component analysis: a self-organizing neural network for nonlinear mapping of data sets. </title> <journal> IEEE Transactions on Neural Networks, 8:148154. </journal>
Reference: <author> DeMers, D. and Cottrell, G. </author> <title> (1993) Non-linear dimension reduction. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 580587, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <title> (1977) Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, 39:138. </journal>
Reference-contexts: Missing data. A frequently occurring problem in applying methods of statistics is that of missing data. Some of the components of the data vectors are not available for all data items, or may not even be applicable or detned. Several simple (e.g., Dixon, 1979) and more complex <ref> (e.g., Dempster et al., 1977) </ref> approaches have been proposed for tackling this problem, from which all of the clustering and projection methods suer likewise. <p> Given an input data set, the goal of the GTM is then to tt the model, in the maximum likelihood sense, to the data set. This is done using the expectation-maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref>, by iteratively estimating trst the probability distribution on the latent grid and then maximizing the likelihood of the input data, given the distribution. The GTM is claimed to have several advantageous properties when compared with the SOM, and no signitcant disadvantages (Bishop et al., 1996b).
Reference: <author> Devijver, P. A. and Kittler, J. </author> <title> (1982) Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Clis, NJ. </address>
Reference: <author> Didday, R. L. </author> <title> (1970) The Simulation and Modeling of Distributed Information Processing in the Frog Visual System. </title> <type> PhD thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Didday, R. L. </author> <title> (1976) A model of visuomotor mechanisms in the frog optic tectum. </title> <publisher> Mathematical Biosciences, 30:169180. </publisher>
Reference: <author> Dixon, J. K. </author> <title> (1979) Pattern recognition with partly missing data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <note> 9:617621. </note> <author> du Toit, S. H. C., Steyn, A. G. W., and Stumpf, R. H. </author> <title> (1986) Graphical Exploratory Data Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY. </address>
Reference-contexts: A demonstration of nonmetric MDS, applied in a dimension reduction task, is given in Figure 3. in Figure 2. Missing data values were treated by the following simple method, which has been demonstrated to produce good results at least in the pattern recognition context <ref> (Dixon, 1979) </ref>. When computing the distance between a pair of data items, only the (squared) dierences between component values that are available are computed. The rest of the dierences are then set to the average of the computed dierences. <p> Missing data. A frequently occurring problem in applying methods of statistics is that of missing data. Some of the components of the data vectors are not available for all data items, or may not even be applicable or detned. Several simple <ref> (e.g., Dixon, 1979) </ref> and more complex (e.g., Dempster et al., 1977) approaches have been proposed for tackling this problem, from which all of the clustering and projection methods suer likewise.
Reference: <author> Erwin, E., Obermayer, K., and Schulten, K. </author> <year> (1992). </year> <title> Self-organizing maps: ordering, convergence properties and energy functions. </title> <journal> Biological Cybernetics, 67:4755. </journal>
Reference-contexts: The GTM has an objective function, which may facilitate theoretical analyses of the method. The SOM does not have an objective function if the input data distribution is continuous <ref> (Erwin et al., 1992) </ref>. In practical applications the data sets are always tnite, however, and the SOM does have a (local) objective function, Equation 7. The methods dier in that the GTM does not have a neighborhood function which governs the smoothness of the mapping in the SOM algorithm.
Reference: <author> Fayyad, U. M. </author> <year> (1996). </year> <title> Data mining and knowledge discovery: making sense out of data. </title> <journal> IEEE Expert, </journal> <month> October, pages </month> <year> 2025. </year>
Reference: <author> Fayyad, U., Piatetsky-Shapiro, G., and Smyth, P. </author> <title> (1996a) Knowledge discovery and data mining: towards a unifying framework. </title> <editor> In Simoudis, E., Han, J., and Fayyad, U., editors, </editor> <booktitle> Proceedings of KDD'96, Second International Conference on Knowledge Discovery & Data Mining, </booktitle> <pages> pages 8288. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address>
Reference: <author> Fayyad, U. M., Piatetsky-Shapiro, G., and Smyth, P., </author> <title> editors (1996b) Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI Press / MIT Press, </publisher> <address> Menlo Park, CA. </address>
Reference: <author> Fayyad, U. M., Piatetsky-Shapiro, G., and Smyth, P. </author> <title> (1996c) From data mining to knowledge discovery: an overview. </title> <editor> In Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., and Uthurusamy, R., editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 134. </pages> <publisher> AAAI Press / MIT Press, </publisher> <address> Menlo Park, CA. </address> <note> 48 Flexer, </note> <author> A. </author> <title> (1997) Limitations of self-organizing maps for vector quantization and multidimensional scaling. </title> <note> To appear in Mozer, </note> <editor> M. C., Jordan, M. I., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle>
Reference: <author> Forsyth, R., </author> <title> editor (1989) Machine Learning: Principles and Techniques. </title> <publisher> Chap-man and Hall, London. </publisher>
Reference: <author> Friedman, J. H. </author> <title> (1987) Exploratory projection pursuit. </title> <journal> Journal of the American Statistical Association, 82:249266. </journal>
Reference: <author> Friedman, J. H. and Tukey, J. W. </author> <title> (1974) A projection pursuit algorithm for exploratory data analysis. </title> <journal> IEEE Transactions on Computers, 23:881890. </journal>
Reference: <author> Fritzke, B. </author> <title> (1991) Let it grow - self-organizing feature maps with problem dependent cell structure. </title> <editor> In Kohonen, T., Mkisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artitcial Neural Networks. Proceedings of ICANN'91, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 403408, </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Fritzke, B. </author> <title> (1994) Growing cell structuresa self-organizing network for unsupervised and supervised learning. Neural Networks, </title> <publisher> 7:14411460. </publisher>
Reference: <author> Fu, K. S. </author> <title> (1974) Syntactic Methods in Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Fukunaga, K. </author> <title> (1972) Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Fyfe, C. and Baddeley, R. </author> <title> (1995) Non-linear data structure extraction using simple Hebbian networks. </title> <journal> Biological Cybernetics, 72:533541. </journal>
Reference-contexts: The detnition of interestingness is based on how much the projected data deviates from normally distributed data in the main body of its distribution. There is also a neural implementation of this idea <ref> (Fyfe and Baddeley, 1995) </ref>. After an interesting projection has been found, the structure that makes the projection interesting may be removed from the data, after which the procedure 2 Ripley (1996) divides statistical data-analysis methods into clustering methods, projection methods, and multidimensional scaling (MDS) methods.
Reference: <author> Gallant, S. I. </author> <title> (1994) Methods for generating or revising context vectors for a plurality of word stems. </title> <type> U.S. Patent number 5,325,298. </type>
Reference: <author> Gallant, S. I., Caid, W. R., Carleton, J., Hecht-Nielsen, R., Pu Qing, K., and Sud-beck, D. </author> <booktitle> (1992) HNC's MatchPlus system. ACM SIGIR Forum, </booktitle> <address> 26(2):3438. </address>
Reference: <author> Garavaglia, S. </author> <title> (1993) A self-organizing map applied to macro and micro analysis of data with dummy variables. </title> <booktitle> In Proceedings of WCNN'93, World Congress on Neural Networks, </booktitle> <pages> pages 362368. </pages> <publisher> Lawrence Erlbaum and INNS Press, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Garrido, L., Gaitan, V., Serra-Ricart, M., and Calbert, X. </author> <title> (1995) Use of multilayer feedforward neural nets as a display method for multidimensional distributions. </title> <journal> International Journal of Neural Systems, 6:273282. </journal>
Reference: <author> Gersho, A. </author> <title> (1979) Asymptotically optimal block quantization. </title> <journal> IEEE Transactions on Information Theory, </journal> <note> 25:373380. 49 Goodhill, </note> <author> G. J., Finch, S., and Sejnowski, T. J. </author> <title> (1995) Quantifying neighborhood preservation in topographic mappings. </title> <type> Technical Report INC-9505, </type> <institution> Institute for Neural Computation, La Jolla, </institution> <address> CA. </address>
Reference: <author> Gray, R. M. </author> <title> (1984) Vector quantization. </title> <journal> IEEE ASSP Magazine, </journal> <month> April, </month> <pages> pages 429. </pages>
Reference: <author> Grossberg, S. </author> <title> (1976) On the development of feature detectors in the visual cortex with applications to learning and reaction-diusion systems. </title> <journal> Biological Cybernetics, 21:145159. </journal>
Reference: <author> Hair, Jr., J. F., Anderson, R. E., Tatham, R. L., and Black, W. C. </author> <title> (1984) Mul-tivariate Data Analysis with Readings (4th edition 1995). </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Clis, NJ. </address>
Reference: <author> Hmlinen, A. </author> <title> (1994) A measure of disorder for the self-organizing map. </title> <booktitle> In Proceedings of ICNN'94, IEEE International Conference on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 659664. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ. </address>
Reference: <author> Hartigan, J. </author> <title> (1975) Clustering Algorithms. </title> <publisher> Wiley, </publisher> <address> New York, NY. </address>
Reference: <author> Hastie, T. and Stuetzle, W. </author> <title> (1989) Principal curves. </title> <journal> Journal of the American Statistical Association, 84:502516. </journal>
Reference-contexts: Principal curves. PCA can be generalized to form nonlinear curves. While in PCA a good projection of a data set onto a linear manifold was constructed, the goal in constructing a principal curve is to project the set onto a nonlinear manifold. The principal curves <ref> (Hastie and Stuetzle, 1989) </ref> are smooth curves 19 that are detned by the property that each point of the curve is the average of all data points that project to it, i.e., for which that point is the closest point on the curve. <p> The SOM algorithm creates a representation of the input data set that follows the data distribution. The representation of the data set is also organized. One possible view (cf. Ritter et al., 1992) to the organization has been provided by the mathematical characterization of principal curves <ref> (Hastie and Stuetzle, 1989) </ref>. 26 Each point on a principal curve is the average of all the points that project to it. The curve is thus formed of conditional expectations of the data. <p> The principal curves are the critical points of this measure, i.e., they are extremal with respect to small, smooth variations <ref> (Hastie and Stuetzle, 1989) </ref>. This implies that any smooth curve which corresponds to a minimum of the average distance from the data items is a principal curve. A decomposition of the cost function.
Reference: <author> Haykin, S. </author> <title> (1994) Neural Networks. A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY. </address>
Reference-contexts: The learning of the multilayer perceptrons with the backpropagation algorithm (cf., e.g., Rumelhart et al., 1986) is known to be very slow <ref> (Haykin, 1994) </ref>, but it is possible that some alternative learning algorithms would be more feasible. 2.4 Self-organizing maps The self-organizing map (SOM) (Kohonen, 1982; Kohonen, 1990; Kohonen, 1995c; Kohonen et al., 1996b) is a neural network algorithm that has been used for a wide variety of applications, mostly for engineering problems
Reference: <author> Hecht-Nielsen, R. </author> <title> (1995) Replicator neural networks for universal optimal source coding. </title> <publisher> Science, 269:18601863. </publisher>
Reference: <author> Hoaglin, D. C. </author> <title> (1982) Exploratory data analysis. </title> <editor> In Kotz, S., Johnson, N. L., and Read, C. B., editors, </editor> <booktitle> Encyclopedia of Statistical Sciences, </booktitle> <volume> volume 2, </volume> <pages> pages 579583. </pages> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Honkela, T., Kaski, S., Lagus, K., and Kohonen, T. </author> <title> (1996) Exploration of full-text databases with self-organizing maps. </title> <booktitle> In Proceedings of ICNN'96, IEEE International Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 5661. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: The basic method is described in Publication 3; experiments with very large maps and document collections are in Publication 4; and the browsing interface and exploration examples are in Publication 5. A partly supervised version of the method has also been constructed <ref> (Honkela et al., 1996) </ref>. The maps that have been presented in the publications are available for exploration in the Internet at the address http://websom.hut.fi/websom/. The advantages gained by using such a SOM-based feature extraction stage in WEBSOM are analyzed in more detail in Publication 6.
Reference: <author> Hotelling, H. </author> <title> (1933) Analysis of a complex of statistical variables into principal components. </title> <journal> Journal of Educational Psychology, 24:417441,498520. </journal>
Reference-contexts: The projection can be used to visualize the data set if a suciently small output dimensionality is chosen. 2.3.1 Linear projection methods Principal component analysis (PCA) <ref> (Hotelling, 1933) </ref> can be used to display the data as a linear projection on such a subspace of the original data space that best preserves the variance in the data. It is a standard method in data analysis; it is well understood, and eective algorithms exist for computing the projection.
Reference: <author> Iivarinen, J., Kohonen, T., Kangas, J., and Kaski, S. </author> <title> (1994) Visualizing the clusters on the self-organizing map. </title> <editor> In Carlsson, C., Jrvi, T., and Reponen, T., editors, </editor> <booktitle> Proceedings of the Conference on Artitcial Intelligence Research in Finland, pages 122126. Finnish Artitcial Intelligence Society, </booktitle> <address> Helsinki, Finland. </address> <note> 50 Jain, </note> <author> A. K. and Dubes, R. C. </author> <title> (1988) Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Clis, NJ. </address>
Reference-contexts: Thus, the cluster structure in the data set can be brought visible by displaying the distances between reference vectors of neighboring units (Kraaijveld et al., 1992; Kraaijveld et al., 1995; Ultsch, 1993b; Ultsch and Siemon, 1990). The cluster display may be constructed as follows <ref> (Iivarinen et al., 1994) </ref>. The distance between each pair of reference vectors is computed and scaled so that the distances tt between a given minimum and maximum value, after optionally removing outliers.
Reference: <author> Jardine, N. and Sibson, R. </author> <title> (1971) Mathematical Taxonomy. </title> <publisher> Wiley, London. </publisher>
Reference: <author> Kangas, J. </author> <title> (1994) On the analysis of pattern sequences by self-organizing maps. </title> <type> PhD thesis, </type> <institution> Helsinki University of Technology, Espoo, Finland. </institution>
Reference: <author> Kaski, S. and Kohonen, T. </author> <title> (1994) Winner-take-all networks for physiological models of competitive learning. Neural Networks, </title> <publisher> 7:973984. </publisher>
Reference: <author> Kaski, S. and Kohonen, T. </author> <title> (1995) Structures of welfare and poverty in the world discovered by the self-organizing map. </title> <type> Technical Report A24, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland. </institution>
Reference: <author> Kasslin, M., Kangas, J., and Simula, O. </author> <title> (1992) Process state monitoring using self-organizing maps. </title> <editor> In Aleksander, I. and Taylor, J., editors, </editor> <booktitle> Artitcial Neural Networks, 2. Proceedings of ICANN'92, International Conference on Artitcial Neural Networks, </booktitle> <pages> pages 15311534, </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Kendall, M. </author> <title> (1975) Multivariate Analysis. </title> <address> Charles Grin & Company, London. </address>
Reference: <author> Kohonen, T. </author> <title> (1981) Construction of similarity diagrams for phonemes by a self-organizing algorithm. </title> <type> Report TKK-F-A463, </type> <institution> Helsinki University of Technology, Espoo, Finland. </institution>
Reference: <author> Kohonen, T. </author> <title> (1982) Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, 43:5969. </journal>
Reference: <author> Kohonen, T. </author> <title> (1984) Self-Organization and Associative Memory. </title> <booktitle> (3rd edition 1989). </booktitle> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Kohonen, T. </author> <title> (1990) The Self-Organizing Map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 78:1464 1480. </address>
Reference: <author> Kohonen, T. </author> <title> (1991) Self-organizing maps: optimization approaches. </title> <editor> In Koho-nen, T., Mkisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artitcial Neural Networks. Proceedings of ICANN'91, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 981990, </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Kohonen, T. </author> <title> (1993) Physiological interpretation of the self-organizing map algorithm. Neural Networks, </title> <publisher> 6:895905. </publisher>
Reference: <author> Kohonen, T. </author> <title> (1995a) The adaptive-subspace SOM (ASSOM) and its use for the implementation of invariant feature detection. </title> <editor> In Fogelman-Souli, F. and Gallinari, P., editors, </editor> <booktitle> Proceedings of ICANN'95, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 310. </pages> <note> EC2 & Cie, Paris. 51 Kohonen, </note> <author> T. </author> <title> (1995b) Emergence of invariant-feature detectors in self-organization. </title> <editor> In Palaniswami, M., Attikiouzel, Y., Marks II, R. J., Fogel, D., and Fukuda, T., editors, </editor> <booktitle> Computational intelligence. A dynamic system perspective, </booktitle> <pages> pages 1731. </pages> <publisher> IEEE Press, </publisher> <address> New York, NY. </address>
Reference: <author> Kohonen, T. </author> <title> (1995c) Self-Organizing Maps. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: Some linear and nonlinear associative memories have been introduced by Kohonen (1984). The representations formed into the hidden layer 3 The algorithm Hastie and Stuetzle (1989) proposed for tnding discretized principal curves resembles the batch version <ref> (Kohonen, 1995c) </ref> of the SOM algorithm, although the details are dierent. 20 of a multilayer perceptron have also been used for the dimension reduction task (DeMers and Cottrell, 1993; Garrido et al., 1995). <p> In the SOM a similar smearing is performed by the neighborhood kernel in the adaptation process (cf. Eq. 6). It is well-known in SOM literature that in the SOM each reference vector represents local conditional expectations of the data items the batch map algorithm <ref> (Kohonen, 1995c) </ref> is essentially a manifestation of this idea. The principal curves or manifolds are thus essentially continuous counterparts of the SOM. The principal curves also have another characterization which, based on the previous discussion, may be used as one source in providing an intuitive understanding of the SOM algorithm.
Reference: <author> Kohonen, T. </author> <title> (1996) Emergence of invariant-feature detectors in the adaptive-subspace self-organizing map. </title> <journal> Biological Cybernetics, 75:281291. </journal>
Reference: <author> Kohonen, T., Hynninen, J., Kangas, J., and Laaksonen, J. </author> <year> (1996a) </year> <month> SOM_PAK: </month> <title> the self-organizing map program package. </title> <type> Technical Report A31, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science, Espoo, Finland. </institution>
Reference-contexts: Even the best analysis methods cannot overcome all mistakes made at this stage. 3.2 Computation of the maps Detailed guidelines for how to actually compute the maps are given by Kohonen (1995c), as well as in the documentation of the public domain program package SOM_PAK <ref> (Kohonen et al., 1996a) </ref>. The reference vectors are trst initialized to lie in an ordered contguration on the plane spanned by the two principal eigenvectors of the data, and thereafter taught in a two-phase process.
Reference: <author> Kohonen, T., Oja, E., Simula, O., Visa, A., and Kangas, J. </author> <year> (1996b). </year> <title> Engineering applications of the self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 84:1358 1384. </address>
Reference: <author> Koikkalainen, P. </author> <title> (1994) Progress with the tree-structured self-organizing map. </title> <editor> In Cohn, A. G., editor, </editor> <booktitle> Proceedings of ECAI'94, 11th European Conference on Artitcial Intelligence, </booktitle> <pages> pages 211215, </pages> <publisher> Wiley, </publisher> <address> Chichester, England. </address>
Reference: <author> Koikkalainen, P. </author> <title> (1995) Fast deterministic self-organizing maps. </title> <editor> In Fogelman-Souli, F. and Gallinari, P., editors, </editor> <booktitle> Proceedings of ICANN'95, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 6368, </pages> <address> EC2 & Cie, Paris. </address>
Reference: <author> Koikkalainen, P. and Oja, E. </author> <title> (1990) Self-organizing hierarchical feature maps. </title> <booktitle> In Proceedings of IJCNN'90 (San Diego), International Joint Conference on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 279284, </pages> <publisher> IEEE Service Center, </publisher> <address> Piscat-away, NJ. </address>
Reference: <author> Kraaijveld, M. A., Mao, J., and Jain, A. K. </author> <title> (1992) A non-linear projection method based on Kohonen's topology preserving maps. </title> <booktitle> In Proceedings of 11ICPR, 11th International Conference on Pattern Recognition, </booktitle> <pages> pages 4145, </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference: <author> Kraaijveld, M. A., Mao, J., and Jain, A. K. </author> <title> (1995) A nonlinear projection method based on Kohonen's topology preserving maps. </title> <journal> IEEE Transactions on Neural Networks, 6:548559. </journal>
Reference: <author> Kruskal, J. B. </author> <title> (1964) Multidimensional scaling by optimizing goodness of tt to a nonmetric hypothesis. </title> <journal> Psychometrika, 29:127. </journal> <volume> 52 Kruskal, </volume> <editor> J. B. and Wish, M. </editor> <title> (1978) Multidimensional Scaling. Sage University Paper series on Quantitative Applications in the Social Sciences, number 07-011. </title> <publisher> Sage Publications, </publisher> <address> Newbury Park, CA. </address>
Reference: <author> Lampinen, J. and Oja, E. </author> <title> (1992) Clustering properties of hierarchical self-organizing maps. </title> <journal> Journal of Mathematical Imaging and Vision, 2:261272. </journal>
Reference: <author> Langley, P. </author> <title> (1996) Elements of Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Lee, R. C. T., Slagle, J. R., and Blum, H. </author> <title> (1977) A triangulation method for the sequential mapping of points from N -space to two-space. </title> <journal> IEEE Transactions on Computers, 26:288292. </journal>
Reference-contexts: The computational complexity can be reduced, however, by restricting attention to a subset of the distances between the data items. When placing a point on a plane its distance from two other points of the plane can be set exactly. This property is used in the triangulation method <ref> (Lee et al., 1977) </ref>. Points are mapped sequentially onto the plane, and the distance of the new item to the two nearest items already mapped is preserved. Alternatively, the distance to the nearest item and a reference point that is common to all items may be preserved.
Reference: <author> Lloyd, S. P. </author> <year> (1957). </year> <title> Least squares quantization in PCM. </title> <type> Unpublished memorandum, </type> <institution> Bell Laboratories. </institution> <note> (Published in IEEE Transactions on Information Theory, 28 129-137, 1982). </note> <author> Lopes da Silva, F. H., Storm van Leeuwen, W., and Rmond, A., </author> <title> editors (1986) Handbook of Electroencephalography and Clinical Neurophysiology. Volume 2: Clinical Applications of Computer Analysis of EEG and other Neurophysiological Signals. </title> <publisher> Elsevier, Amsterdam. </publisher>
Reference: <author> Luttrell, P. S. </author> <year> (1988). </year> <title> Self-organizing multilayer topographic mappings. </title> <booktitle> In Proceedings of ICNN'88, IEEE International Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 93100. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: In Publication 4 we used computational speedups that have been developed by Kohonen; in the other studies the basic SOM algorithm was used. Other speedup methods have also been proposed. For example, a one-dimensional map can be enlarged simply by inserting a reference vector between each pair of neighbors <ref> (Luttrell, 1988) </ref>. The search for the best-matching unit can be speeded up by constructing a tree-structured SOM (Koikkalainen, 1994; Koikkalainen, 1995; Koikkalainen and Oja, 1990), where each level of the tree consists of a separate, progressively larger SOM.
Reference: <author> Luttrell, S. P. </author> <year> (1989). </year> <title> Hierarchical vector quantization. </title> <booktitle> IEE Proceedings, </booktitle> <address> 136:405 413. </address>
Reference-contexts: The outputs of the SOMs in the bottom layer are then combined in a hierarchical manner towards the tnal SOM that takes into account the whole input vector <ref> (Luttrell, 1989) </ref>. Since each of the small SOMs receives only very small-dimensional input vectors, the winning unit can potentially be sought with a single, fast table lookup. 28 Each of the computational speedups could potentially be useful, but the comparison of the methods would require a careful study.
Reference: <author> MacQueen, J. </author> <year> (1967). </year> <title> Some methods for classitcation and analysis of multivariate observations. In Le Cam, </title> <editor> L. M. and Neyman, J., editors, </editor> <booktitle> Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability. Volume I: Statistics, </booktitle> <pages> pages 281297. </pages> <institution> University of California Press, Berkeley and Los Angeles, </institution> <address> CA. </address>
Reference-contexts: Typically the global criteria involve minimizing some measure of dissimilarity in the samples within each cluster, while maximizing the dissimilarity of dierent clusters. A commonly used partitional clustering method, K-means clustering <ref> (MacQueen, 1967) </ref>, will be discussed in some detail since it is closely related to the SOM algorithm.
Reference: <author> Makhoul, J., Roucos, S., and Gish, H. </author> <title> (1985) Vector quantization in speech coding. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 73:15511588. </address>
Reference: <author> Mao, J. and Jain, A. K. </author> <title> (1995) Artitcial neural networks for feature extraction and multivariate data projection. </title> <journal> IEEE Transactions on Neural Networks, 6:296317. </journal>
Reference-contexts: At least in the case of Sammon's mapping this problem may be alleviated somewhat by computing the projection with a multilayer perceptron-type network which learns to minimize the cost function. This can be done by using a learning rule that is analogous to the error backpropagation algorithm <ref> (Mao and Jain, 1995) </ref>. The method does not necessarily reduce the computational complexity of the original algorithm since the backpropagation learning rule is computationally very intensive, but it generalizes the mapping to new samples. <p> This method is not as accurate as the original, however. Besides the computational complexity, the existence of local minima in the cost functions may also cause problems. On the basis of empirical experience Sammon's mapping can easily get stuck at local optima <ref> (cf. Mao and Jain, 1995, and Ripley, 1996) </ref>, and in practice the computation of the projection must be repeated several times starting from dierent initial contgurations.
Reference: <author> Martn-del-Bro, B. and Serrano-Cinca, C. </author> <title> (1993) Self-organizing neural networks for the analysis and representation of data: some tnancial cases. </title> <journal> Neural Computing & Applications, </journal> <note> 1:193206. 53 Martinetz, </note> <author> T. and Schulten, K. </author> <title> (1991) A neural-gas network learns topologies. </title> <editor> In Kohonen, T., Mkisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artitcial Neural Networks. Proceedings of ICANN'91, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 397402. </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Martinetz, T. and Schulten, K. </author> <title> (1994) Topology representing networks. Neural Networks, </title> <publisher> 7:507522. </publisher>
Reference: <author> Marttinen, K. </author> <title> (1993) SOM in statistical analysis: </title> <editor> supermarket customer protl-ing. In Bulsari, A. and Saxn, B., editors, </editor> <booktitle> Proceedings of the Symposium on Neural Network Research in Finland, pages 7580. Finnish Artitcial Intelligence Society, </booktitle> <address> Turku, Finland. </address>
Reference: <author> Michalski, R. S., Carbonell, J., and Mitchell, T., </author> <title> editors (1983) Machine Learning: An Artitcial Intelligence Approach. </title> <publisher> TIOGA Publishing Company, </publisher> <address> Palo Alto, CA. </address>
Reference: <author> Minamimoto, K., Ikeda, K., and Nakayama, K. </author> <title> (1995) Topology analysis of data space using self-organizing map. </title> <booktitle> In Proceedings of ICNN'95, IEEE International Conference on Neural Networks, </booktitle> <pages> pages 789794. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ. </address>
Reference: <author> Mulier, F. and Cherkassky, V. </author> <title> (1995) Self-organization as an iterative kernel smoothing process. Neural Computation, </title> <publisher> 7:11651177. </publisher>
Reference: <author> Murtagh, F. </author> <title> (1995) Interpreting the Kohonen self-organizing feature map using contiguity-constrained clustering. </title> <journal> Pattern Recognition Letters, 16:399408. </journal>
Reference: <author> Nass, M. M. and Cooper, L. N. </author> <title> (1975) A theory for the development of feature detecting cells in visual cortex. </title> <journal> Biological Cybernetics, 19:118. </journal>
Reference: <author> Niedermeyer, E. and Lopes da Silva, F., </author> <title> editors (1987) Electroencephalography: Basic Principles, Clinical Applications and Related Fields. </title> <type> Urban & Schwarzenberg, </type> <institution> Baltimore, </institution> <note> second edition. </note>
Reference: <author> Nunez, P. L. </author> <title> (1981) Electric Fields of the Brain. The Neurophysics of EEG. </title> <publisher> Oxford University Press, </publisher> <address> New York, NY. </address>
Reference: <author> Oja, E. </author> <title> (1983) Subspace Methods of Pattern Recognition. </title> <publisher> Research Studies Press, </publisher> <address> Letchworth, England. </address>
Reference: <author> Oja, E. </author> <title> (1992) Principal components, minor components, and linear neural networks. Neural Networks, </title> <publisher> 5:927935. </publisher>
Reference: <author> Pedrycz, W. and Card, H. C. </author> <title> (1992) Linguistic interpretation of self-organizing maps. </title> <booktitle> In IEEE International Conference on Fuzzy Systems, </booktitle> <pages> pages 371378. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ. </address> <note> 54 Prez, </note> <author> R., Glass, L., and Shlaer, R. J. </author> <title> (1975) Development of specitcity in cat visual cortex. </title> <journal> Journal of Mathematical Biology, 1:275288. </journal>
Reference: <author> Ripley, B. D. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Great Britain. </address>
Reference: <author> Ritter, H. </author> <title> (1991) Asymptotic level density for a class of vector quantization processes. </title> <journal> IEEE Transactions on Neural Networks, 2:173175. </journal>
Reference-contexts: In fact, an expression for the density of the reference vectors of the SOM has been derived in the one-dimensional case <ref> (Ritter, 1991) </ref>; in the limit of a very wide neighborhood and a large number of reference vectors the density is proportional to p (x) 2=3 , where p (x) is the probability density function of the input data.
Reference: <author> Ritter, H. and Kohonen, T. </author> <title> (1989) Self-organizing semantic maps. </title> <journal> Biological Cybernetics, 61:241254. </journal>
Reference-contexts: Similarity relations computed based on the forms of the words would convey almost no information about the meaning and use of the words. Contextual information can, however, be used for constructing useful similarity relationships for textual data <ref> (Ritter and Kohonen, 1989) </ref>. A system that utilizes representations of the average context of words to represent the words, and suitably processed word category histograms to represent documents is discussed in Publications 3, 4, 5, and 6. <p> In the study it was also demonstrated how the missing value problem can be treated. 41 4.3 Full-text document collections A project that aims at constructing methods for exploring full-text document collections, the WEBSOM, started from Timo Honkela's suggestion of using the self-organizing semantic maps <ref> (Ritter and Kohonen, 1989) </ref> as a preprocessing stage for encoding documents. When the documents are organized, using this preprocessing stage, on a map in such a way that nearby locations contain similar documents, exploration of the collection is facilitated by the intuitive neighborhood relations. <p> A simple 42 method is to project them randomly to a space of a lower dimensionality <ref> (Ritter and Kohonen, 1989) </ref>.
Reference: <author> Ritter, H., Martinetz, T., and Schulten, K. </author> <title> (1992) Neural Computation and Self-Organizing Maps: An Introduction. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: The cluster structures will become visible on the special displays that were discussed in Section 2.4.2. Relation to principal curves. The SOM algorithm creates a representation of the input data set that follows the data distribution. The representation of the data set is also organized. One possible view <ref> (cf. Ritter et al., 1992) </ref> to the organization has been provided by the mathematical characterization of principal curves (Hastie and Stuetzle, 1989). 26 Each point on a principal curve is the average of all the points that project to it.
Reference: <author> Ritter, H. and Schulten, K. </author> <year> (1988). </year> <title> Kohonen's self-organizing maps: exploring their computational capabilities. </title> <booktitle> In Proceedings of the ICNN'88, IEEE International Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 109116. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ. </address>
Reference: <author> Rubner, J. and Tavan, P. </author> <title> (1989) A self-organizing network for principal component analysis. </title> <journal> Europhysics Letters, 10:693698. </journal>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <title> (1986) Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., McClelland, J. L., and the PDP Research Group, editors, </editor> <booktitle> Paralled Distributed Processing. Explorations in the Microstructure of Cognition. Volume 1: Foundations, </booktitle> <pages> pages 318362. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Although intriguing, the approach would require a separate study that would compare both the quality of the results and the computational requirements for a network having a practical size. The learning of the multilayer perceptrons with the backpropagation algorithm <ref> (cf., e.g., Rumelhart et al., 1986) </ref> is known to be very slow (Haykin, 1994), but it is possible that some alternative learning algorithms would be more feasible. 2.4 Self-organizing maps The self-organizing map (SOM) (Kohonen, 1982; Kohonen, 1990; Kohonen, 1995c; Kohonen et al., 1996b) is a neural network algorithm that has
Reference: <author> Salton, G. and McGill, M. J. </author> <title> (1983) Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference-contexts: It is a standard practice in information retrieval (IR) <ref> (Salton and McGill, 1983) </ref> to encode documents with vectors, in which each component corresponds to a dierent word, and the value of the component reects the frequency of occurrence of the word in the document.
Reference: <author> Samad, T. and Harp, S. A. </author> <title> (1992) Self-organization with partial data. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <address> 3:205212. </address>
Reference-contexts: When the reference vectors are then adapted using Equation 6, only the components that are available in x will be modited. It has been demonstrated that better results can be obtained with the approach described above than by discarding the data items from which components are missing <ref> (Samad and Harp, 1992) </ref>. However, for data items from which the majority of the indicators are missing it is not justitable to assume that the winner selection is accurate.
Reference: <author> Sammon, Jr., J. W. </author> <title> (1969) A nonlinear mapping for data structure analysis. </title> <journal> IEEE Transactions on Computers, 18:401409. </journal>
Reference-contexts: When computing the distance between a pair of data items, only the (squared) dierences between component values that are available are computed. The rest of the dierences are then set to the average of the computed dierences. Another nonlinear projection method, Sammon's mapping <ref> (Sammon, Jr., 1969) </ref>, is closely related to the metric MDS version described above. It, too, tries to optimize a cost function that describes how well the pairwise distances in a data set 18 are preserved. <p> In his original paper Sammon suggested that clustering could be used as a front-end to his mapping algorithm <ref> (Sammon, Jr., 1969) </ref>. If the SOM is used to perform the clustering, there will be two dierent views to the same data available, which would certainly be useful.
Reference: <author> Schalko, R. J. </author> <title> (1992) Pattern Recognition: Statistical, Structural and Neural Approaches. </title> <publisher> Wiley, </publisher> <address> New York, NY. </address>
Reference: <author> Sedgewick, R. </author> <title> (1988) Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, 2nd edition. </address>
Reference-contexts: Note 3: Computational complexity. The proposed measure is fairly com-putationally intensive. It requires searching for the shortest path between each pair of units on the map, based on the distances between the reference vectors in the data space. The measure can, however, be computed using dynamic programming <ref> (cf., e.g., Sedgewick, 1988) </ref>, which reduces the complexity.
Reference: <author> Serrano-Cinca, C. </author> <title> (1996) Self-organizing neural networks for tnancial diagnosis. </title> <note> To appear in Decision Support Systems. </note>
Reference: <author> Shepard, R. N. </author> <title> (1962) The analysis of proximities: multidimensional scaling with an unknown distance function. </title> <journal> Psychometrika, </journal> <volume> 27:125140; 219246. 55 Simoudis, </volume> <editor> E. </editor> <year> (1996). </year> <title> Reality check for data mining. </title> <journal> IEEE Expert, </journal> <month> October, </month> <pages> pages 2633. </pages>
Reference: <author> Sneath, P. H. A. and Sokal, R. R. </author> <title> (1973) Numerical Taxonomy. </title> <publisher> Freeman, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Swindale, N. W. </author> <title> (1980) A model for the formation of ocular dominance stripes. </title> <journal> Proceedings of the Royal Society of London, B, 208:243264. </journal>
Reference: <author> Therrien, C. W. </author> <title> (1989) Decision, Estimation, and Classitcation. An Introduction to Pattern Recognition and Related Topics. </title> <publisher> Wiley, </publisher> <address> New York, NY. </address>
Reference: <author> Torgerson, W. S. </author> <title> (1952) Multidimensional scaling: I. Theory and method. </title> <publisher> Psy-chometrika, 17:401419. </publisher>
Reference: <author> Tryba, V. and Goser, K. </author> <title> (1991) Self-organizing feature maps for process control in chemistry. </title> <booktitle> In Artitcial Neural Networks. Proceedings of ICANN'91, International Conference on Artitcial Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 847852, </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Tryon, R. C. and Bailey, D. E. </author> <title> (1973) Cluster Analysis. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference: <author> Tukey, J. W. </author> <title> (1977) Exploratory Data Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: The areas of application that are treated in the Publications are introduced in Section 4, and tnally two recent developments in the methodology are discussed in Section 5. 2 METHODS FOR EXPLORATORY DATA ANALYSIS There exist several methods for quickly producing and visualizing simple summaries of data sets <ref> (Tukey, 1977) </ref>. For example, the so-called tve-number summary consisting of the smallest and largest data value, the median, and the trst and third quartiles can be visualized as a drawing, where each number corresponds to some constituent like the altitude of a box. <p> These methods can be used to visualize any kinds of high-dimensional data vectors, either the data items themselves or vectors formed of some descriptors of the data set like the tve-number summaries <ref> (Tukey, 1977) </ref>. Perhaps the simplest method to visualize a data set is to plot a protle of each item, i.e., a two-dimensional graph in which the dimensions are enumerated on the x-axis and the corresponding values on y (Fig. 1 a).
Reference: <author> Ultsch, A. </author> <title> (1993a) Knowledge extraction from self-organizing neural networks. </title> <editor> In Opitz, O., Lausen, B., and Klar, R., editors, </editor> <booktitle> Information and Classitcation, </booktitle> <pages> pages 301306. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: The SOM can also be used for extracting clusters automatically (Lampinen and Oja, 1992; Murtagh, 1995; Pedrycz and Card, 1992; Varts and Versino, 1992), and for rule extraction <ref> (Ultsch, 1993a) </ref>. 4 CASE STUDIES This thesis consists of three case studies in addition to some methodological developments.
Reference: <author> Ultsch, A. </author> <title> (1993b) Self-organizing neural networks for visualization and classi-tcation. </title> <editor> In Opitz, O., Lausen, B., and Klar, R., editors, </editor> <booktitle> Information and Classitcation, </booktitle> <pages> pages 307313. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference: <author> Ultsch, A. and Siemon, H. P. </author> <title> (1990) Kohonen's self organizing feature maps for exploratory data analysis. </title> <booktitle> In Proceedings of ICNN'90, International Neural Network Conference, </booktitle> <pages> pages 305308, </pages> <publisher> Kluwer, Dordrecht. </publisher>
Reference: <author> Varts, A. and Versino, C. </author> <title> (1992) Clustering of socio-economic data with Kohonen maps. Neural Network World, </title> <publisher> 2:813834. </publisher>
Reference: <author> Velleman, P. F. and Hoaglin, D. C. </author> <title> (1981) Applications, Basics, and Computing of Exploratory Data Analysis. </title> <publisher> Duxbury Press, </publisher> <address> Boston, </address> <publisher> MA. </publisher> <editor> von der Malsburg, C. </editor> <title> (1973) Self-organization of orientation sensitive cells in the striate cortex. </title> <journal> Kybernetik, 14:85100. </journal>
Reference: <author> Webb, A. R. </author> <year> (1995). </year> <title> Multidimensional scaling by iterative majorization using radial basis functions. </title> <journal> Pattern Recognition, 28:753759. </journal> <volume> 56 Wish, </volume> <editor> M. and Carroll, J. D. </editor> <title> (1982) Multidimensional scaling and its applications. </title> <editor> In Krishnaiah, P. R. and Kanal, L. N., editors, </editor> <booktitle> Handbook of Statistics, </booktitle> <volume> volume 2, </volume> <pages> pages 317345. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam. </address> <booktitle> World Bank (1992) World Development Report 1992. </booktitle> <publisher> Oxford University Press, </publisher> <address> New York, NY. </address>
Reference-contexts: The method does not necessarily reduce the computational complexity of the original algorithm since the backpropagation learning rule is computationally very intensive, but it generalizes the mapping to new samples. An alternative approach is to use a radial basis function (RBF) network to construct the distance-preserving mapping <ref> (Webb, 1995) </ref>; this could be a promising alternative to the original MDS methods.
Reference: <author> Young, F. W. </author> <title> (1985) Multidimensional scaling. </title> <editor> In Kotz, S., Johnson, N. L., and Read, C. B., editors, </editor> <booktitle> Encyclopedia of Statistical Sciences, </booktitle> <volume> volume 5, </volume> <pages> pages 649659. </pages> <publisher> Wiley, </publisher> <address> New York, NY. </address>
Reference: <author> Young, G. and Householder, A. S. </author> <year> (1938). </year> <title> Discussion of a set of points in terms of their mutual distances. Psychometrika, </title> <publisher> 3:1922. </publisher>
Reference: <author> Zador, P. L. </author> <year> (1982). </year> <title> Asymptotic quantization error of continuous signals and the quantization dimension. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28:139 149. </volume>
Reference: <author> Zhang, X. and Li, Y. </author> <title> (1993) Self-organizing map as a new method for clustering and data analysis. </title> <booktitle> In Proceedings of IJCNN'93 (Nagoya), International Joint Conference on Neural Networks, </booktitle> <pages> pages 24482451. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, NJ. </address>
Reference: <author> Zrehen, S. </author> <title> (1993) Analyzing Kohonen maps with geometry. </title> <editor> In Gielen, S. and Kappen, B., editors, </editor> <booktitle> Proceedings of ICANN'93, International Conference on Artitcial Neural Networks, </booktitle> <pages> pages 609612, </pages> <publisher> Springer, </publisher> <address> London. </address> <month> 57 </month>
Reference-contexts: For example, it can be measured how often the line connecting the reference vectors of neighboring units will be dissected by the Voronoi region of some other unit than the endpoints <ref> (Zrehen, 1993) </ref>. The Voronoi region of unit i is detned to be the set consisting of the points in the input space that are closer to m i than to any other reference vector.
References-found: 134

