URL: http://www.isle.org/~iba/wbayes.ps
Refering-URL: 
Root-URL: 
Title: An Analysis of Bayesian Classifiers  
Author: Pat Langley Wayne Iba Kevin Thompson 
Keyword: Function: learning; Knowledge: experience  
Date: January 15, 1992  
Address: (M/S 269-2)  Moffett Field, CA 94035 USA  
Affiliation: AI Research Branch  NASA Ames Research Center  
Note: Draft (1/14/92) Please do not distribute  Domain: classification; Foundation: mathematical/experimental Primary contact: Pat Langley Phone: (415) 604-4878 Electronic mail: Langley@ptolemy.arc.nasa.gov  
Abstract: In this paper we present an average-case analysis of the Bayesian classifier, a simple probabilistic induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, Boolean attributes that are independent of each other and that follow a single distribution, and the absence of attribute noise. We first calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions; we then use this expression to compute the probability of correct classification over the space of instances. The analysis takes into account the number of training instances, the number of relevant and irrelevant attributes, the distribution of these attributes, and the level of class noise. In addition, we explore the behavioral implications of the analysis by presenting predicted learning curves for a number of artificial domains. We also give experimental results on these domains as a check on our reasoning. Finally, we discuss some unresolved questions about the behavior of Bayesian classifiers and outline directions for future research. Note: Without acknowledgements and references, this paper fits into 12 pages with dimensions 5.5 inches fi 7.5 inches using 12 point LaTeX type. However, we find the current format more desirable. We have not submitted the paper to any other conference or journal. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. A., & Langley, P. </author> <year> (1990). </year> <title> Integrating memory and search in planning. Proceedings of the 1990 Darpa Workshop on Innovative Approaches to Planning, Scheduling, </title> <journal> and Control (pp. </journal> <pages> 301-312). </pages> <address> San Diego, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Buntine, W., & Caruana, R. </author> <year> (1991). </year> <title> Introduction to IND and Recursive Partitioning (NASA Technical Report FIA-91-28). </title> <institution> Moffett Field, CA: NASA Ames Research Center. Cestnik,G., Konenenko,I, & Bratko,I. </institution> <year> (1987). </year> <title> Assistant-86: A Knowledge-Elicitation Tool for Sophisticated Users. </title> <editor> In I.Bratko & N.Lavrac (Eds.) </editor> <booktitle> Progress in Machine Learning, </booktitle> <pages> 31-45, </pages> <publisher> Sigma Press. </publisher>
Reference-contexts: The table shows accuracy results and 95% confidence intervals for each domain, reporting asymptotic accuracy on test sets only. We show results for a Bayesian classifier, a simulation of C4 <ref> (Buntine & Caruana, 1991) </ref>, and a frequency-based algorithm that simply guesses the modal class.
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. </author> <year> (1988). </year> <title> Autoclass: A Bayesian classification system. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 54-64). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning, </booktitle> <pages> 3 , 261-284. </pages>
Reference: <author> Cohen, P. R., & Howe, A. E. </author> <year> (1988). </year> <title> How evaluation guides AI research. </title> <journal> AI Magazine, </journal> <pages> 9 , 35-43. </pages>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: These recent Bayesian learning algorithms are complex and not easily amenable to analysis, but they share a common ancestor that is simpler and more tractable. This supervised algorithm, which we will refer to as a Bayesian classifier, comes originally from work in pattern recognition <ref> (Duda & Hart, 1973) </ref>. The method stores a simple probabilistic summary for each class; this summary contains the conditional probability of each attribute value given the class, as well as the probability (or base rate) of the class.
Reference: <author> Fisher,D. H. & Schlimmer, J. C. </author> <year> (1988). </year> <title> Concept simplification and predictive accuracy. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 22-28). </pages> <address> Ann Arbor, Michigan: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fisher, D. H. </author> <year> (1987). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <booktitle> Machine Learning, </booktitle> <pages> 2 , 139-172. </pages>
Reference: <author> Geiger, D., Paz, A., & Pearl, J. </author> <year> (1990). </year> <title> Learning causal trees from dependency information. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 770-776). </pages> <address> Boston, MA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Cheeseman et al. (1988) have outlined AutoClass, a nonincremental system that uses Bayesian methods to cluster instances into groups. Other researchers have taken a different approach that focuses on the induction of Bayesian inference networks <ref> (e.g., Geiger, Paz, & Pearl, 1990) </ref>. These recent Bayesian learning algorithms are complex and not easily amenable to analysis, but they share a common ancestor that is simpler and more tractable.
Reference: <author> Haussler, D. </author> <year> (1990). </year> <title> Probably approximately correct learning. </title> <booktitle> Proceedings of Bayesian Classifiers 13 the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1101-1108). </pages> <address> Boston, MA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Iba, W., & Gennari, J. H. </author> <year> (1991). </year> <title> Learning to recognize movements. </title> <editor> In D. H. Fisher, M. J. Pazzani, & P. Langley (Eds.), </editor> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The probability of the class after one has corrupted values is P 0 (C) = (1 z)P (C) + z (1 P (C)) = P (C)[1 2z] + z ; as we note elsewhere <ref> (Iba & Langley, 1991) </ref>. For an irrelevant attribute A j , the probability P (A j jC) is unaffected by class noise and remains equal to P (A j ), since the attribute is still independent of the class. However, the situation for relevant attributes is more complicated.
Reference: <author> Iba, W., & Langley, P. </author> <year> (1992). </year> <title> Induction of one-level decision trees. </title> <type> Unpublished manuscript, </type> <institution> AI Research Branch, NASA Ames Research Center, Moffett Field, DA. </institution>
Reference: <author> Kearns, M., Li, M., Pitt, L., & Valiant, L. G. </author> <year> (1987). </year> <title> Recent results on Boolean concept learning. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 337-352). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hirschberg, D. S., & Pazzani, M. J. </author> <year> (1991). </year> <title> Average-case analysis of a k-CNF learning algorithm (Technical Report 91-50). </title> <institution> Irvine: University of California, Department of Information & Computer Science. </institution>
Reference: <author> Kibler, D., & Langley, P. </author> <year> (1988). </year> <title> Machine learning as an experimental science. </title> <booktitle> Proceedings of the Third European Working Session on Learning (pp. </booktitle> <pages> 81-92). </pages> <address> Glasgow: </address> <publisher> Pittman. </publisher>
Reference: <author> Pazzani, M. J., & Sarrett, W. </author> <year> (1990). </year> <title> Average-case analysis of conjunctive learning algorithms. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning/ (pp. </booktitle> <pages> 339-347). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 , 81-106. </pages>
Reference: <author> Rendell, L., & Cho, H. </author> <year> (1990). </year> <title> Empirical learning as a function of concept character. </title> <booktitle> Machine Learning, </booktitle> <pages> 5 , 267-298. </pages>
Reference: <author> Shapiro, A. </author> <year> (1987). </year> <title> Structured induction in expert systems. </title> <address> Reading, MA: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: We show results for a Bayesian classifier, a simulation of C4 (Buntine & Caruana, 1991), and a frequency-based algorithm that simply guesses the modal class. The domains include the "small" soybean domain (Fisher & Schlimmer, 1988), the king-rook-king-pawn chess end game domain <ref> (Shapiro, 1987) </ref>, a domain for predicting lym-phography diseases (Cestnik, Konenenko, & Bratko, 1987), and two biological domains, one for predicting splice junctions in DNA sequences (Towell, Craven, & Shavlik, 1991), and another for predicting DNA promoters (Towell, Shavlik, & Noordweier, 1990).
Reference: <author> Thompson, K., & Langley, P. </author> <year> (1991). </year> <title> Concept formation in structured domains. </title> <editor> In D. H. Fisher, M. J. Pazzani, & P. Langley (Eds.), </editor> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Towell, G. G., Craven, M.W., & Shavlik, J. </author> <year> (1991). </year> <title> Constructive induction in knowledge-based networks. </title> <booktitle> Proceedings of the Eighth International Machine Learning Workshop (pp. </booktitle> <pages> 213-217). </pages> <address> Evanston: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The domains include the "small" soybean domain (Fisher & Schlimmer, 1988), the king-rook-king-pawn chess end game domain (Shapiro, 1987), a domain for predicting lym-phography diseases (Cestnik, Konenenko, & Bratko, 1987), and two biological domains, one for predicting splice junctions in DNA sequences <ref> (Towell, Craven, & Shavlik, 1991) </ref>, and another for predicting DNA promoters (Towell, Shavlik, & Noordweier, 1990). Note that in four of the five domains tested, the mean accuracy for the Bayesian classifier was at least as high as that for the C4-like system. <p> Note that in four of the five domains tested, the mean accuracy for the Bayesian classifier was at least as high as that for the C4-like system. In addition, in the splice-junction domain, the Bayesian classifier performs comparably to the more knowledge-intensive KBANN algorithm <ref> (Towell et al., 1991) </ref>. Note that we are not claiming superiority for the Bayesian classifier, but that it performs reasonably well across a variety of domains, in comparison to other well-known (and more recent) algorithms. Thus, there remain Bayesian Classifiers 3 Table 1.
Reference: <author> Towell, G., Shavlik, J, & Noordewier, M.O. </author> <year> (1990). </year> <title> Refinement of approxi Bayesian Classifiers 14 mate domain theories by knowledge-based neural networks. </title> <booktitle> Proceedings of the Eighth National Conference of the American Association for Artificial Intelligence (pp. </booktitle> <pages> 861-866). </pages> <address> Boston: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: the "small" soybean domain (Fisher & Schlimmer, 1988), the king-rook-king-pawn chess end game domain (Shapiro, 1987), a domain for predicting lym-phography diseases (Cestnik, Konenenko, & Bratko, 1987), and two biological domains, one for predicting splice junctions in DNA sequences (Towell, Craven, & Shavlik, 1991), and another for predicting DNA promoters <ref> (Towell, Shavlik, & Noordweier, 1990) </ref>. Note that in four of the five domains tested, the mean accuracy for the Bayesian classifier was at least as high as that for the C4-like system.
Reference: <author> Yoo, J., & Fisher, D. </author> <year> (1991). </year> <title> Concept formation over problem-solving experience. </title> <editor> In D. H. Fisher, M. J. Pazzani, & P. Langley (Eds.), </editor> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 23

