URL: http://iacoma.cs.uiuc.edu/iacoma-papers/char.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: Characterizing the Caching and Synchronization Performance of a Multiprocessor Operating System  
Author: Josep Torrellas, Anoop Gupta, and John Hennessy 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: Good cache memory performance is essential to achieving high CPU utilization in shared-memory multiprocessors. While the performance of caches is determined by both application and operating system (OS) references, most research has focused on the cache performance of applications alone. This is partially due to the difficulty of measuring OS activity and, as a result, the cache performance of the OS is largely unknown. In this paper, we characterize the cache performance of a commercial System V UNIX running on a four-CPU multiprocessor. The related issue of the performance impact of the OS synchronization activity is also studied. For our study, we use a hardware monitor that records the cache misses in the machine without perturbing it. We study three multiprocessor workloads: a parallel compile, a multiprogrammed load, and a commercial database. Our results show that OS misses occur frequently enough to stall CPUs for 17-21% of their non-idle time. Further, if we include application misses induced by OS interference in the cache, then the stall time reaches 25%. A detailed analysis reveals three major sources of OS misses: instruction fetches, process migration, and data accesses in block operations. As for synchronization behavior, we find that OS synchronization has low overhead if supported correctly and that OS locks show good locality and low contention. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, J. Hennessy, and M. Horowitz. </author> <title> Cache Performance of Operating System and Multiprogramming Work-loads. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: First, Clark [7] reports that the cache performance of the VAX-11/780 measured with a performance monitor is lower than the one predicted with application-only traces. Second, in a simulation using traces of VAX memory references, Agarwal et al <ref> [1] </ref> show that the OS can be responsible for over 50% of the cache miss rate. Related work by Ousterhout [14] and Anderson et al [2] suggests that OS activity is making an increasing impact on the performance of machines.
Reference: [2] <author> T. Anderson, H. Levy, B. Bershad, and E. Lazowska. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Second, in a simulation using traces of VAX memory references, Agarwal et al [1] show that the OS can be responsible for over 50% of the cache miss rate. Related work by Ousterhout [14] and Anderson et al <ref> [2] </ref> suggests that OS activity is making an increasing impact on the performance of machines. This OS activity causes cache misses directly and also indirectly in the applications by displacing the state of the applications from the cache.
Reference: [3] <author> M. J. Bach. </author> <title> The Design of the UNIX Operating System. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1986. </year>
Reference-contexts: IRIX is in turn based on UNIX <ref> [3] </ref> System V and is fully multithreaded except for network functions, which run on CPU 1. All OS data is shared by all threads.
Reference: [4] <author> F. Baskett, T. Jermoluk, and D. Solomon. </author> <title> The 4D-MP Graph--ics Superworkstation: Computing + Graphics = 40 MIPS + 40 MFLOPS and 100,000 Lighted Polygons per Second. </title> <booktitle> In Proceedings of the 33rd IEEE Computer Society International Conference - COMPCON 88, </booktitle> <pages> pages 468-471, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Finally, we conclude in Section 7. 1 2 Experimental Environment In this section, we discuss the hardware and software infrastructure used in our experiments. 2.1 Hardware Setup Our results are based on the analysis of address traces generated by a Silicon Graphics POWER Station 4D/340 <ref> [4] </ref>, a bus-based cache-coherent multiprocessor with four CPUs. Each CPU is a MIPS R3000 with a 64 Kbyte instruction cache and a two level data cache: a 64 Kbyte first-level and a 256 Kbyte second-level cache. All caches are physically-addressed, direct-mapped, and have 16 byte blocks.
Reference: [5] <author> D. Cheriton, H. Goosen, and P. Boyle. </author> <title> Paradigm: A Highly Scalable Shared-Memory Multicomputer Architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 33-46, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: In this section, we discuss the implications of our results for large shared-memory machines organized in clusters such as DASH [10], Paradigm <ref> [5] </ref>, or Encore's Gigamax [21]. First, it may be appropriate to replicate the OS executable across clusters in these machines. This optimization is suggested by the numerous instruction misses in the OS.
Reference: [6] <author> D. Cheriton, A. Gupta, P. Boyle, and H. Goosen. </author> <title> The VMP Multiprocessor: Initial Experience, Refinements and Performance Evaluation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 410-421, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: This helps exploit the spatial locality of the reference stream and therefore reduces data transfer costs. Finally, more sophisticated support for block operations has been suggested by Cheriton et al <ref> [6] </ref>. 4.2.3 Analysis of the Operating System Cache Misses from a Functional Perspective For completeness' sake, we now briefly analyze the OS misses from a functional viewpoint. We classify them according to the high-level operation that the OS was executing when the misses occurred.
Reference: [7] <author> D. Clark. </author> <title> Cache Performance in the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 24-37, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: Other common loads, however, like commercial and software-development loads, may require significant OS activity. Several researchers have pointed out the importance of the cache performance of the OS in sequential machines. First, Clark <ref> [7] </ref> reports that the cache performance of the VAX-11/780 measured with a performance monitor is lower than the one predicted with application-only traces.
Reference: [8] <author> J. Gray. </author> <title> The Benchmark Handbook for Database and Transaction Processing Systems. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: At the most, however, 25 characters can be sent every five seconds. The commands in the input file force the ed session to do character searches and text editing. * Oracle is a scaled down instance of the TP1 database benchmark <ref> [8] </ref> running on an Oracle database. We do not run the standard-sized benchmark because we have limited memory and disk resources and I/O would be a heavy bottleneck. Instead, we reduce the size of the benchmark so that it fits in main memory.
Reference: [9] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications. </title> <booktitle> In ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This OS activity causes cache misses directly and also indirectly in the applications by displacing the state of the applications from the cache. In work based on application-only simulations, Mogul and Borg [13] and Gupta et al <ref> [9] </ref> show the importance of preserving and reusing the cache state of applications. Given all these previous observations, our goal in this paper is to provide an in-depth experimental characterization of the cache performance of a multiprocessor OS.
Reference: [10] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In this section, we discuss the implications of our results for large shared-memory machines organized in clusters such as DASH <ref> [10] </ref>, Paradigm [5], or Encore's Gigamax [21]. First, it may be appropriate to replicate the OS executable across clusters in these machines. This optimization is suggested by the numerous instruction misses in the OS.
Reference: [11] <author> J. D. McDonald and D. Baganoff. </author> <title> Vectorization of a Particle Simulation Method for Hypersonic Rarified Flow. </title> <booktitle> In AIAA Thermodynamics, Plasmadynamics and Lasers Conference, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: All programs are started at the same time. The numeric program, called Mp3d <ref> [11] </ref>, is a 3-D particle simulator used in aeronautics and run using four processes and 50000 particles. Each edit session is created as follows.
Reference: [12] <author> S. McFarling. </author> <title> Program Optimization for Instruction Caches. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The thin spikes in Figure 5 suggest that localized changes in this layout may achieve a reduction of misses. The techniques used to optimize the basic block layout, however, should be slightly different from the existing ones. Current techniques <ref> [12] </ref> are targeted towards code with frequent loop nests. They are based on identifying the most frequently executed loops and then placing the rest of the code to avoid interference with these loops.
Reference: [13] <author> J. Mogul and A. Borg. </author> <title> The Effect of Context Switches on Cache Performance. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 75-84, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This OS activity causes cache misses directly and also indirectly in the applications by displacing the state of the applications from the cache. In work based on application-only simulations, Mogul and Borg <ref> [13] </ref> and Gupta et al [9] show the importance of preserving and reusing the cache state of applications. Given all these previous observations, our goal in this paper is to provide an in-depth experimental characterization of the cache performance of a multiprocessor OS.
Reference: [14] <author> J. Ousterhout. </author> <booktitle> Why Aren't Operating Systems Getting Faster as Fast as Hardware? In Proceedings Summer 1990 USENIX Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Second, in a simulation using traces of VAX memory references, Agarwal et al [1] show that the OS can be responsible for over 50% of the cache miss rate. Related work by Ousterhout <ref> [14] </ref> and Anderson et al [2] suggests that OS activity is making an increasing impact on the performance of machines. This OS activity causes cache misses directly and also indirectly in the applications by displacing the state of the applications from the cache.
Reference: [15] <author> P. Ries. </author> <title> MIPS R4000 Caches and Coherence. </title> <booktitle> In Hot Chips III Symposium Record, </booktitle> <pages> pages 6.1-6.5, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In the simulation, we assume support for atomically reading-modifying-writing lock variables using the load-linked and store-conditional instructions of the MIPS R4000 processor <ref> [15] </ref>. The resulting stall time due to OS synchronization misses is now only 0.7-1.0% of the CPU time (last column of Table 10). This indicates that, with efficient synchronization support, the stall time caused by OS synchronization accesses can be negligible. Table 10: Stall time caused by OS synchronization accesses.
Reference: [16] <author> M. Squillante and E. Lazowska. </author> <title> Using Processor-Cache Affinity in Shared-Memory Multiprocessor Scheduling. </title> <type> Technical Report 89-060-01, </type> <institution> Department of Computer Science, Univer-ity of Washington, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: Process migration should not be completely eliminated, however, for it ensures load balance in the machine. Affinity scheduling <ref> [16, 19, 20] </ref> is one technique that removes misses by encouraging processes to remain in the same CPU while still tolerating process migration for load balance.
Reference: [17] <author> J. Torrellas. </author> <title> Multiprocessor Cache Memory Performance: Characterization and Optimization. </title> <type> Ph.D. dissertation, </type> <institution> Stan-ford University, </institution> <note> to appear, </note> <year> 1992. </year>
Reference-contexts: Due to lack of space, we do not present data for all of the workloads here. However, we show in <ref> [17] </ref> that they all exhibit similar behavior. Overall, the OS synchronizes frequently. For example, the routines that acquire and release locks and semaphores in the OS are usually executed 3-5 times more frequently than the most popular non-synchronizing routines in the OS.
Reference: [18] <author> J. Torrellas, A. Gupta, and J. Hennessy. </author> <title> Characterizing the Cache Performance and Synchronization Behavior of a Multiprocessor Operating System. </title> <type> Technical Report CSL-TR-92-512, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: To see if the size of the database affects the cache performance of the OS, we ran a subset of the experiments using a standard-sized benchmark. We show in <ref> [18] </ref> that the characteristics of the OS misses in the standard benchmark are qualitatively the same as the ones in Oracle. 3.1 Overview of the Cache Behavior We traced each of the workloads for 1-2 minutes. <p> On average, one invocation causes less than 0.1 misses. In addition, it can be shown that the distributions for the number of I- and D-misses generated in one invocation are strongly skewed towards very small values <ref> [18] </ref>. Overall, from the cost of UTLB faults shown in Figure 1, we compute that UTLB fault handling takes the equivalent of 1.5% of application cycles. <p> OS invocations in Multpgm. the duration of Pmake's OS invocations. Figure 3 can be used to build an analytic model of the OS activity in Pmake. The corresponding charts for Multpgm and Oracle are shown in <ref> [18] </ref>. They show that, as in Pmake, an individual OS invocation has a small impact on the cache contents. For completeness' sake, [18] also shows distributions for the number of misses and cycles in invocations of the application for Pmake, Multpgm, and Oracle. <p> Figure 3 can be used to build an analytic model of the OS activity in Pmake. The corresponding charts for Multpgm and Oracle are shown in <ref> [18] </ref>. They show that, as in Pmake, an individual OS invocation has a small impact on the cache contents. For completeness' sake, [18] also shows distributions for the number of misses and cycles in invocations of the application for Pmake, Multpgm, and Oracle.
Reference: [19] <author> J. Torrellas, A. Tucker, and A. Gupta. </author> <title> Evaluating the Benefits of Cache-Affinity Scheduling in Shared-Memory Multiprocessors. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Process migration should not be completely eliminated, however, for it ensures load balance in the machine. Affinity scheduling <ref> [16, 19, 20] </ref> is one technique that removes misses by encouraging processes to remain in the same CPU while still tolerating process migration for load balance.
Reference: [20] <author> R. Vaswani and J. Zahorjan. </author> <title> The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 26-40, </pages> <month> Oc-tober </month> <year> 1991. </year>
Reference-contexts: Process migration should not be completely eliminated, however, for it ensures load balance in the machine. Affinity scheduling <ref> [16, 19, 20] </ref> is one technique that removes misses by encouraging processes to remain in the same CPU while still tolerating process migration for load balance.
Reference: [21] <author> A. W. Wilson. </author> <title> Hierarchical Cache/Bus Architecture for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-252, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: In this section, we discuss the implications of our results for large shared-memory machines organized in clusters such as DASH [10], Paradigm [5], or Encore's Gigamax <ref> [21] </ref>. First, it may be appropriate to replicate the OS executable across clusters in these machines. This optimization is suggested by the numerous instruction misses in the OS.
References-found: 21

