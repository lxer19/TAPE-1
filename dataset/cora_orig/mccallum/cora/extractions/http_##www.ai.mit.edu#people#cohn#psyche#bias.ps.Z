URL: http://www.ai.mit.edu/people/cohn/psyche/bias.ps.Z
Refering-URL: http://www.ai.mit.edu/people/cohn/psyche/
Root-URL: 
Email: cohn@psyche.mit.edu  
Title: Minimizing Statistical Bias with Queries  
Author: David A. Cohn 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1995  
Date: 1552 September 14, 1995  124  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: I describe an exploration criterion that attempts to minimize the error of a learner by minimizing its estimated squared bias. I describe experiments with locally-weighted regression on two simple kinematics problems, and observe that this "bias-only" approach outperforms the more common "variance-only" exploration approach, even in the presence of noise. This report describes research done at the Center for Biological and Computational Learning and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Center is provided in part by a grant from the National Science Foundation under contract ASC-9217041. The author was also funded by ATR Human Information Processing Laboratories, Siemens Corporate Research, and NSF grant CDA-9309300. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Box, G., & Draper, N. </author> <year> (1987). </year> <title> Empirical model-building and response surfaces, </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Cleveland, W., Devlin, S., & Grosse, E. </author> <year> (1988). </year> <title> Regression by local fitting. </title> <journal> Journal of Econometrics, </journal> <volume> 37, </volume> <pages> 87-114. </pages>
Reference: <author> Cohn, D. </author> <title> (1994) Neural network exploration using optimal experiment design. AI Lab Memo 1491, </title> <publisher> MIT. ftp://psyche.mit.edu/pub/cohn/AIM-1491.ps.Z. </publisher>
Reference: <author> Cohn, D., Ghahramani, Z., & Jordan, M. </author> <year> (1995). </year> <title> Active learning with statistical models. </title>
Reference: <editor> In G. Tesauro et al., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press. ftp://psyche.mit.edu/pub/cohn/active-models.ps.Z. </publisher>
Reference: <author> Connor, J. </author> <year> (1993). </year> <title> Bootstrap Methods in Neural Network Time Series Prediction. </title> <editor> In J. Alspector et al., eds., </editor> <booktitle> Proc. of the Int. Workshop on Applications of Neural Networks to Telecommunications, </booktitle> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, N.J. </address>
Reference: <author> Dietterich, T., & Kong, E. </author> <year> (1995). </year> <title> Error-correcting output coding corrects bias and variance. In S. Prieditis 5 Because it assumes that all misfit is due to variance, the variance estimate in Cohn et al. [1995] may be inaccurate if significant bias is present. </title> <editor> 5 and S. Russell, eds., </editor> <booktitle> Proceedings of the 12th Interna--tional Conference on Machine Learning. </booktitle>
Reference: <author> Efron, B. </author> <title> (1983) Estimating the error rate of a prediction rule: some improvements on cross-validation. </title> <journal> em J. Amer. Statist. </journal> <volume> Assoc.g 78 </volume> <pages> 316-331. </pages>
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1993). </year> <title> An introduction to the bootstrap. </title> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference: <author> Fedorov, V. </author> <year> (1972). </year> <title> Theory of Optimal Experiments. </title> <address> New York, NY: </address> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Geman, S., Bienenstock, E., & Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 1-58. </pages>
Reference-contexts: We can then write the expected error of the learner as Z E (^y (x; D) y (x)) jx P (x)dx; (1) where E [] denotes the expectation over P and over training sets D. The expectation inside the integral may be decomposed as follows <ref> (Geman et al., 1992) </ref>: E (^y (x; D) y (x)) jx = (2) h 2 (3) 2 h 2 1 The bias term here is a statistical bias, which is distinct from the inductive bias discussed in some machine learning research.
Reference: <author> MacKay, D. </author> <year> (1992). </year> <title> Information-based objective functions for active data selection, </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 590-604. </pages>
Reference: <author> Paass, G., and Kindermann, J. </author> <year> (1994). </year> <title> Bayesian Query Construction for Neural Network Models. </title> <type> Working Paper 868, </type> <institution> GMD, </institution> <address> Sankt Augustin, Germany. </address>
Reference: <author> Plutowski, M., & White, H. </author> <year> (1993). </year> <title> Selecting concise training sets from clean data. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4, </volume> <pages> 305-318. </pages>
Reference: <author> Schaal, S. & Atkeson, C. </author> <year> (1994). </year> <title> Robot Juggling: An Implementation of Memory-based Learning. </title> <booktitle> Control Systems 14, </booktitle> <pages> 57-71. 6 </pages>
References-found: 15

