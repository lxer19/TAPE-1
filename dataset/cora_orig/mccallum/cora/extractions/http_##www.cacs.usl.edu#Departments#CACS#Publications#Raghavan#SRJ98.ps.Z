URL: http://www.cacs.usl.edu/Departments/CACS/Publications/Raghavan/SRJ98.ps.Z
Refering-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Root-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Title: The Status of Research on Rough Sets for Knowledge Discovery in Databases  
Author: Hayri Sever Vijay V. Raghavan and Thomas D. Johnsten 
Address: 06532 Beytepe, Ankara, Turkey  Lafayette, LA 70504, USA  
Affiliation: The Department of Computer Science Engineering Hacettepe University  The Center for Advanced Computer Studies University of Southwestern Louisiana  
Abstract: Knowledge Discovery in Databases (KDD) has evolved into an important and active area of research because of theoretical challenges and practical applications associated with the problem of discovering (or extracting) interesting and previously unknown knowledge from very large real-world databases. Many aspects of KDD have been investigated in several related fields. The emphasis of ongoing research is to extend existing results to handle characteristics of real-world databases. In this article, we outline the fundamental issues of KDD as well as describe the current status of research on applying rough set theory to KDD. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> BELL, D., AND GUAN, J. </author> <title> Computational methods for rough classification and discovery. </title> <journal> Journal of ASIS 49, </journal> <volume> 5 (1998), </volume> <pages> 403414. </pages>
Reference-contexts: Another question that arises, in the design of algorithms to extract knowledge, is how to quantify the objective function. There are a number of metrics proposed for the evaluation of a rule, namely accuracy [15], significance <ref> [1] </ref>, quality [2], and penalty factor [13]. In [1], the knowledge discovery algorithm exploits lower approximations of given concepts. <p> Another question that arises, in the design of algorithms to extract knowledge, is how to quantify the objective function. There are a number of metrics proposed for the evaluation of a rule, namely accuracy [15], significance <ref> [1] </ref>, quality [2], and penalty factor [13]. In [1], the knowledge discovery algorithm exploits lower approximations of given concepts. <p> Computational Aspects of Rough Set Theory In the literature, there has long been a lack of time complexity analysis of algorithms for frequently used rough set operations. Recently two independent studies have addressed this issue, which is the subject of this section <ref> [1, 3] </ref>. Time complexity of constructing an equivalence relation is shown to be O (lm 2 ), where l and m are number of attributes and objects, respectively [3]. <p> Time complexity of constructing an equivalence relation is shown to be O (lm 2 ), where l and m are number of attributes and objects, respectively [3]. This result correponds to the anlysis of an algorithm, reported in <ref> [1] </ref>, where the goal is to obtain the equivalence relation according to the values of a single attribute. A single concept is defined by a pair of its interior and closure sets. <p> A single concept is defined by a pair of its interior and closure sets. The computation effort for finding either interior (or lower) or closure (or upper) sets is O (lm 2 ), where l and m are number of attributes and objects, respectively <ref> [1, 3] </ref>. The intersection of two equivalence relations is mainly used for reducing the search space (e.g., stepwise backward/forward feature selection [3] or knowledge discovery based on forward selection of significant features [1]). This computation is bounded by m 2 for two equivalence relations on the same set of objects. <p> The intersection of two equivalence relations is mainly used for reducing the search space (e.g., stepwise backward/forward feature selection [3] or knowledge discovery based on forward selection of significant features <ref> [1] </ref>). This computation is bounded by m 2 for two equivalence relations on the same set of objects. <p> If we have a metric to measure the degree of dependency, then we have a way to explore a reduct of X; with a degree of ; where 0 1 [6]. It is shown in <ref> [1] </ref> that finding a reduct of X for Y in S is computationally bounded by l 2 m 2 ; where l and m is a length of X and the number of objects in S; respectively.
Reference: [2] <author> CHOUBEY, S. K., DEOGUN, J. S., RAGHAVAN, V. V., AND SEVER, H. </author> <title> A comparison of feature selection algorithms in the context of rough classifiers. </title> <booktitle> In Proceedings of Fifth IEEE International Conference on Fuzzy Systems (New Orleans, LA, 1996), </booktitle> <volume> vol. 2, </volume> <pages> pp. 11221128. </pages>
Reference-contexts: Another question that arises, in the design of algorithms to extract knowledge, is how to quantify the objective function. There are a number of metrics proposed for the evaluation of a rule, namely accuracy [15], significance [1], quality <ref> [2] </ref>, and penalty factor [13]. In [1], the knowledge discovery algorithm exploits lower approximations of given concepts.
Reference: [3] <author> DEOGUN, J., CHOUBEY, S., RAGHAVAN, V., AND SEVER, H. </author> <title> Feature selection and effective classifiers. </title> <journal> Journal of ASIS 49, </journal> <volume> 5 (1998), </volume> <pages> 423434. </pages>
Reference-contexts: Computational Aspects of Rough Set Theory In the literature, there has long been a lack of time complexity analysis of algorithms for frequently used rough set operations. Recently two independent studies have addressed this issue, which is the subject of this section <ref> [1, 3] </ref>. Time complexity of constructing an equivalence relation is shown to be O (lm 2 ), where l and m are number of attributes and objects, respectively [3]. <p> Recently two independent studies have addressed this issue, which is the subject of this section [1, 3]. Time complexity of constructing an equivalence relation is shown to be O (lm 2 ), where l and m are number of attributes and objects, respectively <ref> [3] </ref>. This result correponds to the anlysis of an algorithm, reported in [1], where the goal is to obtain the equivalence relation according to the values of a single attribute. A single concept is defined by a pair of its interior and closure sets. <p> A single concept is defined by a pair of its interior and closure sets. The computation effort for finding either interior (or lower) or closure (or upper) sets is O (lm 2 ), where l and m are number of attributes and objects, respectively <ref> [1, 3] </ref>. The intersection of two equivalence relations is mainly used for reducing the search space (e.g., stepwise backward/forward feature selection [3] or knowledge discovery based on forward selection of significant features [1]). This computation is bounded by m 2 for two equivalence relations on the same set of objects. <p> The intersection of two equivalence relations is mainly used for reducing the search space (e.g., stepwise backward/forward feature selection <ref> [3] </ref> or knowledge discovery based on forward selection of significant features [1]). This computation is bounded by m 2 for two equivalence relations on the same set of objects.
Reference: [4] <author> DEOGUN, J. S., RAGHAVAN, V. V., SARKAR, A., AND SEVER, H. </author> <title> Data mining: Research trends, challenges, and applications. In Roughs Sets and Data Mining: Analysis of Imprecise Data (Boston, </title> <address> MA, </address> <year> 1997), </year> <editor> T. Y. Lin and N. Cercone, Eds., </editor> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 945. </pages>
Reference-contexts: Fortunately, there exist many near-optimal solutions, or optimal solutions in special cases, with reasonable time complexity that eliminate insignif icant (or redundant) attributes. 2.3 Data Mining Queries The taxonomy provided in <ref> [4] </ref> is summarized below: Hypothesis Testing Query: Hypothesis testing queries are fundamentally distinct from other classes of data mining queries since they do not explicitly discover patterns within the data. Instead their purpose is to evaluate a stated hypothesis against a selected database.
Reference: [5] <author> DEOGUN, J. S., RAGHAVAN, V. V., AND SEVER, H. </author> <title> Rough set based classification methods and extended decision tables. </title> <booktitle> In Proceedings of the International Workshop on Rough Sets and Soft Computing (San Jose, </booktitle> <address> California, </address> <year> 1994), </year> <pages> pp. 302309. </pages>
Reference-contexts: This drawback has been addressed however by numerous works. The alternative is to generalize rough approximation methods by adopting alternative definitions of positive (and boundary) regions <ref> [5, 9] </ref>. For example, in the elementary set approximation of an unknown concept [5], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is greater than a user defined threshold value. <p> This drawback has been addressed however by numerous works. The alternative is to generalize rough approximation methods by adopting alternative definitions of positive (and boundary) regions [5, 9]. For example, in the elementary set approximation of an unknown concept <ref> [5] </ref>, an elementary set is mapped to the positive region of an unknown concept if its degree of membership is greater than a user defined threshold value. Another approach is to shift the problem definition to where a probabilistic approximation space, instead of an algebraic approximation space, is adopted [14]. <p> Recently, it has been argued that `inconsistency' is attributed to the result of a classification method while `nondeterminism' is attributed to the interpretation of that result. It is shown in <ref> [5] </ref>, that inconsistent decision algorithms, under an appropriate representation structure, can be interpreted either deterministically or nondeterministically. This is an important result, particularly when the background knowledge is incomplete and dynamic. Redundant data can be eliminated by pruning insignificant attributes with respect to the problem at hand.
Reference: [6] <author> DEOGUN, J. S., RAGHAVAN, V. V., AND SEVER, H. </author> <title> Exploiting upper approximations in the rough set methodology. </title> <booktitle> In The First International Conference on Knowledge Discovery and Data Mining (Montreal, </booktitle> <address> Quebec, Canada, </address> <month> aug </month> <year> 1995), </year> <editor> U. Fayyad and R. Uthurusamy, </editor> <booktitle> Eds., </booktitle> <pages> pp. 6974. </pages>
Reference-contexts: Furthermore, finding just a single reduct may be too restrictive for some data analysis problems. One plausible approach is to utilize the idea of -reduct defined in the previous subsection <ref> [6] </ref>. To handle missing values, Grzymala-Busse [8] has transformed a given decision table with unknown values to a new and possibly inconsistent decision table by replacing the unknown attribute value with all possible values of that attribute. <p> If we have a metric to measure the degree of dependency, then we have a way to explore a reduct of X; with a degree of ; where 0 1 <ref> [6] </ref>. It is shown in [1] that finding a reduct of X for Y in S is computationally bounded by l 2 m 2 ; where l and m is a length of X and the number of objects in S; respectively.
Reference: [7] <author> FAYYAD, U., PIATETSKY-SHAPIRO, G., AND SMYTH, P. </author> <title> The KDD process for extracting useful knowledge from volumes of data. </title> <journal> Communications of ACM 39, </journal> <volume> 11 (1996), </volume> <pages> 2734. </pages>
Reference-contexts: Finally, we present a taxonomy of knowledge discovery tasks (or data mining queries). Although not exhaustive, our taxonomy nevertheless includes those queries commonly cited in the literature. 2.1 Knowledge Discovery Process In this subsection we briefly outline the KDD process as proposed by Fayyad et al. in <ref> [7] </ref>. Data Selection: The formulation of a data set that is appropriate for the current discovery task. This step may require joining together several existing data sets in order to obtain an appropriate set of examples.
Reference: [8] <author> GRZYMALA-BUSSE, J. W. </author> <title> On the unknown attribute values in learning from examples. </title> <booktitle> In Proceedings of Methodologies for Intelligent Systems, </booktitle> <editor> Z. W. Ras and M. Zemankowa, Eds., </editor> <booktitle> Lecture Notes in AI, </booktitle> <volume> 542. </volume> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991, </year> <pages> pp. 368377. </pages>
Reference-contexts: Furthermore, finding just a single reduct may be too restrictive for some data analysis problems. One plausible approach is to utilize the idea of -reduct defined in the previous subsection [6]. To handle missing values, Grzymala-Busse <ref> [8] </ref> has transformed a given decision table with unknown values to a new and possibly inconsistent decision table by replacing the unknown attribute value with all possible values of that attribute. In other words, he reduced the missing value problem to that of learning from inconsistent examples.
Reference: [9] <author> HASHEMI, R. R., PEARCE, B. A., HINSON, W. G., PAULE, M. G., AND YOUNG, J. F. </author> <title> IQ estimation of monkeys based on human data using rough sets. </title> <booktitle> In Proceedings of the International Workshop on Rough Sets and Soft Computing (San Jose, </booktitle> <address> California, </address> <year> 1994), </year> <pages> pp. 400407. </pages>
Reference-contexts: This drawback has been addressed however by numerous works. The alternative is to generalize rough approximation methods by adopting alternative definitions of positive (and boundary) regions <ref> [5, 9] </ref>. For example, in the elementary set approximation of an unknown concept [5], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is greater than a user defined threshold value.
Reference: [10] <author> KENT, R. E. </author> <title> Rough concept analysis. </title> <booktitle> In Proceedings of the International Workshop on Rough Sets and Knowledge Discovery (Banff, </booktitle> <address> Alberta, Canada, </address> <year> 1993), </year> <pages> pp. 245253. </pages>
Reference-contexts: When we inspect the data mining queries with respect to the rough set methodology, we see that attribute dependency analysis and classification are well investigated subjects, among others. Hypothesis testing and association queries can easily be handled by the rough set methodology. A recent theoretical paper by Kent <ref> [10] </ref> extends the notions of approximation and rough equality to formal concept analysis. An immediate result of this study, in the data mining context, is to use the rough set methodology for the characterization of a concept (or more generally for concept exploration). <p> Although data dependency analysis within the rough set methodology can be applied to the characterization of concepts, the current efforts need to be extended to explicitly include the process of representing relationships between concepts, when a knowledge model contains a set/hierarchy of persistent concepts <ref> [10] </ref>. Acknowledgement This work is supported in part by a grant from the U.S. Department of Energy (under Grant No. DE-FG02 97ER1220).
Reference: [11] <author> LINGRAS, P., AND YAO, Y. </author> <title> Data mining using extensions of rough set model. </title> <journal> Journal of ASIS 49, </journal> <volume> 5 (1998), </volume> <pages> 415422. </pages>
Reference-contexts: He then used rough set theory to induce certain (and/or possible) rules. Alternatively, instead of using an equivalence relation, we can use a partial order relation on object subsets, which is called the generalized rough set model in <ref> [11] </ref>. In this case, an information table becomes a set-valued information table; that is, a field in a tuple may assume more than one value from its domain.
Reference: [12] <author> SLOWINSKI, R., AND STEFANOWISKI, J. </author> <title> Rough classification with valued closeness relation. </title> <booktitle> In Proceedings of the International Workshop on Rough Sets and Knowledge Discovery (San Jose, </booktitle> <address> CA, </address> <year> 1995). </year>
Reference-contexts: In such cases, a mechanism to find the closeness of the given object to known concepts at hand is needed. The usual remedy for this problem is to map non-quantitative values into a numerical scale and use a distance function for the evaluation <ref> [12] </ref>. Computational Aspects of Rough Set Theory In the literature, there has long been a lack of time complexity analysis of algorithms for frequently used rough set operations. Recently two independent studies have addressed this issue, which is the subject of this section [1, 3]. <p> Slowinski & Stefonowski's study on determining the nearest rule <ref> [12] </ref>, in the case that the description of a new object does not match those of known concepts, is a preliminary contribution in enhancing the performance of a rough classifier when the training data set is poorly designed or sampled from a large data set.
Reference: [13] <author> TOLUN, M. R., SEVER, H., AND ULUDAG, M. </author> <title> Improved rule discovery performance on uncertainty. In Research and Development in Knowledge Discovery and Data Mining, </title> <editor> X. Wu, R. Kotagiri, and K. Korb, Eds., </editor> <volume> vol. </volume> <booktitle> 1394 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1998, </year> <pages> pp. 310 321. </pages> <booktitle> Proc. Second Pacific-Asia Conf. </booktitle> <address> PAKDD-98, Melborne, Australia, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: Another question that arises, in the design of algorithms to extract knowledge, is how to quantify the objective function. There are a number of metrics proposed for the evaluation of a rule, namely accuracy [15], significance [1], quality [2], and penalty factor <ref> [13] </ref>. In [1], the knowledge discovery algorithm exploits lower approximations of given concepts. <p> This algorithm assumes a consistent decision table since it is based on the lower approximation. A similar algorithm, called ILA, which employs the notion of an almost elementary set of a given concept, has been proposed in <ref> [13] </ref>. The heuristic exploited by ILA is based on a well-known metric called penalty factor. When we inspect the data mining queries with respect to the rough set methodology, we see that attribute dependency analysis and classification are well investigated subjects, among others.
Reference: [14] <author> YAO, Y. Y., AND WONG, K. M. </author> <title> A decision theoretic framework for approximating concepts. </title> <journal> International Journal Man-Machine Studies 37 (1992), </journal> <volume> 793809. </volume>
Reference-contexts: Another approach is to shift the problem definition to where a probabilistic approximation space, instead of an algebraic approximation space, is adopted <ref> [14] </ref>. In rough set based classification, the terms `inconsistent' and `nondeterministic' decision algorithms (or rules) have been used interchangeably, though they are different concepts. Recently, it has been argued that `inconsistency' is attributed to the result of a classification method while `nondeterminism' is attributed to the interpretation of that result.
Reference: [15] <author> ZIARKO, W. </author> <title> The discovery, analysis, and representation of data dependencies in databases. In Knowledge Discovery in Databases, </title> <editor> G. Piatetsky-Shapiro and W. J. Frawley, Eds. </editor> <publisher> AAAI/MIT, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Under such circumstances, a knowledge discovery system should have the capability of providing approximate decisions with some confidence level <ref> [15] </ref>. Redundant Data: A given data set may contain redundant or insignificant attributes with respect to the problem at hand. <p> Another question that arises, in the design of algorithms to extract knowledge, is how to quantify the objective function. There are a number of metrics proposed for the evaluation of a rule, namely accuracy <ref> [15] </ref>, significance [1], quality [2], and penalty factor [13]. In [1], the knowledge discovery algorithm exploits lower approximations of given concepts.
References-found: 15

