URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/moriarty.efficient-reinforcement.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Title: Machine Learning,  Efficient Reinforcement Learning through Symbiotic Evolution  
Author: DAVID E. MORIARTY AND RISTO MIIKKULAINEN Editor: Leslie Pack Kaelbling 
Keyword: Neuro-Evolution, Reinforcement Learning, Genetic Algorithms, Neural Networks.  
Address: Austin. Austin, TX 78712  
Affiliation: Department of Computer Sciences, The University of Texas at  
Note: c 1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 22,  
Email: moriarty,risto@cs.utexas.edu  
Date: 11-33 (1996)  Received October 20, 1994 Revised October 6, 1995  
Abstract: This article presents a new reinforcement learning method called SANE (Symbiotic, Adaptive Neuro-Evolution), which evolves a population of neurons through genetic algorithms to form a neural network capable of performing a task. Symbiotic evolution promotes both cooperation and specialization, which results in a fast, efficient genetic search and discourages convergence to suboptimal solutions. In the inverted pendulum problem, SANE formed effective networks 9 to 16 times faster than the Adaptive Heuristic Critic and 2 times faster than Q-learning and the GENITOR neuro-evolution approach without loss of generalization. Such efficient learning, combined with few domain assumptions, make SANE a promising approach to a broad range of reinforcement learning problems, including many real-world applications. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1987). </year> <title> Strategy learning with multilayer connectionist representations. </title> <type> Technical Report TR87-509.3, </type> <institution> GTE Labs, </institution> <address> Waltham, MA. </address>
Reference-contexts: Such credit assignment is used to train the action network using a standard supervised learning algorithm such as backpropagation. Two different AHC implementations were tested: A single-layer version (Barto et al., 1983) and a two-layer version <ref> (Anderson, 1987) </ref>. Table 2 lists the parameters for each method. Both implementations were run directly from pole-balance simulators written by Sutton and Anderson, respectively. The learning parameters, network architectures, and control strategy were thus chosen by Sutton and Anderson and presumably reflect parameters that have been found effective.
Reference: <author> Anderson, C. W. </author> <year> (1989). </year> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9 </volume> <pages> 31-37. </pages>
Reference-contexts: Both implementations were run directly from pole-balance simulators written by Sutton and Anderson, respectively. The learning parameters, network architectures, and control strategy were thus chosen by Sutton and Anderson and presumably reflect parameters that have been found effective. Since the state evaluation function to be learned is non-monotonic <ref> (Anderson, 1989) </ref> and single-layer networks can only learn linearly-separable tasks, Barto et al. (1983) discretized the input space into 162 nonoverlapping regions or "boxes" for the single-layer AHC. <p> Each input unit is connected to every hidden unit and to the single output unit. The two-layer networks are trained using 20 D. MORIARTY AND R. MIIKKULAINEN a variant of backpropagation <ref> (Anderson, 1989) </ref>. The output of the action network is interpreted as the probability of choosing that action (push left or right) in both the single and two-layer AHC implementations. 5.2.3.
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference-contexts: The differences in predictions between consecutive states provide effective credit assignment to individual actions selected by the action network. Such credit assignment is used to train the action network using a standard supervised learning algorithm such as backpropagation. Two different AHC implementations were tested: A single-layer version <ref> (Barto et al., 1983) </ref> and a two-layer version (Anderson, 1987). Table 2 lists the parameters for each method. Both implementations were run directly from pole-balance simulators written by Sutton and Anderson, respectively.
Reference: <author> Belew, R. K., McInerney, J., and Schraudolph, N. N. </author> <year> (1991). </year> <title> Evolving networks: Using the genetic algorithm with connectionist learning. </title> <editor> In Farmer, J. D., Langton, C., Rasmussen, S., and Taylor, C., editors, </editor> <booktitle> Artificial Life II. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Brooks, R. A. </author> <year> (1991). </year> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 139-159. </pages> <note> 32 D. </note> <author> MORIARTY AND R. MIIKKULAINEN Collins, R. J., and Jefferson, D. R. </author> <year> (1991). </year> <title> Selection in massively parallel genetic algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 249-256. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An important facet of SANE's neurons is that they form complete input to output mappings, which makes every neuron a primitive solution in its own right. SANE can thus form subsumption-type architectures <ref> (Brooks, 1991) </ref>, where certain neurons provide very crude solutions and other neurons perform higher-level functions that fix problems in the crude solutions. Preliminary studies in simple classification tasks have uncovered some subsumptive behavior among SANE's neurons.
Reference: <author> De Jong, K. A. </author> <year> (1975). </year> <title> An Analysis of the Behavior of a Class of Genetic Adaptive Systems. </title> <type> PhD thesis, </type> <institution> The University of Michigan, </institution> <address> Ann Arbor, MI. </address>
Reference-contexts: As a result, evolution ceases to make timely progress and neuro-evolution is deemed pathologically slow. Several methods have been developed to prevent premature convergence including fitness sharing (Goldberg and Richardson, 1987), adaptive mutation (Whitley et al., 1990), crowding <ref> (Dejong, 1975) </ref>, and local mating (Collins and Jefferson, 1991). Each of these techniques limits convergence through external operations that are often computationally expensive or produce a less efficient search.
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Goldberg, D. E., and Richardson, J. </author> <year> (1987). </year> <title> Genetic algorithms with sharing for multimodal function optimization. </title> <booktitle> In Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> 148-154. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Instead of multiple parallel searches through the encoding space, the search becomes a random walk using the mutation operator. As a result, evolution ceases to make timely progress and neuro-evolution is deemed pathologically slow. Several methods have been developed to prevent premature convergence including fitness sharing <ref> (Goldberg and Richardson, 1987) </ref>, adaptive mutation (Whitley et al., 1990), crowding (Dejong, 1975), and local mating (Collins and Jefferson, 1991). Each of these techniques limits convergence through external operations that are often computationally expensive or produce a less efficient search.
Reference: <author> Grefenstette, J., and Schultz, A. </author> <year> (1994). </year> <title> An evolutionary approach to learning in robots. </title> <booktitle> In Proceedings of the Machine Learning Workshop on Robot Learning, Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference-contexts: SAMUEL has been shown effective in several small problems including the evasive maneuvers problem (Grefenstette et al., 1990) and the game of cat-and-mouse (Grefenstette, 1992), and in more recent work, SAMUEL has been extended to the task of mobile robot navigation <ref> (Grefenstette and Schultz, 1994) </ref>. The main difference between SAMUEL and SANE lies in the choice of representation. Whereas SAMUEL evolves a set of rules for sequential decision tasks, SANE 28 D. MORIARTY AND R. MIIKKULAINEN evolves neural networks.
Reference: <author> Grefenstette, J. J. </author> <year> (1992). </year> <title> An approach to anytime learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> 189-195. </pages>
Reference-contexts: SAMUEL has been shown effective in several small problems including the evasive maneuvers problem (Grefenstette et al., 1990) and the game of cat-and-mouse <ref> (Grefenstette, 1992) </ref>, and in more recent work, SAMUEL has been extended to the task of mobile robot navigation (Grefenstette and Schultz, 1994). The main difference between SAMUEL and SANE lies in the choice of representation. Whereas SAMUEL evolves a set of rules for sequential decision tasks, SANE 28 D.
Reference: <author> Grefenstette, J. J., Ramsey, C. L., and Schultz, A. C. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381. </pages>
Reference-contexts: Related Work Work most closely related to SANE can be divided into two categories: genetic reinforcement learning and coevolutionary genetic algorithms. 7.1. Genetic Reinforcement Learning Several systems have been built or proposed for reinforcement learning through genetic algorithms, including both symbolic and neural network approaches. The SAMUEL system <ref> (Grefenstette et al., 1990) </ref> uses genetic algorithms to evolve rule-based classifiers in sequential decision tasks. Unlike most classifier systems where genetic algorithms evolve individual rules, SAMUEL evolves a population of classifier systems or "tactical plans" composed of several action rules. <p> SAMUEL is a model-based system that is designed to evolve decision plans o*ine in a simulation of the domain and then incrementally add the current best plan to the actual domain. SAMUEL has been shown effective in several small problems including the evasive maneuvers problem <ref> (Grefenstette et al., 1990) </ref> and the game of cat-and-mouse (Grefenstette, 1992), and in more recent work, SAMUEL has been extended to the task of mobile robot navigation (Grefenstette and Schultz, 1994). The main difference between SAMUEL and SANE lies in the choice of representation.
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, </title> <booktitle> Control and Artificial Intelligence. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press. </publisher>
Reference: <author> Horn, J., Goldberg, D. E., and Deb, K. </author> <year> (1994). </year> <title> Implicit niching in a learning classifier system: Nature's way. </title> <journal> Evolutionary Computation, </journal> <volume> 2(1) </volume> <pages> 37-66. </pages>
Reference: <author> Jefferson, D., Collins, R., Cooper, C., Dyer, M., Flowers, M., Korf, R., Taylor, C., and Wang, A. </author> <year> (1991). </year> <title> Evolution as a theme in artificial life: The genesys/tracker system. </title> <editor> In Farmer, J. D., Langton, C., Rasmussen, S., and Taylor, C., editors, </editor> <booktitle> Artificial Life II. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: As a result, evolution ceases to make timely progress and neuro-evolution is deemed pathologically slow. Several methods have been developed to prevent premature convergence including fitness sharing (Goldberg and Richardson, 1987), adaptive mutation (Whitley et al., 1990), crowding (Dejong, 1975), and local mating <ref> (Collins and Jefferson, 1991) </ref>. Each of these techniques limits convergence through external operations that are often computationally expensive or produce a less efficient search. In the next section, a new evolutionary method will be presented that maintains diverse populations without expensive operations or high degrees of randomness. 3.
Reference: <author> Kitano, H. </author> <year> (1990). </year> <title> Designing neural networks using genetic algorithms with graph generation system. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 461-476. </pages>
Reference: <author> Koza, J. R., and Rice, J. P. </author> <year> (1991). </year> <title> Genetic generalization of both the weights and architecture for a neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> 397-404. </pages> <address> New York, NY: </address> <publisher> IEEE. </publisher>
Reference: <author> Lee, K.-F., and Mahajan, S. </author> <year> (1990). </year> <title> The development of a world class Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36. </pages>
Reference-contexts: SANE formed a network to decide which moves from a given board situation are promising enough to be evaluated. Such decisions can establish better play by effectively hiding bad states from the minimax search. Using the powerful evaluation function from Bill <ref> (Lee and Mahajan, 1990) </ref>, the SANE network was able to generate better play while examining 33% fewer board positions than a normal, full-width minimax search using the same evaluation function. Future work on SANE includes applying it to larger real-world domains with multiple decision tasks.
Reference: <author> Lin, L.-J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 293-321. </pages>
Reference-contexts: Note that the Q-function can be represented efficiently as a look-up table only when the state space is small. In a real-world application, the enormous state space would make explicit representation of each state impossible. Larger applications of Q-learning are likely to use neural networks <ref> (Lin 1992) </ref>, which can learn from continuous input values in an infinite state space. Instead of representing each state explicitly, neural networks form internal representations of the state space through their connections and weights, which allows them to generalize well to unobserved states.
Reference: <author> Michie, D., and Chambers, R. A. </author> <year> (1968). </year> <title> BOXES: An experiment in adaptive control. </title> <editor> In Dale, E., and Michie, D., editors, </editor> <booktitle> Machine Intelligence. </booktitle> <address> Edinburgh, UK: </address> <publisher> Oliver and Boyd. </publisher>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1994a). </year> <title> Evolutionary neural networks for value ordering in constraint satisfaction problems. </title> <type> Technical Report AI94-218, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference-contexts: It can thus be implemented in a broad range of tasks including real-world decision tasks. We have implemented SANE in two such tasks in the field of artificial intelligence: value ordering in constraint satisfaction problems and focusing a minimax search <ref> (Moriarty and Miikkulainen, 1994a, 1994b) </ref>. Value ordering in constraint satisfaction problems is a well-studied task where problem-general approaches have performed inconsistently. A SANE network was used to decide the order in which types or classes of cars were assigned on an 30 D. MORIARTY AND R.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1994b). </year> <title> Evolving neural networks to focus minimax search. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1371-1377. </pages> <address> Seattle, WA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Nolfi, S., and Parisi, D. </author> <year> (1992). </year> <title> Growing neural networks. </title> <booktitle> Artificial Life III, </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Pendrith, M. </author> <year> (1994). </year> <title> On reinforcement learning of control actions in noisy and non-Markovian domains. </title> <type> Technical Report UNSW-CSE-TR-9410, </type> <institution> School of Computer Science and Engineering, The University of New South Wales. </institution>
Reference: <author> Potter, M., and De Jong, K. </author> <year> (1995a). </year> <title> Evolving neural networks with collaborative species. </title> <booktitle> In Proceedings of the 1995 Summer Computer Simulation Conference. </booktitle> <address> Ottawa, Canada. </address>
Reference: <author> Potter, M., De Jong, K., and Grefenstette, J. </author> <year> (1995b). </year> <title> A coevolutionary approach to learning sequential decision rules. </title> <booktitle> In Proceedings of the Sixth International Conference on Genetic Algorithms. </booktitle> <address> Pittsburgh, PA. </address>
Reference: <author> Sammut, C., and Cribb, J. </author> <year> (1990). </year> <title> Is learning rate a good performance criterion for learning? In Proceedings of the Seventh International Conference on Machine Learning, </title> <address> 170-178. </address> <publisher> Morgan Kaufmann. </publisher> <editor> SYMBIOTIC EVOLUTION 33 Schaffer, J. D., Whitley, D., and Eshelman, L. J. </editor> <year> (1992). </year> <title> Combinations of genetic algorithms and neural networks: A survey of the state of the art. </title> <booktitle> In Proceedings of the International Workshop on Combinations of Genetic Algorithms and Neural Networks (COGANN-92). </booktitle> <address> Baltimore, MD. </address>
Reference: <author> Smith, R. E. </author> <year> (1994). </year> <title> Is a learning classifier system a type of neural network? Evolutionary Computation, </title> <type> 2(1). </type>
Reference: <author> Smith, R. E., Forrest, S., and Perelson, A. S. </author> <year> (1993). </year> <title> Searching for diverse, cooperative populations with genetic algorithms. </title> <journal> Evolutionary Computation, </journal> <volume> 1(2) </volume> <pages> 127-149. </pages>
Reference: <author> Smith, R. E., and Gray, B. </author> <year> (1993). </year> <title> Co-adaptive genetic algorithms: An example in Othello strategy. </title> <type> Technical Report TCGA 94002, </type> <institution> Department of Engineering Science and Mechanics, The University of Alabama. </institution>
Reference: <author> Steetskamp, R. </author> <title> (1995) Explorations in symbiotic neuro-evolution search spaces. </title> <type> Masters Stage Report, </type> <institution> Department of Computer Science, University of Twente, The Netherlands. </institution>
Reference-contexts: Symbiosis emerges naturally in the current representation of neural networks as collections of hidden neurons, but preliminary experiments with other types of encodings, such as populations of individual SYMBIOTIC EVOLUTION 31 network connections, have been unsuccessful <ref> (Steetskamp, 1995) </ref>. An important facet of SANE's neurons is that they form complete input to output mappings, which makes every neuron a primitive solution in its own right.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: The action network receives the current problem state and chooses an appropriate control action. The evaluation network receives the same input, and evaluates or critiques the current state. The evaluation network is trained using the temporal difference method <ref> (Sutton, 1988) </ref> to predict the expected outcome of the current trial given the current state and the action network's current decision policy. The differences in predictions between consecutive states provide effective credit assignment to individual actions selected by the action network.
Reference: <author> Syswerda, G. </author> <year> (1991). </year> <title> A study of reproduction in generational and steady-state genetic algorithms. </title> <editor> In Rawlings, G., editor, </editor> <booktitle> Foundations of Genetic Algorithms, </booktitle> <pages> 94-101. </pages> <address> San Mateo, CA: </address> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292. </pages>
Reference-contexts: Given accurate Q-function values, called Q values, an optimal decision policy is one that selects the action with the highest associated Q value (expected payoff) for each state. The Q-function is learned through "incremental dynamic programming" <ref> (Watkins and Dayan, 1992) </ref>, which maintains an estimate ^ Q of the Q values and updates the estimates based on immediate payoffs and estimated payoffs from subsequent states.
Reference: <author> Whitley, D. </author> <year> (1989). </year> <title> The GENITOR algorithm and selective pressure. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Whitley, D. </author> <year> (1994). </year> <title> A genetic algorithm tutorial. </title> <journal> Statistics and Computing, </journal> <volume> 4 </volume> <pages> 65-85. </pages>
Reference-contexts: In fitness-proportionate selection, a string s is selected for mating with probability f s =F , where f s is the fitness of string s and F is the average fitness of the population. As the average fitness of the strings increase, the variance in fitness decreases <ref> (Whitley, 1994) </ref>. Without sufficient variance between the best and worst performing strings, the genetic algorithm will be unable to assign significant bias towards the best strings.
Reference: <author> Whitley, D., Dominic, S., Das, R., and Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 259-284. </pages>
Reference-contexts: The original programs written by Sutton and Anderson were used for the AHC implementations, and the simulation code developed by Pendrith (1994) was used for the Q-learning implementation. For GENITOR, the system was reimplemented as described in <ref> (Whitley et al., 1993) </ref>. A control strategy was deemed successful if it could balance a pole for 120,000 time steps. 5.2.1. SANE SANE was implemented to evolve a 2-layer network with 5 input, 8 hidden, and 2 output units. <p> For example, a decision of "move right" with activation 0.9 would move right only 90% of the time. Probabilistic output units allow the network to visit more of the state space during training, and thus incorporate a more global view of the problem into the control policy <ref> (Whitley et al., 1993) </ref>. In the SANE implementation, however, randomness is unnecessary in the decision process since there is a large amount of state space sampling through multiple combinations of neurons. SYMBIOTIC EVOLUTION 19 Table 2. <p> Comparisons between GENITOR's and SANE's search efficiency thus test the hypothesis that symbiotic evolution produces an efficient search without reliance on additional randomness. Since GENITOR has been shown to be effective in evolving neural networks for the inverted pendulum problem <ref> (Whitley et al., 1993) </ref>, it also provides a state-of-the-art neuro-evolution comparison. GENITOR was implemented as detailed in (Whitley et al., 1993) to evolve the weights in a fully-connected 2-layer network, with additional connections from each input unit to the output layer. <p> Since GENITOR has been shown to be effective in evolving neural networks for the inverted pendulum problem <ref> (Whitley et al., 1993) </ref>, it also provides a state-of-the-art neuro-evolution comparison. GENITOR was implemented as detailed in (Whitley et al., 1993) to evolve the weights in a fully-connected 2-layer network, with additional connections from each input unit to the output layer. The network architecture is identical to the two-layer AHC with 5 input units, 5 hidden units and 1 output unit. <p> As demonstrated in the pole-balancing simulations, GENITOR's high mutation rates can lead to a less efficient search than SANE's symbiotic approach. Another difference between SANE and Whitley's approach lies in the network architectures. In the current implementation of GENITOR <ref> (Whitley et al., 1993) </ref>, the network architecture is fixed and only the weights are evolved. The implementor must resolve a priori how the network should be connected.
Reference: <author> Whitley, D., and Kauth, J. </author> <year> (1988). </year> <title> GENITOR: A different genetic algorithm. </title> <booktitle> In Proceedings of the Rocky Mountain Conference on Artificial Intelligence, </booktitle> <pages> 118-130. </pages> <address> Denver, </address> <publisher> CO. </publisher>
Reference: <author> Whitley, D., Starkweather, T., and Bogart, C. </author> <year> (1990). </year> <title> Genetic algorithms and neural networks: Optimizing connections and connectivity. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 347-361. </pages>
Reference-contexts: As a result, evolution ceases to make timely progress and neuro-evolution is deemed pathologically slow. Several methods have been developed to prevent premature convergence including fitness sharing (Goldberg and Richardson, 1987), adaptive mutation <ref> (Whitley et al., 1990) </ref>, crowding (Dejong, 1975), and local mating (Collins and Jefferson, 1991). Each of these techniques limits convergence through external operations that are often computationally expensive or produce a less efficient search.
References-found: 39

