URL: http://www.cs.berkeley.edu/~silkworm/papers/fft.ps
Refering-URL: http://www.cs.berkeley.edu/~silkworm/papers/
Root-URL: 
Title: An FFT algorithm on Vector Intelligent RAM the number of data points in the transform.
Author: Kirby Zhang Nathan Slingerland 
Note: N=2, where N is  
Date: May 18, 1998  
Affiliation: Department of Electrical Engineering and Computer Science University of California, Berkeley  
Pubnum: CS252: Advanced Computer Architecture  
Abstract: We present an implementation of a Fast Fourier Transform on Vector Intelligent RAM architecture. The algorithm computes the Discrete Fourier Transform in O(N log N) time, is self-sorting (no bit-reversed copy phase is required), and has a minimum vector length of p 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Numerical Recipes in C. </editor> <publisher> Cambridge University Press, </publisher> <address> http://www.nr.com. </address>
Reference-contexts: Initial results do suggest, however, that their effects are neglible on these algorithms. 5 Related Work Numerical Recipes in C <ref> [1] </ref> gives an excellent overview of Fast Fourier Transforms. It contains an overview of the various uses of DFT in signal processing, a wide variety of FFT algorithms, discussion on the role of complex numbers, and multi-dimensional transforms.
Reference: [2] <author> Ramesh C. Agarwal and James W. Cooley. </author> <title> Fourier transform and convolution subroutines for the IBM 3090 Vector Facility. </title> <journal> j-IBM-JRD, </journal> <volume> 30(2) </volume> <pages> 145-162, </pages> <month> mar </month> <year> 1986. </year>
Reference-contexts: Markus Hegland [7] propose a unit-stride, self-sorting, and in-place FFT algorithm. This is certainly worth investigating. A number of other authors have investigated FFT algorithms on vector computers as well <ref> [3, 2, 8, 14, 10] </ref>.
Reference: [3] <author> M. Ashworth and A. Lyne. </author> <title> A segmented FFT algorithm for vector computers. </title> <journal> PC, </journal> <volume> 6 </volume> <pages> 217-224, </pages> <year> 1988. </year>
Reference-contexts: Markus Hegland [7] propose a unit-stride, self-sorting, and in-place FFT algorithm. This is certainly worth investigating. A number of other authors have investigated FFT algorithms on vector computers as well <ref> [3, 2, 8, 14, 10] </ref>.
Reference: [4] <author> D. Bailey. </author> <title> A high performance fast Fourier transform algorithm for the CRAY-2. </title> <journal> J. Supercomputing, </journal> <volume> 1 </volume> <pages> 43-60, </pages> <year> 1987. </year>
Reference-contexts: Especially for smaller 3 enclosed. The arrows point to two half-sized DFT's from which the current DFT is computed. transforms, this is a serious under-utilization of the vector hardware. We present an implementation on VIRAM a much better FFT algorithm, borrowing some ideas from [12] and <ref> [4] </ref>. Finally, we discuss how we vectorized the computation of W k N . In this section, we will ignore for the moment the implications of storing and operating on complex numbers. Assume for now that one CPU word is used to represent each datapoint. 3.1 Cooley-Tukey Algorithm FFT algorithms. <p> In section 4, we point out that for small transforms, this can be an unacceptable performance penalty. 3.2 The Two Stockham Variants In light of the deficiencies of the Cooley-Tukey algorithm, we incorporated some of Swarztrauber [12] and Bailey's <ref> [4] </ref> ideas to implement a better FFT algorithm on VIRAM. We accomplish this by taking advantage of two important properties of two variants of the Stockham FFT algorithm [12] * Rather than using an explict reordering phase to accomodate for bit-reversal, reordering is done during each stage of the transform. <p> Thus, it is desirable to maximize LCM (stride; nbanks)=stride. What happens when stride is a power of 2? LCM (stride; nbanks) = M AX (stride; nbanks). David Bailey <ref> [4] </ref> suggests that we add 1 to stride by performing the transpose diagonaly down the matrix, but did not offer an explanation. Suppose that stride fl q = nbanks, for some integer q. <p> Swarztrauber [12] gives an elegant introduction to the Cooley-Tukey algorithm and Stockham variants. We borrowed some of his terminology in this paper. He considered the self-sorting feature of the Stockham algorithm but did not analyze their striding characteristics. David Bailey <ref> [4] </ref> presents his results on implementing a unit-stride vectorized algorithm on Cray-2. His algorithm uses the two Stockham variants interchangeably, with a matrix transpose in between changes. He uses table lookups to compute W k N .
Reference: [5] <author> Barry Cipra. </author> <title> The fft: Making technology fly. </title> <address> http://www.siam.org/siamnews/mtc/mtc593.htm. </address>
Reference-contexts: Any kind of wave phenomenon, be it seismic, tidal, or electromagnetic, is a candidate for Fourier Analysis. Many statistical processes, such as the removal of "noise" from data and computng correlations, are also based on working with Fourier transforms. 1 -Barry Cipra <ref> [5] </ref> In this paper, we present an implementation of an algorithmic solution to the Fourier Transform on the Vector Intelligent RAM architecture. <p> The FFT is the algorithm that makes many of the applications described in the previous section computationally feasible. According to Gil Strang of MIT, the FFT is "the most valuable numerical algorithm in our lifetime" <ref> [5] </ref>.
Reference: [6] <author> J.W. Cooley and J.W. Tukey. </author> <title> An algorithm for the machine calculation of complex fourier series. </title> <journal> Mathematics of Computation, </journal> <volume> 19 </volume> <pages> 297-301, </pages> <year> 1965. </year>
Reference-contexts: Thus, we can write the time complexity of an N-point transform as T (N ) = 2T (N=2) + O (N ) (11) An 8-point FFT algorithm is illustrated in figure 1. 3 FFT on VIRAM The original algorithm proposed by Cooley and Tukey <ref> [6] </ref> for computing the Fast Fourier Transform on digital computers is inappropriate for computation on VIRAM for two reasons. Observe in figure 1 that the transformed elements are stored in locations whose addresses are bit-reversed from the original addresses.
Reference: [7] <author> Markus Hegland. </author> <title> A self-sorting in-place fast Fourier transform algorithm suitable for vector and parallel processing. </title> <journal> j-NUM-MATH, </journal> <volume> 68(4) </volume> <pages> 507-547, </pages> <year> 1994. </year>
Reference-contexts: It would be interesting to compare the performance of the power-of-two matrix transpose versus power-of-two-plus-one version, once the VIRAM for the latter code has been implemented. Markus Hegland <ref> [7] </ref> propose a unit-stride, self-sorting, and in-place FFT algorithm. This is certainly worth investigating. A number of other authors have investigated FFT algorithms on vector computers as well [3, 2, 8, 14, 10].
Reference: [8] <author> J.R. Johnson, W.R. Johnson, D. Rodriguez, and R. Tolmieri. </author> <title> A method for design, modification and implementation of FFT algorithms on various architectures. </title> <journal> Circuits Systems, Signal Process, </journal> <volume> 9(2) </volume> <pages> 449-500, </pages> <year> 1990. </year>
Reference-contexts: Markus Hegland [7] propose a unit-stride, self-sorting, and in-place FFT algorithm. This is certainly worth investigating. A number of other authors have investigated FFT algorithms on vector computers as well <ref> [3, 2, 8, 14, 10] </ref>.
Reference: [9] <author> Paola Moretto. </author> <title> Mapping of speech front-end signal processing to high performance vector architectures. </title> <address> http://www.ICSI.Berkeley.EDU/ftp/global/pub/techreports/1995/tr-95-063.ps.Z. </address>
Reference-contexts: Note that p 22:6. So why do we have vector lengths of 16? Probably due to some inefficiency in the implementation. 15 aspects of his algorithm and formulae, and had to design our own from scratch. Paul Moretto <ref> [9] </ref> discusses a method of vectorizing the 1-dimensional DFT on the T0 processor by breaking it up into a 2-dimensional transform, but he does not actually use any of the Fast Fourier Transform techniques. 6 Future Work Many possibilities in optimizing the existing FFT implementation remain open.
Reference: [10] <author> L. Nicastro and N. D'Amico. </author> <title> An optimized mass storage FFT for vector computers. </title> <journal> Parallel Computing, </journal> <volume> 21 </volume> <pages> 423-432, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Markus Hegland [7] propose a unit-stride, self-sorting, and in-place FFT algorithm. This is certainly worth investigating. A number of other authors have investigated FFT algorithms on vector computers as well <ref> [3, 2, 8, 14, 10] </ref>.
Reference: [11] <author> David A. Pattern and John L. Hennessy. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, California. </address>
Reference-contexts: Unfortunately, the matrix dimensions (LS fi 2N S) in the vectorized FFT algorithm are always in powers of 2. Patterson and Hennesey <ref> [11] </ref> give the following relationship between memory access latency and stride size if we are to achieve a memory bandwidth of one access per cycle. memory access latency (in cycles) &lt; LCM (stride, nbanks)/stride, where LCM is the least common multiple and nbanks is the number of memory banks in the
Reference: [12] <author> P. Swarztrauber. </author> <title> FFT algorithms for vector computers. </title> <journal> PC, </journal> <volume> 1 </volume> <pages> 45-63, </pages> <year> 1984. </year>
Reference-contexts: Especially for smaller 3 enclosed. The arrows point to two half-sized DFT's from which the current DFT is computed. transforms, this is a serious under-utilization of the vector hardware. We present an implementation on VIRAM a much better FFT algorithm, borrowing some ideas from <ref> [12] </ref> and [4]. Finally, we discuss how we vectorized the computation of W k N . In this section, we will ignore for the moment the implications of storing and operating on complex numbers. <p> Thus, vector lengths must start at 1 and increase exponentially thereafter. In section 4, we point out that for small transforms, this can be an unacceptable performance penalty. 3.2 The Two Stockham Variants In light of the deficiencies of the Cooley-Tukey algorithm, we incorporated some of Swarztrauber <ref> [12] </ref> and Bailey's [4] ideas to implement a better FFT algorithm on VIRAM. We accomplish this by taking advantage of two important properties of two variants of the Stockham FFT algorithm [12] * Rather than using an explict reordering phase to accomodate for bit-reversal, reordering is done during each stage of <p> 3.2 The Two Stockham Variants In light of the deficiencies of the Cooley-Tukey algorithm, we incorporated some of Swarztrauber <ref> [12] </ref> and Bailey's [4] ideas to implement a better FFT algorithm on VIRAM. We accomplish this by taking advantage of two important properties of two variants of the Stockham FFT algorithm [12] * Rather than using an explict reordering phase to accomodate for bit-reversal, reordering is done during each stage of the transform. Such "hidden" reordering incurs no computational cost in time. * Two variants of the Stockham algorithm, both having the above property, have exact opposite vectorizability characteristics. <p> It contains an overview of the various uses of DFT in signal processing, a wide variety of FFT algorithms, discussion on the role of complex numbers, and multi-dimensional transforms. A small part of the terminology in this paper is borrowed from that book. Swarztrauber <ref> [12] </ref> gives an elegant introduction to the Cooley-Tukey algorithm and Stockham variants. We borrowed some of his terminology in this paper. He considered the self-sorting feature of the Stockham algorithm but did not analyze their striding characteristics.
Reference: [13] <institution> UC Berkeley VIRAM Group. </institution> <month> Energy-efficient, </month> <title> high-performance memory system design for vector iram. </title> <address> http://iram.cs.berkeley.edu/local/1998-isca/current/isca.ps. </address>
Reference-contexts: A portion of the VIRAM code is included in the appendix. In this section, we will present some preliminary results from the VIRAM memory simulator <ref> [13] </ref>. In the present implementation, the two parts of a complex number are interleaved in memory. This unnecessarily requires stride-2 accesses in all our vector operations. Fortunately, due to VIRAM's 256-bit bus, only one address per cycle is required for stride-2 access, and our results here are not terribly contaminated.
Reference: [14] <author> Qing Yang. </author> <title> Performance of cache memories for vector computers. </title> <journal> j-J-PAR-DIST-COMP, </journal> <volume> 19(3):163-??, </volume> <month> November </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: Markus Hegland [7] propose a unit-stride, self-sorting, and in-place FFT algorithm. This is certainly worth investigating. A number of other authors have investigated FFT algorithms on vector computers as well <ref> [3, 2, 8, 14, 10] </ref>.
References-found: 14

