URL: ftp://ftp.cs.rochester.edu/pub/papers/ai/97.Ringger-Allen.ESCA97.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/ringger/research/esca-workshop-97.html
Root-URL: 
Email: fringger, jamesg@cs.rochester.edu  
Title: ROBUST ERROR CORRECTION OF CONTINUOUS SPEECH RECOGNITION  
Author: Eric K. Ringger James F. Allen 
Web: http://www.cs.rochester.edu/research/trains/  
Address: Rochester, New York 14627-0226 USA  
Affiliation: Department of Computer Science University of Rochester  
Abstract: We present a post-processing technique for correcting errors committed by an arbitrary continuous speechrecognizer. The technique leverages our observation that consistent recognition errors arising from mismatched training and usageconditions can be modeled and corrected. We have implemented a post-processor called SPEECHPP to correct word-level errors, and we show that this post-processing technique applies successfully when the training and usage domains differ even slightly; for the purposes of the recognizer, such a difference manifests itself as differences in the vocabulary and in the likelihoods of word collocations. We hypothesize that other differences between the training and usage conditions yield recognition errors with some consistency also. Hence, we propose that our technique be used to compensate for those mismatches as well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. F. Allen, B. W. Miller, E. K. Ringger, and T. Sikorski. </author> <title> A ro bust system for natural spoken dialogue. </title> <booktitle> In Proceedings of the 1996 Annual Meeting of the Association for Computational Linguistics (ACL'96). ACL, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Mismatched Domain To date, our experiments have involved a mismatch in the domain of discourse. We have used ATIS (airline travel information) data for training the recognizer and TRAINS-95 (train route planning) <ref> [1] </ref> data for testing. Here are a few examples of the kinds of errors that occur when recognizing spontaneous utterances in the TRAINS-95 domain using Sphinx-II [5] running with models trained from ATIS data.
Reference: [2] <author> L. R. Bahl, F. Jelinek, and R. Mercer. </author> <title> A Maximum Like lihood Approach to Continuous Speech Recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), </journal> <volume> 5(2) </volume> <pages> 179-190, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: THE MODELS AND ALGORITHM We applied a noisy channel model and adapted techniques from statistical machine translation (c.f. [3]) and statistical speech recognition (c.f. <ref> [2, 6] </ref>) in order to model the errors that Sphinx-II makes in our domain.
Reference: [3] <author> P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. </author> <title> A Statistical Approach to Machine Translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Furthermore, a recognizer can be used as a server with multiple unknown clients using unknown channels, as long as the clients are permitted to enroll with labeled speech. 2. THE MODELS AND ALGORITHM We applied a noisy channel model and adapted techniques from statistical machine translation (c.f. <ref> [3] </ref>) and statistical speech recognition (c.f. [2, 6]) in order to model the errors that Sphinx-II makes in our domain.
Reference: [4] <author> J. G. E. Forney. </author> <title> The Viterbi Algorithm. </title> <booktitle> In Proceedings of IEEE, </booktitle> <volume> volume 61, </volume> <pages> pages 266-278. </pages> <publisher> IEEE, </publisher> <year> 1973. </year>
Reference-contexts: The search is efficient because it is dynamic programming on partial pre-channel sequence hypotheses, and because all partial hypotheses falling below a threshold offset (a beam) from the best current hypothesis are pruned. This is a Viterbi beam-search (c.f. <ref> [4, 8] </ref>). 3. EXPERIMENTAL RESULTS 3.1. Simple Channel Model This subsection presents results based only on the one-for-one channel model and a back-off bigram language model.
Reference: [5] <author> X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F. Lee, and R. Rosenfeld. </author> <title> The Sphinx-II Speech Recognition System: An Overview. </title> <booktitle> Computer, Speech and Language, </booktitle> <year> 1993. </year>
Reference-contexts: We have used ATIS (airline travel information) data for training the recognizer and TRAINS-95 (train route planning) [1] data for testing. Here are a few examples of the kinds of errors that occur when recognizing spontaneous utterances in the TRAINS-95 domain using Sphinx-II <ref> [5] </ref> running with models trained from ATIS data. In each example, the words tagged REF indicate what was actually said, while those tagged with HYP indicate what the speech recognition (SR) system proposed.
Reference: [6] <author> F. Jelinek. </author> <title> Self-Organized Language Modeling for Speech Recognition. </title> <booktitle> Reprinted in [11]: </booktitle> <pages> 450-506, </pages> <year> 1990. </year>
Reference-contexts: THE MODELS AND ALGORITHM We applied a noisy channel model and adapted techniques from statistical machine translation (c.f. [3]) and statistical speech recognition (c.f. <ref> [2, 6] </ref>) in order to model the errors that Sphinx-II makes in our domain.
Reference: [7] <author> S. M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <booktitle> In IEEE Transactions on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 400-401. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1987. </year>
Reference-contexts: For efficiency and due to sparse data, it is necessary to estimate these distributions with relatively simple models by making independence assumptions. For P [w], we train a word-bigram back-off language model <ref> [7, 10] </ref> from hand-transcribed dialogues previously collected with the TRAINS-95 system.
Reference: [8] <author> B. Lowerre and R. Reddy. </author> <title> The Harpy Speech Understanding System. In Trends in Speech Recognition. </title> <institution> Speech Science Publications, Apple Valley, Minnesota, </institution> <year> 1986. </year> <note> Reprinted in [11]: 576-586. </note>
Reference-contexts: The search is efficient because it is dynamic programming on partial pre-channel sequence hypotheses, and because all partial hypotheses falling below a threshold offset (a beam) from the best current hypothesis are pruned. This is a Viterbi beam-search (c.f. <ref> [4, 8] </ref>). 3. EXPERIMENTAL RESULTS 3.1. Simple Channel Model This subsection presents results based only on the one-for-one channel model and a back-off bigram language model.
Reference: [9] <author> M. Rayner, D. Carter, V. Digalakis, and P. Price. </author> <title> Combining Knowledge Sources to Reorder N -best Speech Hypothesis Lists. </title> <booktitle> In Proceedings ARPA Human Language Technology Workshop, </booktitle> <pages> pages 212-217. ARPA, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Furthermore, the post-processing approach has an advantage over lattice and N-best list rescoring approaches for reducing SR errors: using its channel model, our post-processor can introduce words that are not available in the SR module's output (c.f. <ref> [9] </ref>). In the near future, we plan to pursue the use of word-lattices in place of simple word sequences and expect that they will provide more useful hypotheses to compete in the post-processor's search. 5.
Reference: [10] <author> R. Rosenfeld. </author> <title> The CMU Statistical Language Modeling Toolkit and its use in the 1994 ARPA CSR Evaluation. </title> <booktitle> In Proceedings of the ARPA Spoken Language Systems Technology Workshop, </booktitle> <address> San Mateo, California, </address> <month> January </month> <year> 1995. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For efficiency and due to sparse data, it is necessary to estimate these distributions with relatively simple models by making independence assumptions. For P [w], we train a word-bigram back-off language model <ref> [7, 10] </ref> from hand-transcribed dialogues previously collected with the TRAINS-95 system.
Reference: [11] <author> A. Waibel and K.-F. Lee, </author> <title> editors. </title> <booktitle> Readings in Speech Recog nition. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990. </year>
References-found: 11

