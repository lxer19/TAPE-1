URL: http://www.cs.wisc.edu/~zhichen/papers/jpdc.ps.gz
Refering-URL: http://www.cs.wisc.edu/~zhichen/zhichen.html
Root-URL: 
Title: Semi-Empirical Multiprocessor Performance Predictions  
Author: Zhichen Xu Xiaodong Zhang Lin Sun 
Address: Madison, WI 53706  San Antonio, Texas 78249  Denver, Colorado 80503  
Affiliation: Computer Sciences Department University of Wisconsin Madison  High Performance Computing and Software Laboratory The University of Texas at San Antonio  Cap Gemini America  
Abstract: This paper presents a multiprocessor performance prediction methodology supported by experimental measurements, which predicts the execution time of large application programs on large parallel architectures based on a small set of sample data. We propose a graph model to describe application program behavior. In order to precisely abstract an architecture model for the prediction, important and implicit architecture parameters are obtained by experiments. We focus on performance predictions of application programs in shared-memory and data-parallel architectures. Real world applications are implemented using the shared-memory model on the KSR-1 and using the data-parallel model on the CM-5 for performance measurements and prediction validation. We show that experimental measurements provide strong support for performance predictions on multiprocessors with implicit communications and complex memory systems, such as shared-memory and data-parallel systems, while analytical techniques partially applied in the prediction significantly reduce computer simulation and measurement time.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Kendall Square Research, KSR-1 Technology Background, </institution> <year> 1992. </year>
Reference-contexts: In a thread graph, the thread class and cluster are used to model those language mechanisms that can adapt to system size. An example of such language mechanisms is a parallel region of KSR Fortran <ref> [1] </ref>. A thread class is a template that can be instantiated into different numbers of threads. A thread cluster is instantiated from a thread class and dedicated to a common goal. <p> The parallel architectures we used as the test beds are the KSR-1 <ref> [1] </ref> that supports shared-memory programming model and the CM-5 [2] that supports both message-passing and data-parallel programming models. <p> The problems we used as test seeds are Gauss Elimination (GE), All Pairs Shortest Path (APSP), and a large Electromagnetic Simulation (EM) application [7]. 3.1 Architectural Characteristics 3.1.1 The Shared-Memory KSR-1 The KSR-1 <ref> [1] </ref>, introduced by Kendall Square Research, is a ring-based, cache coherent, shared-memory multiprocessor system with up to 1; 088 64-bit custom superscalar RISC processors (20MHz). A basic ring unit in the KSR-1 has 32 processors. The system uses a two-level hierarchy to interconnect 34 of these rings (1088 processors).
Reference: [2] <author> Thinking Machines Corporation, </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1993. </year>
Reference-contexts: The parallel architectures we used as the test beds are the KSR-1 [1] that supports shared-memory programming model and the CM-5 <ref> [2] </ref> that supports both message-passing and data-parallel programming models. <p> A processor waits for an empty slot to transmit a message. A single bit in the header of the slot identifies it as empty or full as the slots rotate through a ring interface of the processor. 3.1.2 The Connection Machine CM-5 The CM-5 <ref> [2] </ref> is the newest member of the Thinking Machines' Connection Machine family. It is a distributed memory multiprocessor system which can be scaled up to 16K processors and supports both SIMD and MIMD programming models.
Reference: [3] <author> Vikram S. Adve, </author> <title> "Analyzing the behavior and performance of parallel programs", </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Sciences, University of Wisconsin-Madison, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Such a single model is often too simple to describe practically the complexity of application programs and parallel architectures [4]. To have a better control over complex application and architecture behavior, most analytical methods use hierarchical models. In <ref> [3] </ref>, Adve provides a framework for parallel program performance prediction models which well characterizes most of the existing models by a hierarchy of higher and lower level models. In the higher-level component, task graphs [11, 13] are usually used to represent the task-level behavior of the program. <p> The effect of each factor in reducing the speedup is expressed as a multiplicative efficiency factor. For a fairly general class of parallel programs, the hierarchical model can be solved without iterating among the submodels. Adve <ref> [3] </ref> develops a simple deterministic model for parallel program performance prediction. A task-graph-based representation is used to represent both program parallelism as well as scheduling. A graph solution algorithm is used to estimate the parallel execution time of a program.
Reference: [4] <author> M. J. Clement and M. J. Quinn, </author> <title> "Analytical performance prediction on multicomputers", </title> <booktitle> Supercomputing 93, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1993, </year> <pages> pp. 886-894. </pages>
Reference-contexts: Analytical methods that use a single model to abstract both the application and the architecture usually construct a single overhead function to capture the overall overheads inherent in both parts. Such a single model is often too simple to describe practically the complexity of application programs and parallel architectures <ref> [4] </ref>. To have a better control over complex application and architecture behavior, most analytical methods use hierarchical models. In [3], Adve provides a framework for parallel program performance prediction models which well characterizes most of the existing models by a hierarchy of higher and lower level models.
Reference: [5] <author> T. Fahringer and H. Zima, </author> <title> "A static parameter based performance prediction tool for parallel programs", </title> <booktitle> Proceedings of 1993 International Conference on Supercomputing, </booktitle> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993, </year> <pages> pp. 207-219. </pages>
Reference: [6] <author> A. Kapelnikov, R. R. Muntz, and M. D. Ercegovac, </author> <title> "A modeling methodology for the analysis of concurrent systems and computations". </title> <journal> Journal of Parallel Distributed Computing, </journal> <volume> Vol. 6, </volume> <year> 1989, </year> <pages> pp. 568-597. </pages>
Reference-contexts: Part of the experiments were conducted on the CM-5 machines in Los Alamos National Laboratory and in the National Center for Supercomputing Applications at the University of Illinois, and on the KSR-1 machines at Cornell University and at the University of Washington. analytical techniques <ref> [6] </ref>. Simulation is quite expensive in terms of its consumption of computing resources, and it may be more suitable to focus on studying a certain part of architecture and system performance. <p> There is no internal parallelism inside a task, and a task must be executed sequentially. This higher-level model component computes the overall execution time assuming individual task execution times are known. The lower-level components represent system-level effects, and are usually simulated by stochastic processes <ref> [6, 13] </ref> or by some type of system overhead function [10]. Individual task execution times are computed from lower-level components. Thomasian and Bay [13] propose a two-level model for a class of programs which can be represented by directed acyclic graphs. <p> The model proposed in this paper focuses on a boarder categories of applications. The hierarchical model in this paper distinguishes deterministic factors from non-deterministic performance factors, and implicit communications from explicit communications. Both analytic and experimental methods are combined performance prediction. Kapelnikov, Muntz and Ercegovac <ref> [6] </ref> propose a methodology that embodies two modeling domains: the program domain and the physical domain. In the program domain, a graphical model called a computation control graph is used to model a program's structure.
Reference: [7] <author> Y. Lu, et. al., </author> <title> "Implementation of electromagnetic scattering from conductors containing loaded slots on the Connection Machine CM-2", </title> <booktitle> Proceeding of the 6th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <publisher> SIAM Press, </publisher> <year> 1993, </year> <pages> pp. 216-220. </pages>
Reference-contexts: The problems we used as test seeds are Gauss Elimination (GE), All Pairs Shortest Path (APSP), and a large Electromagnetic Simulation (EM) application <ref> [7] </ref>. 3.1 Architectural Characteristics 3.1.1 The Shared-Memory KSR-1 The KSR-1 [1], introduced by Kendall Square Research, is a ring-based, cache coherent, shared-memory multiprocessor system with up to 1; 088 64-bit custom superscalar RISC processors (20MHz). A basic ring unit in the KSR-1 has 32 processors. <p> Connected by the microwave network, the electromagnetic fields in the two slots interact with each other, creating two equivalent magnetic current sources in the slots so that a new scattered EM field is formed above the slots. The well-known moment method <ref> [7] </ref> is used for the numerical model and simulation. First, the loaded slots are imitated. Second, an equivalent admittance matrix of region "A" is calculated by using the pulse basis mode function expansion. <p> The parameters have a direct impact on the computational requirements and simulation resolution of the moment method. For detailed information on the numerical method and implementations, the interested reader may refer to <ref> [7] </ref> and [16]. The shared-memory implementation of the EM program is mainly constructed by a thread cluster (see Figure 4). All edges except edge h5; 6i are loops. Edge h5; 6i corresponds to a critical section. At the end of each segment, there is a barrier.
Reference: [8] <author> V. W. Mak, and S. F. Lundstrom, </author> <title> "Predicting performance of parallel computations", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 1. No.3, </volume> <month> July </month> <year> 1990, </year> <pages> pp. 257-269. 26 </pages>
Reference: [9] <author> D. A. Menas~ce, and L. A. Barroso, </author> <title> "A methodology for performance evaluation of parallel applications on multiprocessors", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 14, No. 1, </volume> <year> 1992, </year> <pages> pp. 1-14. </pages>
Reference: [10] <author> D. Menas~ce, S. H. Noh, S. K. Tripathi, </author> <title> "A methodology for performance prediction of massively parallel applications", </title> <booktitle> Proc. of the Fifth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1993, </year> <pages> pp. 250-257. </pages>
Reference-contexts: This higher-level model component computes the overall execution time assuming individual task execution times are known. The lower-level components represent system-level effects, and are usually simulated by stochastic processes [6, 13] or by some type of system overhead function <ref> [10] </ref>. Individual task execution times are computed from lower-level components. Thomasian and Bay [13] propose a two-level model for a class of programs which can be represented by directed acyclic graphs. <p> In this paper we propose a similar graph traversal algorithm for our thread graph representation of a program. Instead of solving the system-level model on the fly, most of the non-deterministic factors are taken care of through a lower-level model before the graph traversal begins. Menas~ce, Noh and Tripathi <ref> [10] </ref> also use a task graph for their higher-level model component. Instead of using the Markov chain and queueing network, Menas~ce's methodology estimates the execution time of the parallel program based on the interdependency between network delay and program execution. In [10], network delay is modeled as a function of the <p> Menas~ce, Noh and Tripathi <ref> [10] </ref> also use a task graph for their higher-level model component. Instead of using the Markov chain and queueing network, Menas~ce's methodology estimates the execution time of the parallel program based on the interdependency between network delay and program execution. In [10], network delay is modeled as a function of the network message injection rate, which in turn depends on the total communication demand and estimated execution time of the program. The estimated execution time is affected by the network delay.
Reference: [11] <author> J. Mohan, </author> <title> Performance of parallel programs: Model and analyses, </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie-Mellon University., </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: In [3], Adve provides a framework for parallel program performance prediction models which well characterizes most of the existing models by a hierarchy of higher and lower level models. In the higher-level component, task graphs <ref> [11, 13] </ref> are usually used to represent the task-level behavior of the program. A task graph is a directed acyclic graph in which each vertex represents a task and each edge represents the precedence relationship between a pair of tasks.
Reference: [12] <author> X.-H. Sun and J. Zhu, </author> <title> "Performance prediction of scalable computing: a case study", </title> <booktitle> Proceedings of the 28th Hawaii International Conference on System Sciences, </booktitle> <volume> Vol. II, </volume> <publisher> IEEE Computer Society Press, </publisher> <month> January, </month> <year> 1995, </year> <pages> pp. 456-465. </pages>
Reference: [13] <author> A. Thomasian and P. F. </author> <title> Bay, "Analytic queueing network models for parallel processing of task systems", </title> <journal> IEEE Transaction on Computers, </journal> <volume> Vol. C-35, No. 12, </volume> <year> 1986, </year> <pages> pp. 1045-1054. </pages>
Reference-contexts: In [3], Adve provides a framework for parallel program performance prediction models which well characterizes most of the existing models by a hierarchy of higher and lower level models. In the higher-level component, task graphs <ref> [11, 13] </ref> are usually used to represent the task-level behavior of the program. A task graph is a directed acyclic graph in which each vertex represents a task and each edge represents the precedence relationship between a pair of tasks. <p> There is no internal parallelism inside a task, and a task must be executed sequentially. This higher-level model component computes the overall execution time assuming individual task execution times are known. The lower-level components represent system-level effects, and are usually simulated by stochastic processes <ref> [6, 13] </ref> or by some type of system overhead function [10]. Individual task execution times are computed from lower-level components. Thomasian and Bay [13] propose a two-level model for a class of programs which can be represented by directed acyclic graphs. <p> The lower-level components represent system-level effects, and are usually simulated by stochastic processes [6, 13] or by some type of system overhead function [10]. Individual task execution times are computed from lower-level components. Thomasian and Bay <ref> [13] </ref> propose a two-level model for a class of programs which can be represented by directed acyclic graphs. At the higher level, the system behavior is specified by a Markov chain whose states correspond to the combination of tasks in execution. <p> Using the same approach, we predict the execution performance of a program with a larger number of iterations based on the performance of the same program with a small number of iterations. However, the method in <ref> [13] </ref> focuses on iterative algorithms and models the decomposition of a program into processes by using pure analytic functions. The model proposed in this paper focuses on a boarder categories of applications.
Reference: [14] <author> Thin-Fong Tsuei and Mary K. Vernon, </author> <title> "Diagnosing parallel Program speedup limitations using resource contention models", </title> <booktitle> Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <month> August 13-17, </month> <year> 1990, </year> <month> pp.I-185-189. </month>
Reference-contexts: Tsuei and Vernon <ref> [14] </ref> propose a hierarchical analytical model to evaluate the relative importance of factors that can limit speedup on MIMD shared memory multiprocessors. Specifically, they use the hierarchical model to estimate the relative impact of software structure, lock contention, and hardware resource contention on speedup.
Reference: [15] <author> D. F. Vrsalovic, D. P. Siewiorek, Z. Z. Segall, and E. F. Gehringer, </author> <title> "Performance prediction and Calibration for a class of multiprocessors", </title> <journal> IEEE Transaction on Computers, </journal> <volume> Vol. 37, No. 11, </volume> <month> November </month> <year> 1988, </year> <pages> pp. 1353-1365. </pages>
Reference-contexts: At the lower level, the transition rates among the states of the Markov chain are computed using a queueing network solver, which determines the throughput of the computer system for each system state. Vrsalovic, Siewiorek, Segall and Gehringer <ref> [15] </ref> develop an analytic model for predicting the performance of iterative algorithms. Using the same approach, we predict the execution performance of a program with a larger number of iterations based on the performance of the same program with a small number of iterations.
Reference: [16] <author> X. Zhang and L. Sun, </author> <title> "Comparative evaluation and case studies of shared-memory and data-parallel execution patterns on KSR-1 and CM-5", </title> <note> to appear in Scientific Programming. </note>
Reference-contexts: The parameters have a direct impact on the computational requirements and simulation resolution of the moment method. For detailed information on the numerical method and implementations, the interested reader may refer to [7] and <ref> [16] </ref>. The shared-memory implementation of the EM program is mainly constructed by a thread cluster (see Figure 4). All edges except edge h5; 6i are loops. Edge h5; 6i corresponds to a critical section. At the end of each segment, there is a barrier.
Reference: [17] <author> X. Zhang, Y. Yan and K. </author> <title> He, "Evaluation and measurement of multiprocessor latency patterns", </title> <booktitle> Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> April, </month> <year> 1994, </year> <pages> pp. 845-852. </pages>
Reference: [18] <author> X. Zhang, Z. Xu and L. Sun, </author> <title> Performance Modeling and Implications of the Fat-Tree Networks and the CM-5 Architecture, </title> <type> Technical Report, </type> <institution> High Performance Computing and Software laboratory, The University of Texas at San Antonio, </institution> <month> July </month> <year> 1994. </year> <month> 27 </month>
References-found: 18

