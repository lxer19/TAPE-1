URL: ftp://sound.media.mit.edu/pub/Papers/dpwe-keele96.ps.gz
Refering-URL: http://sound.media.mit.edu/papers.html
Root-URL: http://www.media.mit.edu
Email: email: dpwe@icsi.berkeley.edu  
Title: PREDICTION-DRIVEN COMPUTATIONAL AUDITORY SCENE ANALYSIS FOR DENSE SOUND MIXTURES  
Author: Daniel P. W. Ellis 
Address: Berkeley CA 94704 U.S.A.  
Affiliation: International Computer Science Institute  
Date: July 1996  
Note: to be presented at the ESCA workshop on the Auditory Basis of Speech Perception, Keele UK,  
Abstract: We interpret the sound reaching our ears as the combined effect of independent, sound-producing entities in the external world; hearing would have limited usefulness if were defeated by overlapping sounds. Computer systems that are to interpret real-world sounds for speech recognition or for multimedia indexing must similarly interpret complex mixtures. However , existing functional models of audition employ only data-driven processing incapable of making context-dependent inferences in the face of interference. We propose a prediction-driven approach to this problem, raising numerous issues including the need to represent any kind of sound, and to handle multiple competing hypotheses. Results from an implementation of this approach illustrate its ability to analyze complex, ambient sound scenes that would confound previous systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. S. Bregman. </author> <title> Auditory Scene Analysis. </title> <publisher> MIT Press 1990. </publisher>
Reference-contexts: This title acknowledges that the work is founded on experimental and theoretical results from psychoacoustics, such as are described in Bregmans book Auditory Scene Analysis <ref> [1] </ref>. Several important projects in this area have focused on the problem of separating speech from interfering noise (either unwanted speech [2] or more general interfer - ence [3,4]). <p> One well-known instance of such an effect is the continuity illusion discussed in <ref> [1] </ref>, and also used as the motivation for a specific refinement of Browns CASA system in [6]. A simple version of this illusion is illustrated in figure 2, the spectrogram of an example from a set of auditory demonstrations [7]. The sound analysis system of [4]. <p> visible on both sides of the noise burst leads to the inference that it was probably present all the time, even though its presence could not be confirmed directly during the noise. (The precise conditions under which the continuity illusion will occur have been studied extensively and are described in <ref> [1] </ref>). An interesting point to note is that the listener truly hears the tone, i.e. by the time the perception reaches levels of conscious introspection, there is no distinction between a percept based on direct acoustic evidence and one merely inferred from higher-level context.
Reference: [2] <author> M. Weintraub. </author> <title> A theory and computational model of auditory monaural sound separation, </title> <type> Ph.D. thesis, </type> <institution> Dept. of Elec. Eng., Stanford Univ., </institution> <year> 1985. </year>
Reference-contexts: This title acknowledges that the work is founded on experimental and theoretical results from psychoacoustics, such as are described in Bregmans book Auditory Scene Analysis [1]. Several important projects in this area have focused on the problem of separating speech from interfering noise (either unwanted speech <ref> [2] </ref> or more general interfer - ence [3,4]). These approaches may be characterized as data-driven, that is, relying exclusively on locally-defined features present in the input data to create an output through a sequence of transformations.
Reference: [3] <author> M. P. Cooke. </author> <title> Modeling auditory processing and organisation, </title> <type> Ph.D. thesis, </type> <institution> Dept. of Comp. Sci., Univ. of Sheffield, </institution> <year> 1991. </year>
Reference: [4] <author> G. J. Brown. </author> <title> Computational auditory scene analysis: A representational approach, </title> <type> Ph.D. thesis CS-92-22, </type> <institution> Dept. of Comp. Sci., Univ. of Sheffield, </institution> <year> 1992. </year>
Reference-contexts: An equivalent term for this kind of system is bottom-up, meaning that information flows monotonically from low-level, concrete representations to successively sophisticated abstractions. A typical example is illustrated in figure 1, a block diagram description of the system presented in <ref> [4] </ref>. The assumption underlying this kind of processing is that the auditory system isolates individual sounds by applying sophisticated signal processing, resulting in the emergence of integrated sound events, grouped by one or more intrinsic cues such as common onset or periodicity. <p> A simple version of this illusion is illustrated in figure 2, the spectrogram of an example from a set of auditory demonstrations [7]. The sound analysis system of <ref> [4] </ref>. Cues in the input sound are detected by the front end and used to create a representation in terms of objects patches of time-frequency with consistent characteristics.
Reference: [5] <author> A. S. Bregman. </author> <title> Psychological data and computational ASA, in Readings in Computational Auditory Scene Analysis, </title> <editor> ed. H. Okuno & D. F. Rosenthal, </editor> <publisher> Lawrence Erlbaum, </publisher> <year> 1996. </year>
Reference-contexts: on the wider context of the stimulus or other information; this kind of behavior is often termed top-down processing, and to the extent that it is central to real audition we must strive to include it in our computational models. (The case for top-down auditory models is powerfully presented in <ref> [5] </ref>). One well-known instance of such an effect is the continuity illusion discussed in [1], and also used as the motivation for a specific refinement of Browns CASA system in [6].
Reference: [6] <author> M. Cooke, G. Brown. </author> <title> Computational auditory scene analysis: Exploiting principles of perceived continuity, Speech Communication 13, </title> <booktitle> 1993, </booktitle> <pages> 391-399. </pages>
Reference-contexts: One well-known instance of such an effect is the continuity illusion discussed in [1], and also used as the motivation for a specific refinement of Browns CASA system in <ref> [6] </ref>. A simple version of this illusion is illustrated in figure 2, the spectrogram of an example from a set of auditory demonstrations [7]. The sound analysis system of [4].
Reference: [7] <author> A. J. M. Houtsma, T. D. Rossing, W. M. Wagenaars. </author> <title> Auditory Demonstrations, Audio CD, </title> <publisher> Philips/Acous. Soc. </publisher> <address> Am., </address> <year> 1987. </year>
Reference-contexts: A simple version of this illusion is illustrated in figure 2, the spectrogram of an example from a set of auditory demonstrations <ref> [7] </ref>. The sound analysis system of [4]. Cues in the input sound are detected by the front end and used to create a representation in terms of objects patches of time-frequency with consistent characteristics.
Reference: [8] <author> R. M. Warren, </author> <title> Perceptual restoration of missing speech sounds, </title> <booktitle> Science 167, </booktitle> <year> 1970. </year>
Reference-contexts: Thus, an alternative perspective on the continuity illusion is that it isn t an illusion at all, but rather a systematic bias towards a particular kind of interpretation in a genuinely ambiguous situation. An even more dramatic instance of inference in hearing is phonemic restoration first noted in <ref> [8] </ref>. In this phenomenon, a short stretch of a speech recording is excised and replaced by a noisy masking signal such as a loud cough.
Reference: [9] <author> D. P. W. Ellis, D. F. Rosenthal. </author> <title> Mid-level representations for Computational Auditory Scene Analysis, in Readings in Computational Auditory Scene Analysis, </title> <editor> ed. H. Okuno & D. F. Rosenthal, </editor> <publisher> Lawrence Erlbaum, </publisher> <year> 1996. </year>
Reference-contexts: Sound elements & abstractions: The bottom level of the internal world model is in terms of a mid-level representation of generic sound elements <ref> [9] </ref>. In view of the requirement for complete explanation, these elements must encompass any sound that the system could encounter, while at the same time imposing the source-oriented structure that is the tar get of the analysis. <p> Wefts are the elements used to represent wideband peri odic energy in this implementation <ref> [9] </ref> (weft is the Anglo-Saxon word for the parallel fibers running the length of a woven cloth i.e. the part that is not the warp). <p> As described in more detail in <ref> [9] </ref> and [10], the extraction of weft elements starts from common-period features detected in the periodogram, but then refers back to the correlogram to estimate the ener gy contribution of a given periodic pro cess in every frequency channel.
Reference: [10] <author> D. P. W. Ellis. </author> <title> Prediction-driven Computational Auditory Scene Analysis, </title> <type> Ph.D. thesis, </type> <institution> Dept. of Elec. Eng. and Comp. Sci., M.I.T., </institution> <year> 1996. </year>
Reference-contexts: Of course, so far this is only a rough sketch of an architecture rather than a narrow specification. In the next section we present some details of our initial implementation. 3. AN IMPLEMENTATION We have recently completed an implementation of a CASA system based on the prediction-driven approach <ref> [10] </ref>. The system follows the block diagram of figure 3, except that the world model is shallow, lacking any significant abstractions above the level of the basic sound elements. <p> As described in more detail in [9] and <ref> [10] </ref>, the extraction of weft elements starts from common-period features detected in the periodogram, but then refers back to the correlogram to estimate the ener gy contribution of a given periodic pro cess in every frequency channel. <p> To address this uncertainty, subjective listening tests were performed in which listeners were asked to indicate the times of the dif ferent events they could distinguish, as well as giving a label to each one <ref> [10] </ref>. The summaries of these responses are displayed as the horizontal bars on figure 6; the solid bars connect average onset and offset times, with the gray extensions indicating the maximum extents. The bars are labeled with a title summarizing the subjects consensus as to the events iden-4.
Reference: [11] <author> M. Slaney. </author> <title> An efficient implementation of the Patterson-Holdsworth auditory filterbank, </title> <type> Technical report #35, </type> <institution> Apple Computer Co. </institution>
Reference-contexts: Therefore, one output of the front-end is a smoothed time-frequency intensity envelope derived from a simple, linear filter model of the cochlea <ref> [11] </ref>. The smoothing involved in producing this envelope, however , removes all the fine structure present in the individual frequency channels, fine structure that is demonstrably significant in many aspects of auditory perception.
Reference: [12] <author> R. O. Duda, R. F. Lyon, M. Slaney. </author> <title> Correlograms and the separation of sounds, </title> <booktitle> Proc. IEEE Asilomar Conf. on Sigs., Sys., </booktitle> & <address> Comps., </address> <year> 1990. </year>
Reference: [13] <author> M. Slaney, R. F. Lyon. </author> <title> On the importance of time A temporal representation of sound, in Visual Representations of Speech Signals, </title> <editor> ed. M. Cooke, S. Beet & M. Crawford, </editor> <publisher> John Wiley, </publisher> <year> 1992. </year>
Reference: [14] <author> D. K. Mellinger. </author> <title> Event formation and separation in musical sound, </title> <type> Ph.D. thesis, </type> <institution> CCRMA, Stanford Univ. </institution>
Reference-contexts: the correlogram, but the construction of elements to represent wideband periodicity in the input requires reference back to the full detail in the time vs. frequency vs. lag volume. 3.2 Generic sound elements Previous models of auditory source separation, such as the ones mentioned in the introduction, as well as <ref> [14] </ref> and our own previous work [15], have tended to concentrate on periodic sounds (such as pitched musical instruments or voiced speech) as a very significant subset of the sonic world with a promising basis for separation the common period of modulation.
Reference: [15] <author> D. P. W. Ellis. </author> <title> A computer implementation of psychoacoustic grouping rules, </title> <booktitle> Proc. 12th Intl. Conf. on Pattern Recog., </booktitle> <address> Jerusalem, </address> <year> 1994. </year>
Reference-contexts: elements to represent wideband periodicity in the input requires reference back to the full detail in the time vs. frequency vs. lag volume. 3.2 Generic sound elements Previous models of auditory source separation, such as the ones mentioned in the introduction, as well as [14] and our own previous work <ref> [15] </ref>, have tended to concentrate on periodic sounds (such as pitched musical instruments or voiced speech) as a very significant subset of the sonic world with a promising basis for separation the common period of modulation.
Reference: [16] <author> N. Carver, V. Lesser. </author> <title> Blackboard systems for knowledge-based signal understanding, in Symbolic and Knowledge-based Signal Processing, </title> <editor> ed. A. Oppenheim and S. Nawab, </editor> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference: [17] <author> J. M. Winograd, S. H. Nawab. </author> <title> A C++ software environment for the development of embedded signal processing systems, </title> <booktitle> Proc. Intl. Conf. on Acous., Speech & Sig. Proc., </booktitle> <address> Detroit, </address> <year> 1995. </year>
Reference: [18] <author> J. R. Quinlan, R. L. Rivest. </author> <title> Inferring decision trees using the Minimum Description Length principle, </title> <booktitle> Information and Computation 80(30), </booktitle> <year> 1989, </year> <pages> 227-248. </pages>
Reference-contexts: Computational resource is conserved by pursuing only the most highly rated partial solutions. In the implementation, ratings were calculated based upon a minimum-description-length (MDL) parameter which estimated the total length of a code required to represent the input signal using the model implicit in the hypothesis <ref> [18] </ref>. Formally equivalent to Bayesian analysis, MDL permits the integration of model complexity, model parameterization complexity, and goodness-of-fit into a single number and comprised a consistent theoretical basis for ratings assigned to otherwise dis parate hypotheses. speaking bad dog. analysis (top), system-generated elements and subjective event labels.
Reference: [19] <institution> Aware, Inc. Speed-of-sound Megadisk CDROM #1: Sound effects, Computer CDROM, </institution> <year> 1993. </year>
Reference-contexts: The second example is of the kind of dense, ambient sound scene that originally motivated the system. Figure 6 illustrates the systems analysis of the Construction sound example, a 10-second extract of constructionsite ambience from a sound-effects CDROM <ref> [19] </ref>. The time-frequency intensity envelope of the original sound gives some idea of the complexity of this example, with mixtures of background noises, clicks and bangs. The systems analysis was composed of a total of 20 elements as illustrated.
References-found: 19

