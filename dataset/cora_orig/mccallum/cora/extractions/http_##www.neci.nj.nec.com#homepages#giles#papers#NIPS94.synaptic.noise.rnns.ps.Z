URL: http://www.neci.nj.nec.com/homepages/giles/papers/NIPS94.synaptic.noise.rnns.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: 
Email: fkamjim,horne,gilesg@research.nj.nec.com  
Title: Effects of Noise on Convergence and Generalization in Recurrent Networks  
Author: Kam Jim Bill G. Horne C. Lee Giles 
Address: 4 Independence Way, Princeton, NJ 08540  
Affiliation: NEC Research Institute, Inc.,  
Abstract: We introduce and study methods of inserting synaptic noise into dynamically-driven recurrent neural networks and show that applying a controlled amount of noise during training may improve convergence and generalization. In addition, we analyze the effects of each noise parameter (additive vs. multiplicative, cumulative vs. non-cumulative, per time step vs. per string) and predict that best overall performance can be achieved by injecting additive noise at each time step. Extensive simulations on learning the dual parity grammar from temporal strings substantiate these predictions.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chris M. Bishop. </author> <title> Training with noise is equivalent to Tikhonov Regularization. </title> <booktitle> Neural Computation, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference: [2] <author> Robert M. Burton, Jr. and George J. Mpitsos. </author> <title> Event-dependent control of noise enhances learning in neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 627-637, </pages> <year> 1992. </year>
Reference: [3] <author> C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference: [4] <author> Stephen Jose Hanson. </author> <title> A stochastic version of the delta rule. </title> <journal> Physica D., </journal> <volume> 42 </volume> <pages> 265-272, </pages> <year> 1990. </year>
Reference: [5] <author> Kam Jim, C.L. Giles, and B.G. </author> <title> Horne. Synaptic noise in dynamically-driven recurrent neural networks: Convergence and generalization. </title> <institution> Technical Report UMIACS-TR-94-89 and CS-TR-3322, Institute for Advanced Computer Studies, University of Maryland, College Park, MD, </institution> <year> 1994. </year>
Reference: [6] <author> Stephen Judd and Paul W. Munro. </author> <title> Nets with unreliable hidden nodes learn error-correcting codes. </title> <editor> In S.J Hanson, J.D. Cowan, and C.L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 89-96, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [7] <author> Anders Krogh and John A. Hertz. </author> <title> A simple weight decay can improve generalization. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 450-957, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [8] <author> Alan F. Murray and Peter J. Edwards. </author> <title> Synaptic weight noise during multilayer perceptron training: Fault tolerance and training improvements. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 4(4) </volume> <pages> 722-725, </pages> <year> 1993. </year>
Reference: [9] <author> Carlo H. Sequin and Reed D. Clay. </author> <title> Fault tolerance in artificial neural networks. </title> <booktitle> In Proc. of IJCNN, </booktitle> <volume> volume I, </volume> <pages> pages I-703-708, </pages> <year> 1990. </year>
References-found: 9

