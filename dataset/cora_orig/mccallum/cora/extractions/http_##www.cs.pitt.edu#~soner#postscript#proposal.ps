URL: http://www.cs.pitt.edu/~soner/postscript/proposal.ps
Refering-URL: http://www.cs.pitt.edu/~soner/paper-guide.html
Root-URL: 
Title: Data Forwarding: A Program Execution Paradigm for Achieving Improved Data Locality and Increased Parallelism  
Author: Soner Onder 
Degree: Ph.D. Dissertation Proposal  
Date: Nov 9, 1995  
Address: Pittsburgh  
Affiliation: Department of Computer Science University of  
Abstract: Modern processors achieve high performance through caching of instructions and data as well as techniques such as dynamic instruction scheduling and multi-threading. However, there is still a significant room for improvement especially in the areas of data caching and instruction level parallelism. This work presents a novel execution style called data forwarding which creates and exploits data locality through dynamic mapping of data values in memory and by coupling the prefetching of instructions with their data operands. Data forwarding is applicable to sequential, pipelined, superscalar and multi-threaded data flow computing. Not only is high performance achieved by hiding the memory latency and reducing cache misses, but the cache misses that occur are predictable for both data and instructions. Moreover, the technique allows dynamic reordering of instructions over a much larger instruction window than conventional superscalar architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arvind and R.A. </author> <title> Iannucci. Two fundamental issues in multiprocessing. Computation Structures Group Memo 26, </title> <institution> Laboratory for Computer Science, MIT, </institution> <year> 1987. </year>
Reference-contexts: A significant step in the introduction of dataflow based multi-threading was the P-RISC idea proposed by Nikhil and Arvind [36]. In a very clear and simple model, they showed how the two fundamental problems in multiprocessing <ref> [1] </ref>, namely the latency and the cost of synchronization, can be handled using a simple model of sequential threads, fork, and join operations. The P-RISC idea continues to be an important one and evolved with the implementation of *T machine [38].
Reference: [2] <author> J. Baer and T. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <type> Technical Report 91-03-07, </type> <institution> University of Washington, Department of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: Smith [47] notes an increase in the memory traffic accompanied by a decrease in the miss rate when prefetching is employed. Complex hardware prefetch strategies are reported not to be very successful [44]. According to the analysis by Baer <ref> [2] </ref>, decoupled architectures perform an implicit form of prefetching. This is because, the unit that performs data accesses can be ahead of the unit that is processing the functional operations. This form of prefetching becomes very effective for instruction caches, since the predictability of the instruction stream is high.
Reference: [3] <author> T. Ball and S. Horwitz. </author> <title> Constructing control flow from control dependence. </title> <type> Technical Report 1091, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1992. </year>
Reference-contexts: One of the widely used program representations is the program dependence graph (PDG) [16]. A PDG makes explicit for both the data and the control dependences for each operation in a program. Ball and Horwitz <ref> [3] </ref> give algorithms to reconstruct a control flow graph from a control dependence graph such as PDG. Beck and Pingali [4] illustrate how an imperative program which naturally bases on control flow can be converted to a data flow graph for execution on a dataflow processor.
Reference: [4] <author> M. Beck and K. Pingali. </author> <title> From control flow to dataflow. </title> <type> Technical Report 89-1050, </type> <institution> Cornell University, Department of Computer Science, </institution> <year> 1989. </year>
Reference-contexts: A PDG makes explicit for both the data and the control dependences for each operation in a program. Ball and Horwitz [3] give algorithms to reconstruct a control flow graph from a control dependence graph such as PDG. Beck and Pingali <ref> [4] </ref> illustrate how an imperative program which naturally bases on control flow can be converted to a data flow graph for execution on a dataflow processor. Selke [45, 46] provides a semantic basis for PDG transformations and extend on the work of Horwitz [24].
Reference: [5] <author> R. Bodik and R. Gupta. </author> <title> Array data-flow analysis for load-store optimizations in superscalar architectures. </title> <booktitle> In Eighth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Columbus, Ohio, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: As we can see, the number of A cache references is significantly reduced by the above optimization. There are techniques for applying the above transformation using array dependence analysis <ref> [5] </ref>. Enhancing A-cache Locality: For programs in which the pattern of references for array is regular and predictable at compile-time, techniques have been developed to enhance the locality of the cache. Furthermore, prefetching techniques can be applied to effectively fetch portions of arrays that are referenced within loops.
Reference: [6] <author> B. K. Bray. </author> <title> Specialized caches to improve data access performance. </title> <type> Technical Report CSL-TR93-574, </type> <institution> Stanford University, Computer Systems Laboratory, </institution> <year> 1993. </year> <month> 68 </month>
Reference-contexts: Hardware prefetching caches usually employ one block lookahead (OBL) policy, which prefetches block i+1 when block i is referenced. Because of the ubiquity of problems in designing caches, many other techniques such as sub-word caches, tag caches, write caches, fetch caches are also utilized <ref> [6] </ref>. Interestingly, some of the hardware techniques can be realized in software without a significant overhead when data forwarding is employed.
Reference: [7] <author> D. Callahan, K. Kennedy, and A. Portefield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 40-52, </pages> <year> 1991. </year>
Reference-contexts: There are many techniques currently employed to improve the efficiency of memory hierarchies. These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching <ref> [7, 9, 17, 34, 35, 29] </ref>, lock-up free and interleaved caches [30, 48] or software techniques such as blocked algorithms [31] are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement. <p> In this case, the upper level will never miss and a perfect memory hierarchy will be obtained which operates with the speed of the top level, while offering a size as big as the lowest level in the hierarchy. Prefetching <ref> [7] </ref> [9] [35] may be done in the hardware, software or a combination of both. <p> Fu and Patel [17] study prefetching for vector processors. Chen and Baer [9] show through experiments that prefetching caches outperform non-blocking caches in many applications but a hybrid design is better. Callahan and Kennedy extend the work of Porterfield for reducing the cost of prefetching <ref> [7] </ref>. Use of a cache load instruction for array references will benefit data forwarding as well.
Reference: [8] <author> P. Y. Chang, D. R. Kaeli, and Y. Liu. </author> <title> Branch-directed data cache prefetching. </title> <booktitle> In Proceedings of the 2nd Annual Workshop on Shared-Memory Multiprocessor Systems, </booktitle> <year> 1994. </year>
Reference-contexts: Branch directed prefetching associates data references 12 with branch instructions which eliminate the need for a separate table to establish the association, since branch target buffers can be used for this purpose. Branch directed prefetching was proposed by Chang, Kaeli and Liu <ref> [8] </ref>. Simulation results show that by associating instruction references and the data stream, unnecessary prefetches can be greatly reduced. The success of branch directed prefetching is indeed very significant from the data forwarding perspective.
Reference: [9] <author> T. Chen and J. Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 51-61, </pages> <year> 1992. </year>
Reference-contexts: There are many techniques currently employed to improve the efficiency of memory hierarchies. These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching <ref> [7, 9, 17, 34, 35, 29] </ref>, lock-up free and interleaved caches [30, 48] or software techniques such as blocked algorithms [31] are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement. <p> In this case, the upper level will never miss and a perfect memory hierarchy will be obtained which operates with the speed of the top level, while offering a size as big as the lowest level in the hierarchy. Prefetching [7] <ref> [9] </ref> [35] may be done in the hardware, software or a combination of both. <p> Klaiber and Levy [29] examine in detail the hardware support necessary for the cache load instruction and extend on work of Porterfield. Mowry et al [34] study the effects of software controlled prefetching in a multiprocessing environment. Fu and Patel [17] study prefetching for vector processors. Chen and Baer <ref> [9] </ref> show through experiments that prefetching caches outperform non-blocking caches in many applications but a hybrid design is better. Callahan and Kennedy extend the work of Porterfield for reducing the cost of prefetching [7]. Use of a cache load instruction for array references will benefit data forwarding as well.
Reference: [10] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by constrcution or approximation of fixpoints. </title> <booktitle> In Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <year> 1977. </year>
Reference-contexts: Whether or not a representation is directly executable, it must represent the control and the data dependencies of the program. An executable representation allows abstract interpretation of the program which may be useful in designing analysis algorithms and developing proofs of correctness <ref> [10] </ref>. When a program representation is used for optimizations, certain properties of the representation make it more suitable for optimizations. For example, some representations transform loops to tail recursive functions. This transformation makes it difficult to apply optimizations such as loop interchange [42].
Reference: [11] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Efficiently comput-ing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9 </volume> <pages> 319-345, </pages> <year> 1987. </year>
Reference-contexts: Pingali gives a program representation called dependence flow graph (DFG) which is based on a dependence driven execution model [42]. Johnson shows how distance and direction can be represented using dependence flow graphs [27]. The SSA form <ref> [11, 12] </ref> plays an important role in optimizing compilers. This form ensures that 17 each use of a variable is reached by exactly one assignment to that variable. This property makes the representation efficient and easier to deal with a large classes of optimizations.
Reference: [12] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the 16th ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1989. </year>
Reference-contexts: Pingali gives a program representation called dependence flow graph (DFG) which is based on a dependence driven execution model [42]. Johnson shows how distance and direction can be represented using dependence flow graphs [27]. The SSA form <ref> [11, 12] </ref> plays an important role in optimizing compilers. This form ensures that 17 each use of a variable is reached by exactly one assignment to that variable. This property makes the representation efficient and easier to deal with a large classes of optimizations.
Reference: [13] <author> J. B. Dennis. </author> <title> The evolution of "static" data-flow architecture. </title> <editor> In J. Gaudiot and L. Bic, editors, </editor> <booktitle> Advanced Topics in Data-Flow Computing, </booktitle> <pages> pages 35-91. </pages> <address> New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Therefore, there are no associated bookkeeping costs, whereas branch directed prefetching has to rely on a hardware table to keep a list of referenced data elements. 2.2 Data Flow Machines Static dataflow architecture was based on the original design by Dennis and Misuanas [15]. According to Dennis <ref> [13] </ref>, "the guiding principle has been to design hardware systems that would faithfully implement the semantics of computations expressed as dataflow graphs. The original hardware concept envisioned instruction templates having spaces to receive the operands of the instructions".
Reference: [14] <author> J. B. Dennis and G. R. Gao. </author> <title> An efficient pipelined dataflow processor architecture. </title> <booktitle> In Proceedings of the IEEE and ACM SIGARCH Conf. on Supercomputing, </booktitle> <year> 1988. </year>
Reference-contexts: For example, the argument fetch machine proposed by Dennis and Gao <ref> [14, 18] </ref> is dataflow from the instruction scheduling point of view, while instructions fetch operands from the memory and store computed results into the memory like a Von Neumann machine. This machine later evolved into super actor machine (SAM) [25].
Reference: [15] <author> J. B. Dennis and D. P. Misunas. </author> <title> A preliminary architecture for a basic data flow computer. </title> <booktitle> In Proceedings of the 2nd Annual Symposium on Computer Architecture, </booktitle> <year> 1975. </year>
Reference-contexts: Therefore, there are no associated bookkeeping costs, whereas branch directed prefetching has to rely on a hardware table to keep a list of referenced data elements. 2.2 Data Flow Machines Static dataflow architecture was based on the original design by Dennis and Misuanas <ref> [15] </ref>. According to Dennis [13], "the guiding principle has been to design hardware systems that would faithfully implement the semantics of computations expressed as dataflow graphs. The original hardware concept envisioned instruction templates having spaces to receive the operands of the instructions".
Reference: [16] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3), </volume> <year> 1987. </year>
Reference-contexts: One of the widely used program representations is the program dependence graph (PDG) <ref> [16] </ref>. A PDG makes explicit for both the data and the control dependences for each operation in a program. Ball and Horwitz [3] give algorithms to reconstruct a control flow graph from a control dependence graph such as PDG.
Reference: [17] <author> J. W. C. Fu and J. H. Patel. </author> <title> Data prefetching in multiprocessor vector cache memories. </title> <booktitle> In Proceedings of the 18th International Conference on Computer Architecture, </booktitle> <pages> pages 54-63, </pages> <year> 1991. </year>
Reference-contexts: There are many techniques currently employed to improve the efficiency of memory hierarchies. These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching <ref> [7, 9, 17, 34, 35, 29] </ref>, lock-up free and interleaved caches [30, 48] or software techniques such as blocked algorithms [31] are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement. <p> Klaiber and Levy [29] examine in detail the hardware support necessary for the cache load instruction and extend on work of Porterfield. Mowry et al [34] study the effects of software controlled prefetching in a multiprocessing environment. Fu and Patel <ref> [17] </ref> study prefetching for vector processors. Chen and Baer [9] show through experiments that prefetching caches outperform non-blocking caches in many applications but a hybrid design is better. Callahan and Kennedy extend the work of Porterfield for reducing the cost of prefetching [7].
Reference: [18] <author> G. R. Gao, R. Tio, and H. H. J. Hum. </author> <title> Design of an efficient dataflow architecture without data flow. </title> <type> Technical Report TR-SOCS-88.14, </type> <institution> School of Computer Science, McGill University, </institution> <year> 1988. </year>
Reference-contexts: For example, the argument fetch machine proposed by Dennis and Gao <ref> [14, 18] </ref> is dataflow from the instruction scheduling point of view, while instructions fetch operands from the memory and store computed results into the memory like a Von Neumann machine. This machine later evolved into super actor machine (SAM) [25].
Reference: [19] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-directed data prefetching in multi-processors with memory hierarchies. </title> <booktitle> In Proceedings of the 1990 Int. Conf. on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <year> 1990. </year>
Reference-contexts: Porterfield gives algorithms for the insertion of cache load instructions at appropriate places. His work has been extended by Gornish et al. <ref> [19] </ref>. Gornish et al. present algorithms to find the earliest time when a cache load instruction can be inserted. Creating a special cache load instruction is the basic mechanism used by almost any software prefetching algorithm.
Reference: [20] <author> V.G. Grafe, G.S. Davidson, J.E. Hoch, and V.P. Holmes. </author> <title> The *psilon dataflow processor. </title> <booktitle> In Proceedings of the 16th International Conference on Computer Architecture, </booktitle> <pages> pages 36-45, </pages> <year> 1989. </year>
Reference-contexts: This machine supports a cache memory with synchronization control and a hardware mechanism of processor ready queues for fast context switching. This machine later evolved into IBM's empire project, which was later abandoned due to non-technical reasons. ETL in Japan <ref> [20] </ref> and the Sandia National Laboratories in the United States are working on multi-threaded machines based on the ETS way of handling the storage and the tokens. A significant step in the introduction of dataflow based multi-threading was the P-RISC idea proposed by Nikhil and Arvind [36].
Reference: [21] <author> R. Gupta. </author> <title> Generalized dominators and post-dominators. </title> <booktitle> In Conference Record of the Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 246-257, </pages> <year> 1992. </year>
Reference-contexts: This property makes the representation efficient and easier to deal with a large classes of optimizations. Many program representations are computed using the dominance relationships in the program nodes. DJ-Graphs [49] have the form of a dominator tree which allows efficient computation of the SSA form. Gupta <ref> [21] </ref> introduces the idea of multiple node immediate dominators and gives algorithms for using it in various optimizations.
Reference: [22] <author> R. Gupta and C. Chi. </author> <title> Improving instruction cache performance by reducing cache pollution. </title> <booktitle> In Supercomputing'90, </booktitle> <pages> pages 82-91, </pages> <year> 1990. </year> <month> 69 </month>
Reference-contexts: Thus, out-of-order fetch can easily be achieved with software support. One such technique is given by <ref> [22] </ref> for instruction references. The technique is equally applicable to data references with data forwarding. Early restart, write buffers, lock-up free caches and prefetching are all applicable to data forwarding. <p> Since all the instructions and the corresponding data during the execution of an IBB; DBB pair are utilized, once the first (I block; D block) pair is fetched, the rest are prefetched automatically. Repositioning techniques to achieve this goal can be found in <ref> [22] </ref>. 4.8 Multi-threaded DFM Data forwarding allows us to deal with some of the problems of highly parallel machines, such as multi-threaded data flow machines.
Reference: [23] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: The smaller the miss penalty and the miss rate, the better will be the cache performance. Cache misses are broken down to three basic elements. These are the capacity, conflict, and the compulsory misses <ref> [23] </ref>. Capacity misses occur since the size of the cache is not big enough to hold all the needed elements. Since multiple addresses map to the same location in a cache, references to different addresses may collide with each other, causing conflict misses. <p> Superscalar machines evolved from simple reduced instruction set processors, and unlike dataflow based parallel machines they have to rely on complex hardware mechanisms to observe output and anti-dependences. Most widely used technique for this purpose is the scoreboard mechanism which was first used in the CDC6600 machine <ref> [23] </ref>. These complicated mechanisms cause many difficulties in handling interrupts. Nevertheless, superscalar machines continue to play an important role in today's computing arena. <p> At the same time, some publicaly available 66 hardware emulators have been analyzed to assess their suitability for low level simulation of the architecture. Availability of various cache simulators have also been investigated, and it has been decided that the Dinero cache simulator by Hennessy and Patterson <ref> [23] </ref> can be used to simulate the cache behavior of any architecture, provided some simple set of rules are obeyed when generating address traces. The instruction level simulator will be modified to generate address traces in this format.
Reference: [24] <author> S. Horwitz, J. Prins, and T. Reps. </author> <title> On the adequacy of program dependence graphs for rep-resenting program. </title> <type> Technical Report 699, </type> <institution> University of Wisconsin-Madison, Computer Science Department, </institution> <year> 1987. </year>
Reference-contexts: Beck and Pingali [4] illustrate how an imperative program which naturally bases on control flow can be converted to a data flow graph for execution on a dataflow processor. Selke [45, 46] provides a semantic basis for PDG transformations and extend on the work of Horwitz <ref> [24] </ref>. This work yields what is called semantic program dependence graphs [41]. The dependence information and how it is represented leads to many varieties of program representations. Pingali gives a program representation called dependence flow graph (DFG) which is based on a dependence driven execution model [42].
Reference: [25] <author> H.H.J. Hum and G.R. Gao. </author> <title> Efficient support of concurrent threads in a hybrid dataflow/von Neumann architecture. </title> <booktitle> In Proceedings of the IEEE Symposium and Parallel and Distributed Processing, </booktitle> <year> 1991. </year>
Reference-contexts: This machine later evolved into super actor machine (SAM) <ref> [25] </ref>. Many other hybrids does the sequencing at the thread level based on the availability of operands for the thread but use program counters to execute sequential threads. One of the first hybrid machines is the Iannucci's hybrid machine [26].
Reference: [26] <editor> R.A. Iannucci. </editor> <title> Toward a dataflow/von Neumann hybrid architecture. </title> <booktitle> In Proceedings of the 15th International Conference on Computer Architecture, </booktitle> <pages> pages 131-140, </pages> <year> 1988. </year>
Reference-contexts: This machine later evolved into super actor machine (SAM) [25]. Many other hybrids does the sequencing at the thread level based on the availability of operands for the thread but use program counters to execute sequential threads. One of the first hybrid machines is the Iannucci's hybrid machine <ref> [26] </ref>. This machine supports a cache memory with synchronization control and a hardware mechanism of processor ready queues for fast context switching. This machine later evolved into IBM's empire project, which was later abandoned due to non-technical reasons.
Reference: [27] <author> R. Johnson, W. Li, and K. Pingali. </author> <title> An executable representation of distance and direction. </title> <booktitle> In Preliminary Proceedings of the 4th Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: The dependence information and how it is represented leads to many varieties of program representations. Pingali gives a program representation called dependence flow graph (DFG) which is based on a dependence driven execution model [42]. Johnson shows how distance and direction can be represented using dependence flow graphs <ref> [27] </ref>. The SSA form [11, 12] plays an important role in optimizing compilers. This form ensures that 17 each use of a variable is reached by exactly one assignment to that variable. This property makes the representation efficient and easier to deal with a large classes of optimizations.
Reference: [28] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fullyassociative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th International Conference on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <year> 1990. </year>
Reference-contexts: Baer further argues that any strategy which is based upon the sequential spatial locality of references will work better for instruction references. Experimental evidence in support of this argument is provided by Jouppi <ref> [28] </ref>. This work indicates that a stream buffer of four 8-byte blocks can remove up to 85 % of the misses for a 4K instruction cache, but only 35 % of the misses for a 4K data cache.
Reference: [29] <author> A. C. Klaiber and H. M. Levy. </author> <title> An architecture for software-controlled data prefetching. </title> <booktitle> In Proceedings of the 18th International Conference on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <year> 1991. </year>
Reference-contexts: There are many techniques currently employed to improve the efficiency of memory hierarchies. These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching <ref> [7, 9, 17, 34, 35, 29] </ref>, lock-up free and interleaved caches [30, 48] or software techniques such as blocked algorithms [31] are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement. <p> His work has been extended by Gornish et al. [19]. Gornish et al. present algorithms to find the earliest time when a cache load instruction can be inserted. Creating a special cache load instruction is the basic mechanism used by almost any software prefetching algorithm. Klaiber and Levy <ref> [29] </ref> examine in detail the hardware support necessary for the cache load instruction and extend on work of Porterfield. Mowry et al [34] study the effects of software controlled prefetching in a multiprocessing environment. Fu and Patel [17] study prefetching for vector processors.
Reference: [30] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proceedings of the 8th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 81-86, </pages> <year> 1981. </year>
Reference-contexts: These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching [7, 9, 17, 34, 35, 29], lock-up free and interleaved caches <ref> [30, 48] </ref> or software techniques such as blocked algorithms [31] are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement. <p> Write buffers: These buffers allow the delaying the writes in favor of more urgent cache loads. 4. Lock-up free caches: These caches allow initiation of cache loads for uncached blocks while fetching some other block is in progress <ref> [30] </ref>. 5. Prefetching: Prefetching can be done in the hardware, in the software or a combination of them. Hardware prefetching caches usually employ one block lookahead (OBL) policy, which prefetches block i+1 when block i is referenced.
Reference: [31] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 63-74, </pages> <year> 1991. </year>
Reference-contexts: These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching [7, 9, 17, 34, 35, 29], lock-up free and interleaved caches [30, 48] or software techniques such as blocked algorithms <ref> [31] </ref> are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement. For example, the capacity misses can be reduced 3 by employing larger caches, but this is not cost effective. In addition, larger hardware is usually slower. <p> Furthermore, prefetching techniques can be applied to effectively fetch portions of arrays that are referenced within loops. Therefore existing techniques <ref> [35, 31] </ref> can also be used to improve the performance of A caches. Cache line boundary alignment: Considering the memory organization used by DFM, it is appropriate to design the caches such that the instruction cache line size is exactly half of the data cache line size.
Reference: [32] <author> L. A. Lozano C. and G. R. Gao. </author> <title> Exploiting short-lived variables in superscalar processors. </title> <booktitle> In The 28th Annual IEEE-ACM International Symposium on Microarchitecture (MICRO-28), </booktitle> <address> Ann Arbor, Michigan, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: Because of the way data forwarding works, it is not possible to use directly addressable registers in the computation. Various forms of implicitly addressable fast storage such as stack mechanisms, memory to memory machine architectures will be analyzed for their suitability to the data forwarding paradigm. Lozano <ref> [32] </ref> gives one such mechanism in the context of superscalar execution. The final phase of the work will deal with the designing of the instruction set.
Reference: [33] <author> C. Moura. </author> <title> SuperDLX a generic superscalar simulator. </title> <type> Technical Report 64, </type> <institution> McGill University, School of Computer Science, ACAPS Laboratory, </institution> <year> 1993. </year>
Reference-contexts: The simulator will be capable of providing diagnostics about the status of the machine, indicating the degree of parallel execution achieved, the number and the cause of the stalls. The superscalar machine will be compared with a superscalar version of DLX (SuperDLX), using the simulator developed by McGill University <ref> [33] </ref>. 3.4.1 Apply various optimization techniques that are specific to the data forwarding Although many commonly employed program optimizations are applicable to data forwarding style execution, data forwarding provides new opportunities and challenges for employing optimization techniques specific to the paradigm.
Reference: [34] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in sharedmemory multiprocessors. </title> <journal> Journal of Parallel and Distibuted Computing, </journal> <volume> 12 </volume> <pages> 87-106, </pages> <year> 1991. </year>
Reference-contexts: There are many techniques currently employed to improve the efficiency of memory hierarchies. These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching <ref> [7, 9, 17, 34, 35, 29] </ref>, lock-up free and interleaved caches [30, 48] or software techniques such as blocked algorithms [31] are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement. <p> Creating a special cache load instruction is the basic mechanism used by almost any software prefetching algorithm. Klaiber and Levy [29] examine in detail the hardware support necessary for the cache load instruction and extend on work of Porterfield. Mowry et al <ref> [34] </ref> study the effects of software controlled prefetching in a multiprocessing environment. Fu and Patel [17] study prefetching for vector processors. Chen and Baer [9] show through experiments that prefetching caches outperform non-blocking caches in many applications but a hybrid design is better.
Reference: [35] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 62-73, </pages> <year> 1992. </year>
Reference-contexts: There are many techniques currently employed to improve the efficiency of memory hierarchies. These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching <ref> [7, 9, 17, 34, 35, 29] </ref>, lock-up free and interleaved caches [30, 48] or software techniques such as blocked algorithms [31] are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement. <p> In this case, the upper level will never miss and a perfect memory hierarchy will be obtained which operates with the speed of the top level, while offering a size as big as the lowest level in the hierarchy. Prefetching [7] [9] <ref> [35] </ref> may be done in the hardware, software or a combination of both. <p> Furthermore, prefetching techniques can be applied to effectively fetch portions of arrays that are referenced within loops. Therefore existing techniques <ref> [35, 31] </ref> can also be used to improve the performance of A caches. Cache line boundary alignment: Considering the memory organization used by DFM, it is appropriate to design the caches such that the instruction cache line size is exactly half of the data cache line size.
Reference: [36] <author> R. Nikhil and Arvind. </author> <booktitle> Can dataflow subsume von neumann computing? In Proceedings of the 16th International Conference on Computer Architecture, </booktitle> <pages> pages 262-272, </pages> <year> 1989. </year>
Reference-contexts: ETL in Japan [20] and the Sandia National Laboratories in the United States are working on multi-threaded machines based on the ETS way of handling the storage and the tokens. A significant step in the introduction of dataflow based multi-threading was the P-RISC idea proposed by Nikhil and Arvind <ref> [36] </ref>. In a very clear and simple model, they showed how the two fundamental problems in multiprocessing [1], namely the latency and the cost of synchronization, can be handled using a simple model of sequential threads, fork, and join operations.
Reference: [37] <author> R. S. Nikhil. </author> <title> A multithreaded implementation of Id using P-RISC graphs. </title> <booktitle> In Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1993. </year> <month> 70 </month>
Reference-contexts: DJ-Graphs [49] have the form of a dominator tree which allows efficient computation of the SSA form. Gupta [21] introduces the idea of multiple node immediate dominators and gives algorithms for using it in various optimizations. In addition to above mentioned representations, parallel PDG [50] and P-RISC graphs <ref> [37] </ref> are employed in parallel and dataflow architectures as program representations. 18 3 Goals and Approaches The goal of this work is to address three main issues in todays high performance computing machine design by using the concept of data forwarding.
Reference: [38] <author> R. S. Nikhil, G.M. Papadopoulos, and Arvind. </author> <title> *T:a multithreaded massively parallel archi--tecture. </title> <booktitle> In Proceedings of the 19th International Conference on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <year> 1992. </year>
Reference-contexts: The P-RISC idea continues to be an important one and evolved with the implementation of *T machine <ref> [38] </ref>. Data forwarding benefited from the ideas of both ETS and P-RISC.
Reference: [39] <author> S. Onder and R. Gupta. SINAN: </author> <title> An argument forwarding multithreaded architecture. </title> <booktitle> In International Conference on High Performance Computing, </booktitle> <address> New Delhi, India, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Data forwarding benefited from the ideas of both ETS and P-RISC. Multi-threaded data forwarding may provide faster execution of vector operations and in general more efficient than ETS 15 for code with limited parallelism <ref> [39] </ref> 2.4 Superscalar machines A superscalar machine is a machine capable of issuing multiple instructions from a single instruction stream. An instruction is said to issue when it progresses from the fetch stage into the execution stage. <p> These problems can be handled efficiently when data forwarding is applied to a multi-threaded architectural framework. Such a machine is called a multi-threaded data forwarding machine (MDFM). As part of the preliminary work, an MDFM named SINAN has been designed. A paper discussing this design will appear in HiPC-95 <ref> [39] </ref>. 5 Current Status and Remaining Work The following goals have been attained as part of the preliminary work done for this study: 1. A directly executable program representation for data forwarding, namely, the data forwarding graph (DFG) has been developed. 2.
Reference: [40] <author> G.M. Papadopoulos and D.E. Culler. Monsoon: </author> <title> An explicit token-store architecture. </title> <booktitle> In Proceedings of the 17th International Conference on Computer Architecture, </booktitle> <pages> pages 82-91, </pages> <year> 1990. </year>
Reference-contexts: Another problem with tagged token dataflow architectures has been the difficulty of deciding a tag size. The above mentioned problems of tagged token dataflow architectures led to the design of Explicit Token Store architecture (ETS). According to Papadopoulos <ref> [40] </ref>, the central idea in the ETS model is that storage for tokens is dynamically allocated in sizable blocks, with detailed usage of locations within a block determined at compile time. Since activation frames are allocated dynamically and explicitly, storage for all tokens will be ready.
Reference: [41] <author> R. Parsons. </author> <title> Semantic program dependence graphs. </title> <type> Technical Report COMP TR93-202, </type> <institution> Rice University, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Selke [45, 46] provides a semantic basis for PDG transformations and extend on the work of Horwitz [24]. This work yields what is called semantic program dependence graphs <ref> [41] </ref>. The dependence information and how it is represented leads to many varieties of program representations. Pingali gives a program representation called dependence flow graph (DFG) which is based on a dependence driven execution model [42]. Johnson shows how distance and direction can be represented using dependence flow graphs [27].
Reference: [42] <author> K. Pingali, M. Beck, R. Johnson, M. Moudgill, and P. Stodghill. </author> <title> Dependence flow graphs: An algebraic approach to program dependence. </title> <type> Technical Report 90-1152, </type> <institution> Cornell University, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: When a program representation is used for optimizations, certain properties of the representation make it more suitable for optimizations. For example, some representations transform loops to tail recursive functions. This transformation makes it difficult to apply optimizations such as loop interchange <ref> [42] </ref>. The efficiency of the traversal of the representation as a data structure for data dependence information, the compactness of the representation, being directly executable and the storage model are all significant characteristics of a program representation [42]. <p> This transformation makes it difficult to apply optimizations such as loop interchange <ref> [42] </ref>. The efficiency of the traversal of the representation as a data structure for data dependence information, the compactness of the representation, being directly executable and the storage model are all significant characteristics of a program representation [42]. One of the widely used program representations is the program dependence graph (PDG) [16]. A PDG makes explicit for both the data and the control dependences for each operation in a program. <p> This work yields what is called semantic program dependence graphs [41]. The dependence information and how it is represented leads to many varieties of program representations. Pingali gives a program representation called dependence flow graph (DFG) which is based on a dependence driven execution model <ref> [42] </ref>. Johnson shows how distance and direction can be represented using dependence flow graphs [27]. The SSA form [11, 12] plays an important role in optimizing compilers. This form ensures that 17 each use of a variable is reached by exactly one assignment to that variable.
Reference: [43] <author> A. K. Porterfield. </author> <title> Software methods for improvement of cache performance on supercomputer applications. </title> <type> Technical Report COMP TR89-93, </type> <institution> Rice University, Department of Computer Science, </institution> <year> 1989. </year>
Reference-contexts: Since the association is done in the hardware with limited resources, the coupling between the instruction stream and the data stream is weak. Nevertheless, the results of their experiments indicate great success for scientific programs and moderate success for other 11 applications. 2.1.3 Software Controlled Prefetching Porterfield <ref> [43] </ref> proposes a non-blocking cache load instruction which can then be inserted in appropriate places in the program. A cache load instruction is very much like a load instruction, except that it does not block, and it does not specify a destination register.
Reference: [44] <author> S. Przybylski. </author> <title> The performance impact of block sizes and fetch strategies. </title> <booktitle> In Proceedings of the 17th International Conference on Computer Architecture, </booktitle> <pages> pages 160-169, </pages> <year> 1990. </year>
Reference-contexts: Smith [47] notes an increase in the memory traffic accompanied by a decrease in the miss rate when prefetching is employed. Complex hardware prefetch strategies are reported not to be very successful <ref> [44] </ref>. According to the analysis by Baer [2], decoupled architectures perform an implicit form of prefetching. This is because, the unit that performs data accesses can be ahead of the unit that is processing the functional operations.
Reference: [45] <author> R. P. Selke. </author> <title> A rewriting semantics for program dependence graphs. </title> <booktitle> In Conference Record of the Sixteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 12-24, </pages> <address> Austin, Texas, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: Beck and Pingali [4] illustrate how an imperative program which naturally bases on control flow can be converted to a data flow graph for execution on a dataflow processor. Selke <ref> [45, 46] </ref> provides a semantic basis for PDG transformations and extend on the work of Horwitz [24]. This work yields what is called semantic program dependence graphs [41]. The dependence information and how it is represented leads to many varieties of program representations.
Reference: [46] <author> R. P. Selke. </author> <title> Transforming program dependence graphs. </title> <type> Technical Report COMP TR90-131, </type> <institution> Rice University, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: Beck and Pingali [4] illustrate how an imperative program which naturally bases on control flow can be converted to a data flow graph for execution on a dataflow processor. Selke <ref> [45, 46] </ref> provides a semantic basis for PDG transformations and extend on the work of Horwitz [24]. This work yields what is called semantic program dependence graphs [41]. The dependence information and how it is represented leads to many varieties of program representations.
Reference: [47] <author> A. J. Smith. </author> <title> Cache memories. </title> <journal> ACM Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <year> 1982. </year>
Reference-contexts: Prefetching is a very effective technique in reducing the misses and approximating an ideal memory hierarchy. However, it is also a double edged sword. In order to be effective, future references of programs must be correctly predicted, otherwise valuable memory bandwidth is wasted. Smith <ref> [47] </ref> notes an increase in the memory traffic accompanied by a decrease in the miss rate when prefetching is employed. Complex hardware prefetch strategies are reported not to be very successful [44]. According to the analysis by Baer [2], decoupled architectures perform an implicit form of prefetching.
Reference: [48] <author> G. S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processors. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 53-62, </pages> <year> 1991. </year>
Reference-contexts: These techniques try to address the three aspects of cache misses by using hardware, software or a combination of hardware and software approaches. Software and hardware prefetching [7, 9, 17, 34, 35, 29], lock-up free and interleaved caches <ref> [30, 48] </ref> or software techniques such as blocked algorithms [31] are examples of such techniques. On the other hand, existing techniques are less than adequate and there is a significant room for improvement.
Reference: [49] <author> V. C. Sreedhar, Y. Lee, and G. R. Gao. </author> <title> DJ-Graphs and their applications for flowgraph analyses. </title> <type> Technical Report 70, </type> <institution> McGill University, School of Computer Science, ACAPS Laboratory, </institution> <year> 1994. </year>
Reference-contexts: This form ensures that 17 each use of a variable is reached by exactly one assignment to that variable. This property makes the representation efficient and easier to deal with a large classes of optimizations. Many program representations are computed using the dominance relationships in the program nodes. DJ-Graphs <ref> [49] </ref> have the form of a dominator tree which allows efficient computation of the SSA form. Gupta [21] introduces the idea of multiple node immediate dominators and gives algorithms for using it in various optimizations.
Reference: [50] <author> H. Srinivasan and D. Grunwald. </author> <title> An efficient construction of parallel static single assignment form for structured parallel programs. </title> <type> Technical Report CU-CS-564-91, </type> <institution> University of Colorado at Boulder, Department of Computer Science, </institution> <year> 1991. </year> <month> 71 </month>
Reference-contexts: DJ-Graphs [49] have the form of a dominator tree which allows efficient computation of the SSA form. Gupta [21] introduces the idea of multiple node immediate dominators and gives algorithms for using it in various optimizations. In addition to above mentioned representations, parallel PDG <ref> [50] </ref> and P-RISC graphs [37] are employed in parallel and dataflow architectures as program representations. 18 3 Goals and Approaches The goal of this work is to address three main issues in todays high performance computing machine design by using the concept of data forwarding.
References-found: 50

