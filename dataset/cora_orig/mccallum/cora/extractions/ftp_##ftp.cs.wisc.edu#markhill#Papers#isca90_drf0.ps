URL: ftp://ftp.cs.wisc.edu/markhill/Papers/isca90_drf0.ps
Refering-URL: http://www.cs.wisc.edu/~markhill/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Weak Ordering ANew Definition  
Author: Sarita V. Adve Mark D. Hill 
Keyword: Key words: shared-memory multiprocessor, sequential consistency, weak ordering.  
Address: Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: A memory model for a shared memory, multiprocessor commonly and often implicitly assumed by programmers is that of sequential consistency. This model guarantees that all memory accesses will appear to execute atomically and in program order. An alternative model, weak ordering, offers greater performance potential. Weak ordering was first defined by Dubois, Scheurich and Briggs in terms of a set of rules for hardware that have to be made visible to software. The central hypothesis of this work is that programmers prefer to reason about sequentially consistent memory, rather than having to think about weaker memory, or even write buffers. Following this hypothesis, we re-define weak ordering as a contract between software and hardware. By this contract, software agrees to some formally specified constraints, and hardware agrees to appear sequentially consistent to at least the software that obeys those constraints. We illustrate the power of the new definition with a set of software constraints that forbid data races and an implementation for cache-coherent systems that is not allowed by the old definition. 
Abstract-found: 1
Intro-found: 1
Reference: [AdH89] <author> S. V. ADVE and M. D. HILL, </author> <title> Weak Ordering A New Definition And Some Implications, </title> <type> Computer Sciences Technical Report #902, </type> <institution> University of Wisconsin, Madison, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: For the convenience of the reader, the key definitions used throughout the paper are repeated in Appendix C. 2. Related Work This section briefly describes relevant previous work on sequential consistency (Section 2.1) and weak ordering (Section 2.2). A more detailed survey of the subject appears in <ref> [AdH89] </ref>. 2.1. Sequential Consistency Sequential consistency was first defined by Lam-port [Lam79], and discussed for shared memory systems with general interconnection networks, but no caches. For single bus cache-based systems, a number of cache-coherence protocols have been proposed in the literature [ArB86]. Most ensure sequential consistency. <p> The notion of strong ordering as an equivalent of sequential consistency was defined in [DSB86]. However, there do exist programs that can distinguish between strong ordering and sequential consistency <ref> [AdH89] </ref> and hence, strong ordering is not strictly equivalent to sequential consistency. Strong ordering has been discarded in [Sch89] in favor of a similar model, viz., concurrent consistency. A concurrently consistent system is defined to behave like a sequentially consistent system for most practical purposes. <p> The authors mention that for synchronization operations that require a value to be returned, it is possible to send a tentative value before the operation completes, if a processor can undo subsequent operations that may depend on it, after receiving the actual value. In <ref> [AdH89] </ref>, we discuss how this violates condition 3 of Definition 1, but does not violate the new definition of weak ordering below. 3. Weak Ordering ANew Definition. We view weak ordering as an interface (or contract) between software and hardware. <p> This makes sure that the counter will read zero after a bounded number of increments after a synchronization operation is committed. A more dynamic solution involves providing a mechanism to distinguish accesses (and their acks) generated before a particular synchronization operation from those generated after <ref> [AdH89] </ref>. Though processors can be stalled at various points for unbounded amounts of time, deadlock can never occur. <p> We are particularly grateful to Kourosh Gharachorloo for bringing to our attention an error in an earlier version of the proof in Appendix B and to Michel Dubois for pointing out some of the limitations of DRF0. These had been overlooked by us in <ref> [AdH89] </ref>. - --
Reference: [ASH88] <author> A. AGARWAL, R. SIMONI, M. HOROWITZ and J. HENNESSY, </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> Proc. 15th Annual International Symposium on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> 280-289. </pages>
Reference-contexts: In particular, no restrictions are placed on the kind of data a cache may contain, nor are any assumptions made regarding the atomicity of any transactions on the interconnection network. A straightforward directory-based, writeback cache coherence protocol, similar to those discussed in <ref> [ASH88] </ref>, is assumed. In particular, for a write miss on a line that is present in valid (or shared) state in more than one cache, the protocol requires the directory to send messages to invalidate these copies of the line.
Reference: [ArB86] <author> J. ARCHIBALD and J. BAER, </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model, </title> <journal> ACM Transactions on Computer Systems 4, </journal> <month> 4 (November </month> <year> 1986), </year> <pages> 273-298. </pages>
Reference-contexts: Systems with general interconnection networks without caches The execution is possible even if accesses of a processor are issued in program order, but reach memory modules in a different order [Lam79]. Shared-bus systems with caches Even with a cache coherence protocol <ref> [ArB86] </ref>, the execution is possible if the accesses of a processor are issued out-of-order, or if reads are allowed to pass writes in write buffers. <p> Sequential Consistency Sequential consistency was first defined by Lam-port [Lam79], and discussed for shared memory systems with general interconnection networks, but no caches. For single bus cache-based systems, a number of cache-coherence protocols have been proposed in the literature <ref> [ArB86] </ref>. Most ensure sequential consistency. In particular, Rudolph and Segall have developed two protocols, which they formally prove guarantee sequential consistency [RuS84].
Reference: [BeG81] <author> P. A. BERNSTEIN and N. GOODMAN, </author> <title> Concurrency Control in Distributed Systems, </title> <journal> Computing Surveys 13, </journal> <month> 2 (June, </month> <year> 1981), </year> <pages> 185-221. </pages>
Reference-contexts: However, the algorithm depends on detecting conflicting data accesses at compile time and so its success depends on data dependence analysis techniques, which may be quite pessimistic. The conditions for sequential consistency of memory accesses are analogous to the serialization condition for transactions in concurrent database systems <ref> [BeG81, Pap86] </ref>. However, database systems seek to serialize the effects of entire transactions, which may be a series of reads and writes while we are concerned with the atomicity of individual reads and writes.
Reference: [BNR89] <author> R. BISIANI, A. NOWATZYK and M. RAVISHANKAR, </author> <title> Coherent Shared Memory on a Distributed Memory Machine, </title> <booktitle> Proc. International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1989, </year> <month> I-133-141. </month>
Reference-contexts: It was recognized later in [ScD88, Sch89] that the above three conditions are not necessary to meet the intuitive goals of weak ordering. In Section 3, we give a new definition that we believe formally specifies this intuition. Bisiani, Nowatzyk and Ravishankar have proposed an algorithm <ref> [BNR89] </ref> for the implementation of weak ordering on distributed memory systems. Weak ordering is achieved by using timestamps to ensure that a synchronization operation completes only after all accesses previously issued by all processors in the system are complete.
Reference: [BMW85] <author> W. C. BRANTLEY, K. P. MCAULIFFE and J. WEISS, </author> <title> RP3 Process-Memory Element, </title> <booktitle> International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985, </year> <pages> 772-781. </pages>
Reference-contexts: For single bus cache-based systems, a number of cache-coherence protocols have been proposed in the literature [ArB86]. Most ensure sequential consistency. In particular, Rudolph and Segall have developed two protocols, which they formally prove guarantee sequential consistency [RuS84]. The RP3 <ref> [BMW85, PBG85] </ref> is a cache-based system, where processor memory communication is via an Omega network, but the management of cache coherence for shared writable variables is entrusted to the software.
Reference: [Col84] <author> W. W. COLLIER, </author> <title> Architectures for Systems of Parallel Processes, </title> <type> Technical Report 00.3253, </type> <institution> IBM Corp., </institution> <address> Poughkeepsie, N.Y., </address> <month> 27 January </month> <year> 1984. </year>
Reference-contexts: A concurrently consistent system is defined to behave like a sequentially consistent system for most practical purposes. Collier has developed a general framework to characterize architectures as sets of rules, where each rule is a restriction on the order of execution of certain memory operations. <ref> [Col84, Col90] </ref>. He has proved that for most practical purposes, a system where all processors observe all write operations in the same order (called write synchronization), is indistinguishable from a system where all writes are executed atomically. Shasha and Snir have proposed a software algorithm to ensure sequential consistency [ShS88].
Reference: [Col90] <author> W. W. COLLIER, </author> <title> Reasoning about Parallel Architectures, </title> <publisher> Prentice-Hall, </publisher> <address> Inc., </address> <note> To appear 1990. </note>
Reference-contexts: A concurrently consistent system is defined to behave like a sequentially consistent system for most practical purposes. Collier has developed a general framework to characterize architectures as sets of rules, where each rule is a restriction on the order of execution of certain memory operations. <ref> [Col84, Col90] </ref>. He has proved that for most practical purposes, a system where all processors observe all write operations in the same order (called write synchronization), is indistinguishable from a system where all writes are executed atomically. Shasha and Snir have proposed a software algorithm to ensure sequential consistency [ShS88].
Reference: [DeM88] <author> R. DELEONE and O. L. MANGASARIAN, </author> <title> Asynchronous Parallel Successive Overrelaxation for the Symmetric Linear Complementarity Problem, </title> <booktitle> Mathematical Programming 42, </booktitle> <year> 1988, </year> <pages> 347-361. </pages>
Reference-contexts: However, there are some disadvantages of defining a weakly ordered system in this manner. First, there are useful parallel programmer's models that are not easily expressed in terms of sequential consistency. One such model is used by the designers of asynchronous algorithms <ref> [DeM88] </ref>. (We expect, however, it will be straightforward to implement weakly ordered hardware to obtain reasonable results for asynchronous algorithms.) Second, for any potential performance benefit over a sequentially consistent system, the synchronization model of a weakly ordered system will usually constrain software to synchronize using operations visible to the hardware.
Reference: [DSB86] <author> M. DUBOIS, C. SCHEURICH and F. A. BRIGGS, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proc. Thirteenth Annual International Symposium on Computer Architecture 14, </booktitle> <month> 2 (June </month> <year> 1986), </year> <pages> 434-442. </pages>
Reference-contexts: As will be apparent later, this option functions as a weakly ordered system. Dubois, Scheurich and Briggs have analyzed the problem of ensuring sequential consistency in systems that allow caching of shared variables, without impos - 3 - - -- ing any constraints on the interconnection network <ref> [DSB86, DSB88, ScD87, Sch89] </ref>. A sufficient condition for sequential consistency for cache-based systems has been stated [ScD87, Sch89]. The condition is satisfied if all processors issue their accesses in program order, and no access is issued by a processor until its previous accesses have been globally performed. <p> A read is globally performed when the value it returns is bound and the write that wrote this value is globally performed. The notion of strong ordering as an equivalent of sequential consistency was defined in <ref> [DSB86] </ref>. However, there do exist programs that can distinguish between strong ordering and sequential consistency [AdH89] and hence, strong ordering is not strictly equivalent to sequential consistency. Strong ordering has been discarded in [Sch89] in favor of a similar model, viz., concurrent consistency. <p> Weak Ordering Weakly ordered systems depend on explicit, hardware recognizable synchronization operations to order the effects of events initiated by different processors in a system. Dubois, Scheurich and Briggs first defined weak ordering in <ref> [DSB86] </ref> as follows: Definition 1: In a multiprocessor system, storage accesses are weakly ordered if (1) accesses to global synchronizing variables are strongly ordered, (2) no access to a synchronizing variable is issued by a processor before all previous global data accesses have been globally performed, and if (3) no access <p> To allow process migration, a processor is also be required to stall on a context switch until its counter reads zero. location by a given processor are observed in the same order by all processors <ref> [DSB86] </ref>. We assume that condition 1 of Definition 1 requires synchronization operations to be executed in a sequentially consistent manner, and not just strongly ordered. With these additional conditions, our claim can be proved formally in a manner analogous to the proof of Appendix B.
Reference: [DSB88] <author> M. DUBOIS, C. SCHEURICH and F. A. BRIGGS, </author> <title> Synchronization, Coherence, and Event Ordering in Multiprocessors, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 9-21. </pages>
Reference-contexts: As will be apparent later, this option functions as a weakly ordered system. Dubois, Scheurich and Briggs have analyzed the problem of ensuring sequential consistency in systems that allow caching of shared variables, without impos - 3 - - -- ing any constraints on the interconnection network <ref> [DSB86, DSB88, ScD87, Sch89] </ref>. A sufficient condition for sequential consistency for cache-based systems has been stated [ScD87, Sch89]. The condition is satisfied if all processors issue their accesses in program order, and no access is issued by a processor until its previous accesses have been globally performed.
Reference: [GVW89] <author> J. R. GOODMAN, M. K. VERNON and P. J. WOEST, </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors, </title> <booktitle> Proc. Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, </address> <month> April </month> <year> 1989, </year> <pages> 64-75. </pages>
Reference-contexts: Another interesting problem is the construction of other synchronization models optimized for particular software paradigms, such as, sharing only through monitors, or parallelism only from do-all loops, or for specific synchronization primitives offered by specific systems, e.g., QOSB <ref> [GVW89] </ref>. These optimizations may lead to implementations with higher performance. 8. Acknowledgements We would like to thank Vikram Adve, William Collier, Kourosh Gharachorloo, Garth Gibson, Richard Kessler, Viranjit Madan, Bart Miller, Robert Netzer, and Marvin Solomon for their valuable comments on earlier drafts of this paper.
Reference: [Kro81] <author> D. KROFT, </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization, </title> <booktitle> Proc. Eighth Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1981, </year> <pages> 81-87. </pages>
Reference-contexts: The use of many performance enhancing features of uniprocessors, such as write buffers, instruction execution overlap, out-of-order memory accesses and lockup-free caches <ref> [Kro81] </ref> is heavily restricted.
Reference: [Lam78] <author> L. LAMPORT, </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Communications of the ACM 21, </journal> <month> 7 (July </month> <year> 1978), </year> <pages> 558-565. </pages>
Reference-contexts: To formally specify the second feature of DRF0, viz., an indication of when there is "enough" synchronization in a program, we first define a set of happens-before relations for a program. Our definition is closely related to the "happened-before" relation defined by Lamport <ref> [Lam78] </ref> for message passing systems, and the "approximate temporal order" used by Netzer and Miller [NeM89] for detecting races in shared memory - 5 - - -- parallel programs that use semaphores.
Reference: [Lam79] <author> L. LAMPORT, </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, </title> <journal> IEEE Trans. on Computers C-28, </journal> <month> 9 (September </month> <year> 1979), </year> <pages> 690-691. </pages>
Reference-contexts: T. & T. Bell Laboratories, Digital Equipment Corporation, Texas Instruments, Cray Research and the graduate school at the University of Wisconsin-Madison. implicitly) assumed by programmers is that of sequential consistency, formally defined by Lamport <ref> [Lam79] </ref> as follows: [Hardware is sequentially consistent if] the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program. <p> Systems with general interconnection networks without caches The execution is possible even if accesses of a processor are issued in program order, but reach memory modules in a different order <ref> [Lam79] </ref>. Shared-bus systems with caches Even with a cache coherence protocol [ArB86], the execution is possible if the accesses of a processor are issued out-of-order, or if reads are allowed to pass writes in write buffers. <p> Related Work This section briefly describes relevant previous work on sequential consistency (Section 2.1) and weak ordering (Section 2.2). A more detailed survey of the subject appears in [AdH89]. 2.1. Sequential Consistency Sequential consistency was first defined by Lam-port <ref> [Lam79] </ref>, and discussed for shared memory systems with general interconnection networks, but no caches. For single bus cache-based systems, a number of cache-coherence protocols have been proposed in the literature [ArB86]. Most ensure sequential consistency.
Reference: [Lam86] <author> L. LAMPORT, </author> <title> The Mutual Exclusion Problem, </title> <journal> Parts I and II , Journal of the Association of Computing Machinery 33, </journal> <month> 2 (April </month> <year> 1986), </year> <pages> 313-348. </pages>
Reference-contexts: Furthermore, depending on the implementation, synchronization operations could be much slower than data accesses. We believe, however, that slow synchronization operations coupled with fast reads and writes will yield better performance than the alternative, where hardware must assume all accesses could be used for synchronization (as in <ref> [Lam86] </ref>). Third, programmers may wish to debug programs on a weakly ordered system that do not (yet) fully obey the synchronization model. The above definition allows hardware to return random values when the synchronization model is violated. We expect real hardware, however, to be much more well-behaved.
Reference: [NeM89] <author> R. H. B. NETZER and B. MILLER, </author> <title> Detecting Data Races in Parallel Program Executions, </title> <type> Computer Sciences Technical Report #894, </type> <institution> University of Wisconsin, Madison, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: Our definition is closely related to the "happened-before" relation defined by Lamport [Lam78] for message passing systems, and the "approximate temporal order" used by Netzer and Miller <ref> [NeM89] </ref> for detecting races in shared memory - 5 - - -- parallel programs that use semaphores. A happens-before relation for a program is a partial order defined for an execution of the program on an abstract, idealized architecture where all memory accesses are executed atomically and in program order. <p> Furthermore, current work is being done on determining when programs are data-race-free, and in locating the races when they are not <ref> [NeM89] </ref>. 5. An Implementation for Weak Ordering w.r.t. DRF0 In the last section, we gave an example synchronization model to illustrate the use of the new definition of weak ordering.
Reference: [Pap86] <author> C. PAPADIMITRIOU, </author> <title> The Theory of Database Concurrency Control, </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland 20850, </address> <year> 1986. </year>
Reference-contexts: However, the algorithm depends on detecting conflicting data accesses at compile time and so its success depends on data dependence analysis techniques, which may be quite pessimistic. The conditions for sequential consistency of memory accesses are analogous to the serialization condition for transactions in concurrent database systems <ref> [BeG81, Pap86] </ref>. However, database systems seek to serialize the effects of entire transactions, which may be a series of reads and writes while we are concerned with the atomicity of individual reads and writes.
Reference: [PBG85] <author> G. F. PFISTER, W. C. BRANTLEY, D. A. GEORGE, S. L. HARVEY, W. J. KLEINFELDER, K. P. MCAULIFFE, E. A. MELTON, V. A. NORTON and J. WEISS, </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture, </title> <booktitle> International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985, </year> <pages> 764-771. </pages>
Reference-contexts: For single bus cache-based systems, a number of cache-coherence protocols have been proposed in the literature [ArB86]. Most ensure sequential consistency. In particular, Rudolph and Segall have developed two protocols, which they formally prove guarantee sequential consistency [RuS84]. The RP3 <ref> [BMW85, PBG85] </ref> is a cache-based system, where processor memory communication is via an Omega network, but the management of cache coherence for shared writable variables is entrusted to the software.
Reference: [RuS84] <author> L. RUDOLPH and Z. SEGALL, </author> <title> Dynamic Decentralized Cache Schemes for MIMD Parallel Processors, </title> <booktitle> Proc. Eleventh International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1984, </year> <pages> 340-347. </pages>
Reference-contexts: For single bus cache-based systems, a number of cache-coherence protocols have been proposed in the literature [ArB86]. Most ensure sequential consistency. In particular, Rudolph and Segall have developed two protocols, which they formally prove guarantee sequential consistency <ref> [RuS84] </ref>. The RP3 [BMW85, PBG85] is a cache-based system, where processor memory communication is via an Omega network, but the management of cache coherence for shared writable variables is entrusted to the software. <p> Thus, P 0 but not P 1 gains an advantage from the example implementation. One very important case where the example implementation is likely to be slower than one for Definition 1 occurs when software performs repeated testing of a synchronization variable (e.g., the Test from a Test-and-TestAndSet <ref> [RuS84] </ref> or spinning on a barrier count). The example implementation serializes all these synchronization operations, treating them as writes. This can lead to a significant performance degradation. The unnecessary serialization can be avoided by improving on DRF0 to yield a new data-race-free model.
Reference: [ScD87] <author> C. SCHEURICH and M. DUBOIS, </author> <title> Correct Memory Operation of Cache-Based Multiprocessors, </title> <booktitle> Proc. Fourteenth Annual International Symposium on Computer Architecture, </booktitle> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1987, </year> <pages> 234-243. </pages>
Reference-contexts: As will be apparent later, this option functions as a weakly ordered system. Dubois, Scheurich and Briggs have analyzed the problem of ensuring sequential consistency in systems that allow caching of shared variables, without impos - 3 - - -- ing any constraints on the interconnection network <ref> [DSB86, DSB88, ScD87, Sch89] </ref>. A sufficient condition for sequential consistency for cache-based systems has been stated [ScD87, Sch89]. The condition is satisfied if all processors issue their accesses in program order, and no access is issued by a processor until its previous accesses have been globally performed. <p> A sufficient condition for sequential consistency for cache-based systems has been stated <ref> [ScD87, Sch89] </ref>. The condition is satisfied if all processors issue their accesses in program order, and no access is issued by a processor until its previous accesses have been globally performed. A write is globally performed when its modification has been propagated to all processors.
Reference: [ScD88] <author> C. SCHEURICH and M. DUBOIS, </author> <title> Concurrent Miss Resolution in Multiprocessor Caches, </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> University Park PA, </address> <month> August, </month> <year> 1988, </year> <month> I-118-125. </month>
Reference-contexts: It was recognized later in <ref> [ScD88, Sch89] </ref> that the above three conditions are not necessary to meet the intuitive goals of weak ordering. In Section 3, we give a new definition that we believe formally specifies this intuition.
Reference: [Sch89] <author> C. E. SCHEURICH, </author> <title> Access Ordering and Coherence in Shared Memory Multiprocessors, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Engineering, </institution> <type> Technical Report CENG 89-19, </type> <institution> University of Southern California, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: These considerations motivate an alternative programmer's model that relies on synchronization that is visible to the hardware to order memory accesses. Dubois, Scheurich and Briggs have defined such systems in terms of conditions on hardware and have named them weakly ordered <ref> [DSB86,DSB88, Sch89] </ref>. We believe that weak ordering facilitates high performance implementations, but that programmers prefer to reason about sequentially consistent memory rather than weaker memory systems or even write buffers. Hence, a description of memory should not require the specification of the performance enhancing features of the underlying hardware. <p> As will be apparent later, this option functions as a weakly ordered system. Dubois, Scheurich and Briggs have analyzed the problem of ensuring sequential consistency in systems that allow caching of shared variables, without impos - 3 - - -- ing any constraints on the interconnection network <ref> [DSB86, DSB88, ScD87, Sch89] </ref>. A sufficient condition for sequential consistency for cache-based systems has been stated [ScD87, Sch89]. The condition is satisfied if all processors issue their accesses in program order, and no access is issued by a processor until its previous accesses have been globally performed. <p> A sufficient condition for sequential consistency for cache-based systems has been stated <ref> [ScD87, Sch89] </ref>. The condition is satisfied if all processors issue their accesses in program order, and no access is issued by a processor until its previous accesses have been globally performed. A write is globally performed when its modification has been propagated to all processors. <p> The notion of strong ordering as an equivalent of sequential consistency was defined in [DSB86]. However, there do exist programs that can distinguish between strong ordering and sequential consistency [AdH89] and hence, strong ordering is not strictly equivalent to sequential consistency. Strong ordering has been discarded in <ref> [Sch89] </ref> in favor of a similar model, viz., concurrent consistency. A concurrently consistent system is defined to behave like a sequentially consistent system for most practical purposes. <p> It was recognized later in <ref> [ScD88, Sch89] </ref> that the above three conditions are not necessary to meet the intuitive goals of weak ordering. In Section 3, we give a new definition that we believe formally specifies this intuition.
Reference: [ShS88] <author> D. SHASHA and M. SNIR, </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory, </title> <journal> ACM Trans. on Programming Languages and Systems 10, </journal> <month> 2 (April </month> <year> 1988), </year> <pages> 282-312. </pages> - -- 
Reference-contexts: He has proved that for most practical purposes, a system where all processors observe all write operations in the same order (called write synchronization), is indistinguishable from a system where all writes are executed atomically. Shasha and Snir have proposed a software algorithm to ensure sequential consistency <ref> [ShS88] </ref>. Their scheme statically identifies a minimal set of pairs of accesses within a process, such that delaying the issue of one of the elements in each pair until the other is globally performed guarantees sequential consistency.
References-found: 24

