URL: ftp://ftp.cs.washington.edu/tr/1997/02/UW-CSE-97-02-04.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-date.html
Root-URL: http://www.cs.washington.edu
Title: The Cranium Network Interface Architecture: Support for Message Passing on Adaptive Packet Routing Networks  
Author: by Neil R. McKenzie 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Chairperson of Supervisory Committee) Program Authorized to Offer Degree Computer Science and Engineering  
Note: Date  
Date: 1997  January 31, 1997  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> David A. Patterson. </author> <title> Microprocessors in 2020. </title> <publisher> Scientific American, </publisher> <month> September </month> <year> 1995, </year> <pages> pp. 48-51. </pages>
Reference-contexts: The current rate of improvement in the performance of high-end processors is an impressive 55 percent per year; indeed this rate of increase exceeds the historical trend of 35 percent per year <ref> [1] </ref>. This trend is expected to continue unabated for the next two decades. Today's personal computers that cost a few thousand dollars outperform multi-million-dollar supercomputers from the 1970s. <p> There are a myriad of trends that will affect the design and implementation of future scalable parallel computers. The greatest impact comes from the opposite ends of the spectrum: the trends in VLSI technology and the trends in system level design. VLSI technology continues to follow Moore's Law <ref> [1] </ref>, but there are signs that the historical exponential growth in chip density and performance will finally level off in the next decade or two. The trend in system level design is away from large monolithic systems and is heading towards networks of workstations.
Reference: [2] <author> W. Daniel Hillis. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1985. </year>
Reference-contexts: Scalable parallel architectures have been referred to in the literature as MPP (massively parallel processor) machines <ref> [2, 3, 4] </ref> to distinguish them from SMP machines. The remainder of this dissertation is concerned only with scalable MPP architectures. Many different types of scalable networks have been developed for parallel computer systems. <p> Some examples used in commercial systems are the network interfaces in the Thinking Machines CM-1 and CM-5 <ref> [2, 14] </ref>, the Paragon and Teraflops multicomput-ers by Intel [17, 18], and the Cray Research T3D and T3E [19, 20, 21].
Reference: [3] <author> Charles L. Seitz. </author> <title> The Cosmic Cube. </title> <booktitle> Communications of the ACM 28(1), Jan-uary 1985, </booktitle> <pages> pp. 22-33. </pages>
Reference-contexts: Scalable parallel architectures have been referred to in the literature as MPP (massively parallel processor) machines <ref> [2, 3, 4] </ref> to distinguish them from SMP machines. The remainder of this dissertation is concerned only with scalable MPP architectures. Many different types of scalable networks have been developed for parallel computer systems.
Reference: [4] <author> William C. Athas and Charles L. Seitz. </author> <title> Multicomputers: message-passing concurrent computers. </title> <booktitle> IEEE Computer 21(8), </booktitle> <month> August </month> <year> 1988, </year> <pages> pp. 9-24. </pages>
Reference-contexts: Scalable parallel architectures have been referred to in the literature as MPP (massively parallel processor) machines <ref> [2, 3, 4] </ref> to distinguish them from SMP machines. The remainder of this dissertation is concerned only with scalable MPP architectures. Many different types of scalable networks have been developed for parallel computer systems. <p> As the number of nodes increases, the bandwidth bottleneck at the root node dominates the total time. A more efficient technique for large p is a tree broadcast. One type of tree broadcast is based on hypercubes of increasing dimension <ref> [4] </ref>. The root node (node 0) sends to nodes p=2, 3p=4, 7p=8 and so on. Node p=2 sends to node p=4, 3p=8, and so on, recursively. In all there are log p phases. Figure 6.4 illustrates the case for p = 8.
Reference: [5] <editor> Kevin Bolding and Lawrence Snyder, eds. </editor> <booktitle> Proc. of Parallel Computer Routing and Communication Workshop, </booktitle> <address> Seattle WA, May 1994, </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The remainder of this dissertation is concerned only with scalable MPP architectures. Many different types of scalable networks have been developed for parallel computer systems. There exists an extensive literature on this subject; for example, refer to the proceedings of the Parallel Computer Routing and Communication Workshop <ref> [5] </ref>. Scalable networks are constructed from a single basic circuit called the router or routing node. The router is replicated potentially hundreds or thousands of times.
Reference: [6] <author> William J. Dally. </author> <title> Wire-efficient VLSI multiprocessor communication networks. </title> <booktitle> Advanced Research in VLSI: Proc. of the 1987 Stanford Conference, </booktitle> <publisher> MIT Press, </publisher> <year> 1987, </year> <pages> pp. 391-415. </pages>
Reference-contexts: For ease of construction it is helpful if the configuration can be extended in an obvious way, in a fashion similar to the way Lego bricks are stacked together, in either two or three dimensions <ref> [6] </ref>. The physical links between neighboring routers are constructed using metal wires or in some cases using optical fibers.
Reference: [7] <author> Paul Pierce. </author> <title> The NX message passing interface. </title> <booktitle> Parallel Computing 20(4), </booktitle> <month> April </month> <year> 1994, </year> <pages> pp. 463-80. </pages>
Reference-contexts: Finally we state the thesis of this dissertation which concerns the architecture and implementation of a network interface for an MPP computer system. 1.3 The role of the network interface in message passing In general, scalable parallel computers use a style of communication known as message passing <ref> [7, 8, 9] </ref>. The invocation of each message is stated explicitly in the program, 8 by means of send and receive commands 1 . A simple message consists of a block of data, whose size may range from a single bit to millions of bits. <p> In Chapter 3 we explain the difficulty of interfacing processors to adaptive routers, through the use of a specific design example. We then specify the Cranium architecture. Chapter 4 describes the software interface for Cranium, and compares it with other message passing systems such as Intel NX <ref> [7] </ref>. Chapter 5 describes a simulation environment that was developed to evaluate the performance of Cranium, based on the Talisman 19 processor simulator [27] and the Chaos router [15]. Chapter 6 characterizes the performance of Cranium. We begin with an analysis of the basic latency and throughput behavior of Cranium. <p> A complete breakdown of the interrupt mask and status registers is described in Section A.2.3 in Appendix A. 4.5 Comparison with other message passing interfaces 4.5.1 Intel NX The Intel NX message passing interface is a canonical deeply-layered message passing interface for scalable parallel computing <ref> [7, 17, 35] </ref>. It is the message passing system used in every scalable parallel machine produced by Intel, from the earlier iPSC/2 and iPSC/860 machines to the later machines such as the Touchstone Delta and the 75 Paragon. <p> The most popular choice for an efficient implementation language style is the sequential imperative style. Usually a C or Fortran program is augmented with a message-passing library such as the Intel NX library <ref> [7] </ref>. This technique for parallel program development is known as hand-crafted code, where each communication operation is stated explicitly in the program. The competing approach is to write programs in a high-level language that is designed for parallel programming.
Reference: [8] <author> J. J. Dongarra, R. Hempel, A. J. G. Hey and D. W. Walker. </author> <title> A draft standard for message passing in a distributed memory environment. </title> <booktitle> Proceedings of the Fifth ECMWF Workshop on the Use of Parallel Processors in Meteorology: Parallel Supercomputing in Atmospheric Science, </booktitle> <address> Reading, UK, </address> <month> Nov. </month> <year> 1992, </year> <pages> pp. 465-81. </pages>
Reference-contexts: Finally we state the thesis of this dissertation which concerns the architecture and implementation of a network interface for an MPP computer system. 1.3 The role of the network interface in message passing In general, scalable parallel computers use a style of communication known as message passing <ref> [7, 8, 9] </ref>. The invocation of each message is stated explicitly in the program, 8 by means of send and receive commands 1 . A simple message consists of a block of data, whose size may range from a single bit to millions of bits. <p> NX has proven to be quite separable; it has been implemented on a wide variety of non-Intel systems such as Stanford DASH [45], Princeton SHRIMP [42, 43] and UW Meerkat [35]. NX is similar to PVM (Parallel Virtual Machine) and MPI (Message Passing Interface) <ref> [8, 9] </ref>. The basic subset of the NX interface is described by function calls csend () and crecv () that provide blocking communication. The parameter list for the NX functions is similar to the command structure for Cranium's send channels and auto-receive channels.
Reference: [9] <author> J. Bruck, D. Dolev, Ching Tien Ho, M. C. Rosu and R. </author> <title> Strong. Efficient message passing interface (MPI) for parallel computing on clusters of workstations. </title> <booktitle> Proc of 7th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA '95), </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995, </year> <pages> pp. 64-73. 197 </pages>
Reference-contexts: Finally we state the thesis of this dissertation which concerns the architecture and implementation of a network interface for an MPP computer system. 1.3 The role of the network interface in message passing In general, scalable parallel computers use a style of communication known as message passing <ref> [7, 8, 9] </ref>. The invocation of each message is stated explicitly in the program, 8 by means of send and receive commands 1 . A simple message consists of a block of data, whose size may range from a single bit to millions of bits. <p> NX has proven to be quite separable; it has been implemented on a wide variety of non-Intel systems such as Stanford DASH [45], Princeton SHRIMP [42, 43] and UW Meerkat [35]. NX is similar to PVM (Parallel Virtual Machine) and MPI (Message Passing Interface) <ref> [8, 9] </ref>. The basic subset of the NX interface is described by function calls csend () and crecv () that provide blocking communication. The parameter list for the NX functions is similar to the command structure for Cranium's send channels and auto-receive channels.
Reference: [10] <author> Edward W. Felten. </author> <title> Protocol compilation: high-performance communication for parallel programs. </title> <type> PhD dissertation, </type> <institution> University of Washington, Dept. of CSE, </institution> <month> Sept. </month> <year> 1993, </year> <note> UW-CSE-TR 93-09-09. </note>
Reference-contexts: The application program at the receiving processing node is notified that the message has arrived, whereupon code is dispatched to perform any necessary post-processing of the message. Arbitrarily complicated message protocols can be built on top of simple messages. A message protocol <ref> [10] </ref> is an agreement between the sender and the receiver concerning the size, format and sequence of the message. Typically, message passing systems provide a number of different message protocols for the application programmer. <p> An active message includes the address of the receiving node's software handler routine in the message itself, and the overhead of dispatch is thereby reduced to a small fixed cost. Protocol compilers address the protocol choice problem. Parachute <ref> [10] </ref> is one such protocol compiler that analyzes message passing patterns in a parallel program; it automatically gen 17 erates a new program where the optimal protocol is selected for each message in the program. <p> These variations on the basic send/receive interface make it possible to overlap communication and computation and avoid using operating system buffers. Furthermore, 76 there exists a little-known variation on non-blocking communication called force-type messages <ref> [10] </ref>. Force-type messages provide an unbuffered message protocol to the application program. An NX message is designated as force-type if certain high-order bits in the message tag are set (resulting in a very large unsigned or negative tag value).
Reference: [11] <author> John Y. Ngai and Charles L. Seitz. </author> <title> A framework for adaptive routing in mul-ticomputer networks. </title> <booktitle> Proc. of the Symposium on Parallel Architectures and Algorithms, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: Under oblivious routing, each packet in the message follows the one in front along the same path; packets arrive in the same order as they were injected. There are also adaptive routing algorithms. In an adaptive routing algorithm <ref> [11, 12] </ref>, the packets comprising a long message choose different paths, depending on the instantaneous level of congestion encountered at each router. Adaptivity often increases the throughput of the network because it increases the number of paths that can be taken and thereby helps distribute the workload.
Reference: [12] <author> Kevin Bolding and Lawrence Snyder. </author> <title> Mesh and torus chaotic routing. </title> <booktitle> Advanced Research in VLSI and Parallel Systems; Proc. of the 1992 Brown/MIT Conference, </booktitle> <month> March </month> <year> 1992, </year> <pages> pp. 333-347. </pages>
Reference-contexts: Under oblivious routing, each packet in the message follows the one in front along the same path; packets arrive in the same order as they were injected. There are also adaptive routing algorithms. In an adaptive routing algorithm <ref> [11, 12] </ref>, the packets comprising a long message choose different paths, depending on the instantaneous level of congestion encountered at each router. Adaptivity often increases the throughput of the network because it increases the number of paths that can be taken and thereby helps distribute the workload. <p> The processing node and memory are simulated by Talisman [27], a functional simulator augmented with a timing model. The Cranium network interface model was added to Talisman. The network is simulated by the Chaos network simulator <ref> [12, 15] </ref>, a structural simulator written in C. The remainder of this chapter discusses the background of both simulators and the model for Cranium in the test environment. 5.1 Talisman Talisman [27] is a processor simulator created by Robert Bedichek. The strength of Talisman is its fast host execution performance. <p> It was invented by Magda Konstantinidou and Lawrence Snyder as an algorithm for routing in hypercube networks [73]; the algorithm was adapted by Kevin Bolding and Melanie Fulgham to apply it to mesh and torus networks <ref> [12, 74] </ref>. Konstantinidou, Bolding and Fulgham and many others 1 created a structural simulator called the Chaos simulator to evaluate the performance of the routing algorithm. In its current form, it simulates complete networks of chaotic routers organized as two-dimensional meshes or tori. <p> The rest of this section is a brief overview of chaotic routing in general and the design of a prototype CMOS chip that implements the algorithm. Much more detail on chaotic routing can be found in the literature <ref> [12, 15, 13, 66, 73, 74, 75] </ref>. The chaotic routing algorithm uses fixed-length packets and a packet routing technique called virtual cut-through [76]. A packet consists of a sequence of words called phits (physical units); each phit is the width of a router link. <p> If the data set sizes were larger and subsequently the network needed to handle a large number of large messages simultaneously, then internal congestion in the network can cause a performance bottleneck. The impact of the network on communication performance has been the focus of other research projects <ref> [12, 15, 74] </ref>. demonstrate that the communication performance achieved by these benchmarks is spread over a wide range. The horizontal solid lines in the two graphs represent TP peak for the two cases.
Reference: [13] <author> Kevin Bolding and William Yost. </author> <title> Design of a router for fault-tolerant networks. </title> <booktitle> Proc. of Parallel Computer Routing and Communication Workshop, </booktitle> <address> Seattle WA, May 1994, </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 226-240. </pages>
Reference-contexts: Adaptive routing has several other advantages over oblivious routing. It improves the opportunities for fault tolerance, the ability to operate in the presence of failed routers or processors <ref> [13] </ref>. It also simplifies some issues concerning the ability of the operating system to manage multiple user contexts, by saving the state of the network and later re-injecting packet traffic. <p> The rest of this section is a brief overview of chaotic routing in general and the design of a prototype CMOS chip that implements the algorithm. Much more detail on chaotic routing can be found in the literature <ref> [12, 15, 13, 66, 73, 74, 75] </ref>. The chaotic routing algorithm uses fixed-length packets and a packet routing technique called virtual cut-through [76]. A packet consists of a sequence of words called phits (physical units); each phit is the width of a router link.
Reference: [14] <author> Charles Leiserson, Z. S. Abuhamdeh, D. Douglas, C. Feynmann, M. Ganmuki, J. Hill, W. D. Hillis, B. Kuszmaul, M. St. Pierre, D. Wells, M. Wong, S.- W. Yang and R. Zak. </author> <title> The network architecture of the CM-5. </title> <booktitle> Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992, </year> <pages> pp. 272-285. </pages>
Reference-contexts: A network and parallel programming system that depends on in-order packet delivery makes it very difficult to provide this capability in an efficient way. Some examples of adaptive network routers that have been realized in silicon are the CM-5 network router <ref> [14] </ref> and the Chaos router [15]. 13 A potential disadvantage of adaptivity is that packets may arrive in a different order than they are sent. <p> Some examples used in commercial systems are the network interfaces in the Thinking Machines CM-1 and CM-5 <ref> [2, 14] </ref>, the Paragon and Teraflops multicomput-ers by Intel [17, 18], and the Cray Research T3D and T3E [19, 20, 21]. <p> Connecting through the memory bus provides greater flexibility than connecting to the cache bus, and greater performance than connecting through an I/O bus. Network interfaces in this category include the Thinking Machines CM-5 <ref> [14, 34] </ref>, the Cray Research T3D and T3E [19, 20, 21], the Intel Paragon [17] and University of Washington Meerkat-1 [35]. <p> Single errors can be detected through the use of the packet counting scheme described in Section 2.2.4. Lost packets are detected through the use of an application-program timeout. Another technique is used in the CM-5 <ref> [14] </ref> the global packet count in the entire system is continually computed and propagated to all nodes. The idea is like Kirchoff's Law, where the sum of everything that went in should be equal to everything that comes out. <p> The disadvantage is that different optimization techniques may need to be developed for every implementation of the send/receive abstraction, because the implementation styles vary greatly. Under send/receive there is little restriction on the type of network, packet size or the built-in support provided by the network interface. The CM-5 <ref> [14] </ref> network interface provides the canonical example of a simple send/receive model under PIO. To send a packet, the user program stores the destination node ID, the packet payload and the packet length into the memory-mapped network interface registers. <p> Table 2.1 describes the attributes of eight different network interfaces: University of Washington Meerkat-1 [35], Intel Paragon, Princeton SHRIMP [42], CMU iWarp [23], Stanford DASH [45], Thinking Machines CM-5 <ref> [14] </ref>, MIT MDP [22] and HP Labs Hamlyn [57, 58]. Some entries appear more than once because there are multiple attributes to many of these network interfaces. For instance, iWarp contains both systolic communication and DMA. <p> Barriers are therefore fundamentally global in scope. Also, many scalable computer systems support barrier synchronization directly in hardware <ref> [14, 49, 66] </ref>. The latency of a global barrier is often an order of magnitude less than the latency of a regular message. Figure 4.5 is a flow diagram showing the interaction between nodes S and Ra when barrier synchronization is used. <p> AM is fundamentally non-blocking and offers fast recovery of message buffer space. It is ideally suited to a user-level interface that provides a buffered communication protocol, like the user queue in Cranium or the FIFO interface of the CM-5 <ref> [14] </ref>. The advantages of AM are that it is very simple and it introduces minimal overhead on top of the resources provided by the hardware. Dispatching the handler function is usually significantly faster than the tag matching approach of NX, which 78 requires a slow switch/case or if-then-else construct. <p> To improve the performance of application programs that use broadcast, some 111 multicomputers use dedicated hardware and/or a separate broadcast network to reduce the latency of broadcasting. A well known example in a commercial system is the CM-5 control network <ref> [14] </ref>. For the torus network, an elegant solution is provided by the Express Broadcast Network (EBN) [61]. EBN is a low-cost extension to mesh and torus data networks for supporting low-latency broadcast of control messages. EBN increases the width of each link in the existing network by one extra wire. <p> The effects of these modifications can be evaluated by observing the change in maximum throughput achievable by the interface and the change in the cost of communication for application programs. The competing network interfaces of interest are the interfaces in the CM-5, SHRIMP-I and SHRIMP-II <ref> [14, 42, 43] </ref>. The following list of modifications M1 through M4 transform Cranium into interfaces that bear a close resemblance to one of the three competing network interface styles.
Reference: [15] <author> Kevin Bolding. </author> <title> Chaotic routing: design and implementation of an adaptive multicomputer network router. </title> <type> PhD dissertation, </type> <institution> University of Washington, Dept. of CSE, </institution> <address> Seattle WA, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: A network and parallel programming system that depends on in-order packet delivery makes it very difficult to provide this capability in an efficient way. Some examples of adaptive network routers that have been realized in silicon are the CM-5 network router [14] and the Chaos router <ref> [15] </ref>. 13 A potential disadvantage of adaptivity is that packets may arrive in a different order than they are sent. Multiple pathways allow packets to pass one another in the network; packets travel at different rates due to the local instantaneous congestion encountered along the way. <p> Chapter 4 describes the software interface for Cranium, and compares it with other message passing systems such as Intel NX [7]. Chapter 5 describes a simulation environment that was developed to evaluate the performance of Cranium, based on the Talisman 19 processor simulator [27] and the Chaos router <ref> [15] </ref>. Chapter 6 characterizes the performance of Cranium. We begin with an analysis of the basic latency and throughput behavior of Cranium. We measure the performance of parallel programs that run on the simulator. <p> Chapter 3 THE Cranium NETWORK INTERFACE ARCHITECTURE Architecture is frozen music. Goethe This chapter describes the design of the Cranium network interface architecture. The initial motivation for Cranium came from a requirement to design a high-performance companion interface to the Chaos network router <ref> [15, 60] </ref>. The Chaos router has two interesting attributes. It routes small fixed-size packets, using a payload the size of a processor cache-line (e.g. 32 bytes). <p> The processing node and memory are simulated by Talisman [27], a functional simulator augmented with a timing model. The Cranium network interface model was added to Talisman. The network is simulated by the Chaos network simulator <ref> [12, 15] </ref>, a structural simulator written in C. The remainder of this chapter discusses the background of both simulators and the model for Cranium in the test environment. 5.1 Talisman Talisman [27] is a processor simulator created by Robert Bedichek. The strength of Talisman is its fast host execution performance. <p> The rest of this section is a brief overview of chaotic routing in general and the design of a prototype CMOS chip that implements the algorithm. Much more detail on chaotic routing can be found in the literature <ref> [12, 15, 13, 66, 73, 74, 75] </ref>. The chaotic routing algorithm uses fixed-length packets and a packet routing technique called virtual cut-through [76]. A packet consists of a sequence of words called phits (physical units); each phit is the width of a router link. <p> In a heavily loaded network, de-routing can take a packet along a non-minimal path. Since packets take different paths even in a lightly loaded network, the presence of instantaneous congestion can cause packets to overtake one another and arrive out-of-order. Bolding's dissertation <ref> [15] </ref> describes the simulation results of chaotic routing com 2 It is also possible to construct asynchronous versions of chaotic routing, but for simplicity syn chronous circuitry is assumed here. 88 P S pared with two other routing algorithms: the standard dimension-order oblivious algorithm and deflection routing, another adaptive algorithm, under <p> In particular a single-hop path involving two nearest-neighbor routers requires 8 cycles plus the packet length. These assumptions are based on the routing algorithm used in the Chaos router <ref> [15] </ref>. * Processor-network link. The processor-network link is four bytes wide; the maximum throughput into or out from the processing node is four bytes per cycle. Data movement on the link is bi-directional and half-duplex. <p> If the data set sizes were larger and subsequently the network needed to handle a large number of large messages simultaneously, then internal congestion in the network can cause a performance bottleneck. The impact of the network on communication performance has been the focus of other research projects <ref> [12, 15, 74] </ref>. demonstrate that the communication performance achieved by these benchmarks is spread over a wide range. The horizontal solid lines in the two graphs represent TP peak for the two cases. <p> The impact is that signals going on or off chip are delayed by one clock cycle. * Internal FIFOs impose a one clock cycle delay. The architecture of the FIFOs is based on a scheme using a dual-port SRAM and two counters representing the head and tail pointers <ref> [15] </ref>. The dual-port property permits the FIFO to accept an incoming value and propagate an outgoing value concurrently at separate memory locations. * All ALU functions are relatively simple and execute within a single clock cycle.
Reference: [16] <author> Alexander C. Klaiber. </author> <title> Architectural support for compiler-generated data-parallel programs. </title> <type> PhD dissertation, </type> <institution> University of Washington, Dept. of CSE, </institution> <month> Sept. </month> <year> 1994, </year> <note> UW-CSE-TR 94-09-09. </note>
Reference-contexts: Some programs involve bi-modal message traffic, in that they require both long and short messages. In one study of message traffic in a typical set of parallel program benchmarks, 98% of all messages contained fewer than 40 bytes <ref> [16] </ref>. The same study showed that these small messages 16 comprised 55% of the total bytes communicated. The other 45% of the total bytes came from the 2% of the messages that were 40 bytes or longer. Many other studies confirm the bi-modality of message traffic.
Reference: [17] <author> Paul Pierce and Greg Regnier. </author> <title> The Paragon implementation of the NX message passing interface. </title> <booktitle> Proc. of the Scalable High Performance Computing Conference (SHPCC94), </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 184-190. </pages>
Reference-contexts: Some examples used in commercial systems are the network interfaces in the Thinking Machines CM-1 and CM-5 [2, 14], the Paragon and Teraflops multicomput-ers by Intel <ref> [17, 18] </ref>, and the Cray Research T3D and T3E [19, 20, 21]. In all of these systems, the network interface is implemented using a separate chip, and the computing node is based on a standard processor architecture such as the SPARC, the DEC Alpha or the Intel Pentium. <p> Connecting through the memory bus provides greater flexibility than connecting to the cache bus, and greater performance than connecting through an I/O bus. Network interfaces in this category include the Thinking Machines CM-5 [14, 34], the Cray Research T3D and T3E [19, 20, 21], the Intel Paragon <ref> [17] </ref> and University of Washington Meerkat-1 [35]. I/O bus connected network interfaces In parallel systems that use a local area network as its communication backbone, the preferred location of the network interface is at the I/O bus. I/O bus cards are relatively simple and inexpensive to create. <p> A complete breakdown of the interrupt mask and status registers is described in Section A.2.3 in Appendix A. 4.5 Comparison with other message passing interfaces 4.5.1 Intel NX The Intel NX message passing interface is a canonical deeply-layered message passing interface for scalable parallel computing <ref> [7, 17, 35] </ref>. It is the message passing system used in every scalable parallel machine produced by Intel, from the earlier iPSC/2 and iPSC/860 machines to the later machines such as the Touchstone Delta and the 75 Paragon.
Reference: [18] <author> Joseph Carbonaro and Frank Verhoorn. Cavallino: </author> <title> the Teraflops router and NIC. </title> <booktitle> Proc. of Hot Interconnects IV, </booktitle> <address> Stanford University, Palo Alto CA, </address> <month> August </month> <year> 1996, </year> <pages> pp. 157-160. 198 </pages>
Reference-contexts: Some examples used in commercial systems are the network interfaces in the Thinking Machines CM-1 and CM-5 [2, 14], the Paragon and Teraflops multicomput-ers by Intel <ref> [17, 18] </ref>, and the Cray Research T3D and T3E [19, 20, 21]. In all of these systems, the network interface is implemented using a separate chip, and the computing node is based on a standard processor architecture such as the SPARC, the DEC Alpha or the Intel Pentium.
Reference: [19] <author> Steve Scott and Greg Thorson. </author> <title> Optimized routing in the Cray T3D. </title> <booktitle> Proc. of Parallel Computer Routing and Communication Workshop, </booktitle> <address> Seattle WA, May 1994, </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 281-294. </pages>
Reference-contexts: Some examples used in commercial systems are the network interfaces in the Thinking Machines CM-1 and CM-5 [2, 14], the Paragon and Teraflops multicomput-ers by Intel [17, 18], and the Cray Research T3D and T3E <ref> [19, 20, 21] </ref>. In all of these systems, the network interface is implemented using a separate chip, and the computing node is based on a standard processor architecture such as the SPARC, the DEC Alpha or the Intel Pentium. <p> Connecting through the memory bus provides greater flexibility than connecting to the cache bus, and greater performance than connecting through an I/O bus. Network interfaces in this category include the Thinking Machines CM-5 [14, 34], the Cray Research T3D and T3E <ref> [19, 20, 21] </ref>, the Intel Paragon [17] and University of Washington Meerkat-1 [35]. I/O bus connected network interfaces In parallel systems that use a local area network as its communication backbone, the preferred location of the network interface is at the I/O bus. <p> There are many examples of multicomputers that use this communication model, including Cray Research T3D and T3E <ref> [19, 20, 21] </ref>, Fujitsu AP1000 [48, 49], Stanford DASH [45], 37 Tera MTA-1 [46] and Kendall Square Research KSR-1 and KSR-2 [33].
Reference: [20] <author> Steve Scott and Greg Thorson. </author> <title> The Cray T3E network: adaptive routing in a high performance 3-d torus. </title> <booktitle> Proc. of Hot Interconnects IV, </booktitle> <address> Stanford University, Palo Alto CA, </address> <month> August </month> <year> 1996, </year> <pages> pp. 147-156. </pages>
Reference-contexts: Some examples used in commercial systems are the network interfaces in the Thinking Machines CM-1 and CM-5 [2, 14], the Paragon and Teraflops multicomput-ers by Intel [17, 18], and the Cray Research T3D and T3E <ref> [19, 20, 21] </ref>. In all of these systems, the network interface is implemented using a separate chip, and the computing node is based on a standard processor architecture such as the SPARC, the DEC Alpha or the Intel Pentium. <p> Connecting through the memory bus provides greater flexibility than connecting to the cache bus, and greater performance than connecting through an I/O bus. Network interfaces in this category include the Thinking Machines CM-5 [14, 34], the Cray Research T3D and T3E <ref> [19, 20, 21] </ref>, the Intel Paragon [17] and University of Washington Meerkat-1 [35]. I/O bus connected network interfaces In parallel systems that use a local area network as its communication backbone, the preferred location of the network interface is at the I/O bus. <p> There are many examples of multicomputers that use this communication model, including Cray Research T3D and T3E <ref> [19, 20, 21] </ref>, Fujitsu AP1000 [48, 49], Stanford DASH [45], 37 Tera MTA-1 [46] and Kendall Square Research KSR-1 and KSR-2 [33].
Reference: [21] <author> Steve Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> Proc. of ASPLOS VII, </booktitle> <address> Cambridge MA, </address> <month> October </month> <year> 1996, </year> <pages> pp. 26-36. </pages>
Reference-contexts: Some examples used in commercial systems are the network interfaces in the Thinking Machines CM-1 and CM-5 [2, 14], the Paragon and Teraflops multicomput-ers by Intel [17, 18], and the Cray Research T3D and T3E <ref> [19, 20, 21] </ref>. In all of these systems, the network interface is implemented using a separate chip, and the computing node is based on a standard processor architecture such as the SPARC, the DEC Alpha or the Intel Pentium. <p> Connecting through the memory bus provides greater flexibility than connecting to the cache bus, and greater performance than connecting through an I/O bus. Network interfaces in this category include the Thinking Machines CM-5 [14, 34], the Cray Research T3D and T3E <ref> [19, 20, 21] </ref>, the Intel Paragon [17] and University of Washington Meerkat-1 [35]. I/O bus connected network interfaces In parallel systems that use a local area network as its communication backbone, the preferred location of the network interface is at the I/O bus. <p> In the send interface, the processor stalls while the network is busy. In the receive interface, the processor stalls until the packet arrives. Dispatch is implicit as the processor resumes processing. Multicomputers that use stall notification include the Cray T3E <ref> [21] </ref>, DASH [45] and the Tera MTA-1 [46]. The MTA-1 is a multithreaded processor; while the thread that is waiting for the network is stalled, the processor automatically switches to another thread that is not waiting for the network. Stalling often requires special support in the memory system. <p> There are many examples of multicomputers that use this communication model, including Cray Research T3D and T3E <ref> [19, 20, 21] </ref>, Fujitsu AP1000 [48, 49], Stanford DASH [45], 37 Tera MTA-1 [46] and Kendall Square Research KSR-1 and KSR-2 [33].
Reference: [22] <author> William J. Dally, J. A. S. Fiske, John S. Keen, Richard A. Lethin, Michael D. Noakes, Peter R. Nuth, Roy E. Davison and Gregory A. Fyler. </author> <title> The Message-Driven Processor: a multicomputer processing node with efficient mechanisms. </title> <booktitle> IEEE Micro, </booktitle> <month> April </month> <year> 1992, </year> <pages> pp. 23-39. </pages>
Reference-contexts: In all of these systems, the network interface is implemented using a separate chip, and the computing node is based on a standard processor architecture such as the SPARC, the DEC Alpha or the Intel Pentium. Some examples from academic research projects are the MIT Message-Driven Processor (MDP) <ref> [22] </ref>, the CMU iWarp [23] and the Caltech Mosaic [24]. In all of these projects, the network interface is tightly coupled with the processor; i.e. the network interface and the processor are placed on the same silicon. <p> Location D is connected to an I/O bus. 23 is represented by special message registers (operands) and/or instructions (operators). Most tightly-coupled interface designs use special-purpose message instructions (e.g. a send command) in which general-purpose processor registers are the operands. Some examples include the Message Driven Processor (MDP) <ref> [22] </ref>, the Caltech Mosaic [24], the Henry-Joerg network interface [29] and the Start (*T) [30] network interface. An exception is iWarp from CMU [23] whose systolic communication model is based on operands rather than operators. <p> Table 2.1 describes the attributes of eight different network interfaces: University of Washington Meerkat-1 [35], Intel Paragon, Princeton SHRIMP [42], CMU iWarp [23], Stanford DASH [45], Thinking Machines CM-5 [14], MIT MDP <ref> [22] </ref> and HP Labs Hamlyn [57, 58]. Some entries appear more than once because there are multiple attributes to many of these network interfaces. For instance, iWarp contains both systolic communication and DMA. <p> Special-purpose processors have hardware support for fast interrupt handling. For example, the MDP in the J-machine can dispatch an interrupt handler to react to an incoming packet in only three clock cycles <ref> [22] </ref>. However, even if interrupts are more expensive than polling, they may introduce less overhead if they occur only in rare circumstances. For instance, for a given network architecture and application program, injection failure may be very infrequent. <p> A flit is the amount of contiguous information that can be halted by backpressure due to congestion in the network. In a wormhole router a phit and a flit are often the same size; in the J-machine a flit is 36 bits consisting of two 18-bit phits <ref> [22] </ref>.
Reference: [23] <author> Shekhar Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Sussman, J. Sutton, J. Urbanski and J. Webb. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> Proc. of the 17th Annual International Symposium on Computer Architecture, </booktitle> <address> Seattle WA, </address> <month> May </month> <year> 1990, </year> <pages> pp. 70-81. </pages>
Reference-contexts: Some examples from academic research projects are the MIT Message-Driven Processor (MDP) [22], the CMU iWarp <ref> [23] </ref> and the Caltech Mosaic [24]. In all of these projects, the network interface is tightly coupled with the processor; i.e. the network interface and the processor are placed on the same silicon. The motivation for the tight coupling is to reduce the overhead associated with short messages. <p> By contrast, most network interface designs that contain two different mechanisms use two completely separate strategies for short and long messages. For instance, the MIT Alewife interface [26] provides a message passing interface for long messages and a shared memory interface for short messages. The iWarp chip <ref> [23] </ref> also provides two separate strategies, one for large messages called memory communication and the other for small messages called systolic communication. These designs therefore become more complex than is necessary. <p> Some examples include the Message Driven Processor (MDP) [22], the Caltech Mosaic [24], the Henry-Joerg network interface [29] and the Start (*T) [30] network interface. An exception is iWarp from CMU <ref> [23] </ref> whose systolic communication model is based on operands rather than operators. A send command is constructed by using a message register as the destination of an arithmetic operation; a receive command is constructed by using a message register as a source operand. <p> Systolic communication provides very efficient data movement. Once the virtual circuit is created, the application program passes values only as it does not need to pass node IDs or buffer addresses. The iWarp system <ref> [23] </ref> provides a canonical example of systolic communication through its tightly-coupled interface. It uses a custom RISC processor core with on-chip message registers: two pipe inputs for sending and two pipe outputs for receiving. Information is sent immediately on every write to the pipe input register. <p> Table 2.1 describes the attributes of eight different network interfaces: University of Washington Meerkat-1 [35], Intel Paragon, Princeton SHRIMP [42], CMU iWarp <ref> [23] </ref>, Stanford DASH [45], Thinking Machines CM-5 [14], MIT MDP [22] and HP Labs Hamlyn [57, 58]. Some entries appear more than once because there are multiple attributes to many of these network interfaces. For instance, iWarp contains both systolic communication and DMA. <p> The system must run a wide variety of different compute-bound tasks. It is important to be able to run several different jobs concurrently and to provide efficient management for different user contexts (see Sections 3.3.3 and 3.4.3). This requirement rules out a special-purpose approach like that of iWarp <ref> [23] </ref>, an architecture that is best suited to systolic array processing applications such as real-time vision and image processing. 3.2 The difficulty of interfacing with adaptive routers To illustrate the difficulty associated with the design of a network interface for an out-of-order network, we present an interface based on the remote
Reference: [24] <author> Charles L. Seitz and Wen-King Su. </author> <title> The design of the Caltech Mosaic C Mul-ticomputer. </title> <booktitle> Proc. of the 1993 Symposium on Integrated Systems, </booktitle> <address> Seattle WA, </address> <month> April </month> <year> 1993, </year> <pages> pp. 1-22. </pages>
Reference-contexts: Some examples from academic research projects are the MIT Message-Driven Processor (MDP) [22], the CMU iWarp [23] and the Caltech Mosaic <ref> [24] </ref>. In all of these projects, the network interface is tightly coupled with the processor; i.e. the network interface and the processor are placed on the same silicon. The motivation for the tight coupling is to reduce the overhead associated with short messages. <p> Most tightly-coupled interface designs use special-purpose message instructions (e.g. a send command) in which general-purpose processor registers are the operands. Some examples include the Message Driven Processor (MDP) [22], the Caltech Mosaic <ref> [24] </ref>, the Henry-Joerg network interface [29] and the Start (*T) [30] network interface. An exception is iWarp from CMU [23] whose systolic communication model is based on operands rather than operators.
Reference: [25] <author> Thorsten von Eicken, David E. Culler, Seth C. Goldstein and Klaus E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> pp. 256-266. </pages>
Reference-contexts: Notification and dispatch for an arriving message take only three processor instructions, unlike the tens or hundreds of instructions required in conventional designs. Two other related projects address the performance problem in message passing using software-only techniques: active messages and protocol compilers. The motivation for active messages <ref> [25] </ref> is to improve the performance of data movement and software handler dispatch. In an active message, values from the payload of the arriving packet are incorporated immediately into the computation instead of being stored in memory as an intermediate step. <p> Cranium's restrictions allow a simple hardware implementation that delivers the full performance of the network and the memory system. 4.5.2 Active messages An alternative strategy to the heavily-layered approach of Intel NX is the thinly-layered technique known as active messages or AM <ref> [25] </ref>. Every packet in an active message contains a pointer to a handler function. When the packet arrives at the receiving node, the handler function is dispatched immediately and it runs to completion. AM is fundamentally non-blocking and offers fast recovery of message buffer space.
Reference: [26] <author> Anant Agarwal et al. </author> <title> The MIT Alewife machine: a large-scale distributed-memory multiprocessor. </title> <booktitle> Proc. of Workshop on Scalable Shared Memory Multiprocessors, </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <month> 199 </month>
Reference-contexts: The architecture presented in this dissertation indeed uses two different mechanisms for receiving packets but only one send mechanism. By contrast, most network interface designs that contain two different mechanisms use two completely separate strategies for short and long messages. For instance, the MIT Alewife interface <ref> [26] </ref> provides a message passing interface for long messages and a shared memory interface for short messages. The iWarp chip [23] also provides two separate strategies, one for large messages called memory communication and the other for small messages called systolic communication. <p> The extra directory memory and the coherence logic make global coherency much more complicated to implement compared with the basic model. Examples of globally coherent remote memory systems are Stanford DASH [45], MIT Alewife <ref> [26] </ref> and Convex Exemplar, based on the Scalable Coherent Interconnect (SCI) memory interface [50]. All of these systems are based on networks that provide in-order packet delivery. * Princeton SHRIMP [42, 43]. SHRIMP is a hybrid of the systolic and remote store models.
Reference: [27] <author> Robert Bedichek. Talisman: </author> <title> fast and accurate multicomputer simulation. </title> <booktitle> Proceedings of ACM SIGMETRICS '95, </booktitle> <address> Ottawa, Ontario, Canada, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: We then specify the Cranium architecture. Chapter 4 describes the software interface for Cranium, and compares it with other message passing systems such as Intel NX [7]. Chapter 5 describes a simulation environment that was developed to evaluate the performance of Cranium, based on the Talisman 19 processor simulator <ref> [27] </ref> and the Chaos router [15]. Chapter 6 characterizes the performance of Cranium. We begin with an analysis of the basic latency and throughput behavior of Cranium. We measure the performance of parallel programs that run on the simulator. <p> The primary disadvantage is cost, on the order of tens of thousands to millions of dollars. The Cranium test environment is based on a hybrid of functional simulation and structural simulation. The processing node and memory are simulated by Talisman <ref> [27] </ref>, a functional simulator augmented with a timing model. The Cranium network interface model was added to Talisman. The network is simulated by the Chaos network simulator [12, 15], a structural simulator written in C. <p> The Cranium network interface model was added to Talisman. The network is simulated by the Chaos network simulator [12, 15], a structural simulator written in C. The remainder of this chapter discusses the background of both simulators and the model for Cranium in the test environment. 5.1 Talisman Talisman <ref> [27] </ref> is a processor simulator created by Robert Bedichek. The strength of Talisman is its fast host execution performance. Functional processor simulation requires two parts: translation from the target instruction set to the host instruction set and execution of host instructions on the host.
Reference: [28] <author> Thomas E. Anderson, David E. Culler and David A. Patterson. </author> <title> A case for networks of workstations (NOW). </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: We compare their similarities and differences in order to construct a taxonomy. We examine features of network interfaces in scalable, massively parallel (MPP) machines as well as systems based on networks of workstations (NOW) <ref> [28] </ref>. Every network interface is a combination of two interfaces: * The physical interface provides a data path between the computing node and the network. <p> The purpose of this project is to investigate the use of adaptive routing in a local area network, through the construction of a high performance network of workstations (NOW) <ref> [28] </ref>. The proposed configuration of Chaos-LAN is described in Figure 8.1. The environment consists of a collection of workstations from Digital Equipment Corporation based on the Alpha processor. They are connected to a central hub containing a Chaos network consisting of 16 Chaos routers connected as a two-dimensional torus.
Reference: [29] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A tightly coupled processor-network interface. </title> <booktitle> Proc. of the 5th ASPLOS, </booktitle> <month> October </month> <year> 1992, </year> <pages> pp. 111-122. </pages>
Reference-contexts: Most tightly-coupled interface designs use special-purpose message instructions (e.g. a send command) in which general-purpose processor registers are the operands. Some examples include the Message Driven Processor (MDP) [22], the Caltech Mosaic [24], the Henry-Joerg network interface <ref> [29] </ref> and the Start (*T) [30] network interface. An exception is iWarp from CMU [23] whose systolic communication model is based on operands rather than operators.
Reference: [30] <author> Greg M. Papadopoulos, G. A. Boughton, R. Greiner and M. J. Beckerle. </author> <title> *T: integrated building blocks for parallel computing. </title> <booktitle> Proc. of Supercomputing '93, </booktitle> <address> Portland OR, </address> <month> November </month> <year> 1993, </year> <pages> pp. 624-635. </pages>
Reference-contexts: Most tightly-coupled interface designs use special-purpose message instructions (e.g. a send command) in which general-purpose processor registers are the operands. Some examples include the Message Driven Processor (MDP) [22], the Caltech Mosaic [24], the Henry-Joerg network interface [29] and the Start (*T) <ref> [30] </ref> network interface. An exception is iWarp from CMU [23] whose systolic communication model is based on operands rather than operators. <p> In particular, the CM-5 and the J-machine were compared. A related study by Papadopoulos, Boughton, Greiner and Beckerle from MIT and Motorola included a comparison of the *T (Star-T) interface <ref> [30] </ref>. In both studies the abstracted versions of each system were evaluated and compared. The abstracted version of the CM-5 was called CM-5' and likewise the J-machine was abstracted into a system called J'.
Reference: [31] <author> Nanette J. Boden et al. Myrinet: </author> <title> a Gigabit-per-Second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year> <note> Also available on the World Wide Web through http://www.myri.com/research/index.html </note> . 
Reference-contexts: MDPs can be connected in a three-dimensional grid to create an instance of the J-Machine, a massively parallel architecture that scales up to thousands of nodes. The Mosaic is similar except that it is based on a 2-D rather than 3-D mesh. The Myrinet network <ref> [31] </ref> is constructed from arrays of Mosaic chips. The Henry-Joerg and *T network interfaces were integrated into the Motorola 88110, a commercial superscalar RISC processor. <p> This technique is called outboard buffering. Examples of network interfaces that attach to the I/O bus are commercial Ethernet controllers, an ATM PCI interface called DART [39, 40], a PCI-to-PCI bridge called Memory Channel [41], the Myrinet network interface <ref> [31] </ref> and the Princeton SHRIMP network interface [42, 43]. 2.1.2 Data movement The fundamental purpose of the physical interface is data movement. The send interface injects packets into the network and the receive interface ejects packets from the network. <p> Cranium also supports sub-page transfers efficiently. The user queue can receive the first cache line of a page transfer and then subpages can be subsequently transferred using one or more auto-channels. The goals of the Chaos-LAN project are similar to that of the Myrinet NOW <ref> [31] </ref>. At its conclusion, we plan to demonstrate that Chaos-LAN delivers superior performance for comparable cost with Myrinet, or equivalent performance at lower cost. 194 8.8.2 Additional performance studies In addition to the Chaos-LAN hardware implementation project, there are additional performance studies that can be performed.
Reference: [32] <author> Greg M. Papadopoulos. </author> <title> Personal communication. </title> <booktitle> Supercomputing '93, </booktitle> <address> Port-land OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In the case of the *T project, the area required for the on-chip interface is 15 per cent of the total chip area <ref> [32] </ref>. While the extra cost of the integrated interface may not seem to be overwhelming, it is significant enough to deter its acceptance into the commodity marketplace. Recall that the commodity uniprocessor market drives the technology for multiprocessor and scalable parallel systems.
Reference: [33] <author> Gordon Bell. </author> <title> Ultracomputers: a Teraflop Before Its Time. </title> <journal> Communications of the ACM 35(8), </journal> <month> August </month> <year> 1992, </year> <pages> pp. 26-47. </pages>
Reference-contexts: However, cache bus connected designs are difficult to implement and offer very limited support for message passing primitives. As a result, very few network interfaces connect through the cache bus. One notable example is the architecture of the Kendall Square Research KSR-1 <ref> [33] </ref>. The design is based on a principle called COMA, meaning Cache-Only Memory Access (also known as ALLCACHE tm ). In essence, all memory is cache and there is no main memory per se. <p> There are many examples of multicomputers that use this communication model, including Cray Research T3D and T3E [19, 20, 21], Fujitsu AP1000 [48, 49], Stanford DASH [45], 37 Tera MTA-1 [46] and Kendall Square Research KSR-1 and KSR-2 <ref> [33] </ref>. Under the remote memory communication model (also known as remote load/store, put/get, shared memory or non-uniform memory access) processors access remote memory locations directly using load and store operations. Remote memory is actually two communication models: a remote load model and a remote store model.
Reference: [34] <author> John Palmer and Guy L. Steele Jr. </author> <title> Connection Machine Model CM-5 System Overview. </title> <booktitle> IEEE 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <year> 1992, </year> <pages> pp. 474-483. </pages>
Reference-contexts: Connecting through the memory bus provides greater flexibility than connecting to the cache bus, and greater performance than connecting through an I/O bus. Network interfaces in this category include the Thinking Machines CM-5 <ref> [14, 34] </ref>, the Cray Research T3D and T3E [19, 20, 21], the Intel Paragon [17] and University of Washington Meerkat-1 [35].
Reference: [35] <author> Robert Bedichek. </author> <title> The Meerkat multicomputer: tradeoffs in multicomputer architecture. </title> <type> PhD dissertation, </type> <institution> University of Washington, Dept. of CSE, </institution> <address> Seattle WA, </address> <month> June </month> <year> 1994, </year> <note> UW-CSE-TR 93-09-05. </note>
Reference-contexts: Network interfaces in this category include the Thinking Machines CM-5 [14, 34], the Cray Research T3D and T3E [19, 20, 21], the Intel Paragon [17] and University of Washington Meerkat-1 <ref> [35] </ref>. I/O bus connected network interfaces In parallel systems that use a local area network as its communication backbone, the preferred location of the network interface is at the I/O bus. I/O bus cards are relatively simple and inexpensive to create. <p> Table 2.1 describes the attributes of eight different network interfaces: University of Washington Meerkat-1 <ref> [35] </ref>, Intel Paragon, Princeton SHRIMP [42], CMU iWarp [23], Stanford DASH [45], Thinking Machines CM-5 [14], MIT MDP [22] and HP Labs Hamlyn [57, 58]. Some entries appear more than once because there are multiple attributes to many of these network interfaces. <p> A complete breakdown of the interrupt mask and status registers is described in Section A.2.3 in Appendix A. 4.5 Comparison with other message passing interfaces 4.5.1 Intel NX The Intel NX message passing interface is a canonical deeply-layered message passing interface for scalable parallel computing <ref> [7, 17, 35] </ref>. It is the message passing system used in every scalable parallel machine produced by Intel, from the earlier iPSC/2 and iPSC/860 machines to the later machines such as the Touchstone Delta and the 75 Paragon. <p> NX has proven to be quite separable; it has been implemented on a wide variety of non-Intel systems such as Stanford DASH [45], Princeton SHRIMP [42, 43] and UW Meerkat <ref> [35] </ref>. NX is similar to PVM (Parallel Virtual Machine) and MPI (Message Passing Interface) [8, 9]. The basic subset of the NX interface is described by function calls csend () and crecv () that provide blocking communication. <p> Talisman maintains models of the memory system, the instruction cache and data cache, the translation lookaside buffer, the execution pipeline and the write buffer (a three-element FIFO). The timing model in Talisman was calibrated against the Meerkat-1 hardware prototype <ref> [35] </ref>. Through the use of a hand-tuned set of approximately 30 timing parameters, the timing of Talisman concurs with the timing measured in Meerkat-1 within a few percent. The results were verified over a wide range of benchmark programs. <p> Delta This study was performed by Bedichek for his PhD dissertation <ref> [35] </ref>. Bedichek designed and implemented a four-node hardware prototype of Meerkat and also devel 142 oped the software simulator described in the previous chapter of this dissertation. A set of parallel benchmarks were written in C and made use of the Intel NX message-passing library. <p> Second, the processor at the receiving node under Meerkat must flush the cache explicitly for each message. The cache flush operation itself represents about 30% of the cost of communication when using messages with sizes between 128 bytes and 1K bytes <ref> [35] </ref>. Under Cranium, cache and memory are kept locally coherent. Third, the processors in Meerkat stall during communication. This feature precludes the possibility of overlapping communication and computation.
Reference: [36] <author> Peter Steenkiste. </author> <title> A systematic approach to host interface design for high-speed networks. </title> <journal> IEEE Network, </journal> <month> July </month> <year> 1993, </year> <pages> pp. 8-17. </pages>
Reference-contexts: The I/O bus bridge usually becomes a performance bottleneck in message passing as it increases latency and decreases the throughput. In some I/O bus based network interface designs, dynamic RAM modules are added to the interface card <ref> [36, 37, 38] </ref>. This technique is called outboard buffering.
Reference: [37] <author> Peter Druschel, Mark Abbott, Michael Pagels and Larry Peterson. </author> <title> Network subsystem design. </title> <journal> IEEE Network, </journal> <month> July </month> <year> 1993, </year> <pages> pp. 8-17. 200 </pages>
Reference-contexts: The I/O bus bridge usually becomes a performance bottleneck in message passing as it increases latency and decreases the throughput. In some I/O bus based network interface designs, dynamic RAM modules are added to the interface card <ref> [36, 37, 38] </ref>. This technique is called outboard buffering.
Reference: [38] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards and John Lumley. Afterburner: </author> <title> a network-independent card provides architectural support for high-performance protocols. </title> <journal> IEEE Network, </journal> <month> July </month> <year> 1993, </year> <pages> pp. 36-43. </pages>
Reference-contexts: The I/O bus bridge usually becomes a performance bottleneck in message passing as it increases latency and decreases the throughput. In some I/O bus based network interface designs, dynamic RAM modules are added to the interface card <ref> [36, 37, 38] </ref>. This technique is called outboard buffering.
Reference: [39] <author> Randy Osborne. </author> <title> A hybrid deposit model for low overhead communication in high speed LANs. </title> <type> Technical report TR 94-02, </type> <institution> MERL | A Mitsubishi Electric Research Laboratory, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: In some I/O bus based network interface designs, dynamic RAM modules are added to the interface card [36, 37, 38]. This technique is called outboard buffering. Examples of network interfaces that attach to the I/O bus are commercial Ethernet controllers, an ATM PCI interface called DART <ref> [39, 40] </ref>, a PCI-to-PCI bridge called Memory Channel [41], the Myrinet network interface [31] and the Princeton SHRIMP network interface [42, 43]. 2.1.2 Data movement The fundamental purpose of the physical interface is data movement.
Reference: [40] <author> Randy Osborne, Qin Zheng, John Howard, Ross Casley, Doug Hahn and Takeo Nakabayashi. </author> <title> DART: a low overhead ATM network interface chip. </title> <booktitle> Proc. of Hot Interconnects '96, </booktitle> <institution> Stanford University, </institution> <month> August </month> <year> 1996, </year> <pages> pp. 175-186. </pages>
Reference-contexts: In some I/O bus based network interface designs, dynamic RAM modules are added to the interface card [36, 37, 38]. This technique is called outboard buffering. Examples of network interfaces that attach to the I/O bus are commercial Ethernet controllers, an ATM PCI interface called DART <ref> [39, 40] </ref>, a PCI-to-PCI bridge called Memory Channel [41], the Myrinet network interface [31] and the Princeton SHRIMP network interface [42, 43]. 2.1.2 Data movement The fundamental purpose of the physical interface is data movement. <p> A typical example of traffic shaping in an ATM network interface is leaky bucket traffic shaping. A packet is sent every R'th cycle, where R is greater than L, the number of flits in a packet <ref> [40] </ref>. The advantage of traffic shaping is that it prevents the network from saturation and provides throughput guarantees. In the multicomputer domain, traffic shaping is performed in the application program, if at all.
Reference: [41] <author> Richard Gillett and Richard Kaufmann. </author> <title> Experience using the first-generation Memory Channel for PCI network. </title> <booktitle> Proc. of Hot Interconnects '96, </booktitle> <institution> Stanford University, </institution> <month> August </month> <year> 1996, </year> <pages> pp. 205-214. </pages>
Reference-contexts: This technique is called outboard buffering. Examples of network interfaces that attach to the I/O bus are commercial Ethernet controllers, an ATM PCI interface called DART [39, 40], a PCI-to-PCI bridge called Memory Channel <ref> [41] </ref>, the Myrinet network interface [31] and the Princeton SHRIMP network interface [42, 43]. 2.1.2 Data movement The fundamental purpose of the physical interface is data movement. The send interface injects packets into the network and the receive interface ejects packets from the network.
Reference: [42] <author> Mattias A. Blumrich, Kai Li, R. Alpert, Cezary Dubnicki, Edward W. Fel-ten and J. Sandberg. </author> <title> A virtual memory-mapped network interface for the SHRIMP multicomputer. </title> <booktitle> Proc. of the 21st International Symposium on Computer Architecture, </booktitle> <address> Chicago IL, </address> <month> April </month> <year> 1994, </year> <pages> pp. 142-153. </pages>
Reference-contexts: This technique is called outboard buffering. Examples of network interfaces that attach to the I/O bus are commercial Ethernet controllers, an ATM PCI interface called DART [39, 40], a PCI-to-PCI bridge called Memory Channel [41], the Myrinet network interface [31] and the Princeton SHRIMP network interface <ref> [42, 43] </ref>. 2.1.2 Data movement The fundamental purpose of the physical interface is data movement. The send interface injects packets into the network and the receive interface ejects packets from the network. The two competing styles for data movement are programmed I/O (PIO) and direct memory access (DMA). <p> There are many machines that use interrupts, including the CM-5, the MDP and Meerkat-1. * No notification upon message arrival. There is no explicit notification provided by the network interface when a packet arrives. A network interface in this category is the Princeton SHRIMP <ref> [42] </ref>. 2.1.4 Argument checking and protection It is mandatory that commands and arguments to the message passing system are checked by a secure, trusted entity. Checking is performed by either the operating system software or hardware that acts on behalf of the operating system. <p> Examples of globally coherent remote memory systems are Stanford DASH [45], MIT Alewife [26] and Convex Exemplar, based on the Scalable Coherent Interconnect (SCI) memory interface [50]. All of these systems are based on networks that provide in-order packet delivery. * Princeton SHRIMP <ref> [42, 43] </ref>. SHRIMP is a hybrid of the systolic and remote store models. When two nodes wish to communicate, there must first be a logical link constructed between the two nodes via an operating system call. <p> Table 2.1 describes the attributes of eight different network interfaces: University of Washington Meerkat-1 [35], Intel Paragon, Princeton SHRIMP <ref> [42] </ref>, CMU iWarp [23], Stanford DASH [45], Thinking Machines CM-5 [14], MIT MDP [22] and HP Labs Hamlyn [57, 58]. Some entries appear more than once because there are multiple attributes to many of these network interfaces. For instance, iWarp contains both systolic communication and DMA. <p> A single buffer can be used for multicast, that is, to send several messages at once to a set of nodes. Note also that the buffer map and the node map are completely orthogonal. This technique contrasts with SHRIMP <ref> [42] </ref>, in which an entry in the buffer map and the node map are tied together; a given buffer can only be transferred to its associated node. This coupling between buffer and node is less flexible than Cranium's orthogonal approach. <p> NX has proven to be quite separable; it has been implemented on a wide variety of non-Intel systems such as Stanford DASH [45], Princeton SHRIMP <ref> [42, 43] </ref> and UW Meerkat [35]. NX is similar to PVM (Parallel Virtual Machine) and MPI (Message Passing Interface) [8, 9]. The basic subset of the NX interface is described by function calls csend () and crecv () that provide blocking communication. <p> The effects of these modifications can be evaluated by observing the change in maximum throughput achievable by the interface and the change in the cost of communication for application programs. The competing network interfaces of interest are the interfaces in the CM-5, SHRIMP-I and SHRIMP-II <ref> [14, 42, 43] </ref>. The following list of modifications M1 through M4 transform Cranium into interfaces that bear a close resemblance to one of the three competing network interface styles.
Reference: [43] <author> Mattias A. Blumrich, Cezary Dubnicki, Edward W. Felten and Kai Li. </author> <title> Protected, user-level DMA for the SHRIMP network interface. </title> <booktitle> Proc. of High-Performance Computer Architecture 2, </booktitle> <address> San Jose CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: This technique is called outboard buffering. Examples of network interfaces that attach to the I/O bus are commercial Ethernet controllers, an ATM PCI interface called DART [39, 40], a PCI-to-PCI bridge called Memory Channel [41], the Myrinet network interface [31] and the Princeton SHRIMP network interface <ref> [42, 43] </ref>. 2.1.2 Data movement The fundamental purpose of the physical interface is data movement. The send interface injects packets into the network and the receive interface ejects packets from the network. The two competing styles for data movement are programmed I/O (PIO) and direct memory access (DMA). <p> Examples of globally coherent remote memory systems are Stanford DASH [45], MIT Alewife [26] and Convex Exemplar, based on the Scalable Coherent Interconnect (SCI) memory interface [50]. All of these systems are based on networks that provide in-order packet delivery. * Princeton SHRIMP <ref> [42, 43] </ref>. SHRIMP is a hybrid of the systolic and remote store models. When two nodes wish to communicate, there must first be a logical link constructed between the two nodes via an operating system call. <p> NX has proven to be quite separable; it has been implemented on a wide variety of non-Intel systems such as Stanford DASH [45], Princeton SHRIMP <ref> [42, 43] </ref> and UW Meerkat [35]. NX is similar to PVM (Parallel Virtual Machine) and MPI (Message Passing Interface) [8, 9]. The basic subset of the NX interface is described by function calls csend () and crecv () that provide blocking communication. <p> The effects of these modifications can be evaluated by observing the change in maximum throughput achievable by the interface and the change in the cost of communication for application programs. The competing network interfaces of interest are the interfaces in the CM-5, SHRIMP-I and SHRIMP-II <ref> [14, 42, 43] </ref>. The following list of modifications M1 through M4 transform Cranium into interfaces that bear a close resemblance to one of the three competing network interface styles. <p> The network interfaces in SHRIMP-I and SHRIMP-II require a content addressable memory to support the virtual connections between source and destination node. In particular, SHRIMP-II uses a CAM called the Network Interface Page Table. The NIPT is indexed with 15 bits to map 32K distinct remote pages <ref> [43] </ref>. The number of simultaneous mappings is not specified. The scalability of the size, cost and latency of the NIPT may be a concern in the implementation of the SHRIMP-II network interface. * Ease of use.
Reference: [44] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan-Kaufmann, </publisher> <year> 1990, </year> <booktitle> chapter 9.7, </booktitle> <pages> pp. 535-537. </pages>
Reference-contexts: Like PIO, access to memory is avoided entirely because the sources and sinks for message data appear directly in the processor's register set. The DMA cache coherence problem is also avoided (see below). Cache coherence A pitfall in using DMA is the cache coherence problem <ref> [44] </ref>. To send a message, the processor writes values to memory and then initiates the DMA.
Reference: [45] <author> Daniel Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <booktitle> IEEE Computer 25(3), </booktitle> <month> March </month> <year> 1992, </year> <pages> pp. 63-79. </pages>
Reference-contexts: In the send interface, the processor stalls while the network is busy. In the receive interface, the processor stalls until the packet arrives. Dispatch is implicit as the processor resumes processing. Multicomputers that use stall notification include the Cray T3E [21], DASH <ref> [45] </ref> and the Tera MTA-1 [46]. The MTA-1 is a multithreaded processor; while the thread that is waiting for the network is stalled, the processor automatically switches to another thread that is not waiting for the network. Stalling often requires special support in the memory system. <p> There are many examples of multicomputers that use this communication model, including Cray Research T3D and T3E [19, 20, 21], Fujitsu AP1000 [48, 49], Stanford DASH <ref> [45] </ref>, 37 Tera MTA-1 [46] and Kendall Square Research KSR-1 and KSR-2 [33]. Under the remote memory communication model (also known as remote load/store, put/get, shared memory or non-uniform memory access) processors access remote memory locations directly using load and store operations. <p> Maintaining global coherence generates network traffic which would often not be necessary under non-globally coherent systems. The extra directory memory and the coherence logic make global coherency much more complicated to implement compared with the basic model. Examples of globally coherent remote memory systems are Stanford DASH <ref> [45] </ref>, MIT Alewife [26] and Convex Exemplar, based on the Scalable Coherent Interconnect (SCI) memory interface [50]. All of these systems are based on networks that provide in-order packet delivery. * Princeton SHRIMP [42, 43]. SHRIMP is a hybrid of the systolic and remote store models. <p> Table 2.1 describes the attributes of eight different network interfaces: University of Washington Meerkat-1 [35], Intel Paragon, Princeton SHRIMP [42], CMU iWarp [23], Stanford DASH <ref> [45] </ref>, Thinking Machines CM-5 [14], MIT MDP [22] and HP Labs Hamlyn [57, 58]. Some entries appear more than once because there are multiple attributes to many of these network interfaces. For instance, iWarp contains both systolic communication and DMA. <p> NX has proven to be quite separable; it has been implemented on a wide variety of non-Intel systems such as Stanford DASH <ref> [45] </ref>, Princeton SHRIMP [42, 43] and UW Meerkat [35]. NX is similar to PVM (Parallel Virtual Machine) and MPI (Message Passing Interface) [8, 9]. The basic subset of the NX interface is described by function calls csend () and crecv () that provide blocking communication. <p> We use cycle counts rather than absolute time (e.g. microseconds). As the underlying implementation technology continues to improve over time, the relative performance of processors and routers is expected to remain constant in terms of clock cycles <ref> [45] </ref>. Even though DRAM access latency is not improving at 102 the same rate as processors and networks are, the throughput of memory subsystems are keeping pace by means of pipelining and widening the memory bus. * Network latency. The network is a square two-dimensional torus mesh.
Reference: [46] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> Proc. of 1990 International Conference on Supercomputing, </booktitle> <address> Amsterdam, Netherlands, </address> <month> June </month> <year> 1990, </year> <pages> pp. 1-6. 201 </pages>
Reference-contexts: In the send interface, the processor stalls while the network is busy. In the receive interface, the processor stalls until the packet arrives. Dispatch is implicit as the processor resumes processing. Multicomputers that use stall notification include the Cray T3E [21], DASH [45] and the Tera MTA-1 <ref> [46] </ref>. The MTA-1 is a multithreaded processor; while the thread that is waiting for the network is stalled, the processor automatically switches to another thread that is not waiting for the network. Stalling often requires special support in the memory system. <p> There are many examples of multicomputers that use this communication model, including Cray Research T3D and T3E [19, 20, 21], Fujitsu AP1000 [48, 49], Stanford DASH [45], 37 Tera MTA-1 <ref> [46] </ref> and Kendall Square Research KSR-1 and KSR-2 [33]. Under the remote memory communication model (also known as remote load/store, put/get, shared memory or non-uniform memory access) processors access remote memory locations directly using load and store operations. <p> By contrast, iWarp's systolic communication is limited to two physical connections into and two out from each processing node. SHRIMP is also based on a network that delivers packets in-order. * Tera MTA-1 <ref> [46] </ref>. The MTA-1 is unusual for several reasons. Each network node contains either a memory module or a processor, but not both. Both remote load and remote store are necessary because processors do contain neither caches nor local memory. <p> A tightly-coupled interface between the network and the processor 48 was ruled out. Designs that require synchronizing memory or multi-threaded processors in an architecture like the Tera MTA-1 <ref> [46] </ref> were also ruled out. Cranium is coupled to the memory bus and takes advantage of hardware cache coherence that is built into most modern high-performance processors (see Sec tion 3.4.2). * Cranium provides support for general-purpose scientific computing. The system must run a wide variety of different compute-bound tasks. <p> The cost of FIFO entry/exit, header decode and the receiving node's test and branch are omitted. The cost of this test and branch can be eliminated when the processor architecture uses a synchronization mechanism like the full-empty bits in the Tera MTA-1 <ref> [46] </ref>. The cycle counts contained in the four following rows are based on the simulator described in Chapter 5 and the timing analysis of the Teschio implementation of Cranium presented in Section7.4.
Reference: [47] <author> Brian N. Bershad, David D. Redell and John R. Ellis. </author> <title> Fast mutual exclusion for uniprocessors. </title> <booktitle> Proc. of the 5th Annual International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992, </year> <pages> pp. 223-233. </pages>
Reference-contexts: Under cancel-retry the network interface destroys the partially constructed packet and notifies the user program running on the sending node that the injection failed, and the user program must retry the operation when it resumes execution. Another technique is to use a restartable atomic sequence (RAS) <ref> [47] </ref>. The segment of the user code that sends or receives a packet is marked as a critical section.
Reference: [48] <author> H. Ishihata, T. Shimizu, M. Ikesaka, S. Inano and M. Ikesaka. </author> <title> Architecture of [the] highly parallel AP1000 computer. </title> <journal> Systems and Computers in Japan, </journal> <volume> 24(7), </volume> <year> 1993, </year> <pages> pp. 69-77. </pages>
Reference-contexts: There are many examples of multicomputers that use this communication model, including Cray Research T3D and T3E [19, 20, 21], Fujitsu AP1000 <ref> [48, 49] </ref>, Stanford DASH [45], 37 Tera MTA-1 [46] and Kendall Square Research KSR-1 and KSR-2 [33]. Under the remote memory communication model (also known as remote load/store, put/get, shared memory or non-uniform memory access) processors access remote memory locations directly using load and store operations.
Reference: [49] <author> K. Hayashi, T. Doi, T. Horie, Y. Koyanagi, O. Shiraki, N. Imamura, H. Ishi-hata and T. Shindo. </author> <title> AP1000+: architectural support of PUT/GET interface for parallelizing compiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(11), </volume> <month> Nov. </month> <year> 1994, </year> <pages> pp. 196-207. </pages>
Reference-contexts: There are many examples of multicomputers that use this communication model, including Cray Research T3D and T3E [19, 20, 21], Fujitsu AP1000 <ref> [48, 49] </ref>, Stanford DASH [45], 37 Tera MTA-1 [46] and Kendall Square Research KSR-1 and KSR-2 [33]. Under the remote memory communication model (also known as remote load/store, put/get, shared memory or non-uniform memory access) processors access remote memory locations directly using load and store operations. <p> Barriers are therefore fundamentally global in scope. Also, many scalable computer systems support barrier synchronization directly in hardware <ref> [14, 49, 66] </ref>. The latency of a global barrier is often an order of magnitude less than the latency of a regular message. Figure 4.5 is a flow diagram showing the interaction between nodes S and Ra when barrier synchronization is used.
Reference: [50] <author> D. V. James et al. </author> <title> Distributed directory scheme: Scalable Coherent Interface. </title> <booktitle> IEEE Computer 23(6), </booktitle> <month> June </month> <year> 1990, </year> <pages> pp. 74-77. </pages>
Reference-contexts: The extra directory memory and the coherence logic make global coherency much more complicated to implement compared with the basic model. Examples of globally coherent remote memory systems are Stanford DASH [45], MIT Alewife [26] and Convex Exemplar, based on the Scalable Coherent Interconnect (SCI) memory interface <ref> [50] </ref>. All of these systems are based on networks that provide in-order packet delivery. * Princeton SHRIMP [42, 43]. SHRIMP is a hybrid of the systolic and remote store models.
Reference: [51] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems 7(4), </journal> <month> November </month> <year> 1989, </year> <pages> pp. 321-359. </pages>
Reference-contexts: These schemes require the compiler or the linker to automatically convert remote loads and stores in parallel programs into other communication primitives, such as send and receive primitives. Some examples are Ivy <ref> [51] </ref>, Munin [52], Midway [53], Tempest and Blizzard [54], and TreadMarks [55]. These schemes help improve the portability of programs across parallel systems, regardless of the organization of the network interface. For some programs, the performance of these software schemes approach that of native hardware support for remote memory.
Reference: [52] <author> John B. Carter, John K. Bennett and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> Proc. of 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991, </year> <pages> pp. 92-103. </pages>
Reference-contexts: These schemes require the compiler or the linker to automatically convert remote loads and stores in parallel programs into other communication primitives, such as send and receive primitives. Some examples are Ivy [51], Munin <ref> [52] </ref>, Midway [53], Tempest and Blizzard [54], and TreadMarks [55]. These schemes help improve the portability of programs across parallel systems, regardless of the organization of the network interface. For some programs, the performance of these software schemes approach that of native hardware support for remote memory.
Reference: [53] <author> B. N. Bershad, M. J. Zekauskas and W. A. Sawdon. </author> <title> Midway distributed shared memory system. </title> <booktitle> Proc. of COMPCON, </booktitle> <year> 1993, </year> <pages> pp. 528-537. </pages>
Reference-contexts: These schemes require the compiler or the linker to automatically convert remote loads and stores in parallel programs into other communication primitives, such as send and receive primitives. Some examples are Ivy [51], Munin [52], Midway <ref> [53] </ref>, Tempest and Blizzard [54], and TreadMarks [55]. These schemes help improve the portability of programs across parallel systems, regardless of the organization of the network interface. For some programs, the performance of these software schemes approach that of native hardware support for remote memory.
Reference: [54] <author> Mark D. Hill, James R. Larus and David A. Wood. </author> <title> Tempest: a substrate for portable parallel programs. </title> <booktitle> Proc. of COMPCON, </booktitle> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: These schemes require the compiler or the linker to automatically convert remote loads and stores in parallel programs into other communication primitives, such as send and receive primitives. Some examples are Ivy [51], Munin [52], Midway [53], Tempest and Blizzard <ref> [54] </ref>, and TreadMarks [55]. These schemes help improve the portability of programs across parallel systems, regardless of the organization of the network interface. For some programs, the performance of these software schemes approach that of native hardware support for remote memory.
Reference: [55] <author> Christiana Amza et al. TreadMarks: </author> <title> shared memory computing on networks of workstations. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1996, </year> <pages> pp. 18-28. </pages>
Reference-contexts: These schemes require the compiler or the linker to automatically convert remote loads and stores in parallel programs into other communication primitives, such as send and receive primitives. Some examples are Ivy [51], Munin [52], Midway [53], Tempest and Blizzard [54], and TreadMarks <ref> [55] </ref>. These schemes help improve the portability of programs across parallel systems, regardless of the organization of the network interface. For some programs, the performance of these software schemes approach that of native hardware support for remote memory.
Reference: [56] <author> John Wilkes. Hamlyn: </author> <title> an interface for sender-based communications. </title> <type> Technical report HPL-OSR-92-13, </type> <institution> Hewlett-Packard Company, HP Labs, Operating System Research Dept., </institution> <month> November </month> <year> 1992. </year> <month> 202 </month>
Reference-contexts: The unbuffered protocol can be supported only under a particular type of automatic-receive DMA; all other interfaces use a buffered protocol. The Hamlyn network 41 Automatic Buffered Proc. initiated Unbuffered Receive interfaces interface from HP Labs <ref> [56, 57, 58] </ref> supports automatic, unbuffered DMA. The authors' terminology for this property is sender-managed communication. Hamlyn is also one of the few network interface architectures that supports a network that delivers packets out-of-order. The principal advantage of an unbuffered protocol is the ability to filter notification information.
Reference: [57] <author> Greg Buzzard, David Jacobson, Scott Marovich and John Wilkes. Hamlyn: </author> <title> an high-performance network interface for sender-based memory management. </title> <booktitle> Proc. of Hot Interconnects III Symposium, </booktitle> <address> Stanford University, Palo Alto, CA, </address> <month> August </month> <year> 1995. </year> <note> Also available as technical report HPL-95-86, </note> <institution> Hewlett-Packard Company, HP Labs, Computer Systems Laboratory, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: The unbuffered protocol can be supported only under a particular type of automatic-receive DMA; all other interfaces use a buffered protocol. The Hamlyn network 41 Automatic Buffered Proc. initiated Unbuffered Receive interfaces interface from HP Labs <ref> [56, 57, 58] </ref> supports automatic, unbuffered DMA. The authors' terminology for this property is sender-managed communication. Hamlyn is also one of the few network interface architectures that supports a network that delivers packets out-of-order. The principal advantage of an unbuffered protocol is the ability to filter notification information. <p> Table 2.1 describes the attributes of eight different network interfaces: University of Washington Meerkat-1 [35], Intel Paragon, Princeton SHRIMP [42], CMU iWarp [23], Stanford DASH [45], Thinking Machines CM-5 [14], MIT MDP [22] and HP Labs Hamlyn <ref> [57, 58] </ref>. Some entries appear more than once because there are multiple attributes to many of these network interfaces. For instance, iWarp contains both systolic communication and DMA. These systems were chosen as representative examples of the wide span of possibilities that are available to the network interface designer.
Reference: [58] <author> Greg Buzzard, David Jacobson, Milton Mackey, Scott Marovich and John Wilkes. </author> <title> An implementation of the Hamlyn sender-managed interface architecture. </title> <booktitle> Proc. of the 2nd Symposium on Operating Systems Design and Implementation, </booktitle> <address> Seattle WA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: The unbuffered protocol can be supported only under a particular type of automatic-receive DMA; all other interfaces use a buffered protocol. The Hamlyn network 41 Automatic Buffered Proc. initiated Unbuffered Receive interfaces interface from HP Labs <ref> [56, 57, 58] </ref> supports automatic, unbuffered DMA. The authors' terminology for this property is sender-managed communication. Hamlyn is also one of the few network interface architectures that supports a network that delivers packets out-of-order. The principal advantage of an unbuffered protocol is the ability to filter notification information. <p> Table 2.1 describes the attributes of eight different network interfaces: University of Washington Meerkat-1 [35], Intel Paragon, Princeton SHRIMP [42], CMU iWarp [23], Stanford DASH [45], Thinking Machines CM-5 [14], MIT MDP [22] and HP Labs Hamlyn <ref> [57, 58] </ref>. Some entries appear more than once because there are multiple attributes to many of these network interfaces. For instance, iWarp contains both systolic communication and DMA. These systems were chosen as representative examples of the wide span of possibilities that are available to the network interface designer.
Reference: [59] <author> Ellen Spertus, S. C. Goldstein, K. E. Schauser, T. von Eicken, D. E. Culler and W. J. Dally. </author> <title> Evaluation of mechanisms for fine-grained parallel programs in the J-machine and the CM-5. </title> <booktitle> Proc. of the 20th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 302-313. </pages>
Reference-contexts: Both polling and interrupts offer greater fairness than stalling at the cost of increasing the overhead. On standard RISC processors, an interrupt is more expensive than a poll. On the CM-5, which uses Sparc processors, interrupts take ten times as many processor cycles as polling <ref> [59] </ref>. Special-purpose processors have hardware support for fast interrupt handling. For example, the MDP in the J-machine can dispatch an interrupt handler to react to an incoming packet in only three clock cycles [22]. <p> The weakness of this study was that there is little insight into how the structure of the network interface forces the software overhead to be much larger than necessary. 141 6.4.2 Study #2: CM-5 vs. J-machine vs. Star-T This study by Spertus et al <ref> [59] </ref> provided much more insight into how the structure of the network interface affects the overall performance of scalable systems. In particular, the CM-5 and the J-machine were compared.
Reference: [60] <author> Neil McKenzie, Kevin Bolding, Carl Ebeling and Lawrence Snyder. Cranium: </author> <title> an interface for message passing on adaptive packet routing networks. </title> <booktitle> Proc. of Parallel Computer Routing and Communication Workshop, </booktitle> <address> Seattle WA, May 1994, </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 266-280. </pages>
Reference-contexts: Chapter 3 THE Cranium NETWORK INTERFACE ARCHITECTURE Architecture is frozen music. Goethe This chapter describes the design of the Cranium network interface architecture. The initial motivation for Cranium came from a requirement to design a high-performance companion interface to the Chaos network router <ref> [15, 60] </ref>. The Chaos router has two interesting attributes. It routes small fixed-size packets, using a payload the size of a processor cache-line (e.g. 32 bytes).
Reference: [61] <author> William Yost. </author> <title> Cost effective fault tolerance for network routing. </title> <type> Technical Report UW-CSE-95-03-03, </type> <institution> University of Washington, Dept. of CSE, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: The Chaos network does not provide a separate data network for operating system packets. In order to guarantee the delivery of system packets, the system relies on the existence of a separate control network dedicated to global operations such as barriers and eurekas. The Express Broadcast Network (EBN) <ref> [61] </ref> describes the design and implementation of one such control network that can be used in conjunction with the Chaos data network. <p> A well known example in a commercial system is the CM-5 control network [14]. For the torus network, an elegant solution is provided by the Express Broadcast Network (EBN) <ref> [61] </ref>. EBN is a low-cost extension to mesh and torus data networks for supporting low-latency broadcast of control messages. EBN increases the width of each link in the existing network by one extra wire.
Reference: [62] <author> Ludmila Cherkasova and Tomas Rokicki. </author> <title> Alpha message scheduling for optimizing communication latency in distributed systems. </title> <booktitle> Proc. of 13th IFAC Workshop on Distributed Computer Control Systems, </booktitle> <year> 1995. </year>
Reference-contexts: Another packet scheduling algorithm that has been discussed in the literature is alpha scheduling <ref> [62, 63] </ref>. In alpha scheduling, the priority of a message is a function of its arrival time, the length of the message and a tuning factor called alpha (ff).
Reference: [63] <author> Ludmila Cherkasova, Vadim Kotov and Tomas Rokicki. </author> <title> The impact of message scheduling for packet switching interconnect fabrics. </title> <booktitle> Proc. of 29th Hawaii International Conference on System Sciences, </booktitle> <year> 1996. </year>
Reference-contexts: Another packet scheduling algorithm that has been discussed in the literature is alpha scheduling <ref> [62, 63] </ref>. In alpha scheduling, the priority of a message is a function of its arrival time, the length of the message and a tuning factor called alpha (ff).
Reference: [64] <author> Charles P. Thacker, David G. Conroy and Lawrence C. Stewart. </author> <title> The Alpha Demonstration Unit: a high-performance multiprocessor. </title> <journal> Communications of the ACM 36(2), </journal> <month> February </month> <year> 1993, </year> <pages> pp. 55-67. 203 </pages>
Reference-contexts: A particular strategy that works well for both MPP and SMP is multi-level, a strategy supported by the bus used in the Alpha Demonstration Unit (ADU) <ref> [64] </ref> (see Section 7.1.2). The message data are written to memory and selectively invalidated or updated in the processor cache. This model requires the processor to have a multi-level cache with the inclusion property: everything valid in level k is also valid in level k + 1. <p> The bus is pipelined and allows the network interface to read or write memory at the rate of two packet payloads every ten cycles. These timing assumptions are based on the system bus used in the Alpha Demonstration Unit (ADU) <ref> [64] </ref>. 6.1.1 Latency of a single packet Table 6.1 breaks down the latency of a single packet into four components: the software overhead at the sending node (Send SW), the network interface hardware latency at the sending node (Send HW), the network interface hardware latency at the receiving node (Recv HW) <p> The Chaos network uses a two dimensional torus mesh topology. * It connects directly to the processor-memory bus in the computing nodes of the multicomputer. The processor-memory bus used in this implementation is a split-transaction bus similar to the one designed for the Alpha Demonstration Unit <ref> [64] </ref>. Connecting at the processor-memory bus yields higher performance than connecting through an I/O bus such as PCI (see Section 2.1.1). * It requires implementation technology that is well within the limits of today's CMOS fab lines. We estimate that the circuit can be implemented using fewer than 400,000 gates. <p> Clock Bus state Transfer 1 Transfer 2 Transfer 3 A.1 A.2 D.10 D.11 D.12 D.13 A.3 D.20 D.21 D.22D.00 D.01 D.02 D.03 D.23 A.4 D.30 . . . 7.1.2 The ADU bus The processor-memory bus is a representative design based on the Alpha Demonstration Unit <ref> [64] </ref>, known in this dissertation as the ADU bus. The ADU bus is synchronous and it multiplexes address and data onto the same 64 wires. <p> Cache coherency issues are not discussed further in this chapter. The interested reader may refer to Sections 2.1.2 and 3.4.2 in this dissertation and the article by Thacker, Conroy and Stewart <ref> [64] </ref> for more detail. Our implementation of the ADU bus is clocked at 100 MHz and the cycle time is 10 nsec. This clock rate was chosen to match the access delay of standard DRAM technology, which is roughly 50 nsec. <p> Pin count The number of pins required for Teschio is computed from the sum of number of signals used its two external linkages the ADU bus and the P link plus some extra pins for "glue." The ADU bus contains 102 signals <ref> [64] </ref>. The P link contains 41 signals (see Section D.1). We assume that there are another 12 signals devoted to clocks, built-in self-test and other auxiliary functions. The total number of signals is 102 + 41 + 12 = 155. <p> This unit of transfer turned out to be quite efficient, especially when used in conjunction with a memory bus architecture that works directly with cache line units of data, such as the ADU bus <ref> [64] </ref> (see Section 7.1.2). A message passing system or shared memory system that implements with a smaller unit of transfer directly is likely to increase the complexity of implementation significantly without increasing its performance.
Reference: [65] <author> ChangYun Park. </author> <title> Predicting deterministic execution times of real-time programs. </title> <type> PhD dissertation, </type> <institution> University of Washington, Department of CSE, </institution> <month> Summer </month> <year> 1992, </year> <note> UW-CSE-TR 92-08-02. </note>
Reference: [66] <author> Kevin Bolding and William Yost. </author> <title> The Express Broadcast Network: a network for low-latency broadcast of control messages. </title> <booktitle> Proc. of 1995 Intl. Conf. on Algorithms and Architectures for Parallel Processing, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Barriers are therefore fundamentally global in scope. Also, many scalable computer systems support barrier synchronization directly in hardware <ref> [14, 49, 66] </ref>. The latency of a global barrier is often an order of magnitude less than the latency of a regular message. Figure 4.5 is a flow diagram showing the interaction between nodes S and Ra when barrier synchronization is used. <p> The rest of this section is a brief overview of chaotic routing in general and the design of a prototype CMOS chip that implements the algorithm. Much more detail on chaotic routing can be found in the literature <ref> [12, 15, 13, 66, 73, 74, 75] </ref>. The chaotic routing algorithm uses fixed-length packets and a packet routing technique called virtual cut-through [76]. A packet consists of a sequence of words called phits (physical units); each phit is the width of a router link.
Reference: [67] <author> Robert F. Cmelik and David Keppel. Shade: </author> <title> a fast instruction-set simulator for execution profiling. </title> <type> Technical report UW-CSE 93-06-06, </type> <month> June </month> <year> 1993. </year> <note> Also available as Sun Microsystems Laboratories technical report SMLI 93-12. </note>
Reference-contexts: The most efficient functional simulators impose a slowdown of 10 to 100 host cycles per simulated cycle. In the literature there are a wide variety of processor simulators that provide functional simulation; an excellent survey of techniques can be found in the papers on Shade <ref> [67, 68] </ref>. * Structural simulation is simulating at the register-transfer level of the architecture of the underlying hardware (the network router or processor). Structures such as the cache, the memory system, the routing algorithm of the network, etc. are closely modeled on a cycle by cycle basis.
Reference: [68] <author> Robert F. Cmelik and David Keppel. Shade: </author> <title> a fast instruction-set simulator for execution profiling. </title> <booktitle> Proceedings of ACM SIGMETRICS '94, </booktitle> <address> Nashville TN, </address> <month> May </month> <year> 1994, </year> <pages> pp. 128-137. </pages>
Reference-contexts: The most efficient functional simulators impose a slowdown of 10 to 100 host cycles per simulated cycle. In the literature there are a wide variety of processor simulators that provide functional simulation; an excellent survey of techniques can be found in the papers on Shade <ref> [67, 68] </ref>. * Structural simulation is simulating at the register-transfer level of the architecture of the underlying hardware (the network router or processor). Structures such as the cache, the memory system, the routing algorithm of the network, etc. are closely modeled on a cycle by cycle basis.
Reference: [69] <author> W. Culbertson, R. Amerson, R. Carter, P. Kuekes and G. Snider. </author> <booktitle> The Teramac configurable custom computer. Proc. of the International Society of Optical Engineering (SPIE) Field Programmable Gate Arrays (FPGAs) for Fast Board Development and Reconfigurable Computing, </booktitle> <address> Philadelphia, PA, </address> <month> Oct. </month> <year> 1995, </year> <pages> pp. 201-209. </pages>
Reference-contexts: The host system is constructed from an array of programmable logic devices such as Xilinx FPGAs. Two examples of hardware simulators are the Teramac from Hewlett-Packard Labs <ref> [69] </ref> and the System Realizer M3000 from Quickturn [70]. Hardware simulators provide the same degree of fine-grain detail as structural simulators, except that execution speed is comparable to a very fast functional simulator. The primary disadvantage is cost, on the order of tens of thousands to millions of dollars.
Reference: [70] <author> Quickturn Design Systems, Inc. </author> <title> System Realizer Family page on the World Wide Web, </title> <note> accessed on October 17, 1996. URL: http://www.quickturn.com /prod/realizer/sysreal.htm </note>
Reference-contexts: The host system is constructed from an array of programmable logic devices such as Xilinx FPGAs. Two examples of hardware simulators are the Teramac from Hewlett-Packard Labs [69] and the System Realizer M3000 from Quickturn <ref> [70] </ref>. Hardware simulators provide the same degree of fine-grain detail as structural simulators, except that execution speed is comparable to a very fast functional simulator. The primary disadvantage is cost, on the order of tens of thousands to millions of dollars.
Reference: [71] <author> Nancy Kronenberg, Thomas R. Benson, Wayne M. Cardoza, R. Jagannathan and Benjamin J. Thomas. </author> <title> Porting OpenVMS from VAX to Alpha AXP. </title> <journal> Communications of the ACM 36(2), </journal> <month> February </month> <year> 1993, </year> <pages> pp. 45-53. </pages>
Reference-contexts: Static translation (essentially, recompiling from one binary format to another) avoids the decoding step entirely at run-time. This strategy was used by DEC to migrate VMS programs from the VAX to the Alpha AXP <ref> [71] </ref>. However, static translation prevents some types of programs from executing correctly, including programs that modify their instruction space on-the-fly (i.e. self-modifying or run-time compiled code). For generality, Talisman translates instructions at run-time. To optimize host execution performance, Talisman caches the decoded information.
Reference: [72] <author> Motorola Inc. </author> <title> The MC88100 RISC Microprocessor User's Manual, second edition. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs NJ, </address> <year> 1990. </year>
Reference-contexts: This communication model was replaced by the Cranium application programmer interface. 5.1.1 Talisman's timing model The timing model used in Talisman is based on a black-box cost analysis of each of the major structural units in the Motorola 88100 <ref> [72] </ref>. Talisman maintains models of the memory system, the instruction cache and data cache, the translation lookaside buffer, the execution pipeline and the write buffer (a three-element FIFO). The timing model in Talisman was calibrated against the Meerkat-1 hardware prototype [35]. <p> The extra processors contribute little to improving host execution latency but make it possible to run several independent simulations concurrently. The simulated target architecture is based on a 20-MHz Motorola 88100 processor <ref> [72] </ref> and a pair of 88200 cache/memory units [77]. <p> The network interface registers are mapped into the user program's address space. The user program stores values to these registers to initiate send and receive commands; the program loads these registers to retrieve status information from the network interface. These timing assumptions are based on the Motorola 88100 processor <ref> [72] </ref>. * Memory performance. It takes 10 cycles to load or store 32 bytes (the size of the packet payload, also the size of a cache line). The bus is synchronous; it is 1 There is a distinction between a physical digit (phit) and a flow-control unit (flit).
Reference: [73] <author> Smaragda Konstantinidou and Lawrence Snyder. </author> <title> The Chaos router. </title> <journal> IEEE Transactions on Computers, </journal> <month> Dec. </month> <year> 1994. </year> <month> 204 </month>
Reference-contexts: See Section 6.2.4 for further discussion of the two memory models supported in the Talisman simulator. 5.2 Chaos network simulator Chaotic routing is a non-minimal adaptive algorithm for routing fixed-length packets. It was invented by Magda Konstantinidou and Lawrence Snyder as an algorithm for routing in hypercube networks <ref> [73] </ref>; the algorithm was adapted by Kevin Bolding and Melanie Fulgham to apply it to mesh and torus networks [12, 74]. Konstantinidou, Bolding and Fulgham and many others 1 created a structural simulator called the Chaos simulator to evaluate the performance of the routing algorithm. <p> The rest of this section is a brief overview of chaotic routing in general and the design of a prototype CMOS chip that implements the algorithm. Much more detail on chaotic routing can be found in the literature <ref> [12, 15, 13, 66, 73, 74, 75] </ref>. The chaotic routing algorithm uses fixed-length packets and a packet routing technique called virtual cut-through [76]. A packet consists of a sequence of words called phits (physical units); each phit is the width of a router link.
Reference: [74] <author> Melanie Fulgham and Lawrence Snyder. </author> <title> A study of chaotic routing with nonuniform traffic. </title> <type> Technical Report UW-CSE-93-06-01, </type> <institution> University of Washing-ton, Dept. of CSE, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: It was invented by Magda Konstantinidou and Lawrence Snyder as an algorithm for routing in hypercube networks [73]; the algorithm was adapted by Kevin Bolding and Melanie Fulgham to apply it to mesh and torus networks <ref> [12, 74] </ref>. Konstantinidou, Bolding and Fulgham and many others 1 created a structural simulator called the Chaos simulator to evaluate the performance of the routing algorithm. In its current form, it simulates complete networks of chaotic routers organized as two-dimensional meshes or tori. <p> The rest of this section is a brief overview of chaotic routing in general and the design of a prototype CMOS chip that implements the algorithm. Much more detail on chaotic routing can be found in the literature <ref> [12, 15, 13, 66, 73, 74, 75] </ref>. The chaotic routing algorithm uses fixed-length packets and a packet routing technique called virtual cut-through [76]. A packet consists of a sequence of words called phits (physical units); each phit is the width of a router link. <p> If the data set sizes were larger and subsequently the network needed to handle a large number of large messages simultaneously, then internal congestion in the network can cause a performance bottleneck. The impact of the network on communication performance has been the focus of other research projects <ref> [12, 15, 74] </ref>. demonstrate that the communication performance achieved by these benchmarks is spread over a wide range. The horizontal solid lines in the two graphs represent TP peak for the two cases.
Reference: [75] <author> Kevin Bolding, Sen-Ching Cheung, Sung-Eun Choi, Carl Ebeling, Soha Has-soun, Ton Ngo and Robert Wille. </author> <title> The Chaos router chip: design and implementation of an adaptive router. </title> <booktitle> Proceedings of IFIP Conf. on VLSI, </booktitle> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The rest of this section is a brief overview of chaotic routing in general and the design of a prototype CMOS chip that implements the algorithm. Much more detail on chaotic routing can be found in the literature <ref> [12, 15, 13, 66, 73, 74, 75] </ref>. The chaotic routing algorithm uses fixed-length packets and a packet routing technique called virtual cut-through [76]. A packet consists of a sequence of words called phits (physical units); each phit is the width of a router link. <p> The strength of the results provided the motivation to implement a prototype chaotic router. The router was implemented as a 132-pin CMOS chip, created by a team of graduate students supervised by Bolding and Ebeling <ref> [75] </ref>. five bi-directional channels: the NEWS channels (north, east, west and south) that connect to other routing nodes and the P channel that connects to the processing node. Also shown are the packet buffers, called frames. Each frame is a FIFO for a full packet.
Reference: [76] <author> Parviz Kermani and Leonard Kleinrock. </author> <title> Virtual cut-through: a new computer communication switching technique. </title> <booktitle> Computer Networks 3, </booktitle> <year> 1979, </year> <pages> pp. 267-286. </pages>
Reference-contexts: Much more detail on chaotic routing can be found in the literature [12, 15, 13, 66, 73, 74, 75]. The chaotic routing algorithm uses fixed-length packets and a packet routing technique called virtual cut-through <ref> [76] </ref>. A packet consists of a sequence of words called phits (physical units); each phit is the width of a router link. A packet in flight in the network has two modes, similar to a slinky.
Reference: [77] <author> Motorola Inc. </author> <title> The MC88200 Cache/Memory Management Unit User's Manual, second edition. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs NJ, </address> <year> 1990. </year>
Reference-contexts: Ideally, we would like Talisman to model the multi-level scheme for coherence discussed in Section 3.4.2. However, Talisman models only a single-level cache, because it was based on the Meerkat-1 hardware prototype. The single-level cache in Meerkat-1 is implemented by the Motorola 88200 cache-MMU chip <ref> [77] </ref>. It 90 was judged to be too difficult to modify Talisman to model a two-level cache (both on-chip and external). <p> The extra processors contribute little to improving host execution latency but make it possible to run several independent simulations concurrently. The simulated target architecture is based on a 20-MHz Motorola 88100 processor [72] and a pair of 88200 cache/memory units <ref> [77] </ref>. One 88200 is used as an instruction cache and another as a data cache; they complement the 88100's separate instruction and data busses. (This separation is also known as a Harvard architecture.) The 88200 contains 16K bytes of data organized as 16 byte lines with four-way set associativity.
Reference: [78] <author> M. Barnett, R. Littlefield, D. G. Payne and R. van de Geijn. </author> <title> Global combine algorithms on mesh architectures with wormhole routing. </title> <booktitle> Proc. of 7th IPPS, </booktitle> <address> Newport Beach CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: There are a variety of methods for implementing a broadcast on a point-to-point network <ref> [78] </ref>. The most obvious approach is for the root node to send single messages directly to all the other nodes. In a system with p nodes, the root node sends p 1 messages. This approach works well only if p is small.
Reference: [79] <author> David Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian and T. von Eicken. </author> <title> LogP: towards a realistic model of parallel computation. </title> <booktitle> Proc. of 4th Principles and Practices of Parallel Processing, </booktitle> <year> 1993, </year> <pages> pp. 1-12. </pages>
Reference-contexts: When these assumptions are taken into account, it turns out that the hypercube scheme is not always the optimal pattern for tree broadcast. Culler et al. describe the construction of the optimal pattern for broadcast on a point-to-point network <ref> [79, 80, 81] </ref>. The pattern for optimal tree broadcast is a function of the relative values of the g, the network distance and the software overhead.
Reference: [80] <author> R. M. Karp, A. Sahay, E. Santos and K. E. Schauser. </author> <title> Optimal broadcast and summation in the LogP model. </title> <booktitle> 5th Symp. on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: When these assumptions are taken into account, it turns out that the hypercube scheme is not always the optimal pattern for tree broadcast. Culler et al. describe the construction of the optimal pattern for broadcast on a point-to-point network <ref> [79, 80, 81] </ref>. The pattern for optimal tree broadcast is a function of the relative values of the g, the network distance and the software overhead. <p> The bounds on latency and throughput of tree broadcast determine the bounds on latency and throughput for barriers and global combines based on trees. The optimal pattern for the fan-in phase of the global combine is exactly the reverse of the optimal pattern for broadcast <ref> [80] </ref>. The lower bound on execution time for barriers and global combines is therefore twice the lower bound for broadcast. Global combines are slightly slower than barriers due to the extra time needed to execute the combining operation.
Reference: [81] <author> Albert Alexandrov, Mihai Ionescu, Klaus E. Schauser and Chris Scheiman. LogGP: </author> <title> incorporating long messages into the LogP model: one step closer towards a realistic model for parallel computation. </title> <booktitle> 7th Annual Symposium on Parallel Algorithms and Architectures (SPAA'95), </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: When these assumptions are taken into account, it turns out that the hypercube scheme is not always the optimal pattern for tree broadcast. Culler et al. describe the construction of the optimal pattern for broadcast on a point-to-point network <ref> [79, 80, 81] </ref>. The pattern for optimal tree broadcast is a function of the relative values of the g, the network distance and the software overhead.
Reference: [82] <author> S. S. Mukherjee, S. D. Sharma, Mark D. Hill, James R. Larus, Anne Rogers and Joel Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> Proc. of the 5th ACM SIGPLAN PPoPP, </booktitle> <month> July </month> <year> 1995. </year> <month> 205 </month>
Reference-contexts: Load imbalance often dominates the total running time of the parallel application program. A well-balanced program is more likely to isolate the network interface as the performance bottleneck than an unbalanced program is. Problems that have sparse data structures, such as particle-in-cell simulations, tend to be difficult to load-balance <ref> [82] </ref>. While these types of computations are important and would need to work well with Cranium in principle, they are less suitable as benchmarks for detailed comparison of network interface features. The most popular choice for an efficient implementation language style is the sequential imperative style.
Reference: [83] <author> Manish Gupta, Sam Midkiff, Edith Schonberg, Ven Seshadri, David Shields, Ko-Yang Wang, Wai-Mee Ching and Ton Ngo. </author> <title> An HPF Compiler for the IBM SP2. </title> <booktitle> Proc. of Supercomputing '95, </booktitle> <address> San Diego CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The competing approach is to write programs in a high-level language that is designed for parallel programming. The HLL program is translated into a message-passing program by a source-to-source compiler. While research prototypes of parallel HLL implementations are improving <ref> [83] </ref>, the message-passing programs produced by commercially available parallel HLL compilers do not yet provide the efficiency of hand-crafted message-passing code. 6.2.2 Suite of parallel benchmark programs The following subsections describe the benchmarks that comprise the suite used to evaluate Cranium: Fourier transform, bucket sort, Jacobi successive-over-relaxation, Gaussian elimination and dense
Reference: [84] <author> E. Oran Brigham. </author> <title> The Fast Fourier Transform. </title> <booktitle> Copyright 1974 Prentice-Hall, </booktitle> <address> Englewood Cliffs NJ. ISBN 0-13-307496-X. </address>
Reference-contexts: Fast Fourier Transform The discrete Fourier Transform (DFT) <ref> [84] </ref> is a standard computation in digital signal processing for converting a vector of k complex points sampled in the time domain into a vector of k complex points in the frequency domain.
Reference: [85] <author> J. W. Cooley and J. W. Tukey. </author> <title> An algorithm for the machine computation of complex Fourier series. </title> <booktitle> Mathematics of Computation 19(90), </booktitle> <year> 1965, </year> <pages> pp. 297-301. </pages>
Reference-contexts: DFT generalizes to multiple dimensions; for simplicity the benchmark used in this chapter uses one-dimensional input and output data sets. A straightforward coding of DFT is a matrix-vector product that requires O (k 2 ) complex multiplications. Fast Fourier Transform <ref> [85] </ref> is a variant of DFT that is more computationally efficient; the FFT algorithm executes O (k log k) complex multiplications. 115 FFT executes in log k phases of computation. The first log p phases require both communication and computation, where p is the number of processing nodes.
Reference: [86] <author> Guy E. Blelloch, Charles Leiserson, Bruce M. Maggs, C. Greg Plaxton, Stephen J. Smith and Marco Zagha. </author> <title> A comparison of sorting algorithms for the Connection Machine CM-2. </title> <booktitle> Proc. of 3rd Annual ACM SPAA, </booktitle> <address> Hilton Head SC, </address> <year> 1991, </year> <pages> pp. 3-16. </pages>
Reference-contexts: One strategy to improve load balancing is to perform a two-pass algorithm. In the first pass the input data set is sampled to determine the boundaries of each bucket dynamically <ref> [86] </ref>, so that all buckets will contain roughly the same number of keys when the sorting operation is complete. To simplify this implementation of the sorting benchmark, the bucket boundaries are determined statically. The keys are generated using a uniform random distribution.
Reference: [87] <author> Gilbert Strang. </author> <title> Linear Algebra and Its Applications, second edition. </title> <publisher> Academic Press, </publisher> <address> New York NY, </address> <year> 1980, </year> <note> ISBN 0-12-673660-X. </note>
Reference-contexts: The latter version more closely reflects the maximum throughput that the benchmark can achieve from the network interface. Gaussian elimination Gaussian elimination (or Gauss for short) is a standard algorithm for inverting an nfin matrix to solve a system of linear equations <ref> [87] </ref>. Here is the implementation used in this chapter. The matrix is partitioned into rows, with one or more rows allocated to each processing node. The algorithm has n phases.
Reference: [88] <author> L. F. Cannon. </author> <title> A cellular computer to implement the Kalman filter algorithm. </title> <type> PhD dissertation, </type> <institution> Montana State University, </institution> <year> 1969. </year>
Reference-contexts: Therefore the dense matrix product solver has good load balance and is a better benchmark for evaluating network interfaces than sparse matrix product is. The standard algorithm for dense matrix multiplication (DMM) is called Cannon's algorithm <ref> [88] </ref>. The algorithm is very regular and well suited to special-purpose systolic cellular automata hardware [89]. It has been implemented on general-purpose multicomputers using a variety of parallel high-level languages including C* [90], Spot [91] and Orca C [92]. Here is a brief description of the algorithm.
Reference: [89] <author> H. T. Kung and C. E. Leiserson. </author> <title> Systolic arrays. In Introduction to VLSI Systems, </title> <editor> C. A. Mead and L. A. Conway, </editor> <publisher> Addison-Wesley, </publisher> <year> 1980, </year> <journal> section 8.3, </journal> <pages> pp. 271-292. </pages>
Reference-contexts: The standard algorithm for dense matrix multiplication (DMM) is called Cannon's algorithm [88]. The algorithm is very regular and well suited to special-purpose systolic cellular automata hardware <ref> [89] </ref>. It has been implemented on general-purpose multicomputers using a variety of parallel high-level languages including C* [90], Spot [91] and Orca C [92]. Here is a brief description of the algorithm. For simplicity, the two input matrices A and B are square and each contains nfin elements.
Reference: [90] <author> W. Daniel Hillis and Guy L. Steele, Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM 29(12), </journal> <month> December </month> <year> 1986, </year> <pages> pp. 1170-1183. </pages>
Reference-contexts: The standard algorithm for dense matrix multiplication (DMM) is called Cannon's algorithm [88]. The algorithm is very regular and well suited to special-purpose systolic cellular automata hardware [89]. It has been implemented on general-purpose multicomputers using a variety of parallel high-level languages including C* <ref> [90] </ref>, Spot [91] and Orca C [92]. Here is a brief description of the algorithm. For simplicity, the two input matrices A and B are square and each contains nfin elements. The desired product C is AB and is also an n fi n matrix.
Reference: [91] <author> David G. Socha. </author> <title> Supporting fine-grain computation on distributed-memory parallel computers. </title> <type> PhD dissertation, </type> <institution> University of Washington, Department of CSE, </institution> <month> June </month> <year> 1991, </year> <note> UW-CSE-TR 91-07-01. </note>
Reference-contexts: The standard algorithm for dense matrix multiplication (DMM) is called Cannon's algorithm [88]. The algorithm is very regular and well suited to special-purpose systolic cellular automata hardware [89]. It has been implemented on general-purpose multicomputers using a variety of parallel high-level languages including C* [90], Spot <ref> [91] </ref> and Orca C [92]. Here is a brief description of the algorithm. For simplicity, the two input matrices A and B are square and each contains nfin elements. The desired product C is AB and is also an n fi n matrix.
Reference: [92] <author> Calvin Lin. </author> <title> The portability of parallel programs across MIMD computers. </title> <type> PhD dissertation, </type> <institution> University of Washington, Department of CSE, </institution> <month> December </month> <year> 1992, </year> <note> UW-CSE-TR 92-12-04. 206 </note>
Reference-contexts: The algorithm is very regular and well suited to special-purpose systolic cellular automata hardware [89]. It has been implemented on general-purpose multicomputers using a variety of parallel high-level languages including C* [90], Spot [91] and Orca C <ref> [92] </ref>. Here is a brief description of the algorithm. For simplicity, the two input matrices A and B are square and each contains nfin elements. The desired product C is AB and is also an n fi n matrix.
Reference: [93] <author> Calvin Lin and Lawrence Snyder. </author> <title> Accommodating polymorphic data decompositions in explicitly parallel programs. </title> <booktitle> Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994, </year> <pages> pp. 68-74. </pages>
Reference-contexts: Rotating the input arrays requires only nearest-neighbor communication. The standard approach alternates communication and computation phases without overlap, but a simple transformation makes it possible to overlap communication and computation. There are two approaches to distributing the array data across the processing nodes <ref> [93] </ref>. The two-dimensional approach is to divide the arrays into rectangular tiles as in Jacobi. The one-dimensional approach is to distribute rows of the input arrays across the processing nodes as in Gauss.
Reference: [94] <author> Thomas T. Kwan, Brian K. Totty and Daniel A. Reed. </author> <title> Communication and computation performance of the CM-5. </title> <booktitle> Proc. of Supercomputing 93, </booktitle> <address> Portland OR, </address> <month> November </month> <year> 1993, </year> <pages> pp. 192-201. </pages>
Reference-contexts: Network interfaces that were previously studied included those in the Thinking Machines CM-5, the Intel Paragon, the MIT J-machine, the Motorola Star-T, the Intel Touchstone Delta and the UW Meerkat. 6.4.1 Study #1: CM-5 vs. Paragon This study by Kwan, Totty and Reed <ref> [94] </ref> focused on the measurements gathered from the CM-5 and the Paragon. Through the use of simple throughput and latency benchmarks, the authors demonstrated that the CM-5 achieves a throughput rating of 8 MB/sec out of 20 MB/sec maximum and the Paragon achieves 20 MB/s out of 200 MB/sec maximum.
Reference: [95] <author> David E. Culler, Seth C. Goldstein, Klaus E. Schauser and Thorsten von Eicken. TAM: </author> <title> a compiler controlled threaded abstract machine. </title> <editor> J. </editor> <booktitle> of Parallel and Distributed Computing 18(3), </booktitle> <month> July </month> <year> 1993, </year> <pages> pp. 347-370. </pages>
Reference-contexts: Polling accounts for 33% of the send cost. The weakness of this study was that it used an unconventional language and run-time system to perform the evaluation. The benchmarks were written in the dataflow language Id90; the run-time system is called TAM (Threaded Abstract Machine) <ref> [95] </ref>. An artifact of TAM is that it only uses small messages of no more than sixteen 32-bit words. While this study was interesting in the case of small messages, it provided little insight into the study of systems and benchmarks that perform well using large messages.
Reference: [96] <author> Ted Kehl, Steve Burns and Chris Fisher. </author> <title> Self-tuned clocks and crystal clocks. </title> <type> Technical report TR 94-05-03, </type> <institution> Dept. of CSE, University of Washington, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The whole multicomputer system is a globally asynchronous collection of these synchronous domains. The linkage between the clock domains is implemented in the router chip using an asynchronous design methodology such as self-tuning <ref> [96] </ref> or self-timing [97]. 147 Data Data Data DataAddr Data Data Data DataAddr DataAddr . . .
Reference: [97] <author> Scott Hauck. </author> <title> Asynchronous design methodologies: an overview. </title> <booktitle> Proceedings of the IEEE 83(1), </booktitle> <pages> pp. 69-93, </pages> <month> January </month> <year> 1995. </year> <note> Also available as University of Washington Dept. of CSE TR 93-05-07, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The whole multicomputer system is a globally asynchronous collection of these synchronous domains. The linkage between the clock domains is implemented in the router chip using an asynchronous design methodology such as self-tuning [96] or self-timing <ref> [97] </ref>. 147 Data Data Data DataAddr Data Data Data DataAddr DataAddr . . .
Reference: [98] <author> Greg Chesson. </author> <title> HIPPI-6400 overview. </title> <booktitle> Proc. of Hot Interconnects '96, </booktitle> <institution> Stanford University, </institution> <month> August </month> <year> 1996, </year> <pages> pp. 121-128. </pages>
Reference-contexts: An FPLD simplifies the manufacturing step: if the signature were hard-wired into the printed circuit board, then a unique circuit board layout would be needed for every processing node. One standard for these binary signatures is called the Universal Logical Address, a 48-bit code defined by IEEE specification 802.3 <ref> [98] </ref>. While this code guarantees uniqueness it may contain no information about the topology of the network. Application programs do not directly use the naming scheme described above.
Reference: [99] <author> Tenaski V. Ramabadran and Sunil S. Gaitonde. </author> <title> A tutorial on CRC computations. </title> <booktitle> IEEE Micro 8(4), </booktitle> <month> Aug. </month> <year> 1988, </year> <pages> pp. 62-75. </pages>
Reference-contexts: With two-dimensional parity, it takes a minimum of four single-bit errors to pass undetected. Furthermore the erroneous bits must form the corners of a rectangle to be undetectable (Figure 7.4b). An alternative to Teschio's bit-serial parity is the standard cyclic redundancy code (CRC) <ref> [99, 100] </ref>. A 16-bit CRC code can be encoded serially using 16 bit shifts per word using very simple hardware, but this implementation is very slow. Implementations of a parallel CRC encoder and decoder tend to be complicated 151 [100].
Reference: [100] <author> Guido Albertengo and Riccardo Sisto. </author> <title> Parallel CRC generation. </title> <booktitle> IEEE Micro 10(5), </booktitle> <month> Oct. </month> <year> 1990, </year> <pages> pp. 63-71. </pages>
Reference-contexts: With two-dimensional parity, it takes a minimum of four single-bit errors to pass undetected. Furthermore the erroneous bits must form the corners of a rectangle to be undetectable (Figure 7.4b). An alternative to Teschio's bit-serial parity is the standard cyclic redundancy code (CRC) <ref> [99, 100] </ref>. A 16-bit CRC code can be encoded serially using 16 bit shifts per word using very simple hardware, but this implementation is very slow. Implementations of a parallel CRC encoder and decoder tend to be complicated 151 [100]. <p> A 16-bit CRC code can be encoded serially using 16 bit shifts per word using very simple hardware, but this implementation is very slow. Implementations of a parallel CRC encoder and decoder tend to be complicated 151 <ref> [100] </ref>. By contrast, the vertical parity circuitry is trivial to implement as it requires only an XOR gate per bit. 7.2 Teschio internal structure level of detail beyond that shown in Figure 7.1.
Reference: [101] <author> Walter A. Hiatt. </author> <title> Mitsubishi Electric Semiconductor Products. Marketing literature from the Electronic Device Group (EDG) division of Mitsubishi Electric America (MEA). </title> <note> Presentation given on January 9, </note> <institution> 1997 at MERL | A Mit-subishi Electric Research Laboratory, </institution> <address> Cambridge MA. </address>
Reference-contexts: In this section we focus on three issues: clock frequency, pin count and gate count (area). 179 Clock frequency A representative ASIC vendor is Mitsubishi Electric's Electronic Device Group (EDG) <ref> [101] </ref>. EDG supports many different ASIC chip processes, including the 0.6, 0.5 and 0.35 m fabs. All three support clock rates of 100 MHz or more. <p> Altogether, Teschio contains approximately 20K bits, less than 3K bytes of memory. We construct a conservative estimate of gate count via the following line of reasoning. It takes 10 gates to implement one bit of SRAM memory that includes self-test capability <ref> [101] </ref>. The total number of gates devoted to Teschio's memory is therefore approximately 200,000. To calculate an upper bound on chip area, we use the assumption that SRAM occupies 50% or more of the total area.
Reference: [102] <author> Mark Shand et al. </author> <title> The DEC PCI Pamette V1. World Wide Web site, </title> <note> http://www.research.digital.com:80/SRC/pamette/. 207 </note>
Reference-contexts: Up to 16 workstations can be used. The link between each workstation and the hub is based on the Fibrechannel physical layer. The network interface is the DEC PCI Pamette card <ref> [102] </ref>, a PCI card that sits between the Alpha and the Fibrechannel encoder-decoder. The Pamette contains several Xilinx field programmable gate arrays (FPGAs) that can be programmed, tested and debugged using a variety of circuits.
Reference: [103] <author> M.J. Feeley, W.E. Morgan, F.H. Pighin, A.R. Karlin, H.M. Levy, and C.A. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: To evaluate the effectiveness of Chaos-LAN and the Pamette-based network interface, we plan to run a software environment called the Global Memory System (GMS) <ref> [103, 104] </ref> on the NOW. GMS is based on the idea of paging over the network to idle workstations instead of to local disk.
Reference: [104] <author> H.A. Jamrozik, M.J. Feeley, G.M. Voelker, J. Evans II, A.R. Karlin, H.M. Levy, and M.K. Vernon. </author> <title> Reducing network latency using subpages in a global memory environment. </title> <booktitle> Proceedings of the Seventh ACM Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VII), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: To evaluate the effectiveness of Chaos-LAN and the Pamette-based network interface, we plan to run a software environment called the Global Memory System (GMS) <ref> [103, 104] </ref> on the NOW. GMS is based on the idea of paging over the network to idle workstations instead of to local disk.
References-found: 104

