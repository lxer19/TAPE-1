URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P604.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Keyword: Index Terms Distributed algorithm, hypercube, relational algebra, relational database systems.  
Note: NICHOLAS T. KARONIS Preprint ANL/MCS-P604-0796  
Abstract: An Alternative Method to Remove Duplicate Tuples Resulting from Operations in the Relational Algebra in a Cube-Connected Multicomputer System Abstract The problem of performing database operations on parallel architectures has received much attention, both as applied and theoretical areas of research. Much of the attention has been focused on performing these operations on distributed-memory architectures, for example, a hypercube. Algorithms that perform, in particular, relational database operations on a hypercube typically exploit the hypercube's unique interconnectivity to not only process the relational operators efficiently but also perform dynamic load balancing. Certain relational operators (e.g., projection and union) can produce interim relations that contain duplicate tuples. As a result, an algorithm for a relational database system must address the issue of removing duplicate tuples from these interim relations. The algorithms accomplish this by compacting the relation into hypercubes of smaller and smaller dimensions. We present an alternative method for removing duplicate tuples from a relation that is distributed over a hypercube by using the embedded ring found in every hypercube. Through theoretical analysis of the algorithm and empirical observation, we demonstrate that using the ring to remove the duplicate tuples is significantly more efficient than using the hypercube. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. K. Baru and O. Frieder, </author> <title> "Database operations in a Cube-connected multicomputer system," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 38, no. 6, </volume> <pages> pp. 920-927, </pages> <year> 1989. </year>
Reference-contexts: Several parallel computers based on a hypercube's connectivity currently exist, for example, Caltech's MARK III [16], the Floating Point Systems T-series machines [6], Intel iPSC/II [19], and the NCUBE/10 [9]. An algorithm was presented <ref> [1, 7, 2] </ref> to perform relational database operations in a distributed processing environment. It is described as a cube-connected multicomputer system whereby the processes are connected in a hypercube, they communicate via message passing, and the tuples of the relations are horizontally partitioned over the set of processes. <p> The algorithm we present uses the ring embedded in all hypercubes [14], thereby reducing the overall execution time. Additionally, the algorithm we present leaves the tuples more evenly distributed (balanced) over the processes. A balanced set of tuples is an imperative for proper load balancing <ref> [1, 7] </ref>. 2 The Hypercube Remove-Duplicates Algorithm Here we describe the algorithm presented in [1, 7, 2]. Figures 1a-e show an example 2-D hypercube with relation R having 4n tuples. <p> Additionally, the algorithm we present leaves the tuples more evenly distributed (balanced) over the processes. A balanced set of tuples is an imperative for proper load balancing [1, 7]. 2 The Hypercube Remove-Duplicates Algorithm Here we describe the algorithm presented in <ref> [1, 7, 2] </ref>. Figures 1a-e show an example 2-D hypercube with relation R having 4n tuples. We assume that this relation is the result of a recently performed projection and therefore might have duplicate 2 tuples that must be removed. <p> One solution to this problem was 17 presented in <ref> [1, 7, 2] </ref>, which compacts the relation into hypercubes of smaller and smaller dimensions. We presented an alternate algorithm for removing duplicate tuples that used the embedded ring found in every hypercube.
Reference: [2] <author> C. K. Baru and O. Frieder, </author> <title> "Implementing relational database operations in a cube-connected multicomputer," </title> <booktitle> in Proc. IEEE COMPDEC 3rd Int. Conf. Data Eng., </booktitle> <address> Feb. 1987, Los Angeles, CA. </address>
Reference-contexts: Several parallel computers based on a hypercube's connectivity currently exist, for example, Caltech's MARK III [16], the Floating Point Systems T-series machines [6], Intel iPSC/II [19], and the NCUBE/10 [9]. An algorithm was presented <ref> [1, 7, 2] </ref> to perform relational database operations in a distributed processing environment. It is described as a cube-connected multicomputer system whereby the processes are connected in a hypercube, they communicate via message passing, and the tuples of the relations are horizontally partitioned over the set of processes. <p> Additionally, the algorithm we present leaves the tuples more evenly distributed (balanced) over the processes. A balanced set of tuples is an imperative for proper load balancing [1, 7]. 2 The Hypercube Remove-Duplicates Algorithm Here we describe the algorithm presented in <ref> [1, 7, 2] </ref>. Figures 1a-e show an example 2-D hypercube with relation R having 4n tuples. We assume that this relation is the result of a recently performed projection and therefore might have duplicate 2 tuples that must be removed. <p> One solution to this problem was 17 presented in <ref> [1, 7, 2] </ref>, which compacts the relation into hypercubes of smaller and smaller dimensions. We presented an alternate algorithm for removing duplicate tuples that used the embedded ring found in every hypercube.
Reference: [3] <author> C.K. Baru and S.Y.W. Su, </author> <title> "The architecture of SM3: A dynamically/partionable multicom-puter with switchable memory," </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. C-35, </volume> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: In the early days, special-purpose hardware was built to improve performance. These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines [4, 23]. More recent efforts have focused on multiprocessor systems, for example, DIRECT [23], GAMMA [5], GRACE [13], MIRDM [17], NON-VON [10], SM3 <ref> [3] </ref>, and TERADATA [24]. Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing.
Reference: [4] <author> P. B. Berra and E. Oliver, </author> <title> "The role of associative array processors in data base machine architecture," </title> <journal> IEEE Computer, </journal> <volume> vol. 12, </volume> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38. 1 performance by using existing models. In the early days, special-purpose hardware was built to improve performance. These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines <ref> [4, 23] </ref>. More recent efforts have focused on multiprocessor systems, for example, DIRECT [23], GAMMA [5], GRACE [13], MIRDM [17], NON-VON [10], SM3 [3], and TERADATA [24]. Many of these systems use parallel algorithms to enhance database processing.
Reference: [5] <author> D. J. DeWitt et al., </author> <title> "GAMMA A high performance dataflow database machine," </title> <booktitle> in Proc. Int. Conf. Very Large Data Bases, </booktitle> <address> Aug. 1986, Kyoto, Japan. </address>
Reference-contexts: In the early days, special-purpose hardware was built to improve performance. These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines [4, 23]. More recent efforts have focused on multiprocessor systems, for example, DIRECT [23], GAMMA <ref> [5] </ref>, GRACE [13], MIRDM [17], NON-VON [10], SM3 [3], and TERADATA [24]. Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing.
Reference: [6] <author> K. A. Frenkel, </author> <title> "Evaluating two massively parallel machines," </title> <journal> Commmun. ACM, </journal> <volume> vol. 29, </volume> <pages> pp. 752-758, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing. Several parallel computers based on a hypercube's connectivity currently exist, for example, Caltech's MARK III [16], the Floating Point Systems T-series machines <ref> [6] </ref>, Intel iPSC/II [19], and the NCUBE/10 [9]. An algorithm was presented [1, 7, 2] to perform relational database operations in a distributed processing environment.
Reference: [7] <author> O. Frieder, </author> <title> "Database processing on a cube-connected multicomputer," </title> <type> Ph.D. Dissertation, </type> <institution> Dep. EECS, University of Michigan, </institution> <address> Ann Arbor, MI, 41809, </address> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: Several parallel computers based on a hypercube's connectivity currently exist, for example, Caltech's MARK III [16], the Floating Point Systems T-series machines [6], Intel iPSC/II [19], and the NCUBE/10 [9]. An algorithm was presented <ref> [1, 7, 2] </ref> to perform relational database operations in a distributed processing environment. It is described as a cube-connected multicomputer system whereby the processes are connected in a hypercube, they communicate via message passing, and the tuples of the relations are horizontally partitioned over the set of processes. <p> The algorithm we present uses the ring embedded in all hypercubes [14], thereby reducing the overall execution time. Additionally, the algorithm we present leaves the tuples more evenly distributed (balanced) over the processes. A balanced set of tuples is an imperative for proper load balancing <ref> [1, 7] </ref>. 2 The Hypercube Remove-Duplicates Algorithm Here we describe the algorithm presented in [1, 7, 2]. Figures 1a-e show an example 2-D hypercube with relation R having 4n tuples. <p> Additionally, the algorithm we present leaves the tuples more evenly distributed (balanced) over the processes. A balanced set of tuples is an imperative for proper load balancing [1, 7]. 2 The Hypercube Remove-Duplicates Algorithm Here we describe the algorithm presented in <ref> [1, 7, 2] </ref>. Figures 1a-e show an example 2-D hypercube with relation R having 4n tuples. We assume that this relation is the result of a recently performed projection and therefore might have duplicate 2 tuples that must be removed. <p> One solution to this problem was 17 presented in <ref> [1, 7, 2] </ref>, which compacts the relation into hypercubes of smaller and smaller dimensions. We presented an alternate algorithm for removing duplicate tuples that used the embedded ring found in every hypercube.
Reference: [8] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1994. </year>
Reference-contexts: If we wish to increase the amount of data being communicated without increasing the comparison time, we simply increase p 0 and/or N . Both methods were implemented by using Argonne National Laboratory's implementation of MPI <ref> [8, 20, 21, 22] </ref>. They were executed on the 128-processor IBM POWERparallel System at Argonne National Laboratory running AIX version 3.2.4 and the AIX Parallel Environment [11], which includes IBM's message-passing library MPL. Argonne's implementation of MPI on their POWERparallel System calls MPL directly.
Reference: [9] <author> J. P. Hayes et al., </author> <title> "Architecture of a hypercube supercomputer," </title> <booktitle> IEEE Micro, </booktitle> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing. Several parallel computers based on a hypercube's connectivity currently exist, for example, Caltech's MARK III [16], the Floating Point Systems T-series machines [6], Intel iPSC/II [19], and the NCUBE/10 <ref> [9] </ref>. An algorithm was presented [1, 7, 2] to perform relational database operations in a distributed processing environment.
Reference: [10] <author> B. Hillyer, D. E. Shaw, and A. Nigam, </author> <title> "NON-VON's performance on certain database benchmarks," </title> <journal> IEEE Trans. Software Eng., </journal> <volume> vol. SE-12, </volume> <month> Apr. </month> <year> 1986. </year> <title> [11] "IBM AIX Parallel Environment-Parallel Programming Subroutine Reference (2.0)", </title> <institution> International Business Machines Corporation, Kingston, </institution> <address> NY., </address> <month> June, </month> <year> 1994. </year>
Reference-contexts: In the early days, special-purpose hardware was built to improve performance. These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines [4, 23]. More recent efforts have focused on multiprocessor systems, for example, DIRECT [23], GAMMA [5], GRACE [13], MIRDM [17], NON-VON <ref> [10] </ref>, SM3 [3], and TERADATA [24]. Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing.
Reference: [12] <author> C. Jard, J. -F. Monin, and R. Groz, </author> <title> "Development of Veda, a prototyping tool for distributed algorithms," </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> vol. 14, no. 3, </volume> <pages> pp. 339-352, </pages> <month> Mar. </month> <year> 1988. </year> <month> 19 </month>
Reference: [13] <author> M. Kitsuregawa, H. Tanaka, and T. Moto-Oka, </author> <title> "Architecture and performance of relational algebra machine GRACE," </title> <booktitle> in Proc. Int. Conf. Parallel Processing, </booktitle> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: In the early days, special-purpose hardware was built to improve performance. These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines [4, 23]. More recent efforts have focused on multiprocessor systems, for example, DIRECT [23], GAMMA [5], GRACE <ref> [13] </ref>, MIRDM [17], NON-VON [10], SM3 [3], and TERADATA [24]. Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing.
Reference: [14] <author> N. T. Karonis, </author> <title> "On the verification of complex protocols," </title> <type> Ph.D. Dissertation, </type> <institution> Syracuse University, </institution> <year> 1992. </year>
Reference-contexts: Accordingly, the algorithm describes how to remove duplicate tuples from these relations by using an operation called Relation Compaction (RC), which exploits the hypercube's unique topology. We present an alternative algorithm for removing duplicate tuples. The algorithm we present uses the ring embedded in all hypercubes <ref> [14] </ref>, thereby reducing the overall execution time. Additionally, the algorithm we present leaves the tuples more evenly distributed (balanced) over the processes.
Reference: [15] <author> N. T. Karonis, </author> <title> "Timing parallel programs that use message passing," </title> <journal> J. Parallel Distribut. Comput., </journal> <volume> vol. 14, no. 1, </volume> <pages> pp. 29-36, </pages> <month> Jan. </month> <year> 1992. </year>
Reference: [16] <author> J. C. Peterson, J. O. Tuazon, D. Lieberman, and M. Pniel, </author> <title> "The MARK III hypercube-ensemble concurrent computer," </title> <booktitle> in Proc. Int. Conf. Parallel Processing, </booktitle> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing. Several parallel computers based on a hypercube's connectivity currently exist, for example, Caltech's MARK III <ref> [16] </ref>, the Floating Point Systems T-series machines [6], Intel iPSC/II [19], and the NCUBE/10 [9]. An algorithm was presented [1, 7, 2] to perform relational database operations in a distributed processing environment.
Reference: [17] <author> G. Z. Oadah and K. B. Irani, </author> <title> "A database machine for very large relational databases," </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. C-34, </volume> <month> Nov. </month> <year> 1985. </year>
Reference-contexts: In the early days, special-purpose hardware was built to improve performance. These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines [4, 23]. More recent efforts have focused on multiprocessor systems, for example, DIRECT [23], GAMMA [5], GRACE [13], MIRDM <ref> [17] </ref>, NON-VON [10], SM3 [3], and TERADATA [24]. Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing.
Reference: [18] <author> S. Y. W. Su and C. K. Baru, </author> <title> "Dynamically partitionable multicomputers with switchable memory," </title> <journal> J. Parallel Distribut. Comput., </journal> <volume> vol. 1, no. 2, </volume> <year> 1984. </year>
Reference: [19] <editor> Intel iPSC Data Sheet, </editor> <volume> Order No. </volume> <pages> 280101-001, </pages> <year> 1985. </year>
Reference-contexts: One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing. Several parallel computers based on a hypercube's connectivity currently exist, for example, Caltech's MARK III [16], the Floating Point Systems T-series machines [6], Intel iPSC/II <ref> [19] </ref>, and the NCUBE/10 [9]. An algorithm was presented [1, 7, 2] to perform relational database operations in a distributed processing environment.
Reference: [20] <author> Message Passing Interface Forum, </author> <title> "MPI: A Message Passing Interface Forum," </title> <journal> Int. J. Supercomputer Apps., </journal> <volume> vol. 8, no. 3/4, </volume> <pages> pp. 165-414, </pages> <year> 1994. </year>
Reference-contexts: If we wish to increase the amount of data being communicated without increasing the comparison time, we simply increase p 0 and/or N . Both methods were implemented by using Argonne National Laboratory's implementation of MPI <ref> [8, 20, 21, 22] </ref>. They were executed on the 128-processor IBM POWERparallel System at Argonne National Laboratory running AIX version 3.2.4 and the AIX Parallel Environment [11], which includes IBM's message-passing library MPL. Argonne's implementation of MPI on their POWERparallel System calls MPL directly.
Reference: [21] <author> Message Passing Interface Forum, </author> <title> "The MPI Message Passing Interface Standard", </title> <note> http://www.mcs.anl.gov/mpi/standard.html, May 1995. </note>
Reference-contexts: If we wish to increase the amount of data being communicated without increasing the comparison time, we simply increase p 0 and/or N . Both methods were implemented by using Argonne National Laboratory's implementation of MPI <ref> [8, 20, 21, 22] </ref>. They were executed on the 128-processor IBM POWERparallel System at Argonne National Laboratory running AIX version 3.2.4 and the AIX Parallel Environment [11], which includes IBM's message-passing library MPL. Argonne's implementation of MPI on their POWERparallel System calls MPL directly.
Reference: [22] <author> M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and J. Dongarra, </author> <title> MPI: The Complete Reference, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: If we wish to increase the amount of data being communicated without increasing the comparison time, we simply increase p 0 and/or N . Both methods were implemented by using Argonne National Laboratory's implementation of MPI <ref> [8, 20, 21, 22] </ref>. They were executed on the 128-processor IBM POWERparallel System at Argonne National Laboratory running AIX version 3.2.4 and the AIX Parallel Environment [11], which includes IBM's message-passing library MPL. Argonne's implementation of MPI on their POWERparallel System calls MPL directly.
Reference: [23] <institution> Special issue on Database Machines, </institution> <note> IEEE Trans. Comput., vol. C-28, </note> <month> June </month> <year> 1979. </year>
Reference-contexts: Department of Energy, under Contract W-31-109-Eng-38. 1 performance by using existing models. In the early days, special-purpose hardware was built to improve performance. These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines <ref> [4, 23] </ref>. More recent efforts have focused on multiprocessor systems, for example, DIRECT [23], GAMMA [5], GRACE [13], MIRDM [17], NON-VON [10], SM3 [3], and TERADATA [24]. Many of these systems use parallel algorithms to enhance database processing. <p> In the early days, special-purpose hardware was built to improve performance. These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines [4, 23]. More recent efforts have focused on multiprocessor systems, for example, DIRECT <ref> [23] </ref>, GAMMA [5], GRACE [13], MIRDM [17], NON-VON [10], SM3 [3], and TERADATA [24]. Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing.
Reference: [24] <author> TERADATA, </author> <title> DBC/1012 Data Base Computer Concepts and Facilities, Release 1.3, </title> <month> June </month> <year> 1985. </year> <month> 20 </month>
Reference-contexts: These database machines were aimed at relieving I/O and other bottlenecks found in conventional machines [4, 23]. More recent efforts have focused on multiprocessor systems, for example, DIRECT [23], GAMMA [5], GRACE [13], MIRDM [17], NON-VON [10], SM3 [3], and TERADATA <ref> [24] </ref>. Many of these systems use parallel algorithms to enhance database processing. One parallel architecture in particular, the hypercube, has been an architecture of choice for database processing.
References-found: 23

