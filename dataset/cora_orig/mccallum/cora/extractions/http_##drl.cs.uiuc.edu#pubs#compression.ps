URL: http://drl.cs.uiuc.edu/pubs/compression.ps
Refering-URL: http://drl.cs.uiuc.edu/panda/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: fseamons,winslettg@cs.uiuc.edu  
Title: A Data Management Approach for Handling Large Compressed Arrays in High Performance Computing  
Author: Kent E. Seamons and Marianne Winslett 
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: Poor parallel i/o performance has recently been recognized as a roadblock to scalability of parallel architectures, algorithms, and data sets. For i/o of large arrays, the storage of arrays by subarray divisions| chunking|has been shown to improve i/o performance substantially in many circumstances. In this paper we show how to increase the performance advantages of chunking by combining it with data compression, and describe the results of experiments with compressed chunks from scientific data sets on the Intel iPSC/860. For a particular fixed array size and compression ratio, uncompressed chunk i/o is faster than compressed chunk i/o when the number of processors is small; the reverse holds when the number of processors is large, as the cost of compression is spread over a larger number of processors. With good compression ratios and large numbers of processors, we obtained an effective logical i/o rate for compressed chunks that exceeds the theoretical possible maximum for uncompressed data, by adding compression to an existing chunked i/o library. Our results suggest that compression may be a good technique for handling sparse arrays in parallel i/o. 
Abstract-found: 1
Intro-found: 1
Reference: [Bell87] <author> J. L. Bell and G. S. Patterson, Jr. </author> <title> Data organization in large numerical computations. </title> <journal> The Journal of Supercomputing, </journal> <volume> 1(1), </volume> <year> 1987. </year>
Reference-contexts: contribution in this paper is to show how to combine compression and chunking to achieve an overall i/o time speedup that is greater than that achieved using either technique alone. 2 Chunking Dealing with array chunks, in and out of memory, has attracted considerable current interest in the research community <ref> [Bell87, Brezany92, Corbett93, French91, Galbreath93, Karpovich94, Sarawagi94] </ref>, as it seems to be a crucial issue for many distributed, parallel, and out-of-core computations. We expect chunking to play a major role in future parallel i/o libraries and in file systems designed for parallel machines.
Reference: [Bordawekar93] <author> R. Bordawekar, J. del Rosario, and A. Choudary. </author> <title> Design and evaluation of primitives for parallel i/o. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: Similarly, for input of a large array in traditional order, each processor can read a contiguous section of the array on disk, and then the processors can synchronize and exchange data among themselves until each processor has its desired chunk of the array and can begin processing <ref> [Bordawekar93] </ref>. This approach has the advantage that arrays on disk are in traditional order and can be easily read by any application program. However, we see two reasons why storage of chunks on disk will usually be preferable to dynamic rechunking of traditional-order arrays. <p> In the future, we plan to also support dynamic rechunking of data in memory, in the style of <ref> [Bordawekar93] </ref>. For writes, Panda examines the previously declared physical schema for the data set to determine how to arrange the data of the current chunk on disk.
Reference: [Brezany92] <author> P. Brezany, M. Gerndt, P. Mehrotra, and H. Zima. </author> <title> Concurrent file operations in a high performance FORTRAN. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-238, </pages> <year> 1992. </year>
Reference-contexts: From a software viewpoint, what can be done to improve parallel i/o performance? In the case of i/o of large arrays, we agree with <ref> [Brezany92, Corbett93, Karpovich94, Sarawagi94] </ref> that one very promising software technique for speeding up i/o is chunking| the division of a large multidimensional array into subarrays, with storage on disk by subarray rather than in `traditional' row major or column major order. <p> contribution in this paper is to show how to combine compression and chunking to achieve an overall i/o time speedup that is greater than that achieved using either technique alone. 2 Chunking Dealing with array chunks, in and out of memory, has attracted considerable current interest in the research community <ref> [Bell87, Brezany92, Corbett93, French91, Galbreath93, Karpovich94, Sarawagi94] </ref>, as it seems to be a crucial issue for many distributed, parallel, and out-of-core computations. We expect chunking to play a major role in future parallel i/o libraries and in file systems designed for parallel machines.
Reference: [Corbett93] <author> P. F. Corbett, D. G. Feitelson, J. Prost, and S. Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: From a software viewpoint, what can be done to improve parallel i/o performance? In the case of i/o of large arrays, we agree with <ref> [Brezany92, Corbett93, Karpovich94, Sarawagi94] </ref> that one very promising software technique for speeding up i/o is chunking| the division of a large multidimensional array into subarrays, with storage on disk by subarray rather than in `traditional' row major or column major order. <p> contribution in this paper is to show how to combine compression and chunking to achieve an overall i/o time speedup that is greater than that achieved using either technique alone. 2 Chunking Dealing with array chunks, in and out of memory, has attracted considerable current interest in the research community <ref> [Bell87, Brezany92, Corbett93, French91, Galbreath93, Karpovich94, Sarawagi94] </ref>, as it seems to be a crucial issue for many distributed, parallel, and out-of-core computations. We expect chunking to play a major role in future parallel i/o libraries and in file systems designed for parallel machines.
Reference: [French91] <author> J. C. French, T. W. Pratt, and M. Das. </author> <title> Performance measurement of a parallel input/output system for the intel iPSC/2 hypercube. </title> <booktitle> In Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of a Computer Systems, </booktitle> <pages> pages 178-187, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: contribution in this paper is to show how to combine compression and chunking to achieve an overall i/o time speedup that is greater than that achieved using either technique alone. 2 Chunking Dealing with array chunks, in and out of memory, has attracted considerable current interest in the research community <ref> [Bell87, Brezany92, Corbett93, French91, Galbreath93, Karpovich94, Sarawagi94] </ref>, as it seems to be a crucial issue for many distributed, parallel, and out-of-core computations. We expect chunking to play a major role in future parallel i/o libraries and in file systems designed for parallel machines.
Reference: [Galbreath93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel i/o. </title> <booktitle> In Proceeding of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: contribution in this paper is to show how to combine compression and chunking to achieve an overall i/o time speedup that is greater than that achieved using either technique alone. 2 Chunking Dealing with array chunks, in and out of memory, has attracted considerable current interest in the research community <ref> [Bell87, Brezany92, Corbett93, French91, Galbreath93, Karpovich94, Sarawagi94] </ref>, as it seems to be a crucial issue for many distributed, parallel, and out-of-core computations. We expect chunking to play a major role in future parallel i/o libraries and in file systems designed for parallel machines.
Reference: [Hiranandani92] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8), </volume> <month> August </month> <year> 1992. </year>
Reference: [HDF94] <institution> National Center for Supercomputing Applications, University of Illinois. </institution> <note> NCSA HDF Reference Manual, Version 3.3, </note> <month> February </month> <year> 1994. </year>
Reference-contexts: When only a small portion of the data is of interest to the application, the overhead of decompressing the entire array will be excessive. Compression at the file level is particularly inappropriate, for in the common case of self-describing files <ref> [HDF94, NetCDF92] </ref> an application might have to decompress an entire file just to look at the metadata for the file and determine whether it is of interest. Similarly, a naive approach to compression will not work well for distributed workstation computations.
Reference: [HPF93] <author> High Performance Fortran Forum. </author> <title> High performance Fortran language specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference: [Karpovich94] <author> J. F. Karpovich, A. S. Grimshaw, and J. C. </author> <title> French. Extensible file systems (ELFS): An object-oriented approach to high performance file i/o. </title> <booktitle> In Proceedings of the International Conference on Object-Oriented Programming, Systems, Languages, and Applications, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: From a software viewpoint, what can be done to improve parallel i/o performance? In the case of i/o of large arrays, we agree with <ref> [Brezany92, Corbett93, Karpovich94, Sarawagi94] </ref> that one very promising software technique for speeding up i/o is chunking| the division of a large multidimensional array into subarrays, with storage on disk by subarray rather than in `traditional' row major or column major order. <p> contribution in this paper is to show how to combine compression and chunking to achieve an overall i/o time speedup that is greater than that achieved using either technique alone. 2 Chunking Dealing with array chunks, in and out of memory, has attracted considerable current interest in the research community <ref> [Bell87, Brezany92, Corbett93, French91, Galbreath93, Karpovich94, Sarawagi94] </ref>, as it seems to be a crucial issue for many distributed, parallel, and out-of-core computations. We expect chunking to play a major role in future parallel i/o libraries and in file systems designed for parallel machines.
Reference: [Kotz94] <author> D. Kotz. </author> <title> Disk-directed i/o for MIMD multiprocessors. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: The application-level interface gives us a greater degree of control than would be possible with automatic file-level or disk-level compression. It would be interesting to investigate the combination of compression and chunking with disk-directed i/o <ref> [Kotz94] </ref>, a promising file-system-level technique for improving parallel i/o performance. Our experiments were all run on a parallel machine.
Reference: [NetCDF92] <institution> University Corporation for Atmospheric Research, Unidata Program Center. </institution> <note> NetCDF User's Guide, Version 2.0, </note> <month> October </month> <year> 1992. </year>
Reference-contexts: When only a small portion of the data is of interest to the application, the overhead of decompressing the entire array will be excessive. Compression at the file level is particularly inappropriate, for in the common case of self-describing files <ref> [HDF94, NetCDF92] </ref> an application might have to decompress an entire file just to look at the metadata for the file and determine whether it is of interest. Similarly, a naive approach to compression will not work well for distributed workstation computations.
Reference: [Nitzberg92] <author> B. Nitzberg. </author> <title> Performance of the iPSC/860 concurrent file system. </title> <type> Technical Report RND-92-020, </type> <institution> NASA Ames Research Center, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: As the number of processors increases past 32 processors, throughput for uncompressed data begins to drop because the CFS file system does not cope well with large numbers of simultaneous i/o requests <ref> [Nitzberg92] </ref>. While uncompressed throughput is falling, compressed throughput is rising, because the overhead of compressing the chunks is divided among more and more processors. At some point between 8 and 64 processors, depending on the compression ratio, throughput is greater for compressed chunks than for uncompressed chunks. <p> The higher the compression ratio, the sooner compression is useful. The iPSC/860 at NAS can sustain i/o rates of 10MB/s for uncompressed chunks in theory, and 7-8MB/s in practice <ref> [Nitzberg92] </ref>. These limits are imposed by bandwidth of the i/o node-to-disk connection and the i/o nodes' buffer size. But Figure 4 shows a maximum i/o throughput of 18MB/s for compressed chunks. size constant and using the lzrw3-a utility to compress array chunks. <p> For example, deliberate staggering of i/o requests has been found to increase performance of CFS i/o requests <ref> [Nitzberg92] </ref>. To test this hypothesis, we performed runs where all processors were synchronized after compression and before writing, and found no significant change from the results in Figure 5.
Reference: [Pierce89] <author> P. Pierce. </author> <title> A concurrent file system for a highly parallel mass storage subsystem. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <month> March </month> <year> 1989. </year>
Reference: [Sarawagi94] <author> S. Sarawagi and M. Stonebraker. </author> <title> Efficient organization of large multidimensional arrays. </title> <booktitle> In Proceedings of the 10th International Conference on Data Engineering, </booktitle> <month> Febru-ary </month> <year> 1994. </year>
Reference-contexts: From a software viewpoint, what can be done to improve parallel i/o performance? In the case of i/o of large arrays, we agree with <ref> [Brezany92, Corbett93, Karpovich94, Sarawagi94] </ref> that one very promising software technique for speeding up i/o is chunking| the division of a large multidimensional array into subarrays, with storage on disk by subarray rather than in `traditional' row major or column major order. <p> contribution in this paper is to show how to combine compression and chunking to achieve an overall i/o time speedup that is greater than that achieved using either technique alone. 2 Chunking Dealing with array chunks, in and out of memory, has attracted considerable current interest in the research community <ref> [Bell87, Brezany92, Corbett93, French91, Galbreath93, Karpovich94, Sarawagi94] </ref>, as it seems to be a crucial issue for many distributed, parallel, and out-of-core computations. We expect chunking to play a major role in future parallel i/o libraries and in file systems designed for parallel machines.
Reference: [Seamons94a] <author> K. E. Seamons and M. Winslett. </author> <title> Physical schemas for large multidimensional arrays in scientific computing applications. </title> <booktitle> In Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: The work is embodied in the Panda i/o library, accessible on-line at URL http://bunny.cs.uiuc.edu/CADR/winslett/arrays.html. Panda's high- and low-level interfaces are described in <ref> [Seamons94a] </ref>; the performance improvements we obtained by using chunking are described in chunks.
Reference: [Seamons94b] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array i/o. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: In our experiments, we have found that appropriate use of chunking typically reduces output time for large arrays by a factor of ten <ref> [Seamons94b] </ref>. One might wonder whether a chunked organization of data on disk is really necessary. <p> The storage of data on disk in a manner that preserves that locality|i.e., chun-ked storage|can be a great performance advantage for the visualizer <ref> [Seamons94b] </ref>. Another promising high-level software technique for reducing parallel i/o time is data compression. Like chunking, compression tends to concentrate the data of interest to an application on fewer disk pages, so that fewer i/o operations are needed. <p> Storage on disk by chunks allows efficient assembly of subarrays in multiple dimensions from disk to main memory, and resembles the BLOCK approach to distributing each dimension of an array across multiple processors in distributed memory computers. <ref> [Seamons94b] </ref>. In the remainder of this section we very briefly describe Panda's interfaces. Panda's low-level interface supports calls to read and write logical chunks of an array. <p> In general, postprocessors such as visualizers will run faster on chunked data than on unchun-ked data <ref> [Seamons94b] </ref> because of the higher degree of locality. If chunks are compressed, however, an application must read an entire chunk even if it only needs a small portion of that chunk, so perhaps i/o costs would rise for applications that only looked at a small amount of data.
Reference: [Welch84] <author> T. A. Welch. </author> <title> A technique for high performance data compression. </title> <journal> IEEE Computer, </journal> <volume> 17 </volume> <pages> 8-19, </pages> <month> June </month> <year> 1984. </year>
References-found: 18

