URL: http://www.cis.hut.fi/~aapo/ps/NC97.ps
Refering-URL: http://www.cis.hut.fi/~oja/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: aapo.hyvarinen@hut.fi, erkki.oja@hut.fi  
Title: A Fast Fixed-Point Algorithm for Independent Component Analysis  
Author: Aapo Hyvrinen and Erkki Oja 
Address: Rakentajanaukio 2C, 02150 Espoo, Finland  
Affiliation: Helsinki University of Technology Laboratory of Computer and Information Science  
Abstract: This paper will appear in Neural Computation, 9:1483-1492, 1997. Abstract We introduce a novel fast algorithm for Independent Component Analysis, which can be used for blind source separation and feature extraction. It is shown how a neural network learning rule can be transformed into a txed-point iteration, which provides an algorithm that is very simple, does not depend on any user-detned parameters, and is fast to converge to the most accurate solution allowed by the data. The algorithm tnds, one at a time, all non-Gaussian independent components, regardless of their probability distributions. The computations can be performed either in batch mode or in a semi-adaptive manner. The convergence of the algorithm is rigorously proven, and the convergence speed is shown to be cubic. Some comparisons to gradient based algorithms are made, showing that the new algorithm is usually 10 to 100 times faster, sometimes giving the solution in just a few iterations.
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S., Cichocki, A., and Yang, H. </author> <year> (1996). </year> <title> A new learning algorithm for blind source separation. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing 8 (Proc. NIPS'95), </booktitle> <pages> pages 757763. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Bell, A. and Sejnowski, T. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. Neural Computation, </title> <publisher> 7:11291159. </publisher>
Reference: <author> Bell, A. and Sejnowski, T. </author> <year> (1996a). </year> <title> The 'independent components' of natural scenes are edge tlters. </title> <journal> Vision Research. </journal> <note> To appear. </note>
Reference: <author> Bell, A. and Sejnowski, T. </author> <year> (1996b). </year> <title> Learning higher-order structure of a natural sound. </title> <journal> Network, </journal> <volume> 7:261 266. </volume>
Reference: <author> Cardoso, J.-F. </author> <year> (1990). </year> <title> Eigen-structure of the fourth-order cumulant tensor with application to the blind source separation problem. </title> <booktitle> In Proc. ICASSP'90, </booktitle> <pages> pages 26552658, </pages> <address> Albuquerque, NM, USA. </address>
Reference: <author> Cardoso, J.-F. </author> <year> (1992). </year> <title> Iterative techniques for blind source separation using only fourth-order cumulants. </title> <booktitle> In Proc. EUSIPCO, </booktitle> <pages> pages 739742, </pages> <address> Brussels, Belgium. </address>
Reference: <author> Comon, P. </author> <year> (1994). </year> <title> Independent component analysis a new concept? Signal Processing, </title> <publisher> 36:287314. </publisher>
Reference-contexts: Two interesting applications of ICA are blind source separation and feature extraction. In the simplest form of ICA <ref> (Comon, 1994) </ref>, we observe m scalar random variables v 1 ; v 2 ; :::; v m which are assumed to be linear combinations of n unknown independent components s 1 ; s 2 ; :::; s n that are mutually statistically independent, and zero-mean.
Reference: <author> Delfosse, N. and Loubaton, P. </author> <year> (1995). </year> <title> Adaptive blind separation of independent sources: a deation approach. Signal Processing, </title> <publisher> 45:5983. </publisher>
Reference-contexts: We introduce an algorithm using a very simple, yet highly ecient, txed-point iteration scheme for tnding the local extrema of the kurtosis of a linear combination of the observed variables. It is well-known <ref> (Delfosse and Loubaton, 1995) </ref> that tnding the local extrema of kurtosis is equivalent to estimating the non-Gaussian independent components. However, the convergence of our algorithm will be proven independently of these well-known results. The computations can be performed either in batch mode or semi-adaptively.
Reference: <author> Hurri, J., Hyvrinen, A., Karhunen, J., and Oja, E. </author> <year> (1996). </year> <title> Image feature extraction using independent component analysis. </title> <booktitle> In Proc. NORSIG'96, </booktitle> <pages> pages 475478, </pages> <address> Espoo, Finland. 9 Hyvrinen, </address> <publisher> A. </publisher> <year> (1997a). </year> <title> A family of txed-point algorithms for independent component analysis. </title> <booktitle> In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP'97), </booktitle> <pages> pages 39173920, </pages> <address> Munich, Germany. </address>
Reference: <author> Hyvrinen, A. </author> <year> (1997b). </year> <title> Independent component analysis by minimization of mutual information. </title> <type> Technical report, </type> <institution> Helsinki University of Technology, Laboratory of Computer and Information Science. </institution>
Reference: <author> Hyvrinen, A. and Oja, E. </author> <year> (1996). </year> <title> A neuron that learns to separate one independent component from linear mixtures. </title> <booktitle> In Proc. IEEE Int. Conf. on Neural Networks, </booktitle> <pages> pages 6267, </pages> <address> Washington, D.C. </address>
Reference-contexts: have been sphered: it holds kurt (w T x) = Ef (w T x) 4 g 3 [Ef (w T x) 2 g] 2 = Ef (w T x) 4 g 3kwk 4 (5) Also the constraint kwk = 1 must be taken into account, e.g. by a penalty term <ref> (Hyvrinen and Oja, 1996) </ref>. Then the tnal objective function is J (w) = Ef (w T x) 4 g 3kwk 4 + F (kwk 2 ) (6) where F is a penalty term due to the constraint. <p> The txed points w of the learning rule (7) are obtained by taking the expectations and equating the change in the weight to 0: Efx (w T x) 3 g 3kwk 2 w + f (kwk 2 )w = 0 (8) 2 Note that in <ref> (Hyvrinen and Oja, 1996) </ref>, the second term in J was also included in the penalty term. 4 The time index t has been dropped. A deterministic iteration could be formed from eq. (8) by a number of ways, e.g. by standard numerical algorithms for solving such equations.
Reference: <author> Jutten, C. and Herault, J. </author> <year> (1991). </year> <title> Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. Signal Processing, </title> <publisher> 24:110. </publisher>
Reference: <author> Moreau, E. and Macchi, O. </author> <year> (1993). </year> <title> New self-adaptive algorithms for source separation based on contrast functions. </title> <booktitle> In Proc. IEEE Signal Processing Workshop on Higher Order Statistics, </booktitle> <pages> pages 215219, </pages> <address> Lake Tahoe, USA. </address>
Reference: <author> Oja, E. and Karhunen, J. </author> <year> (1995). </year> <title> Signal separation by nonlinear hebbian learning. </title> <editor> In Palaniswami, M., Attikiouzel, Y., Marks, R., Fogel, D., and Fukuda, T., editors, </editor> <booktitle> Computational Intelligence a Dynamic System Perspective, </booktitle> <pages> pages 83 97. </pages> <publisher> IEEE Press, </publisher> <address> New York. </address>
Reference: <author> Shalvi, O. and Weinstein, E. </author> <year> (1993). </year> <title> Super-exponential methods for blind deconvolution. </title> <journal> IEEE Trans. on Information Theory, 39(2):504:519. </journal> <volume> 10 </volume>
References-found: 15

