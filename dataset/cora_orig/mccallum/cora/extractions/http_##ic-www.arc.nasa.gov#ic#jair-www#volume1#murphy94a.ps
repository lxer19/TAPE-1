URL: http://ic-www.arc.nasa.gov/ic/jair-www/volume1/murphy94a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/murphy94a.html
Root-URL: 
Email: pmurphy@ics.uci.edu  pazzani@ics.uci.edu  
Title: Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction  
Author: Patrick M. Murphy Michael J. Pazzani 
Address: Irvine, CA 92717  
Affiliation: Department of Information Computer Science University of California,  
Note: Journal of Artificial Intelligence Research 1 (1994) 257-275 Submitted 11/93; published 3/94  
Abstract: We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees.
Abstract-found: 1
Intro-found: 1
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. </author> <year> (1987). </year> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24, </volume> <pages> 377-380. </pages>
Reference-contexts: We have shown that for some concepts, the preference for simpler decision trees does not result in an increase in predictive accuracy on unseen test data, even when the simple trees are consistent with the training data. Like Schaffer, we do not dispute the theoretical results on Occam's razor <ref> (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987) </ref>, minimum description length (Quinlan & Rivest, 1989; Muggleton et al., 1992), or minimizing the number of leaves of a decision tree (Fayyad & Irani, 1990). <p> Rather, we point out that for a variety of reasons, the assumptions behind these theoretical results mean that the results do not apply to the experiments reported here. For example, <ref> (Blumer et al., 1987) </ref> indicates that if one finds an hypothesis in a sufficiently small hypothesis space (and simpler hypotheses are one example of a small hypothesis space) and this hypothesis is consistent with a sufficiently large sample of training data, one can be fairly confident that it will be fairly <p> This bias is appropriate for learning simple concepts. For more complex concepts, the opposite bias, preferring the more complex hypotheses, is unlikely to produce an accurate hypothesis <ref> (Blumer et al., 1987) </ref> and (Fayyad & Irani, 1990) due to the large number of consistent complex hypotheses.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks. </publisher>
Reference: <author> Fayyad, U., & Irani, K. </author> <year> (1990). </year> <title> What should be minimized in a decision tree?. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, AAAI-90. </booktitle>
Reference-contexts: Like Schaffer, we do not dispute the theoretical results on Occam's razor (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987), minimum description length (Quinlan & Rivest, 1989; Muggleton et al., 1992), or minimizing the number of leaves of a decision tree <ref> (Fayyad & Irani, 1990) </ref>. Rather, we point out that for a variety of reasons, the assumptions behind these theoretical results mean that the results do not apply to the experiments reported here. <p> However, it does not say that on average this hypothesis will be more accurate than other consistent hypotheses not in this small hypothesis space. The <ref> (Fayyad & Irani, 1990) </ref> paper explicitly states that the results on minimizing the number of leaves of decision trees are worst case results and should not be used to make absolute statements concerning improvements in performances. <p> This bias is appropriate for learning simple concepts. For more complex concepts, the opposite bias, preferring the more complex hypotheses, is unlikely to produce an accurate hypothesis (Blumer et al., 1987) and <ref> (Fayyad & Irani, 1990) </ref> due to the large number of consistent complex hypotheses.
Reference: <author> Fayyad, U., & Irani, K. </author> <year> (1992). </year> <title> The attribute selection problem in decision tree generation.. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI-92. </booktitle>
Reference: <author> Muggleton, S., Srinivasan, A., & Bain, M. </author> <year> (1992). </year> <title> Compression, significance and accuracy. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop. </booktitle>
Reference-contexts: This last result is somewhat surprising since one gets the impression from reading the machine learning literature <ref> (Muggleton, Srinivasan, & Bain, 1992) </ref> that the smaller hypothesis (i.e., the one that provides the most compression of the data (Rissanen, 1978)) is likely to be more accurate. We will explore this issue in further detail in Section 3.
Reference: <author> Murphy, P., & Aha, D. </author> <year> (1994). </year> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: We will call the set of decision trees that are consistent with the training data a decision forest. Our experiments were run on several artificial concepts for which we know the correct answer and two naturally occurring databases from real world tasks available from the UCI Machine Learning Repository <ref> (Murphy & Aha, 1994) </ref> in which the correct answer is not known. The goal of the experiments were to gain insight into how factors such as the size of a consistent decision tree are related to the error rate on classifying unseen test instances.
Reference: <author> Pazzani, M. </author> <year> (1990). </year> <title> Creating a memory of causal relationships: An integration of empirical and explanation-based learning methods. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: We believe that the only way to learn complex hypotheses reliably is to have some bias (e.g., prior domain knowledge) which favors particular complex hypotheses such as combinations of existing hypotheses learned inductively as in OCCAM <ref> (Pazzani, 1990) </ref>.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5 Programs for Machine Learning. </title> <address> San Mateo,CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Six features represent the functionality of a multiplexor and 2 features are irrelevant. The minimum sized tree has 7 nodes. This particular concept was chosen because it is difficult for a top-down inductive decision tree learner with limited look ahead to find a small hypothesis <ref> (Quinlan, 1993) </ref>. On each trial, we selected 20 examples randomly and tested on the remaining examples.
Reference: <author> Quinlan, J., & Rivest, R. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <month> 227-248. </month> <title> 274 Exploring the Decision Forest Rissanen, </title> <editor> J. </editor> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14, </volume> <pages> 465-471. </pages>
Reference: <author> Schaffer, C. </author> <year> (1992). </year> <title> Sparse data and the effect of overfitting avoidance in decision tree induction. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI-92. </booktitle>
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 (2), </volume> <pages> 153-178. </pages>
Reference: <author> Schaffer, C. </author> <year> (1994). </year> <title> A conservation law for generalization performance. </title> <type> Unpublished Manuscript. </type>
Reference-contexts: However, it is not clear whether this can be done without knowledge of the distribution of concepts one is likely to encounter <ref> (Schaffer, 1994) </ref>. We also note that our results may be due to the small size of the training sets relative to the size of the correct tree. We tried to rule out this possibility by using larger training sets (31 of the 32 possible examples) and by testing simpler concepts.
Reference: <author> Valiant, L. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 (11), </volume> <pages> 1134-1142. </pages>
Reference-contexts: In our previous experiments, we used a methodology that is typical in empirical evaluations of machine learning systems: the training data and the test data are disjoint. In contrast, most theoretical work on the PAC model <ref> (Valiant, 1984) </ref> assumes that the training and test data are generated from the same probability distribution over the examples. <p> We believe that the only way to learn complex hypotheses reliably is to have some bias (e.g., prior domain knowledge) which favors particular complex hypotheses such as combinations of existing hypotheses learned inductively as in OCCAM (Pazzani, 1990). Indeed, <ref> (Valiant, 1984) </ref> advocates a similar position: "If the class of learnable concepts is as severely limited as suggested by our results, then it would follow that the only way of teaching more complex concepts is to build them up from simpler ones." 271 Murphy & Pazzani Acknowledgements We thank Ross Quinlan,
Reference: <author> Wallace, C., & Patrick, J. </author> <year> (1993). </year> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11 (1), </volume> <pages> 7-22. 275 </pages>
References-found: 15

