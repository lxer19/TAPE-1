URL: http://www.eecs.umich.edu/techreports/cse/1998/CSE-TR-361-98.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse98.html
Root-URL: http://www.cs.umich.edu
Email: E-mail: sviji@eecs.umich.edu  
Title: Improving Performance of an L1 Cache With an Associated Buffer  
Author: Vijayalakshmi Srinivasan 
Date: February 12, 1998  
Address: 1301 Beal Avenue, Ann Arbor, MI 48109-2122,USA.  
Affiliation: Electrical Engineering and Computer Science, University of Michigan,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A.Agarwal, S.D.Pudar, </author> <title> Column-Associative Caches: A Technique for Reducing the Miss Rate of Direct Mapped Caches, </title> <booktitle> Proceedings of the 20th Int'l Symposium on Computer Architecture, </booktitle> <pages> May 1993 pp 179-190. </pages>
Reference-contexts: In a design with a direct mapped cache A, and a fully associative cache B (victim buffer), we see that ff and fi are swapped between caches A and B on every reference. Hence this scheme can potentially lead to an excessive number of swaps. Agarwal and Pudar <ref> [1] </ref> present a Column Associative Cache to reduce conflict misses by dynamically applying a second hashing function during conflicts to place some cache lines in a different set. This scheme relies on the effectiveness (orthogonality) of the two hash functions.
Reference: [2] <author> L.A. Belady, </author> <title> A Study of Replacement Algorithms for a Virtual Storage Computer, </title> <journal> IBM Systems Journal, </journal> <volume> Vol. 5, </volume> <year> 1966, </year> <month> pp.78-101. </month>
Reference-contexts: We use the pseudo-opt experiments to first understand the effectiveness of replacement decisions on cache performance and then use these results to help guide the search for good replacement algorithms for our NT-Victim cache design. Our pseudo-opt replacement algorithm is an adaptation of Belady's <ref> [2] </ref> MIN algorithm, which is an optimum replacement policy for a fully associative cache. On a miss, the block to be replaced is the one whose next reference is farthest in the future. This choice clearly minimizes the number of misses in a cache set.
Reference: [3] <author> James E. Bennett, Michael J. Flynn, </author> <note> Prediction Caches for Superscalar Processors To be published in Proceedings of the 30th Annual Int'l Symposium on Microar-chitecture November 1997. </note>
Reference-contexts: However, we improve this scheme by taking into account the temporality of a block when making swap decisions. For instruction caches, Selective Victim caching shows dramatic performance improvement compared to Victim caching; however, it shows no performance improvement for data caches. Bennett and Flynn <ref> [3] </ref> propose Prediction Caches that use a history of recent cache misses to predict future misses. Prediction caches combine the features of prefetching and Victim caching. This scheme, like Victim caches, helps alleviate hot spots in cache. However, none of the above four studies addresses the problem of cache pollution.
Reference: [4] <author> C-H.Chi, H. Dietz, </author> <booktitle> Improving Cache Performance by Selective Cache Bypass Proceedings of the 22nd Annual Hawaii Int'l Conference on System Science,Jan 1989. </booktitle>
Reference: [5] <author> M.Hill, </author> <title> A Case for Direct-Mapped Caches IEEE Computer, </title> <address> Vol.21, No.12, </address> <month> Dec </month> <year> 1988, </year> <pages> pp 25-40. </pages>
Reference-contexts: However, integer programs often have irregular access patterns that are more difficult for the compiler to optimize. Our work concentrates on performance improvement of L1 caches for integer benchmarks. To have low access times, the main L1 cache is usually designed to have low associativity <ref> [5] </ref>. A direct-mapped cache results in the lowest access time, but often suffers from high miss rate due to conflicts among memory references.
Reference: [6] <author> Teresa Johnson , Wen-mei W.Hwu, </author> <title> Run-time Adaptive Cache Hierarchy Management via Reference Analysis Proceedings of the 24th Int'l Symposium on Computer Architecture, </title> <note> June 1997 pp 315-326. </note>
Reference-contexts: This degrades the performance by not utilizing cache space that might be available in L1. Although we use a similar mechanism to tag blocks as temporal, we improve this scheme by allowing both temporal and non-temporal blocks to reside in cache A and in cache B. Johnson and Hwu <ref> [6] </ref> have investigated a technique for dynamic analysis of program data access behavior, which is then used to guide the placement of data within the cache hierarchy. In this scheme, infrequently accessed data bypass the cache when they conflict with much more frequently accessed data. <p> As our design focus is on performance improvement of direct-mapped caches, we conclude that the working set of this benchmark is too large to fit into direct-mapped caches of size less than 512K. This agrees with the conclusion in <ref> [6] </ref>. Johnson and Hwu also observe that many of the memory accesses in compress are to its hash tables, htab and codetab. Due to large hash table sizes and the fact that the hash table accesses have little temporal or spatial locality, there is very little reuse in L1 cache.
Reference: [7] <author> N.P.Jouppi, </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers, </title> <booktitle> Proceedings of the 17th Int'l Symposium on Computer Architecture, </booktitle> <pages> May 1990 pp 364-373. </pages>
Reference-contexts: Prior work shows that the performance of a direct mapped L1 cache can be improved by adding a small fully associative buffer in the same level, the primary examples being Victim cache, Assist cache, and NTS cache. Victim cache <ref> [7] </ref> reduces conflict misses of a direct mapped cache by adding a small fully associative victim buffer that stores blocks that are recently replaced in the main cache, i.e., recent "victims". <p> We present results in section 5, and conclusions and future work in section 6. 2 Related Work Optimizing small caches for numerical workloads and reducing conflict misses have received extensive examination. One approach proposed by Jouppi <ref> [7] </ref> is Victim Caching, where a small fully-associative "victim buffer" is introduced between the L1 direct-mapped (DM) cache and main memory. Blocks replaced from the L1 cache are stored in the victim buffer. References that are served by the victim buffer have a very low 3 miss penalty.
Reference: [8] <author> G.Kurpanek, K.Chan, J.Zheng, E.DeLano, W.Bryg, PA7200: </author> <title> A PA-RISC Processor With Integrated High Performance MP Bus Interface, </title> <booktitle> COMPCON Digest of Papers, </booktitle> <month> Feb </month> <year> 1994, </year> <pages> pp. 375-382. 26 </pages>
Reference-contexts: Victim cache [7] reduces conflict misses of a direct mapped cache by adding a small fully associative victim buffer that stores blocks that are recently replaced in the main cache, i.e., recent "victims". Assist cache <ref> [8] </ref> reduces block interference and cache pollution by loading blocks from memory into a small fully associative buffer. Blocks are promoted to the main cache only if they exhibit temporal locality. <p> Prediction caches combine the features of prefetching and Victim caching. This scheme, like Victim caches, helps alleviate hot spots in cache. However, none of the above four studies addresses the problem of cache pollution. Unlike the above methods, the HP-7200 Assist Cache <ref> [8] </ref> places a conventional L1 cache in parallel with a small fully associative cache, guaranteeing a one-cycle lookup in both units. Blocks from memory are first loaded into the assist cache (cache B) and 4 are promoted into main cache (cache A) only if they exhibit temporal locality. <p> In our current model there is no penalty for a hit in cache B. As in the design of Assist cache <ref> [8] </ref>, we place caches A and B in parallel, in the same level of the memory hierarchy. In order to understand the effect of swaps on the miss ratio we study the miss ratio without swaps for both Victim and NT-victim caches.
Reference: [9] <author> Yale N. Patt, Sanjay J Patel, Marius Evers, Daniel H. </author> <title> Friendly Jared Stark, One Billion Transistors, One Uniprocessor, One Chip IEEE Computer, </title> <address> Vol.30, No.9, </address> <month> Sept </month> <year> 1997, </year> <pages> pp 51-57. </pages>
Reference-contexts: 1 Introduction Over the last decade, processor cycle time has been decreasing much faster than main memory access time. In addition, aggressive superscalar machines target 16 instructions per clock cycle <ref> [9] </ref>. With these effects, given that about a third of a program's instruction mix are memory references, data cache performance becomes even more critical. However, data caches are not always used efficiently. In this study we focus on data cache performance optimization.
Reference: [10] <author> Jude A. Rivers, Edward S. Davidson, </author> <title> Reducing Conflicts in Direct-Mapped Caches with a Temporality-Based Design, </title> <booktitle> Proceedings of the 1996 International Conference on Parallel Processing, </booktitle> <volume> Vol I, </volume> <month> August </month> <year> 1996, </year> <pages> pp 151-162. </pages>
Reference-contexts: Assist cache [8] reduces block interference and cache pollution by loading blocks from memory into a small fully associative buffer. Blocks are promoted to the main cache only if they exhibit temporal locality. NTS cache <ref> [10] </ref> uses a small fully associative buffer in parallel with the main cache for storing and managing blocks containing non-temporal data. This reduces block conflicts and cache pollution and separates the reference stream 2 into temporal and non-temporal block references. <p> We improve this scheme by placing all incoming blocks in cache A and giving victims of replacement a longer lifetime in cache B, so they have a greater chance to become temporal. The NTS cache proposed by Rivers and Davidson <ref> [10] </ref> supplements the conventional direct-mapped cache (cache A) with a parallel fully associative cache (cache B). This scheme separates the reference stream into temporal and non-temporal block references. Blocks are treated as non-temporal until they become temporal. <p> Cache B is small compared to cache A because it is a fully associative structure; if it is large, the cost of the tag search may offset the gain in performance. The NT detection unit is a hardware bit-map structure similar to the one described in <ref> [10] </ref>. <p> If the access results in a miss in cache A, but a hit in cache B, it is simply fetched from cache B, and counted as a hit for the purpose of performance evaluation. We tag each block as either temporal or non-temporal following the ideas in <ref> [10] </ref>. <p> Such blocks are also referred to as NT-blocks. In this study, unlike <ref> [10] </ref>, we have assumed that NT/T information is not carried into the next level of the hierarchy. Every block begins each lifetime in the L1 cache as an NT-block; it becomes a T-block as soon as it exhibits temporality during this lifetime.
Reference: [11] <author> Jude A. Rivers, Edward S. Tam, Edward S. Davidson, </author> <title> On Effective Data Supply for Multi-Issue Processors, </title> <booktitle> Proceedings of the 1997 IEEE International Conference on Computer Design, </booktitle> <month> October </month> <year> 1997, </year> <pages> pp 519-528. </pages>
Reference: [12] <author> Alan J. Smith, </author> <title> Cache Memories Computing Surveys, </title> <month> Sept. </month> <year> 1982, </year> <pages> pp 473-530. </pages>
Reference: [13] <author> A.Srivastava, A.Eustace, </author> <title> ATOM: A System for Building Customized Program Analysis Tools, </title> <booktitle> Proceedings of ACM SIGPLAN Notices Conference on Programming Language Implementations, </booktitle> <month> June </month> <year> 1994, </year> <pages> pp 196-205. </pages>
Reference-contexts: This motivates us to design a cache which reduces the number of swaps compared to a Victim cache, without compromising the miss ratio. 4.3 Simulator We use trace-driven simulations to evaluate the performance of NT-victim cache. A Memory reference trace of each benchmark is collected using ATOM <ref> [13] </ref>. We simulate Victim, NT-victim, Assist, and NTS caches. As a base case, we also simulate a direct-mapped L1 cache, without any cache B. In the case of NT-victim caches we simulate all the three replacement policies, namely, NT-LRU, LRU and NT-LRU-1/2.
Reference: [14] <author> Dimitrios Stiliadis, Anujan Varma, </author> <title> Selective Victim Caching: A Method to Improve the Performance of Direct Mapped Caches, </title> <journal> IEEE Transactions On Computers, </journal> <volume> Vol 46, No. </volume> <pages> 5, </pages> <note> May 1997 pp 603-610. </note>
Reference-contexts: The latency of checking for a cache hit can increase depending on the number of links that must be followed to fetch the block. A small hash table lookup time is critical for good performance of this scheme. Selective Victim caching proposed by Stiliadis and Varma <ref> [14] </ref> places incoming blocks selectively in the main cache (cache A) or a small victim buffer (cache B) using a prediction scheme. This scheme swaps a block from cache B to cache A based on its history of use.
Reference: [15] <author> Harold S. Stone, </author> <title> High-Performance Computer Architecture, </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1993, </year> <pages> pp 74-75. </pages>
Reference-contexts: A replacement policy can give higher priority to T-blocks so as to retain them in favor of NT-blocks. * NT-block lifetime: We quote from <ref> [15] </ref>: "As the replacement algorithm chooses lines closer and closer to the most-recently used line, however, the risk becomes greater that the replacement algorithm will make a mistake and cast out a line that should be retained for a future hit." Therefore, in order to avoid replacing NT-blocks too soon, i.e.,
Reference: [16] <author> Edward S Tam, Jude A. Rivers, Edward S Davidson, </author> <title> Flexible Timing Simulation of Multiple-Cache Configurations, </title> <type> CSE Tech Report, </type> <institution> University of Michigan. </institution>
Reference-contexts: In this study we focus on miss ratio and the number of swaps; we plan to analyze latency effects in future work. Hence we have simulated our design using a timing simulator - mlcache <ref> [16] </ref>.
Reference: [17] <editor> O.Temam, N.Drach, </editor> <booktitle> Software Assistance for Data Caches Proceedings of 1st International Symposium on High Performance Computer Architecture, </booktitle> <month> Jan </month> <year> 1995, </year> <pages> pp 154-163. </pages>
Reference: [18] <author> Gary Tyson, Matthew Farrens, John Matthews, Andrew R. Pleszkun, </author> <title> Managing Data Caches Using Selective Cache Line Replacement Int'l Journal of Parallel Programming, </title> <journal> Vol. </journal> <volume> 25, No. 3, </volume> <pages> 1997 pp 213-242. 27 </pages>
Reference-contexts: However, data caches are not always used efficiently. In this study we focus on data cache performance optimization. The design of an on-chip first-level cache involves a fundamental tradeoff between miss ratio and access time. Tyson et al. <ref> [18] </ref> show that cache block placement and replacement strategy can be improved using characteristics of individual load instruction. In numeric programs there are several known compiler techniques for optimizing data cache performance. However, integer programs often have irregular access patterns that are more difficult for the compiler to optimize. <p> In this scheme, infrequently accessed data bypass the cache when they conflict with much more frequently accessed data. This reduces conflicts and alleviates cache pollution. All the above methods describe a modification of the cache design to enhance the performance of an L1 cache. Tyson et al <ref> [18] </ref> present a detailed characterization of data cache behavior for individual load instructions and describe a scheme to selectively allocate cache lines according to the characteristics of the load instructions. Their results suggest that data reference behavior plays a critical role in cache block placement and replacement decisions.
References-found: 18

