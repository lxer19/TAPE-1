URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/jair-ecoc.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: tgd@cs.orst.edu  eb004@isa.cc.uob.bh  
Title: Solving Multiclass Learning Problems via Error-Correcting Output Codes  
Author: Thomas G. Dietterich Ghulum Bakiri 
Address: 303 Dearborn Hall  Corvallis, OR 97331 USA  
Affiliation: Department of Computer Science,  Oregon State University  Department of Computer Science University of Bahrain Isa Town, Bahrain  
Note: Journal of Artificial Intelligence Research 2 (1995) 263-286 Submitted 8/94; published 1/95  
Abstract: Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt; 2 values (i.e., k "classes"). The definition is acquired by studying collections of training examples of the form hx i ; f(x i )i. Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that|like the other methods|the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.
Abstract-found: 1
Intro-found: 1
Reference: <author> Adaptive Solutions (1992). </author> <title> CNAPS back-propagation guide. </title> <type> Tech. rep. </type> <year> 801-20030-04, </year> <title> Adaptive Solutions, </title> <publisher> Inc., </publisher> <address> Beaverton, OR. </address>
Reference-contexts: For neural networks, we employed two implementations. In most domains, we used the extremely fast backpropagation implementation provided by the CNAPS neurocomputer <ref> (Adaptive Solutions, 1992) </ref>. This performs simple gradient descent with a fixed learning rate. The gradient is updated after presenting each training example; no momentum term was employed.
Reference: <author> Bakiri, G. </author> <year> (1991). </year> <title> Converting English text to speech: A machine learning approach. </title> <type> Tech. rep. </type> <institution> 91-30-2, Department of Computer Science, Oregon State University, Corvallis, </institution> <address> OR. </address>
Reference: <author> Barnard, E., & Cole, R. A. </author> <year> (1989). </year> <title> A neural-net training program based on conjugate-gradient optimization. </title> <type> Tech. rep. CSE 89-014, </type> <institution> Oregon Graduate Institute, Beaverton, </institution> <address> OR. </address>
Reference-contexts: Weight update arithmetic does not round, but instead performs jamming (i.e., forcing the lowest order bit to 1 when low order bits are lost due to shifting or multiplication). On the speech recognition, letter recognition, and vowel data sets, we employed the opt system distributed by Oregon Graduate Institute <ref> (Barnard & Cole, 1989) </ref>. This implements the conjugate gradient algorithm and updates the gradient after each complete pass through the training examples (known as per-epoch updating). No learning rate is required for this approach.
Reference: <author> Bose, R. C., & Ray-Chaudhuri, D. K. </author> <year> (1960). </year> <title> On a class of error-correcting binary group codes. </title> <journal> Information and Control, </journal> <volume> 3, </volume> <pages> 68-79. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: For cases in which f takes only the values f0; 1g|binary functions|there are many algorithms available. For example, the decision-tree methods, such as C4.5 (Quinlan, 1993) and CART <ref> (Breiman, Friedman, Olshen, & Stone, 1984) </ref> can construct trees whose leaves are labeled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm (Rosenblatt, 1958) and the error backpropagation (BP) algorithm (Rumelhart, Hinton, & Williams, 1986), are best suited to learning binary functions.
Reference: <author> Bridle, J. S. </author> <year> (1990). </year> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. </pages> <address> 211-217 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Many researchers have employed other error measures, particularly cross-entropy (Hinton, 1989) and classification figure-of-merit (CFM, Hamp-shire II & Waibel, 1990). Many researchers also advocate using a softmax normalizing layer at the outputs of the network <ref> (Bridle, 1990) </ref>. While each of these configurations has good theoretical support, Richard and Lippmann (1991) report that squared error works just as well as these other measures in producing accurate posterior probability estimates. Furthermore, cross-entropy and CFM tend to overfit more easily than squared error (Lippmann, personal communication; Weigend, 1993).
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 17-24 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R. O., Machanik, J. W., & Singleton, R. C. </author> <year> (1963). </year> <title> Function modeling experiments. </title> <type> Tech. rep. 3605, </type> <institution> Stanford Research Institute. </institution>
Reference-contexts: This suggests that there might be some advantage to employing error-correcting codes as a distributed representation. Indeed, the idea of employing error-correcting, distributed representations can be traced to early research in machine learning <ref> (Duda, Machanik, & Singleton, 1963) </ref>. 264 Error-Correcting Output Codes Table 1: A distributed code for the digit recognition task.
Reference: <author> Freund, Y. </author> <year> (1992). </year> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pp. 391-398. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference: <author> Hampshire II, J. B., & Waibel, A. H. </author> <year> (1990). </year> <title> A novel objective function for improved phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1 (2), </volume> <pages> 216-228. </pages>
Reference: <author> Hastie, T., Tibshirani, R., & Buja, A. </author> <title> (In Press). Flexible discriminant analysis by optimal scoring. </title> <journal> Journal of the American Statistical Association. </journal>
Reference: <author> Hinton, G. </author> <year> (1989). </year> <title> Connectionist learning procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 185-234. </pages>
Reference-contexts: In digit recognition (e.g., c fl1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Dietterich & Bakiri LeCun, Boser, Denker, Henderson, Howard, Hubbard, & Jackel, 1989), the function maps each hand-printed digit to one of k = 10 classes. Phoneme recognition systems <ref> (e.g., Waibel, Hanazawa, Hinton, Shikano, & Lang, 1989) </ref> typically classify a speech segment into one of 50 to 60 phonemes. Decision-tree algorithms can be easily generalized to handle these "multiclass" learning tasks. <p> No learning rate is required for this approach. Both the CNAPS and opt attempt to minimize the squared error between the computed and desired outputs of the network. Many researchers have employed other error measures, particularly cross-entropy <ref> (Hinton, 1989) </ref> and classification figure-of-merit (CFM, Hamp-shire II & Waibel, 1990). Many researchers also advocate using a softmax normalizing layer at the outputs of the network (Bridle, 1990).
Reference: <author> Hocquenghem, A. </author> <year> (1959). </year> <title> Codes corecteurs d'erreurs. </title> <journal> Chiffres, </journal> <volume> 2, </volume> <pages> 147-156. </pages> <note> 284 Error-Correcting Output Codes Kong, </note> <author> E. B., & Dietterich, T. G. </author> <year> (1995). </year> <title> Why error-correcting output coding works with decision trees. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, Oregon State University, Corvallis, </institution> <address> OR. </address>
Reference: <author> Lang, K. J., Hinton, G. E., & Waibel, A. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 (1), </volume> <pages> 23-43. </pages>
Reference: <author> LeCun, Y., Boser, B., Denker, J. S., Henderson, B., Howard, R. E., Hubbard, W., & Jackel, L. D. </author> <year> (1989). </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 (4), </volume> <pages> 541-551. </pages>
Reference-contexts: For one-per-class neural networks, many researchers have found that the difference in activity between the class with the highest activity and the class with the second-highest activity is a good measure of confidence <ref> (e.g., LeCun et al., 1989) </ref>. If this difference is large, then the chosen class is clearly much better than the others. If the difference is small, then the chosen class is nearly tied with another class. This same measure can be applied to the class probability estimates produced by C4.5.
Reference: <author> Murphy, P., & Aha, D. </author> <year> (1994). </year> <title> UCI repository of machine learning databases [machine-readable data repository]. </title> <type> Tech. rep., </type> <institution> University of California, Irvine. </institution>
Reference-contexts: The glass, vowel, soybean, audi-ologyS, ISOLET, letter, and NETtalk data sets are available from the Irvine Repository of machine learning databases <ref> (Murphy & Aha, 1994) </ref>. 1 The POS (part of speech) data set was provided by C. Cardie (personal communication); an earlier version of the data set was described by Cardie (1993).
Reference: <author> Natarajan, B. K. </author> <year> (1991). </year> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning Machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: To assign a new case, x, to one of these classes, each of the f i is evaluated on x, and x is assigned the class j of the function f j that returns the highest activation <ref> (Nilsson, 1965) </ref>. We will call this the one-per-class approach, since one binary function is learned for each class. An alternative approach explored by some researchers is to employ a distributed output code. This approach was pioneered by Sejnowski and Rosenberg (1987) in their widely-known NETtalk system.
Reference: <author> Perrone, M. P., & Cooper, L. N. </author> <year> (1993). </year> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In Mammone, R. J. (Ed.), </editor> <title> Neural networks for speech and image processing. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Peterson, W. W., & Weldon, Jr., E. J. </author> <year> (1972). </year> <title> Error-Correcting Codes. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference-contexts: Error-correcting codes only succeed if the errors made in the individual bit positions are relatively uncorrelated, so that the number of simultaneous errors in many bit positions is small. If there are many simultaneous errors, the error-correcting code will not be able to correct them <ref> (Peterson & Weldon, 1972) </ref>. The errors in columns i and j will also be highly correlated if the bits in those columns are complementary. This is because algorithms such as C4.5 and backpropagation treat a class and its complement symmetrically.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: For cases in which f takes only the values f0; 1g|binary functions|there are many algorithms available. For example, the decision-tree methods, such as C4.5 <ref> (Quinlan, 1993) </ref> and CART (Breiman, Friedman, Olshen, & Stone, 1984) can construct trees whose leaves are labeled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm (Rosenblatt, 1958) and the error backpropagation (BP) algorithm (Rumelhart, Hinton, & Williams, 1986), are best suited to learning binary functions.
Reference: <author> Richard, M. D., & Lippmann, R. P. </author> <year> (1991). </year> <title> Neural network classifiers estimate bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> <volume> 3 (4), </volume> <pages> 461-483. </pages>
Reference: <author> Rosenblatt, F. </author> <year> (1958). </year> <title> The perceptron: a probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65 (6), </volume> <pages> 386-408. </pages>
Reference-contexts: For example, the decision-tree methods, such as C4.5 (Quinlan, 1993) and CART (Breiman, Friedman, Olshen, & Stone, 1984) can construct trees whose leaves are labeled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm <ref> (Rosenblatt, 1958) </ref> and the error backpropagation (BP) algorithm (Rumelhart, Hinton, & Williams, 1986), are best suited to learning binary functions. Theoretical studies of learning have focused almost entirely on learning binary functions (Valiant, 1984; Natarajan, 1991).
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. In Parallel Distributed Processing Explorations in the Microstructure of Cognition, </title> <journal> chap. </journal> <volume> 8, </volume> <pages> pp. 318-362. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: For example, the decision-tree methods, such as C4.5 (Quinlan, 1993) and CART (Breiman, Friedman, Olshen, & Stone, 1984) can construct trees whose leaves are labeled with binary values. Most artificial neural network algorithms, such as the perceptron algorithm (Rosenblatt, 1958) and the error backpropagation (BP) algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>, are best suited to learning binary functions. Theoretical studies of learning have focused almost entirely on learning binary functions (Valiant, 1984; Natarajan, 1991).
Reference: <author> Schapire, R. E. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5 (2), </volume> <pages> 197-227. </pages>
Reference: <author> Sejnowski, T. J., & Rosenberg, C. R. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce english text. </title> <journal> Journal of Complex Systems, </journal> <volume> 1 (1), </volume> <pages> 145-168. </pages>
Reference: <author> Selman, B., Levesque, H., & Mitchell, D. </author> <year> (1992). </year> <title> A new method for solving hard satisfiability problems. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> pp. 440-446. </pages> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: We formulate this as a propositional satisfiability problem and apply the GSAT algorithm <ref> (Selman, Levesque, & Mitchell, 1992) </ref> to attempt a solution. A solution is required to include exactly L columns (the desired length of the code) while ensuring that the Hamming distance between every two columns is between d and L d, for some chosen value of d.
Reference: <author> Snedecor, G. W., & Cochran, W. G. </author> <year> (1989). </year> <title> Statistical Methods. </title> <institution> Iowa State University Press, Ames, IA. </institution> <note> Eighth Edition. </note>
Reference: <author> Valiant, L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Commun. ACM, </journal> <volume> 27 (11), </volume> <pages> 1134-1142. </pages> <note> 285 Dietterich & Bakiri Waibel, </note> <author> A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. </author> <year> (1989). </year> <title> Phoneme recognition using time-delay networks. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37 (3), </volume> <pages> 328-339. </pages>
Reference: <author> Weigend, A. </author> <year> (1993). </year> <title> Measuring the effective number of dimensions during backpropagation training. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pp. 335-342. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Wilkinson, R. A., Geist, J., Janet, S., et al. </author> <year> (1992). </year> <title> The first census optical character recognition systems conference. </title> <type> Tech. rep. NISTIR 4912, </type> <institution> National Institute of Standards and Technology. </institution> <month> 286 </month>
Reference-contexts: However, if it is uncertain, then the envelope should be "rejected", and sent to a human being who can attempt to read the postal code and process the envelope <ref> (Wilkinson, Geist, Janet, et al., 1992) </ref>. One way to assess the quality of the class probability estimates of a classifier is to compute a "rejection curve". When the learning algorithm classifies a new case, we require it to also output a "confidence" level.
References-found: 31

