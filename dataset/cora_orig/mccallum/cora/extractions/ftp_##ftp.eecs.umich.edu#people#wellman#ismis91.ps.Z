URL: ftp://ftp.eecs.umich.edu/people/wellman/ismis91.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/wellman/Publications.html
Root-URL: http://www.cs.umich.edu
Email: doyle@lcs.mit.edu  shoham@cs.stanford.edu  wellman@wrdc.af.mil  
Title: A Logic of Relative Desire (Preliminary Report)  
Author: Jon Doyle Yoav Shoham Michael P. Wellman 
Address: 545 Technology Square Cambridge, MA 02139  Stanford, CA 94305  WL/AAA-1 Wright-Patterson AFB, OH 45433  
Affiliation: MIT Laboratory for Computer Science  Department of Computer Science Stanford University  Wright Laboratory AI Office  
Note: Reprinted from Proceedings of the Sixth International Symposium on Methodologies for In--telligent Systems (ISMIS'91), October, 1991, Z. W. Ras and M. Zemankova, editors, Berlin: Springer-Verlag, pp. 16-31.  
Abstract: Although many have proposed formal characterizations of belief structures as bases for rational action, the problem of characterizing rational desires has attracted little attention. AI relies heavily on goal conditions interpreted (apparently) as absolute expressions of desirability, but these cannot express varying degrees of goal satisfaction or preferences among alternative goals. Our previous work provided a relative interpretation of goals as qualitative statements about preferability, all else equal. We extend that treatment to the comparison of arbitrary propositions, and develop a propositional logic of relative desire suitable for formalizing properties of planning and problem-solving methods. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Doyle. </author> <title> A model for deliberation, action, and introspection. </title> <type> AI-TR 581, </type> <institution> Mas-sachusetts Institute of Technology, Artificial Intelligence Laboratory, </institution> <address> Cambridge, MA, </address> <year> 1980. </year>
Reference-contexts: Each goal represents a partition of possible states into those satisfying and those not satisfying the goal. Goals serve a dual role in most planning 16 systems, capturing aspects of intentions as well as desires <ref> [1] </ref>. Besides expressing the desirability of a state, adopting a goal typically represents some commitment to pursuing that state. In perhaps the simplest interpretation of goals as desires, the states satisfying the goal are considered desirable in an absolute sense.
Reference: [2] <author> R. C. Jeffrey. </author> <title> The Logic of Decision. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <note> second edition, </note> <year> 1983. </year>
Reference-contexts: 1 Introduction Question your desires.|William Shakespeare Standard theories of rational action take decisions of the agent to depend on beliefs about the relative desirability of the results of its available actions <ref> [2, 3] </ref>. The predominant approach to planning in artificial intelligence represents desires by conditions on the state of the world called goals. Each goal represents a partition of possible states into those satisfying and those not satisfying the goal. <p> This highlights the importance of developing more refined languages for specifying the objectives of planning agents. Ordinary desires can depend on probability judgments as well as preferences. Unlike other approaches to reasoning about desirability (e.g., Jeffrey's <ref> [2] </ref>), our inference rules do not depend on probability distributions. We could strengthen these rules to take advantage of probabilistic information when available, but some situations call for an ability to reason about desires separate from beliefs. Further work will focus on strengthening and expanding the set of inference rules.
Reference: [3] <author> R. L. Keeney and H. Raiffa. </author> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: 1 Introduction Question your desires.|William Shakespeare Standard theories of rational action take decisions of the agent to depend on beliefs about the relative desirability of the results of its available actions <ref> [2, 3] </ref>. The predominant approach to planning in artificial intelligence represents desires by conditions on the state of the world called goals. Each goal represents a partition of possible states into those satisfying and those not satisfying the goal. <p> Much work in decision theory concerns conditions under which one can represent ~ by order-preserving, real-valued utility functions, and with identifying regularities in preferences that justify utility functions with convenient structural properties <ref> [3] </ref>. Although we expect that utility theory will have much to offer for a calculus of desires for reasoning systems, our logic relies only on the ordinal preference relation, not on any numerical representations.
Reference: [4] <author> J. Marks. </author> <title> Introduction: On the need for theory of desire. </title> <editor> In J. Marks, editor, </editor> <title> The Ways of Desire. </title> <publisher> Precedent Publishing, </publisher> <address> Chicago, </address> <year> 1986. </year>
Reference-contexts: The resulting logic provides a general framework for representing and reasoning about desires. While such frameworks are common for beliefs, formal theories of desires have heretofore been conspicuously absent in artificial intelligence as well as philosophy <ref> [4] </ref>. We begin by introducing the basic notion of preference over models, which serves as the fundamental concept underlying our definition of relative desire over logical sentences. We follow by developing a collection of inference rules by which relative desires among some sentences entail desires over related sentences.
Reference: [5] <author> Y. Shoham. </author> <title> Agent-oriented programming. </title> <type> Technical Report STAN-CS-90-1335, </type> <institution> Stan-ford University Computer Science Department, </institution> <year> 1990. </year>
Reference-contexts: The formal treatment of desire also plays an important role in the framework of agent-oriented programming (AOP), proposed in <ref> [5] </ref>. AOP|a specialization of object-oriented programming|views agents as modifying their mental states as a result of informing one another, requesting information, and performing other kinds of communicative acts. In current AOP setups, the mental state of an agent consists of "motivation-free" components: beliefs, commitments, choices, and capabilities.
Reference: [6] <author> M. P. Wellman. </author> <title> Formulation of Tradeoffs in Planning Under Uncertainty. </title> <publisher> Pitman and Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: We expect that some of these re strictions may be alleviated by applying less ambiguous model modification rules, perhaps based on minimality criteria from the theory of conditionals and belief revision. The logic presented here constitutes part of a comprehensive decision-theoretic account 30 of planning (see also <ref> [6] </ref>), and a more thorough treatment of the issue of goals and utilities is in preparation [8]. The formal treatment of desire also plays an important role in the framework of agent-oriented programming (AOP), proposed in [5].
Reference: [7] <author> M. P. Wellman and J. Doyle. </author> <title> Preferential semantics for goals. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: For example, we might hold that achieving conditions p and q would be more desirable than achieving neither, but that if we can achieve only one we would prefer to achieve p. Decision-theoretic preference orders express exactly these sorts of comparisons, and in related work <ref> [7] </ref>, we provide a decision-theoretic semantics for goals in terms of preference orders and multiattribute outcome spaces. In our semantics, we relativize the notion of goal by restricting preference comparisons to fixed contexts. <p> More generally, we employ comparative similarity orders like those used in theories of counterfactuals and belief revision. Elsewhere, we relate the notion of relative desire to formalizations of conditional and deontic logics. 4 Propositional desires In our semantics for goals viewed as desires <ref> [7] </ref>, we represent states as vectors of attributes. We designate particular conditions as binary attributes, and define goalhood as preference for those conditions, holding all other attributes constant. <p> If we have f:p; q; r 1 ; :r 2 g fp; :q; r 1 ; r 2 g in addition, then p ^ r q ^ r does not hold. Restricted relative desire captures part of the notion of framing defined in <ref> [7] </ref>. Rather than beginning by supposing a set of atomic sentences and their models, our treatment there follows decision theory by supposing a set of possible outcomes and then representing these outcomes as vectors of attributes by means of a one-to-one framing function : ! i=1 A i . <p> The definition of desires formalizes the intuition that goals are propositions that are preferred to their opposites, other things being equal. The logic extends our previous work <ref> [7] </ref> by considering relative desire between arbitrary conditions expressed in a propositional language, and we provided a collection of inference rules that support reasoning about relative desire.
Reference: [8] <author> M. P. Wellman, J. Doyle, and T. Dean. </author> <title> Goals, preferences, and utilities: A reconciliation. </title> <note> in preparation, 1991. 31 </note>
Reference-contexts: The proofs of these results exploit such restrictions to ensure the uniqueness of modifications to a given model. We impose similar restrictions in application of the inference rules sanctioned by Theorem 11 and other results below. In <ref> [8] </ref> we present an alternative formalization which requires that modifications of models minimize the changes made in the initial model. In the simplest case, this means using a model rather than its modifications when the model already makes the sentence true. <p> The logic presented here constitutes part of a comprehensive decision-theoretic account 30 of planning (see also [6]), and a more thorough treatment of the issue of goals and utilities is in preparation <ref> [8] </ref>. The formal treatment of desire also plays an important role in the framework of agent-oriented programming (AOP), proposed in [5]. AOP|a specialization of object-oriented programming|views agents as modifying their mental states as a result of informing one another, requesting information, and performing other kinds of communicative acts.
References-found: 8

