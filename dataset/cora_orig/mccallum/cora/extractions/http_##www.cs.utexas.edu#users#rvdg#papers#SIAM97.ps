URL: http://www.cs.utexas.edu/users/rvdg/papers/SIAM97.ps
Refering-URL: http://www.cs.utexas.edu/users/plapack/new/pubs.html
Root-URL: 
Title: PLAPACK: Parallel Linear Algebra Package  
Author: Philip Alpatov Greg Baker Carter Edwards John Gunnels Greg Morrow James Overfelt Robert van de Geijn yz Yuan-Jye J. Wu 
Abstract: The PLAPACK project represents an effort to provide an infrastructure for implementing application friendly high performance linear algebra algorithms. The package uses a more application-centric data distribution, which we call Physically Based Matrix Distribution, as well as an object based (MPI-like) style of programming. It is this style of programming that allows for highly compact codes, written in C but usable from FORTRAN, that more closely reflect the underlying blocked algorithms. We show that this can be attained without sacrificing high performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. Agarwal, F. Gustavson, and M. Zubair, </author> <title> A high-performance matrix multiplication algorithm on a distributed memory parallel computer using overlapped communication, </title> <institution> IBM J. of Research and Development, </institution> <month> 38(6) </month> <year> (1994). </year>
Reference-contexts: be implemented by partitioning A = A 0 A 1 and B = B @ B 1 1 C and noticing that C = AB = A 0 B 0 + A 1 B 1 + Thus the implementation of this operation can proceed as a sequence of rank-k updates <ref> [1, 5, 10] </ref>. Let us concentrate of one update: C = C + A k B k . Partitioning these matrices as they were when we discussed distribution of matrices yields C = B B @ C 1;0 C 1;1 C 1;N1 . . .
Reference: [2] <author> S. Balay, W. Gropp, L. Curfman McInnes, and B. Smith, </author> <title> PETSc 2.0 users manual, </title> <type> Tech. Report ANL-95/11, </type> <institution> Argonne National Laboratory, </institution> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: To achieve this, we have adopted an "object based" approach to programming. This object based approach has already been popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University [3], the PETSc library at Argonne National Laboratory <ref> [2] </ref>, and the Message-Passing Interface [8].
Reference: [3] <author> P. Bangalore, A. Skjellum, C. Baldwin, and S. G. Smith, </author> <title> Dense and iterative concurrent linear algebra in the multicomputer toolbox, </title> <booktitle> in Proceedings of the Scalable Parallel Libraries Conference (SPLC '93) (1993), </booktitle> <pages> pp. 132-141. </pages>
Reference-contexts: To achieve this, we have adopted an "object based" approach to programming. This object based approach has already been popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University <ref> [3] </ref>, the PETSc library at Argonne National Laboratory [2], and the Message-Passing Interface [8].
Reference: [4] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> Scalapack: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> in Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <publisher> IEEE Comput. Soc. Press (1992), </publisher> <pages> pp. 120-127. </pages>
Reference-contexts: However, the surprising discovery has been that this approach greatly simplifies the implementation of the infrastructure, allowing much more generality (in future extensions of the infrastructure) while reducing the amount of code required when compared to previous generation parallel dense linear algebra libraries <ref> [4] </ref>. In this paper, we primarily concentrate on giving the reader a flavor of what it is like to code using the PLAPACK infrastructure. 2 Natural Description of Linear Algebra Algorithms Let us consider the simple example of implementation of the Cholesky factorization.
Reference: [5] <author> Almadena Chtchelkanova, John Gunnels, Greg Morrow, James Overfelt, and Robert A. van de Geijn, </author> <title> Block Parallel implementation of BLAS: General techniques for level 3 BLAS, in Concurrency: </title> <journal> Practice and Experience, </journal> <note> to appear. </note>
Reference-contexts: be implemented by partitioning A = A 0 A 1 and B = B @ B 1 1 C and noticing that C = AB = A 0 B 0 + A 1 B 1 + Thus the implementation of this operation can proceed as a sequence of rank-k updates <ref> [1, 5, 10] </ref>. Let us concentrate of one update: C = C + A k B k . Partitioning these matrices as they were when we discussed distribution of matrices yields C = B B @ C 1;0 C 1;1 C 1;N1 . . .
Reference: [6] <author> J. J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff, </author> <title> A set of level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM TOMS, </journal> <volume> 16(1) (1990), </volume> <pages> pp. 1-17. </pages>
Reference: [7] <author> C. Edwards, P. Geng, A. Patra, and R. van de Geijn, </author> <title> Parallel matrix distributions: have we been doing it all wrong?, </title> <type> Tech. Report TR-95-40, </type> <institution> Dept of Computer Sciences, The University of Texas at Austin, </institution> <year> 1995. </year>
Reference-contexts: This sets up the next iteration, which is also the next level of recursion in our original algorithm. 3 Physically Based Matrix Distribution We postulate in <ref> [7] </ref> that one should never start by considering how to decompose the matrix. Rather, one should start by considering how to decompose the physical problem to be solved. <p> For details, see <ref> [7, 9] </ref>. We need to clearly state that in the initial implementation of PLAPACK, we only implement this special case. Generalizations are planned. 4 4 Linear Algebra Objects In Section 2, we introduced the concept of encoding all information about a matrix in an object.
Reference: [8] <author> M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and J. Dongarra, </author> <title> MPI: The Complete Reference, </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: To achieve this, we have adopted an "object based" approach to programming. This object based approach has already been popularized for high performance parallel computing by libraries like the Toolbox being developed at Mississippi State University [3], the PETSc library at Argonne National Laboratory [2], and the Message-Passing Interface <ref> [8] </ref>.
Reference: [9] <author> Robert van de Geijn, </author> <title> Using PLAPACK: Parallel Linear Algebra Package, </title> <publisher> The MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: For details, see <ref> [7, 9] </ref>. We need to clearly state that in the initial implementation of PLAPACK, we only implement this special case. Generalizations are planned. 4 4 Linear Algebra Objects In Section 2, we introduced the concept of encoding all information about a matrix in an object. <p> For more information regarding the PLAPACK project, check our web page http://www.cs.utexas.edu/users/plapack/. In particular, further performance results are available at that site. Acknowledgements Portions of this paper were taken, with permission, from Using PLAPACK: Parallel Linear Algebra Package by R. van de Geijn, published by The MIT Press <ref> [9] </ref>.
Reference: [10] <author> R. van de Geijn and J. Watts, SUMMA: </author> <title> Scalable universal matrix multiplication algorithm, in Concurrency: </title> <journal> Practice and Experience, </journal> <note> to appear. </note>
Reference-contexts: be implemented by partitioning A = A 0 A 1 and B = B @ B 1 1 C and noticing that C = AB = A 0 B 0 + A 1 B 1 + Thus the implementation of this operation can proceed as a sequence of rank-k updates <ref> [1, 5, 10] </ref>. Let us concentrate of one update: C = C + A k B k . Partitioning these matrices as they were when we discussed distribution of matrices yields C = B B @ C 1;0 C 1;1 C 1;N1 . . .
Reference: [11] <author> Y.-J. J. Wu, P. A. Alpatov, C. Bischof, and R. A. van de Geijn, </author> <title> A parallel implementation of symmetric band reduction using PLAPACK, PRISM Working Note 35, </title> <booktitle> in Proceedings of Scalable Parallel Library Conference, </booktitle> <institution> Mississippi State University, </institution> <year> 1996. </year>
Reference-contexts: Naturally, we are only trying to give a flavor of how such algorithms are implemented. For an example of a much more complex algorithm, the parallel implementation of reduction to banded form, see <ref> [11] </ref>. 6 int pgemm_ab_panpan ( int nb_alg, PLA_Obj alpha, PLA_Obj a, PLA_Obj b, PLA_Obj beta, PLA_Obj c ) 5 PLA_Obj acur = NULL, a1 = NULL, a1dup = NULL, a1dup_cur = NULL, bcur = NULL, b1 = NULL, b1dup = NULL, b1dup_cur = NULL, one = NULL; MPI_Datatype datatype; 10 int
References-found: 11

