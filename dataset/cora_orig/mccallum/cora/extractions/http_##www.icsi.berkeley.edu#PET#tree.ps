URL: http://www.icsi.berkeley.edu/PET/tree.ps
Refering-URL: http://www.icsi.berkeley.edu/PET/nsf-index.html
Root-URL: http://www.icsi.berkeley.edu
Title: Analysis of Random Processes via Or-And Tree Evaluation  
Author: Michael G. Luby Michael Mitzenmacher M. Amin Shokrollahi 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Alon, J. Edmonds, M. Luby, </author> <title> "Linear Time Erasure Codes With Nearly Optimal Recovery", </title> <booktitle> Proc. of the 36 th Annual Symp. on Foundations of Computer Science, </booktitle> <year> 1995, </year> <pages> pp. 512-519. </pages>
Reference: [2] <author> N. Alon, M. Luby, </author> <title> "A Linear Time Erasure-Resilient Code With Nearly Optimal Recovery", </title> <journal> IEEE Transactions on Information Theory (special issue devoted to coding theory), </journal> <volume> Vol. 42, No. 6, </volume> <month> November </month> <year> 1996, </year> <pages> pp. 1732-1736. </pages>
Reference: [3] <author> R. Boppana, </author> <title> "Amplification of probabilistic Boolean formulas", </title> <booktitle> Advances in Computer Research, </booktitle> <volume> Vol. 5: </volume> <booktitle> Randomness and Computation, </booktitle> <publisher> JAI Press, </publisher> <address> Greenwich, CI, </address> <year> 1989, </year> <pages> pp. 27-45. </pages>
Reference-contexts: fact that a (2; 2)-regular OR-AND tree has a critical value of = (3 5)=2, and that if y `1 = + * then y ` &gt; + c* for a constant c &gt; 1. (The analogous rate of divergence is true if y `1 = *.) In further work, <ref> [3] </ref> and [5] provide beautiful proofs that the construction size of [13] is optimal, this time using amplification analysis to prove a lower bound. Our work has a similar spirit to the amplification work, but it differs in several ways.
Reference: [4] <author> A. Broder, A. Frieze, E. Upfal, </author> <title> `On the Satisfiability and Maximum Satis-fiability of Random 3-CNF Formulas", </title> <booktitle> Proc. of the 4 th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <year> 1993, </year> <pages> pp. 322-330. </pages>
Reference-contexts: unifying, intuitive, and powerful framework for carrying out the analysis of several previously studied random processes of interest, including the random loss-resilient codes introduced in [9], the greedy algorithm for matchings in random graphs studied in [7], and the threshold for solving random k-SAT formula using the pure literal rule <ref> [4] </ref>. In addition, generalization of these problems not previously analyzed can now be analyzed in a straightforward manner. For example, we can analyze generalizations of the processes considered in [9]. <p> For a uniformly chosen k-SAT formula, (x) = exp ((x 1)) and (x) = x k1 . By using Maple, one can verify that Condition 12 is satisfied for all y when k = 3 at the threshold value = 1:63. This result has been found previously by <ref> [4] </ref> and [11] using a different approach.
Reference: [5] <author> M. Dubiner, U. Zwick, </author> <title> "Amplification by Read-Once Formulas", </title> <journal> Siam J. on Computing, </journal> <volume> Vol. 26, No. 1, </volume> <month> Feb. </month> <year> 1997, </year> <pages> pp. 15-38. </pages>
Reference-contexts: 1 Introduction We introduce a new set of probabilistic analysis tools related to the amplification method introduced by [12] and further developed and used in [13], <ref> [5] </ref>. <p> Our analysis is related to the study of amplification, initiated by Moore and Shannon [12], and continued in several works [13], <ref> [5] </ref>. Consider the probability the root of the tree evaluates to 0 when the value of each leaf is independently chosen to be 0 with probability p. Let us denote this probability as y ` . <p> a (2; 2)-regular OR-AND tree has a critical value of = (3 5)=2, and that if y `1 = + * then y ` &gt; + c* for a constant c &gt; 1. (The analogous rate of divergence is true if y `1 = *.) In further work, [3] and <ref> [5] </ref> provide beautiful proofs that the construction size of [13] is optimal, this time using amplification analysis to prove a lower bound. Our work has a similar spirit to the amplification work, but it differs in several ways. <p> i children with probability ff i independent of any other node, and similarly each "AND" node choose to have j children with probability fi j independent of any other node. (Some previous research introduced a variant of this form and used it in a limited way in their construction, e.g., <ref> [5] </ref>.) A further generalization is to allow two positive values a and b such that each "OR" node is independently shortcircuited to produce the value 1 with probability a, and each "AND" node is independently shortcircuited to produce the value 0 with probability b.
Reference: [6] <author> A. Frieze, S. Suen, </author> <title> "Analysis of Two Simple Heuristics on a Random Instance of k-SAT", </title> <journal> J. of Algorithms, </journal> <volume> Vol. 20, </volume> <year> 1996, </year> <pages> pp. 312-355. </pages>
Reference-contexts: of the pure literal rule has been studied previously with respect to randomly chosen k-SAT formula (<ref> [6] </ref>, [11]). Here, we show how the tree analysis gives a direct explanation of the behavior of the pure literal rule for randomly chosen k-SAT formula with respect to the same distributions considered in [6] and [11]. With this new analysis, it is also straightforward to analyze distributions that were not previously considered and which would be much harder to analyze using previous techniques applied to this problem.
Reference: [7] <author> R. Karp, M. Sipser, </author> <title> "Maximum Matchings in Sparse Random Graphs", </title> <booktitle> FOCS, </booktitle> <year> 1981, </year> <pages> pp. 364-375. </pages>
Reference-contexts: These tools provide a unifying, intuitive, and powerful framework for carrying out the analysis of several previously studied random processes of interest, including the random loss-resilient codes introduced in [9], the greedy algorithm for matchings in random graphs studied in <ref> [7] </ref>, and the threshold for solving random k-SAT formula using the pure literal rule [4]. In addition, generalization of these problems not previously analyzed can now be analyzed in a straightforward manner. For example, we can analyze generalizations of the processes considered in [9]. <p> The advantage of the tree analysis approach employed in this paper is that it is easily possible to analyze substantially different distributions for choosing the formula with little additional difficulty. 10 4 Greedy Matching Analysis In the paper <ref> [7] </ref>, the analysis of a simple and fast heuristic for finding matchings was described and analyzed with respect to randomly chosen graphs. The tree analysis approach can be used to provide an alternative analysis.
Reference: [8] <author> T.G. Kurtz, </author> <title> Approximation of Population Processes, </title> <booktitle> CBMS-NSF Regional Conf. Series in Applied Math, </booktitle> <publisher> SIAM, </publisher> <year> 1981. </year>
Reference-contexts: For example, the analysis of the loss-resilient codes described in [9] was done by modeling the random process using differential equations, solving the equations to obtain a polynomial, and using a version of Kurtz's theorem <ref> [8] </ref> to make the connection between the behavior of the random process and that of the polynomial. (This type of approach was pioneered in the analysis of algorithms domain in [10].) One of the ingredients lacking in the previous analysis was a simple intuitive connection between the polynomial solution and the
Reference: [9] <author> M. Luby, M. Mitzenmacher, M. A. Shokrollahi, D. Spielman, V. Stemann, </author> <title> "Practical Loss-Resilient Codes", </title> <booktitle> Proc. 29 th Symp. on Theory of Computing, </booktitle> <year> 1997. </year> <month> 11 </month>
Reference-contexts: These tools provide a unifying, intuitive, and powerful framework for carrying out the analysis of several previously studied random processes of interest, including the random loss-resilient codes introduced in <ref> [9] </ref>, the greedy algorithm for matchings in random graphs studied in [7], and the threshold for solving random k-SAT formula using the pure literal rule [4]. In addition, generalization of these problems not previously analyzed can now be analyzed in a straightforward manner. <p> In addition, generalization of these problems not previously analyzed can now be analyzed in a straightforward manner. For example, we can analyze generalizations of the processes considered in <ref> [9] </ref>. As another example, we can analyze the behavior of the pure literal rule on random k-SAT formula chosen from distributions not handled by previous analyses. Our main tool is a simple analysis of the probability an Or-And tree formula evaluates to 1. <p> Some of the analyses we present in the paper were previously done using different methodologies. For example, the analysis of the loss-resilient codes described in <ref> [9] </ref> was done by modeling the random process using differential equations, solving the equations to obtain a polynomial, and using a version of Kurtz's theorem [8] to make the connection between the behavior of the random process and that of the polynomial. (This type of approach was pioneered in the analysis <p> In the next three sections, we apply this simple lemma to the analysis of loss-resilient codes, the pure literal rule for random k-CNF formula, and the greedy matching algorithm for random graphs. 2 Loss-Resilient Code Analysis 2.1 Essentials of the Codes The codes described in <ref> [9] </ref> consist of a cascading sequence of random bipartite graphs. To describe the encoding and decoding process, as well as to carry out the analysis, we need only consider one bipartite graph. <p> The decoding process terminates successfully with all message bits recovered iff the graph recovery rule ends with no remaining left nodes with label 0. 2.2 New Analysis of the Original Process The paper <ref> [9] </ref> gave an analysis of the decoding process described in the previous subsection using differential equations to model the process, and then solving these equations as polynomials. In this subsection, we obtain the same result using Lemma 1. <p> Let (p 0 ; p 1 ; : : :; p L ) and (q 0 ; q 1 ; : : : ; q R ) be probability vectors. As in <ref> [9] </ref>, consider choosing a random bipartite graph with n left nodes and m right nodes as follows: Each node on the left is chosen to have degree i with probability p i , and each node on the right is chosen to have degree j with probability p j , where <p> This turns out to be equivalent to the condition given in <ref> [9] </ref> for this process to end successfully. 2.3 The Overall Analysis It is not hard to argue that the process terminates with all message values successfully recovered if the probability a message bit is not directly received is ffi and if Condition (6) is fulfilled. <p> Then, using the expansion properties of the random graph, using standard combinatorial arguments as outlined in <ref> [9] </ref>, it is not hard to argue that if at most fl 0 n message bits are left recovered then the decoding process fails to recover more than O (n fl 00 ) message bits with probability at most inverse polynomial in n, for some constant fl 00 &lt; 1. <p> the process terminates in a state where a uniformly chosen edge is adjacent to left node with a missing message bit with probability at most fl if there is a constant * &gt; 0 such that ffi (1 (1 ffi 0 ) (1 x)) &lt; x (1 *) (7) In <ref> [9] </ref> a procedure is described for finding (close to) optimal right probabilities 1 ; : : : ; R for a given set of left probabilities 1 ; : : : ; L using a linear programming approach. However, [9] did not describe how to find the optimal left probabilities for <p> ffi 0 ) (1 x)) &lt; x (1 *) (7) In <ref> [9] </ref> a procedure is described for finding (close to) optimal right probabilities 1 ; : : : ; R for a given set of left probabilities 1 ; : : : ; L using a linear programming approach. However, [9] did not describe how to find the optimal left probabilities for a given set of right probabilities. Using Condition (6) (or, even stronger, Condition (7), it is easy to see how to use the methodology de scribed in [9] to do exactly this. <p> However, <ref> [9] </ref> did not describe how to find the optimal left probabilities for a given set of right probabilities. Using Condition (6) (or, even stronger, Condition (7), it is easy to see how to use the methodology de scribed in [9] to do exactly this. In fact, Condition (6) is in some sense of the dual of the corresponding condition described in [9], which was (1 ffi (1 x)) &gt; x (1 + *) (8) for some constant * &gt; 0 and for all x 2 (0; 1]. <p> Using Condition (6) (or, even stronger, Condition (7), it is easy to see how to use the methodology de scribed in <ref> [9] </ref> to do exactly this. In fact, Condition (6) is in some sense of the dual of the corresponding condition described in [9], which was (1 ffi (1 x)) &gt; x (1 + *) (8) for some constant * &gt; 0 and for all x 2 (0; 1]. It is from Condition (8) that [9] shows how to find the optimal right probabilities for a given set of left probabilities. <p> In fact, Condition (6) is in some sense of the dual of the corresponding condition described in <ref> [9] </ref>, which was (1 ffi (1 x)) &gt; x (1 + *) (8) for some constant * &gt; 0 and for all x 2 (0; 1]. It is from Condition (8) that [9] shows how to find the optimal right probabilities for a given set of left probabilities. We leave it as an exercise how to use the Or-And tree analysis to easily derive Condition (8).
Reference: [10] <author> M. </author> <title> Mitzenmacher, </title> <type> Ph.D. thesis. </type>
Reference-contexts: the random process using differential equations, solving the equations to obtain a polynomial, and using a version of Kurtz's theorem [8] to make the connection between the behavior of the random process and that of the polynomial. (This type of approach was pioneered in the analysis of algorithms domain in <ref> [10] </ref>.) One of the ingredients lacking in the previous analysis was a simple intuitive connection between the polynomial solution and the original process. With the new analysis, this intuitive connection is direct and compelling.
Reference: [11] <author> M. Mitzenmacher, </author> <title> "Tight Thresholds for the Pure Literal Rule", </title> <type> DEC/SRC Technical Report 1997-011, </type> <month> June </month> <year> 1997. </year>
Reference-contexts: The behavior of the pure literal rule has been studied previously with respect to randomly chosen k-SAT formula ([6], <ref> [11] </ref>). Here, we show how the tree analysis gives a direct explanation of the behavior of the pure literal rule for randomly chosen k-SAT formula with respect to the same distributions considered in [6] and [11]. <p> pure literal rule has been studied previously with respect to randomly chosen k-SAT formula ([6], <ref> [11] </ref>). Here, we show how the tree analysis gives a direct explanation of the behavior of the pure literal rule for randomly chosen k-SAT formula with respect to the same distributions considered in [6] and [11]. With this new analysis, it is also straightforward to analyze distributions that were not previously considered and which would be much harder to analyze using previous techniques applied to this problem. <p> By using Maple, one can verify that Condition 12 is satisfied for all y when k = 3 at the threshold value = 1:63. This result has been found previously by [4] and <ref> [11] </ref> using a different approach.
Reference: [12] <author> E. Moore, C. Shannon, </author> <title> "Reliable circuits using less reliable relays", </title> <journal> J. Franklin Inst., </journal> <volume> 262, </volume> <year> 1956, </year> <pages> pp. 191-208 and 281-297. </pages>
Reference-contexts: 1 Introduction We introduce a new set of probabilistic analysis tools related to the amplification method introduced by <ref> [12] </ref> and further developed and used in [13], [5]. <p> Our analysis is related to the study of amplification, initiated by Moore and Shannon <ref> [12] </ref>, and continued in several works [13], [5]. Consider the probability the root of the tree evaluates to 0 when the value of each leaf is independently chosen to be 0 with probability p. Let us denote this probability as y ` .
Reference: [13] <author> L. G. Valiant, </author> <title> "Short Monotone Formulae for the Majority Function", </title> <journal> J. of Algorithms, </journal> <volume> Vol. 5, </volume> <year> 1984, </year> <pages> pp. 363-366. 12 </pages>
Reference-contexts: 1 Introduction We introduce a new set of probabilistic analysis tools related to the amplification method introduced by [12] and further developed and used in <ref> [13] </ref>, [5]. <p> Our analysis is related to the study of amplification, initiated by Moore and Shannon [12], and continued in several works <ref> [13] </ref>, [5]. Consider the probability the root of the tree evaluates to 0 when the value of each leaf is independently chosen to be 0 with probability p. Let us denote this probability as y ` . <p> Of primary interest in these studies is the rate of amplification, i.e., what is the rate at which y ` goes to either 0 or 1 as a function of `. One work that uses exactly this type of analysis is the elegant randomized construction given in <ref> [13] </ref> of a polynomial size monotone boolean formula that computes the majority function. <p> of = (3 5)=2, and that if y `1 = + * then y ` &gt; + c* for a constant c &gt; 1. (The analogous rate of divergence is true if y `1 = *.) In further work, [3] and [5] provide beautiful proofs that the construction size of <ref> [13] </ref> is optimal, this time using amplification analysis to prove a lower bound. Our work has a similar spirit to the amplification work, but it differs in several ways. We generalize to allow the number of children of each node to vary in the following way.
References-found: 13

