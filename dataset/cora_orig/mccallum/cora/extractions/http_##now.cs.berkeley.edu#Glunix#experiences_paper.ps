URL: http://now.cs.berkeley.edu/Glunix/experiences_paper.ps
Refering-URL: http://now.cs.berkeley.edu/Papers2/recent.html
Root-URL: 
Email: fghorm,dpetrou,steverod,vahdat,teag@cs.berkeley.edu  
Title: GLUnix: a Global Layer Unix for a Network of Workstations use for over two years
Author: Douglas P. Ghormley, David Petrou, Steven H. Rodrigues, Amin M. Vahdat, and Thomas E. Anderson 
Note: GLUnix has been in daily  can be run in 1.3 seconds and that the centralized GLUnix master is not the  
Date: August 14, 1997  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California at Berkeley  
Abstract: Recent improvements in network and workstation performance have made clusters an attractive architecture for diverse workloads, including sequential and parallel interactive applications. However, although viable hardware solutions are available today, the largest challenge in making such a cluster usable lies in the system software. This paper describes the design and implementation of GLUnix, an operating system layer for a cluster of workstations. GLUnix is designed to provide transparent remote execution, support for interactive parallel and sequential jobs, load balancing, and backward compatibility for existing application binaries. GLUnix is a multi-user, user-level system which was constructed to be easily portable to a number of platforms. This paper relates our experiences with designing, building, and running GLUnix. We evaluate the original goals of the project in contrast with the final features of the system. The GLUnix architecture and implementation are presented, along with performance and scalability measurements. The discussion focuses on the lessons we have learned from the system, including a characterization of the limitations of a user-level implementation and the social considerations encountered when supporting a large user community.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas Anderson, Brian Bershad, Edward Lazowska, and Henry Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <journal> In ACM Transactions on Computer Systems, </journal> <pages> pages 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Alternatively, the system need not kill the entire parallel program whenever a single process in it dies. The master could notify the remaining processes of the failure, using a mechanism such as Scheduler Activations <ref> [1] </ref>, and continue normal operation.
Reference: [2] <author> Thomas E. Anderson, David E. Culler, David A. Patterson, </author> <title> and the NOW Team. A Case for NOW (Networks of Workstations). </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: The narrowing of the performance gap between commodity workstations and supercomputers as well as the availability of high-speed, commodity local area networks have led to the ability to run both compute-bound and massively parallel programs on networks of workstations (NOWs) <ref> [2] </ref>. Consequently, NOWs are capable of servicing all three of the historical workloads. Given the availability of commodity high-performance workstations and networks, the primary challenge in building a NOW lies in the system software needed to manage the cluster.
Reference: [3] <author> Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang. </author> <title> Serverless Network File Systems. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 109-126, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Since the Berkeley NOW proposal included a high performance global file system <ref> [3] </ref>, GLUnix assumes that each node of the cluster shares the same file systems, making pathnames valid on all nodes. Further, GLUnix uses a shared directory to store the network address of the GLUnix master for bootstrapping. Our cluster uses NFS-mounted file systems for this purpose. <p> The data presented in Figure 8 reveals two things. First, network and file system costs account for a significant portion of the remote execution time. Porting GLUnix to use Active Messages (which provides a very low-latency RPC) and a cluster file system like xFS <ref> [3] </ref> (which provides improved caching over NFS) would improve remote execution time significantly. Second, the master is not a central bottleneck. Only 0.7 msec of the remote execution latency (less than 1%) is spent in the master. <p> Modifying the Makefile to add dependencies between the source files and the executable can resolve this problem for a given Makefile. However, requiring users to modify Makefiles to avoid consistency problems in the filesystem is neither transparent nor reasonable. This argues for a strongly-consistent cluster file system <ref> [19, 3] </ref>. 22 file: file.o cc -o file file.o .o.c: Table 4: Sample Makefile which exposes inconsistency of NFS when run under glumake. 8 Social Considerations Over the past few years we have learned a number of things from supporting an active user community.
Reference: [4] <author> Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau and David E. Culler and Joseph M. Hellerstein and David A. Patterson. </author> <title> High-Performance Sorting on Networks of Workstations. </title> <booktitle> In SIGMOD '97, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Implementations of the parallel programming language Split-C [13] and the Message Passing Interface (MPI) [30] use the GLUnix library routines to run on the Berkeley NOW. A number of interesting parallel programs have been implemented using these facilities, including: NOW-Sort <ref> [4] </ref>, which holds the record for the world's fastest sort time: 8.4 GB in under one minute, p-murphi [14], a parallel version of the popular protocol verification tool, and a 32K by 32K LINPACK implementation that achieved 10.1 GFlops on 100 UltraSPARC-I processors, placing the Berkeley NOW 345th on the list <p> The second most common use of the cluster has been as a parallel compilation server via the glumake utility. The cluster has been used to achieve a world record in disk-to-disk sorting <ref> [4] </ref>, for simulations of advanced log structured file sys 4 See http://http.cs.berkeley.edu/~eanders/pictures/index.html 15 tem research [24], two-dimensional particle-in-cell plasma, three-dimensional fast Fourier transforms, and genetic inference algorithms. GLUnix was also found to be extremeley useful for testing and system administration. <p> However, the system required significant evolution, redesign and performance to achieve the current performance levels. In this section, we will describe some 19 of our experience with improving the performance of remote execution. These changes were driven by the needs of the parallel NOW-Sort application <ref> [4] </ref>, which currently holds the record for the fastest disk-to-disk sort. To set the record, NOW-Sort had to sort as much data as possible in 60 seconds. The sort was structured as a parallel program, with each node sorting a portion of a large datafile.
Reference: [5] <author> Remzi Arpaci, Andrea Dusseau, Amin Vahdat, Lok Liu, Thomas Anderson, and David Patterson. </author> <title> The Interaction of Parallel and Sequential Workload on a Network of Workstations. </title> <booktitle> In Proceedings of Performance/Sigmetrics, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: The feasibility of supporting integrated parallel and sequential workloads was established by an early simulation study <ref> [5] </ref> of workloads measured from a 32-node CM-5 at Los Alamos National Lab and a 70-node workstation cluster at U.C. Berkeley. The study concluded that harvesting idle cycles on a 60-node workstation cluster could support both the parallel and sequential workloads with minimal slowdown.
Reference: [6] <author> Remzi Arpaci, Andrea Dusseau, Amin Vahdat, Lok Liu, Thomas Anderson, and David Patterson. </author> <title> The Interaction of Parallel and Sequential Workload on a Network of Workstations. </title> <type> Technical Report CSD-94-838, </type> <institution> U.C. Berkeley, </institution> <month> October </month> <year> 1994. </year> <month> 26 </month>
Reference-contexts: The system should also dynamically migrate jobs to new nodes either as load becomes unbalanced or as nodes are dynamically added or removed from the system. * Coscheduling of Parallel Jobs: Efficient execution of communicating parallel jobs requires that the individual processes be scheduled at roughly the same time <ref> [26, 6, 16] </ref>. * Binary Compatibility: The system should be able to provide remote execution, load balancing, and coscheduling with out requiring application modification or relinking. Existing applications should be able to transparently benefit from new features. * High Availability: When a node crashes, the system should continue to operate.
Reference: [7] <author> Remzi H. Arpaci, David E. Culler, Arvind Krishnamurthy, Steve Steinberg, and Kathy Yelick. </author> <title> Empirical Evaluation of the CRAY T3D: A Compiler Perspective. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems such as PVM [29] typically take this approach. However, a number of studies <ref> [12, 17, 7, 16] </ref> demonstrate that this technique leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process.
Reference: [8] <author> Allan Bricker, Michael Litzkow, and Miron Livny. </author> <title> Condor Technical Summary. </title> <type> Technical Report 1069, </type> <institution> University of Wisconsin Madison, Computer Science Department, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: PVM [29] did provide support for parallel jobs but did not provide gang scheduling for closely synchronized parallel jobs. PVM also did not integrate the parallel job environment with interactive sequential jobs or provide dynamic load balancing. Condor <ref> [8] </ref> was able to dynamically balance cluster load by migrating jobs, but only if those jobs were restricted to a certain set of features and were re-linked with a special library. Further, Condor did not transparently support existing binaries and operated only in batch mode. <p> Even in the case where exported system primitives are sufficient, the performance of those primitives (sys-tem calls, page mappings, signal delivery) may be unacceptable. In particular, we were concerned that the limitations of previous user level systems <ref> [8] </ref> may have been an indication that the functionality needed to implement remote execution transparently was not available at the user level. <p> In this section we briefly list some of the projects with goals similar to our own. LSF [37] provides command line integration with a load balancing and queuing facility. Both V [11] and Sprite [15] support process migration in the kernel, while Condor <ref> [8] </ref> provides a user-level, though not fully transparent, solution. PVM [29] and Linda [9] provide support for parallel programs on clusters of distributed workstations, and PVM has only recently begun to support coscheduling [26].
Reference: [9] <author> N. Carriero and D. Gelernter. </author> <title> Linda in Context. </title> <journal> Communications of the ACM, </journal> <month> April </month> <year> 1989. </year>
Reference-contexts: LSF [37] provides command line integration with a load balancing and queuing facility. Both V [11] and Sprite [15] support process migration in the kernel, while Condor [8] provides a user-level, though not fully transparent, solution. PVM [29] and Linda <ref> [9] </ref> provide support for parallel programs on clusters of distributed workstations, and PVM has only recently begun to support coscheduling [26].
Reference: [10] <author> J. Chapin, M. Rosenblum, S. Devine, T. Lahiri, D. Teodosiu, and A. Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multipro cessors. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 12-25, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: PVM [29] and Linda [9] provide support for parallel programs on clusters of distributed workstations, and PVM has only recently begun to support coscheduling [26]. More recently, the Hive <ref> [10] </ref> effort sought to provide fault containment clusters of SMPs, and the Solaris MC [20] group modified the Solaris kernel to provide a single system image to sequential and multithreaded applications. 10 Future Work There are two further research efforts that have arisen out of GLUnix.
Reference: [11] <author> David R. Cheriton. </author> <title> The V Distributed System. </title> <booktitle> In Communications of the ACM, </booktitle> <pages> pages 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Centralized designs, on the other hand, are easier to design, implement, and debug since they simplify state management and synchronization. Testaments to the surprising scalability of the centralized architecture of V <ref> [31, 11] </ref> suggested that a centralized design might have acceptable performance for the system scale we were targeting (100 nodes). <p> In this section we briefly list some of the projects with goals similar to our own. LSF [37] provides command line integration with a load balancing and queuing facility. Both V <ref> [11] </ref> and Sprite [15] support process migration in the kernel, while Condor [8] provides a user-level, though not fully transparent, solution. PVM [29] and Linda [9] provide support for parallel programs on clusters of distributed workstations, and PVM has only recently begun to support coscheduling [26].
Reference: [12] <author> Mark Crovella, Prakash Das, Czarek Dubnicki, Thomas LeBlanc, and Evangelos Markatos. </author> <title> Multiprogramming on multiprocessors. </title> <type> Technical Report 385, </type> <institution> University of Rochester, Computer Science Department, </institution> <month> February </month> <year> 1991. </year> <note> Revised May. </note>
Reference-contexts: In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems such as PVM [29] typically take this approach. However, a number of studies <ref> [12, 17, 7, 16] </ref> demonstrate that this technique leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process.
Reference: [13] <author> David E Culler, Andrea Dusseau, Seth C. Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Ketherine Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <year> 1993. </year>
Reference-contexts: User experience with the GLUnix has been largely positive, and the system has enabled or assisted systems research and program development in a number of areas. Implementations of the parallel programming language Split-C <ref> [13] </ref> and the Message Passing Interface (MPI) [30] use the GLUnix library routines to run on the Berkeley NOW.
Reference: [14] <author> D.L. Dill, A. Drexler, A.J. Hu, and C.H Yang. </author> <title> Protocol Verification as a Hardware Design Aid. </title> <booktitle> In International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: A number of interesting parallel programs have been implemented using these facilities, including: NOW-Sort [4], which holds the record for the world's fastest sort time: 8.4 GB in under one minute, p-murphi <ref> [14] </ref>, a parallel version of the popular protocol verification tool, and a 32K by 32K LINPACK implementation that achieved 10.1 GFlops on 100 UltraSPARC-I processors, placing the Berkeley NOW 345th on the list of the world's 500 fastest supercomputers 3 . 5 GLUnix Usage GLUnix has been running and in daily
Reference: [15] <author> Fred Douglis and John Ousterhout. </author> <title> Transparent Process Migration: Design Alternatives and the Sprite Implementation. </title> <journal> Software Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-85, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Further, Condor did not transparently support existing binaries and operated only in batch mode. Sprite [25] provided dynamic load balancing through process migration <ref> [15] </ref> and maintained full UNIX I/O and job control semantics for remote jobs. However, it did not support parallel jobs and was a complete custom operating system, making hardware updates difficult to track. <p> In this section we briefly list some of the projects with goals similar to our own. LSF [37] provides command line integration with a load balancing and queuing facility. Both V [11] and Sprite <ref> [15] </ref> support process migration in the kernel, while Condor [8] provides a user-level, though not fully transparent, solution. PVM [29] and Linda [9] provide support for parallel programs on clusters of distributed workstations, and PVM has only recently begun to support coscheduling [26].
Reference: [16] <author> Andrea C. Dusseau, Remzi H. Arpaci, and David E. Culler. </author> <title> Effective Distributed Scheduling of Parallel Workloads. </title> <booktitle> In Proceedings of the 1996 ACM SIGMETRICS Conference, </booktitle> <year> 1996. </year>
Reference-contexts: The system should also dynamically migrate jobs to new nodes either as load becomes unbalanced or as nodes are dynamically added or removed from the system. * Coscheduling of Parallel Jobs: Efficient execution of communicating parallel jobs requires that the individual processes be scheduled at roughly the same time <ref> [26, 6, 16] </ref>. * Binary Compatibility: The system should be able to provide remote execution, load balancing, and coscheduling with out requiring application modification or relinking. Existing applications should be able to transparently benefit from new features. * High Availability: When a node crashes, the system should continue to operate. <p> In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems such as PVM [29] typically take this approach. However, a number of studies <ref> [12, 17, 7, 16] </ref> demonstrate that this technique leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process. <p> The GLUnix batch facility (see Section 8.2) has been used in situations where very large numbers of short and medium simulations were run, for example, the simulations for Implicit Coscheduling research <ref> [16] </ref>. 6 Performance and Scalability This section evaluates the scalability and performance of the system, relating some of our experiences in performance tuning. All of the data presented in this section is measured with GLUnix running on 167MHz Sun UltraSparcs running Solaris 2.5. <p> The first is an effort called Implicit Coscheduling <ref> [16] </ref> which is examining the effect of enabling communicating processes to coschedule themselves using various heuristics. The algorithm uses adaptive two-phase blocking. When a process would normally block, waiting for a communication event, it instead spins for an adaptive period of time.
Reference: [17] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Gang Scheduling Performance Benefits for Fine-Grained Synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(4) </volume> <pages> 306-18, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems such as PVM [29] typically take this approach. However, a number of studies <ref> [12, 17, 7, 16] </ref> demonstrate that this technique leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process.
Reference: [18] <author> Douglas P. Ghormley, David Petrou, and Thomas E. Anderson. SLIC: </author> <title> Secure Loadable Interposition Code. </title> <type> Technical Report CSD 96-920, </type> <institution> University of California at Berkeley, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: In this way, the constituent processes of a parallel program dynamically adjust themselves to run at approximately the same time despite independent local operating system schedulers. Experimentation reveals that implicit coscheduling performs within +/-35% of coscheduling without the need for global coordination. The second research effort is SLIC <ref> [18] </ref> which is a method for alleviating the restrictions of implementing system facilities at the user-level.
Reference: [19] <author> J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satyanarayanan, R. Sidebotham, and M. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-82, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Modifying the Makefile to add dependencies between the source files and the executable can resolve this problem for a given Makefile. However, requiring users to modify Makefiles to avoid consistency problems in the filesystem is neither transparent nor reasonable. This argues for a strongly-consistent cluster file system <ref> [19, 3] </ref>. 22 file: file.o cc -o file file.o .o.c: Table 4: Sample Makefile which exposes inconsistency of NFS when run under glumake. 8 Social Considerations Over the past few years we have learned a number of things from supporting an active user community.
Reference: [20] <author> Yousef A. Khalidi, Jose M. Bernabeu, Vlada Matena, Ken Shirriff, and Moti Thadani. </author> <title> Solaris MC: A multi computer OS. </title> <booktitle> In Pro ceedings of the 1996 USENIX Conference, </booktitle> <pages> pages 191-203, </pages> <address> Berkeley, CA, USA, </address> <month> January </month> <year> 1996. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: PVM [29] and Linda [9] provide support for parallel programs on clusters of distributed workstations, and PVM has only recently begun to support coscheduling [26]. More recently, the Hive [10] effort sought to provide fault containment clusters of SMPs, and the Solaris MC <ref> [20] </ref> group modified the Solaris kernel to provide a single system image to sequential and multithreaded applications. 10 Future Work There are two further research efforts that have arisen out of GLUnix.
Reference: [21] <author> James R. Larus and Eric Schnarr. EEL: </author> <title> Machine-independent Executable Editing. </title> <booktitle> In ACM SIGPLAN '95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <volume> volume 30(6), </volume> <pages> pages 291-300, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: While intercepting such events is relatively straightforward when modifying the underlying operating system, it is often more difficult to implement portably at the user-level. Section 7 details this and other problems associated with user-level solutions. Despite the above limitations, we believed that binary rewriting techniques <ref> [33, 21] </ref> or new kernel extension technologies could be employed to catch and redirect the appropriate system events to support transparent remote execution.
Reference: [22] <author> Steven S. Lumetta and David E. Culler. </author> <title> The mantis parallel debugger. </title> <booktitle> In Proceedings of the SIGMETRICS Symposium on Parallel and Distributed Tools, </booktitle> <pages> pages 118-26, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Table 2 shows some common GLUnix library functions. Glib Spawn () is the basic function to run a program under GLU-nix. Glib CoSpawn () is used to place an application on the same nodes as a currently running application; this function is currently used by the Mantis <ref> [22] </ref> parallel debugger. Glib GetMyNpid () and Glib GetMyVNN () return the Npid and VNN, respectively, of the requesting application. Glib Signal () is used to send a signal to one or more of the VNNs of a program running under GLUnix.
Reference: [23] <author> Steve Maguire. </author> <title> Writing Solid Code. </title> <publisher> Microsoft Press, </publisher> <year> 1993. </year>
Reference-contexts: This technique actually had the opposite effect. Rather than making the system more stable, masking faults in this way separated the point of fault origin from the point of failure, making the bugs more difficult to locate and fix. The book Writing Solid Code <ref> [23] </ref> led us to a adopt a less tolerant internal structure. System debugging became significantly easier and proceeded more quickly. Making the inter-component interactions less tolerant of each other's failures would also probably make the system easier to debug, but would also make it less stable in the short term.
Reference: [24] <author> Jeanna Neefe Matthews, Drew Roselli, Adam M. Costello, Randolph Y. Wang, and Thomas E. Anderson. </author> <title> Improving the Performance of Log-structured File Systems with Adaptive Methods. </title> <booktitle> In Proceedingsof the 16th ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: The second most common use of the cluster has been as a parallel compilation server via the glumake utility. The cluster has been used to achieve a world record in disk-to-disk sorting [4], for simulations of advanced log structured file sys 4 See http://http.cs.berkeley.edu/~eanders/pictures/index.html 15 tem research <ref> [24] </ref>, two-dimensional particle-in-cell plasma, three-dimensional fast Fourier transforms, and genetic inference algorithms. GLUnix was also found to be extremeley useful for testing and system administration. Using GLUnix for interactive parallel stress testing of the Myrinet network revealed bugs which could not be recreated using traditional UNIX functionality such as rsh.
Reference: [25] <author> J. Ousterhout, A. Cherenson, F. Douglis, M. Nelson, and B. Welch. </author> <title> The Sprite Network Operating System. </title> <journal> IEEE Computer, </journal> <volume> 21(2):23 36, </volume> <month> February </month> <year> 1988. </year> <month> 27 </month>
Reference-contexts: Condor [8] was able to dynamically balance cluster load by migrating jobs, but only if those jobs were restricted to a certain set of features and were re-linked with a special library. Further, Condor did not transparently support existing binaries and operated only in batch mode. Sprite <ref> [25] </ref> provided dynamic load balancing through process migration [15] and maintained full UNIX I/O and job control semantics for remote jobs. However, it did not support parallel jobs and was a complete custom operating system, making hardware updates difficult to track.
Reference: [26] <author> John K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In Third International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: The system should also dynamically migrate jobs to new nodes either as load becomes unbalanced or as nodes are dynamically added or removed from the system. * Coscheduling of Parallel Jobs: Efficient execution of communicating parallel jobs requires that the individual processes be scheduled at roughly the same time <ref> [26, 6, 16] </ref>. * Binary Compatibility: The system should be able to provide remote execution, load balancing, and coscheduling with out requiring application modification or relinking. Existing applications should be able to transparently benefit from new features. * High Availability: When a node crashes, the system should continue to operate. <p> Just like MPP operating systems, GLUnix provides specialized scheduling support for parallel programs in the form of barriers and coscheduling <ref> [26] </ref>. Parallel programs under GLUnix invoke a barrier synchronization by calling the Glunix Barrier () library call. When an individual process of a parallel program enters the barrier, it is blocked until all other processes of the program have also 8 entered the barrier. <p> At this point the GLUnix Barrier () library call will return and the process will continue executing. GLUnix implements an approximation of coscheduling through a simple user-level strategy. The GLUnix master uses a matrix algorithm <ref> [26] </ref> to determine a time-slice order for all runnable parallel programs. Periodically, the master sends a scheduling message to all daemons instructing them to run a new program, identified by NPID. Each daemon maintains a NPID to local pid mapping in its process database. <p> Both V [11] and Sprite [15] support process migration in the kernel, while Condor [8] provides a user-level, though not fully transparent, solution. PVM [29] and Linda [9] provide support for parallel programs on clusters of distributed workstations, and PVM has only recently begun to support coscheduling <ref> [26] </ref>.
Reference: [27] <author> James S. Plank, Micah Beck, Gerry Kingsley, and Kai Li. Libckpt: </author> <title> Transparent Checkpointing under Unix. </title> <booktitle> In Proceedings of the 1995 USENIX Summer Conference, </booktitle> <pages> pages 213-223, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The master could notify the remaining processes of the failure, using a mechanism such as Scheduler Activations [1], and continue normal operation. Currently the system does not provide any support for checkpointing or restarting applications, though applications are free to use checkpointing packages such as Libckpt <ref> [27] </ref> if necessary. 4.3 Transparent Remote Execution When users run GLUnix programs from the command-line, the executed process becomes the GLUnix startup process. The startup process must be linked to the GLUnix library and invokes a GLUnix library routine to send a remote execution request to the master.
Reference: [28] <author> Sun Microsystems. XDR: </author> <title> External Data Representation Standard. </title> <type> Technical Report RFC-1014, </type> <institution> Sun Microsystems, Inc., </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Enabling multi-platform support for GLUnix would require porting to a network data transport mechanism such as XDR <ref> [28] </ref> and developing a mechanism for supporting multi-platform binaries. 4.2 System Architecture Overview This section presents a brief overview of the individual GLUnix components making up the internal architecture. 9 and daemons are located on hamlet, macbeth, romeo, and othello.
Reference: [29] <author> V. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <address> De cember 1990. </address>
Reference-contexts: LSF [37] and Utopia [36] provided support for remote execution of interactive sequential jobs, but did not support parallel jobs or dynamic load balancing. PVM <ref> [29] </ref> did provide support for parallel jobs but did not provide gang scheduling for closely synchronized parallel jobs. PVM also did not integrate the parallel job environment with interactive sequential jobs or provide dynamic load balancing. <p> In the absence of coscheduling, the individual operating systems in the NOW would independently schedule the processes of a parallel application. User-level cluster systems such as PVM <ref> [29] </ref> typically take this approach. However, a number of studies [12, 17, 7, 16] demonstrate that this technique leads to unacceptable execution times for frequently-communicating processes. This slowdown occurs when a process stalls while attempting to communicate or synchronize with a currently unscheduled process. <p> UNIX provides two distinct output streams, stdout and stderr. While stderr data is usually directed to a user's console, stdout can be easily redirected to a file or to another program using standard shell redirection facilities. Many remote execution facilities such as rsh and PVM <ref> [29] </ref> do not keep these two I/O streams separate and thus do not maintain proper redirection and piping semantics. GLUnix maintains two separate output streams. The I/O semantics that GLUnix does not properly handle concern tty behavior. <p> LSF [37] provides command line integration with a load balancing and queuing facility. Both V [11] and Sprite [15] support process migration in the kernel, while Condor [8] provides a user-level, though not fully transparent, solution. PVM <ref> [29] </ref> and Linda [9] provide support for parallel programs on clusters of distributed workstations, and PVM has only recently begun to support coscheduling [26].
Reference: [30] <author> The MPI Forum. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 878-883, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: User experience with the GLUnix has been largely positive, and the system has enabled or assisted systems research and program development in a number of areas. Implementations of the parallel programming language Split-C [13] and the Message Passing Interface (MPI) <ref> [30] </ref> use the GLUnix library routines to run on the Berkeley NOW.
Reference: [31] <author> Marvin M. </author> <title> Theimer. </title> <type> Personal communication, </type> <month> June </month> <year> 1994. </year>
Reference-contexts: Centralized designs, on the other hand, are easier to design, implement, and debug since they simplify state management and synchronization. Testaments to the surprising scalability of the centralized architecture of V <ref> [31, 11] </ref> suggested that a centralized design might have acceptable performance for the system scale we were targeting (100 nodes).
Reference: [32] <author> Thorsten von Eicken, David E. Culler, Steh C. Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The Comm module is currently implemented using Berkeley Stream Sockets over TCP/IP, though it is designed to be easily replaced by a module using faster communication primitives, such as Active Messages <ref> [32] </ref>. GLUnix events consist of both messages and signals, with the main event loop located in the Comm module. When a message arrives, the Msg module unpacks the message into a local data structure and then invokes the message handler.
Reference: [33] <author> Robert Wahbe, Steven Lucco, Thomas Anderson, and Susan Graham. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203-216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: While intercepting such events is relatively straightforward when modifying the underlying operating system, it is often more difficult to implement portably at the user-level. Section 7 details this and other problems associated with user-level solutions. Despite the above limitations, we believed that binary rewriting techniques <ref> [33, 21] </ref> or new kernel extension technologies could be employed to catch and redirect the appropriate system events to support transparent remote execution.
Reference: [34] <author> D. Walsh, B. Lyon, G. Sager, J. M. Chang, D. Goldberg, S. Kleiman, T. Lyon, R. Sandberg, and P. Weiss. </author> <title> Overview of the Sun Network File System. </title> <booktitle> In Proceedings of the 1985 USENIX Winter Conference, </booktitle> <pages> pages 117-124, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Clearly, such functionality is not supported by either standard user-level services or Solaris's /proc API. 7.4 NFS The weak cache consistency policy of NFS <ref> [34] </ref> for file attributes limits the utility of the glumake utility. To improve performance, NFS clients cache recently accessed file attributes. Elements in this cache are flushed after a timeout period, eliminating most stale accesses in traditional computing environments.
Reference: [35] <author> Neil Webber. </author> <title> Operating System Support for Portable Filesystem Extensions. </title> <booktitle> In Proceedings of the 1993 USENIX Winter Conference, </booktitle> <pages> pages 219-228, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Unfortunately, Modifying an existing kernel considerably impacts system portability: not only are kernel modifications not portable across platforms, but kernel modifications are often not even valid across different releases of the same operating system <ref> [35] </ref>. In 1993, the primary operating systems available for state-of-the-art architectures were commercial products. Modifying a commercial operating system limits the ability to distribute the resulting system. Further, the time required to obtain source, learn, modify, and debug a commercial kernel is significant.
Reference: [36] <author> Sognian Zhou, Jingwen Wang, Xiaohn Zheng, and Pierre Delisle. </author> <title> Utopia: A Load Sharing Facility for Large, Heterogeneous Dis tributed Computing Systems. </title> <type> Technical Report CSRI-257, </type> <institution> University of Toronto, </institution> <year> 1992. </year>
Reference-contexts: When we set out in 1993 to select the system layer for our NOW, some of the features listed above were present in a few existing systems, but no system provided a complete solution. LSF [37] and Utopia <ref> [36] </ref> provided support for remote execution of interactive sequential jobs, but did not support parallel jobs or dynamic load balancing. PVM [29] did provide support for parallel jobs but did not provide gang scheduling for closely synchronized parallel jobs.
Reference: [37] <author> Songnian Zhou. </author> <title> LSF: Load Sharing in Large-scale Heterogeneous Distributed Systems. </title> <booktitle> In Proceedings of the Workshop on Cluster Computing, </booktitle> <month> December </month> <year> 1992. </year> <month> 28 </month>
Reference-contexts: When we set out in 1993 to select the system layer for our NOW, some of the features listed above were present in a few existing systems, but no system provided a complete solution. LSF <ref> [37] </ref> and Utopia [36] provided support for remote execution of interactive sequential jobs, but did not support parallel jobs or dynamic load balancing. PVM [29] did provide support for parallel jobs but did not provide gang scheduling for closely synchronized parallel jobs. <p> While the implementation of such systems has naturally matured over the years, to our knowledge, no single system supports all of our desired functionality for a NOW operating system. In this section we briefly list some of the projects with goals similar to our own. LSF <ref> [37] </ref> provides command line integration with a load balancing and queuing facility. Both V [11] and Sprite [15] support process migration in the kernel, while Condor [8] provides a user-level, though not fully transparent, solution.
References-found: 37

