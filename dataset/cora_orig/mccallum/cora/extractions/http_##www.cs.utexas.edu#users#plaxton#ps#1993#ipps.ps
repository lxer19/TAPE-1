URL: http://www.cs.utexas.edu/users/plaxton/ps/1993/ipps.ps
Refering-URL: http://www.cs.utexas.edu/users/plaxton/html/abc.html
Root-URL: 
Title: Sorting-Based Selection Algorithms for Hypercubic Networks  
Author: P. Berthome A. Ferreira ; B. M. Maggs S. Perennes C. G. Plaxton 
Abstract: This paper presents several deterministic algorithms for selecting the kth largest record from a set of n records on any n-node hypercubic network. All of the algorithms are based on the selection algorithm of Cole and Yap, as well as on various sorting algorithms for hypercubic networks. Our fastest algorithm runs in O(lg n lg fl n) time, very nearly matching the trivial (lg n) lower bound. Previously, the best upper bound known for selection was O(lg n lg lg n). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ajtai, J. Komlos, W. L. Steiger, and E. Szemeredi. </author> <title> Deterministic selection in O(log log n) parallel time. </title> <booktitle> In Proceedings of the 18th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 188-195, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Cole and Yap [5] then described an O ((lg lg n) 2 ) selection algorithm for this model. The running time was later improved to O (lg lg n) by Ajtai, Komlos, Steiger, and Szemeredi <ref> [1] </ref>. The comparisons performed by the latter algorithm are specified by an expander graph, however, making it unlikely that this algorithm can be efficiently implemented on a hypercu-bic network. A different set of upper and lower bounds hold in the PRAM models.
Reference: [2] <author> P. Beame and J. Hastad. </author> <title> Optimal bounds for decision problems on the CRCW PRAM. </title> <booktitle> In Proceedings of the 19th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 83-93, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: The comparisons performed by the latter algorithm are specified by an expander graph, however, making it unlikely that this algorithm can be efficiently implemented on a hypercu-bic network. A different set of upper and lower bounds hold in the PRAM models. Beame and Hastad <ref> [2] </ref> proved an (lg n= lg lg n) lower bound on the time for selection in the CRCW comparison PRAM using a polynomial number of processors. Vishkin [11] discovered an O (lg n lg lg n) time PRAM algorithm that uses O (n= lg n lg lg n) processors.
Reference: [3] <author> M. Blum, R. W. Floyd, V. R. Pratt, R. L. Rivest, and R. E. Tarjan. </author> <title> Time bounds for selection. </title> <journal> J. Comput. Sys. Sci., </journal> <volume> 7 </volume> <pages> 448-461, </pages> <year> 1973. </year>
Reference-contexts: In [9], Plaxton also showed that any deterministic algorithm for solving the selection problem on a p-processor hypercubic network requires ((n=p) lg lg p + lg p) time in the worst case. Since the selection problem can be solved in linear time sequentially <ref> [3] </ref>, the lower bound implies that it is not possible to design a deterministic hypercubic selection algorithm with linear speedup. For n = p the lower bound is (lg n), which is the diameter of the network.
Reference: [4] <author> R. Cole. </author> <title> An optimal selection algorithm. </title> <type> Technical Report #209, </type> <institution> Ultracomputer Research Laboratory, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: The algorithm is work-efficient (i.e., exhibits optimal speedup) because the processor time product is equal to the time, O (n), of the fastest sequential algorithm for this problem. Cole <ref> [4] </ref> later found an O (lg n lg fl n) time work-efficient PRAM algorithm. 1.4 Outline In Section 2 we describe a selection refinement algorithm. This algorithm is used as a subroutine in each of the selection algorithms that we define.
Reference: [5] <author> R. Cole and C. K. Yap. </author> <title> A parallel median algorithm. </title> <journal> Information Processing Letters, </journal> <volume> 20 </volume> <pages> 137-139, </pages> <year> 1985. </year>
Reference-contexts: The fastest runs in O (log n log fl n) time on an n-node network. The fastest previously known algorithm ran in O (log n log log n) time [9]. The algorithms use a technique called succesive sampling, which was previously used by Cole and Yap <ref> [5] </ref> to solve the selection problem in an idealized model of computation called the parallel comparision model. <p> The lower bound implies a lower bound on the time to select the kth smallest record as well. Valiant also showed how to find the largest record in O (lg lg n) time. Cole and Yap <ref> [5] </ref> then described an O ((lg lg n) 2 ) selection algorithm for this model. The running time was later improved to O (lg lg n) by Ajtai, Komlos, Steiger, and Szemeredi [1]. <p> Finally, in Section 2.5 we show how to avoid the non-uniformity introduced in Section 2.4. 2 Selection Refinement In this section, we develop an efficient subroutine for selection refinement based on the parallel comparison model algorithm of Cole and Yap <ref> [5] </ref>. There are two major differences.
Reference: [6] <author> R. E. Cypher and C. G. Plaxton. </author> <title> Deterministic sorting in nearly logarithmic time on the hypercube and related computers. </title> <journal> JCSS, </journal> <volume> 47 </volume> <pages> 501-548, </pages> <year> 1993. </year>
Reference-contexts: The algorithms also use as subroutines sorting algorithms for hyper-cubic networks due to Nassimi and Sahni [8] and Cypher and Plaxton <ref> [6] </ref>. 1.1 Hypercubic networks A hypercube contains n = 2 d nodes, each of which has a distinct d-bit label (d must be a nonnegative integer). A 1 Laboratoire de l'Informatique du Parallelisme, Institut IMAG, Ecole Normale Superieure de Lyon, 46, Allee d'Italie, 69364 Lyon Cedex 07, France. <p> The selection problem can also be solved in O (log n log log n) using the hypercubic sorting algorithm of Cypher and Plax-ton <ref> [6] </ref>. In [9], Plaxton also showed that any deterministic algorithm for solving the selection problem on a p-processor hypercubic network requires ((n=p) lg lg p + lg p) time in the worst case. <p> An O (lg n lg fl n) algorithm is presented is Section 2.4. This improvement in the running time is obtained at the expense of using a non-uniform variant of the Sharesort algorithm <ref> [6] </ref> that requires a certain amount of preprocessing. Finally, in Section 2.5 we show how to avoid the non-uniformity introduced in Section 2.4. 2 Selection Refinement In this section, we develop an efficient subroutine for selection refinement based on the parallel comparison model algorithm of Cole and Yap [5]. <p> implies that T (ffi; y) = O (ffi) and hence T (d; 0) = O (d lg lg d) = O (lg n lg (3) n). (4) n) algorithm We can improve the time bound achieved in Section 2.2 by making use of the Sharesort algorithm of Cypher and Plax-ton <ref> [6] </ref>. Several variants of that algorithm exist; in particular, detailed descriptions of two versions of Sharesort may be found in [6]. Both of these variants are designed to sort n records on an n-processor hypercubic network. <p> O (lg n lg (3) n). (4) n) algorithm We can improve the time bound achieved in Section 2.2 by making use of the Sharesort algorithm of Cypher and Plax-ton <ref> [6] </ref>. Several variants of that algorithm exist; in particular, detailed descriptions of two versions of Sharesort may be found in [6]. Both of these variants are designed to sort n records on an n-processor hypercubic network. The first algorithm runs in O (lg n (lg lg n) 3 ) time and the second algorithm, which is somewhat more complicated, runs in O (lg n (lg lg n) 2 ) time. <p> There exists a non-uniform deterministic algorithm for sorting these records in time O (lg n (lg lg n lg lg p Proof: As indicated in Section 2.3, there are a number of variants of the Sharesort algorithm of Cypher and Plax-ton <ref> [6] </ref>. These algorithms differ solely in the way that the so-called shared key sorting subroutine is implemented. <p> Perhaps the simplest variant of Sharesort runs in O (lg n lg lg n) time and relies upon an optimal logarithmic time shared key sorting subroutine. This particular result is mentioned in the original Sharesort paper <ref> [6] </ref> and more fully described by Leighton [7, Section 3.5.3]. Although it is the fastest of the Sharesort variants, this sorting algorithm suffers from the disadvantage that it is non-uniform. <p> Let M (x) denote the task of merging x sorted lists of length x 4 . One possible recurrence for performing the merge is (minor technical details related to integrality constraints are dealt with in <ref> [6] </ref> and will not be addressed here) M (n ) M (n ) + M (n ) + O (lg n) + SKS (n); (3) where SKS (n) denotes the time required to solve the shared key sorting problem. <p> The fastest known uniform version of the shared key 6 sorting subroutine takes O (lg n lg lg n) time, which leads to an O (lg n (lg lg n) 2 ) running time for the corresponding uniform variant of Sharesort <ref> [6] </ref>. In the following, we will show how to adapt this uniform version of Sharesort to obtain a uniform version of a "sparse" Sharesort, that is, an algorithm for sorting n records on p n processors. <p> By a similar analysis as that provided in Section 2.4, the theorem will follow if we can prove that SSKS (n; p) = lg n (lg lg n lg lg p n ): (5) We now sketch certain minor modifications to the SharedKeySort () algorithm of <ref> [6] </ref> that will yield a "SparseSharedKeySort ()" routine with the desired properties. The SharedKeySort () procedure consists of essentially two subroutine calls; these calls are made to the subroutines PlanRoute () and DoRoute (), respectively.
Reference: [7] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees and Hypercubes. </title> <publisher> Morgan-Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: All of the algorithms described in this paper use the edges of the hypercube in a very restricted way. At each time step, only the edges associated with a single dimension are used, and consecutive dimensions are used on consecutive steps. Such algorithms are called normal <ref> [7, Section 3.1.4] </ref>. The bounded-degree variants of the hypercube, including the butterfly, cube-connected cycles, and shu*e-exchange network, can all simulate any normal hypercube algorithm with constant slowdown [7, Sections 3.2.3 and 3.3.3]. <p> Such algorithms are called normal [7, Section 3.1.4]. The bounded-degree variants of the hypercube, including the butterfly, cube-connected cycles, and shu*e-exchange network, can all simulate any normal hypercube algorithm with constant slowdown <ref> [7, Sections 3.2.3 and 3.3.3] </ref>. For simplicity, we will describe all of the algorithms in terms of the hypercube. 1.2 Successive sampling The algorithms in this paper use the following successive sampling strategy. Initially, the sample consists of the entire set S of n records. <p> Perhaps the simplest variant of Sharesort runs in O (lg n lg lg n) time and relies upon an optimal logarithmic time shared key sorting subroutine. This particular result is mentioned in the original Sharesort paper [6] and more fully described by Leighton <ref> [7, Section 3.5.3] </ref>. Although it is the fastest of the Sharesort variants, this sorting algorithm suffers from the disadvantage that it is non-uniform.
Reference: [8] <author> D. Nassimi and S. Sahni. </author> <title> Parallel permutation and sorting algorithms and a new generalized connection network. </title> <journal> Journal of the ACM, </journal> <volume> 29 </volume> <pages> 642-667, </pages> <year> 1982. </year>
Reference-contexts: The algorithms use a technique called succesive sampling, which was previously used by Cole and Yap [5] to solve the selection problem in an idealized model of computation called the parallel comparision model. The algorithms also use as subroutines sorting algorithms for hyper-cubic networks due to Nassimi and Sahni <ref> [8] </ref> and Cypher and Plaxton [6]. 1.1 Hypercubic networks A hypercube contains n = 2 d nodes, each of which has a distinct d-bit label (d must be a nonnegative integer). <p> There are two major differences. First, we use Nassimi and Sahni's sparse enumeration sort <ref> [8] </ref> instead of a constant time sort (as is possible in the parallel comparison model), and second we obtain a total running time that is proportional to the running time of the largest call to sparse enumeration sort, whereas in the Cole and Yap algorithm, the running time is proportional to
Reference: [9] <author> C. G. Plaxton. </author> <title> Efficient Computation on Sparse Interconnection Networks. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, Stanford, </institution> <address> CA, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: The fastest runs in O (log n log fl n) time on an n-node network. The fastest previously known algorithm ran in O (log n log log n) time <ref> [9] </ref>. The algorithms use a technique called succesive sampling, which was previously used by Cole and Yap [5] to solve the selection problem in an idealized model of computation called the parallel comparision model. <p> refinement algorithm until the set of remaining elements is small enough to be sorted directly. 1.3 Previous work The fastest previously known algorithm for solving the selection problem on a hypercubic network is due to Plaxton and runs in O (log n log log n) time on an n-node network <ref> [9] </ref>. The selection problem can also be solved in O (log n log log n) using the hypercubic sorting algorithm of Cypher and Plax-ton [6]. In [9], Plaxton also showed that any deterministic algorithm for solving the selection problem on a p-processor hypercubic network requires ((n=p) lg lg p + lg <p> selection problem on a hypercubic network is due to Plaxton and runs in O (log n log log n) time on an n-node network <ref> [9] </ref>. The selection problem can also be solved in O (log n log log n) using the hypercubic sorting algorithm of Cypher and Plax-ton [6]. In [9], Plaxton also showed that any deterministic algorithm for solving the selection problem on a p-processor hypercubic network requires ((n=p) lg lg p + lg p) time in the worst case. <p> Hence T (d; 0) = O (d lg d) = O (lg n lg lg n). This algorithm is essentially equivalent to that described by Plaxton in <ref> [9] </ref>. Prior to this algorithm, the best bounds known for selection on the hypercube were given by sorting algorithms. 2.2 An O (lg n lg n) algorithm Throughout this subsection, we will refer to the O (lg n lg lg n) selection algorithm of Section 2.1 as the "basic" algorithm.
Reference: [10] <author> L. G. Valiant. </author> <title> Parallelism in comparison problems. </title> <journal> SIAM J. Comput., </journal> <volume> 4 </volume> <pages> 348-355, </pages> <year> 1975. </year>
Reference-contexts: Since the selection problem can be solved in linear time sequentially [3], the lower bound implies that it is not possible to design a deterministic hypercubic selection algorithm with linear speedup. For n = p the lower bound is (lg n), which is the diameter of the network. In <ref> [10] </ref>, Valiant proved an (lg lg n) lower bound on the time to find the largest record in a set of n records using n processors in an idealized model called the parallel comparison model.
Reference: [11] <author> U. Vishkin. </author> <title> An optimal parallel algorithm for selection. </title> <type> Technical Report 106, </type> <institution> Courant Institute of Mathematical Sciences, Department of Computer Science, </institution> <month> December </month> <year> 1983. </year> <month> 7 </month>
Reference-contexts: A different set of upper and lower bounds hold in the PRAM models. Beame and Hastad [2] proved an (lg n= lg lg n) lower bound on the time for selection in the CRCW comparison PRAM using a polynomial number of processors. Vishkin <ref> [11] </ref> discovered an O (lg n lg lg n) time PRAM algorithm that uses O (n= lg n lg lg n) processors. The algorithm is work-efficient (i.e., exhibits optimal speedup) because the processor time product is equal to the time, O (n), of the fastest sequential algorithm for this problem.
References-found: 11

