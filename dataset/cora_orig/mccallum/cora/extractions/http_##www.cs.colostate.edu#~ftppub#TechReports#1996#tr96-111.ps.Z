URL: http://www.cs.colostate.edu/~ftppub/TechReports/1996/tr96-111.ps.Z
Refering-URL: http://www.cs.colostate.edu/~howe/pubs.html
Root-URL: 
Title: Gathering Agent for Querying Web Search Engines  
Author: Daniel Dreilinger Adele E. Howe 
Address: Fort Collins, CO 80523  Fort Collins, CO 80523-1873  
Affiliation: Computer Science Department, Colorado State University  Technical  Computer Science Department Colorado State University  
Note: An Information  This research was supported in part by ARPA-AFOSR contract F30602-93-C-0100 and NSF Research Initiation Award #RIA IRI-930857  
Pubnum: Report CS-96-111  
Email: email: fdreiling,howeg@cs.colostate.edu  
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Web: url: http://www.cs.colostate.edu/~fdreiling,howeg  WWW: http://www.cs.colostate.edu  
Abstract: Computer Science Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert Armstrong, Dayne Freitag, Thorsten Joachims, and Tom Mitchell. WebWatcher: </author> <title> A learning apprentice for the World Wide Web. </title> <editor> In Craig Knoblock and Alon Levy, editors, </editor> <booktitle> Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: Last year's Spring Symposium on Information Gathering included a large number of papers on diverse agents [10]. Some of the agents focused on the Web as an information source; agents have been designed for filtering, browsing [2,13], and traversing the Web <ref> [1] </ref>, as well as searching specific information on it (e.g., FAQ files for newsgroups [9]) and heterogeneous sources [17]. Agents often reference domain dependent databases and utilize a complex logical or semantic domain model [17,11,6]. <p> This is roughly analogous to wandering around the country side hoping to find an address by following road signs, but lacking a map. In fact, the WebWatcher project <ref> [1] </ref> addresses just this problem by assisting users in following links from starting at a general subject Web page (e.g., a Machine Learning page with links to other sites) to finding the required, related information.
Reference: [2] <author> Marko Balabanovic and Yoav Shoham. </author> <title> Learning information retrieval agents: Experiments with automated web browsing. </title> <editor> In Craig Knoblock and Alon Levy, editors, </editor> <booktitle> Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: More consideration has to be given to the user's role in the search process. For example, two different modes of Internet information acquisition should be accommodated: searching for specific information, and browsing (a.k.a. "net surfing") for interesting information <ref> [2] </ref>. User's in the former mode might need to see results from a greater number of search engines, while those in the latter model might be satisfied with results from fewer sources. Ideally, these two modes could be further decomposed.
Reference: [3] <author> C. Mic Bowman, Peter B. Danzig, Udi Manber, and Michael F. Schwartz. </author> <title> Scalable internet resource discovery: research problems and approaches. </title> <journal> CACM, </journal> <volume> 37(8), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: Instead, information for the meta-index is accumulated incrementally by learning from data on actual user queries and the results returned by the search engines. 2.2 Searching the Web in Parallel The natural response to the problem of too many places to search is an additional level of abstraction <ref> [3] </ref>, which we call meta-search. One variety of meta-search tool is simply a list of pointers to possible search sites, e.g. [12,4]. These indexes are useful in that they increase the user's awareness of where they can search, but they are inconvenient.
Reference: [4] <author> William Cross. </author> <title> All-in-one search page. </title> <address> http://www.albany.net/allinone/. </address>
Reference: [5] <author> David Eichmann. </author> <title> Ethical web agents. </title> <booktitle> In Electronic Proceedings of the Second World Wide Web Conference '94: Mosaic and the Web, </booktitle> <year> 1994. </year> <note> http://www.ncsa.uiuc.edu/SDG/IT94/Proceedings/Agents/eichmann.ethical/ethics.html. </note>
Reference-contexts: This operation must be performed in the presence of two goals: minimizing resource consumption and maximizing search quality. Resource limitations make it impractical to send every query to every known search engine and programs that did so would be considered to be poor citizens of the Web <ref> [5] </ref>. Furthermore, it is undesirable; users are better served with a small set of results (less than 30) from related search engines rather than being inundated with quasi-relevant information, and having to wait for the privilege.
Reference: [6] <author> Richard Fikes, Robert Engelmore, Adam Farquhar, and Wanda Pratt. </author> <title> Network-based information brokers. </title> <editor> In Craig Knoblock and Alon Levy, editors, </editor> <booktitle> Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: Agents often reference domain dependent databases and utilize a complex logical or semantic domain model [17,11,6]. For example, <ref> [6] </ref> describes a distributed purchasing agent that aids in location and pricing of products. An internal predicate logic repre 1 sentation relates product descriptions to information domains. Using output from one information source as input for another is a common technique in domain-specific systems.
Reference: [7] <author> David Filo and Jerry Yang. </author> <note> Yahoo home page. http://www.yahoo.com. </note>
Reference-contexts: WebWatcher learns to predict links likely to be of interest to a user by passively "watching" the user traverse links and offering suggestions. Alternatively, if you have used the World Wide Web, you have no doubt encountered some of the myriad search services, such as Yahoo <ref> [7] </ref> and Web Crawler [18]. These powerful search engines can aid the process of searching on the Web. Search results are displayed in the form of another Web document consisting of a list of links that are deemed relevant to the search query.
Reference: [8] <author> Luis Gravano, Hector Garca-Molina, and Anthony Tomasic. </author> <title> Precision and recall of gloss estimators for database discovery. </title> <booktitle> In Proceedings of the 3rd international Conference on Parallel and Distributed Information Systems (PDIS'94), </booktitle> <year> 1994. </year>
Reference-contexts: Thus, Yahoo's results are typically more relevant, but at the cost of returning only a fraction of the quantity that Web Crawler returns. The availability of many search engines leads to the text-database discovery problem <ref> [8] </ref>. In most information retrieval, a corpus of documents is the target of queries; in the text-database discovery problem, the corpus of documents is reached through other databases. <p> Thus a query is matched not to a set of documents but rather to a set of databases or, in this case, search engines. The GlOSS (Glossary-of-Servers Server) project <ref> [8] </ref> has suggested a solution to the text-database discovery problem. A meta-index is constructed by integrating the indexes of each of the databases. For each database and each word, the number of documents containing that word is included in the meta-index. <p> This may add a prohibitive amount of administrative complexity as the number of databases increases. While our solution involves a meta-index similar to that described in <ref> [8] </ref>, it differs in that no assumption is made about the availability of search engine indexes, which are often unavailable.
Reference: [9] <author> Kristian Hammond, Robin Burke, Charles Martin, and Steve Lytinen. FAQFinder: </author> <title> A case-based approach to knowledge navigation. </title> <editor> In Craig Knoblock and Alon Levy, editors, </editor> <booktitle> Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: Some of the agents focused on the Web as an information source; agents have been designed for filtering, browsing [2,13], and traversing the Web [1], as well as searching specific information on it (e.g., FAQ files for newsgroups <ref> [9] </ref>) and heterogeneous sources [17]. Agents often reference domain dependent databases and utilize a complex logical or semantic domain model [17,11,6]. For example, [6] describes a distributed purchasing agent that aids in location and pricing of products. An internal predicate logic repre 1 sentation relates product descriptions to information domains.
Reference: [10] <author> Craig Knoblock and Alon Levy, </author> <title> editors. </title> <booktitle> Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: Last year's Spring Symposium on Information Gathering included a large number of papers on diverse agents <ref> [10] </ref>. Some of the agents focused on the Web as an information source; agents have been designed for filtering, browsing [2,13], and traversing the Web [1], as well as searching specific information on it (e.g., FAQ files for newsgroups [9]) and heterogeneous sources [17].
Reference: [11] <author> Craig A. Knoblock. </author> <title> Integrating planning and execution for information gathering. </title> <booktitle> In Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference: [12] <author> Martijn Koster. </author> <title> Configurable unified search engine (cusi). </title> <note> http://pubweb.nexor.co.uk/public/cusi/doc/about.html. 16 </note>
Reference: [13] <author> Henry Lieberman. Letizia: </author> <title> An agent that assists web browsing. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 924-929, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference: [14] <author> Steve Madere. </author> <note> DejaNews research service. http://dejanews3.dejanews.com/. </note>
Reference-contexts: 1 Information Gathering on the Web Without help, finding a specific site or particular information among the deluge of informally connected sites on the World Wide Web is difficult. Tools for searching the Web, such as Lycos [15] and DejaNews <ref> [14] </ref>, match user queries to Web resources of interest. These programs act as directories and indexes to the Web, considerably alleviating the navigational difficulties.
Reference: [15] <author> Michael Mauldin. Lycos, </author> <title> the catalog of the internet. </title> <address> http://www.lycos.com/. </address>
Reference-contexts: 1 Information Gathering on the Web Without help, finding a specific site or particular information among the deluge of informally connected sites on the World Wide Web is difficult. Tools for searching the Web, such as Lycos <ref> [15] </ref> and DejaNews [14], match user queries to Web resources of interest. These programs act as directories and indexes to the Web, considerably alleviating the navigational difficulties.
Reference: [16] <author> Michael L. Mauldin and John R. R. Leavitt. </author> <title> Web agent related research at the center for machine translation. </title> <booktitle> Presented at the SIGNIDR meeting, August 4, 1994 in McLean, </booktitle> <address> Virginia, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Each search tool is limited by the sub-corpus of documents known to it and by its ability to find relevant documents within that corpus. It has been suggested that search engines should not attempt to thoroughly index the entire Web space, as that will lead to much repeated effort <ref> [16] </ref>. A user with a specific information need will often need to query several search engines before finding relevant documents. To address the problem of navigating the search engines, we have developed a meta-search agent, called SavvySearch, that directs queries to search engines judged to be most appropriate.
Reference: [17] <author> Tim Oates, M. V. NagendraPrasad, and Victor R. Lesser. </author> <title> Cooperative information gathering: A distributed problem solving approach. </title> <type> Technical report, </type> <institution> University of Massachusetts, </institution> <year> 1994. </year>
Reference-contexts: Some of the agents focused on the Web as an information source; agents have been designed for filtering, browsing [2,13], and traversing the Web [1], as well as searching specific information on it (e.g., FAQ files for newsgroups [9]) and heterogeneous sources <ref> [17] </ref>. Agents often reference domain dependent databases and utilize a complex logical or semantic domain model [17,11,6]. For example, [6] describes a distributed purchasing agent that aids in location and pricing of products. An internal predicate logic repre 1 sentation relates product descriptions to information domains.
Reference: [18] <author> Brian Pinkerton. </author> <note> Web Crawler home page. http://webcrawler.com/. </note>
Reference-contexts: Alternatively, if you have used the World Wide Web, you have no doubt encountered some of the myriad search services, such as Yahoo [7] and Web Crawler <ref> [18] </ref>. These powerful search engines can aid the process of searching on the Web. Search results are displayed in the form of another Web document consisting of a list of links that are deemed relevant to the search query.
Reference: [19] <author> Erik Selberg and Oren Etzioni. </author> <title> The MetaCrawler WWW search engine. </title> <address> http://metacrawler.cs.washington.edu:8080/home.html. </address>
Reference-contexts: It is conceivable to retrieve the full text of all referenced documents and perform additional conventional information retrieval on them. This is a particularly appealing approach if waiting time is not a priority and has been implemented effectively in the MetaCrawler system <ref> [19] </ref>. 3.2 Acquiring Supporting Knowledge The supporting knowledge includes static and dynamic knowledge about costs and the successes of queries. Thus far, we have measured cost based on network load, local CPU usage, and expected quality of information.
Reference: [20] <author> Erik Selberg and Oren Etzioni. </author> <title> Multi-service search and comparison using the MetaCrawler. </title> <booktitle> In Proceedings of the 4th International World Wide Web Conference, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Furthermore, the user must interpret the heterogeneous results which appear in the native formats of the various search engines. These shortcomings motivate true meta-search engines, such as SavvySearch (the system described here) and MetaCrawler <ref> [20] </ref>. MetaCrawler distributes user queries in parallel to all search engines within its knowledge base. The full text of each result is retrieved from the Web and then ranked using conventional information retrieval techniques. An added benefit of retrieving the documents is the ability to apply a sophisticated query language.
Reference: [21] <author> Shlomo Zilberstein. </author> <title> An anytime computation approach to information gathering. </title> <booktitle> In Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments, </booktitle> <address> Palo Alto, CA, </address> <year> 1995. </year> <month> 17 </month>
Reference-contexts: Resource reasoning is included to balance network resource consumption against response quality <ref> [21] </ref>. Much information can be obtained from a query; for example, we can look at properties of specific search terms used, or we can introduce ancillary query components, such as the general nature of information sought.
References-found: 21

