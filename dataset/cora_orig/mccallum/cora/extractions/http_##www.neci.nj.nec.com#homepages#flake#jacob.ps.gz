URL: http://www.neci.nj.nec.com/homepages/flake/jacob.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/flake/
Root-URL: http://www.neci.nj.nec.com
Email: flake@neci.nj.nec.com  
Title: Optimizing the Properties of the Jacobian Matrix of Doubly Differentiable Systems  
Author: Gary William Flake 
Date: November 23, 1998  
Address: 4 Independence Way Princeton, NJ 08540  
Affiliation: NEC Research Institute  
Abstract: For many problems, "correct" behavior of a model must be expressed not only as a function of a model's input-output mapping but also of the model's Jacobian matrix (the matrix of partial derivatives of the model outputs with respect to the model inputs evaluated at a particular input point). This paper introduces the J-prop learning algorithm, which is a general method for rapidly computing the exact partial derivatives of simple functions of the Jacobian matrix of a model with respect to the weights, where the term "model" is meant to include nonlinear regression, multilayer perceptrons, radial basis function networks, and any other type of doubly differentiable data-driven model class. J-prop has as special cases some earlier algorithms, such as Tangent Prop and Double Backpropagation, but it can also be used to optimize the eigenvectors, eigenvalues, and the determinant of the Jacobian matrix, making it very general with respect to the model types, applications, and objective function that can be used. Possible applications of J-prop include forcing stability and sensitivity conditions, regularization, building low-complexity encoder-decoder networks, exploiting a priori knowledge in domain specific problems, designing controllers, and even blind source separation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Deco and D. Obradovic. </author> <title> An Information Theoretic Approach to Neural Computing. Perspective in Neural Computing. </title> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: Optimizing the determinate of the Jacobian of a nonlinear systems (or, more accurately, its logarithm) is known to have many applications in the area of independent component analysis (ICA) and blind source separation (BSS) <ref> [1, 7] </ref>. The J-prop algorithm may be able to extend existing linear ICA and PCA algorithms into nonlinear versions. 5 Conclusions and Future Work This paper has introduced a general method for calculating the weight gradient of simple functions of the Jacobian matrix of nonlinear systems that are twice differentiable.
Reference: [2] <author> G. Deco and B. Schurmann. </author> <title> Dynamic modeling of chaotic time series. </title> <editor> In Russell Greiner, Thomas Petsche, and Stephen Jose Hanson, editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, volume IV of Making Learning Systems Practical, chapter 9, </booktitle> <pages> pages 137-153. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1997. </year>
Reference-contexts: Of course, the caveat here is the requirements on the amount and integrity of the training data, which are rarely found in real-world applications. This problem was specifically demonstrated by Principe, Rathie and Kuo [10] and Deco and Schurmann <ref> [2] </ref>, who showed that using noisy training data from chaotic systems can lead to models that are accurate in the input-output sense, but inaccurate in their estimates of quantities related to the Jacobian of the unknown system such as the largest Lyapunov exponent and the correlation dimension.
Reference: [3] <author> H. Drucker and Y. Le Cun. </author> <title> Improving generalization performance using double backpropagation. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(6), </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: Weight decay is arguably optimal for models in which the output is linear in the weights because minimizing the magnitude of the weights is equivalent to minimizing the magnitude of the model's first derivatives. However, in the nonlinear case, weight decay can have suboptimal performance <ref> [3] </ref> because large (or small) weights do not always correspond to having large (or small) first derivatives. Training with noisy inputs [13] is another method often used to regularize the first derivatives of a model. The simplest methods add a fixed amount of noise to the training data input vectors. <p> When compared to weight decay, training with noise has the advantage that it explicitly penalizes large first derivatives of the model; however, it is strictly a numerical approximation to something can be done analytically. For example, the Double Backpropagation algorithm <ref> [3] </ref> adds an additional penalty term to the error function equal to jj@E=@xjj 2 . <p> to the tasks of lifelong learning [16] and explanation-based learning [15] which both have the goal of training models to be consistent with earlier models (and are, therefore, similar in spirit to the boot-strap method described in the previous subsection). 4.3 Double Backpropagation Like Tangent Prop, the Double Backpropagation algorithm <ref> [3] </ref> uses an error function that is a linear combi nation of the usual L 2 norm on the output residuals plus an L 2 norm the first derivative of the normal error 8 function respect to the model inputs.
Reference: [4] <author> G. W. Flake. </author> <title> A poor man's approach to controlling chaos with neural networks. </title> <booktitle> In The Proceedings of the World Congress on Neural Networks, </booktitle> <pages> 95. </pages>
Reference-contexts: In any event, deflating the largest eigenvalue has been shown to be an effective procedure for controlling chaotic systems <ref> [6, 4] </ref>. 4.7 Optimizing the Determinant Many applications require one to optimize the determinant of a system's n fi n Jacobian matrix.
Reference: [5] <author> G. E. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Proc. Eigth Annual Conf. Cognitive Science Society, </booktitle> <pages> pages 1-12, </pages> <address> Hillsdale, NJ, 1986. </address> <publisher> Erlbaum. </publisher>
Reference-contexts: Several methods to combat this type of over-fitting have been proposed. One of the earliest methods, weight decay <ref> [5] </ref>, uses a penalty term on the magnitude of the weights. Weight decay is arguably optimal for models in which the output is linear in the weights because minimizing the magnitude of the weights is equivalent to minimizing the magnitude of the model's first derivatives.
Reference: [6] <author> E. Ott, C. Grebogi, and J.A. Yorke. </author> <title> Controlling chaotic dynamical systems. </title> <editor> In D.K. Campbell, editor, </editor> <booktitle> CHAOS: Soviet-American Perspectives on Nonlinear Science, </booktitle> <pages> pages 153-172. </pages> <institution> American Institute of Physics, </institution> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In any event, deflating the largest eigenvalue has been shown to be an effective procedure for controlling chaotic systems <ref> [6, 4] </ref>. 4.7 Optimizing the Determinant Many applications require one to optimize the determinant of a system's n fi n Jacobian matrix.
Reference: [7] <author> L. Parra, G. Deco, and S. Miesbach. </author> <title> Redundancy reduction with information preserving nonlinear maps. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 6 </volume> <pages> 61-72, </pages> <year> 1995. </year>
Reference-contexts: Optimizing the determinate of the Jacobian of a nonlinear systems (or, more accurately, its logarithm) is known to have many applications in the area of independent component analysis (ICA) and blind source separation (BSS) <ref> [1, 7] </ref>. The J-prop algorithm may be able to extend existing linear ICA and PCA algorithms into nonlinear versions. 5 Conclusions and Future Work This paper has introduced a general method for calculating the weight gradient of simple functions of the Jacobian matrix of nonlinear systems that are twice differentiable.
Reference: [8] <author> Barak A. Pearlmutter. </author> <title> Fast exact multiplication by the Hessian. </title> <journal> Neural Computation, </journal> <volume> 6(1) </volume> <pages> 147-160, </pages> <year> 1994. </year>
Reference-contexts: The method is very similar to a technique introduced by Pearlmutter <ref> [8] </ref> for 4 calculating the product of the Hessian of an MLP and arbitrary vector. This proposed method is presented in five steps. First, we will define an auxiliary error function that has a few useful mathematical properties that simplify the derivation.
Reference: [9] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <journal> Proc. IEEE, </journal> <volume> 78(9) </volume> <pages> 1481-1497, </pages> <month> Sept </month> <year> 1990. </year>
Reference-contexts: While this improved results, the solution is only applicable to the domain of MLPs and chaotic systems. Early in their development of radial basis function networks (RBFNs) Poggio and Girosi <ref> [9] </ref> showed that RBFNs have the desirable property that one can effectively place a bound on the errors of all of the higher derivatives of the model.
Reference: [10] <author> J. Principe, A. Rathie, and J. Kuo. </author> <title> Prediction of chaotic time series with neural networks and the issues of dynamic modeling. Bifurcations and Chaos, </title> <address> 2(4):989, </address> <year> 1992. </year>
Reference-contexts: Of course, the caveat here is the requirements on the amount and integrity of the training data, which are rarely found in real-world applications. This problem was specifically demonstrated by Principe, Rathie and Kuo <ref> [10] </ref> and Deco and Schurmann [2], who showed that using noisy training data from chaotic systems can lead to models that are accurate in the input-output sense, but inaccurate in their estimates of quantities related to the Jacobian of the unknown system such as the largest Lyapunov exponent and the correlation
Reference: [11] <author> R. Rico-Martinez, J. S. Anderson, and I. G. Kevrekidis. </author> <title> Self-consistency in neural network-based nlpc analysis with applications to time-series processing. Private Communication. (Need to get proper citation). </title>
Reference-contexts: An encoder-decoder network is said to be consistent <ref> [11] </ref> if for all inputs, x, f d (f e (x)) = f d (f e (f d (f e (x)))); that is, when used as an input, the output of the network maps back to itself, which is always true in the case of PCA.
Reference: [12] <author> H. S. Seung. </author> <title> Learning continuous attractors in recurrent networks. </title> <editor> In M. I. Jordan, M. J. Kearns, and S. A. Solla, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 10. </volume> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: One necessary (but not necessarily sufficient) condition for consistency is @ 1 jjy xjj = @y 2 2 J T e = 0: Moreover, a related problem is found when attempting to train networks similar to encoder-decoder networks to have stable attractors and, therefore, act as continuous associative memories <ref> [12] </ref>.
Reference: [13] <author> J. Sietsma and R. J. F. Dow. </author> <title> Neural network pruning|why and how. </title> <booktitle> In Proc. IEEE Int. Conf. Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 325-333. </pages> <publisher> IEEE, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: However, in the nonlinear case, weight decay can have suboptimal performance [3] because large (or small) weights do not always correspond to having large (or small) first derivatives. Training with noisy inputs <ref> [13] </ref> is another method often used to regularize the first derivatives of a model. The simplest methods add a fixed amount of noise to the training data input vectors.
Reference: [14] <author> P. Simard, B. Victorri, Y. Le Cun, and J. Denker. </author> <title> Tangent prop|A formalism for specifying selected invariances in an adaptive network. </title> <editor> In John E. Moody, Steve J. Hanson, and Richard P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 895-903. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1992. </year>
Reference-contexts: Double Backpropagation can be seen as a special case of J-prop, the algorithm derived in the next section. As to the general problem of coercing the first derivatives of a model to specific values, Simard, et al., <ref> [14] </ref> introduced the Tangent Prop algorithm, which was used to train MLPs for optical character recognition to be insensitive to small affine transformations in the character space. Tangent Prop is very similar to the algorithm proposed in this paper but differs in several respects. <p> In this way, incremental learning can be performed in such a way that subsequent models are forced to have first derivatives that are consistent with earlier models. 4.2 Tangent Prop The Tangent Prop algorithm <ref> [14] </ref> is designed to train neural networks in such a way that they learn invariances relative to a classification task. For example, if a classifier is trained to recognize letters of the alphabet, then the optimal classifier would be invariant in small translations, rotations, and scalings of the input images.
Reference: [15] <author> S. B. Thrun and T. M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In the Proc. of the Thirteenth Int. Joint Conf. on Art. Int. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <month> 12 </month>
Reference-contexts: The invariance approach can be applied to any domain in which a classification task has known invariance properties. However, the basic idea was also expanded by Thrun and Mitchell and applied to the tasks of lifelong learning [16] and explanation-based learning <ref> [15] </ref> which both have the goal of training models to be consistent with earlier models (and are, therefore, similar in spirit to the boot-strap method described in the previous subsection). 4.3 Double Backpropagation Like Tangent Prop, the Double Backpropagation algorithm [3] uses an error function that is a linear combi nation
Reference: [16] <author> S. B. Thrun and T. M. Mitchell. </author> <title> Learning one more thing. </title> <booktitle> In the Proc. of the Fourteenth Joint Conf. on Art. Int. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: The invariance approach can be applied to any domain in which a classification task has known invariance properties. However, the basic idea was also expanded by Thrun and Mitchell and applied to the tasks of lifelong learning <ref> [16] </ref> and explanation-based learning [15] which both have the goal of training models to be consistent with earlier models (and are, therefore, similar in spirit to the boot-strap method described in the previous subsection). 4.3 Double Backpropagation Like Tangent Prop, the Double Backpropagation algorithm [3] uses an error function that is
Reference: [17] <author> A. S. Weigend, H. G. Zimmermann, and R. Neuneier. Clearning. </author> <booktitle> In Neural Networks in Financial Engineering, </booktitle> <address> (NNCM95). </address> <year> 1995. </year>
Reference-contexts: Training with noisy inputs [13] is another method often used to regularize the first derivatives of a model. The simplest methods add a fixed amount of noise to the training data input vectors. More complicated versions, such as cleaning <ref> [17] </ref>, adaptively learn a simple noise model on the input space which allows for adaptation to be specific to the noise levels in the training data.
Reference: [18] <author> H. White and A. R. Gallant. </author> <title> On learning the derivatives of an unknown mapping with multilayer feedforward networks. </title> <editor> In Halbert White, editor, </editor> <booktitle> Artificial Neural Networks, chapter 12, </booktitle> <pages> pages 206-223. </pages> <publisher> Blackwell, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year>
Reference-contexts: Finally, Section 5 contains a summary and describes future work. 2 Background Previous work concerning the modeling of an unknown function and its derivatives can be divided into works that are descriptive or prescriptive. Perhaps the best known descriptive result is due to White, et al. <ref> [18, 19] </ref>, who show that a multilayer perceptron (MLP) can approximate the higher derivatives of an unknown function in the limit as the number of training points goes to infinity and with the assumption of zero noise.
Reference: [19] <author> H. White, K. Hornik, and M. Stinchcombe. </author> <title> Universal approximation of an unknown mapping and its derivative. </title> <editor> In Halbert White, editor, </editor> <booktitle> Artificial Neural Networks, chapter 6, </booktitle> <pages> pages 55-77. </pages> <publisher> Blackwell, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year> <month> 13 </month>
Reference-contexts: Finally, Section 5 contains a summary and describes future work. 2 Background Previous work concerning the modeling of an unknown function and its derivatives can be divided into works that are descriptive or prescriptive. Perhaps the best known descriptive result is due to White, et al. <ref> [18, 19] </ref>, who show that a multilayer perceptron (MLP) can approximate the higher derivatives of an unknown function in the limit as the number of training points goes to infinity and with the assumption of zero noise.
References-found: 19

