URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/waiting-algs.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/waiting-algs.html
Root-URL: 
Title: Waiting Algorithms for Synchronization in Large-Scale Multiprocessors  
Author: Beng-Hong Lim and Anant Agarwal 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Through analysis and experiments, this paper investigates two-phase waiting algorithms to minimize the cost of waiting for synchronization in large-scale multiprocessors. In a two-phase algorithm, a thread first waits by polling a synchronization variable. If the cost of polling reaches a limit L poll and further waiting is necessary, the thread is blocked, incurring an additional fixed cost, B. The choice of L poll is a critical determinant of the performance of two-phase algorithms. We focus on methods for statically determining L poll because the run-time overhead of dynamically determining L poll can be comparable to the cost of blocking in large-scale multiprocessor systems with lightweight threads. Our experiments show that always-block (L poll = 0) is a good waiting algorithm with performance that is usually close to the best of the algorithms compared. We show that even better performance can be achieved with a static choice of L poll based on knowledge of likely wait-time distributions. Motivated by the observation that different synchronization types exhibit different wait-time distributions, we prove that a static choice of L poll can yield close to optimal on-line performance against an adversary that is restricted to choosing wait times from a fixed family of probability distributions. This result allows us to make an optimal static choice of L poll based on synchronization type. For exponentially distributed wait times, we prove that setting L poll = ln(e 1)B results in a waiting cost that is no more than e=(e 1) times the cost of an optimal off-line algorithm. For uniformly distributed wait times, we prove that setting L poll = 1 2 ( 5 1)B results in a waiting cost that is no more than ( p 5 + 1)=2 (the golden ratio) applications on the Alewife multiprocessor simulator corroborate our theoretical findings. times the cost of an optimal off-line algorithm. Experimental measurements of several parallel
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and M. Cherian. </author> <title> Adaptive Backoff Synchronization Techniques. </title> <booktitle> In Proceedings 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 396-406, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: As pointed out in Section 1, this is because we investigate producer-consumer and barrier synchronization in addition to mutual-exclusion synchronization, and because of the difference in the machine architectures and blocking costs. Other studies <ref> [1, 4, 11, 23] </ref> have focused on reducing bus (or network) interference caused when spinning is used as a waiting mechanism. These studies explored methods to reduce the overhead of memory contention while spin waiting for locks and barriers.
Reference: [2] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: It accomplishes this by rapidly switching control of the processor to a different thread whenever a high latency operation is encountered. While previous multithreaded designs switch contexts at every cycle [27, 12], Alewife's multithreaded processor <ref> [2] </ref> switches contexts only on synchronization faults and remote cache misses. This style is called block multithreading [20] and has the advantage of high single thread performance. Since multithreading introduces novel methods for manipulating threads, we define the following terms to avoid ambiguity. <p> Nodes are connected via a two-dimensional mesh network. The memory controller synthesizes a globally-shared address space and maintains cache coherence using the LimitLESS directory protocol [9]. A description of Alewife's processor, Sparcle, can be found in <ref> [2] </ref>. Sparcle is designed to meet several requirements significant to multiprocessing: it tolerates latencies through block multithread-ing, and it handles traps efficiently through a rapid-trap-dispatch mechanism. Sparcle has four hardware contexts so that multiple threads co-reside on a single processor.
Reference: [3] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubi-atowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: With signalling, the waiting thread suspends execution and relinquishes control of the processor until the synchronization condition is satisfied. Traditionally, multiprocessors provide spinning and blocking as mechanisms for polling and signalling respectively. Multithreaded multiprocessors, such as Alewife <ref> [3] </ref>, additionally provide more efficient polling and signalling mechanisms called switch-spinning and switch-blocking, which will be described in Section 2. A waiting algorithm chooses among available waiting mechanisms during synchronization faults. A common waiting algorithm used in shared-memory multiprocessors is to always-spin. <p> The first two mechanisms are commonly found in traditional multiprocessors, while the last two are additional mechanisms provided by multithreaded multiprocessors. To provide firmer grounding, we describe these mechanisms in the context of the MIT Alewife multiprocessor <ref> [3] </ref>, a distributed-memory multiprocessor supporting the shared-memory programming model. We begin with a brief overview of multithread-ing. Multithreading is commonly prescribed as a method for tolerating latencies and increasing processor utilization in a large-scale multiprocessor.
Reference: [4] <author> Thomas E. Anderson. </author> <title> The Performance Implications of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: As pointed out in Section 1, this is because we investigate producer-consumer and barrier synchronization in addition to mutual-exclusion synchronization, and because of the difference in the machine architectures and blocking costs. Other studies <ref> [1, 4, 11, 23] </ref> have focused on reducing bus (or network) interference caused when spinning is used as a waiting mechanism. These studies explored methods to reduce the overhead of memory contention while spin waiting for locks and barriers.
Reference: [5] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <booktitle> In Proceedings of the Workshop on Graph Reduction, (Springer-Verlag Lecture Notes in Computer Science 279), </booktitle> <pages> pages 336-369, </pages> <month> September/October </month> <year> 1986. </year>
Reference-contexts: Adaptive algorithms incur substantial run-time overheads in large-scale machines with fine-grained synchronization because they need to maintain histories of wait times at each of the many synchronization locations. Furthermore, they are not suitable for single-assignment synchronization types like I-structures <ref> [5] </ref> because of the absence of wait time histories for the synchronization locations. In using static two-phase algorithms, we have transformed the problem of choosing between spinning and blocking to the problem of deciding the appropriate value for L poll . <p> We consider three types of synchronization: producer-consumer, barrier, and mutual exclusion. Producer-consumer synchronization is performed between one producer and one or more consumers of the data produced. 2 Examples of this type of synchronization include futures [13] and I-structures <ref> [5] </ref>. Barrier synchronization ensures that all threads participating in a barrier have reached a point in a program before proceeding. Mutual-exclusion synchronization is used to provide exclusive access to data structures and critical sections of code. <p> An empty vector slot doubles as the queue pointer for waiting readers. An error is signalled if a write is attempted on a full slot. J-structures can be used to implement I-structure <ref> [5] </ref> semantics. We allow a J-structure slots to be reset. A reset empties the slot, permitting multiple assignments. Reusing J-structure slots in this way allows efficient cache performance. However, depending on the application, the programmer is responsible for proper synchronization between readers and resetters of a slot.
Reference: [6] <author> J. Aspnes, M.P. Herlihy, and N. Shavit. </author> <title> Counting Networks and Multi-Processor Coordination. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Theory of Computing, </booktitle> <pages> pages 348-358, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: These accumulates and broadcasts also serve as barriers between phases. 19 Jacobi-Bar solves exactly the same problem as Jacobi, but with a global barrier synchronization between iterations. Like in Jacobi, only nearest neighbor communication is necessary within an iteration. Mutual Exclusion CountNet tests an implementation of a counting network <ref> [6] </ref>. Threads repeatedly try to increment the value of a counter through a bitonic counting network so as to reduce contention and allow parallelism. Threads acquire and release mutexes at each network node as they traverse the network.
Reference: [7] <author> Paul S. Barth, Rishiyur S. Nikhil, and Arvind. M-structures: </author> <title> Extending a parallel, non-strict, functional language with state. </title> <booktitle> In Proceedings of the 5th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 538-568, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: A non-locking read returns the value found in a slot if full; otherwise it returns an invalid value. An L-structure therefore allows mutually exclusive access to each of its slots. The locking and unlocking L-structure reads and writes are sufficient to implement M-structures <ref> [7] </ref>. L-structures are different from M-structures in that they allow multiple non-locking readers. Semaphores A semaphore is implemented as a one-element L-structure. semaphore-p and semaphore-v are easily implemented using L-structure reads and writes. Semaphores are used to implement mutual-exclusion.
Reference: [8] <author> B. Bershad. </author> <title> Practical considerations for lock-free concurrent objects. </title> <type> Technical Report CMU-CS-91-183, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> Sept </month> <year> 1991. </year>
Reference-contexts: However, these methods can be useful in the polling phase of two-phase algorithms to reduce the detrimental effects of contention. Several researchers have recently been advocating the use of lock-free methods for synchronization <ref> [8, 14, 30] </ref>, using load-linked/store-conditional [17] or compare-and-swap [16] as primitives. In the context of this paper, lock-free synchronization is a form of optimistic polling, and the results in this paper apply.
Reference: [9] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV)., </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: Nodes are connected via a two-dimensional mesh network. The memory controller synthesizes a globally-shared address space and maintains cache coherence using the LimitLESS directory protocol <ref> [9] </ref>. A description of Alewife's processor, Sparcle, can be found in [2]. Sparcle is designed to meet several requirements significant to multiprocessing: it tolerates latencies through block multithread-ing, and it handles traps efficiently through a rapid-trap-dispatch mechanism.
Reference: [10] <author> H. Davis and J. Hennessy. </author> <title> Characterizing the Synchronization Behavior of Parallel Programs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(9) </volume> <pages> 198-211, </pages> <month> September </month> <year> 1988. </year> <month> 29 </month>
Reference-contexts: 1 Introduction Threads executing on a multiprocessor synchronize to ensure program correctness. As multipro cessors scale in size, the grain size of threads will decrease to satisfy higher parallelism require ments <ref> [10] </ref>, causing a corresponding increase in synchronization rates and in the frequency of waits due to synchronization. Waiting threads waste processor cycles and incur a cost that is related not only to the wait times encountered but also to the efficiency of the waiting algorithm.
Reference: [11] <author> Gary Graunke and Shreekant Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Mul--tiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 60-70, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: As pointed out in Section 1, this is because we investigate producer-consumer and barrier synchronization in addition to mutual-exclusion synchronization, and because of the difference in the machine architectures and blocking costs. Other studies <ref> [1, 4, 11, 23] </ref> have focused on reducing bus (or network) interference caused when spinning is used as a waiting mechanism. These studies explored methods to reduce the overhead of memory contention while spin waiting for locks and barriers.
Reference: [12] <author> R.H. Halstead and T. Fujita. MASA: </author> <title> A Multithreaded Processor Architecture for Parallel Symbolic Computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <address> New York, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: Multithreading is commonly prescribed as a method for tolerating latencies and increasing processor utilization in a large-scale multiprocessor. It accomplishes this by rapidly switching control of the processor to a different thread whenever a high latency operation is encountered. While previous multithreaded designs switch contexts at every cycle <ref> [27, 12] </ref>, Alewife's multithreaded processor [2] switches contexts only on synchronization faults and remote cache misses. This style is called block multithreading [20] and has the advantage of high single thread performance. Since multithreading introduces novel methods for manipulating threads, we define the following terms to avoid ambiguity.
Reference: [13] <author> Robert H. Halstead. </author> <title> Multilisp: A Language for Parallel Symbolic Computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-539, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: We consider three types of synchronization: producer-consumer, barrier, and mutual exclusion. Producer-consumer synchronization is performed between one producer and one or more consumers of the data produced. 2 Examples of this type of synchronization include futures <ref> [13] </ref> and I-structures [5]. Barrier synchronization ensures that all threads participating in a barrier have reached a point in a program before proceeding. Mutual-exclusion synchronization is used to provide exclusive access to data structures and critical sections of code. <p> L-structures are different from M-structures in that they allow multiple non-locking readers. Semaphores A semaphore is implemented as a one-element L-structure. semaphore-p and semaphore-v are easily implemented using L-structure reads and writes. Semaphores are used to implement mutual-exclusion. Futures Futures specify parallelism in Multilisp <ref> [13] </ref> and synchronization on the return values of the threads. It is a form of producer-consumer synchronization. The future object is simply a memory word that initially holds the queue of waiting consumers and eventually holds the value of the future when resolved.
Reference: [14] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data structures. </title> <booktitle> In Second ACM SIGPLAN Synposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 197-206, </pages> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: However, these methods can be useful in the polling phase of two-phase algorithms to reduce the detrimental effects of contention. Several researchers have recently been advocating the use of lock-free methods for synchronization <ref> [8, 14, 30] </ref>, using load-linked/store-conditional [17] or compare-and-swap [16] as primitives. In the context of this paper, lock-free synchronization is a form of optimistic polling, and the results in this paper apply.
Reference: [15] <author> Qin Huang. </author> <title> An Analysis of Concurrent Priority Queue Algorithms. </title> <type> Master's thesis, </type> <institution> EECS Department, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Threads acquire and release mutexes at each network node as they traverse the network. FibHeap tests an implementation of a scalable priority queue based on a Fibonacci heap <ref> [15] </ref>. Mutexes are used to ensure atomic updates to the heap and scalability is achieved by distributing mutexes throughout the data structure to avoid points of high lock contention and allow parallelism. The test involves repeatedly executing insert and extract-min operations on the priority queue.
Reference: [16] <institution> IBM System/370 Principles of Operation. IBM, Order Number GA22-7000. </institution>
Reference-contexts: However, these methods can be useful in the polling phase of two-phase algorithms to reduce the detrimental effects of contention. Several researchers have recently been advocating the use of lock-free methods for synchronization [8, 14, 30], using load-linked/store-conditional [17] or compare-and-swap <ref> [16] </ref> as primitives. In the context of this paper, lock-free synchronization is a form of optimistic polling, and the results in this paper apply.
Reference: [17] <author> E. Jensen, G. Hagensen, and J. Broughton. </author> <title> A new approach to exclusive data access in shared memory multiprocessors. </title> <type> Technical Report UCRL-97663, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: However, these methods can be useful in the polling phase of two-phase algorithms to reduce the detrimental effects of contention. Several researchers have recently been advocating the use of lock-free methods for synchronization [8, 14, 30], using load-linked/store-conditional <ref> [17] </ref> or compare-and-swap [16] as primitives. In the context of this paper, lock-free synchronization is a form of optimistic polling, and the results in this paper apply.
Reference: [18] <author> A. Karlin, K. Li, M. Manasse, and S. Owicki. </author> <title> Empirical Studies of Competitive Spinning for a Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: It extends earlier work in significant ways by combining analysis and experimentation. Previous empirical work <ref> [18] </ref> focused on waiting for spin-locks on small-scale bus-based machines. <p> On the other hand, always-block is found to be generally efficient, except in one case where wait times were mostly shorter than B. This observation is contrary to results recently reported in <ref> [18] </ref>, and is due to several factors. The study in [18] is focused on waiting for mutual-exclusion locks on a small bus-based multiprocessor. On such a machine, communication latencies are shorter than in network-based multiprocessors. <p> On the other hand, always-block is found to be generally efficient, except in one case where wait times were mostly shorter than B. This observation is contrary to results recently reported in <ref> [18] </ref>, and is due to several factors. The study in [18] is focused on waiting for mutual-exclusion locks on a small bus-based multiprocessor. On such a machine, communication latencies are shorter than in network-based multiprocessors. <p> Zahorjan et al. show that lock wait times are not significantly affected by run-time factors if lock holders are never descheduled. This allows wait-time profiles to be accurate predictors of future wait times. In <ref> [18] </ref>, Karlin et al. found that a static two-phase algorithm, with L poll based on wait-time profiles of mutual-exclusion synchronization, had the best performance among the algorithms they considered. 5 Experimental Framework To show that static two-phase waiting algorithms work well in practice and to corroborate the analysis of the previous <p> Program execution time measurements are also compared because it is the ultimate performance metric. 3 This is a different measure from that used in <ref> [18] </ref>, where mutex wait time was measured from the first failed request to the time the mutex is released by the holder. 18 Parameter Default setting Number of Processors 64 Cache Coherence Protocol LimitLESS 1 Cache Size 64KB (4096 blocks) Cache Block Size 16 bytes Network Topology 2-D Mesh Network Channel <p> We observe that a shorter polling phase results in better performance than Ss=b=1 in MGrid and Jacobi because producer arrival rates were low. Under such conditions, i.e., when &lt; 1=fiB, our theoretical analysis predicts that Ss=b=0:5 will perform better than Ss=b=1. In <ref> [18] </ref>, Karlin et al. also observed by analyzing measured wait-time profiles that setting L poll to 0:5B can result in lower waiting costs. Surprisingly, Ss=b=0:5 also performed better than =b=0. <p> In recent work, Karlin et al. <ref> [18] </ref> empirically studied the performance of two-phase waiting algorithms for mutual-exclusion locks on a small bus-based machine. They investigated both static and dynamic methods for choosing L poll .
Reference: [19] <author> A. Karlin, M. Manasse, L. McGeoch, and S. Owicki. </author> <title> Competitive Randomized Algorithms for Non-Uniform Problems. </title> <booktitle> In Proceedings 1st Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 301-309, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: These allow the expression and efficient execution of fine-grain parallelism and lead to more frequent synchronization operations and shorter wait times compared to programs on small-scale machines that use less scalable synchronization methods. In previous analytical work, Karlin et al. <ref> [19] </ref> present an efficient randomized waiting algorithm that achieves an optimal on-line competitive factor of e=(e 1) against a weak adversary. (See 3 Section 4 for a definition of competitive factors, adversaries and optimality.) We prove in this paper that if we restrict the adversary by fixing the wait time distribution <p> An on-line algorithm is strongly competitive, and thus optimal, if it possesses the smallest possible competitive factor. The cost of a waiting algorithm depends on the sequence of wait times presented to it by an adversary. Using terminology in <ref> [19] </ref>, a strong adversary is one that chooses wait times in response to previous choices of L poll by the waiting algorithm. A weak adversary is one that chooses wait times without considering previous choices of L poll by the waiting algorithm. <p> When L poll = B, the worst possible scenario is to block after polling, incurring a cost of 2B, when the optimal off-line algorithm would have blocked immediately, incurring a cost B. If we weaken the adversary and consider expected costs we can achieve lower competitive factors. In <ref> [19] </ref>, Karlin et al. present a dynamic, randomized two-phase waiting algorithm with an expected competitive factor of e=(e 1) 1:58 and prove this factor to be optimal for on-line algorithms against a weak adversary. <p> It follows that one cannot construct a two-phase algorithm with a competitive factor lower than e=(e 1). This competitive factor matches the lower bound obtained in <ref> [19] </ref> against a weak adversary. 2 In light of this lower bound, the natural question to ask is whether a single static value for ff can attain this lower bound under exponentially distributed wait times.
Reference: [20] <author> Kiyoshi Kurihara, David Chaiken, and Anant Agarwal. </author> <title> Latency Tolerance through Multi-threading in Large-Scale Multiprocessors. </title> <booktitle> In Proceedings International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Japan, April 1991. </address> <publisher> IPS Press. </publisher>
Reference-contexts: While previous multithreaded designs switch contexts at every cycle [27, 12], Alewife's multithreaded processor [2] switches contexts only on synchronization faults and remote cache misses. This style is called block multithreading <ref> [20] </ref> and has the advantage of high single thread performance. Since multithreading introduces novel methods for manipulating threads, we define the following terms to avoid ambiguity. Hardware context A set of registers that implements the processor-resident state of a thread.
Reference: [21] <author> Beng-Hong Lim. </author> <title> Waiting Algorithms for Synchronization in Large-Scale Multiprocessors. </title> <institution> MIT/LCS TR-498, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> February </month> <year> 1991. </year> <title> S.M. </title> <type> Thesis. </type>
Reference-contexts: A more detailed description of these benchmarks can be found in <ref> [21] </ref>. Producer-Consumer MGrid applies the multigrid algorithm to solving Poisson's equation on a 2-D grid. Communication is nearest-neighbor except during shrink and expand phases. The 2-D grid is partitioned into subgrids, and a thread is assigned to each subgrid.
Reference: [22] <author> S. Lo and V. Gligor. </author> <title> A Comparative Analysis of Multiprocessor Scheduling Algorithms. </title> <booktitle> In 7th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 356-363. </pages> <publisher> IEEE, </publisher> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: His Medusa system implemented two-phase waiting with a user-settable L poll . In a later study of multiprocessor scheduling algorithms, Lo and Gligor <ref> [22] </ref> found that use of two-phase waiting (with L poll in between B and 2B) improved the performance of group scheduling. In a theoretical study of competitive algorithms, Karlin et al. presented a dynamic, randomized algorithm that achieves a competitive factor of e=(e 1).
Reference: [23] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: All the released waiters try to acquire the lock at once, exacerbating the wait times at that lock. Recently published techniques for more efficient spin-waiting on locks can be used to improve the performance of polling for highly contended locks <ref> [23] </ref>. <p> As pointed out in Section 1, this is because we investigate producer-consumer and barrier synchronization in addition to mutual-exclusion synchronization, and because of the difference in the machine architectures and blocking costs. Other studies <ref> [1, 4, 11, 23] </ref> have focused on reducing bus (or network) interference caused when spinning is used as a waiting mechanism. These studies explored methods to reduce the overhead of memory contention while spin waiting for locks and barriers.
Reference: [24] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead. </author> <title> Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: The synchronization structure of the program can be most easily viewed as a recursive function call tree with synchronizations occurring at each node of the tree. The program was dynamically partitioned with lazy task creation <ref> [24] </ref>. Queens solves the n-queens problem: given an n fi n chess board, place n queens such that no two queens are on the same row, column, or diagonal. <p> Ss=b=1 has the best overall performance among the three waiting algorithms. Ss==1 encounters deadlock and times out in unmatched MGrid and Jacobi and thus performs poorly. This problem with deadlock is not present for unmatched Queens and Factor because they are dynamically partitioned with lazy task creation <ref> [24] </ref>. =b=0 performs reasonably well except for matched Jacobi which has very short wait times. 6.2 Barrier Synchronization Because of their nature, wait times at barriers are likely to be long: a waiting thread is likely to be held up for a large number of other threads, especially in large-scale machines.
Reference: [25] <author> John K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30. </pages> <publisher> IEEE, </publisher> <year> 1982. </year> <month> 30 </month>
Reference-contexts: An algorithm that combines the advantages of polling and signalling is the two-phase waiting algorithm, first suggested by Ousterhout <ref> [25] </ref>. In a two 1 As anecdotal evidence, blocking was tried as a performance enhancer for a system routine in the DYNIX operating system for the Sequent multiprocessor, unexpectedly causing bad performance under certain conditions. This fact was subsequently used in an advertising campaign by a competitor. <p> In summary, the cost of blocking can be reduced to less than 100 cycles with hardware support for single-cycle load/stores, making a strong case for minimizing the run-time overhead of choosing L poll . 7 Related Work Two-phase waiting was first proposed by Ousterhout <ref> [25] </ref> who observed that blocking should be avoided if wait times are short, and suggested "pausing" a waiting process for some fixed time before blocking. His Medusa system implemented two-phase waiting with a user-settable L poll .
Reference: [26] <author> L. Rudolph and Z. Segall. </author> <title> Dynamic decentralized cache schemes for MIMD parallel processors. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1984. </year>
Reference-contexts: Other studies [1, 4, 11, 23] have focused on reducing bus (or network) interference caused when spinning is used as a waiting mechanism. These studies explored methods to reduce the overhead of memory contention while spin waiting for locks and barriers. In cases of high lock contention, simple test&test&set <ref> [26] </ref> leads to contention at the memory module during lock releases due to sudden bursts of waiting threads vying for the lock. Exponential backoff and software queueing were shown to be effective methods for reducing the contention at locks and barriers.
Reference: [27] <author> B.J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <booktitle> SPIE, </booktitle> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: Multithreading is commonly prescribed as a method for tolerating latencies and increasing processor utilization in a large-scale multiprocessor. It accomplishes this by rapidly switching control of the processor to a different thread whenever a high latency operation is encountered. While previous multithreaded designs switch contexts at every cycle <ref> [27, 12] </ref>, Alewife's multithreaded processor [2] switches contexts only on synchronization faults and remote cache misses. This style is called block multithreading [20] and has the advantage of high single thread performance. Since multithreading introduces novel methods for manipulating threads, we define the following terms to avoid ambiguity. <p> By default, context switching occurs in a round-robin fashion so that control eventually returns to each resident thread. 5.1 Hardware Support for Synchronization Alewife's primitive hardware mechanisms for synchronization are full/empty bits and efficient traps. As in the HEP <ref> [27] </ref>, a full/empty bit is associated with each memory word and atomic read/modify/write operations can be performed on the full/empty bit. read-and-empty atomically reads a memory word and simultaneously resets the associated full/empty bit.
Reference: [28] <author> SPARC Architecture Manual, </author> <year> 1988. </year> <institution> SUN Microsystems, Mountain View, California. </institution>
Reference-contexts: The number of instructions can also be reduced through careful handcrafting of the relevant portions of the scheduler. 25 Most of the cycles spent in blocking are due to saving and restoring registers. Since Sparcle loads take two cycles and stores three <ref> [28] </ref>, the cost of blocking is higher than it would be on a processor with single-cycle loads and stores. With single-cycle load/stores, the cost of blocking can be reduced to 114 base cycles. Alternatively, we could pipeline the loads and stores of the registers.
Reference: [29] <author> K.S. Trivedi. </author> <title> Probability and Statistics with Reliability, Queueing and Computer Science Applications. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference: [30] <author> J.M. Wing and C. Gong. </author> <title> A Library of Concurrent Objects and Their Proofs of Correctness. </title> <type> Technical Report CMU-CS-90-151, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> Jul </month> <year> 1990. </year>
Reference-contexts: However, these methods can be useful in the polling phase of two-phase algorithms to reduce the detrimental effects of contention. Several researchers have recently been advocating the use of lock-free methods for synchronization <ref> [8, 14, 30] </ref>, using load-linked/store-conditional [17] or compare-and-swap [16] as primitives. In the context of this paper, lock-free synchronization is a form of optimistic polling, and the results in this paper apply.
Reference: [31] <author> P.-C. Yew, N.-F. Tzeng, and D.H. Lawrie. </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Barriers Barriers ensure that all participating threads have reached a point in a program before proceeding. To avoid excessive traffic to a single location, and to distribute the enqueuing and release operations, we use software combining trees <ref> [31] </ref> to implement barriers. 5.3 Simulation Environment While the implementation of the Alewife machine is in progress, a cycle-by-cycle simulator called ASIM is being used for software and applications development. ASIM faithfully simulates the complete machine.
Reference: [32] <author> J. Zahorjan and E. Lazowska. </author> <title> Spinning Versus Blocking in Parallel Systems with Uncertainty. </title> <type> Technical Report TR-88-03-01, </type> <institution> Dept. of Computer Science, University of Washington, </institution> <address> Seattle, WA, </address> <month> Mar </month> <year> 1988. </year>
Reference-contexts: In a two 1 As anecdotal evidence, blocking was tried as a performance enhancer for a system routine in the DYNIX operating system for the Sequent multiprocessor, unexpectedly causing bad performance under certain conditions. This fact was subsequently used in an advertising campaign by a competitor. See <ref> [32] </ref>. 2 phase waiting algorithm, a waiting thread first polls a synchronization variable until the cost of polling reaches a limit L poll . If further waiting is necessary at the end of the polling phase, the thread resorts to a signalling mechanism for waiting, incurring a fixed cost B. <p> In a study on the effect of data dependence and multiprogramming on expected wait times, Zahorjan et al. <ref> [32] </ref> showed that wait times can be highly dependent on run-time factors. They concluded that both sources of run-time uncertainty can lead to sharply increased wait times in the case of barrier synchronization. <p> If we know that U &gt; 2fiB, we should choose =b=0, otherwise we should choose Ss==1. Therefore, with accurate information about U the competitive factor is at most 4=3 as shown in Figure 3. However, as observed by Zahorjan et al. <ref> [32] </ref>, barrier wait times are highly dependent on run-time factors making it hard to predict U . If we cannot reliably predict U , we should choose Ss=b=0:62 to obtain the best competitive factor of 1.62 (the golden ratio), as prescribed by Theorem 4, and as illustrated in Figure 3.
References-found: 32

