URL: ftp://ftp.idsia.ch/pub/juergen/HQ-LEARNING.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/topics.html
Root-URL: 
Email: marco@idsia.ch juergen@idsia.ch  
Title: HQ-Learning Adaptive Behavior 6:2, 1997 (in press)  
Author: Marco Wiering Jurgen Schmidhuber 
Keyword: reinforcement learning, hierarchical Q-learning, POMDPs, non-Markov, subgoal learning.  
Web: http://www.idsia.ch  
Address: Corso Elvezia 36 CH-6900 Lugano Switzerland  
Affiliation: IDSIA  
Abstract: HQ-learning is a hierarchical extension of Q()-learning designed to solve certain types of partially observable Markov decision problems (POMDPs). HQ automatically decomposes POMDPs into sequences of simpler subtasks that can be solved by memoryless policies learnable by reactive subagents. HQ can solve partially observable mazes with more states than those used in most previous POMDP work.
Abstract-found: 1
Intro-found: 1
Reference: <author> Boutilier, C. and Poole, D. </author> <year> (1996). </year> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In AAAI-1996: Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1168-1175, </pages> <address> Portland, OR. </address>
Reference: <author> Caironi, P. V. C. and Dorigo, M. </author> <year> (1994). </year> <title> Training Q-agents. </title> <type> Technical Report IRIDIA-94-14, </type> <institution> Universite Libre de Bruxelles. </institution> <note> 16 Chrisman, </note> <author> L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth International Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188. </pages> <publisher> AAAI Press, </publisher> <address> San Jose, California. </address>
Reference: <author> Cliff, D. and Ross, S. </author> <year> (1994). </year> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3 </volume> <pages> 101-150. </pages>
Reference: <author> Cohn, D. A. </author> <year> (1994). </year> <title> Neural network exploration using optimal experiment design. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 679-686. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dayan, P. and Hinton, G. </author> <year> (1993). </year> <title> Feudal reinforcement learning. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 271-278. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Digney, B. </author> <year> (1996). </year> <title> Emergent hierarchical control structures: Learning reactive/hierarchical relationships in reinforcement environments. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 363-372. </pages> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> Fedorov, V. V. </author> <year> (1972). </year> <title> Theory of optimal experiments. </title> <publisher> Academic Press. </publisher>
Reference: <author> Hihi, S. E. and Bengio, Y. </author> <year> (1996). </year> <title> Hierarchical recurrent neural networks for long-term dependencies. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 493-499. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1997). </year> <title> Long short-term memory. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 1681-1726. </pages>
Reference: <author> Humphrys, M. </author> <year> (1996). </year> <title> Action selection methods using reinforcement learning. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 135-144. </pages> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> Jaakkola, T., Singh, S. P., and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 345-352. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Jordan, M. I. and Rumelhart, D. E. </author> <year> (1990). </year> <title> Supervised learning with a distal teacher. </title> <type> Technical Report Occasional Paper #40, </type> <institution> Center for Cog. Sci., Massachusetts Institute of Technology. </institution>
Reference: <author> Kaelbling, L., Littman, M., and Cassandra, A. </author> <year> (1995). </year> <title> Planning and acting in partially observable stochastic domains. </title> <type> Technical report, </type> <institution> Brown University, Providence RI. </institution>
Reference-contexts: This explains the recent interest in heuristic methods for finding good but not necessarily optimal solutions, e.g., Schmidhuber (1991c), McCallum (1993), Ring (1994), Cliff and Ross (1994), Jaakkola, Singh and Jordan (1995). 1 Unfortunately, however, previous methods do not scale up very well <ref> (Littman, Cassandra and Kaelbling 1995) </ref>. This paper presents HQ-learning, a novel approach based on finite state memory implemented in a sequence of agents. HQ does not need a model of the POMDP and appears to scale up more reasonably than other approaches.
Reference: <author> Koenig, S. and Simmons, R. G. </author> <year> (1996). </year> <title> The effect of representation and knowedge on goal-directed exploration with reinforcement learnign algorithm. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 228-250. </pages>
Reference-contexts: See, e.g., Peng and Williams (1996), or Wiering and Schmidhuber's fast Q () implementation (1997). Online Q () should not use "action-penalty" <ref> (Koenig and Simmons 1996) </ref>, however, because punishing varying actions in response to ambiguous inputs will trap the agent in cyclic behavior. Combined dynamics. Q-table policies are reactive and learn to solve RPPs. HQ-table policies are metastrategies for composing RPP sequences.
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference-contexts: One limitation of his method is that the system has no means of remembering and using any information other than that immediately perceivable. HQ-learning, however, can profit from remembering previous events for very long time periods. Levin Search. Wiering and Schmidhuber (1996) use Levin search (LS) through program space <ref> (Levin 1973) </ref> to discover programs computing solutions for large POMDPs. LS is of interest because of its amazing theoretical properties: for a broad class of search problems, it has the optimal order of computational complexity.
Reference: <author> Lin, L. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh. </institution>
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In D. Cliff, P. Husbands, J. A. M. and Wilson, S. W., editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3, </booktitle> <pages> pages 297-305. </pages> <publisher> MIT Press/Bradford Books. 17 Littman, </publisher> <editor> M. </editor> <year> (1996). </year> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Brown University, Providence, RI. </institution>
Reference: <author> Littman, M., Cassandra, A., and Kaelbling, L. </author> <year> (1995). </year> <title> Learning policies for partially observable environments: Scaling up. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 362-370. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: This explains the recent interest in heuristic methods for finding good but not necessarily optimal solutions, e.g., Schmidhuber (1991c), McCallum (1993), Ring (1994), Cliff and Ross (1994), Jaakkola, Singh and Jordan (1995). 1 Unfortunately, however, previous methods do not scale up very well <ref> (Littman, Cassandra and Kaelbling 1995) </ref>. This paper presents HQ-learning, a novel approach based on finite state memory implemented in a sequence of agents. HQ does not need a model of the POMDP and appears to scale up more reasonably than other approaches.
Reference: <author> McCallum, R. A. </author> <year> (1993). </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Amherst, MA. </address>
Reference: <author> McCallum, R. A. </author> <year> (1996). </year> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 315-324. </pages> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> Moore, A. and Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130. </pages>
Reference-contexts: HQ-learning does not depend on estimating this probability, although belief vectors (Littman, 1996) or a world model <ref> (e.g., Moore 1993) </ref> might help to speed up learning.
Reference: <author> Nguyen, D. and Widrow, B. </author> <year> (1989). </year> <title> The truck backer-upper: An example of self learning in neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> pages 357-363. </pages> <publisher> IEEE Press. </publisher>
Reference: <author> Parr, R. and Russell, S. </author> <year> (1995). </year> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Internatianal Joint Conference on Artifician Intelligence (IJCAI-95), </booktitle> <pages> pages 1088-1094. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Peng, J. and Williams, R. </author> <year> (1996). </year> <title> Incremental multi-step Q-learning. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 283-290. </pages>
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas. </institution>
Reference: <author> Ron, D., Singer, Y., and Tishby, N. </author> <year> (1994). </year> <title> Learning probabilistic automata with variable memory length. </title> <editor> In Aleksander, I. and Taylor, J., editors, </editor> <booktitle> Proceedings Computational Learning Theory. </booktitle> <publisher> ACM Press. </publisher>
Reference-contexts: HQ-learning, however, can reuse the same policy and generalize well from previous to "similar" problems. McCallum's U-tree (1996) is quite similar to Ring's system. It uses prediction suffix trees <ref> (see Ron, Singer and Tishby 1994) </ref> in which the branches reflect decisions based on current or previous inputs/actions. Q-values are stored in the leaves, which correspond to clusters of instances collected and stored during the entire learning phase.
Reference: <author> Sa lustowicz, R. P. and Schmidhuber, J. </author> <year> (1997). </year> <title> Probabilistic incremental program evolution. </title> <journal> Evolutionary Computation, </journal> <volume> 5(2) </volume> <pages> 123-141. </pages> <note> See ftp://ftp.idsia.ch/pub/rafal/PIPE.ps.gz. </note>
Reference: <author> Schmidhuber, J. </author> <year> (1991a). </year> <title> Curious model-building control systems. </title> <booktitle> In Proc. International Joint Conference on Neural Networks, Singapore, </booktitle> <volume> volume 2, </volume> <pages> pages 1458-1463. </pages> <publisher> IEEE. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1991b). </year> <title> Learning to generate sub-goals for action sequences. </title> <editor> In Kohonen, T., Makisara, K., Simula, O., and Kangas, J., editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 967-972. </pages>
Reference: <institution> Elsevier Science Publishers B.V., North-Holland. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1991c). </year> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1992). </year> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1997). </year> <note> What's interesting? Technical Report IDSIA-35-97, IDSIA. </note>
Reference: <author> Schmidhuber, J., Zhao, J., and Schraudolph, N. </author> <year> (1997a). </year> <title> Reinforcement learning with self-modifying policies. </title> <editor> In Thrun, S. and Pratt, L., editors, </editor> <title> Learning to learn. </title> <type> Kluwer. </type> <note> in press. 18 Schmidhuber, </note> <author> J., Zhao, J., and Wiering, M. </author> <year> (1997b). </year> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 26 </volume> <pages> 105-130. </pages>
Reference-contexts: For instance, in additional experiments with a "self-referential" system that embeds its policy-modifying method within the policy itself, SSA is able to solve huge POMDPs with more than 10 13 states <ref> (Schmidhuber et al. 1997a) </ref>. It may be 14 possible to combine SSA with HQ-learning in an advantageous way. Multiple Q-learners. Like HQ-learning, Humphrys' W-learning (1996) uses multiple Q-learning agents.
Reference: <author> Singh, S. </author> <year> (1992). </year> <title> The efficient learning of multiple task sequences. </title> <editor> In Moody, J., Hanson, S., and Lippman, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 251-258, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sondik, E. J. </author> <year> (1971). </year> <title> The Optimal Control of Partially Observable Markov Decision Processes. </title> <type> PhD thesis, Unpublished doctoral thesis, </type> <institution> Stanford University, </institution> <address> CA. </address>
Reference-contexts: After each observation the belief vector is updated using action/observation models and Bayes' formula. This compresses the history of previous events into a probability distribution. Based on this "belief state" an optimal action can be chosen <ref> (Sondik 1971) </ref>. Dynamic programming algorithms are used to compute optimal policies based on the belief states. Problems with this approach are that the nature of the underlying MDP needs to be known, and that it is computationally very expensive.
Reference: <author> Storck, J., Hochreiter, S., and Schmidhuber, J. </author> <year> (1995). </year> <title> Reinforcement driven information acquisition in nondeterministic environments. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 159-164. </pages> <address> EC2 & Cie, Paris. </address>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: This requires at least one bit of memory | your current environmental input by itself is not sufficient. The most widely used reinforcement learning (RL) algorithms, such as Q-learning (Watkins 1989; Watkins and Dayan 1992) and TD () <ref> (Sutton 1988) </ref>, fail if the task requires creating short-term memories of relevant events to disambiguate identical sensory inputs observed in different states. They are limited to Markov decision problems (MDPs): at any given time the probabilities of the possible next states depend only on the current state and action.
Reference: <author> Sutton, R. S. </author> <year> (1995). </year> <title> TD models: Modeling the world at a mixture of time scales. </title> <editor> In Priedi-tis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 531-539. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Teller, A. </author> <year> (1994). </year> <title> The evolution of mental models. </title> <editor> In Kenneth E. Kinnear, J., editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 199-219. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: This makes it much more stable. Program evolution with memory cells (MCs). Certain techniques for automatic program synthesis based on evolutionary principles can be used to evolve short-term memorizing programs that read and write MCs during runtime <ref> (e.g., Teller, 1994) </ref>. A recent such method is Probabilistic Incremental Program Evolution (PIPE | Salustowicz and Schmidhuber, 1997). PIPE iteratively 13 generates successive populations of functional programs according to an adaptive probability dis-tribution over all possible programs. On each iteration it uses the best program to refine the distribution.
Reference: <author> Tham, C. </author> <year> (1995). </year> <title> Reinforcement learning of multiple tasks using a hierarchical CMAC architecture. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 15(4) </volume> <pages> 247-274. </pages>
Reference: <author> Thrun, S. </author> <year> (1992). </year> <title> Efficient exploration in reinforcement learning. </title> <type> Technical Report CMU-CS-92-102, </type> <institution> Carnegie-Mellon University. </institution>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, Oxford. </institution>
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference: <author> Whitehead, S. </author> <year> (1992). </year> <title> Reinforcement Learning for the Adaptive Control of Perception and Action. </title> <type> PhD thesis, </type> <institution> University of Rochester. </institution>
Reference-contexts: 1 Introduction The problem. Sensory information is usually insufficient to infer the environment's state <ref> (perceptual aliasing, Whitehead 1992) </ref>. This complicates goal-directed behavior. For instance, suppose your instructions for the way to the station are: "Follow this road to the traffic light, turn left, follow that road to the next traffic light, turn right, there you are.". Suppose you reach one of the traffic lights. <p> HQ ()-learning: motivation. Q-learning's lookahead capability is restricted to one step. It cannot solve all RPPs because it cannot properly assign credit to different actions leading to identical next states <ref> (Whitehead 1992) </ref>. For instance, suppose you walk along a wall that looks the same everywhere except in the middle where there is a picture. The goal is to reach the left corner where there is reward. This RPP is solvable by an RP.
Reference: <author> Wiering, M. and Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Wiering, M. A. and Schmidhuber, J. </author> <year> (1997). </year> <title> Fast online Q(). </title> <type> Technical Report IDSIA-21-97, </type> <note> IDSIA. In preparation. </note>
Reference: <author> Wilson, S. </author> <year> (1994). </year> <title> ZCS: A zeroth level classifier system. </title> <journal> Evolutionary Computation, </journal> <volume> 2 </volume> <pages> 1-18. </pages>
Reference: <author> Wilson, S. </author> <year> (1995). </year> <title> Classifier fitness based on accuracy. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2) </volume> <pages> 149-175. </pages>
Reference: <author> Wilson, S. </author> <year> (1996). </year> <title> Explore/exploit strategies in autonomy. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> Proc. of the Fourth International Conference on Simulation of Adaptive Behavior: From Animals to Animats 4, </booktitle> <pages> pages 325-332. </pages> <publisher> MIT Press/Bradford Books. </publisher>
Reference: <author> Zhang, N. L. and Liu, W. </author> <year> (1996). </year> <title> Planning in stochastic domains: Problem characteristics and approximation. </title> <type> Technical Report HKUST-CS96-31, </type> <institution> Hong Kong University of Science and Technology. </institution>
Reference: <author> Zhao, J. and Schmidhuber, J. </author> <year> (1996). </year> <title> Incremental self-improvement for life-time multi-agent reinforcement learning. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 516-525. </pages> <publisher> MIT Press, Bradford Books. </publisher> <pages> 19 </pages>
References-found: 52

