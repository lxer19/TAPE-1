URL: http://www.research.att.com/~schapire/papers/FreundSc98.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: fyoav, schapireg@research.att.com  
Title: Large Margin Classification Using the Perceptron Algorithm  
Author: Yoav Freund Robert E. Schapire 
Address: 180 Park Avenue Florham Park, NJ 07932-0971 USA  
Affiliation: AT&T Labs  
Note: Proceedings of the Eleventh Annual Conference on ComputationalLearning Theory, 1998.  
Abstract: We introduce and analyze a new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method. Like Vapnik's maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik's algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. A. Aizerman, E. M. Braverman, and L. I. Rozo-noer. </author> <title> Theoretical foundations of the potential function method in pattern recognition learning. </title> <journal> Automation and Remote Control, </journal> <volume> 25 </volume> <pages> 821-837, </pages> <year> 1964. </year>
Reference-contexts: This problem can sometimes be solved by the elegant method of kernel functions. The use of kernel functions for classification problems was suggested by Boser, Guyon and Vapnik [3], continuing the work of Aizerman, Braverman and Rozonoer <ref> [1] </ref>. Kernel functions are functions of two variables K (x; y) which can be represented as an inner product F (x) F (y) for some function F : R n ! R N and some N &gt; 0.
Reference: [2] <author> H. D. </author> <title> Block. The perceptron: A model for brain functioning. </title> <journal> Reviews of Modern Physics, </journal> <volume> 34 </volume> <pages> 123-135, </pages> <year> 1962. </year> <note> Reprinted in "Neurocomputing" by Anderson and Rosenfeld. </note>
Reference-contexts: This prediction rule is then used for predicting the labels on the test set. Block <ref> [2] </ref>, Novikoff [13] and Minsky and Papert [12] have shown that if the data are linearly separable, then the perceptron algorithm will make a finite number of mistakes, and therefore, if repeatedly cycled through the training set, will converge to a vector which correctly classifies all of the examples. <p> Second, we review an analysis of the leave-one-out conversion of an on-line learning algorithm to a batch learning algorithm. 2 3.1 THE ON-LINE PERCEPTRON ALGORITHM IN THE SEPARABLE CASE Our analysis is based on the following well known result first proved by Block <ref> [2] </ref> and Novikoff [13]. The significance of this result is that the number of mistakes does not depend on the dimension of the instances. This gives reason to believe that the perceptron algorithm might perform well in high dimensional spaces.
Reference: [3] <author> Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. </author> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-152, </pages> <year> 1992. </year>
Reference-contexts: However, from a computational point of view, computing the values of the additional coordinates can become prohibitively hard. This problem can sometimes be solved by the elegant method of kernel functions. The use of kernel functions for classification problems was suggested by Boser, Guyon and Vapnik <ref> [3] </ref>, continuing the work of Aizerman, Braverman and Rozonoer [1]. Kernel functions are functions of two variables K (x; y) which can be represented as an inner product F (x) F (y) for some function F : R n ! R N and some N &gt; 0.
Reference: [4] <author> Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 44(3) </volume> <pages> 427-485, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: In this work we decided to use deterministic voting rather than randomization. The following theorem follows directly from Helmbold and Warmuth [7]. (See also Kivinen and Warmuth [8] and Cesa-Bianchi et al. <ref> [4] </ref>.) Theorem 3 Assume all examples (x; y) are generated i.i.d. Let E be the expected number of mistakes that the online algorithm A makes on a randomly generated sequence of m + 1 examples.
Reference: [5] <author> Corinna Cortes and Vladimir Vapnik. </author> <title> Support-vector networks. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 273-297, </pages> <month> Septem-ber </month> <year> 1995. </year>
Reference-contexts: We repeated some of the experiments performed by Cortes and Vapnik <ref> [5] </ref> on the use of SVM on the problem of classifying handwritten digits. We tested both the voted-perceptron algorithm and a variant based on averaging rather than voting. <p> prediction of the voted-perceptron when using kernels is only marginally more expensive than calculating the prediction of the final prediction vector, assuming that both methods are trained for the same number of epochs. 5 5 EXPERIMENTS In our experiments, we followed closely the experimental setup used by Cortes and Vapnik <ref> [5] </ref> in their experiments on the NIST OCR database. 2 We chose to use this setup because the dataset is widely available and because LeCun et al. [10] have published a detailed comparison of the performance of some of the best digit classification systems in this setup. <p> The best of the algorithms that they tested is (a rather old version of) boosting on top of the neural net LeNet 4 which achieves an error rate of 0.7%. A version of the optimal margin classifier algorithm <ref> [5] </ref>, using the same kernel function, performs significantly better than ours, achieving a test error rate of 1.1% for d = 4. 6 CONCLUSIONS AND SUMMARY The most significant result of our experiments is that running the perceptron algorithm in a higher dimensional space using kernel functions produces very significant improvements
Reference: [6] <author> S. I. Gallant. </author> <title> Optimal linear discriminants. </title> <booktitle> In Eighth International Conference on Pattern Recognition, </booktitle> <pages> pages 849-852. </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: A prediction rule that has survived for a long time is likely to be better than one that has only survived for a few iterations. This method was suggested by Gallant <ref> [6] </ref> who called it the pocket method. Littlestone [11], suggested a two-phase method in which the performance of all of the rules is tested on a seperate test set and the rule with the least error is then used.
Reference: [7] <author> David P. Helmbold and Manfred K. Warmuth. </author> <title> On weak learning. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50 </volume> <pages> 551-573, </pages> <year> 1995. </year>
Reference-contexts: We named the new algorithm the voted-perceptron algorithm. The algorithm is based on the well known perceptron algorithm of Rosenblatt [14, 15] and a transformation of online learning algorithms to batch learning algorithms developed by Helm-bold and Warmuth <ref> [7] </ref>. Moreover, we show that kernel functions can be used with our algorithm so that we can run our algorithm efficiently in very high dimensional spaces. Our algorithm and its analysis involve little more than combining these three known methods. <p> In this paper, we propose to use a more sophisticated method of applying the on-line perceptron algorithm to batch learning, namely, a variation of the leave-one-out method of Helmbold and Warmuth <ref> [7] </ref>. In the voted-perceptron algorithm, we store more information during training and then use this elaborate information to generate better predictions on the test data. The algorithm is detailed in Figure 1. <p> Here we use a different method for converting the online perceptron algorithm into a batch learning algorithm, the method combines all of the rules generated by the online algorithm after it was run for just a single time through the training data. We now describe Helmbold and Warmuth's <ref> [7] </ref> very simple leave-one-out method of converting an online learning algorithm into a batch learning algorithm. Our voted-perceptron algorithm is a simple application of this general method. We start with the randomized version. <p> It is straightforward to show that taking a majority vote runs the risk of doubling the probability of mistake while it has the potential of significantly decreasing it. In this work we decided to use deterministic voting rather than randomization. The following theorem follows directly from Helmbold and Warmuth <ref> [7] </ref>. (See also Kivinen and Warmuth [8] and Cesa-Bianchi et al. [4].) Theorem 3 Assume all examples (x; y) are generated i.i.d. Let E be the expected number of mistakes that the online algorithm A makes on a randomly generated sequence of m + 1 examples.
Reference: [8] <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Additive ver-sus exponentiated gradient updates for linear prediction. </title> <journal> Information and Computation, </journal> <volume> 132(1) </volume> <pages> 1-64, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: In this work we decided to use deterministic voting rather than randomization. The following theorem follows directly from Helmbold and Warmuth [7]. (See also Kivinen and Warmuth <ref> [8] </ref> and Cesa-Bianchi et al. [4].) Theorem 3 Assume all examples (x; y) are generated i.i.d. Let E be the expected number of mistakes that the online algorithm A makes on a randomly generated sequence of m + 1 examples.
Reference: [9] <author> Norbert Klasner and Hans Ulrich Simon. </author> <title> From noise-free to noise-tolerant and from on-line to batch learning. </title> <booktitle> In Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 250-264, </pages> <year> 1995. </year>
Reference-contexts: However, we now give a generalized version of the theorem which allows for some mistakes in the training set. As far as we know, this theorem is new, although the proof technique is very similar to that of Klasner and Simon <ref> [9, Theorem 2.2] </ref>. Theorem 2 Let h (x 1 ; y 1 ); : : : ; (x m ; y m )i be a sequence of labeled examples with jjx i jj R. Let u be any vector with jjujj = 1 and let fl &gt; 0.
Reference: [10] <author> Y. LeCun, L. D. Jackel, L. Bottou, A. Brunot, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, U. A. Muller, E. Sackinger, P. Simard, and V Vapnik. </author> <title> Comparison of learning algorithms for handwritten digit recognition. </title> <booktitle> In International Conference on Artificial Neural Networks, </booktitle> <pages> pages 53-60, </pages> <year> 1995. </year>
Reference-contexts: for the same number of epochs. 5 5 EXPERIMENTS In our experiments, we followed closely the experimental setup used by Cortes and Vapnik [5] in their experiments on the NIST OCR database. 2 We chose to use this setup because the dataset is widely available and because LeCun et al. <ref> [10] </ref> have published a detailed comparison of the performance of some of the best digit classification systems in this setup. Examples in this NIST database consist of labeled digital images of individual handwritten digits. <p> Using normalized vectors seems to sometimes help a bit for the last method, but can help or hurt performance slightly for the average method; in any case, the differences in performance between using normalized and unnormalized vectors are always minor. LeCun et al. <ref> [10] </ref> give a detailed comparison of algorithms on this dataset. The best of the algorithms that they tested is (a rather old version of) boosting on top of the neural net LeNet 4 which achieves an error rate of 0.7%.
Reference: [11] <author> Nick Littlestone. </author> <title> From on-line to batch learning. </title> <booktitle> In Proceedings of the Second Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 269-284, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: A prediction rule that has survived for a long time is likely to be better than one that has only survived for a few iterations. This method was suggested by Gallant [6] who called it the pocket method. Littlestone <ref> [11] </ref>, suggested a two-phase method in which the performance of all of the rules is tested on a seperate test set and the rule with the least error is then used.
Reference: [12] <author> Marvin Minsky and Seymour Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> The MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: This prediction rule is then used for predicting the labels on the test set. Block [2], Novikoff [13] and Minsky and Papert <ref> [12] </ref> have shown that if the data are linearly separable, then the perceptron algorithm will make a finite number of mistakes, and therefore, if repeatedly cycled through the training set, will converge to a vector which correctly classifies all of the examples.
Reference: [13] <author> A. B. J. Novikoff. </author> <title> On convergence proofs on perceptrons. </title> <booktitle> In Proceedings of the Symposium on the Mathematical Theory of Automata, </booktitle> <volume> volume XII, </volume> <pages> pages 615-622, </pages> <year> 1962. </year>
Reference-contexts: This prediction rule is then used for predicting the labels on the test set. Block [2], Novikoff <ref> [13] </ref> and Minsky and Papert [12] have shown that if the data are linearly separable, then the perceptron algorithm will make a finite number of mistakes, and therefore, if repeatedly cycled through the training set, will converge to a vector which correctly classifies all of the examples. <p> Second, we review an analysis of the leave-one-out conversion of an on-line learning algorithm to a batch learning algorithm. 2 3.1 THE ON-LINE PERCEPTRON ALGORITHM IN THE SEPARABLE CASE Our analysis is based on the following well known result first proved by Block [2] and Novikoff <ref> [13] </ref>. The significance of this result is that the number of mistakes does not depend on the dimension of the instances. This gives reason to believe that the perceptron algorithm might perform well in high dimensional spaces.
Reference: [14] <author> F. Rosenblatt. </author> <title> The perceptron: A probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65 </volume> <pages> 386-407, </pages> <year> 1958. </year> <note> (Reprinted in Neurocomputing (MIT Press, 1988).). </note>
Reference-contexts: In this paper, we introduce a new and simpler algorithm for linear classification which takes advantage of data that are linearly separable with large margins. We named the new algorithm the voted-perceptron algorithm. The algorithm is based on the well known perceptron algorithm of Rosenblatt <ref> [14, 15] </ref> and a transformation of online learning algorithms to batch learning algorithms developed by Helm-bold and Warmuth [7]. Moreover, we show that kernel functions can be used with our algorithm so that we can run our algorithm efficiently in very high dimensional spaces. <p> We use jjxjj to denote the Euclidean length of x. For most of the paper, we assume that labels y are in f1; +1g. The basis of our study is the classical perceptron algorithm invented by Rosenblatt <ref> [14, 15] </ref>. This is a very simple algorithm most naturally studied in the on-line learning model. The on-line perceptron algorithm starts with an initial zero prediction vector v = 0. It predicts the label of a new instance x to be y = sign (v x).
Reference: [15] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: In this paper, we introduce a new and simpler algorithm for linear classification which takes advantage of data that are linearly separable with large margins. We named the new algorithm the voted-perceptron algorithm. The algorithm is based on the well known perceptron algorithm of Rosenblatt <ref> [14, 15] </ref> and a transformation of online learning algorithms to batch learning algorithms developed by Helm-bold and Warmuth [7]. Moreover, we show that kernel functions can be used with our algorithm so that we can run our algorithm efficiently in very high dimensional spaces. <p> We use jjxjj to denote the Euclidean length of x. For most of the paper, we assume that labels y are in f1; +1g. The basis of our study is the classical perceptron algorithm invented by Rosenblatt <ref> [14, 15] </ref>. This is a very simple algorithm most naturally studied in the on-line learning model. The on-line perceptron algorithm starts with an initial zero prediction vector v = 0. It predicts the label of a new instance x to be y = sign (v x).
Reference: [16] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: 1 INTRODUCTION One of the most influential developments in the theory of machine learning in the last few years is Vapnik's work on support vector machines (SVM) <ref> [16] </ref>. Vapnik's analysis suggests the following simple method for learning complex binary classifiers. First, use some fixed mapping F to map the instances into some very high dimensional space in which the two classes are linearly separable.
Reference: [17] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> Theory of pattern recognition. </title> <publisher> Nauka, </publisher> <address> Moscow, </address> <year> 1974. </year> <note> (In Russian). </note>
Reference-contexts: On the other hand, the resulting algorithm is very simple and easy to implement, and the theoretical bounds on the expected generalization error of the new algorithm are almost identical to the bounds for SVM given by Vapnik and Chervonenkis <ref> [17] </ref> in the linearly separable case. We repeated some of the experiments performed by Cortes and Vapnik [5] on the use of SVM on the problem of classifying handwritten digits. We tested both the voted-perceptron algorithm and a variant based on averaging rather than voting. <p> list of weighted perceptrons: h (v 1 ; c 1 ); : : : ; (v k ; c k )i an unlabeled instance: x compute a predicted label y as follows: s = i=1 c i sign (v i x); y = sign (s) : of Vapnik and Chervonenkis <ref> [17] </ref> for the linearly separable case. This theorem bounds the generalization error of the consistent perceptron found after the perceptron algorithm is run to convergence. Interestingly, for the linearly separable case, the theorems yield very similar bounds. <p> Thus, for linearly separable data, when T ! 1, the voted-perceptron algorithm converges to the regular use of the perceptron algorithm, which is to predict using the final prediction vector. As we have recently learned, the performance of the final prediction vector has been analyzed by Vapnik and Chervo-nenkis <ref> [17] </ref>. We discuss their bound at the end of this section. We now give our analysis for the case T = 1. The analysis is in two parts and mostly combines known material. <p> A rather similar theorem was proved by Vapnik and Cher-vonenkis <ref> [17, Theorem 6.1] </ref> for training the perceptron algorithm to convergence and predicting with the final perceptron vector. Theorem 6 (Vapnik and Chervonenkis) Assume all examples are generated i.i.d. at random.
Reference: [18] <author> Vladimir N. </author> <title> Vapnik. </title> <journal> Statistical Learning Theory. </journal> <note> Wi-ley, 1998 (to appear). 9 </note>
Reference-contexts: This gives us some indication that running the voted-perceptron algorithm with T = 1 might be better than running it to convergence; however, our experiments do not support this prediction. Vapnik <ref> [18] </ref> also gives a very similar bound for the expected error of support-vector machines. There are two differences between the bounds. First, the set of vectors on which the perceptron makes a mistake is replaced by the set of essential support vectors.
References-found: 18

