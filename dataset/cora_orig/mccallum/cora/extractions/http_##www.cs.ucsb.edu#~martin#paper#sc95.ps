URL: http://www.cs.ucsb.edu/~martin/paper/sc95.ps
Refering-URL: http://www.cs.ucsb.edu/~martin/paper/index.html
Root-URL: http://www.cs.ucsb.edu
Email: martin@cs.ucsb.edu  
Title: Communication Optimizations for Parallel Computing Using Data Access Information  
Author: Martin C. Rinard 
Address: Santa Barbara, California 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: Given the large communication overheads characteristic of modern parallel machines, optimizations that eliminate, hide or parallelize communication may improve the performance of parallel computations. This paper describes our experience automatically applying communication optimizations in the context of Jade, a portable, implicitly parallel programming language designed for exploiting task-level concurrency. Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to declare how parts of the program access data. The Jade implementation uses this data access information to automatically extract the concurrency and apply communication optimizations. Jade implementations exist for both shared memory and message passing machines; each Jade implementation applies communication optimizations appropriate for the machine on which it runs. We present performance results for several Jade applications running on both a shared memory machine (the Stanford DASH machine) and a message passing machine (the Intel iPSC/860). We use these results to characterize the overall performance impact of the communication optimizations. For our application set replicating data for concurrent read access and improving the locality of the computation by placing tasks close to the data that they access are the most important optimizations. Broadcasting widely accessed data has a significant performance impact on one application; other optimizations such as concurrently fetching remote data and overlapping computation with communication have no effect.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Program Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The more advanced Jade constructs also support pipelined access to objects [17]. Parallelizing compilers for message passing machines analyze a program written in a serial programming language and generate parallel code <ref> [1] </ref>. Part of the compilation task entails the generation of message passing operations that transfer data produced on one processor to all of the processors that consume the data.
Reference: [2] <author> H. Bal, M. Kaashoek, and A. Tanenbaum. Orca: </author> <title> A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Many parallel languages, on the other hand, fail to give the implementation the data usage information that it needs to automatically apply sophisticated communication optimizations on a variety of computational platforms <ref> [10, 2, 6] </ref>. This paper describes our experience with communication optimizations in the context of Jade [19, 20], a portable, implicitly parallel programming language designed for exploiting task-level concurrency.
Reference: [3] <author> R. Berrendorf and J. Helin. </author> <title> Evaluating the basic performance of the Intel iPSC/860 parallel computer. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 4(3) </volume> <pages> 223-240, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: As part of the parallelization process the implementation exploits its information about how tasks will access data to automatically apply the communication optimizations appropriate for the hardware platform at hand. Jade implementations exist for shared memory machines (the Stanford DASH machine [14]), message passing machines (the Intel iPSC/860 <ref> [3] </ref>) and heterogeneous collections of workstations. Jade programs port without modification between all platforms. The first contribution of this paper is a set of implementation techniques for automatically applying communication optimizations on both shared memory and message passing machines. <p> At larger numbers of processors the Jade task management overhead plays a progressively larger role in the performance of the application | the iPSC/860 does not support the fine-grained communication required for efficient task management <ref> [3] </ref>. Figure 20 presents the task management percentages on the main processor for this computation. This figure shows that, at 16 processors and above, the task management overhead is the limiting factor on the overall performance.
Reference: [4] <author> M. Carlisle and A. Rogers. </author> <title> Software caching and computation migration in Olden. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: It takes place at a fine granularity, with communication operations that transfer a single cache line typically issued every few iterations of a loop. The Jade communication optimizations, on the other hand, are designed to operate with coarse-grain tasks that access large shared objects. Olden <ref> [4] </ref> and Prelude [13] attempt to improve locality using computation migration. When a piece of compu-tation accesses a remote piece of data, it may migrate to the processor that owns the data.
Reference: [5] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Tempest [8, 12] is a collection of communication mechanisms designed for implementation on a range of architectures. Programmers can apply communication optimizations by selecting an appropriate communication mechanism from a library or by coding their own custom protocol. Munin <ref> [5] </ref> is a page-based distributed shared memory system that provides several different coherence mechanisms. The programmer selects a coherence protocol for each piece of data; the idea is that the selected coherence protocol is the most efficient for the access pattern of that piece of data.
Reference: [6] <author> R. Chandra, A. Gupta, and J. Hennessy. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Many parallel languages, on the other hand, fail to give the implementation the data usage information that it needs to automatically apply sophisticated communication optimizations on a variety of computational platforms <ref> [10, 2, 6] </ref>. This paper describes our experience with communication optimizations in the context of Jade [19, 20], a portable, implicitly parallel programming language designed for exploiting task-level concurrency. <p> It fits naturally into the execution model, should in practice never impair the performance, and may prove useful for some other applications. 6 Related Work Chandra, Gupta and Hennessy <ref> [6] </ref> have designed, implemented and measured a scheduling algorithm for the parallel language COOL running on DASH. The goal of the scheduling algorithm is to enhance the locality of the computation while balancing the load. <p> Jade runs on a wide variety of hardware platforms, including both shared memory and message passing machines. This paper evaluates the performance impact of a full range of communication optimizations for a set of complete applications running on both shared memory and message passing platforms. The research presented in <ref> [6] </ref> only evaluates locality optimizations on a shared memory platform. Fowler and Kontothanassis [9] have developed an explicitly parallel system that uses object affinity scheduling to enhance locality. The system associates objects with processors.
Reference: [7] <author> I. Duff, R. Grimes, and J. Lewis. </author> <title> Sparse matrix problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The data set for Ocean is a square 192 by 192 grid. The timing runs omit an initial I/O phase. For Panel Cholesky the timing runs factor the BCSSTK15 matrix from the Harwell-Boeing sparse matrix benchmark set <ref> [7] </ref>. The performance numbers only measure the actual numerical factorization, omitting initial I/O and a symbolic factorization phase.
Reference: [8] <author> B. Falsafi, A. Lebeck, S. Reinhardt, I. Schoinas, M. Hill, J. Larus, A. Rogers, and D. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: While the protocol worked well for applications such as Water and String with regular, repetitive communication patterns, it degraded the performance of other applications by generating an excessive amount of communication. Tempest <ref> [8, 12] </ref> is a collection of communication mechanisms designed for implementation on a range of architectures. Programmers can apply communication optimizations by selecting an appropriate communication mechanism from a library or by coding their own custom protocol.
Reference: [9] <author> R. Fowler and L. Kontothanassis. </author> <title> Improving processor and cache locality in fine-grain parallel computations using object-affinity scheduling and continuation passing. </title> <type> Technical Report 411, </type> <institution> Dept. of Computer Science, University of Rochester, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: This paper evaluates the performance impact of a full range of communication optimizations for a set of complete applications running on both shared memory and message passing platforms. The research presented in [6] only evaluates locality optimizations on a shared memory platform. Fowler and Kontothanassis <ref> [9] </ref> have developed an explicitly parallel system that uses object affinity scheduling to enhance locality. The system associates objects with processors. Each task and thread has a location field that specifies the processor on which to execute the task or thread.
Reference: [10] <author> R. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Many parallel languages, on the other hand, fail to give the implementation the data usage information that it needs to automatically apply sophisticated communication optimizations on a variety of computational platforms <ref> [10, 2, 6] </ref>. This paper describes our experience with communication optimizations in the context of Jade [19, 20], a portable, implicitly parallel programming language designed for exploiting task-level concurrency.
Reference: [11] <author> J. Harris, S. Lazaratos, and R. Michelena. </author> <title> Tomographic string inversion. </title> <booktitle> In 60th Annual International Meeting, Society of Exploration and Geophysics, Extended Abstracts, </booktitle> <pages> pages 82-85, </pages> <year> 1990. </year>
Reference-contexts: The complete applications are Water, which evaluates forces and potentials in a system of water molecules in the liquid state, String <ref> [11] </ref>, which computes a velocity model of the geology between two oil wells, and Ocean, which simulates the role of eddy and boundary currents in influencing large-scale ocean movements. The computational kernel is Panel Cholesky, which factors a sparse positive-definite matrix.
Reference: [12] <author> M. Hill, J. Larus, and D. Wood. </author> <title> Tempest: A substrate for portable parallel programs. </title> <booktitle> In Proceedings of the 1995 Spring COMPCON, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: While the protocol worked well for applications such as Water and String with regular, repetitive communication patterns, it degraded the performance of other applications by generating an excessive amount of communication. Tempest <ref> [8, 12] </ref> is a collection of communication mechanisms designed for implementation on a range of architectures. Programmers can apply communication optimizations by selecting an appropriate communication mechanism from a library or by coding their own custom protocol.
Reference: [13] <author> W. Hsieh, P. Wang, and W. Weihl. </author> <title> Computation migration: Enhancing locality for distributed-memory parallel systems. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: It takes place at a fine granularity, with communication operations that transfer a single cache line typically issued every few iterations of a loop. The Jade communication optimizations, on the other hand, are designed to operate with coarse-grain tasks that access large shared objects. Olden [4] and Prelude <ref> [13] </ref> attempt to improve locality using computation migration. When a piece of compu-tation accesses a remote piece of data, it may migrate to the processor that owns the data. Olden also contains a data movement mechanism that copies data to accessing processors instead of moving computation to data.
Reference: [14] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: As part of the parallelization process the implementation exploits its information about how tasks will access data to automatically apply the communication optimizations appropriate for the hardware platform at hand. Jade implementations exist for shared memory machines (the Stanford DASH machine <ref> [14] </ref>), message passing machines (the Intel iPSC/860 [3]) and heterogeneous collections of workstations. Jade programs port without modification between all platforms. The first contribution of this paper is a set of implementation techniques for automatically applying communication optimizations on both shared memory and message passing machines. <p> We compute the time spent in tasks by reading the 60ns counter on DASH <ref> [14] </ref> just before each task executes, reading it again just after the task completes, then using the difference to update a variable containing the running sum of the total time spent executing tasks. Figures 6 through 9 plot the time spent executing tasks for each of the applications.
Reference: [15] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: While all of these optimizations may improve the overall performance of the application, it is unclear how to best apply the optimizations. Requiring the programmer to implement the optimizations directly may lead to an explosion of low-level, machine-specific communication constructs distributed throughout the source code <ref> [15] </ref>. Many parallel languages, on the other hand, fail to give the implementation the data usage information that it needs to automatically apply sophisticated communication optimizations on a variety of computational platforms [10, 2, 6]. <p> Mowry, Lam and Gupta have evaluated the performance impact of prefetching in the context of a shared memory multiprocessor that transfers data at the granularity of cache lines. The prefetch instructions are inserted either directly by the programmer <ref> [15] </ref>, or, for statically analyzable programs, by the compiler [16]. We see this communication optimization as orthogonal to the Jade communication optimizations. It takes place at a fine granularity, with communication operations that transfer a single cache line typically issued every few iterations of a loop.
Reference: [16] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Mowry, Lam and Gupta have evaluated the performance impact of prefetching in the context of a shared memory multiprocessor that transfers data at the granularity of cache lines. The prefetch instructions are inserted either directly by the programmer [15], or, for statically analyzable programs, by the compiler <ref> [16] </ref>. We see this communication optimization as orthogonal to the Jade communication optimizations. It takes place at a fine granularity, with communication operations that transfer a single cache line typically issued every few iterations of a loop.
Reference: [17] <author> M. Rinard. </author> <title> The Design, Implementation and Evaluation of Jade, a Portable, Implicitly Parallel Programming Language. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: In Section 6 we survey related work; we conclude in Section 7. 2 The Jade Programming Language This section provides a brief overview of the Jade language; other publications contain a complete description <ref> [17, 18, 20] </ref>. Jade is a set of constructs that programmers use to describe how a program written in a sequential, imperative language accesses data. <p> Jade eliminates these limitations by providing a more advanced construct and additional access specification statements <ref> [17] </ref>. These language features allow programmers to express more advanced concurrency patterns with multiple synchronization points within a single task. 3 Communication Optimization Algorithms Access specifications give Jade implementations advance information about how each task will access data. <p> The synchronizer uses a queue-based algorithm to determine when tasks can execute without violating the dynamic data dependence constraints <ref> [17] </ref>. The scheduler takes the resulting pool of enabled tasks generated by the synchronizer and assigns them to processors for execution, using a distributed task stealing algorithm to dynamically balance the load. <p> This heuristic attempts to improve the locality of the computation by executing tasks close to the data that they will access. We have implemented several variants of the locality heuristic, each tailored for the different memory hierarchies of different machines <ref> [17] </ref>. <p> If multiple tasks read the same object, the communicator replicates the object so that the tasks can concurrently read their own local copies instead of serially reading a unique copy. The computation that determines which processor owns the object is integrated into the Jade synchronization algorithm <ref> [17] </ref>. Each remote object fetch therefore generates two messages: a small message from the processor that will execute the task to the owner requesting the object and a potentially much larger message containing the object sent from the owner back to the executing processor. <p> For Panel Cholesky several factors combine to limit the performance, among them an inherent lack of con-currency in the basic parallel computation [21] and the task management overhead, which lengthens the critical path <ref> [17] </ref>. Figure 11 presents the task management percentage for Panel Cholesky. <p> This figure shows that, at 16 processors and above, the task management overhead is the limiting factor on the overall performance. For Panel Cholesky the locality optimization level has an effect, but the effect is diluted by other sources of inefficiency such as the task management overhead <ref> [17] </ref>. As for the DASH versions, moving tasks off their target processors in an attempt to balance the load fails to improve the overall performance. <p> The more advanced Jade constructs also support pipelined access to objects <ref> [17] </ref>. Parallelizing compilers for message passing machines analyze a program written in a serial programming language and generate parallel code [1]. Part of the compilation task entails the generation of message passing operations that transfer data produced on one processor to all of the processors that consume the data.
Reference: [18] <author> M. Rinard and M. Lam. </author> <booktitle> Semantic foundations of Jade. In Proceedings of the Nineteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 105-118, </pages> <address> Albuquerque, NM, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: In Section 6 we survey related work; we conclude in Section 7. 2 The Jade Programming Language This section provides a brief overview of the Jade language; other publications contain a complete description <ref> [17, 18, 20] </ref>. Jade is a set of constructs that programmers use to describe how a program written in a sequential, imperative language accesses data.
Reference: [19] <author> M. Rinard, D. Scales, and M. Lam. </author> <title> Heterogeneous parallel programming in Jade. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 245-256, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Many parallel languages, on the other hand, fail to give the implementation the data usage information that it needs to automatically apply sophisticated communication optimizations on a variety of computational platforms [10, 2, 6]. This paper describes our experience with communication optimizations in the context of Jade <ref> [19, 20] </ref>, a portable, implicitly parallel programming language designed for exploiting task-level concurrency. Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to describe how parts of the program access data.
Reference: [20] <author> M. Rinard, D. Scales, and M. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Many parallel languages, on the other hand, fail to give the implementation the data usage information that it needs to automatically apply sophisticated communication optimizations on a variety of computational platforms [10, 2, 6]. This paper describes our experience with communication optimizations in the context of Jade <ref> [19, 20] </ref>, a portable, implicitly parallel programming language designed for exploiting task-level concurrency. Jade programmers start with a program written in a standard serial, imperative language, then use Jade constructs to describe how parts of the program access data. <p> In Section 6 we survey related work; we conclude in Section 7. 2 The Jade Programming Language This section provides a brief overview of the Jade language; other publications contain a complete description <ref> [17, 18, 20] </ref>. Jade is a set of constructs that programmers use to describe how a program written in a sequential, imperative language accesses data.
Reference: [21] <author> E. Rothberg. </author> <title> Exploiting the memory hierarchy in sequential and parallel sparse Cholesky factorization. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: The task management overhead delays the creation of tasks, which in turn extends the critical path of the computation. For Panel Cholesky several factors combine to limit the performance, among them an inherent lack of con-currency in the basic parallel computation <ref> [21] </ref> and the task management overhead, which lengthens the critical path [17]. Figure 11 presents the task management percentage for Panel Cholesky.
Reference: [22] <author> D. Scales and M. S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First Usenix Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: This system differs from the COOL affinity scheduler and the Jade locality heuristic in that it has a single FIFO queue per processor. There is no provision to avoid interleaving the execution of multiple tasks with different affinity objects. SAM <ref> [22] </ref> provides the abstraction of a single shared object store on message passing machines. The SAM implementation automatically replicates data for concurrent read access; experimental results from SAM applications illustrate the importance of this optimization [22]. <p> SAM <ref> [22] </ref> provides the abstraction of a single shared object store on message passing machines. The SAM implementation automatically replicates data for concurrent read access; experimental results from SAM applications illustrate the importance of this optimization [22]. SAM also provides lower-level primitives that programmers can use to explicitly control the movement of data. These primitives allow programmers to overlap communication and computation by asynchronously prefetching data and eagerly transferring data from producing to consuming processors.
Reference: [23] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The computational kernel is Panel Cholesky, which factors a sparse positive-definite matrix. The SPLASH benchmark set <ref> [23] </ref> contains variants of the Water, Ocean and Panel Cholesky applications. We next discuss the parallel behavior of each application. * Water: Water performs an interleaved sequence of parallel and serial phases.
References-found: 23

