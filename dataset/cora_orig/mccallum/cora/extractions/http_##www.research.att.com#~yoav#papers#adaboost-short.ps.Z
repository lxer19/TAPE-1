URL: http://www.research.att.com/~yoav/papers/adaboost-short.ps.Z
Refering-URL: http://www.research.att.com/~yoav/publications.html
Root-URL: 
Email: fyoav, schapireg@research.att.com  
Title: A decision-theoretic generalization of on-line learning and an application to boosting how the weight-update rule
Author: Yoav Freund Robert E. Schapire 
Address: 600 Mountain Avenue Murray Hill, NJ 07974-0636  
Affiliation: AT&T Bell Laboratories  
Date: March, 1995.  
Note: Appearing in the proceedings of the Second European Conference on Computational Learning Theory,  also show  
Abstract: We consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update rule of Littlestone and Warmuth [10] can be adapted to this model yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games and prediction of points in R n
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Nicolo Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We call this loss function the mixture loss. In this paper, we always assume that the loss suffered by any strategy is bounded so that, without loss of generality, ` t i 2 <ref> [0; 1] </ref>. Besides this condition, we make no assumptions about the form of the loss vectors ` t , or about the manner in which they are generated; indeed, the adversary's choice for ` t may even depend on the allocator's chosen mixture p t . <p> Thus, as T increases, this difference decreases to zero. Our results for the on-line allocation model can be applied to a wide variety of learning problems, as we describe in Section 3. In particular, we generalize the results of Littlestone and Warmuth [10] and Cesa-Bianchi et al. <ref> [1] </ref> for the problem of predicting a binary sequence using the advice of a team of experts. <p> That is, at time t, Hedge (fi) chooses the distribution vector p t = P N i After the loss vector ` t has been received, the weight vector w t is updated using the multiplicative rule w t+1 i U fi (` t Here, U fi : <ref> [0; 1] </ref> ! [0; 1] is any function, parameterized by fi 2 [0; 1], which satisfies fi r U fi (r) 1 (1 fi)r: (3) (It can be shown that such a value U fi (r) always exists for any fi and r in the stated ranges [10].) 2.1 Analysis The <p> is, at time t, Hedge (fi) chooses the distribution vector p t = P N i After the loss vector ` t has been received, the weight vector w t is updated using the multiplicative rule w t+1 i U fi (` t Here, U fi : <ref> [0; 1] </ref> ! [0; 1] is any function, parameterized by fi 2 [0; 1], which satisfies fi r U fi (r) 1 (1 fi)r: (3) (It can be shown that such a value U fi (r) always exists for any fi and r in the stated ranges [10].) 2.1 Analysis The analysis of Hedge <p> vector p t = P N i After the loss vector ` t has been received, the weight vector w t is updated using the multiplicative rule w t+1 i U fi (` t Here, U fi : <ref> [0; 1] </ref> ! [0; 1] is any function, parameterized by fi 2 [0; 1], which satisfies fi r U fi (r) 1 (1 fi)r: (3) (It can be shown that such a value U fi (r) always exists for any fi and r in the stated ranges [10].) 2.1 Analysis The analysis of Hedge (fi) mimics directly that given by Littlestone and Warmuth. <p> Consider the following set-up used by Chung [2]. We are given a decision space D, a space of outcomes Y, and a bounded loss function : D fi Y ! <ref> [0; 1] </ref>. (Actually, our results require only that be bounded, but, by rescaling, we can assume that its range is [0; 1].) At every time step t, the learning algorithm selects a decision d t 2 D, receives an outcome y t 2 Y, and suffers loss (d t ; y <p> We are given a decision space D, a space of outcomes Y, and a bounded loss function : D fi Y ! <ref> [0; 1] </ref>. (Actually, our results require only that be bounded, but, by rescaling, we can assume that its range is [0; 1].) At every time step t, the learning algorithm selects a decision d t 2 D, receives an outcome y t 2 Y, and suffers loss (d t ; y t ). <p> Bounds of this type were previously proved in the binary case (k = 2) by Littlestone and Warmuth [10] using the same algorithm. Their algorithm was later improved by Vovk [12] and Cesa-Bianchi et al. <ref> [1] </ref>. The main result of this section is a proof that such bounds can be shown to hold for any bounded loss function. Example 2. The loss function may represent an arbitrary matrix game, such as rock, paper, scissors. <p> To see how to handle the problem of finding deterministic predictions, notice that the loss function (d; y) is convex with respect to d: jj (ad 1 + (1 a)d 2 ) yjj ajjd 1 yjj + (1 a)jjd 2 yjj (13) for any a 2 <ref> [0; 1] </ref> and any y 2 Y. Thus we can do as follows. At time t, the learner predicts with the weighted average of the experts' predictions: d t = P N i e t i where e t n is the prediction of the ith expert at time t. <p> After some amount of time, the learner must output a hypothesis h : X ! <ref> [0; 1] </ref>. 1 The error of the hypothesis h is the expected value E x~D (jh (x) c (x)j) where x is chosen according to D. <p> To be more precise, let r (i) = t=1 log 1 P T fi t be a weighted average of the weak hypotheses h t . We will here consider final hypotheses of the form h f (i) = F (r (i)) where F : <ref> [0; 1] </ref> ! [0; 1]. For the version of AdaBoost given in Figure 1, F (r) is the hard threshold that equals 1 if r 1=2 and 0 otherwise. In this section, we will instead use soft threshold functions that take values in [0; 1]. <p> To be more precise, let r (i) = t=1 log 1 P T fi t be a weighted average of the weak hypotheses h t . We will here consider final hypotheses of the form h f (i) = F (r (i)) where F : <ref> [0; 1] </ref> ! [0; 1]. For the version of AdaBoost given in Figure 1, F (r) is the hard threshold that equals 1 if r 1=2 and 0 otherwise. In this section, we will instead use soft threshold functions that take values in [0; 1]. <p> = F (r (i)) where F : <ref> [0; 1] </ref> ! [0; 1]. For the version of AdaBoost given in Figure 1, F (r) is the hard threshold that equals 1 if r 1=2 and 0 otherwise. In this section, we will instead use soft threshold functions that take values in [0; 1]. As mentioned above, when h f (i) 2 [0; 1], we can interpret h f as a randomized hypothesis and h f (i) as the probability of predicting 1. Then the error E i~D [jh f (i) c (i)j] is simply the probability of an incorrect prediction. <p> <ref> [0; 1] </ref>. For the version of AdaBoost given in Figure 1, F (r) is the hard threshold that equals 1 if r 1=2 and 0 otherwise. In this section, we will instead use soft threshold functions that take values in [0; 1]. As mentioned above, when h f (i) 2 [0; 1], we can interpret h f as a randomized hypothesis and h f (i) as the probability of predicting 1. Then the error E i~D [jh f (i) c (i)j] is simply the probability of an incorrect prediction. Theorem 6. <p> Theorem 6. Let * 1 ; : : : ; * T be as in Theorem 5, and let r (i) be as defined above. Let the modified final hypothesis be defined by h f = F (r (i)) where F satisfies the following for r 2 <ref> [0; 1] </ref>: 1 t=1 ! 1=2r Then the error * of h f is bounded above by * 2 T 1 t=1 * t (1 * t ): For instance, it can be shown that the sigmoid function F (r) = Q T 2r1 1 satisfies the conditions of the theorem.
Reference: 2. <author> Thomas H. Chung. </author> <title> Approximate methods for sequential decision making using expert advice. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 183-189, </pages> <year> 1994. </year>
Reference-contexts: Related generalizations of the expert prediction model were studied by Vovk [12], Kivinen and Warmuth [9], and Haussler, Kivinen and Warmuth [8]. Like us, these authors focused primarily on multiplicative weight-update algorithms. Chung <ref> [2] </ref> also presented a generalization, giving the problem a game-theoretic treatment. Finally, in Section 4, we show how a similar algorithm can be used for boosting, i.e., for converting any weak PAC learning algorithm into a strong PAC learning algorithm. <p> Lemma 3 can also be applied to the other bounds given in Theorem 2 to obtain analogous results. 3 Applications The framework described up to this point is quite general and can be applied in a wide variety of learning problems. Consider the following set-up used by Chung <ref> [2] </ref>.
Reference: 3. <author> Thomas M. </author> <title> Cover. Universal portfolios. </title> <journal> Mathematics of Finance, </journal> <volume> 1(1) </volume> <pages> 1-29, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Thus, it can also be applied, for example, to the squared-distance loss function (d; y) = jjd yjj 2 , as well as the log loss function (d; y) = ln (d y) used by Cover <ref> [3] </ref> for the design of universal investment portfolios. (In this last case, D is the set of probability vectors on n points, and Y = [1=B; B] n for some constant B &gt; 0.) In many of the cases listed above, superior algorithms or analyses are known.
Reference: 4. <author> Harris Drucker, Robert Schapire, and Patrice Simard. </author> <title> Boosting performance in neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 705-719, </pages> <year> 1993. </year>
Reference-contexts: An interesting alternative to this practice would be to use the algorithm AdaBoost to find a combination rule which, by Theorem 5, has a guaranteed non-trivial accuracy. Finally, note that AdaBoost, unlike boost-by-majority, combines the weak hypotheses by summing their probabilistic predictions. Drucker, Schapire and Simard <ref> [4] </ref>, in experiments they performed using boosting to improve the performance of a real-valued neural network, observed that summing the outcomes of the networks and then selecting the best prediction performs better than selecting the best prediction of each network and 13 then combining them with a majority rule.
Reference: 5. <author> Yoav Freund. </author> <title> Data Filtering and Distribution Modeling Algorithms for Machine Learning. </title> <type> PhD thesis, </type> <institution> University of California at Santa Cruz, </institution> <year> 1993. </year> <note> Retrievable from: ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-37.ps.Z. </note>
Reference-contexts: Chung [2] also presented a generalization, giving the problem a game-theoretic treatment. Finally, in Section 4, we show how a similar algorithm can be used for boosting, i.e., for converting any weak PAC learning algorithm into a strong PAC learning algorithm. Unlike the previous boosting algorithms of Freund <ref> [5, 6] </ref> and Schapire [11], the new algorithm needs no prior knowledge of the accuracy of the hypotheses of the weak learning algorithm. <p> Schapire [11] showed that any weak learning algorithm can be efficiently transformed or boosted into a strong learning algorithm. Later, Freund <ref> [5, 6] </ref> presented the boost-by-majority algorithm that is considerably more efficient than Schapire's algorithm.
Reference: 6. <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <journal> Information and Computation, </journal> <note> To appear. </note>
Reference-contexts: Chung [2] also presented a generalization, giving the problem a game-theoretic treatment. Finally, in Section 4, we show how a similar algorithm can be used for boosting, i.e., for converting any weak PAC learning algorithm into a strong PAC learning algorithm. Unlike the previous boosting algorithms of Freund <ref> [5, 6] </ref> and Schapire [11], the new algorithm needs no prior knowledge of the accuracy of the hypotheses of the weak learning algorithm. <p> Schapire [11] showed that any weak learning algorithm can be efficiently transformed or boosted into a strong learning algorithm. Later, Freund <ref> [5, 6] </ref> presented the boost-by-majority algorithm that is considerably more efficient than Schapire's algorithm. <p> This method, called boosting by sampling, is described in detail by Freund <ref> [6, Section 3.2] </ref>. The new boosting algorithm is described in Figure 1; we call the algorithm AdaBoost because it adjusts adaptively to the errors of the weak hypotheses returned by WeakLearn. The algorithm has a main loop which is iterated T times. <p> This is exactly the same closed-form bound that is given for the boost-by-majority algorithm <ref> [6] </ref>.
Reference: 7. <author> James Hannan. </author> <title> Approximation to Bayes risk in repeated play. </title> <editor> In M. Dresher, A. W. Tucker, and P. Wolfe, editors, </editor> <title> Contributions to the Theory of Games, </title> <booktitle> volume III, </booktitle> <pages> pages 97-139. </pages> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: nothing at all about the game that is being played (so that is unknown to the learner), and even if the adversarial opponent has complete knowledge both of the game that is being played and the algorithm that is being used by the learner. (See the related work of Hannan <ref> [7] </ref>.) 7 Example 4. Suppose that D = Y is the unit ball in R n , and that (d; y) = jjd yjj.
Reference: 8. <author> David Haussler, Jyrki Kivinen, and Manfred K. Warmuth. </author> <title> Tight worst-case loss bounds for predicting with expert advice. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: Our bounds express explicitly the rate at which the loss of the learning algorithm approaches that of the best expert. Related generalizations of the expert prediction model were studied by Vovk [12], Kivinen and Warmuth [9], and Haussler, Kivinen and Warmuth <ref> [8] </ref>. Like us, these authors focused primarily on multiplicative weight-update algorithms. Chung [2] also presented a generalization, giving the problem a game-theoretic treatment.
Reference: 9. <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Using experts for predicting continuous outcomes. </title> <booktitle> In Computational Learning Theory: </booktitle> <volume> EuroCOLT '93, </volume> <pages> pages 109-120, </pages> <year> 1994. </year>
Reference-contexts: Our bounds express explicitly the rate at which the loss of the learning algorithm approaches that of the best expert. Related generalizations of the expert prediction model were studied by Vovk [12], Kivinen and Warmuth <ref> [9] </ref>, and Haussler, Kivinen and Warmuth [8]. Like us, these authors focused primarily on multiplicative weight-update algorithms. Chung [2] also presented a generalization, giving the problem a game-theoretic treatment. <p> In the one-dimensional case (n = 1), this case was previously analyzed by Littlestone and Warmuth [10], and later improved upon by Kivinen and Warmuth <ref> [9] </ref>. This result depends only on the convexity and the bounded range of the loss function (d; y) with respect to d.
Reference: 10. <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: In Section 2, we show that Littlestone and Warmuth's <ref> [10] </ref> weighted majority algorithm can be generalized to handle this problem, and we prove a number of bounds on the net loss. <p> Thus, as T increases, this difference decreases to zero. Our results for the on-line allocation model can be applied to a wide variety of learning problems, as we describe in Section 3. In particular, we generalize the results of Littlestone and Warmuth <ref> [10] </ref> and Cesa-Bianchi et al. [1] for the problem of predicting a binary sequence using the advice of a team of experts. <p> The algorithm and its analysis are direct generalizations of Littlestone and Warmuth's weighted majority algorithm <ref> [10] </ref>. The algorithm maintains a weight vector whose value at time t is denoted w t = hw t N i. At all times, all weights will be nonnegative. <p> U fi : [0; 1] ! [0; 1] is any function, parameterized by fi 2 [0; 1], which satisfies fi r U fi (r) 1 (1 fi)r: (3) (It can be shown that such a value U fi (r) always exists for any fi and r in the stated ranges <ref> [10] </ref>.) 2.1 Analysis The analysis of Hedge (fi) mimics directly that given by Littlestone and Warmuth. The main idea is to derive upper and lower bounds on P N T +1 i which, together, imply an upper bound on the loss of the algorithm. <p> Bounds of this type were previously proved in the binary case (k = 2) by Littlestone and Warmuth <ref> [10] </ref> using the same algorithm. Their algorithm was later improved by Vovk [12] and Cesa-Bianchi et al. [1]. The main result of this section is a proof that such bounds can be shown to hold for any bounded loss function. Example 2. <p> Thus, our results in this case give explicit bounds on the total error (i.e., distance between predicted and observed points) for the learner relative to the best of a team of experts. In the one-dimensional case (n = 1), this case was previously analyzed by Littlestone and Warmuth <ref> [10] </ref>, and later improved upon by Kivinen and Warmuth [9]. This result depends only on the convexity and the bounded range of the loss function (d; y) with respect to d.
Reference: 11. <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: Finally, in Section 4, we show how a similar algorithm can be used for boosting, i.e., for converting any weak PAC learning algorithm into a strong PAC learning algorithm. Unlike the previous boosting algorithms of Freund [5, 6] and Schapire <ref> [11] </ref>, the new algorithm needs no prior knowledge of the accuracy of the hypotheses of the weak learning algorithm. Rather, it adapts to the accuracies of the generated hypotheses and 2 generates a weighted majority hypothesis in which the weight of each weak hypothesis is a function of its accuracy. <p> A weak PAC-learning algorithm satisfies the same conditions but only for * 1=2 fl where fl &gt; 0 is either a constant, or decreases as 1=p where p is a polynomial in the relevant parameters. Schapire <ref> [11] </ref> showed that any weak learning algorithm can be efficiently transformed or boosted into a strong learning algorithm. Later, Freund [5, 6] presented the boost-by-majority algorithm that is considerably more efficient than Schapire's algorithm.
Reference: 12. <author> Volodimir G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383, </pages> <year> 1990. </year> <month> 15 </month>
Reference-contexts: Our bounds express explicitly the rate at which the loss of the learning algorithm approaches that of the best expert. Related generalizations of the expert prediction model were studied by Vovk <ref> [12] </ref>, Kivinen and Warmuth [9], and Haussler, Kivinen and Warmuth [8]. Like us, these authors focused primarily on multiplicative weight-update algorithms. Chung [2] also presented a generalization, giving the problem a game-theoretic treatment. <p> Bounds of this type were previously proved in the binary case (k = 2) by Littlestone and Warmuth [10] using the same algorithm. Their algorithm was later improved by Vovk <ref> [12] </ref> and Cesa-Bianchi et al. [1]. The main result of this section is a proof that such bounds can be shown to hold for any bounded loss function. Example 2. The loss function may represent an arbitrary matrix game, such as rock, paper, scissors.
References-found: 12

