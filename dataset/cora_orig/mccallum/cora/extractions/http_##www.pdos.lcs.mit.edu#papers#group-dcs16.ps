URL: http://www.pdos.lcs.mit.edu/papers/group-dcs16.ps
Refering-URL: http://www.pdos.lcs.mit.edu/PDOS-papers.html
Root-URL: 
Title: An Evaluation of the Amoeba Group Communication System  
Author: M. Frans Kaashoek Andrew S. Tanenbaum 
Address: Cambridge, U.S.A. Amsterdam, The Netherlands  
Affiliation: Laboratory for Computer Science Dept. of Math and Computer Science M.I.T. Vrije Universiteit  
Abstract: The Amoeba group communication system has two unique aspects: (1) it uses a sequencer-based protocol with negative acknowledgements for achieving a total order on all group messages; and (2) users choose the degree of fault tolerance they desire. This paper reports on our design decisions in retrospect, the performance of the Amoeba group system, and our experiences using the system. We conclude that sequencer-based group protocols achieve high performance (comparable to Amoeba's fast remote procedure call implementation), that the scalability of our sequencer-based protocols is limited by message processing time, and that the flexibility and modularity of user-level implementations of protocols is likely to outweigh the potential performance loss. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Amir, D. Dolev, S. Kramer, and D. Malki. Transis: </author> <title> A communication sub-system for high availability. </title> <booktitle> In Proc. 22nd Int'l Symp. on Fault-Tolerant Computing, </booktitle> <pages> pages 76-84, </pages> <address> Boston, MA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: After publication of our initial results showing that a sequencer-based protocol performs well [17], a number of designers of group systems have chosen to use a sequencer-based protocol; most newer systems, such as Horus [33] and Transis <ref> [1] </ref>, use a dynamic sequencer-based protocol, in which the sequencer migrates to the sender so that the next message can be sent without having to go remotely to acquire a sequence number. Initial experience with these systems indicates that this is an appropriate choice when messages come in bursts. <p> For point-to-point communication many flow control algorithms exists [29], but it is not immediately clear how these should be extended to multicast communication. Some recent progress has been made in this area <ref> [1] </ref>, but the results are not widely applicable yet. The measurements in this section therefore do not include the time for flow control and we have used an arbitrary, but reasonable upper bound to the message size. <p> In some applications one process sends multiple messages before the next process sends a message. The performance of these applications could have benefited from a migrating sequencer, as used in more recent systems such as Horus [33] and Transis <ref> [1] </ref>. Instead, we found ourselves placing the process that is sending most messages on the kernel that runs the sequencer. In retrospect, the performance gained by migrating the sequencer may be worth the additional complexity in the protocol for distributing the history buffer. <p> The failure detection and group rebuilding code turned out to be the hardest parts of the system to get correct. Newer versions of Isis [26] and more recent systems such systems such Transis <ref> [1] </ref> separate these pieces of functionality cleanly. 6 Related Work In this section we will compare Amoeba with other complete group communication packages and their protocols; a detailed comparison of our reliable broadcast protocol with other protocols can be found in [14]. <p> Amir et al. describe a recently-built system, called Tran-sis, that supports a number of protocols with varying properties <ref> [1] </ref>. It offers membership protocols, basic multicast (reliable group communication without order), causal-ordered multicast, totally-ordered multicast, and safe multicast (i.e., it delivers a message after all active processors have acknowledged it).
Reference: [2] <author> R. Bhoedjang, T. Ruhl, R. Hofman, K. Langendoen, H. Bal, and F. Kaashoek. Panda: </author> <title> A portable platform to support parallel programming languages. </title> <booktitle> In Proc. of Third Symp. on Experiences with Distributed and Multiprocessor S, </booktitle> <pages> pages 213-226, </pages> <address> San Diego, CA, </address> <month> Sept </month> <year> 1993. </year>
Reference-contexts: Since late 1992 this version of the Amoeba system has been publicly and commercially available; over 150 sites have picked the system up. In 1993 the core of the group communication protocols was incorporated in the Panda system <ref> [2] </ref>, a portable platform for parallel computing, which runs on cluster of UNIX workstations and supercomputers, such as a 512-node Parsytec and a 128-node CM-5 [12, 13]. Both the supercomputers provide reliable communication, so the protocols on these machines are less complex.
Reference: [3] <author> K.P. Birman, R. Cooper, T.A. Joseph, K.P. Kane, F. Schmuck, and M. Wood. </author> <title> Isis a distributed programming environment. Technical Report User's Guide and Reference Manual, </title> <institution> Cornell University, </institution> <address> Ithaca, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Second, the system did not have good support for a process (re)joining a given group. A library for atomic state transfer as provided in Isis <ref> [3] </ref> would have again simplified building these fault-tolerant programs. Wood discusses building fault-tolerant applications for Amoeba in more detail [35]. Our decision to make the group primitives blocking and to achieve parallelism through running multiple threads per process has forced us to write cleanly-structured applications.
Reference: [4] <author> K.P. Birman and T.A. Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 5(1) </volume> <pages> 47-76, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: The actual implementation of their protocol uses, unlike ours, physical broadcast instead of multicast for all messages and is restricted to a single LAN. The protocols that are used in the first complete system supporting ordered group communication, described in <ref> [4] </ref>, are implemented in the Isis system. The Isis system is primarily intended for doing fault-tolerant computing. Thus, Isis tries to make broadcast as fast as possible in the context of possible processor failures.
Reference: [5] <author> K.P. Birman, A. Schiper, and P. Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: First, distributed protocols for total ordering are more complex, and often perform worse. For example, the distributed protocols for total ordering in Isis have been replaced by a dynamic-centralized protocol because the distributed one was too complex and slow <ref> [5] </ref>. Second, today's computers are very reliable and it is therefore unlikely that the sequencer will crash. The major disadvantage of having a sequencer is that the protocol does not scale to enormous groups. In practice, however, this drawback is minor. <p> If, however, an application requires fault tolerance, our system can trade performance against fault tolerance. The primary difference is that Isis emphasizes fault tolerance where as our work emphasizes high performance. Recently the protocols for Isis have been redesigned <ref> [5] </ref>. The system is now completely based on a broadcast primitive that provides causal ordering. The implementation 9 of this primitive uses reliable point-to-point communica-tion. The protocol for totally-ordered broadcast is based on causal broadcast.
Reference: [6] <author> A.D. Birrell and B.J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> Feb. </month> <year> 1984. </year> <month> 10 </month>
Reference-contexts: The proposed primitives are also efficient: if a network supports physical multicast, a reliable group send can be done in just slightly more than two messages on the average, so that the performance of a reliable group send is roughly comparable to that of a remote procedure call (RPC) <ref> [6] </ref>. In addition, the This research was performed at the Vrije Universiteit as part of the first author's Ph.D. thesis primitives are flexible: user applications can, for example, trade performance against fault tolerance. <p> Similar observations have been made for RPC systems <ref> [6] </ref>. Unfortunately the decision to have multiple threads per process and nonblocking group primitives sometimes made it hard to port the group system to other existing systems based on a different model.
Reference: [7] <author> J. Chang and N.F. Maxemchuk. </author> <title> Reliable broadcast protocols. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 2(3) </volume> <pages> 251-273, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Third, ac-knowledgements are piggybacked on regular data messages to further reduce the number of protocol messages. These ideas are well known techniques. Chang and Maxemchuck, for example, discuss a protocol similar to ours that also combines these three ideas <ref> [7] </ref>. Although at first sight it may seem strange to use a centralized sequencer in a distributed system, this decision is attractive. First, distributed protocols for total ordering are more complex, and often perform worse. <p> However, this can be implemented by a client and a server (e.g., the protocol described by Navaratnam, Chanson, and Neufeld [22] runs on top of V). The protocols in our systems were influenced by Chang and Maxemchuck (CM), who describe a family of broadcast protocols <ref> [7] </ref>. These protocols differ mainly in the degree of fault tolerance that they provide. Our protocol for r = 0 resembles their protocol that is not fault tolerant (i.e., it may lose messages if processors fail), but ours is optimized for the common case of no communication failures.
Reference: [8] <author> D.R. Cheriton and W. Zwaenepoel. </author> <title> Distributed process groups in the v kernel. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 3(2) </volume> <pages> 77-107, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: The first system supporting group communication, described in <ref> [8] </ref>, is the V system. It integrates RPC communication with broadcast communication in a flexible way. If a client sends a request message to a process group, V tries to deliver the message at all members in the group.
Reference: [9] <author> S.E. Deering and D.R. Cheriton. </author> <title> Multicast routing in datagram internetworks and extended lans. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 8(2) </volume> <pages> 85-110, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Both the RPC and group communication modules use the Fast Local Internet Protocol (FLIP) [14] to send messages. FLIP is a connectionless (datagram) protocol, roughly analogous to IP [25], but with increased functionality. For the experiments performed in this section we could have used multicast-IP <ref> [9] </ref> instead of FLIP, but FLIP has other properties that makes it attractive for distributed computing. One of the major differences between IP and FLIP is that IP addresses identify a host while FLIP addresses identify a process or a group of processes.
Reference: [10] <author> M.J. Fischer, N.A. Lynch, </author> <title> and M.S. Paterson. Impossibility of distributed consensus with one faulty process. </title> <journal> Journal of the ACM, </journal> <volume> 32(2) </volume> <pages> 374-382, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: In this case the recovery algorithm starts again until it succeeds or fails. To rebuild a group requires consensus on which proces sors are alive. However, it is known that achieving consensus in an asynchronous distributed system with one faulty processor is impossible <ref> [10] </ref>. To be able to reach a decision about whether a process is alive, the algorithm sends messages asking the recipient to respond. If after a certain number of trials a process does not respond, the process is declared dead.
Reference: [11] <author> V. Hadzilacos and S. Toueg. </author> <title> Distributed Systems 2nd ed., chapter Fault-Tolerant Broadcasts and Related Problems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1993. </year>
Reference-contexts: Applications requiring these semantics have to implement them explicitly. For a more thorough discussion of the relation between broadcast semantics, failures, protocols for different types of failures, see Hadzilacos and Toueg <ref> [11] </ref>. 2.2 Ordering The group primitives guarantee a total ordering (with FIFO) per group. If two members send messages A and B concurrently, the protocol guarantees that all members of the group either receive first message A and then B, or first B and then A.
Reference: [12] <author> H-P. Heinzle, H.E. Bal, and K.G. Langendoen. </author> <title> Implementing object-based distributed shared memory on transputers. </title> <booktitle> In World Transputer Congress 1994, </booktitle> <pages> pages 390-405, </pages> <address> Lake Como, Italy, </address> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: In 1993 the core of the group communication protocols was incorporated in the Panda system [2], a portable platform for parallel computing, which runs on cluster of UNIX workstations and supercomputers, such as a 512-node Parsytec and a 128-node CM-5 <ref> [12, 13] </ref>. Both the supercomputers provide reliable communication, so the protocols on these machines are less complex. The group communication primitives have been used in running parallel applications [15, 18, 30, 35].
Reference: [13] <author> W.C. Hsieh, K.L. Johnson, M.F. Kaashoek, D.A. Wal-lach, and W.E. Weihl. </author> <title> Efficient implementation of high-level languages on user-level communication. </title> <type> Technical Report MIT/LCS/TR-616, </type> <address> Cambridge, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: In 1993 the core of the group communication protocols was incorporated in the Panda system [2], a portable platform for parallel computing, which runs on cluster of UNIX workstations and supercomputers, such as a 512-node Parsytec and a 128-node CM-5 <ref> [12, 13] </ref>. Both the supercomputers provide reliable communication, so the protocols on these machines are less complex. The group communication primitives have been used in running parallel applications [15, 18, 30, 35].
Reference: [14] <author> M.F. Kaashoek. </author> <title> Group Communication in Distributed Computer Systems. </title> <type> PhD thesis, </type> <institution> Vrije Universiteit, Am-sterdam, </institution> <year> 1992. </year>
Reference-contexts: A detailed description of the implementation can be found elsewhere <ref> [14, 16] </ref>. We give enough information to be able to understand the performance experiments described in the next section. 3.1 The broadcast protocols In the common case, the Amoeba broadcast protocol uses two messages per SendToGroup. <p> The communication system in the kernel consists of 3 layers (see Table 2). The top layer implements the protocols for group communication and RPC. The protocols described in the previous section are implemented here. Both the RPC and group communication modules use the Fast Local Internet Protocol (FLIP) <ref> [14] </ref> to send messages. FLIP is a connectionless (datagram) protocol, roughly analogous to IP [25], but with increased functionality. For the experiments performed in this section we could have used multicast-IP [9] instead of FLIP, but FLIP has other properties that makes it attractive for distributed computing. <p> [26] and more recent systems such systems such Transis [1] separate these pieces of functionality cleanly. 6 Related Work In this section we will compare Amoeba with other complete group communication packages and their protocols; a detailed comparison of our reliable broadcast protocol with other protocols can be found in <ref> [14] </ref>. The first system supporting group communication, described in [8], is the V system. It integrates RPC communication with broadcast communication in a flexible way. If a client sends a request message to a process group, V tries to deliver the message at all members in the group.
Reference: [15] <author> M.F. Kaashoek, R. Michiels, H.E. Bal, </author> <title> and A.S. Tanen-baum. Transparent fault-tolerance in parallel Orca programs. </title> <booktitle> In Proc. Symp. on Experiences with Distributed and Multiprocessor Systems II, </booktitle> <pages> pages 297-312, </pages> <address> New-port Beach, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Both the supercomputers provide reliable communication, so the protocols on these machines are less complex. The group communication primitives have been used in running parallel applications <ref> [15, 18, 30, 35] </ref>. This section reviews some of the design decision given our experience with using and implementing the group communication protocols. We will focus on the lessons learned. The sequencer-based protocol has proven to be an efficient, simple, and robust protocol to implement and to use. <p> Amoeba applications using group communication fall into two broad categories: (1) parallel computations and (2) replicated servers. Although we have developed and implemented a consistent checkpointing scheme for parallel applications <ref> [15] </ref>, most of the parallel applications are just restarted if a processor failures happens. All of them run with a resilience degree of zero. The replicated servers tend to run in small groups (about 3 members) and the overhead for the acknowledgements for a higher resilience degree is acceptable.
Reference: [16] <author> M.F. Kaashoek and A.S. Tanenbaum. </author> <title> Group communication in the amoeba distributed operating system. </title> <booktitle> In Proc. Eleventh Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pages 222-230, </pages> <address> Arlington, TX, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: A detailed description of the implementation can be found elsewhere <ref> [14, 16] </ref>. We give enough information to be able to understand the performance experiments described in the next section. 3.1 The broadcast protocols In the common case, the Amoeba broadcast protocol uses two messages per SendToGroup.
Reference: [17] <author> M.F. Kaashoek, A.S. Tanenbaum, S. Flynn Hummel, and H.E. Bal. </author> <title> An efficient reliable broadcast protocol. </title> <journal> Operating Systems Review, </journal> <volume> 23(4) </volume> <pages> 5-20, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: This decision simplifies the implementation because the sender of message always knows where the sequencer is, but may result in a loss of performance, as a message has to be transmitted first to the sequencer. After publication of our initial results showing that a sequencer-based protocol performs well <ref> [17] </ref>, a number of designers of group systems have chosen to use a sequencer-based protocol; most newer systems, such as Horus [33] and Transis [1], use a dynamic sequencer-based protocol, in which the sequencer migrates to the sender so that the next message can be sent without having to go remotely
Reference: [18] <author> M.F. Kaashoek, A.S. Tanenbaum, and K. Verstoep. </author> <title> Using group communication to implement a fault-tolerant directory service. </title> <booktitle> In Proc. 13th Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pages 130-139, </pages> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Both the supercomputers provide reliable communication, so the protocols on these machines are less complex. The group communication primitives have been used in running parallel applications <ref> [15, 18, 30, 35] </ref>. This section reviews some of the design decision given our experience with using and implementing the group communication protocols. We will focus on the lessons learned. The sequencer-based protocol has proven to be an efficient, simple, and robust protocol to implement and to use.
Reference: [19] <author> M.F. Kaashoek, R. van Renesse, H. van Staveren, </author> <title> and A.S. Tanenbaum. Flip: an internetwork protocol for supporting distributed systems. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 11(1) </volume> <pages> 73-106, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: We discuss the reasons that led to these design decisions in turn. 2.1 Reliability Amoeba's group primitives offer reliable communication: the group protocol automatically recovers from lost, garbled, and duplicate messages. Although Amoeba's network protocol supports unreliable group communication <ref> [19] </ref>, we decided to make only reliable group communication available to the programmer. This decision has the potential disadvantage that some users pay in performance for semantics that they do not need.
Reference: [20] <author> C. Maeda and B. Bershad. </author> <title> Protocol service decomposition for high performance networking. </title> <booktitle> 14th ACM Symp. on Operating Systems Principles, </booktitle> <pages> pages 244-255, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Recently Oey et al. reported on running the protocols in user space [23]. They measured a 32% performance decrease in communication performance for synthetic benchmarks, but for most applications the performance decrease was very small. In addition, recent work by Thekkath et al. [32], and Maeda and Ber-shad <ref> [20] </ref> shows how good performance can be obtained by carefully dividing the communication functionality between a user server and application library. It should be noted that at the time we implemented the Amoeba protocols these results were unknown.
Reference: [21] <author> S.J. Mullender, G. van Rossum, A.S. Tanenbaum, R. van Renesse, and H. van Staveren. </author> <title> Amoeba: a distributed operating system for the 1990s. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 44-53, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In addition, the This research was performed at the Vrije Universiteit as part of the first author's Ph.D. thesis primitives are flexible: user applications can, for example, trade performance against fault tolerance. The Amoeba group communication primitives have been implemented in the kernel of the Amoeba distributed operating system <ref> [21, 31] </ref>. The delay for a null broadcast to a group of 30 processes running on 20-MHz MC68030s connected by 10 Mbit/s Ethernet is 2.8 msec. The maximum throughput per group is 815 broadcasts per second per group.
Reference: [22] <author> S. Navaratnam, S. Chanson, and G. Neufeld. </author> <title> Reliable group communication in distributed systems. </title> <booktitle> In Proc. Eighth Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pages 439-446, </pages> <address> San Jose, CA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Additional replies from other members can be collected by the client by calling GetReply. Thus, the V system does not provide reliable, ordered broadcasting. However, this can be implemented by a client and a server (e.g., the protocol described by Navaratnam, Chanson, and Neufeld <ref> [22] </ref> runs on top of V). The protocols in our systems were influenced by Chang and Maxemchuck (CM), who describe a family of broadcast protocols [7]. These protocols differ mainly in the degree of fault tolerance that they provide.
Reference: [23] <author> M. Oey, K. Langendoen, and H.E. Bal. </author> <title> Comparing kernel-space and user-space communication protocols on Amoeba. </title> <booktitle> In Proc. of 15th Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pages 238-246, </pages> <address> Vancou-ver, Canada, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: We decided to implement the group protocols in the kernel, because we believed that the implementation should perform well in order to attract applications. It is unclear whether this was the right decision. Recently Oey et al. reported on running the protocols in user space <ref> [23] </ref>. They measured a 32% performance decrease in communication performance for synthetic benchmarks, but for most applications the performance decrease was very small.
Reference: [24] <author> L.L. Peterson, N.C. Buchholtz, and R.D. Schlichting. </author> <title> Preserving and using context information in IPC. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 7(3) </volume> <pages> 217-246, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: In addition to the layering of broadcast services, Transis has two other distinctive properties. It provides support for groups to remerge after a partition and it implements multicast flow control. Preliminary performance results using broadcast (instead of multicast) show that the system performs well. In <ref> [24] </ref> a communication mechanism is described called Psync. In Psync a group consists of a fixed number of processes and is closed. Messages are causally ordered. A library routine provides a primitive for total ordering.
Reference: [25] <author> J. Postel. </author> <title> Internet protocol. </title> <type> Technical Report RFC 791, </type> <institution> SRI Network Information Center, </institution> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: The top layer implements the protocols for group communication and RPC. The protocols described in the previous section are implemented here. Both the RPC and group communication modules use the Fast Local Internet Protocol (FLIP) [14] to send messages. FLIP is a connectionless (datagram) protocol, roughly analogous to IP <ref> [25] </ref>, but with increased functionality. For the experiments performed in this section we could have used multicast-IP [9] instead of FLIP, but FLIP has other properties that makes it attractive for distributed computing.
Reference: [26] <author> A.M. Ricciardi and K.P. Birman. </author> <title> Using process groups to implement failure detection in asynchronous envi-ronme. </title> <booktitle> In Proc. of the Tenth ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pages 341-351, </pages> <address> Quebec, Canada, </address> <year> 1991. </year>
Reference-contexts: We should have put this functionality in a separate module so that we could have reasoned about it independently of the rest of the system. The failure detection and group rebuilding code turned out to be the hardest parts of the system to get correct. Newer versions of Isis <ref> [26] </ref> and more recent systems such systems such Transis [1] separate these pieces of functionality cleanly. 6 Related Work In this section we will compare Amoeba with other complete group communication packages and their protocols; a detailed comparison of our reliable broadcast protocol with other protocols can be found in [14].
Reference: [27] <author> F.B. Schneider. </author> <title> Byzantine generals in action: Implementing fail-stop processes. </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> 2(2) </volume> <pages> 145-154, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: We decided to make the recovery from processor failures an option because providing these semantics is expensive and many applications do not need to recover from processor failures. We assume that processors fail due to crash failures <ref> [27] </ref>. Stronger semantics, such as automatic recovery from Byzantine failures (i.e., processors sending malicious or contradictory messages) and automatic recovery from network partitions, are not supported by the group primitives. Applications requiring these semantics have to implement them explicitly.
Reference: [28] <author> F.B. Schneider. </author> <title> Implementing fault-tolerant services using the state machine approach: A tutorial. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(4) </volume> <pages> 299-319, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: It never happens that member 1 sees A and then B, and member 2 sees B and then A. Many distributed applications are easy to implement with a total ordering, as the programmer can think of processes running in lockstep <ref> [28] </ref>. In the past, most designers have chosen weaker protocols than we have, making application building more difficult. There are three key ideas that make our approach feasible. First, to guarantee a total ordering the protocol uses a central 2 machine per group, called the sequencer.
Reference: [29] <author> A.S. Tanenbaum. </author> <title> Computer Networks 2nd ed. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Messages larger than a network packet size have to be fragmented into multiple packets. To prevent a sender from overrunning a receiver, flow control has to be performed on messages consisting of multiple packets. For point-to-point communication many flow control algorithms exists <ref> [29] </ref>, but it is not immediately clear how these should be extended to multicast communication. Some recent progress has been made in this area [1], but the results are not widely applicable yet.
Reference: [30] <author> A.S. Tanenbaum, M.F. Kaashoek, and H.E. Bal. </author> <title> Parallel programming using shared objects and broadcasting. </title> <journal> IEEE Computer, </journal> <volume> 25(8) </volume> <pages> 10-19, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Both the supercomputers provide reliable communication, so the protocols on these machines are less complex. The group communication primitives have been used in running parallel applications <ref> [15, 18, 30, 35] </ref>. This section reviews some of the design decision given our experience with using and implementing the group communication protocols. We will focus on the lessons learned. The sequencer-based protocol has proven to be an efficient, simple, and robust protocol to implement and to use.
Reference: [31] <author> A.S. Tanenbaum, R. van Renesse, H. van Staveren, G. Sharp, S.J. Mullender, A. Jansen, and G. van Rossum. </author> <title> Experiences with the amoeba distributed operating system. </title> <journal> Commun. ACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: In addition, the This research was performed at the Vrije Universiteit as part of the first author's Ph.D. thesis primitives are flexible: user applications can, for example, trade performance against fault tolerance. The Amoeba group communication primitives have been implemented in the kernel of the Amoeba distributed operating system <ref> [21, 31] </ref>. The delay for a null broadcast to a group of 30 processes running on 20-MHz MC68030s connected by 10 Mbit/s Ethernet is 2.8 msec. The maximum throughput per group is 815 broadcasts per second per group.
Reference: [32] <author> C.A. Thekkath, T.D. Nguyen, E. Moy, and E. La--zowska. </author> <title> Implementing network protocols at user level. </title> <booktitle> In Proc. of Int'l Conf. on Communications Architectures, Protocols and Applicatio, </booktitle> <pages> pages 64-73, </pages> <address> San Francisco, CA, 13-17, </address> <year> 1993. </year>
Reference-contexts: Recently Oey et al. reported on running the protocols in user space [23]. They measured a 32% performance decrease in communication performance for synthetic benchmarks, but for most applications the performance decrease was very small. In addition, recent work by Thekkath et al. <ref> [32] </ref>, and Maeda and Ber-shad [20] shows how good performance can be obtained by carefully dividing the communication functionality between a user server and application library. It should be noted that at the time we implemented the Amoeba protocols these results were unknown.
Reference: [33] <author> R. van Renesse, K.P. Birman, R. Cooper, B. Glade, and P. Stephenson. </author> <title> A RISC approach to process groups. </title> <booktitle> In Proc. Usenix Workshop on Micro-kernels and Other Kernel Architectures, </booktitle> <address> Seattle, WA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: After publication of our initial results showing that a sequencer-based protocol performs well [17], a number of designers of group systems have chosen to use a sequencer-based protocol; most newer systems, such as Horus <ref> [33] </ref> and Transis [1], use a dynamic sequencer-based protocol, in which the sequencer migrates to the sender so that the next message can be sent without having to go remotely to acquire a sequence number. <p> In some applications one process sends multiple messages before the next process sends a message. The performance of these applications could have benefited from a migrating sequencer, as used in more recent systems such as Horus <ref> [33] </ref> and Transis [1]. Instead, we found ourselves placing the process that is sending most messages on the kernel that runs the sequencer. In retrospect, the performance gained by migrating the sequencer may be worth the additional complexity in the protocol for distributing the history buffer. <p> A reimplementation of Isis, called Horus, achieves very high performance by packing multiple messages in a single network packet, by avoiding major bottlenecks in the communication path, and by using multicast-IP <ref> [33] </ref>. Amir et al. describe a recently-built system, called Tran-sis, that supports a number of protocols with varying properties [1]. It offers membership protocols, basic multicast (reliable group communication without order), causal-ordered multicast, totally-ordered multicast, and safe multicast (i.e., it delivers a message after all active processors have acknowledged it).
Reference: [34] <author> Deborah A. Wallach, Wilson C. Hsieh, Kirk L. John-son, M. Frans Kaashoek, and William E. Weihl. </author> <title> Optimistic active messages: A mechanism for scheduling communication with computation. </title> <booktitle> In Proc. of 5th Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 217-226, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Therefore, we have concluded that it is more important to reduce the software overhead of message processing than to make the protocol more distributed. We are currently experimenting with a new approach, called optimistic active messages, which reduces this software overhead for message processing <ref> [34] </ref>. In some applications one process sends multiple messages before the next process sends a message. The performance of these applications could have benefited from a migrating sequencer, as used in more recent systems such as Horus [33] and Transis [1].
Reference: [35] <author> M.D. Wood. </author> <title> Replicated RPC using amoeba closed group communication. </title> <booktitle> In Proc. of the 13th Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pages 499-507, </pages> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Both the supercomputers provide reliable communication, so the protocols on these machines are less complex. The group communication primitives have been used in running parallel applications <ref> [15, 18, 30, 35] </ref>. This section reviews some of the design decision given our experience with using and implementing the group communication protocols. We will focus on the lessons learned. The sequencer-based protocol has proven to be an efficient, simple, and robust protocol to implement and to use. <p> Second, the system did not have good support for a process (re)joining a given group. A library for atomic state transfer as provided in Isis [3] would have again simplified building these fault-tolerant programs. Wood discusses building fault-tolerant applications for Amoeba in more detail <ref> [35] </ref>. Our decision to make the group primitives blocking and to achieve parallelism through running multiple threads per process has forced us to write cleanly-structured applications.
Reference: [36] <author> M. Young, A. Tevenian, R. Rashid, D. Golub, J. Ep-pinger, J. Chew, W. Bolosky, D. Black, and R. Baron. </author> <title> Duality of memory and communication in the implementation of a multiprocessor. </title> <booktitle> In Proc. Eleventh Symp. on Operating Systems Principles, </booktitle> <pages> pages 63-67, </pages> <address> Austin, TX, </address> <month> Nov. </month> <year> 1987. </year> <month> 12 </month>
Reference-contexts: sequencer must copy the message three times: one additional copy from the history buffer to the Lance interface to broadcast the message. (If our Lance interface could send directly from main memory, this last copy could have been avoided.) If Amoeba had support for sophisticated memory management primitives like Mach <ref> [36] </ref>, the second copy from the history buffer to user space could also have been avoided; in this case one could map the page containing the history buffer into the user's address space, although manipulating the memory maps also group size is equal to the number of senders. group size is
References-found: 36

