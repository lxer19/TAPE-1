URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/dbj/www/ftp/srds92.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/dbj/www/ft.html
Root-URL: 
Email: mootaz@cs.rice.edu, dbj@cs.cmu.edu, willy@cs.rice.edu  
Title: The Performance of Consistent Checkpointing  
Author: Elmootazbellah Nabil Elnozahy David B. Johnson Willy Zwaenepoel 
Address: Houston, Texas 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: Consistent checkpointing provides transparent fault tolerance for long-running distributed applications. In this paper we describe performance measurements of an implementation of consistent checkpointing. Our measurements show that consistent checkpointing performs remarkably well. We executed eight compute-intensive distributed applications on a network of 16 diskless Sun-3/60 workstations, comparing the performance without check-pointing to the performance with consistent checkpoints taken at 2-minute intervals. For six of the eight applications, the running time increased by less than 1% as a result of the checkpointing. The highest overhead measured for any of the applications was 5.8%. Incremental check-pointing and copy-on-write checkpointing were the most effective techniques in lowering the running time overhead. These techniques reduce the amount of data written to stable storage and allow the checkpoint to proceed concurrently with the execution of the processes. The overhead of synchronizing the individual process checkpoints to form a consistent global checkpoint was much smaller. We argue that these measurements show that consistent checkpointing is an efficient way to provide fault tolerance for long-running distributed applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ahamad and L. Lin. </author> <title> Using checkpoints to localize the effects of faults in distributed systems. </title> <booktitle> In Proceedings of the 8th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 1-11, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Much of the previous work in consistent checkpoint-ing has focused on minimizing the number of processes that must participate in taking a consistent checkpoint or in rolling back <ref> [1, 11, 15, 17] </ref>. Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint [2, 5, 8, 16, 19, 24, 28, 29]. <p> our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint [2, 5, 8, 16, 19, 24, 28, 29], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [1, 11, 15, 17] </ref>, or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4, 13, 25]. There are very few empirical studies of consistent checkpointing and its performance. Bhargava et al. [3] reported on the performance of checkpointing.
Reference: [2] <author> B. Bhargava and S-R. Lian. </author> <title> Independent checkpoint-ing and concurrent rollback recovery for distributed systems | an optimistic approach. </title> <booktitle> In Proceedings of the 7th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 3-12, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>. In this paper, we focus instead on the overhead of consistent checkpointing on the failure-free running time of distributed application programs. We report measurements of an implementation of consistent checkpointing and analyze the various components of the overhead resulting from consistent check-pointing. <p> In contrast, in optimistic checkpointing <ref> [2] </ref>, each process takes checkpoints independently. The system attempts to construct a consistent system state from the available process checkpoints. Optimistic checkpointing avoids the overhead of checkpoint synchronization, but may lead to extensive rollbacks and the domino effect [2, 20, 22]. <p> In contrast, in optimistic checkpointing [2], each process takes checkpoints independently. The system attempts to construct a consistent system state from the available process checkpoints. Optimistic checkpointing avoids the overhead of checkpoint synchronization, but may lead to extensive rollbacks and the domino effect <ref> [2, 20, 22] </ref>. It also requires garbage collection of process checkpoints no longer needed. To measure the effect of the synchronization on check-pointing overhead, we modified our implementation to use optimistic checkpointing. <p> 3.0 5.8 tsp 0.0 0.0 Table 8 Optimistic vs. consistent checkpointing: % increase in running time pointing, consistent checkpointing appears the method of choice for our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4,
Reference: [3] <author> B. Bhargava, S-R. Lian, and P-J. Leu. </author> <title> Experimental evaluation of concurrent checkpointing and rollback-recovery algorithms. </title> <booktitle> In Proceedings of the International Conference on Data Engineering, </booktitle> <pages> pages 182-189, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: There are very few empirical studies of consistent checkpointing and its performance. Bhargava et al. <ref> [3] </ref> reported on the performance of checkpointing. They concluded that, in their environment, the messages used for synchronizing a checkpoint were an important source of overhead. Their conclusion is different from ours, because of the small size of the programs used in their study (4 to 48 kilobytes).
Reference: [4] <author> A. Borg, W. Blau, W. Graetsch, F. Herrmann, and W. Oberle. </author> <title> Fault tolerance under UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 1-24, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: [2, 5, 8, 16, 19, 24, 28, 29], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world <ref> [4, 13, 25] </ref>. There are very few empirical studies of consistent checkpointing and its performance. Bhargava et al. [3] reported on the performance of checkpointing. They concluded that, in their environment, the messages used for synchronizing a checkpoint were an important source of overhead.
Reference: [5] <author> D. Briatico, A. Ciuffoletti, and L. Simoncini. </author> <title> A distributed domino-effect free recovery algorithm. </title> <booktitle> In Proceedings of the 4th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 207-215, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>. In this paper, we focus instead on the overhead of consistent checkpointing on the failure-free running time of distributed application programs. We report measurements of an implementation of consistent checkpointing and analyze the various components of the overhead resulting from consistent check-pointing. <p> Each consistent checkpoint is identified by a monotonically increasing Consistent Checkpoint Number (CCN). Every application message is tagged with the CCN of its sender, enabling the protocol to run in the presence of message re-ordering or loss <ref> [5, 16] </ref>. We use this check-pointing protocol both for its simplicity and because we have found that it performs well in our environment. The protocol proceeds as follows: 1. <p> 3.0 5.8 tsp 0.0 0.0 Table 8 Optimistic vs. consistent checkpointing: % increase in running time pointing, consistent checkpointing appears the method of choice for our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4,
Reference: [6] <author> K.M. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: With consistent checkpointing, the state of each process is saved separately on stable storage as a process checkpoint, and the checkpointing of individual processes is synchronized such that the collection of checkpoints represents a consistent state of the whole system <ref> [6] </ref>. After a failure, failed processes are restarted on any available machine and their address space is restored from their latest checkpoint on stable storage. Surviving processes may have to rollback to their latest checkpoint on stable storage in order to remain consistent with recovering processes [15]. <p> We use this check-pointing protocol both for its simplicity and because we have found that it performs well in our environment. The protocol proceeds as follows: 1. The coordinator starts a new consistent checkpoint by incrementing CCN and sending marker messages <ref> [6] </ref> that contain CCN to each process in the system. 2. Upon receiving a marker message, a process takes a tentative checkpoint by saving the process's kernel and server state and writing the modified pages of the address space to the checkpoint file, as explained in Section 2.1.
Reference: [7] <author> D.R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Each workstation is equipped with a 20-MHz Mo-torola MC68020 processor and 4 megabytes of memory, of which 740 kilobytes are consumed by the operating system. These machines run a version of the V-System distributed operating system <ref> [7] </ref> to which we have added our checkpointing mechanisms. Our experimental environment also includes two shared Sun-3/140 network file servers, each using a 16-MHz MC68020 processor and a Fujitsu Eagle disk, on which the checkpoints are written.
Reference: [8] <author> F. Cristian and F. Jahanian. </author> <title> A timestamp-based checkpointing protocol for long-lived distributed computations. </title> <booktitle> In Proceedings of the 10th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 12-20, </pages> <address> Bologna, Italy, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>. In this paper, we focus instead on the overhead of consistent checkpointing on the failure-free running time of distributed application programs. We report measurements of an implementation of consistent checkpointing and analyze the various components of the overhead resulting from consistent check-pointing. <p> 3.0 5.8 tsp 0.0 0.0 Table 8 Optimistic vs. consistent checkpointing: % increase in running time pointing, consistent checkpointing appears the method of choice for our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4,
Reference: [9] <author> R. Fitzgerald and R.F. Rashid. </author> <title> The integration of virtual memory management and interprocess communication in accent. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(2) </volume> <pages> 147-177, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: We have considered two alternative solutions to this problem. The first solution uses copy-on-write memory protection, supported by the memory management hardware <ref> [9] </ref>. At the start of an incremental checkpoint, the pages to be written to stable storage are write-protected. After writ ing each page to stable storage, the checkpoint server removes the protection from the page.
Reference: [10] <author> J.N. Gray. </author> <title> Notes on database operating systems. </title> <editor> In R. Bayer, R.M. Graham, and G. Seegmuller, editors, </editor> <booktitle> Operating Systems: An Advanced Course, volume 60 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: After the tentative checkpoint has been completely written to stable storage, the process sends a success message to the coordinator. 4. The coordinator collects the responses from all processes, and if all tentative checkpoints have been successful, it sends a commit message <ref> [10] </ref> to each process; otherwise, it sends an abort message. When a process receives a commit message from the coordinator, it makes the tentative checkpoint permanent and discards the previous permanent checkpoint.
Reference: [11] <author> S. Israel and D. Morris. </author> <title> A non-intrusive checkpoint-ing protocol. </title> <booktitle> In The Phoenix Conference on Communications and Computers, </booktitle> <pages> pages 413-421, </pages> <year> 1989. </year>
Reference-contexts: Much of the previous work in consistent checkpoint-ing has focused on minimizing the number of processes that must participate in taking a consistent checkpoint or in rolling back <ref> [1, 11, 15, 17] </ref>. Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint [2, 5, 8, 16, 19, 24, 28, 29]. <p> our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint [2, 5, 8, 16, 19, 24, 28, 29], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [1, 11, 15, 17] </ref>, or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4, 13, 25]. There are very few empirical studies of consistent checkpointing and its performance. Bhargava et al. [3] reported on the performance of checkpointing.
Reference: [12] <author> D.B. Johnson. </author> <title> Distributed System Fault Tolerance Using Message Logging and Checkpointing. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: This scheme is similar to that used by Li et al. [18] in their concurrent checkpointing technique for small physical memories. Unlike our implementation, however, they did not implement incremental checkpoint-ing. The second solution that we considered uses pre-copying <ref> [12, 27] </ref>. If the number of pages to be written to stable storage is below some threshold, the pages are copied at once to a separate area in memory and are then written from there to stable storage without interrupting the process's execution.
Reference: [13] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Recovery in distributed systems using optimistic message logging and checkpointing. </title> <journal> Journal of Algorithms, </journal> <volume> 11(3) </volume> <pages> 462-491, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: [2, 5, 8, 16, 19, 24, 28, 29], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world <ref> [4, 13, 25] </ref>. There are very few empirical studies of consistent checkpointing and its performance. Bhargava et al. [3] reported on the performance of checkpointing. They concluded that, in their environment, the messages used for synchronizing a checkpoint were an important source of overhead.
Reference: [14] <author> M.F. Kaashoek, R. Michiels, H.E. Bal, </author> <title> and A.S. Tanenbaum. Transparent fault-tolerance in parallel orca programs. </title> <booktitle> In Symposium on Experiences with Distributed and Multiprocessor Systems III, </booktitle> <pages> pages 297-312, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For such small sizes, the overhead of writing data to stable storage is indeed negligible, making the communication overhead an important factor. For larger applications, the overhead of writing data to stable storage dominates. Kaashoek et al. <ref> [14] </ref> implemented consistent checkpoint-ing to add fault tolerance to Orca, a distributed shared object-oriented language. Their implementation takes advantage of the ordered broadcasts already present in the Orca runtime system to order marker messages with respect to application messages.
Reference: [15] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and rollback-recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: After a failure, failed processes are restarted on any available machine and their address space is restored from their latest checkpoint on stable storage. Surviving processes may have to rollback to their latest checkpoint on stable storage in order to remain consistent with recovering processes <ref> [15] </ref>. Much of the previous work in consistent checkpoint-ing has focused on minimizing the number of processes that must participate in taking a consistent checkpoint or in rolling back [1, 11, 15, 17]. <p> Much of the previous work in consistent checkpoint-ing has focused on minimizing the number of processes that must participate in taking a consistent checkpoint or in rolling back <ref> [1, 11, 15, 17] </ref>. Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint [2, 5, 8, 16, 19, 24, 28, 29]. <p> Each process maintains one perma-nent checkpoint, belonging to the most recent consistent checkpoint. During each run of the protocol, each process takes a tentative checkpoint, which replaces the permanent one only if the protocol terminates successfully <ref> [15] </ref>. Each consistent checkpoint is identified by a monotonically increasing Consistent Checkpoint Number (CCN). Every application message is tagged with the CCN of its sender, enabling the protocol to run in the presence of message re-ordering or loss [5, 16]. <p> our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint [2, 5, 8, 16, 19, 24, 28, 29], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [1, 11, 15, 17] </ref>, or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4, 13, 25]. There are very few empirical studies of consistent checkpointing and its performance. Bhargava et al. [3] reported on the performance of checkpointing.
Reference: [16] <author> T.H. Lai and T.H. Yang. </author> <title> On distributed snapshots. </title> <journal> Information Processing Letters, </journal> <volume> 25 </volume> <pages> 153-158, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>. In this paper, we focus instead on the overhead of consistent checkpointing on the failure-free running time of distributed application programs. We report measurements of an implementation of consistent checkpointing and analyze the various components of the overhead resulting from consistent check-pointing. <p> Each consistent checkpoint is identified by a monotonically increasing Consistent Checkpoint Number (CCN). Every application message is tagged with the CCN of its sender, enabling the protocol to run in the presence of message re-ordering or loss <ref> [5, 16] </ref>. We use this check-pointing protocol both for its simplicity and because we have found that it performs well in our environment. The protocol proceeds as follows: 1. <p> 3.0 5.8 tsp 0.0 0.0 Table 8 Optimistic vs. consistent checkpointing: % increase in running time pointing, consistent checkpointing appears the method of choice for our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4,
Reference: [17] <author> P. Leu and B. Bhargava. </author> <title> Concurrent robust check-pointing and recovery in distributed systems. </title> <booktitle> In Proceedings of the International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1988. </year>
Reference-contexts: Much of the previous work in consistent checkpoint-ing has focused on minimizing the number of processes that must participate in taking a consistent checkpoint or in rolling back <ref> [1, 11, 15, 17] </ref>. Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint [2, 5, 8, 16, 19, 24, 28, 29]. <p> our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint [2, 5, 8, 16, 19, 24, 28, 29], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back <ref> [1, 11, 15, 17] </ref>, or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4, 13, 25]. There are very few empirical studies of consistent checkpointing and its performance. Bhargava et al. [3] reported on the performance of checkpointing.
Reference: [18] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> Real-time, concurrent checkpoint for parallel programs. </title> <booktitle> In Proceedings of the 1990 Conference on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 79-88, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: If insufficient memory is available to allocate a new page for handling the copy-on-write fault, the process is blocked until memory can be allocated. This scheme is similar to that used by Li et al. <ref> [18] </ref> in their concurrent checkpointing technique for small physical memories. Unlike our implementation, however, they did not implement incremental checkpoint-ing. The second solution that we considered uses pre-copying [12, 27]. <p> Furthermore, the results in Section 4.4 indicate that the amount of data written to stable storage can be reduced significantly by writing only modified pages to the checkpoint. Li et al. <ref> [18] </ref> described several checkpointing methods for programs executing on shared memory multiprocessors. Their results showed that nonblocking copy-on-write checkpointing reduces the overhead for checkpointing programs running on shared memory multiprocessors. They did not implement incremental checkpointing, which we found to be an important optimization.
Reference: [19] <author> K. Li, J.F. Naughton, and J.S. Plank. </author> <title> Checkpoint-ing multicomputer applications. </title> <booktitle> In Proceedings of the 10th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 1-10, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>. In this paper, we focus instead on the overhead of consistent checkpointing on the failure-free running time of distributed application programs. We report measurements of an implementation of consistent checkpointing and analyze the various components of the overhead resulting from consistent check-pointing. <p> 3.0 5.8 tsp 0.0 0.0 Table 8 Optimistic vs. consistent checkpointing: % increase in running time pointing, consistent checkpointing appears the method of choice for our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4,
Reference: [20] <author> B. Randell. </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1(2):220-232, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: In contrast, in optimistic checkpointing [2], each process takes checkpoints independently. The system attempts to construct a consistent system state from the available process checkpoints. Optimistic checkpointing avoids the overhead of checkpoint synchronization, but may lead to extensive rollbacks and the domino effect <ref> [2, 20, 22] </ref>. It also requires garbage collection of process checkpoints no longer needed. To measure the effect of the synchronization on check-pointing overhead, we modified our implementation to use optimistic checkpointing.
Reference: [21] <author> M. Rosenblum and J.K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 1-15, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: When a process receives an abort message, it discards its tentative checkpoint. 2.3 Stable Storage Each process checkpoint is stored as a file on a shared network file server. The file server structures the disk as a sequential log in order to optimize write operations <ref> [21] </ref>. Files that store different checkpoints of the same process physically share data blocks, in order to efficiently store the incremental changes to the checkpoint file.
Reference: [22] <author> D.L. Russell. </author> <title> State restoration in systems of communicating processes. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(2):183-194, </volume> <month> March </month> <year> 1980. </year>
Reference-contexts: In contrast, in optimistic checkpointing [2], each process takes checkpoints independently. The system attempts to construct a consistent system state from the available process checkpoints. Optimistic checkpointing avoids the overhead of checkpoint synchronization, but may lead to extensive rollbacks and the domino effect <ref> [2, 20, 22] </ref>. It also requires garbage collection of process checkpoints no longer needed. To measure the effect of the synchronization on check-pointing overhead, we modified our implementation to use optimistic checkpointing.
Reference: [23] <author> R.D. Schlichting and F.B. Schneider. </author> <title> Fail-stop processors: An approach to designing fault-tolerant computing systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(3) </volume> <pages> 222-238, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: We report and analyze our performance measurements of this implementation in Section 4. In Section 5, we compare our research with related work, and in Section 6, we present our conclusions. 2 Implementation The system is assumed to consist of a collection of fail-stop <ref> [23] </ref> processes. A process consists of a single address space, residing on a single machine, and all threads executing in that address space.
Reference: [24] <author> M. Spezialetti and P. Kearns. </author> <title> Efficient distributed snapshots. </title> <booktitle> In Proceedings of the 6th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 382-388, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>. In this paper, we focus instead on the overhead of consistent checkpointing on the failure-free running time of distributed application programs. We report measurements of an implementation of consistent checkpointing and analyze the various components of the overhead resulting from consistent check-pointing. <p> 3.0 5.8 tsp 0.0 0.0 Table 8 Optimistic vs. consistent checkpointing: % increase in running time pointing, consistent checkpointing appears the method of choice for our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4,
Reference: [25] <author> R.E. Strom and S.A. Yemini. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: [2, 5, 8, 16, 19, 24, 28, 29], limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world <ref> [4, 13, 25] </ref>. There are very few empirical studies of consistent checkpointing and its performance. Bhargava et al. [3] reported on the performance of checkpointing. They concluded that, in their environment, the messages used for synchronizing a checkpoint were an important source of overhead.
Reference: [26] <author> Y. Tamir and C.H. Sequin. </author> <title> Error recovery in mul-ticomputers using global checkpoints. </title> <booktitle> In 1984 International Conference on Parallel Processing, </booktitle> <pages> pages 32-41, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Science Foundation under Grants CDA-8619893 and CCR-9116343, by the Texas Advanced Technology Program under Grant No. 003604014, and by an IBM Graduate Fellowship. fl Author's current address: School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213-3890. for transparently adding fault tolerance to distributed applications without requiring additional programmer effort <ref> [26, 30] </ref>. With consistent checkpointing, the state of each process is saved separately on stable storage as a process checkpoint, and the checkpointing of individual processes is synchronized such that the collection of checkpoints represents a consistent state of the whole system [6].
Reference: [27] <author> M. Theimer, K. Lantz, and D.R. Cheriton. </author> <title> Preempt-able remote execution facilities in the V-system. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 2-12, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: This scheme is similar to that used by Li et al. [18] in their concurrent checkpointing technique for small physical memories. Unlike our implementation, however, they did not implement incremental checkpoint-ing. The second solution that we considered uses pre-copying <ref> [12, 27] </ref>. If the number of pages to be written to stable storage is below some threshold, the pages are copied at once to a separate area in memory and are then written from there to stable storage without interrupting the process's execution.
Reference: [28] <author> Z. Tong, R.Y. Kain, and W.T. Tsai. </author> <title> A lower overhead checkpointing and rollback recovery scheme for distributed systems. </title> <booktitle> In Proceedings of the 8th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 12-20, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>. In this paper, we focus instead on the overhead of consistent checkpointing on the failure-free running time of distributed application programs. We report measurements of an implementation of consistent checkpointing and analyze the various components of the overhead resulting from consistent check-pointing. <p> 3.0 5.8 tsp 0.0 0.0 Table 8 Optimistic vs. consistent checkpointing: % increase in running time pointing, consistent checkpointing appears the method of choice for our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4,
Reference: [29] <author> K. Venkatesh, T. Radhakrishnan, and H.F. Li. </author> <title> Optimal checkpointing and local recording for domino-free rollback recovery. </title> <journal> Information Processing Letters, </journal> <volume> 25 </volume> <pages> 295-303, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Another issue that has received considerable attention is how to reduce the number of messages required to synchronize the consistent checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>. In this paper, we focus instead on the overhead of consistent checkpointing on the failure-free running time of distributed application programs. We report measurements of an implementation of consistent checkpointing and analyze the various components of the overhead resulting from consistent check-pointing. <p> 3.0 5.8 tsp 0.0 0.0 Table 8 Optimistic vs. consistent checkpointing: % increase in running time pointing, consistent checkpointing appears the method of choice for our environment. 5 Related Work Previous work in checkpointing has concentrated on issues such as reducing the number of messages required to synchronize a checkpoint <ref> [2, 5, 8, 16, 19, 24, 28, 29] </ref>, limiting the number of hosts that have to participate in taking the checkpoint or in rolling back [1, 11, 15, 17], or using message logging to eliminate the need for synchronizing the checkpoints and to accelerate input-output interactions with the outside world [4,
Reference: [30] <author> K.-L. Wu and W.K. Fuchs. </author> <title> Recoverable distributed shared memory. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4) </volume> <pages> 460-469, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Science Foundation under Grants CDA-8619893 and CCR-9116343, by the Texas Advanced Technology Program under Grant No. 003604014, and by an IBM Graduate Fellowship. fl Author's current address: School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213-3890. for transparently adding fault tolerance to distributed applications without requiring additional programmer effort <ref> [26, 30] </ref>. With consistent checkpointing, the state of each process is saved separately on stable storage as a process checkpoint, and the checkpointing of individual processes is synchronized such that the collection of checkpoints represents a consistent state of the whole system [6].
References-found: 30

