URL: http://ibm.tc.cornell.edu/ibm/pps/doc/vulcan.ps
Refering-URL: http://ibm.tc.cornell.edu/ibm/pps/doc/
Root-URL: http://www.tc.cornell.edu
Title: Architecture and Implementation of Vulcan  
Author: Craig B. Stunkel Dennis G. Shea Bulent Abali Monty M. Denneau Peter H. Hochschild Douglas J. Joseph Ben J. Nathanson Michael Tsao Philip R. Varker 
Address: P.O. Box 218, Yorktown Heights, NY 10598  
Affiliation: IBM Thomas J. Watson Research Center  
Date: April 1994  
Note: in Proceedings of the 8th International Parallel Processing Symposium, Cancun, Mexico,  
Abstract: IBM's recently announced Scalable POWERparallel 1 family of systems is based upon the Vulcan architecture, and the currently available 9076 SP1 1 parallel system utilizes fundamental Vulcan technology. The experimental Vulcan parallel processor is designed to scale to many thousands of microprocessor-based nodes. To support a machine of this size, the nodes and network incorporate a number of unusual features to scale aggregate bandwidth, enhance reliability, diagnose faults, and simplify cabling. The multistage Vulcan network is a unified data and service network driven by a single oscillator. The attempt is made to detect all network errors via CRC checking and component shadowing. Switching elements contain a dynamically allocated shared buffer for storing blocked packet flits from any input port. This paper describes key elements of Vulcan's hardware architecture and implementation details of the Vulcan prototype. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S.-W. Yang, and R. Zak, </author> <title> "The network architecture of the Connection Machine CM-5," </title> <booktitle> in Proc. 1992 Symp. Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272-285, </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction The experimental Vulcan parallel processor is designed to scale to many thousands of microprocessor-based nodes. To support large, reliable systems, Vulcan incorporates a number of unusual features. The Vulcan network is a unified data and service network driven by a single oscillator. Network devices communicate synchronously <ref> [1, 2, 3] </ref> but need not be clocked by the same phase of the global oscillator. Instead, synchronization is accomplished via phase adjustment of data signals [2]. Each switching element contains a large shared buffer that is dynamically allocated, dependent upon requests from the element's input ports. <p> Bidirec-tionality also naturally incorporates redundancy. Bidirectional links also simplify our service software by allowing the software to fully test a single link or device while relying solely on proven intermediate devices. Bidirectional MIN's are related to practical implementations of fat-tree networks <ref> [1] </ref> derived from Leiserson's idealized fat-trees [9].
Reference: [2] <author> R. D. Rettberg, W. R. Crowther, P. P. Carvey, and R. S. Tomlinson, </author> <title> "The Monarch parallel processor hardware design," </title> <journal> IEEE Computer, </journal> <volume> vol. 23, </volume> <pages> pp. 18-30, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The experimental Vulcan parallel processor is designed to scale to many thousands of microprocessor-based nodes. To support large, reliable systems, Vulcan incorporates a number of unusual features. The Vulcan network is a unified data and service network driven by a single oscillator. Network devices communicate synchronously <ref> [1, 2, 3] </ref> but need not be clocked by the same phase of the global oscillator. Instead, synchronization is accomplished via phase adjustment of data signals [2]. Each switching element contains a large shared buffer that is dynamically allocated, dependent upon requests from the element's input ports. <p> The Vulcan network is a unified data and service network driven by a single oscillator. Network devices communicate synchronously [1, 2, 3] but need not be clocked by the same phase of the global oscillator. Instead, synchronization is accomplished via phase adjustment of data signals <ref> [2] </ref>. Each switching element contains a large shared buffer that is dynamically allocated, dependent upon requests from the element's input ports. The communication protocol maintains maximum bandwidth even for long communication links. Vulcan was designed|and a working prototype built| in the Parallel Systems department at IBM Research. <p> Priority is assigned to the receivers on a least-recently-served basis. 3.5 Tuning In the Vulcan prototype, all tuning is software-controlled, and utilizes delay logic at each Vulcan output port (VOP). In contrast, the Monarch parallel processor incorporated a hardwired method for tuning a serial link <ref> [2] </ref>. Two service commands enable the service software to synchronize data arrival for a channel. One command (tuning-control) adds an identical amount of delay to each of the VOP's Data and Tag outputs, and selects a separate delay adder for the VOP's Token input.
Reference: [3] <author> M. Noakes and W. J. Dally, </author> <title> "System design of the J-machine," </title> <booktitle> in Proc. Sixth MIT Conf. Advanced Research in VLSI, </booktitle> <pages> pp. 179-194, </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction The experimental Vulcan parallel processor is designed to scale to many thousands of microprocessor-based nodes. To support large, reliable systems, Vulcan incorporates a number of unusual features. The Vulcan network is a unified data and service network driven by a single oscillator. Network devices communicate synchronously <ref> [1, 2, 3] </ref> but need not be clocked by the same phase of the global oscillator. Instead, synchronization is accomplished via phase adjustment of data signals [2]. Each switching element contains a large shared buffer that is dynamically allocated, dependent upon requests from the element's input ports.
Reference: [4] <author> W. J. Dally, </author> <title> "Virtual-channel flow control," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Each packet contains route information examined by switching elements to forward the packet correctly to its destination. The smallest unit on which flow-control is performed is called a flow control digit, or flit <ref> [4] </ref>. In the Vulcan prototype a flit is one byte. The width of the data transmitted by an output port is also one byte. A common measure of aggregate network bandwidth is bisection bandwidth. <p> Here, we adopt Dally's distinctions between wormhole routing and virtual cut-through <ref> [11, 4] </ref>. In wormhole routing, each flit of a packet is advanced to the appropriate output port as soon is it arrives at a switching element input port. When the head of a packet is blocked, the flits are buffered in place. <p> As soon as the output port is free, packet transfer resumes. The output port is chosen according to route information contained in one of the first flits (in Vulcan, the second flit) of the packet. In wormhole routing, flow-control is flit-based <ref> [4] </ref>, not packet-based as in virtual cut-through [12]. Vulcan flow-control differs from wormhole routing in that packet flits are not typically buffered in place when a packet is blocked.
Reference: [5] <author> D. H. Lawrie, </author> <title> "Access and alignment of data in an array processor," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-24, </volume> <pages> pp. 1145-1155, </pages> <month> Dec. </month> <year> 1975. </year>
Reference-contexts: Here, bisection bandwidth is defined as the maximum achievable bandwidth across a worst possible cut in the network, where this cut divides the system into halves with identical numbers of nodes. 2.2 Network topology We chose a bidirectional multistage interconnection network (MIN) topology. Common MIN's such as Omega <ref> [5] </ref> and Banyan [6] networks provide bisection bandwidth that scales linearly with the number of nodes while maintaining a fixed number of communication ports per switching element. Scalable networks are richly connected and require some long communication links to support large numbers of nodes.
Reference: [6] <author> L. R. Goke and G. J. Lipovski, </author> <title> "Banyan networks for partitioning multiprocessor systems," </title> <booktitle> in Proc. 1st Ann. Symp. on Computer Architecture, </booktitle> <pages> pp. 21-28, </pages> <year> 1973. </year>
Reference-contexts: Common MIN's such as Omega [5] and Banyan <ref> [6] </ref> networks provide bisection bandwidth that scales linearly with the number of nodes while maintaining a fixed number of communication ports per switching element. Scalable networks are richly connected and require some long communication links to support large numbers of nodes.
Reference: [7] <author> G. J. Lipovski and M. Malek, </author> <title> Parallel Computing: Theory and Comparisons. </title> <address> New York, NY: </address> <publisher> Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: Scalable networks are richly connected and require some long communication links to support large numbers of nodes. Thus, a scalable network should employ a protocol that maintains bandwidth over long links (see x2.5.1). In a bidirectional MIN <ref> [7, 8] </ref> each communication link comprises two channels, which carry data in opposite directions. The switch board in Figure 1 illustrates two stages of a bidirectional MIN constructed from 4-way to 4-way bidirectional switch elements. <p> Bidirectional MIN's are related to practical implementations of fat-tree networks [1] derived from Leiserson's idealized fat-trees [9]. The Vulcan network topology resembles bidirectional SW 269 Banyan networks <ref> [7] </ref>, which are topologically equivalent to practical fat-trees for which the number of parents at every intermediate stage is equal to the number of children. 2.3 Single global oscillator The Vulcan network operates synchronously|all network devices are driven by an image of a single oscillator.
Reference: [8] <author> I. D. Scherson and C.-H. Chien, </author> <title> "Least common ancestor networks," </title> <booktitle> in Proc. 7th Int. Parallel Processing Symp., </booktitle> <pages> pp. 507-513, </pages> <year> 1993. </year>
Reference-contexts: Scalable networks are richly connected and require some long communication links to support large numbers of nodes. Thus, a scalable network should employ a protocol that maintains bandwidth over long links (see x2.5.1). In a bidirectional MIN <ref> [7, 8] </ref> each communication link comprises two channels, which carry data in opposite directions. The switch board in Figure 1 illustrates two stages of a bidirectional MIN constructed from 4-way to 4-way bidirectional switch elements.
Reference: [9] <author> C. E. Leiserson, "Fat-trees: </author> <title> Universal networks for hardware-efficient supercomputing," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-34, </volume> <pages> pp. 892-901, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Bidirec-tionality also naturally incorporates redundancy. Bidirectional links also simplify our service software by allowing the software to fully test a single link or device while relying solely on proven intermediate devices. Bidirectional MIN's are related to practical implementations of fat-tree networks [1] derived from Leiserson's idealized fat-trees <ref> [9] </ref>.
Reference: [10] <author> L. Lamport, </author> <title> "Time, clocks, and the ordering of events in a distributed system," </title> <journal> Communications of the ACM, </journal> <volume> vol. 21, </volume> <pages> pp. 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Stuck or open faults are detected in a timely fashion. * Synchronous operation, in conjunction with appropriate hardware mechanisms, facilitates the achievement of a closely synchronized and non-drifting global time, trivializing the well-known clock synchronization problem <ref> [10] </ref>. Each device in the Vulcan network accesses this time via its local copy: a load-able incrementer called the Absolute Time Counter (ATC).
Reference: [11] <author> W. J. Dally and C. L. Seitz, </author> <title> "Deadlock-free message routing in multiprocessor interconnection networks," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-36, </volume> <pages> pp. 547-553, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Here, we adopt Dally's distinctions between wormhole routing and virtual cut-through <ref> [11, 4] </ref>. In wormhole routing, each flit of a packet is advanced to the appropriate output port as soon is it arrives at a switching element input port. When the head of a packet is blocked, the flits are buffered in place.
Reference: [12] <author> P. Kermani and L. Kleinrock, </author> <title> "Virtual cut-through: A new computer communications switching technique," </title> <journal> Computer Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 267-286, </pages> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: As soon as the output port is free, packet transfer resumes. The output port is chosen according to route information contained in one of the first flits (in Vulcan, the second flit) of the packet. In wormhole routing, flow-control is flit-based [4], not packet-based as in virtual cut-through <ref> [12] </ref>. Vulcan flow-control differs from wormhole routing in that packet flits are not typically buffered in place when a packet is blocked. In addition to small buffers at each input port, each switching element contains a relatively large shared buffer called the central queue.
Reference: [13] <author> M. M. Denneau, P. H. Hochschild, and G. Schich-man, </author> <title> "The switching network of the TF-1 parallel supercomputer," </title> <booktitle> Supercomputing Magazine, </booktitle> <pages> pp. 7-10, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: The chip is mounted in a 223 pin ceramic pin grid array package, and is designed for 50 MHz operation. The design of this chip was originally targeted for the TF-1 machine <ref> [13] </ref>. 271 3.1 Receivers The switch chip contains eight identical receiver modules, one associated with each of the eight input ports.
Reference: [14] <author> Y. Tamir and G. L. Frazier, </author> <title> "Hardware support for high-priority traffic in VLSI communication switches," </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> vol. 14, </volume> <pages> pp. 402-416, </pages> <month> April </month> <year> 1992. </year> <month> 274 </month>
Reference-contexts: It accepts packet chunks from the receivers, stores them, and eventually passes them to the appropriate transmitters. The Vulcan central buffering scheme is similar to the centrally-buffered, dynamically-allocated switch described by Tamir and Frazier <ref> [14] </ref>, but the Vulcan chip allocates central queue space on a chunk basis instead of a packet basis. The central queue stores packets until they can be transmitted. The storage, a 128 by 64-bit dual-port RAM, holds up to 128 eight-flit packet chunks.
References-found: 14

