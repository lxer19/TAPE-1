URL: ftp://robotics.stanford.edu/pub/gjohn/papers/lintree-preproc.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: gjohn@cs.Stanford.EDU  
Title: Robust Linear Discriminant Trees  
Author: George H. John 
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Abstract: We present a new method for the induction of classification trees with linear dis-criminants as the partitioning function at each internal node. This paper presents two main contributions: first, a novel objective function called soft entropy which is used to identify optimal coefficients for the linear discriminants, and second, a novel method for removing outliers called iterative re-filtering which boosts performance on many datasets. These two ideas are presented in the context of a single learning algorithm called DT-SEPIR. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1991), </year> <title> "Instance-based learning algorithms", </title> <booktitle> Machine Learning 6(1), </booktitle> <pages> 37-66. </pages>
Reference-contexts: Another way of looking at iterative re-filtering is as a pattern selection method. The selection of patterns for training is an ubiquitous problem in pattern recognition. Many methods are passive, accepting data from a training sample in some random order or all at once as a set. Other methods <ref> (Aha 1991) </ref> actively accept or reject training patterns from a temporal sequence presented to them.
Reference: <author> Bennett, K. P. & Mangasarian, O. L. </author> <year> (1992), </year> <title> Neural network training via linear programming, </title> <editor> in P. M. Pardalos, ed., </editor> <booktitle> "Advances in Optimization and Parallel Computing", </booktitle> <publisher> North Holland, Amster-dam, </publisher> <pages> pp. 56-67. </pages>
Reference: <author> Bichsel, M. & Seitz, P. </author> <year> (1989), </year> <title> "Minimum class entropy: A maximum information approach to layered networks", </title> <booktitle> Neural Networks 2, </booktitle> <pages> 133-141. </pages>
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Chap-man & Hall, </publisher> <address> New York. </address>
Reference-contexts: The essential ingredient is the objective function: the splitting criterion I which we wish to minimize. All of the splitting criteria we are aware of|twoing <ref> (Breiman et al. 1984) </ref>, entropy, Gini, delta (Morgan & Messenger 1973), gain-ratio (Quinlan 1993), C-sep (Fayyad & Irani 1992)|are defined on sets, but have straightforward extensions to weighted sets. <p> In general, unless time is a very limited resource, the best results are achieved by starting "big" and then shrinking <ref> (Breiman et al. 1984) </ref>. This should apply to pattern selection as well, and thus we suspect backward pattern selection will give greater performance than forward selection. 5 Experiments with DT-SEPIR We ran cross-validation experiments comparing DT-SE, DT-SEP and DT-SEPIR with CART and a CART-like algorithm, OC1. <p> All missing values were either removed or replaced with their mean or mode. Three different algorithms were used: the DT-SE algorithms discussed in this paper, CART <ref> (Breiman et al. 1984) </ref>, and OC1 (Murthy, Kasif & Salzberg 1994, Murthy, Salzberg & Kasif 1993).
Reference: <author> Brent, R. P. </author> <year> (1991), </year> <title> "Fast training algorithms for neural networks", </title> <journal> IEEE Transactions on Neural Networks 2(3), </journal> <pages> 346-354. </pages>
Reference: <author> Brodley, C. E. & Utgoff, P. E. </author> <title> (n.d.), "Multivariate decision trees", Machine Learning. </title> <publisher> Forthcoming. </publisher>
Reference: <author> Dempster, A. P., Laird, N. M. & Rubin, D. B. </author> <year> (1977), </year> <title> "Maximum likelihood from incomplete data via the EM algorithm", </title> <journal> Journal of the Royal Statistical Society B 39, </journal> <pages> 1-38. </pages>
Reference-contexts: Along the same lines, Jor-dan & Jacobs (1993) present a method that allows simultaneous optimization of all coefficients in the tree by defining the model likelihood and using the EM algorithm <ref> (Dempster, Laird & Rubin 1977) </ref> for optimization. Soft splits are used not only for tree construction but also during classification. However, their method requires the user to specify the tree structure a priori.
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley. </publisher>
Reference-contexts: Friedman (1977) gives an excellent discussion of practically every issue related to the induction of decision trees from data, including linear discriminant splits using Fisher's linear discriminant <ref> (Duda & Hart 1973) </ref>. The earliest attempt at building linear discriminant trees seems to be Henrichon, Jr. & Fu (1969), who use the maximum eigenvector of the covariance matrix to determine the split. Lin & Fu (1983) use k-means clustering to find the splitting function.
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1992), </year> <title> The attribute selection problem in decision tree generation, </title> <booktitle> in "AAAI-92: Proceedings of the Tenth National Conference on Artificial Intelligence", American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <pages> pp. 104-110. </pages>
Reference-contexts: The essential ingredient is the objective function: the splitting criterion I which we wish to minimize. All of the splitting criteria we are aware of|twoing (Breiman et al. 1984), entropy, Gini, delta (Morgan & Messenger 1973), gain-ratio (Quinlan 1993), C-sep <ref> (Fayyad & Irani 1992) </ref>|are defined on sets, but have straightforward extensions to weighted sets.
Reference: <author> Friedman, J. H. </author> <year> (1977), </year> <title> "A recursive partitioning decision rule for nonparametric classification", </title> <journal> IEEE Transactions on Computers pp. </journal> <pages> 404-408. </pages>
Reference: <author> Guyon, I., Boser, B. & Vapnik, V. </author> <year> (1993), </year> <title> Automatic capacity tuning of very large VC-dimension classifiers, </title> <editor> in S. J. Hanson, J. Cowan & C. L. Giles, eds, </editor> <booktitle> "Advances in Neural Information Processing Systems", </booktitle> <volume> Vol. 5, </volume> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 147-154. </pages>
Reference: <author> Hastie, T. J. & Tibshirani, R. J. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: This step of filtering the pruned instances out of the training sample and rebuilding yields the DT-SEPIR (Iterative Re-filtering) algorithm. Though common in regression in the guise of robust (Hubel 1977) or resistant <ref> (Hastie & Tibshirani 1990, Chapter 9) </ref> fitting, in the context of classification this appears to be novel. In Section 2 we formalize the problem of finding a splitting function for an arbitrary splitting criterion as a function optimization problem.
Reference: <author> Heath, D., Kasif, S. & Salzberg, S. </author> <year> (1993), </year> <title> Induction of oblique decision trees, </title> <editor> in R. Bajcsy, ed., </editor> <booktitle> "Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence", </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Henrichon, Jr., E. G. & Fu, K.-S. </author> <year> (1969), </year> <title> "A non-parametric partitioning procedure for pattern classification", </title> <journal> IEEE Transactions on Computers C-18(7), </journal> <pages> 614-624. </pages>
Reference-contexts: We follow the traditional recursive partitioning approach to tree building, but rather than partitioning the data on a single axis we instead employ a linear discriminant at each node to recursively split the data <ref> (Henrichon, Jr. & Fu 1969, Friedman 1977, Breiman et al. 1984) </ref>. The issue of how to find a good discriminant naturally arises. We discuss problems in previous approaches and propose a new splitting criterion, soft entropy.
Reference: <author> Hubel, P. J. </author> <year> (1977), </year> <title> Robust Statistical Procedures, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: This step of filtering the pruned instances out of the training sample and rebuilding yields the DT-SEPIR (Iterative Re-filtering) algorithm. Though common in regression in the guise of robust <ref> (Hubel 1977) </ref> or resistant (Hastie & Tibshirani 1990, Chapter 9) fitting, in the context of classification this appears to be novel. In Section 2 we formalize the problem of finding a splitting function for an arbitrary splitting criterion as a function optimization problem.
Reference: <author> John, G. H. </author> <year> (1994), </year> <title> Finding multivariate splits in decision trees using function optimization, </title> <booktitle> in "AAAI-94: Proceedings of the Twelfth National Conference on Artificial Intelligence", American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> p. </address> <note> 1463. Abstract. </note>
Reference-contexts: Other methods (Aha 1991) actively accept or reject training patterns from a temporal sequence presented to them. The difference between our method and theirs is somewhat akin to the difference between forward and backward feature subset selection <ref> (John, Kohavi & Pfleger 1994) </ref> or forward and backward (construction/pruning) search methods over neural net or decision tree architectures. In general, unless time is a very limited resource, the best results are achieved by starting "big" and then shrinking (Breiman et al. 1984).
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <editor> in H. Hirsh & W. Cohen, eds, </editor> <booktitle> "Machine Learning: Proceedings of the Eleventh International Conference", </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages> <note> Available by anonymous ftp in starry.Stanford.EDU:pub/gjohn/papers/ml94.ps. </note>
Reference-contexts: Other methods (Aha 1991) actively accept or reject training patterns from a temporal sequence presented to them. The difference between our method and theirs is somewhat akin to the difference between forward and backward feature subset selection <ref> (John, Kohavi & Pfleger 1994) </ref> or forward and backward (construction/pruning) search methods over neural net or decision tree architectures. In general, unless time is a very limited resource, the best results are achieved by starting "big" and then shrinking (Breiman et al. 1984).
Reference: <author> Jordan, M. I. & Jacobs, R. A. </author> <year> (1993), </year> <title> Supervised learning and divide-and-conquer: A statistical approach, </title> <editor> in P. Utgoff, ed., </editor> <booktitle> "Proceedings of the Tenth International Conference on Machine Learning", </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Koutsougeras, C. & Papachristou, C. A. </author> <year> (1988), </year> <title> Training of a neural network for pattern classification based on an entropy measure, </title> <booktitle> in "IEEE International Conference on Neural Networks", </booktitle> <publisher> IEEE Press, </publisher> <pages> pp. 247-254. </pages>
Reference: <author> Lin, Y. K. & Fu, K. S. </author> <year> (1983), </year> <title> "Automatic classification of cervical cells using a binary tree classifier", </title> <booktitle> Pattern Recognition 16(1), </booktitle> <pages> 69-80. </pages>
Reference: <author> Loh, W.-Y. & Vanichsetakul, N. </author> <year> (1988), </year> <title> "Tree-structured classification via generalized discriminant analysis", </title> <journal> Journal of the American Statistical Association 83(403), </journal> <pages> 715-725. </pages>
Reference: <author> Michie, D., Spiegelhalter, D. J. & Taylor, C. C. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Experimental methodology is first discussed, then results are presented and analyzed. 5.1 Methodology Five datasets were gathered from the UCI (Mur-phy & Aha 1994) and Statlog <ref> (Michie, Spiegelhalter & Taylor 1994) </ref> collections. In the Vote1 dataset, the task is to predict the political party of congress members given their votes on key issues. The most predictive attribute has been removed.
Reference: <author> Morgan, J. N. & Messenger, R. C. </author> <year> (1973), </year> <title> THAID: a sequential analysis program for the analysis of nominal scale dependent variables, </title> <institution> University of Michi-gan. </institution>
Reference-contexts: The essential ingredient is the objective function: the splitting criterion I which we wish to minimize. All of the splitting criteria we are aware of|twoing (Breiman et al. 1984), entropy, Gini, delta <ref> (Morgan & Messenger 1973) </ref>, gain-ratio (Quinlan 1993), C-sep (Fayyad & Irani 1992)|are defined on sets, but have straightforward extensions to weighted sets.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> "UCI repository of machine learning databases", Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference: <author> Murthy, S. K., Salzberg, S. & Kasif, S. </author> <year> (1993), </year> <note> "OC1", Available by anonymous ftp in blaze.cs.jhu.edu:pub/oc1. </note>
Reference: <author> Murthy, S., Kasif, S. & Salzberg, S. </author> <year> (1994), </year> <title> "A system for induction of oblique decision trees", </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> 1-32. </pages>
Reference-contexts: All missing values were either removed or replaced with their mean or mode. Three different algorithms were used: the DT-SE algorithms discussed in this paper, CART (Breiman et al. 1984), and OC1 <ref> (Murthy, Kasif & Salzberg 1994, Murthy, Salzberg & Kasif 1993) </ref>. CART was set to use linear splits whenever the number of instances at the node was greater than ten times the number of attributes. (When using linear splits, CART seems to be quite numerically fragile.
Reference: <author> Qing-Yun, S. & Fu, K. S. </author> <year> (1983), </year> <title> "A method for the design of binary-tree classifiers", </title> <booktitle> Pattern Recognition 16(6), </booktitle> <pages> 593-603. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> "Induction of decision trees", </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The essential ingredient is the objective function: the splitting criterion I which we wish to minimize. All of the splitting criteria we are aware of|twoing (Breiman et al. 1984), entropy, Gini, delta (Morgan & Messenger 1973), gain-ratio <ref> (Quinlan 1993) </ref>, C-sep (Fayyad & Irani 1992)|are defined on sets, but have straightforward extensions to weighted sets.
Reference: <author> Sahami, M. </author> <year> (1993), </year> <title> Learning non-linearly separable boolean functions with linear threshold unit trees and madaline-style networks, </title> <booktitle> in "AAAI-93: Proceedings of the Eleventh National Conference on Artificial Intelligence", American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <pages> pp. 335-341. </pages>
Reference: <author> Sankar, A. & Mammone, R. J. </author> <year> (1991), </year> <title> Optimal pruning of neural tree networks for improved generalization, </title> <booktitle> in "IJCNN-91-SEATTLE: International Joint Conference on Neural Networks", </booktitle> <publisher> IEEE Press, </publisher> <address> Seattle, WA, </address> <pages> pp. II: 219-224. </pages>
Reference: <author> Sethi, I. K. </author> <year> (1990), </year> <title> "Entropy nets: from decision trees to neural networks", </title> <booktitle> Proceedings of the IEEE 78(10), </booktitle> <pages> 1605-1613. </pages>
References-found: 32

