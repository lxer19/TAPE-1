URL: file://ftp.cis.ohio-state.edu/pub/communication/papers/pcrcw97_dsm_net_design.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~panda/wormhole_pub.html
Root-URL: 
Email: fdai,pandag@cis.ohio-state.edu  
Author: Dai and Dhabaleswar K. Panda 
Keyword: current and future generation DSM systems.  
Address: Columbus, OH 43210-1277  
Affiliation: Dept. of Computer and Information Science The Ohio State University,  
Date: June 1997.  
Note: To be presented in Workshop on Parallel Computer Routing and Communication (PCRCW'97), Atlanta, GA,  Donglai  
Abstract: How Can We Design Better Networks Abstract. Most DSM research in current years have ignored the impact of interconnection network altogether. Similarly, most of the interconnection network research have focused on better network designs by using synthetic (uniform/non-uniform) traffic. Both these trends do not lead to any concrete guidelines about designing better networks for the emerging Distributed Shared Memory (DSM) paradigm. In this paper, we address these issues by taking a three-step approach. First, we propose a comprehensive parameterized model to estimate the performance of an application on a DSM system. This model takes into account all key aspects of a DSM system: application, processor, cache/memory hierarchy, coherence protocol, and network. Next, using this model we evaluate the impact of different network design choices (link speed, link width, topology, ratio between router to physical link delay) on the overall performance of DSM applications and establish guidelines for designing better networks for DSM systems. Finally, we use simulations of SPLASH2 benchmark suites to validate our design guidelines. Some of the important design guidelines established in this paper are: 1) better performance is achieved by increasing link speed instead of link width, 2) increasing dimension of a network under constant bisection bandwidth constraint is not at all beneficial, and 3) network contention experienced by short messages is very crucial to the overall performance. These guidelines together with several others lay a good foundation for designing better networks for for DSM Systems? ?
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. Chandra, J. R. Larus, and A. Rogers. </author> <booktitle> Where is time spent in message-passing and shared-memory programs? In Proceedings of the sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 61-73, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Once a set of values are given to the above parameters/terms, this equation can predict the execution time of any given application. It can also identify the bottlenecks associated with different components of the system. From the recent literature <ref> [1, 10] </ref>, the current generation DSM systems and applications exhibit the following range of values for the parameters used in our model: T grn from 2 to 500 processor cycles, R hit from 0.90 to 0.99, R miss from 0.01 to 0.10, T hit from 0 to 2 processor cycles, T
Reference: 2. <author> D. Dai and D. K. Panda. </author> <title> Reducing Cache Invalidation Overheads in Wormhole DSMs Using Multidestination Message Passing. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, pages I:138-145, </booktitle> <address> Chicago, IL, </address> <month> Aug </month> <year> 1996. </year>
Reference-contexts: On a memory block access, the first word of the block was returned in 30 pcycles (150 ns). The successive words in the block followed in a pipelined fashion. The machine was assumed to be using a full-mapped, invalidation-based, three-state directory coherence protocol <ref> [2, 4, 7, 9] </ref>. The node controller took 3 pcycles to forward a message and 14 pcycles to manipulate the directory. The network interface took 15 pcycles to construct a message and 8 pcycles to dispatch it.
Reference: 3. <author> D. Dai and D. K. Panda. </author> <title> How Can We Design Better Networks for DSM Systems? Technical Report OSU-CISRC-3/97-TR19, </title> <institution> The Ohio State University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: The node simulator modeled the internal structures, as well as the queuing and contention at the node controller, main memory, and cache. For more information about the simulation setup, please refer to <ref> [3] </ref>. The major difference between our simulated architecture and the FLASH was as follow: sequential memory consistency, instead of the release memory consistency, was enforced by the coherence protocol. A load/store miss stalled the processor until the first word of data was returned. <p> The data presented here were for routing delay of 1 ncycle. Other results of different topologies can be found in <ref> [3] </ref>. Under Constant Link Width Constraint: Figures 2 (e) and 2 (k) show the execution time and average message latencies of each application running under various topologies. The link width was kept at 16 bits for all experiments.
Reference: 4. <author> D. Dai and D. K. Panda. </author> <title> How Much Does Network Contention Affect Distributed Shared Memory Performance? In Proceedings of the International Conference on Parallel Processing, </title> <address> Chicago, IL, </address> <month> Aug </month> <year> 1997. </year>
Reference-contexts: For more detailed research result on the performance impact of network contention under various processor speed, cache/memory hierarchy, and network design choices in current generation DSMs, readers are requested to refer to <ref> [4] </ref>. Now let us consider the average message latencies for short and long messages, as indicated in Eqns. 9 and 10, respectively. <p> On a memory block access, the first word of the block was returned in 30 pcycles (150 ns). The successive words in the block followed in a pipelined fashion. The machine was assumed to be using a full-mapped, invalidation-based, three-state directory coherence protocol <ref> [2, 4, 7, 9] </ref>. The node controller took 3 pcycles to forward a message and 14 pcycles to manipulate the directory. The network interface took 15 pcycles to construct a message and 8 pcycles to dispatch it.
Reference: 5. <author> W. J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Network. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Now let us consider the average message latencies for short and long messages, as indicated in Eqns. 9 and 10, respectively. Based on the wormhole-routing principles <ref> [5] </ref>, the no-contention network latencies (i.e., T snet and T lnet ) can be further estimated as follows: T snet = D fl (T rout + T phy ) + (dL short =W e 1) fl (T sw + T phy ) (19) T lnet = D fl (T rout + <p> For a fair comparison, we consider two different strategies for varying topologies: under constant link width constraint and under constant bisection bandwidth constraint <ref> [5] </ref>. Under the former constraint, link width is kept constant as the dimension of the network changes. Under the second constraint, link width is changed so as to maintain constant bisection bandwidth.
Reference: 6. <author> J. Duato, S. Yalamanchili, and L. Ni. </author> <title> Interconnection Networks: An Engineering Approach. </title> <publisher> The IEEE Computer Society Press, </publisher> <year> 1997. </year>
Reference-contexts: Thus, these models fail to provide any concrete guidelines for designing efficient networks for DSM systems. ? This research is supported in part by NSF Grant MIP-9309627 and NSF Career Award MIP-9502294. At the same time, a lot of research in interconnection network <ref> [6] </ref> has studied the impact of link speed, link width, routing delay, routing adaptivity, or topology on the overall network performance based on synthetic traffic (uniform/nonuniform/hot-spot), arbitrary message length, and message generation intervals following some statistical distributions. <p> It is well known that the average message latency in such a network contains two components: no-contention latency and contention delays. Since it is difficult to model network contention analytically <ref> [6] </ref>, we consider no-contention latencies only in this section. In the simulation section, we measure message latencies with and without contention and compare them with one another.
Reference: 7. <author> D. Lenoski et al. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: On a memory block access, the first word of the block was returned in 30 pcycles (150 ns). The successive words in the block followed in a pipelined fashion. The machine was assumed to be using a full-mapped, invalidation-based, three-state directory coherence protocol <ref> [2, 4, 7, 9] </ref>. The node controller took 3 pcycles to forward a message and 14 pcycles to manipulate the directory. The network interface took 15 pcycles to construct a message and 8 pcycles to dispatch it.
Reference: 8. <author> D. Lenoski et al. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: A synchronization operation is usually a special variant of shared memory reference, such as Fetch&Inc instruction, memory fence instruction, or QOLB scheme <ref> [8] </ref> supported in typical DSM systems. Thus, we can rewrite T sync as follows: T sync = N bal fl T grn + T shmem (3) where N bal is a load balance parameter. It depends on both the application and load balancing algorithm. <p> The network interface took 15 pcycles to construct a message and 8 pcycles to dispatch it. The synchronization protocol assumed in the system was the QOLB protocol similar to the one used in DASH <ref> [8] </ref>. The node simulator modeled the internal structures, as well as the queuing and contention at the node controller, main memory, and cache. For more information about the simulation setup, please refer to [3].
Reference: 9. <author> J. Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <year> 1994. </year>
Reference-contexts: In this paper, we take on such a challenge and solve it in three steps. First, we propose a parameterized performance model for a CC-NUMA architecture, similar to the FLASH <ref> [9] </ref> using any general network. <p> the following section, we present detailed execution-driven simulation results to validate these guidelines and study the possible changes caused by network contention. 4 Simulation Results In order to validate the analytical model and the guidelines derived in the last section, we simulated a DSM system similar to the FLASH machine <ref> [9] </ref>. Be- fore presenting and discussing the simulation results, we first describe the basic architectural features assumed, the applications and their input sizes, and the performance metrics used in our experiments. 4.1 Simulation Environment We simulated a 64-node DSM system with a default interconnection as an 8 fi 8 mesh. <p> On a memory block access, the first word of the block was returned in 30 pcycles (150 ns). The successive words in the block followed in a pipelined fashion. The machine was assumed to be using a full-mapped, invalidation-based, three-state directory coherence protocol <ref> [2, 4, 7, 9] </ref>. The node controller took 3 pcycles to forward a message and 14 pcycles to manipulate the directory. The network interface took 15 pcycles to construct a message and 8 pcycles to dispatch it.
Reference: 10. <author> S. C. Woo et al. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <year> 1995. </year>
Reference-contexts: Finally, we use an application-driven simulation approach to validate our model and obtain additional insights such as the impact of network contention. The evaluations are based on running four SPLASH2 <ref> [10] </ref> benchmark applications on a simulated 64 processor system. <p> Once a set of values are given to the above parameters/terms, this equation can predict the execution time of any given application. It can also identify the bottlenecks associated with different components of the system. From the recent literature <ref> [1, 10] </ref>, the current generation DSM systems and applications exhibit the following range of values for the parameters used in our model: T grn from 2 to 500 processor cycles, R hit from 0.90 to 0.99, R miss from 0.01 to 0.10, T hit from 0 to 2 processor cycles, T <p> The sizes of memory, message header and cache are small by the current standard in real DSM systems. However, these carefully selected values are large enough to run the SPLASH2 benchmark applications using the recommended input sizes from <ref> [10] </ref> and to keep the simulation time reasonable. 3.1 Impact of Link Speed Let us study the impact of link/router speed on the overall performance of DSM systems. Consider an 8 fi 8 mesh with 16-bit wide links as our network. <p> A load/store miss stalled the processor until the first word of data was returned. Therefore, at most one outstanding request could be pending in a processor. We used four SPLASH2 <ref> [10] </ref> applications|Barnes, LU, Radix, and Water in our simulation experiments. Problem sizes for these applications were as follows: Barnes (8K particles, = 1:0, 4 time steps), LU (512fi512 doubles, 8fi8 blocks), Radix (1M keys, 1K radix, max 1M ), and Water (512 molecules, 4 time steps). <p> Problem sizes for these applications were as follows: Barnes (8K particles, = 1:0, 4 time steps), LU (512fi512 doubles, 8fi8 blocks), Radix (1M keys, 1K radix, max 1M ), and Water (512 molecules, 4 time steps). These sizes were recommended in <ref> [10] </ref> to keep the important behaviors of the applications to be the same as in a full fledged system. 4.2 Performance Metrics We present all our simulation results in two sets: execution time and network latency.
Reference: 11. <author> K. Gharachorloo et al. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Let us assume that the processor in our modeled system supports multithreading and multiple outstanding requests. Such a processor model is powerful enough to capture almost all the characteristics introduced by existing latency reduction or latency tolerance techniques, such as lockup-free caches [13], relaxed memory consistency <ref> [11] </ref>, hardware and software data prefetching, speculative load and execution [14], and multithreading. As an example, let us consider the behavior of a typical application on a processor for a short duration, as depicted in Fig. 1 (b). Assume two threads, thr1 and thr2 are mapped onto this processor.
Reference: 12. <author> J. L. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The cache was assumed to operate in dual-port mode using write-back and write-allocate policies. The instruction latencies, issue rules, and memory interface were modeled based on the DLX design <ref> [12] </ref>. The memory bus was assumed to be 8 bytes wide. On a memory block access, the first word of the block was returned in 30 pcycles (150 ns). The successive words in the block followed in a pipelined fashion.
Reference: 13. <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-85, </pages> <year> 1981. </year>
Reference-contexts: Let us assume that the processor in our modeled system supports multithreading and multiple outstanding requests. Such a processor model is powerful enough to capture almost all the characteristics introduced by existing latency reduction or latency tolerance techniques, such as lockup-free caches <ref> [13] </ref>, relaxed memory consistency [11], hardware and software data prefetching, speculative load and execution [14], and multithreading. As an example, let us consider the behavior of a typical application on a processor for a short duration, as depicted in Fig. 1 (b).
Reference: 14. <author> A. Rogers and Kai Li. </author> <title> Software support for speculative loads. </title> <booktitle> In The Fifth International Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 38-50, </pages> <month> October </month> <year> 1992. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Such a processor model is powerful enough to capture almost all the characteristics introduced by existing latency reduction or latency tolerance techniques, such as lockup-free caches [13], relaxed memory consistency [11], hardware and software data prefetching, speculative load and execution <ref> [14] </ref>, and multithreading. As an example, let us consider the behavior of a typical application on a processor for a short duration, as depicted in Fig. 1 (b). Assume two threads, thr1 and thr2 are mapped onto this processor.
References-found: 14

