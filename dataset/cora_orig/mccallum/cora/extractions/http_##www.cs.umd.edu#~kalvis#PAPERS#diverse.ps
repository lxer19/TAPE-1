URL: http://www.cs.umd.edu/~kalvis/PAPERS/diverse.ps
Refering-URL: http://www.cs.umd.edu/~kalvis/PAPERS/
Root-URL: 
Email: kalvis@cs.umd.edu  rusins@cclu.lv  smith@cs.umd.edu  
Title: On Duality in Learning and the Selection of Learning Teams  
Author: Kalvis Apstis Rusi~ns Freivalds Carl H. Smith 
Address: College Park, MD 20742, USA  Rai~na bulvaris 29 Rga, LV-1459, Latvia  College Park, MD 20742, USA  
Affiliation: Department of Computer Science University of Maryland  Institute of Mathematics and Computer Science University of Latvia  Department of Computer Science University of Maryland  
Abstract: fl Results collected in this paper were presented previously at conferences [AFS94] and [FS93]. 
Abstract-found: 1
Intro-found: 1
Reference: [AFK + 92] <author> K. Apstis, R. Freivalds, M. Krik~is, R. Simanovskis, and J. Smotrovs. </author> <title> Unions of identifiable classes of total recursive functions. </title> <editor> In K. Jantke, editor, </editor> <booktitle> Analogical and Inductive Inference, </booktitle> <pages> pages 99-107. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> No. </volume> <pages> 642. </pages>
Reference: [AFS94] <author> K. Apstis, R. Freivalds, and C. Smith. </author> <title> Choosing a learning team: A topological approach. </title> <booktitle> In Proceedings of the 26 th Symposium on the Theory of Computing, </booktitle> <pages> pages 283-289. </pages> <publisher> ACM, </publisher> <year> 1994. </year>
Reference: [AGS89] <author> D. Angluin, W. I. Gasarch, and C. H. Smith. </author> <title> Training sequences. </title> <journal> Theoretical Computer Science, </journal> <volume> 66 </volume> <pages> 255-272, </pages> <year> 1989. </year>
Reference-contexts: In this paper we investigate the dual problem: Given some set of concepts, which algorithms can learn all those concepts? From <ref> [AGS89] </ref> we know that in the theory of inductive inference sometimes one concept must be mastered before the learning of another concept can be initiated. This observation is consistent with the common human learning behavior.
Reference: [AS83] <author> D. Angluin and C. H. Smith. </author> <title> Inductive inference: Theory and methods. </title> <journal> Computing Surveys, </journal> <volume> 15 </volume> <pages> 237-269, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction All current theoretical approaches to machine learning tend to focus on a particular machine or a collection of machines and then find the class of concepts which can be learned by these machines under certain constraints defining a criterion of successful learning <ref> [AS83, OSW86] </ref>. In this paper we investigate the dual problem: Given some set of concepts, which algorithms can learn all those concepts? From [AGS89] we know that in the theory of inductive inference sometimes one concept must be mastered before the learning of another concept can be initiated.
Reference: [Bar74] <author> J. Barzdins. </author> <title> Two theorems on the limiting synthesis of functions. </title> <editor> In Barzdins, editor, </editor> <booktitle> Theory of Algorithms and Programs, </booktitle> <volume> volume 1, </volume> <pages> pages 82-88. </pages> <institution> Latvian State University, Riga, U.S.S.R., </institution> <year> 1974. </year>
Reference: [BB75] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference: [CS83] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25(2) </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference: [DKV92] <author> R. Daley, B. Kalyanasundaram, and M. Velauthapillai. </author> <title> Breaking the probability 1/2 barrier in fin-type learning. </title> <editor> In L. Valiant and M. War-muth, editors, </editor> <booktitle> Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 203-217. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: A precise correspondence between team learning and probabilistic learning [Pit89] intensified the study of team learning. So far, all of the studies of team learning focus on how team size compares with other parameters relevant to the learning process <ref> [Sch86, PS88, FSV89, KZ91, DPVW91, DKV92, DKV93] </ref>. The goal has always been to see which parameter settings allowed for more powerful teams of learning machines to be constructed. In contrast, we address the issue of how to compose teams.
Reference: [DKV93] <author> R. Daley, B. Kalyanasundaram, and M. Velauthapillai. </author> <title> Capabilities of fallible finite learning. </title> <booktitle> In The 1993 Workshop on Computational Learning Theory. The Association for Computing, </booktitle> <year> 1993. </year>
Reference-contexts: A precise correspondence between team learning and probabilistic learning [Pit89] intensified the study of team learning. So far, all of the studies of team learning focus on how team size compares with other parameters relevant to the learning process <ref> [Sch86, PS88, FSV89, KZ91, DPVW91, DKV92, DKV93] </ref>. The goal has always been to see which parameter settings allowed for more powerful teams of learning machines to be constructed. In contrast, we address the issue of how to compose teams.
Reference: [DPVW91] <author> R. Daley, L. Pitt, M. Velauthapillai, and T. </author> <title> Will. Relations between probabilistic and team one-shot learners. </title> <editor> In M. Warmuth and L. Valiant, editors, </editor> <booktitle> Proceedings of the 1991 Workshop on Computational Learning 20 Theory, </booktitle> <pages> pages 228-239, </pages> <address> Palo Alto, CA., 1991. </address> <publisher> Morgan Kaufmann Pub--lishers. </publisher>
Reference-contexts: A precise correspondence between team learning and probabilistic learning [Pit89] intensified the study of team learning. So far, all of the studies of team learning focus on how team size compares with other parameters relevant to the learning process <ref> [Sch86, PS88, FSV89, KZ91, DPVW91, DKV92, DKV93] </ref>. The goal has always been to see which parameter settings allowed for more powerful teams of learning machines to be constructed. In contrast, we address the issue of how to compose teams.
Reference: [Eng89] <author> R. Engelking. </author> <title> General Topology. </title> <publisher> Heldermann Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Let U; V be arbitrary classes of recursive functions. We summarize some of the previous results: * U U , * U = U . But U [ V 6= U [ V , therefore U ! U is not a closure operator <ref> [Eng89] </ref>. To see that U [ V 6= U [ V , consider U; V 2 EX such that U [ V 62 EX, as in Theorem 10. Then U [ V = F , but U [ V = U [ V 6= F .
Reference: [FS93] <author> R. Freivalds and C. Smith. </author> <title> On the duality between mechanistic learners and what it is they learn. </title> <booktitle> In Lecture Notes in Artificial Intelligence Vol. </booktitle> <volume> 744, </volume> <pages> pages 137-149. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference: [FSV89] <author> R. Freivalds, C. Smith, and M. Velauthapillai. </author> <title> Trade-offs amongst parameters effecting the inductive inferribility of classes of recursive functions. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 323-349, </pages> <year> 1989. </year>
Reference-contexts: A precise correspondence between team learning and probabilistic learning [Pit89] intensified the study of team learning. So far, all of the studies of team learning focus on how team size compares with other parameters relevant to the learning process <ref> [Sch86, PS88, FSV89, KZ91, DPVW91, DKV92, DKV93] </ref>. The goal has always been to see which parameter settings allowed for more powerful teams of learning machines to be constructed. In contrast, we address the issue of how to compose teams.
Reference: [Gol67] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference: [KF92] <author> M. Krik~is and R. Freivalds. </author> <title> Inductive inference of total recursive functions by probabilistic and deterministric strategies. </title> <type> In Technical Report, </type> <institution> YALEU/DCS/TR-936, </institution> <month> November </month> <year> 1992. </year>
Reference: [Kur58] <author> C. </author> <title> Kuratowski. </title> <booktitle> Topologie, 4 th edition, volume 20-21 of Monografie Matematyczne. </booktitle> <address> Panstwowe Wydawnictwo Naukowe, </address> <year> 1958. </year>
Reference: [KZ91] <author> E. Kinber and T. Zeugmann. </author> <title> One-sided error probabilistic inductive inference and reliable frequency identification. </title> <journal> Information and Computation, </journal> <volume> 92 </volume> <pages> 253-284, </pages> <year> 1991. </year>
Reference-contexts: A precise correspondence between team learning and probabilistic learning [Pit89] intensified the study of team learning. So far, all of the studies of team learning focus on how team size compares with other parameters relevant to the learning process <ref> [Sch86, PS88, FSV89, KZ91, DPVW91, DKV92, DKV93] </ref>. The goal has always been to see which parameter settings allowed for more powerful teams of learning machines to be constructed. In contrast, we address the issue of how to compose teams.
Reference: [MY78] <author> M. Machtey and P. Young. </author> <title> An Introduction to the General Theory of Algorithms. </title> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Classes of recursive functions, i.e. subsets of F , are denoted by U , V , W , with or without decorations. By using standard encoding techniques, the natural numbers can serve as indices for programs <ref> [MY78, Smi94a] </ref>. The function computed by program i is denoted by ' i . The collection of functions ' 0 , ' 1 , ' 2 , , forms an acceptable programming system.
Reference: [OSW86] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: 1 Introduction All current theoretical approaches to machine learning tend to focus on a particular machine or a collection of machines and then find the class of concepts which can be learned by these machines under certain constraints defining a criterion of successful learning <ref> [AS83, OSW86] </ref>. In this paper we investigate the dual problem: Given some set of concepts, which algorithms can learn all those concepts? From [AGS89] we know that in the theory of inductive inference sometimes one concept must be mastered before the learning of another concept can be initiated.
Reference: [Pit89] <author> L. Pitt. </author> <title> Probabilistic inductive inference. </title> <journal> Journal of the ACM, </journal> <volume> 36(2) </volume> <pages> 383-433, </pages> <year> 1989. </year>
Reference-contexts: This suggests that perhaps different types of knowledge are needed to solve some problems. Indeed, most of the papers in our field have at least two coauthors. A precise correspondence between team learning and probabilistic learning <ref> [Pit89] </ref> intensified the study of team learning. So far, all of the studies of team learning focus on how team size compares with other parameters relevant to the learning process [Sch86, PS88, FSV89, KZ91, DPVW91, DKV92, DKV93].
Reference: [PS88] <author> L. Pitt and C. Smith. </author> <title> Probability and plurality for aggregations of learning machines. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 77-92, </pages> <year> 1988. </year>
Reference-contexts: A precise correspondence between team learning and probabilistic learning [Pit89] intensified the study of team learning. So far, all of the studies of team learning focus on how team size compares with other parameters relevant to the learning process <ref> [Sch86, PS88, FSV89, KZ91, DPVW91, DKV92, DKV93] </ref>. The goal has always been to see which parameter settings allowed for more powerful teams of learning machines to be constructed. In contrast, we address the issue of how to compose teams.
Reference: [Rog67] <author> H. Rogers Jr. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: Section 6 concentrates on classes unlearnable by a 3 single IIM. 2 Preliminaries The set of all natural numbers is denoted by IN, the set of all single argument recursive functions by F , and the set of partial recursive functions by P <ref> [Rog67] </ref>. Letters h; i; j; k; l; m; n; x; y vary over IN, f and g over F , and '; over P. Classes of recursive functions, i.e. subsets of F , are denoted by U , V , W , with or without decorations. <p> Consider the dual task: For a given function f, find the class of machines which learn it. We find that all such families EX 1 (f ) are pairwise isomorphic accordingly to Definition 20. The definition of recursive operator can be found in <ref> [Rog67] </ref>. Theorem 14 ([Rog67]) Let : P ! P be a recursive operator and f and g partial recursive functions. Then has the following properties: 1. Monotonicity. g h ) (g) (h). 2. <p> Then the respective witnesses and 1 can be chosen to be recursive permutations, i.e. recursive one to one mappings of IN onto itself. Proof: Let g; h be arbitrary witnesses for and 1 respectively. They can be easily made one to one by padding <ref> [Rog67] </ref>. Assume that maps ff 2 P to fi 2 P. Let A; B be the collections of indices for the functions ff and fi respectively.
Reference: [Sch86] <author> G. Schafer. </author> <title> Some results in the theory of effective program synthesis: Learning by defective information. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> 215. </volume> <publisher> Springer-Verlag, </publisher> <year> 1986. </year> <month> 21 </month>
Reference-contexts: A precise correspondence between team learning and probabilistic learning [Pit89] intensified the study of team learning. So far, all of the studies of team learning focus on how team size compares with other parameters relevant to the learning process <ref> [Sch86, PS88, FSV89, KZ91, DPVW91, DKV92, DKV93] </ref>. The goal has always been to see which parameter settings allowed for more powerful teams of learning machines to be constructed. In contrast, we address the issue of how to compose teams.
Reference: [Smi82] <author> C. H. Smith. </author> <title> The power of pluralism for automatic program synthesis. </title> <journal> Journal of the ACM, </journal> <volume> 29(4) </volume> <pages> 1144-1165, </pages> <year> 1982. </year>
Reference-contexts: Team learning has been a prevalent theme in the study of inductive inference [Smi94b]. An early result asserts that the larger the team allowed, the larger the class of learnable sets of functions becomes <ref> [Smi82] </ref>. This suggests that perhaps different types of knowledge are needed to solve some problems. Indeed, most of the papers in our field have at least two coauthors. A precise correspondence between team learning and probabilistic learning [Pit89] intensified the study of team learning.
Reference: [Smi94a] <author> C. Smith. </author> <title> A Recursive Introduction to the Theory of Computation. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Classes of recursive functions, i.e. subsets of F , are denoted by U , V , W , with or without decorations. By using standard encoding techniques, the natural numbers can serve as indices for programs <ref> [MY78, Smi94a] </ref>. The function computed by program i is denoted by ' i . The collection of functions ' 0 , ' 1 , ' 2 , , forms an acceptable programming system.
Reference: [Smi94b] <author> C. Smith. </author> <title> Three decades of team learning. </title> <booktitle> In Proceedings of AII/ALT'94, Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Our results indicate that this is indeed the case, but only if instead of a single concept we consider a suitable infinite set of concepts. Team learning has been a prevalent theme in the study of inductive inference <ref> [Smi94b] </ref>. An early result asserts that the larger the team allowed, the larger the class of learnable sets of functions becomes [Smi82]. This suggests that perhaps different types of knowledge are needed to solve some problems. Indeed, most of the papers in our field have at least two coauthors.
Reference: [Wey52] <author> H. </author> <title> Weyl. Symmetry. </title> <publisher> Princeton University Press, </publisher> <year> 1952. </year>
Reference-contexts: The function f 1 is not identified by T 0 , a contradiction. X Corollary 48 If W 2 [k; m]FIN where k m &gt; 1 2 , then all functions in W are isolated. Proof: Combine Theorems 44 and 47. X 7 Conclusions Felix Klein's Erlagen Program <ref> [Wey52] </ref> introduced a highly successful research paradigm to the mathematics community. The basic idea is to look at the inverse of a problem and study transformations.
Reference: [Wie74] <author> R. Wiehagen. </author> <title> Inductive inference of recursive functions. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 32 </volume> <pages> 462-464, </pages> <year> 1974. </year> <month> 22 </month>
References-found: 28

